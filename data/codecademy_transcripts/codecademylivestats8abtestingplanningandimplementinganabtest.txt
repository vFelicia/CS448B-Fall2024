00:02 - waiting for everyone to file in
00:04 - waiting to see if we're live
00:08 - so he did such a good job last week of
00:10 - um not having any awkwardness in the
00:13 - first 20 seconds of the stream
00:15 - i don't know i honestly don't know how i
00:17 - did that
00:21 - but hopefully i don't know
00:25 - hopefully i can repeat it someday yeah
00:28 - the timing right cool
00:32 - let's see why okay cool we are up and
00:36 - running
00:37 - yay awesome hello everyone
00:40 - dr monkey uk it's always the first one
00:43 - the best
00:45 - it's awesome let us know you're here um
00:49 - welcome to the eighth stream at eighth
00:52 - and final stream in this series
00:54 - um potentially we have more content in
00:58 - the
00:58 - master stats with python
01:02 - path coming out so maybe someday in the
01:05 - future we'll do
01:06 - more streams like this but uh this is
01:09 - the last one in this series so
01:11 - we're really excited to jump in today
01:13 - and do some
01:15 - um and have a conversation about a b
01:17 - testing which i
01:18 - feel like is a very uh commonly used
01:22 - but sometimes misunderstood
01:25 - way that statistics gets used in a lot
01:28 - of different industries and a lot of
01:29 - different companies
01:30 - so i'm excited to dig in per usual we
01:34 - are
01:34 - looking at the youtube chat hi jamie
01:38 - um and hi dr monkey uk
01:41 - if anybody else is here say hello let us
01:44 - know if you have any questions
01:46 - um and feel free to
01:49 - join us in coding on your own computer
01:53 - if you would like yeah before we get too
01:56 - far into this one i'll do a plug for our
01:58 - upcoming series um after this one is
02:00 - over next week we're starting another
02:02 - one
02:02 - with uh g1 another curriculum developer
02:05 - is going to be leading it about kind of
02:06 - creative coding and
02:08 - specifically using javascript and a
02:11 - javascript library called p5.js so i'll
02:13 - plug that more at the very end of the
02:14 - show
02:15 - but uh yeah we're uh we're doing another
02:17 - series starting next week
02:19 - it's gonna be super awesome i'm going to
02:21 - watch all of them and
02:23 - finally learn some things
02:26 - so yeah very excited all right i'm going
02:29 - to share my screen
02:32 - uh let me make sure this time that i get
02:35 - the right
02:37 - one cool
02:41 - um can you see that alex yes that looks
02:44 - good to me
02:45 - okay suddenly usually i can see
02:48 - your face somewhere on the screen but
02:50 - now i can't see
02:52 - so you'll just see i'll be here you'll
02:55 - be
02:55 - just a voice um somewhere in the
02:59 - background
03:00 - all right so um so like i said today
03:03 - we're going to be
03:04 - talking about um about a b
03:07 - testing and um
03:10 - and we're going to approach this in a
03:14 - similar way to the other content that
03:17 - we've approached
03:19 - or the other statistical concepts that
03:21 - we've learned in the series
03:23 - we're gonna think about both um
03:26 - a researcher perspective so someone that
03:28 - works at a company and then we're gonna
03:30 - think about
03:31 - um a like all-powerful perspective
03:35 - if you had all the information you could
03:37 - possibly
03:39 - have in the world um what
03:42 - decisions or what would you then
03:46 - be able to glean from this process um
03:49 - so a little bit of background
03:53 - sophie and before we go too far i also
03:54 - wanted to plug that uh for the last 15
03:56 - minutes or so of this stream we're gonna
03:58 - bring
03:59 - on uh dan layfield who is a product
04:02 - manager here at codecademy
04:03 - he runs a lot of the a b testing that we
04:05 - do and so
04:06 - we can ask him kind of what this looks
04:08 - like in the real world
04:10 - so yeah another thing to think about if
04:12 - you have any questions that you want to
04:13 - ask
04:14 - somebody who's implementing some of
04:16 - these statistical concepts
04:18 - um we'll try to leave like 10 15 minutes
04:20 - for dan
04:22 - exactly yeah i'm super excited because i
04:24 - think i
04:25 - i like to focus on a lot of the theory
04:27 - uh
04:28 - or i have focused on a lot of the theory
04:30 - in these live streams
04:32 - and so to have somebody that is actually
04:34 - implementing this
04:35 - in real time i think is a really nice
04:37 - complement to
04:38 - everything that we've been learning so
04:41 - cool um
04:42 - okay so i guess the thing that we should
04:45 - start with is maybe
04:47 - just talking through what an a b test is
04:50 - or like what an a b test is kind of
04:53 - used to do so alex
04:57 - i mean you're not an expert in av
05:00 - testing but i'm sure you've seen some a
05:03 - b
05:04 - tests before seen some results or
05:07 - heard about them so can you give like a
05:11 - one minute explanation of what you think
05:14 - an a b test
05:15 - is all about sure so i my general sense
05:18 - of what an a b test
05:19 - is is um comparing two different options
05:24 - um of something and comparing
05:28 - the results of those options so um
05:31 - we've talked about this with curriculum
05:32 - development where we don't actively do
05:34 - this but it'll be something
05:35 - great that we would love to be able to
05:37 - do is like
05:38 - oh here's a paragraph about for loops
05:41 - written this way here's a paragraph
05:43 - paragraph about four loops written the
05:44 - other way which one is better
05:46 - um or which one do learners find easier
05:48 - to understand or do better on the quiz
05:50 - or
05:51 - or whatever that metric is that we're
05:52 - interested about um
05:54 - we compare those simultaneously and i
05:56 - think one thing that
05:58 - i think that we could do is run
06:01 - you know run paragraph one for week one
06:04 - and then
06:04 - edit it and run paragraph two for week
06:06 - two but i think one of the main
06:10 - important things about a b testing is
06:11 - that it has to be like run
06:12 - simultaneously right or you try to like
06:14 - keep the groups that are
06:16 - interacting with the content as similar
06:18 - as possible i guess
06:20 - yeah we can definitely talk about that a
06:22 - little bit but yes i think
06:24 - in an ideal world you want to set up
06:27 - your experiments so that
06:28 - your two groups that you're comparing
06:30 - are identical
06:32 - in or your two or more groups that
06:34 - you're comparing are identical
06:36 - in um observable and unobservable ways
06:40 - and the only way that
06:41 - you can guarantee or that you can
06:45 - make the unobservable make the group's
06:48 - balance based on unobservable
06:49 - characteristics
06:50 - is by using randomization
06:54 - um and so that means that like you
06:56 - basically have to have a group of people
06:59 - and then randomly assign them to groups
07:03 - and if you do it based off of something
07:06 - temporal like time like the first 500
07:09 - people and then the second
07:11 - 500 people um then those groups are no
07:15 - longer randomly
07:17 - um right you could argue that the first
07:19 - 500 people had some different motivation
07:21 - or just a different
07:22 - group of people or maybe you know
07:26 - maybe people who um
07:29 - are accessing codecademy between the
07:32 - hours of
07:33 - 8 a.m to 5 p.m
07:36 - gmt are different than um
07:40 - the ones that are accessing it from 5
07:42 - p.m
07:43 - to the next right so
07:46 - yeah so um one thing i'm gonna do so
07:50 - that was a
07:51 - great explanation and one thing we're
07:53 - gonna do here is we're gonna actually
07:55 - simulate some data that might look like
07:57 - data that came from an a b
07:59 - test so i've written out some code to do
08:01 - this here
08:02 - but um i'm gonna write out that code
08:06 - in front of you guys right now um again
08:09 - so that we can talk through
08:10 - what it's doing so um
08:14 - let's imagine for a minute that we're
08:18 - interested in um two different versions
08:21 - of a
08:22 - subscribe button and we'll call them
08:24 - like version a
08:25 - and version b um and
08:29 - we want to know whether version b
08:32 - or version a is better in terms of
08:35 - the proportion of people that are gonna
08:38 - click on the button
08:39 - if it surfaced to them so um
08:42 - it's a stupid example but we're just
08:44 - gonna go with it for simplicity
08:46 - what a great plug to subscribe to our
08:48 - youtube channel and
08:50 - academy pro and all of our various
08:51 - things we want you to subscribe to
08:53 - exactly
08:54 - we'll pretend that we're trying to get
08:56 - you to subscribe
08:58 - um and so we can imagine
09:02 - for right now that um we have some
09:05 - subscribe button that we're already
09:06 - surfacing to people so
09:08 - uh let's call that the like
09:12 - a button um in the in the spirit of a b
09:15 - testing so um
09:19 - let's say we've got our a button
09:23 - that is the button that we're using now
09:25 - um
09:26 - and based on historical data about
09:29 - half of people who see that button are
09:34 - are clicking it that would be a very
09:36 - high rate but we're just going to
09:37 - pretend like
09:38 - that's how popular we are um
09:42 - and then let's say
09:45 - we want to know if some new button that
09:48 - we're going to design
09:50 - is going to be better than the button
09:52 - that we already have
09:54 - and let's say that we'll feel
09:57 - like it's a better button if it performs
10:01 - such that 60 percent of people click on
10:05 - it
10:06 - um one quick note
10:09 - and this is just like a vocabulary thing
10:12 - so you'll see right that
10:15 - our existing button has 50 rate
10:18 - our new button we're hoping is 10
10:21 - percent higher so
10:22 - a 60 click rate the difference between
10:25 - these two numbers is 10
10:27 - but a lot of the time people will use
10:30 - another word
10:31 - called lift which often
10:35 - or is generally reported as the percent
10:38 - increase of these percentages so
10:43 - that sounds complicated but basically
10:45 - what it means is
10:47 - 60 right
10:50 - is 2 i believe am i doing this right
10:53 - 20 higher
10:57 - than 50 right so
11:00 - so look at looking down i am already
11:02 - like looking ahead to that other block
11:03 - of code where you have one plus lift
11:05 - times a rate so
11:08 - if we want if we want b rate to be 0.6
11:11 - that lift
11:11 - is going to be it's not going to be 10
11:14 - because that'll be
11:15 - 1.1 times a rate which would be
11:19 - 0.55 so yeah i think it's going to be 20
11:21 - but so you could do a little bit of
11:22 - algebra to figure out what that
11:24 - lift is based on what percentage you
11:26 - actually want the b rate to be
11:27 - yeah so i'm not going to go into too
11:29 - much detail on that but
11:31 - um but for the time being let's say that
11:33 - the a rate was
11:35 - the our baseline rate that we already
11:37 - have is about 50
11:39 - and we want to see if we can increase
11:40 - that to at least 60
11:43 - um so let's let's simulate some data
11:46 - then
11:47 - um so where i'm going to use the same
11:50 - random functions that we've been using
11:52 - prior to this so
11:54 - i'm going to use the numpy.random.choice
11:58 - function
11:58 - i'll just copy these over but we'll well
12:02 - yeah we'll change things around we'll
12:05 - like
12:06 - hard code some of these numbers just so
12:07 - that you see what's going on
12:10 - so let's say we show
12:14 - this button that we've in our experiment
12:17 - we're going to show this button that we
12:18 - already have
12:20 - to 100 people
12:23 - right and we're gonna simulate some data
12:27 - where for those 100 people
12:31 - they have a 50 probability of
12:34 - clicking the button and a 50 probability
12:37 - of not
12:38 - clicking the button yeah and so this
12:39 - syntax looks really similar to
12:41 - in previous lessons when we were
12:42 - flipping a coin right the coin could
12:45 - either be heads or tails
12:46 - and we said okay a fair coin is 50 50
12:49 - but sometimes we were flipping an unfair
12:51 - coin where it was like ninety percent
12:53 - ten percent right and so that's what
12:54 - that p value is
12:56 - or that p array is doing is saying the
12:58 - the odds of getting
13:00 - yes is um the first the first value and
13:03 - the odds of getting no is that second
13:04 - value
13:05 - so for a rate it's 50 50 and for b rate
13:07 - it's going to be
13:08 - um what 60 40 i guess 4.6 i guess i can
13:11 - put in the
13:12 - actual numbers in here so later
13:16 - we can kind of code these with things
13:18 - like a rate 1 minus a rate
13:21 - b rate 1 minus b rate but just so that
13:24 - we can read this
13:25 - super easily let's just put this all
13:28 - together so you can see okay
13:32 - so if we simulate some data like this so
13:35 - we're simulating again
13:36 - data where um people have a 50
13:40 - chance of clicking the a button they
13:42 - have a 60 chance of clicking the b
13:44 - button then we can kind of assemble this
13:48 - in a in a data frame
13:52 - so i'm gonna i'm gonna put together a
13:55 - data frame
13:56 - where i'm gonna have a column that tells
13:59 - me whether or not someone
14:01 - clicked a button i'm gonna have a column
14:03 - that tells me which button they saw
14:06 - so to do that what i'm gonna do is i'm
14:09 - actually going to
14:11 - combine these two
14:14 - arrays into a single array and there's a
14:17 - bunch of different ways to do that
14:19 - i have i have grown to really like lists
14:23 - and pythons
14:25 - in terms of like appending arrays or
14:28 - appending lists and like
14:30 - it's happening so sophie uh sophie
14:32 - joined codecademy and was a huge r fan
14:34 - and i was always the python guy
14:36 - and now now she loves python lists yeah
14:39 - well
14:40 - they are pretty nice i think that bass
14:43 - python
14:44 - is like easier
14:47 - than bass r in terms of like
14:50 - general programming things like but
14:54 - in terms of actually running actually
14:57 - doing an analysis
14:58 - or like running statistical it's really
15:00 - easy to do that in r
15:04 - um okay so if i do this really quickly
15:06 - i'm just gonna show you guys what this
15:07 - looks like
15:08 - if i just add these two lists together
15:11 - all it's gonna
15:12 - do is it's gonna take it's gonna take
15:16 - this list and append this list on to the
15:19 - end so
15:20 - just put all of those together so we
15:22 - just have to know that the first hundred
15:24 - were the people that saw button a and
15:25 - the 700 are people that saw button b
15:28 - exactly so yeah the first hundred are
15:30 - the people that saw button a
15:31 - and so now i'm gonna make another list
15:33 - to go alongside this one
15:35 - um that says so this is gonna be
15:39 - uh clicked or outcome
15:42 - maybe we'll call it and then
15:46 - we'll make a second list where we have
15:49 - a hundred a's and then a hundred b's so
15:51 - it can kind of line up right alongside
15:54 - this one so i'm gonna do
15:57 - list a or sorry list
16:02 - where i put a
16:05 - and i'm gonna this is why i like this
16:08 - you can do that and that will just
16:11 - repeat it
16:12 - a hundred times so let me print what
16:15 - this looks like
16:16 - so what i just did was created a list
16:19 - with
16:20 - a hundred a's in a row and then
16:23 - a hundred b's in a row yeah one thing uh
16:27 - and maybe some python experts in the
16:28 - chat i'm always a little bit hesitant to
16:30 - do that i think with like
16:32 - objects those might all when you use
16:35 - multiplication with a list
16:36 - it might all point to the exact same
16:38 - object so it's like if you edited one
16:40 - you'd be editing all of them oh really
16:43 - but i think strings might be different i
16:45 - and i honestly i don't even know if
16:46 - that's true but i've always been a
16:47 - little bit hesitant to use that
16:49 - um or it's good to check to make sure
16:52 - that
16:52 - these are not all copies of each other
16:54 - if you're using um
16:56 - multiplication like that wow i did not
16:59 - know that
17:00 - um well i know that this works
17:04 - for what i need it to do i turn it back
17:07 - to an empire
17:08 - so i think it will be okay but that's
17:11 - super interesting
17:12 - yeah some and that might not even be
17:14 - accurate but for whatever reason i've
17:16 - always been slightly paranoid about
17:17 - using
17:18 - that uh that multiplication with lists
17:20 - um
17:21 - well then now i don't like python
17:23 - anymore
17:25 - jokes okay um
17:28 - all right here we go
17:31 - let's see if we can
17:37 - this okay so here is our
17:41 - simulated data where the first hundred
17:45 - rows saw group a or stop button a
17:48 - the second hundred rows saw button b and
17:50 - then we have
17:52 - simulated information about whether or
17:54 - not they clicked
17:55 - the button um and every time we run this
17:58 - we're gonna get slightly different
18:00 - a slightly different data set but the
18:02 - probability of clicking in group a
18:04 - versus group b will be the same every
18:07 - time so right now i see
18:08 - yes no yes yes yes and if i run this
18:11 - again
18:12 - now i've got yes no no no no so it's
18:15 - going to be slightly different every
18:17 - time
18:17 - yeah so if you one thing that we haven't
18:19 - really talked about as we've been
18:20 - doing all these exercises where we're
18:22 - simulating stuff by like random
18:24 - distributions
18:25 - is we've never talked about how to like
18:26 - seed your
18:28 - random number generator and why you
18:30 - might want to do that
18:32 - um can you talk a bit about like what it
18:34 - means to to
18:35 - see your random library sure
18:39 - yeah so i think one way and here i'm
18:42 - gonna actually
18:42 - just to make this a little easier to see
18:45 - i'm gonna print
18:46 - um a cross tabulation of this
18:50 - uh data
18:53 - group clicked
18:56 - um so that we can see like four numbers
18:59 - instead of
19:00 - oops yeah so we can see four numbers
19:03 - instead of the whole data set
19:05 - um okay so like ada alex was saying
19:09 - um when we run
19:12 - something that's like when we use a
19:15 - function that is using some sort of
19:17 - random number generator um
19:21 - we sometimes might want to
19:25 - like write our code in a way that
19:28 - another person could replicate it so um
19:32 - if i pass this this code on to alex
19:35 - and i want to say like this is what i
19:36 - did and i want you to see the same
19:38 - output that
19:39 - i got or if i want to have him like
19:41 - replicate exactly what i was doing
19:44 - um that gets really difficult when
19:46 - you've got
19:48 - random functions in your code
19:52 - um so one thing you can do is you can
19:55 - set
19:56 - a seed i think um it's just like
19:59 - random look it up as you're doing this
20:00 - it might be np.random.seed but
20:02 - yeah i think that's like random
20:07 - i don't know let's try it
20:12 - yeah i think that'll do it okay so
20:15 - now you'll see right this time we got 50
20:18 - we'll just focus on this number 51
20:20 - but you'll see um now that i've set a
20:23 - seed
20:25 - i should get the same numbers every time
20:28 - i
20:28 - run this so even though it's a random
20:31 - process
20:32 - i'm generating the same random outcome
20:36 - every time i run this by setting the
20:38 - seed and you can set the seat this
20:39 - number to anything
20:41 - and every unique number that you input
20:43 - or every unique seed that you input will
20:45 - give you
20:46 - um a different result but like as long
20:50 - as you use the same
20:51 - random seed you'll get the same result
20:53 - every time
20:54 - yeah compared to if we if we delete that
20:56 - line and just keep running it those
20:57 - numbers are going to be bouncing around
20:59 - right they're they're going to be
21:00 - different each time um
21:03 - i find this like i find this helpful i
21:05 - use this a lot in
21:06 - my like coursework when i'd be working
21:08 - on a problem set that somehow involved
21:11 - um random numbers and you know i was
21:13 - like trying to debug my code or trying
21:14 - to like
21:15 - trace through my code to understand
21:16 - what's happening and it's really
21:18 - difficult if
21:19 - some part of that is changing every time
21:21 - you run your program and so
21:23 - yeah i find it helpful for debugging
21:25 - where it's like okay you know that these
21:27 - numbers are always going to be the same
21:28 - even though
21:28 - they're still random um and that way you
21:31 - can kind of like trace through what's
21:32 - happening
21:33 - nice yeah that's super helpful uh
21:37 - i i feel like i use this a lot um if i'm
21:40 - ever
21:41 - trying to do an analysis where i need
21:45 - to use a function that has some sort of
21:47 - random generator like a lot of
21:49 - machine learning algorithms even have
21:52 - um some like use some sort of
21:55 - random gener random process as part of
21:59 - fitting the algorithm so like trying to
22:02 - find the minimum of the
22:04 - of some surface you'll like have you'll
22:06 - start in random places on the surface or
22:08 - something to try to
22:10 - converge towards the minimum and so you
22:12 - get slightly different
22:13 - numbers every time um and
22:17 - if you want your research to be
22:19 - reproducible which is a really good
22:21 - practice
22:22 - or you want somebody else to review your
22:23 - work then
22:25 - it's a good idea to use a random seed
22:27 - because then somebody else can
22:29 - verify um that they get the same result
22:33 - cool yeah um okay so
22:37 - now that we have this kind of fake data
22:39 - set we can think about
22:41 - what we would do as a researcher if we
22:44 - got this data
22:45 - so if we were working at a company
22:49 - and we saw we ran this experiment and we
22:53 - saw these numbers we saw
22:56 - in group a 47
22:59 - people did not click our button and 53
23:01 - people did versus
23:02 - in group b 37 people did not click the
23:05 - button and 63 did
23:07 - you might think okay well this looks at
23:09 - least
23:10 - somewhat convincing that a higher
23:13 - proportion of people
23:15 - are clicking the button in group b
23:18 - and now we want to know is that
23:22 - is that sustainable if we run this
23:24 - experiment again so same way
23:25 - as we were discussing last week and the
23:28 - week before
23:29 - hypothesis testing is really built
23:31 - around this idea that
23:33 - you can only observe a sample but you
23:35 - want to know something about a larger
23:37 - population that you can't observe
23:38 - and in the case of this fake a b test
23:41 - that we're
23:41 - running the larger sample that we can
23:44 - observe is all the users who could ever
23:47 - or all the visitors to youtube that
23:50 - could ever
23:51 - view this subscribe button either way
23:54 - and um and we can't see what all of them
23:58 - would do
23:58 - so we only want to change the button
24:00 - because it takes some effort
24:02 - if we feel reasonably confident based on
24:05 - our sample
24:06 - that we'll still see a higher rate of
24:09 - clicks
24:10 - among the larger population to whom we
24:13 - surface these buttons so
24:17 - um we're gonna run a hypothesis test
24:20 - i don't think we've gone over this
24:21 - hypothesis this particular hypothesis
24:23 - test
24:24 - in um in one of these sessions yet
24:27 - but depending on what kind of a b test
24:29 - and what metric you're
24:31 - interested in you might use different
24:34 - hypothesis tests
24:35 - in this case you can kind of think of
24:37 - this as two categorical variables right
24:39 - what group you're in
24:40 - and whether or not you click something
24:43 - and so the outcome we care about
24:45 - because it's categorical it's like do
24:48 - you click or not click
24:49 - um one of the tests that hypothesis
24:52 - tests we could use here is
24:53 - a chi-square test if the outcome we
24:56 - cared about was something
24:57 - quantitative like how much time they
25:00 - spent
25:00 - watching our youtube video or something
25:02 - like that um
25:04 - then we would need a different kind of
25:05 - test
25:07 - so i'm just gonna demo running this test
25:11 - i'm not gonna go into the kind of
25:13 - specifics
25:14 - of what's involved with the test but
25:17 - um we can come back to that if there's
25:21 - time i just want to make sure that we
25:22 - have
25:23 - time to talk about that the results and
25:26 - why they're
25:27 - interesting um so i believe
25:30 - let me just grab i'm gonna grab the
25:34 - uh the test from here because i always
25:38 - forget
25:40 - what the function is and the order of
25:43 - the parameters
25:44 - sophie do you want to roast me about the
25:46 - chi-square test
25:48 - oh one of my favorite stories
25:53 - or one of my favorite moments from when
25:55 - we were building this
25:57 - stats curriculum is alex the thought
26:00 - that the chi square test which it's
26:02 - totally valid you're not wrong
26:04 - not that it was pronounced chai like the
26:06 - t
26:07 - and i don't know it just made me laugh
26:11 - sophie's the stats expert no i just
26:15 - i mean like kai is a
26:18 - a greek letter right yeah
26:21 - [Laughter]
26:23 - but now i now whenever i think of it i
26:25 - think of like
26:27 - someone eating or drinking two cups of
26:29 - tea and it makes me happy
26:32 - yeah maybe test the tea
26:35 - um okay so what i'm going to do is i'm
26:39 - going to save
26:40 - this contingency table as
26:44 - a b contingency and that's the input
26:47 - that i give
26:48 - to this function so basically when i run
26:51 - this test
26:52 - i input this two by two table
26:56 - and then it's going to give me a p value
26:58 - which is testing
26:59 - the null hypoth hypothesis that there's
27:01 - no association between these two
27:03 - variables so
27:04 - there's no relationship between the
27:07 - group
27:08 - identity and whether or not somebody
27:11 - clicked um i'm seeing some questions in
27:14 - the chat
27:15 - uh are we looking to see that the
27:17 - observed higher rate of clicks it's not
27:19 - just higher
27:20 - lift but is 20 higher before then
27:22 - checking to see if it translates to the
27:24 - greater population with our hypothesis
27:27 - test um
27:30 - so that's a good question so
27:34 - i think it will become a little bit
27:36 - clearer in the next step
27:38 - but for right now let's just imagine
27:41 - that we
27:41 - are we are all powerful and we know that
27:45 - these are the true rates
27:46 - um when we come back to the next piece
27:49 - of it
27:50 - we'll talk through how if you're
27:53 - if you're the researcher um and you're
27:56 - planning an a b test you have to think
27:58 - about
27:59 - how big of a difference you care to
28:01 - measure
28:02 - in your test so once you once you have
28:05 - some data
28:07 - you definitely want to measure what is
28:09 - the rate in group a
28:10 - what is the rate in group b and how
28:13 - how large is that and then run the
28:15 - hypothesis test to see it
28:17 - if that's a significant difference right
28:20 - that you can reject the null hypothesis
28:23 - that the difference
28:24 - or that the association um
28:27 - does not exist or that you can reject
28:29 - the null hypothesis that these numbers
28:31 - are um are the same
28:34 - but so sophie i guess the question that
28:36 - i have then is
28:38 - do you set those rates um
28:41 - you set those rates before you view the
28:45 - the samples right whereas it's not
28:46 - because i could see a world where you're
28:48 - like
28:48 - okay we've like made this change we see
28:50 - that um
28:52 - you know group a got 53 yeses group b
28:55 - got 63 yeses is that a real difference
28:59 - right like
29:00 - i could see coming up with those rates
29:02 - after the fact
29:03 - after getting the two samples but yeah
29:06 - so
29:06 - basically this so i will explain this
29:11 - in the next step but for
29:15 - for a hypothesis test or for an a b test
29:18 - usually you're comparing and we can ask
29:22 - dan a little bit about this later too
29:26 - but oftentimes you're comparing
29:28 - something that already exists
29:30 - to a new um a new
29:33 - version okay and so you'll have some
29:37 - baseline data so you'll probably know
29:39 - this number
29:41 - right you'll know like the thing that
29:43 - already exists
29:44 - how does that perform on the website but
29:47 - then
29:48 - you're not going to know what this
29:50 - number is right
29:52 - the only thing that you need to decide
29:54 - before the test
29:55 - is how big of a difference
29:58 - you care about because for example
30:02 - right there could be a true difference
30:05 - but this could be
30:08 - 0.5001
30:10 - and like you could have a test that
30:13 - detects that difference
30:15 - right we call that like a effect size
30:17 - and this would be a small effect size or
30:19 - a small
30:20 - lift so you could design a test to
30:24 - to detect that difference but you
30:25 - probably don't care about it
30:27 - so we'll come back to this when we
30:30 - come back to the kind of
30:34 - researcher side of it but for right now
30:36 - let's just imagine
30:37 - that these are known rates okay
30:40 - so if these are known rates
30:44 - we could generate this data set we could
30:46 - run our chi-square test
30:48 - the null hypothesis for the chi-square
30:50 - test is that there's no
30:51 - association between these two things
30:53 - another way of thinking about that is
30:55 - that these two rights are the same
30:57 - and then we can print our p-value and
30:59 - see whether we reject the null
31:01 - hypothesis
31:02 - that there's no association and in this
31:05 - case we wouldn't
31:06 - oh that's surprising i would think
31:16 - that
31:20 - why am i getting oh there we go so in
31:23 - this one
31:24 - we got .002 it's surprising that we got
31:28 - a p-value of 0.57 i would have thought
31:30 - that
31:31 - with this big of a difference we would
31:33 - usually reject the null hypothesis
31:36 - and think that there was a significant
31:38 - difference but i guess not
31:40 - could it be sample size of size of 100
31:43 - isn't big enough
31:44 - maybe we'll have to come back to this so
31:47 - um i i think this is the next
31:50 - interesting question once we get in a
31:52 - little bit further into our simulation
31:54 - let's put this let's make this a bigger
31:56 - difference make it like
31:59 - and then that has to propagate through
32:01 - clicks b
32:02 - oh oh that's right
32:06 - because i hard coded it
32:11 - yeah so now i'm getting smaller
32:14 - i'm getting small p values yeah
32:17 - um right so if this p value is small
32:20 - enough
32:20 - below some significance threshold then
32:23 - we reject the null hypothesis
32:25 - and we say that we we think that these
32:27 - things are associated
32:28 - which in the case of this example would
32:31 - lead us to
32:31 - believe that um
32:34 - the b rate was or the the b button was
32:38 - better at getting people to
32:40 - click on our button
32:44 - okay so let's now
32:47 - zoom back out and let's do what we've
32:50 - done a bunch of times
32:52 - and put this all into a for loop and
32:54 - then think about what happens when we
32:56 - run the same
32:58 - random experiment a bunch of times
33:03 - and each time let's think about
33:06 - whether or not we get a significant
33:08 - result
33:09 - as i'm setting this up um
33:12 - alex do you want to talk through so
33:17 - from i'm thinking from the perspective
33:19 - of a researcher
33:20 - what do you think are some of the
33:24 - pro like some of the things that are
33:29 - some of the considerations that you
33:31 - might want to take in terms of
33:33 - how you design this experiment like do
33:36 - you think
33:37 - that um it's just always good to have
33:40 - as large of a sample size as you could
33:43 - possibly get or do you think you want to
33:44 - prioritize like
33:46 - um how the size of the difference that
33:50 - you're trying to measure
33:52 - you have any ideas for like what what
33:54 - are the considerations that you might
33:56 - have to take into account
33:58 - as you're trying to design this um
34:01 - yeah interesting i don't know if i have
34:03 - any anything that
34:05 - sticks out to the top on the top of my
34:07 - head but yeah i would say that
34:09 - in general i would assume that get
34:11 - getting as large as sample size as
34:12 - possible
34:13 - is good um i would
34:16 - again another thing that i kind of when
34:19 - i hear about a b testing the thing that
34:20 - i hear about is like trying to get the
34:22 - groups as um as
34:26 - random as possible or as similar as
34:27 - possible um and randomly split up
34:30 - so doing things like making sure that
34:33 - they're kind of interacting with this at
34:35 - the same time
34:36 - um what could um
34:40 - what do you think i'm trying to like ask
34:43 - you yeah
34:44 - leave me a question but come up with the
34:46 - right way to frame them
34:48 - um what is
34:54 - so okay so let's say that there's a
34:56 - difference a real difference between two
34:58 - groups like let's say that
35:00 - button b really is better than button a
35:04 - right um but
35:07 - it's only a little bit better
35:12 - and it's it's hard for us to measure
35:16 - it there's kind of there there are two
35:18 - different ways that we could make a
35:20 - mistake or there's
35:21 - in this case there's one way that we
35:22 - could make a mistake we could make a
35:24 - mistake
35:25 - by not finding that difference even
35:27 - though there is one
35:29 - um right so that's that has to do with
35:32 - like p-value right and what p-value we
35:34 - consider being significant or not
35:38 - um but on the flip side if there's no uh
35:42 - there's no difference between the groups
35:44 - and
35:45 - we make a mistake and we find that there
35:47 - is a difference
35:48 - then that's also not a great thing
35:52 - and so maybe can you walk through
35:56 - from the perspective of a company like
36:00 - how someone might think through those
36:01 - considerations like what goes wrong
36:03 - when you make each of those kinds of
36:05 - mistakes
36:07 - or what's the what could go wrong if you
36:09 - make either of those kinds of mistakes
36:12 - like practically what could go wrong
36:13 - well so like you're yeah
36:15 - ultimately these tests result in some
36:17 - decision being made right and so if
36:20 - um and i'm not gonna know whether it's
36:23 - type one or type two but if it's um
36:26 - if you say that you found a significant
36:28 - difference and like okay let's go ahead
36:30 - and implement this button and there's
36:31 - really no difference
36:32 - then that's a lot of wasted work to to
36:35 - go and make that change
36:37 - um and i suppose the opposite is true
36:39 - for the other type of error if you find
36:40 - that there's no difference but there
36:41 - really is one
36:42 - then you've missed out on this
36:43 - opportunity
36:46 - yeah so right so exactly
36:49 - there there's two kind of things going
36:52 - on here where
36:53 - you if there's a real difference then
36:57 - you want to be able to detect it but if
37:00 - there's not a real difference
37:02 - and you detect one then you're
37:05 - potentially going to put
37:06 - resources into changing something that
37:10 - doesn't need to be changed or doesn't
37:12 - help you so
37:13 - there's kind of two different types of
37:16 - mistakes that you might want to think
37:17 - about
37:18 - prioritizing or not prioritizing when
37:22 - you're trying to plan an a b test
37:25 - so um i'm gonna just grab
37:28 - because i i'm conscious of time and i
37:31 - want to make sure that we have
37:32 - plenty of time to um ask questions
37:36 - later uh with dan so
37:39 - i'm going to just grab some code from
37:41 - here
37:42 - but essentially what i'm going to do
37:45 - in fact i'm just going to grab all of
37:48 - this
37:49 - and then i'll talk through what it's
37:50 - doing yeah
37:52 - so if you while you do that we have a
37:53 - question from the chat of um and i think
37:55 - i know the answer this but
37:56 - uh maybe i'm wrong can this be used to
38:00 - hypothesize difference in male female
38:02 - wages like can you say okay
38:04 - male wages are group a female
38:07 - female wages or group b um
38:10 - can that be something that you use this
38:12 - for yeah so
38:14 - you could use a different kind of
38:17 - hypothesis
38:18 - test for that in that case you're
38:20 - talking about an association between
38:23 - a categorical variable like male and
38:26 - female
38:27 - or whatever other
38:30 - genders exist right so you've got a
38:33 - categorical variable and then you have
38:36 - or what's usually treated as a
38:37 - categorical variable and then you have
38:40 - um a quantitative variable
38:43 - which is wages and so if you're looking
38:45 - at an association between those two
38:48 - then you would need a different
38:49 - hypothesis test but you could definitely
38:52 - think about it in a similar way
38:58 - you would probably use like a two-sample
39:01 - t-test
39:02 - or you could also use a z-test um
39:05 - okay so let me talk through what i've
39:07 - got here
39:09 - so same as we had before now
39:12 - i've rewritten this as lift but
39:15 - i'm gonna just uh
39:18 - i'm gonna just put in point six again
39:21 - there we go point seven
39:22 - um okay so we've got a group a rate a
39:25 - group b
39:26 - rate we've got our sample size which is
39:29 - getting split
39:30 - into the two groups so actually here
39:32 - we've only got 50 in each group
39:34 - maybe i'll up it to 200 so we've got 100
39:38 - in each group
39:39 - we stimulate our data we run our
39:41 - chi-square test
39:42 - and then we'll save the result in this
39:45 - list
39:46 - called results where we'll save
39:48 - significant
39:49 - if the p-value is less than our
39:51 - significance threshold
39:52 - and we'll save not significant if it's
39:55 - not
39:56 - less than our significance threshold for
39:59 - right now we'll use a significance
40:00 - threshold of 0.05
40:03 - and then at the very end we're going to
40:05 - calculate the
40:06 - the proportion of the results that were
40:08 - significant and we'll calculate the
40:10 - proportion that are not significant
40:12 - okay okay so let's run this
40:17 - and so before we run this we expect this
40:19 - to be a significant
40:21 - difference or at least i would expect
40:23 - this to be a significant difference
40:25 - um and so i would expect that proportion
40:29 - to be pretty high if a lot of them are
40:31 - significant and not many of them are not
40:33 - significant
40:35 - yep exactly so in this case
40:38 - we've got 73 percent that are
40:40 - significant
40:41 - and 27 that are not significant
40:46 - so um let's talk through for just a
40:49 - second
40:51 - what situation we're in so in this case
40:55 - right because we have a group a
40:58 - rate in this example that's different
41:01 - from our group b
41:02 - rate so again we're all powerful all
41:05 - knowing we know
41:06 - that these rates are different we're in
41:08 - the situation where
41:09 - we want to reject the null hypothesis
41:12 - because there really is a difference
41:14 - right like the correct
41:16 - answer would be a significant p-value
41:20 - right so in this case we've got
41:23 - that the proportion of significant
41:25 - results is
41:27 - is 0.73 so 73
41:30 - of the time we detected this true
41:33 - difference
41:34 - and 27 of the time we did not
41:38 - what do you think will happen like if i
41:40 - make this
41:42 - bigger so the difference is even bigger
41:45 - alex what do you think will happen will
41:48 - this number
41:49 - go up or will it go down i imagine it
41:51 - will go up they seem to be it seems to
41:53 - be like this
41:54 - this is very similar to like how the
41:55 - p-value was the
41:57 - type one error rate right um so i'm
42:00 - guessing yeah
42:01 - yeah that will be the next question but
42:03 - yeah so
42:05 - exactly so if i make this difference
42:08 - even bigger
42:10 - and i run this now 98
42:14 - of the time the results are significant
42:17 - so
42:18 - the point is here the larger the
42:21 - difference between the two groups
42:24 - the more likely it is that we'll be able
42:27 - to detect a difference when there really
42:29 - is one
42:30 - so can i ask like the inverse what if
42:32 - these two groups are actually the same
42:34 - and if they're actually the same then
42:36 - shouldn't we expect to get
42:38 - like in if there was no randomization
42:40 - then we would expect to get
42:41 - 100 not significant right
42:44 - so yeah actually uh
42:49 - what do you remember or do you wanna
42:52 - take a venture or venture guess what the
42:56 - uh
42:56 - proportion of significant results will
42:59 - be
43:00 - if i do this is is this the p-value now
43:04 - is this going to be 0.05
43:06 - it's not the p-value but it's going to
43:08 - be the signal or the significant
43:09 - threshold yeah that's what i'm at
43:10 - yeah it should be approximately 0.05 so
43:13 - this is
43:14 - actually what we were going over the
43:15 - other week yeah cool
43:17 - right so you'll see that which is fun
43:21 - um right so we gotta
43:24 - take into account exactly what alex is
43:26 - saying
43:28 - two different types of mistakes if we
43:31 - if the truth is that these two rates are
43:33 - the same
43:34 - then we're going to make a mistake
43:38 - about the same percent of the time as
43:40 - whatever
43:41 - significance threshold we set for our
43:43 - test and if the truth
43:45 - is that the rates are different then
43:49 - we're going to be more able or more
43:51 - likely able to detect that difference if
43:54 - it's large the other thing that will
43:58 - help us detect a difference
43:59 - is the sample size so
44:02 - right now if we had this at 200
44:06 - and we've got this proportion of
44:08 - significant results that's
44:10 - equal to 73 if we up this to 400
44:15 - we're gonna get a higher proportion of
44:16 - results that are significant
44:18 - so what does this mean practically when
44:20 - you're like designing a
44:22 - experiment um so practically
44:26 - and that's where we're gonna kind of
44:27 - move in and we'll move into
44:29 - this discussion and we'll move into our
44:31 - discussion then
44:33 - um is practically the things that you
44:36 - need to know
44:37 - as your cal as you're setting up an a b
44:40 - test
44:40 - are the baseline conversion rate
44:44 - the lowest in this case
44:47 - it's the smallest difference that you
44:49 - care to measure so
44:51 - right we saw that as long as the
44:54 - difference between the two groups is
44:55 - some number or bigger
44:57 - your probability of
45:00 - of detecting a real difference is just
45:03 - going to go up
45:04 - right so whenever we calculate
45:07 - a sample size we use something we use a
45:11 - statistical power level
45:12 - of 80 and that corresponds to
45:17 - whoops this uh
45:21 - proportion of results that are
45:22 - significant if there is
45:24 - a real difference oops real difference
45:27 - between the groups
45:28 - so here actually if we had
45:32 - um a sample size of 400 and the real
45:35 - difference between the groups was 20
45:38 - then our power would be 99
45:42 - so so this tool is saying if we're only
45:45 - interested in 80
45:46 - per area and 80 statistical power then
45:48 - we don't need 400 people we only need
45:49 - 100 people or something like that yeah
45:52 - we're basically saying
45:53 - that if we want
45:56 - power of at least 80 for to detect
46:01 - a difference in
46:04 - rates of at least 10 percent
46:07 - for two different variants how big is
46:09 - the sample size that we need
46:11 - and that's what this calculator is is
46:14 - calculating for us
46:15 - and really the things that you need to
46:17 - know to plan an experiment are
46:19 - how big of a sample do we need for each
46:22 - group
46:23 - you might need to calculate the duration
46:25 - so like how much
46:26 - traffic do you have um and how long is
46:29 - it going to take you to
46:30 - show your buttons to that large of the
46:34 - sample size
46:35 - um and then right like how big
46:38 - of and in order to calculate these
46:41 - things you need to know
46:42 - um this is kind of the inverse of that
46:45 - significance threshold so
46:46 - a 5 or a 0.05 significance threshold
46:50 - would be a confidence level of 95
46:52 - percent
46:53 - this gets written different ways in
46:55 - different
46:56 - online calculators that you might use
46:59 - and then you need to know statistical
47:01 - power
47:01 - and this is really like minimum
47:04 - detectable effect which is
47:06 - the minimum rate difference that you
47:10 - would care to measure
47:12 - okay should we bring dan on sophie i
47:14 - don't know if you see him with the
47:15 - screen share but dan is here
47:16 - oh yay hi actually sophie unless we
47:20 - want to show anything on screen maybe
47:22 - this is best if we cut the screen charge
47:23 - so we can
47:24 - see each other oh yeah now i can see all
47:27 - of you
47:28 - this is so much nicer than just talking
47:30 - into a screen
47:32 - welcome so um alex do you want to give a
47:35 - quick intro
47:36 - and then yeah dan i was wondering if you
47:39 - could introduce yourself
47:40 - um talk a little bit about what you do
47:43 - at codecademy and how
47:44 - yeah how you run a b test yeah sure so
47:47 - great to meet everybody
47:49 - so i'm a product manager here at code
47:50 - academy at bennett code academy
47:52 - about five years been a pm for three
47:54 - years really the job of a product
47:56 - manager and a consumer-focused company
47:58 - like codeacademy
47:59 - is to like pick key parts of our user
48:02 - experience and try to improve them
48:04 - i think like in an ideal sense uh
48:07 - you're getting a chance to do a ton of
48:09 - user research and pull data together and
48:10 - then come up with hypotheses about like
48:12 - what could prove
48:13 - user experience uh frankly hypotheses
48:16 - you're really really confident and you
48:17 - have a bunch of great evidence for you
48:18 - don't ship in a b test because you just
48:20 - want to get the right thing out there as
48:21 - fast as possible
48:22 - but abjs are great at deferring risk so
48:25 - if you're really not sure about which
48:27 - one of two options is better for the
48:29 - users in the business
48:30 - a b tests are a great way of measuring
48:33 - awesome that was a really great segue
48:36 - into
48:37 - i i've prepared a couple of questions so
48:39 - i'll try to watch
48:40 - them but uh it's a good segue into my
48:43 - first question which is
48:45 - i'm curious if you could talk about the
48:47 - kinds of questions that codecademy
48:49 - tries to answer with a b tests like what
48:52 - what are some examples
48:54 - yeah so we've done a lot of testing uh
48:56 - for consumer
48:57 - focused company like ours like the
48:59 - bottom of our funnel is very important
49:01 - so specifically
49:02 - our pricing pages our checkout pages how
49:04 - our pricing looks
49:05 - the pricing of the plans themselves is
49:08 - potentially like a really high leverage
49:10 - point
49:10 - in the experience but also like a
49:12 - potentially really risky thing to change
49:14 - so when we go do a bunch of product
49:17 - development work on things like our
49:18 - pricing page like how exactly
49:20 - should we be conveying what's in the
49:22 - free and the paid product
49:23 - uh is a great place to do testing i
49:25 - don't know which uh sophie which
49:27 - little experiment write-up you have you
49:29 - can probably pull up whichever one
49:31 - uh makes sense and i can we can talk
49:33 - about exactly how that went
49:35 - sure uh i think there's one on a pricing
49:38 - positioning let me share my screen again
49:43 - uh and uh folks in chat if you have
49:45 - questions for dan too uh feel free to
49:47 - yeah happy thanksgiving
49:48 - throw him and chat we can give it to him
49:51 - all right let's see
49:54 - i think give me one second
49:59 - yeah no worries so i think like really
50:02 - the job of the product manager is to
50:03 - like deploy
50:05 - the uh resources of the company in the
50:07 - way that is the best for the business
50:09 - and the user base
50:10 - uh so we're constantly trying to come up
50:11 - with ideas about what could be better an
50:13 - experience
50:14 - uh and then shift them and measure them
50:17 - so here's a good example of a test we
50:19 - ran on our pricing page
50:21 - so to test uh the hypothesis that like
50:25 - users might be more interested in our
50:27 - paid products if we conveyed the amount
50:29 - of content
50:31 - in the paid product versus just that
50:33 - there is a difference in content
50:35 - so if you scroll down i think
50:38 - uh the screenshot's probably conveyed
50:40 - the clearest so
50:42 - really important uh in a b testing is
50:44 - clear hypothesis
50:45 - clear kpis that you want to measure uh
50:48 - and then
50:48 - the control the variant was just like
50:51 - that there is a difference between the
50:52 - free and the paid product and the
50:55 - variant
50:56 - yeah zoom in a little just displays like
51:00 - the
51:00 - number of hours of learning content i
51:04 - think versus the number of courses
51:05 - of learning content yeah so in the
51:08 - control there
51:09 - that first line is hours and then sophie
51:11 - if you scroll down to the other one
51:13 - uh yeah number of courses yeah and if i
51:16 - recall correctly
51:18 - i think the course count lost which kind
51:20 - of makes sense is like hours of content
51:23 - is kind of like an opaque
51:24 - confusing concept uh to users we're
51:27 - really just trying to figure out like
51:28 - what is the best way of showing the
51:30 - difference between the products
51:32 - uh my recollection is number of courses
51:34 - make more sense which is a little bit
51:35 - more intuitive
51:36 - so this is a great example of like
51:38 - something we legitimately didn't know
51:39 - the answer to
51:40 - and sits in like an important part of
51:42 - the application itself
51:45 - so that like therefore we want to test
51:46 - it and uh
51:48 - tests like this also go on to inform
51:50 - other teams
51:52 - like the marketing team and how they
51:53 - display the the paid content to other
51:55 - people
51:56 - because what's really important in
51:57 - testing is not necessarily like the
51:59 - exact designs that went into a
52:01 - controller of variants
52:02 - it's the underlying thing that a user
52:04 - understands at a certain point in the
52:05 - journey
52:06 - so this insight is like likely
52:09 - applicable to other teams of code
52:10 - academy you can help make the experience
52:11 - better for everybody
52:13 - dan i'm curious about when you are
52:15 - designing these experiments
52:17 - are you doing the kinds of things that
52:18 - sophie was showing off of like
52:21 - like using these kind of calculators to
52:22 - determine what's the sample size how
52:24 - how long are we going to run it for like
52:26 - can you talk a little bit about actually
52:27 - implementing
52:28 - one of these yeah sure what that looks
52:30 - like so i think we we kind of start at
52:32 - the product development phase like i was
52:34 - describing earlier
52:35 - we'll pick apart uh the experience and
52:37 - really do as much research as we can to
52:39 - figure out what could be improved
52:41 - uh from them we come up with kind of
52:42 - like a tiered list of like what are we
52:44 - most confident in what are we least
52:45 - confident the super confident things
52:47 - just get chipped
52:48 - uh the things that kind of in the middle
52:50 - that are a little more risky tend to get
52:51 - tested
52:52 - and before we do any tests we want to
52:55 - make sure that the test
52:56 - the page we're running on has enough
52:58 - traffic that we can like get to a
53:00 - statistically significant conclusion in
53:02 - a reasonable amount of time
53:04 - so like when we've honestly screwed this
53:05 - up in the past is like we put too many
53:07 - variants on a page
53:08 - we do the math wrong and we realize
53:10 - it'll take four months to get an answer
53:11 - here which
53:12 - is way too long at a startup like really
53:14 - we want to be coming to conclusions
53:16 - uh in typically under a month unless the
53:19 - test is really really high leverage
53:21 - so like if we realize that we don't have
53:23 - the bandwidth to
53:24 - like ship four variants maybe we cut the
53:26 - variance size down to two or three
53:28 - which then affects like how our design
53:30 - and engineering processes work
53:32 - which means like hopefully we get to an
53:33 - answer faster and we have the designer
53:35 - spin up less variance
53:37 - and maybe we'd ship tests in multiple
53:38 - rounds instead of all at once
53:41 - i'm curious if um you all
53:44 - think a lot about minimum detectable
53:47 - effect before you start
53:49 - an a b test so like do you think about
53:51 - how big of a difference do you care
53:53 - about
53:55 - before you try to plan out the sample
53:57 - size or do you try to
53:58 - estimate how big of a difference you
54:00 - think there will be
54:02 - it's a little bit of like chicken or the
54:04 - egg
54:05 - like if um if we know we don't have a
54:07 - lot of traffic on a page and we're doing
54:09 - something
54:11 - that's riskier you need to make a bigger
54:13 - change
54:14 - to meet the minimum detectable effect
54:17 - threshold
54:18 - so like if uh i want to say in that test
54:21 - you just showed
54:22 - kind of makes sense that a tweak within
54:24 - a pricing grid would only be like a four
54:26 - to five percent change
54:27 - whereas like if we completely overhaul
54:29 - the design of the page that may be a 15
54:31 - change uh which means it's a riskier
54:34 - test to run because there's more
54:35 - engineering and design hours that go
54:37 - into the variant but we know like if we
54:40 - want to
54:41 - be statistically confident and change
54:43 - where we don't have a big population
54:44 - size
54:45 - like the change has to be order
54:46 - magnitude really large to like observe
54:49 - that effect
54:50 - in that time threshold yeah
54:53 - that's interesting and do you do you
54:54 - consider
54:57 - like the i guess you could touch on it a
54:59 - little bit there but the
55:00 - engineering resources or the design
55:02 - resources of like okay we think that
55:04 - this is going to be
55:06 - a large change and relatively speaking
55:08 - there's not a ton of engineering work
55:09 - that needs to be done here or
55:11 - this change isn't going to be huge but
55:13 - you know uh we can
55:14 - we can ship it within three days if we
55:16 - get some engineers working on it or
55:17 - sometimes
55:18 - yeah definitely i mean as a product
55:21 - manager you're always playing the
55:22 - balancing game
55:22 - of like in an ideal world you're
55:24 - shipping the
55:26 - like lowest effort highest impact the
55:29 - lowest risk work
55:30 - at all times right so like a great
55:33 - example of a change that we didn't a b
55:34 - test
55:35 - uh is on our pricing pages and on our
55:37 - checkout pages we use the dollar sign
55:39 - to denote our prices which makes sense
55:42 - if you're in the us but is confusing to
55:43 - users in australia
55:45 - uh and new zealand and canada and fiji
55:47 - which also use the dollar sign
55:48 - so like that we're just confident in
55:50 - that like denoting the currency with an
55:52 - abbreviation is clearer
55:54 - so we don't test that okay like i
55:57 - ideally
55:58 - in a perfect world like we're finding
56:00 - little stuff like that constantly
56:02 - and just shipping it out really really
56:03 - fast but you inevitably get to areas of
56:06 - development that you
56:07 - just don't know the right path forward
56:08 - and that's where testing is great
56:10 - i'm curious if you can speak at all
56:12 - about once you
56:14 - have run a test how you then
56:17 - decide whether or not you're going to
56:20 - ship something so
56:22 - let's say you run a test and um you
56:24 - detect a difference between the two
56:26 - groups
56:26 - but it's really really small um do you
56:30 - still
56:31 - ship it as long as it's like
56:33 - incrementally better or
56:35 - how do you make what are some of the
56:36 - considerations that you
56:38 - yeah i mean i think
56:41 - in um in tests that are themselves lower
56:46 - risk like in areas of the app that don't
56:48 - get a lot of traffic or like away from
56:50 - the core
56:50 - flows uh to me the biggest risk is you
56:53 - had a winner and you didn't ship it
56:55 - because those incremental wins do
56:56 - add up to bigger wins so like if the
56:59 - overall risk is not
57:01 - that great we probably just default to
57:03 - shipping it
57:04 - and then see if we can catch like a
57:06 - longitudinal difference
57:07 - in just observing the traffic on the
57:09 - page over a long period of time
57:11 - it's like in a really well-rounded
57:12 - testing program like you have the
57:14 - analytics set up to like observe the
57:16 - test groups outside of the initial
57:18 - experimentation windows
57:20 - um because we've run enough tests here
57:22 - that we've definitely seen cases where
57:23 - like we see a variant when win
57:25 - during an experiment but like outside of
57:28 - the initial observation window it flat
57:30 - lines
57:30 - and goes back to being the same because
57:32 - maybe you're running on tests of like
57:34 - existing users and you just tweak
57:35 - something and it looks new
57:37 - more people click on it and those things
57:39 - can be like actually very hard to
57:41 - control for
57:42 - like practically uh like in a startup
57:45 - but there's definitely things that are
57:47 - like higher risk
57:49 - that we'll want to have like really
57:50 - extend the observation window and be
57:51 - confident in our results
57:53 - like if we're changing like pricing or
57:55 - our trial model or like anything that
57:57 - you know we use to pay all the bills
57:58 - around here
58:00 - did you super interesting because i it
58:02 - reminds me we had the conversation
58:05 - before you joined about how this is kind
58:07 - of it's a random
58:09 - process and so even if there's
58:12 - um a difference that you observe in a
58:14 - particular group it doesn't necessarily
58:16 - mean that that difference
58:18 - is true for the entire population
58:20 - because you only observed a very
58:22 - small proportion of people but i didn't
58:24 - think about the
58:25 - other part of that which is that for the
58:28 - people that are just seeing a new
58:30 - uh feature there's like some sort of
58:32 - maybe newness effect
58:34 - that is also playing a role and so it's
58:36 - not fully
58:37 - yeah definitely and i think like in
58:38 - ideal scenarios like you're
58:41 - just shipping things to new users so
58:44 - when we do things in the new
58:45 - new first time user experience or the
58:47 - home page like those are easier to
58:48 - control for because a lot of people
58:49 - going through
58:50 - those i've never seen any of these
58:51 - things before but like if we wanted to
58:53 - improve like the fifth module of a
58:55 - course
58:56 - um like the population size is so much
58:58 - smaller that deep into the experience
59:00 - that realistically it's you'll never be
59:02 - able to control in our size anyway
59:04 - just for new users i'm curious is
59:08 - a b testing a pretty common thing that
59:10 - product managers do
59:12 - i know um or when i think about a b
59:15 - testing at code academy i like think
59:16 - about your team and you know
59:18 - sophie and i were talking about how we
59:19 - wish we did this more in curriculum or
59:21 - it doesn't really happen
59:22 - like we wish we could a b test the
59:24 - curriculum itself that we write
59:26 - so like how what kind of jobs use
59:29 - a b testing is it a lot of product
59:31 - managers and if not like how did you
59:33 - kind of find your way into
59:35 - the space yeah sure um i think it is
59:38 - heavily used in product management and
59:40 - specifically
59:41 - in product management in consumer facing
59:43 - companies so
59:44 - if code academy was a primarily like b2b
59:48 - business like uh a b testing can be
59:51 - helpful but realistically most of your
59:52 - customers are on long-term contracts
59:54 - and it's tough to really measure like a
59:56 - trial or a conversion event
59:59 - and this like signals in the kpi is a
60:01 - little blurrier they definitely still do
60:02 - it
60:03 - uh but really the consumer companies
60:04 - also have the traffic size where you
60:06 - have enough of a population
60:08 - to run a high volume of a b tests like
60:11 - booking.com
60:12 - uh and facebook and uber and google are
60:14 - really like the companies that like have
60:16 - massive size and also like a ton of
60:18 - infrastructure built up
60:19 - um infrastructure like is an interesting
60:22 - thing to mention there because like
60:23 - tests are not cheap to run so you should
60:26 - be running them like as often as you
60:28 - need to to defer risk but no more
60:30 - it's very tempting as like a product
60:32 - manager you don't want to ship
60:32 - everything as an a b test because you
60:34 - don't know
60:34 - like definitively if you're right about
60:36 - something
60:38 - whereas like doing like longitudinal
60:39 - studies are less satisfying because data
60:41 - has
60:42 - variance in it anyway so it's tough to
60:44 - say like if the thing you did is really
60:45 - like
60:46 - moving a business forward but as you
60:48 - mentioned in curriculum like it's
60:50 - it's tough to really control for these
60:52 - things and you need a lot of like
60:54 - infrastructure and data science
60:55 - resources
60:57 - to like make a change shift
61:01 - it's much harder um so there's
61:03 - definitely like a cost
61:04 - to them but they can be super powerful
61:08 - yeah it's super interesting wow thank
61:11 - you for joining us
61:12 - this was really good it's awesome
61:15 - i think like we were saying before it's
61:18 - been
61:18 - a lot of theory in these uh in these
61:21 - live streams but it's good to remember
61:23 - that this is a like a real tool that
61:26 - people are using
61:27 - in a job setting and um using it to
61:30 - answer questions and
61:31 - build a business and uh make a product
61:34 - better so
61:35 - it's really helpful yeah definitely i
61:37 - mean the theory is super important like
61:39 - without understanding like the theory
61:40 - it's tough to like
61:41 - design and sequence these tests but like
61:44 - testing is definitely like an applied
61:46 - science yeah we're like there's a theory
61:48 - of how you're doing it but typically
61:50 - you're in
61:50 - environments that you can't perfectly
61:52 - control so you like adhere as close to
61:54 - the theory as possible
61:55 - uh but frequently there's things in your
61:57 - way yeah
61:59 - for sure yeah it almost reminds me of
62:01 - physics classes where you're like
62:03 - all in all physics classes you're like
62:05 - like wind resistance doesn't exist and
62:07 - you're like learning about
62:08 - theory and then you come to apply it and
62:10 - you're like wait a minute there's a
62:11 - million different factors here though if
62:12 - you could perfectly isolate these things
62:14 - it would be awesome but yeah like in
62:15 - reality you can there's tracking
62:17 - problems
62:18 - right there's other teams working in
62:19 - areas that you can't work in
62:21 - uh so you the theory is key but it's
62:23 - really just like applying it
62:24 - in the best way yeah and also knowing
62:27 - how the theory
62:29 - relates to the the actual practical
62:32 - applications like
62:33 - understanding that your sample is not
62:37 - actually a random or a
62:40 - perfect representation of the population
62:42 - you care about which in this case is
62:43 - exactly
62:44 - all users that might ever access this
62:46 - and you there's no way to ever get a
62:48 - random sample from that population
62:50 - so yeah the the application is very hard
62:52 - like even if you ship tests to like a 95
62:55 - uh statistical significance level it
62:56 - means one of every test
62:58 - 20 of your tests could be wrong right
63:00 - and if you ship enough tests a year you
63:01 - definitely have a bunch of those that
63:02 - you're measuring wrong just practically
63:05 - yeah yeah and it's i think it's also a
63:08 - huge issue
63:08 - at those bigger companies that do have
63:11 - infrastructure
63:12 - um if they're running like thousands of
63:14 - tests
63:16 - at a time and it's hard to
63:19 - make decisions based off of that because
63:20 - you know that you could be making errors
63:23 - exactly like tests overlap they conflict
63:25 - you get
63:26 - the observations you're seeing don't
63:28 - hold with time like at the
63:30 - uh he's messing with reality but it's
63:32 - still super powerful
63:35 - yeah all right i think
63:38 - we are just about at time but i'm just
63:41 - checking the chat to see if we got any
63:43 - questions
63:44 - i don't see any yeah let me um
63:47 - so let me do a plug so well first of all
63:49 - this is the end of this series
63:50 - so congratulations sophie you did eight
63:52 - weeks in a row of this
63:54 - um so thank you for running really
63:56 - amazing sessions
63:58 - um i i learned a lot and i think
64:01 - you did a great job so thank you sophie
64:04 - and thank you for helping with all of
64:06 - them i just like sit here in the
64:08 - background for most of them and
64:09 - occasionally say something so um that's
64:11 - not accurate at all
64:14 - um next week we are starting another
64:16 - series
64:17 - again it's going to be on um creative
64:20 - coding
64:21 - um some really cool projects that jiwan
64:22 - is going to go through i'm putting a
64:24 - link
64:24 - in the chat right now to our events page
64:28 - um uh you can you can find all the
64:31 - events there which
64:32 - will actually send you to the to the
64:33 - youtube um pages even though they're not
64:35 - publicly yet
64:36 - um but keep an eye out on our youtube
64:38 - page because uh
64:40 - shortly i will publish all of these
64:41 - events um so you can see
64:43 - when we're running them uh current plan
64:46 - is to keep doing them
64:47 - on thursdays at 4pm uh but we might
64:50 - actually
64:51 - uh we might actually change that as
64:53 - we're going so keep an eye out on that
64:55 - events page subscribe to the youtube
64:57 - channel all that good stuff
64:59 - to know when we're doing stuff so
65:02 - all right and with that