00:00 - faster than we think because um
00:03 - i know i i agree because the the
00:05 - recordings always have this awkward 15
00:06 - minute sec
00:07 - or 15 second seconds but worth checking
00:11 - i'm gonna put myself on mute for a
00:13 - second so i can get the
00:14 - chat popped out all right
00:19 - you're still on mute sophie
00:22 - i gotta i gotta get the um the youtube
00:25 - chat
00:25 - so i'm gonna mute myself for a second
00:27 - you welcome hello everyone uh
00:30 - welcome to another edition of codecademy
00:32 - live uh
00:33 - i'm back with sophie who's gonna be
00:35 - teaching a
00:36 - uh stats course today um we are
00:39 - live right now on a couple of different
00:40 - platforms sophie and i are gonna
00:42 - both be looking primarily at the youtube
00:45 - chat so if you want to
00:46 - uh chat with us you can come over to our
00:48 - youtube channel which is where we'll be
00:49 - hanging out if you're looking at the
00:51 - this in the recorded suck
00:52 - uh recorded um pre-recorded on youtube
00:56 - um we are live streaming every tuesday
00:58 - at 4 p.m eastern and so
01:00 - yeah we would love for you to come along
01:02 - you can ask questions uh we're trying to
01:03 - make this
01:04 - as interactive as possible um i see a
01:07 - lot of people in the chat already saying
01:09 - we got someone from boston uh hello from
01:12 - greece again
01:13 - he's like he's all over the place he's
01:15 - uh he's coming to every one of these so
01:16 - we appreciate him
01:17 - um yeah so welcome everyone
01:21 - yes welcome excited for this
01:27 - can you hear me yeah you're very low
01:30 - can you say something else can you hear
01:31 - me now yeah you're very quiet
01:34 - huh let me take
01:38 - my headphones out yeah now
01:41 - that seems good weird
01:45 - i i feel like my computer has like
01:48 - a ghost inside of it or something that
01:51 - just
01:51 - comes out during live streams where i
01:53 - have all the technical issues
01:55 - like live streams when it's least uh
01:58 - least convenient well that's the time
02:00 - that my internet dropped in the middle
02:01 - of a live stream so we're doing better
02:02 - than that
02:03 - yes we are all right well
02:06 - um so i think we can pretty much get
02:10 - started
02:10 - uh i think alex already mentioned this
02:12 - but we are streaming on a couple
02:14 - different platforms
02:15 - we'll keep an eye on the chat on the
02:17 - youtube chat so if you have questions
02:19 - drop them in there um alex will
02:22 - interrupt me if necessary
02:24 - and otherwise i'll also keep one eye on
02:26 - it as well
02:28 - um for context a little bit of context
02:31 - today
02:31 - we are going to start with a new data
02:35 - set um
02:36 - and we're gonna start looking at some
02:38 - summary statistics so
02:40 - last week we went through a little bit
02:42 - of a data cleaning process
02:44 - um this week we're gonna kind of shift
02:45 - gears and we're going to
02:47 - uh and we're gonna go focus on our first
02:50 - piece of our analysis which is
02:51 - kind of looking at one variable in our
02:53 - data set at a time and trying to make
02:55 - some um visualizations and draw some
02:58 - conclusions
02:59 - uh about our data which is very exciting
03:02 - um if one last thing is the
03:05 - sorry it's sorry to cut you off sophie
03:07 - uh one last thing is that if you want to
03:09 - code along i just posted a link to a
03:10 - github
03:11 - account in the chat it's also in the
03:13 - youtube description so that has
03:15 - sophie's starter code and solution code
03:18 - perfect yeah and the the solution code
03:21 - is going to be a little different i
03:22 - think
03:22 - than what's posted right now but i'll
03:24 - make sure to update the solution code
03:26 - after today so that it really represents
03:29 - what we
03:30 - what we covered last time um
03:33 - cool uh so
03:37 - let me share my screen um
03:40 - and i'm gonna share
03:44 - oops
03:48 - i'm gonna share just the uh
03:52 - just the um jupiter notebook this time
03:57 - alex can you see that yep that looks
04:00 - good to me
04:02 - awesome so if you did not attend last
04:05 - time
04:05 - um that our first in our first live
04:08 - stream we went through the process of
04:10 - actually uh opening up a jupiter
04:12 - notebook and
04:14 - um running our first little bit of code
04:17 - i'm gonna skip that
04:18 - this time uh but if you
04:21 - want you can always go back and watch
04:23 - the first live stream to kind of get
04:25 - caught up cool cool
04:28 - um awesome so the so what we're going to
04:32 - do is we're going to look at a slightly
04:33 - different data set this time
04:35 - um i'm going to just run this first
04:37 - block so that we get all of our
04:39 - libraries imported
04:40 - this data set uh i i picked this
04:44 - just because i think it's really fun um
04:46 - so this is
04:47 - it's a data set that we got
04:50 - as part of like a partnership with
04:52 - street easy
04:54 - um which is this it's a website that you
04:56 - can go to to look at apartments
04:58 - um website and app that you can go to to
05:00 - look at apartments in new york city
05:02 - uh and in fact i found my last apartment
05:06 - on stream
05:09 - fun uh and and it's kind of fun so these
05:13 - are all new york city apartments
05:14 - uh we can get a quick sense of the data
05:17 - by looking at the first five rows so
05:19 - there's some things in here that we
05:20 - probably don't care about like
05:22 - a rental id and a building id we're not
05:25 - gonna really make sense of those things
05:27 - um but uh we've got some other things
05:31 - that are interesting we've got
05:32 - rent uh this is in u.s dollars
05:35 - we've got a number of bedrooms bathrooms
05:39 - size and square feet uh
05:43 - minutes the subway the floor that it's
05:45 - on the building age and years
05:48 - a bunch of amenities so these are all
05:50 - like things that the apartment might
05:52 - have
05:52 - where one means that they have that
05:54 - thing and zero means that they don't
05:56 - and then some information about where
05:58 - the apartment is located
06:01 - so we got lots of interesting variables
06:02 - in here um last time we were talking
06:05 - about
06:06 - uh a little where we talked a little bit
06:09 - about
06:10 - how um when we
06:13 - looked through here
06:17 - this is this is all stuff that we did
06:18 - last time so we
06:20 - last time did some replacing and
06:23 - changing a variable
06:24 - so i'm just giving you guys the code for
06:26 - that right now
06:28 - but um remember last time we looked at
06:30 - this
06:31 - output of rentals dot
06:36 - describe and then we did include
06:40 - equals all that just includes all of the
06:42 - variables so we say we want to
06:44 - summarize everything and remember we got
06:47 - this output where
06:49 - we saw that different types of variables
06:51 - or different
06:52 - uh columns were being summarized in
06:54 - different ways so we saw
06:56 - that rental id and building id
06:59 - even though it's nonsensical we're
07:01 - getting like means and standard
07:03 - deviations
07:04 - same with rent bedrooms um
07:07 - and now over here has roof deck
07:11 - is getting we're getting this top half
07:14 - of the
07:15 - the describe the uh the number of unique
07:18 - values the top
07:19 - most frequently that frequent value and
07:22 - the num the frequency of that
07:24 - we're getting these different summaries
07:27 - um
07:28 - alex can you maybe remind everyone
07:31 - how we or which variables uh
07:35 - had each different type of summary and
07:38 - how we kind of figured that out like
07:40 - why as a recap from last time are we
07:42 - getting these different summaries
07:44 - right so last time was pretty much all
07:47 - about data cleaning where
07:49 - looking at the name of our variable we
07:50 - realized like okay
07:52 - this this variable is actually not a
07:53 - continuous number it's a category of one
07:56 - two three four five
07:57 - and you know there's there are lots of
07:59 - like subcategories within that where it
08:00 - could be ordered it could be unordered
08:01 - there's like lots of different ways that
08:04 - things could be a category um but that's
08:06 - this that's this top half right of those
08:08 - are categories so that's something like
08:10 - um i think the the neighborhood is a
08:12 - good example um
08:14 - right of there's like no way to take the
08:15 - mean of what neighborhood you are you're
08:17 - in uh right there's no way to take the
08:18 - mean of the upper west side and lower
08:20 - east side and all these different
08:21 - neighborhoods
08:22 - um so that's a category we can report um
08:25 - the most frequent one we can report the
08:27 - what are the other ones the
08:28 - total number of responses i think that
08:29 - 93 for neighborhood is
08:31 - unique yeah i think so for unique
08:34 - is the number of unique values and then
08:37 - it's the
08:38 - top most frequent one and then the
08:40 - frequency of that top one
08:42 - right so there are 579 apartments from
08:45 - the upper west side and there are 93
08:47 - total
08:48 - um neighborhoods represented in this
08:51 - data yeah
08:52 - um awesome so right we saw that we got
08:55 - some different summary statistics
08:56 - and today our goal is to really dig into
08:59 - what each of these summary statistics
09:01 - really means
09:02 - and um and try to put them and put the
09:04 - summary statistics in context because
09:07 - right now i'm staring at this table
09:09 - i see a lot of numbers um it's really
09:11 - hard for me to parse and really
09:13 - understand
09:14 - what's going on um the
09:17 - this amount of numbers is better than
09:19 - looking at the entire data set
09:21 - um but i think in order to really
09:23 - understand this we've got to break it
09:25 - down even further yeah
09:26 - and something to even call out here
09:27 - right away is uh
09:30 - that scientific notation there on
09:31 - building id which again building id
09:33 - taking the mean of a bill tonight
09:34 - building id doesn't really make a ton of
09:36 - sense and we probably won't even use
09:37 - that
09:37 - but this might be something that you see
09:39 - elsewhere in data that you're working
09:40 - with and something to keep track of
09:42 - what does this what does this mean
09:43 - sophia 5.12
09:45 - e plus yeah that's a great point um
09:50 - this is actually something that you're
09:51 - if you are taking content on codecademy
09:54 - it's something that
09:55 - um we you know we try to avoid to avoid
10:00 - confusion but then i think that when
10:02 - people get working with their own data
10:04 - sets
10:05 - it's something that throws a lot of
10:06 - people off um so this is scientific
10:09 - notation
10:10 - this e plus 0 7 means multiplied by
10:14 - 10 to the seventh
10:17 - essentially what that means in terms of
10:19 - this number is that
10:20 - it's not actually five point one two two
10:23 - it's five point one two two
10:25 - times ten to the seventh that's like an
10:27 - equal million or something right yeah
10:29 - and multiplying by
10:30 - ten to the 7th is equivalent to moving
10:33 - this decimal place over
10:34 - seven places so um one two
10:37 - three four five six and then another one
10:41 - seven so that would be like another zero
10:43 - then here so this number is like five
10:45 - one two two zero zero seven
10:48 - zero which again makes sense for the
10:50 - building id it's probably like an
10:51 - eight digit id or something so again
10:53 - that number is totally meaningless but i
10:55 - think it's something to notice that like
10:57 - if you're working with your own data you
10:59 - might you might miss that right you
11:00 - might see like okay the mean is like 5.3
11:02 - great uh but you don't realize that it's
11:04 - 5.3 times e to the
11:06 - negative four which means that you tack
11:08 - four zeros on to the start of it
11:09 - or times or uh e to the oh seven which
11:13 - means there's a lot of zeros at the end
11:14 - so something to
11:15 - something to be aware of is to like keep
11:17 - an eye out for that i'm sure there's
11:18 - ways to
11:20 - customize pandas uh to either not
11:23 - include that or include that and that
11:24 - kind of stuff but
11:25 - there there definitely are ways to do
11:27 - that one of the things that i found
11:29 - um that i discovered is that
11:33 - so there's different settings for
11:35 - different packages so you can set
11:37 - the you can basically set it to not use
11:40 - scientific notation within numpy
11:42 - but then if you use a pandas function or
11:45 - you use like
11:46 - a function from scipy it's going to
11:48 - ignore that so
11:51 - so if you're trying to work with those
11:53 - settings it can get a little frustrating
11:55 - sometimes
11:56 - all of these packages are like calling
11:57 - each other and it's not clear which uh
11:59 - what settings takes precedent yeah um
12:02 - okay so i think we should start with
12:05 - quant
12:05 - a quantitative variable so um again just
12:08 - for a little bit of context today we're
12:10 - going to start
12:11 - we're going to focus just on one
12:13 - variable at a time and by variable
12:15 - i just mean one column of our data set
12:18 - um there are lots of
12:21 - other kinds of statistical analyses that
12:24 - have to do with
12:25 - looking at the relationships between
12:26 - variables so for example like
12:29 - we probably expect that bigger
12:32 - apartments tend to cost more money
12:34 - and we might be curious about those
12:36 - types of relationships between
12:38 - multiple variables but at this moment
12:40 - we're gonna start
12:41 - by looking at one variable at a time
12:44 - um i'm gonna imagine i think that i am
12:48 - because this is kind of true i'm gonna
12:50 - imagine that i am looking
12:52 - for an apartment in new york city and
12:54 - i'm trying to understand what typical
12:56 - apartments
12:57 - look like or what typical apartments go
13:00 - for
13:00 - in terms of rent um and so i'm using
13:04 - this data set to try to
13:05 - understand what a typical apartment in
13:08 - new york city looks like
13:09 - um it's kind of true because eventually
13:13 - i'm going to move back to new york city
13:14 - so this is great yeah i was going to say
13:16 - i see in chat we have a lot of people
13:18 - from cambridge watching and i know that
13:19 - there's a really great uh open data set
13:21 - about
13:22 - um boston housing as well um i think the
13:24 - cambridge person is my mom
13:26 - there we go so uh if you're looking to
13:29 - if you're looking to buy a house
13:30 - sophie's mom
13:31 - uh you just did yeah there you go
13:34 - it's really funny yeah definitely wave
13:37 - rider
13:38 - from cambridge that's my mom for sure um
13:42 - we're we really like waves in this
13:44 - family it's a whole
13:45 - it's a whole thing um okay so
13:49 - quantitative variables
13:50 - uh so i'm gonna pick rent again trying
13:53 - to understand
13:54 - what's the typical rent monthly cost
13:57 - essentially for
13:58 - an apartment in new york city so
14:01 - like we saw in this describe
14:04 - uh output before um for rent we're
14:07 - getting a bunch of summary statistics we
14:09 - have the mean
14:11 - standard deviation minimum we've got
14:13 - some percentiles here and a maximum
14:17 - we're going to kind of gloss over i
14:20 - think these
14:21 - for now but let's start with the mean
14:24 - i'm not going to go into a ton of detail
14:26 - about how to calculate the mean or what
14:29 - the mean
14:30 - is but mean same thing as average
14:33 - it's basically a way of calculating
14:37 - some sort of typical rent um
14:40 - so our goal here is like what's a
14:42 - typical value
14:44 - in this data set um it's calculated by
14:47 - adding up
14:48 - all the values and dividing by the
14:50 - number of values
14:51 - um so there's a couple different ways in
14:55 - python that you can calculate the mean
14:57 - um so one of them is to use the numpy
15:00 - package
15:02 - which we've saved as mp and we can just
15:04 - say
15:05 - rentals.rent and calculate the mean that
15:09 - way
15:10 - we also can do um
15:14 - we can use this dot mean
15:17 - method on this column
15:20 - um and that will give us the same number
15:23 - so
15:24 - again i think within the path on
15:26 - codecademy we teach
15:27 - one or the other um but it's it's true
15:30 - for all of these things
15:31 - there's lots of different ways to do
15:33 - them
15:35 - it's also a good example of something
15:36 - that is easy enough to calculate by hand
15:38 - that if you want to
15:39 - you know practice your coding skills
15:41 - it's fairly easy to
15:43 - uh make a loop or you know it's not
15:45 - fairly easy but it's good practice of
15:46 - making a loop adding up all the numbers
15:48 - dividing by the total
15:49 - it's a statistic that the formula isn't
15:51 - complicated enough that if you're more
15:52 - interested in coding than you are
15:54 - in statistics then that's a good one to
15:56 - be able to like practice your coding
15:57 - skills
15:58 - yeah totally um i think one of the first
16:01 - things that i did
16:03 - when i started learning statistical
16:04 - computing or the one of the first
16:06 - assignments that i ever had in school
16:08 - was basically rewriting all these
16:09 - functions myself
16:11 - and when we get into the uh
16:15 - the like hypothesis testing
16:19 - segment we're gonna actually do a little
16:21 - bit of that we're gonna write our own
16:23 - functions to do some hypothesis testing
16:26 - which i think is a really
16:27 - important thing to do in order to
16:29 - understand
16:31 - how hypothesis tests work so
16:34 - okay so we've got the mean there
16:38 - are a couple different summary
16:39 - statistics i can use to try to
16:40 - understand
16:41 - what a typical price is though so um the
16:44 - mean is not the only one
16:46 - uh if you've taken like a a math class
16:50 - or a basic statistics class you probably
16:52 - have heard of some of these other ones
16:54 - um the median is another really great
16:57 - way of trying to estimate
16:59 - kind of a typical value of a
17:00 - quantitative variable
17:02 - um instead of
17:09 - i'm going to print this out
17:16 - to get the median instead of adding up
17:19 - all the numbers
17:20 - and dividing by the number of numbers
17:23 - you're essentially
17:25 - reordering all the rents so that they're
17:28 - in ascending order
17:32 - and then looking for the middle value so
17:35 - again um i think i can print out like
17:38 - if i print out rentals dot rent
17:42 - um that i think i'll do actually
17:46 - 0 to 10.
17:49 - right so this column is just a bunch of
17:51 - numbers
17:52 - if i was to reorder them and basically
17:55 - just like cross off
17:57 - the lowest one the highest one the next
17:59 - lowest one the next highest one the next
18:01 - lowest one the next highest one
18:03 - um i would find the median and so
18:06 - one thing i'm noticing right off the bat
18:08 - as i'm doing this is that
18:09 - the median rental price is a lot lower
18:13 - than
18:14 - the mean rental price um
18:17 - so almost by like a thousand dollars
18:20 - a month which for those who are not used
18:23 - to us dollars
18:24 - it's kind of a lot um and so
18:28 - that's something that we might want to
18:30 - investigate further
18:31 - why that is it usually means that the
18:34 - data is something
18:35 - that we call skewed um which we'll we'll
18:38 - look at
18:39 - a little bit later and then you even
18:41 - almost start picturing it in your head
18:43 - of like
18:43 - if i'm crossing off both from each side
18:46 - i'm gonna run into the median before i
18:47 - run into the mean
18:49 - and so there's something about like
18:51 - there was more data down at that end
18:53 - uh you know below the mean then uh
18:56 - than there was above it ah
19:00 - so yeah it has to do with
19:04 - how it has to do with outliers actually
19:06 - even more so because
19:08 - or large like super large values
19:13 - because if you think about how you're
19:15 - calculating the mean
19:16 - um you're you're adding up all the
19:19 - numbers and dividing by the number of
19:21 - numbers
19:22 - and if you've got like a couple of
19:23 - apartments that are going for
19:25 - 500 000 or i mean that's crazy but
19:29 - i don't know what the we should see what
19:30 - the maximum value is i think it prints
19:33 - it
19:33 - prints it up here um
19:37 - right so the maximum rent is 20 000
19:40 - a month so that that rent
19:44 - that really high rental value is pulling
19:47 - this mean
19:48 - up whereas if you're calculating the
19:50 - median
19:51 - let's say you just had three rental
19:54 - prices and the rental prices were
19:56 - like 100 200 and 300
20:02 - then the median would be 200 and the
20:04 - mean would be 200.
20:06 - but if you had the rental prices 100 200
20:10 - 20 000. the mean is going to get pulled
20:14 - way way way high um but the median is
20:18 - still going to be
20:18 - the middle value which is still 200. so
20:22 - um so it has to do with basically those
20:25 - outliers
20:26 - i saw somebody uh mention on here the
20:28 - trimmed mean
20:29 - so that just essentially means getting
20:32 - rid of some outliers before you
20:34 - calculate the mean
20:35 - and that can definitely help uh that can
20:38 - definitely help
20:39 - prevent the mean from being biased one
20:42 - way
20:43 - by really big outliers see it looks like
20:46 - trim underscore mean it might be the
20:48 - function if you uh
20:49 - if you want to try it rather than mean
20:51 - we can see if it's different
20:54 - um here we'll try
20:58 - rentals trim dot
21:02 - run dot trim yeah this one
21:05 - might not put a sci-fi function we'll
21:07 - see what happens do you put like .05 or
21:10 - something like that
21:10 - to tell it how much to what's the um
21:14 - yeah yeah it's in the example that i'm
21:16 - seeing like 0.01
21:17 - but this is also a different it's a
21:20 - sci-fi function so it might not work
21:22 - perfectly with our um
21:24 - okay so i think there
21:28 - somewhere i can i'll i'll look into it
21:31 - as we
21:31 - as we keep going and get an example cool
21:36 - um okay so we've got two
21:39 - measures of typical rents um i'm gonna
21:42 - give us
21:43 - one more so if you've taken again a
21:46 - stats class
21:47 - um i see somebody actually
21:51 - sent us this code awesome um
21:56 - sorry if you've taken a stats class you
21:58 - probably have seen mean median and mode
22:01 - as kind of the three main measurements
22:03 - of like a typical
22:05 - uh value of a quantitative variable one
22:08 - thing i'll point out is that for
22:10 - something like rent so mode is just the
22:13 - most common value
22:15 - um and for something like rent where you
22:17 - get you have a lot of different
22:19 - values in this data set it might be true
22:22 - that
22:23 - none of the no two apartments have
22:26 - exactly the same rent
22:28 - just because of random like these are
22:31 - just random
22:33 - uh numbers that where there's a lot of
22:36 - different options like
22:37 - something could be two five five zero or
22:40 - two five five
22:41 - two or whatever will probably be unique
22:43 - there's gonna be not a ton of overlap
22:45 - right
22:45 - most of them will be unique so it's
22:48 - possible that something could be the
22:49 - mode just because there's
22:52 - two apartments that happen to have the
22:54 - same rent um so
22:55 - i would be careful about using the mode
22:58 - for quantitative variables but
22:59 - um i think it's worth printing just so
23:02 - that we can see in this case it actually
23:03 - does make sense
23:05 - um in the mean i'm gonna do that in one
23:09 - sec i saw that
23:10 - iowanus posted this uh this code thank
23:13 - you as i always got our back
23:14 - you've got our back yeah so let's let's
23:17 - try it
23:18 - um so yeah it's uh
23:22 - it's from scipy uh
23:25 - scipy.stats.trimmedmean
23:27 - and then you give it data and then uh
23:29 - value
23:30 - yeah oh wait sorry say that again it's
23:33 - from scipy
23:34 - yeah let me uh i'll drop it in our chat
23:47 - yeah let's see i'm gonna import
23:53 - scipy dot stats
24:00 - and i then
24:08 - it looks like the one that i want to
24:10 - send was just
24:12 - mean is it really sci-fi
24:15 - we can uh well this is also a good
24:17 - opportunity to show um
24:19 - uh looking at documentation too there's
24:22 - the documentation in the
24:23 - in the chat if you want it
24:26 - let's try it yeah
24:34 - no so it's trim amine trim underscore
24:36 - meme
24:37 - yeah and you might need to uh
24:40 - yeah was that the let us know if that's
24:43 - the one you meant
24:44 - i want us or if you meant something else
24:54 - i don't know get ready get rid of trim
24:56 - equals just just do
24:58 - 0.1
25:01 - sorry what i get rid of trim equals just
25:03 - try 0.1
25:04 - that might be they might have changed
25:05 - the parameter name
25:09 - okay yeah so this is a lot smaller
25:13 - so that's a good indication that
25:16 - and it's a lot more similar to the
25:17 - median that's a good indication that
25:20 - there is some large outliers or there
25:22 - are some large
25:23 - outliers that are impacting this um i
25:26 - think this is a good time actually
25:28 - to look at documentation so i'm going to
25:29 - do that really fast
25:32 - let's take a look so again i i just
25:34 - googled
25:36 - pandas trimmed me into to get this and
25:38 - you can see it's coming from the
25:40 - scipy.stats package um yeah and then
25:43 - and then down in those examples you can
25:45 - see uh
25:46 - stats.trim mean and then you give it
25:49 - your data set of x
25:50 - and then this second uh
25:53 - this second uh parameter of 0.01 or
25:57 - 0.1 um i imagine we don't really want to
26:01 - get into what that
26:01 - is actually doing but presumably that's
26:03 - talking about like which outliers to cut
26:05 - off or how many outliers to cut off
26:07 - yeah so it it's telling us oh i guess
26:10 - the reason why it wasn't working is that
26:11 - the
26:12 - parameter was named proportion to cut um
26:15 - and it says that's the fraction to cut
26:18 - off both tails of the distribution which
26:20 - is a fancy way of saying
26:22 - if we put 0.1 it's going to take
26:26 - 10 of the data from the lowest
26:30 - and highest end so um so if we crank
26:33 - that up to like
26:34 - 0.5 which would probably be absurd if
26:35 - like cutting out half of our data
26:37 - we're going to be i don't even know if
26:39 - that will get closer to the median or
26:40 - not
26:41 - i guess it depends on well i think it
26:42 - cuts it off both ends so i don't think
26:44 - we can
26:44 - 0.5 yeah um but if we cut it
26:48 - to like even more um
26:52 - yeah it's going to be even closer and if
26:54 - we do
26:55 - a humor here if we do 0.4999 that's
26:58 - going to like
26:59 - leave basically only the median right it
27:01 - cuts 50 off from one side 50
27:03 - from the other side
27:06 - yeah brain's gonna close exactly
27:09 - um yeah so i see this question so we're
27:13 - effectively calculating
27:14 - um the median for eighty percent of our
27:17 - data yes and the answer if we if we set
27:19 - this to 0.1
27:20 - then we're only keeping 80 of our data
27:24 - to calculate that mean
27:25 - um so yeah awesome
27:28 - um cool so
27:32 - this is a great way like i said to get
27:34 - some numerical understanding of
27:37 - typical rents but a much easier way i
27:41 - think to kind of
27:42 - understand all of these numbers in
27:44 - context
27:45 - is to try to look at a histogram oh i
27:48 - forgot to
27:49 - do the mode and i really quickly i think
27:52 - we can
27:53 - actually no um so for the mode
27:59 - i think that will
28:03 - work and it's interesting actually the
28:05 - mode is a little bit similar to
28:07 - uh the summary statistic that we like
28:09 - for categorical variables of like
28:11 - what's the most frequent um what's the
28:13 - most frequent number and so you can see
28:15 - how you were talking about how it's not
28:16 - like
28:16 - the most useful summary statistics for
28:19 - these um quantitative variables because
28:21 - it's like
28:21 - these rent prices there could be there's
28:23 - not going to be a ton of overlap
28:24 - necessarily
28:25 - yeah i think so one of the things that
28:28 - i have seen reported frequently is
28:32 - a mode that's arranged so i think it
28:35 - will help
28:36 - if we print the um the histogram
28:39 - so what i'm going to do is i'm actually
28:41 - going to make a histogram
28:43 - of rhett and then we're gonna talk
28:46 - through what this visualization can tell
28:48 - us
28:49 - um there are a lot of different ways
28:53 - again to make histograms but this is
28:58 - probably one of the easiest actually i'm
29:01 - gonna
29:02 - not use plt.show in here for a second
29:05 - because i know
29:06 - that that is going to print out um
29:10 - some more information to us that might
29:11 - be helpful uh
29:13 - so let's take a look at this picture and
29:16 - see if we can kind of dissect it so
29:19 - you'll see there's there are bars i
29:21 - think by default there are
29:22 - 10 3 4 5 6 7 eight nine
29:25 - ten um and each rectangular bar
29:29 - in here uh is
29:33 - representing the height is representing
29:35 - the frequency
29:36 - of prices in that range
29:40 - so for example this this range
29:43 - or this bar is ranging from let's say
29:46 - like 2
29:46 - 000 it looks like from about like 2 000
29:49 - to maybe
29:51 - 4 000 or something like that the scale
29:54 - is a little hard to tell
29:56 - um but or actually we can read off
29:59 - exactly what it is
30:01 - um i would assume that that's what this
30:04 - is telling us
30:08 - yeah so i think this is so here these
30:11 - are the
30:11 - points so actually the first bar is
30:14 - running all the way from
30:15 - 1200 1250 to uh
30:19 - 31.25 um
30:23 - and it's it's kind of a little hard to
30:25 - see but it's saying
30:26 - that between these two rental prices so
30:29 - between 1250 and 31.25
30:32 - there are seven 1779
30:36 - apartments that are in that range
30:39 - nice the second one is a little bit
30:42 - easier to see where it's going from that
30:44 - 3125 to 5000
30:47 - and you can see the second bar the right
30:50 - side of the second bar is right at that
30:51 - 5000 tick mark
30:54 - right exactly i just remembered last
30:57 - time
30:58 - i meant to remember to zoom in again
31:00 - yeah
31:01 - yeah it looks good
31:04 - yes uh cool and so
31:08 - each bar right we've just now divided up
31:11 - the rents
31:11 - into these kind of bins they're called
31:15 - um so we've divided it into 10 equally
31:19 - sized bins
31:20 - um and then the heights of each bar
31:23 - represent the number of
31:25 - of apartments in that bin so this really
31:28 - gives us a sense
31:29 - of how frequent different price ranges
31:32 - are
31:32 - for our apartments and just taking a
31:35 - look at this we can already see
31:37 - kind of what we were talking about
31:38 - before how there's going to be
31:40 - a smaller number of really expensive
31:42 - apartments that are kind of drawing that
31:44 - mean
31:45 - upwards from where uh the majority of
31:48 - the data is so you can kind of imagine
31:50 - like
31:51 - we said remember that the median was
31:55 - 3600 so that's somewhere
31:58 - down here and you can kind of imagine if
32:01 - i
32:02 - um i think i can add or let me add a
32:04 - verb
32:05 - collide this um
32:08 - actually you know before i even do that
32:11 - i'm gonna actually
32:12 - increase the number of bins here so by
32:15 - default there were 10
32:16 - let's increase the number of bins to 20
32:18 - just so that we can get a little more
32:20 - detailed of a view of this
32:24 - so actually it looks like there's two
32:25 - bars that are almost exactly the same
32:27 - height here
32:28 - interesting and then
32:31 - i'm going to pull this code from the
32:33 - final
32:37 - the final code
32:40 - just because it's a lot to write out
32:44 - but um
32:49 - let's throw the
32:55 - let's just throw the meat or let's throw
32:56 - the median on actually
32:59 - um and i called it i called it something
33:02 - different didn't i
33:04 - called it rent median
33:09 - and we'll label it again um
33:12 - okay so all this is doing is it's going
33:14 - to plot a vertical line
33:16 - on top of this that's located at the
33:22 - median
33:24 - gonna get rid of that okay so here's my
33:26 - median and so
33:27 - it it kind of turns out and i think this
33:30 - is
33:31 - hard to it's almost hard to visualize
33:34 - here just because
33:36 - the bins are so um like the
33:39 - the way that the x-axis gets plotted on
33:42 - on histograms is always a little bit
33:44 - hard to say
33:44 - or hard to see but basically what this
33:47 - is saying is
33:48 - there are as many apartments on this
33:50 - side of that black line
33:52 - as there are on this whole side of the
33:55 - like other side of the black line like
33:57 - in theory the
33:58 - area of this side should be equal to the
34:01 - area of this side
34:02 - i don't think it's quite right just
34:05 - because
34:06 - of how the bins and the x-axis are
34:09 - lining up if we add like
34:12 - this it'll be easier to see and now i
34:14 - need to make it
34:15 - max out at like 300. um
34:20 - yeah so now you can probably see a
34:22 - little more clearly like we're saying
34:24 - that the area over here
34:25 - is equal to the total area over here
34:30 - so if i wonder if we can look at uh can
34:33 - we look at another column just for
34:34 - another example
34:36 - yeah would you like um i love the
34:39 - minutes to subway and i'm trying to even
34:41 - think of what this this might look like
34:42 - whether it's going to be
34:44 - it might just be totally flat
34:47 - right where there's like an equal number
34:48 - of apartments all distances from
34:51 - subways maybe not i don't know i'm gonna
34:54 - have to think about it
34:57 - men too
35:07 - so it seems like most here we'll add
35:09 - some more bends for
35:13 - um also this so there's two main
35:15 - plotting libraries
35:16 - in python there's uh
35:20 - matplotlib and then seaborn which is
35:23 - built i think on top of matplotlib
35:26 - and seaborn i know like if i do
35:30 - snss i think i imported eborn as
35:33 - sns if i do sns.dist plot
35:38 - with kde equals false
35:42 - i'll get the same thing but it will do
35:44 - it will
35:45 - automatically try to decide how many
35:47 - bins to use
35:48 - based on my data whereas matplotlib just
35:52 - always uses 10 no no matter what i pass
35:54 - to it
35:55 - um so just as an aside i think
35:59 - like you should use whatever you feel
36:02 - comfortable with and there's lots of
36:04 - there are lots of different options for
36:07 - doing the same thing
36:08 - but i'm i might start doing this just
36:10 - because it's a little
36:12 - it's a lot easier to kind of see this
36:15 - distribution
36:16 - anyway um okay so we've got most
36:19 - it seems like most apartments are
36:21 - basically zero to ten minutes
36:23 - yeah i guess that makes sense like very
36:25 - very few apartments that are
36:27 - really really far away from a subway and
36:29 - then we seem to randomly have a couple
36:31 - apartments that are like 45 minutes away
36:34 - yeah so that's a good example of like we
36:36 - can maybe consider that
36:38 - uh data worth throwing out of like
36:40 - something is something
36:41 - is clearly weird about that data yeah
36:45 - it's interesting i i mean it would be
36:47 - interesting to go in and look at those
36:49 - apartments like filter try to filter
36:51 - this data set
36:52 - oh i bet it's staten island do you think
36:55 - so
36:56 - probably that's what i would guess right
36:58 - is there i don't know if there are
36:59 - subways over on staten island i don't
37:00 - think so or whatever would a subway
37:03 - station be considered there but uh
37:07 - [Music]
37:12 - greater than 40. i think we can do this
37:15 - yeah i think so like print it out
37:18 - um hmm
37:23 - so it looks like we've got a bun a
37:25 - couple in the upper east side
37:27 - more upper east upper west and can we
37:30 - confirm that this is actually
37:32 - can we look at their mints to subway and
37:33 - make sure that they're all over 40.
37:35 - sorry guys um 43
37:38 - 43. so a bunch of 43 which is odd um
37:42 - i would think that that has to be error
37:44 - right there's no way that
37:46 - they're all exactly 43 and the upper
37:48 - west side is like you know the middle of
37:50 - manhattan there's no way that
37:51 - they're 43 minutes away from the upper
37:53 - west side what would you do in a
37:54 - situation like this of like
37:56 - i think that this data is wrong like
37:58 - what's the right thing to do
38:01 - that you think is wrong that's a really
38:04 - hard question
38:05 - so i don't claim to have an answer to
38:08 - that
38:09 - um i think one of the things
38:14 - one of the first steps i would take at
38:16 - least here is
38:17 - i know this uh i know this data set
38:20 - was kind of like created for us in
38:23 - partnership with street easy
38:25 - um and i don't know if like maybe they
38:28 - pulled
38:29 - a bunch of apartments from the same
38:31 - building or something like i would
38:33 - actually
38:34 - first look to see if maybe
38:38 - building id like for some of these if i
38:41 - said
38:42 - like building id was the same for all
38:45 - a bunch of them oh yeah like this one
38:47 - this one this one
38:49 - so i guess the short answer just like
38:51 - dive into the data further to figure out
38:53 - what's going on
38:54 - yeah so i would first look at the data
38:56 - i'd try to see if there was any sort of
38:59 - reason why a bunch of them should have
39:00 - the same minutes to subway
39:03 - um i think what's more confusing to me
39:05 - is
39:06 - in this like histogram is that there's a
39:09 - clear bar
39:10 - here at 43 and like nothing on
39:13 - either side of it that i really see um
39:15 - and as you're scrolling through here
39:16 - like
39:17 - these are not all the same building so i
39:19 - don't really know
39:20 - um like it might have to do
39:24 - something with like the data collection
39:26 - um
39:28 - or how this data set was filtered uh
39:32 - and so you'd really this would be again
39:34 - like a situation where you'd have to be
39:36 - a little bit of a sleuth
39:37 - um and and look into it but this is
39:40 - uh somebody asked are they all on the
39:43 - top floor with a broken elevator
39:46 - that's great i mean maybe you can look
39:49 - into it
39:50 - but i think i think this is a really
39:52 - good point um more generally for the
39:54 - importance of doing this kind of
39:56 - analysis this univariate analysis and
39:59 - looking at the
40:00 - looking at these distributions because
40:03 - this is how you catch weird anomalies
40:05 - and then um and then you can go back and
40:08 - kind of be like
40:11 - okay why is this happening is it a data
40:13 - error what other data cleaning steps do
40:15 - i need to do
40:16 - uh and this is really a cyclic or
40:19 - cyclical process so like
40:20 - we are presenting this as if like you're
40:23 - gonna clean your
40:24 - data and then you're gonna do this but
40:26 - this process might lead you to go back
40:29 - and do some additional
40:30 - cleaning steps because you suddenly
40:32 - realize that there's something weird
40:33 - going on um and it's super super
40:36 - important
40:37 - i was also having i like had a
40:39 - conversation with somebody after
40:41 - uh the the last
40:44 - live stream actually i should just say i
40:46 - had a conversation with my dad after the
40:48 - last laughter
40:49 - because my parents are probably one of
40:53 - very few
40:53 - two very few few people who are watching
40:55 - this and uh
40:57 - and he was saying you know like in
41:00 - practice data cleaning a lot of the time
41:02 - gets
41:02 - automated like you're not gonna go
41:04 - necessarily have to go through
41:06 - um and change the data types of
41:08 - everything or change the missing data
41:10 - that you have
41:12 - to um to like something that python can
41:15 - recognize as missing data because you'll
41:17 - probably
41:18 - want to write a script to check for
41:19 - common forms of missing data and like
41:22 - maybe if you work in a company and you
41:25 - understand
41:26 - like common data issues you might write
41:29 - scripts
41:30 - or write functions that can
41:33 - do all that data cleaning in one step
41:35 - but i think like this
41:37 - this piece of it really highlights the
41:39 - importance of
41:41 - not just using the same script on
41:45 - lots of different data sets because it
41:48 - still is really important to get to know
41:49 - your data and
41:50 - understand understand each variable and
41:54 - understand what's going on
41:55 - because otherwise like there's all sorts
41:57 - of weird things that can creep into a
41:59 - data set without your realizing
42:01 - and if you don't do these steps then you
42:03 - might never know about it
42:04 - yeah and it's probably worth pointing
42:05 - out that it's someone's job to do that
42:08 - data cleaning right where like somebody
42:09 - at your company needs to define like
42:11 - okay this
42:11 - this data is gonna be categorical this
42:13 - is this is gonna be
42:14 - um numerical right but it's probably
42:18 - like the
42:19 - the database engineer or whoever is like
42:22 - setting up the database in your
42:24 - in your company but you know somewhere
42:26 - the data is coming in somebody's gonna
42:27 - have to define what it looks like and
42:28 - hopefully
42:29 - once it gets to you as like a data
42:31 - scientist or a data analyst
42:33 - hopefully it's been clean for you a
42:34 - little bit um and you won't have to
42:36 - mess around with it now that stuff too
42:37 - much yeah
42:39 - um okay so one thing
42:43 - so far we've really focused we've really
42:45 - been focused on
42:47 - central location um or in other words
42:50 - mean median and mode
42:51 - trying to understand what a typical
42:53 - value looks like
42:55 - um we we plotted the median on here we
42:58 - could also plot the mean and
42:59 - the um and the mode uh what i was saying
43:02 - to alex before about
43:04 - mode and sometimes mode being reported
43:07 - as a
43:08 - as a range for quantitative variables is
43:10 - that essentially
43:11 - you might look at this histogram um
43:14 - i'm gonna change well uh
43:18 - you might look at this histogram and
43:19 - look which bar is the tallest
43:21 - and then report that range as the mode
43:24 - rather than reporting a single value as
43:26 - the mode
43:27 - um but we can kind of get a sense for
43:29 - this the mean is
43:30 - um or the median is here the mean is
43:33 - going to be a little bit larger than the
43:35 - median
43:35 - and then the mode i think is going to be
43:39 - maybe a little bit lower um
43:43 - yeah and i would say histograms even are
43:45 - sometimes like deceptive in showing the
43:47 - mode because it could be possible that
43:49 - the most frequent number is in some bar
43:51 - that's not the tallest bar
43:53 - right where it's like right if there are
43:55 - if there are 300
43:56 - you know if every single bar at 2500 is
43:59 - exactly 2500
44:00 - that's probably going to be the mode
44:02 - even though it's not the tallest bar
44:04 - because in the tallest bar it could be
44:05 - split up among
44:06 - 10 different numbers or something right
44:07 - which is why i think giving a range as
44:10 - the mode for a quantitative variable is
44:11 - really a smarter way to summarize that
44:15 - variable um okay so
44:18 - typical apartment looks like it's
44:20 - somewhere around
44:22 - i'm going to say like 36 37
44:25 - 3700 a month
44:28 - but there's another piece to this
44:30 - visualization which is
44:33 - it doesn't just tell us essentially what
44:35 - the
44:36 - kind of middle value or most common
44:38 - value or
44:39 - um most typical value is it also gives
44:43 - you some
44:43 - information about how spread out values
44:46 - are
44:46 - around that so i mentioned before this
44:49 - being kind of a skewed dis
44:52 - i would call this a skewed distribution
44:54 - which essentially just means that
44:56 - it's it's not symmetrical like it
44:59 - doesn't just go
45:00 - up and come down and there's an equal
45:02 - number of
45:03 - apartments on either side of the mean
45:06 - um in other like in other distributions
45:10 - this might
45:11 - look super super uh symmetrical
45:14 - but but it's not so um
45:18 - and then the other thing we can glean
45:20 - from this is that
45:22 - it seems at least on the right side of
45:24 - this histogram
45:25 - like values are pretty spread out like
45:29 - you can be on the right hand side of
45:31 - this line and you could be
45:32 - anywhere from thirty seven hundred
45:35 - dollars a month
45:36 - all the way up to twenty twenty thousand
45:40 - um and so there's a lot of variation
45:41 - here
45:43 - and if you're summarizing a quantitative
45:44 - variable you probably
45:46 - want to capture that variation somehow
45:49 - because i'm just going to imagine like
45:53 - so i'm talking to alex uh i'm looking
45:55 - for apartments
45:57 - in new york city i've downloaded this
45:58 - data set try to understand
46:00 - what typical apartments in new york city
46:02 - look like
46:03 - i'm like okay alex the median
46:07 - the median rental price is
46:10 - um 3600 dollars a month
46:14 - so that i'm gonna try to get like a
46:17 - an apartment that's about that expensive
46:19 - because i feel
46:21 - like median sounds good to me um or like
46:24 - that sounds like a reasonable price for
46:26 - me to ask from a landlord
46:28 - obviously there's more things that go
46:30 - into that like how many bedrooms i need
46:31 - but we're gonna simplify this
46:33 - and um and then maybe alex is like okay
46:37 - well
46:38 - you can go a little above the median
46:41 - because you know like maybe you can
46:43 - splurge a little bit
46:44 - um and and so i might come back and say
46:48 - well i don't know how much above i could
46:50 - go because i don't know
46:52 - if rental prices are all really close to
46:55 - the media
46:56 - and then there's going to be no
46:57 - buildings that are
47:00 - way higher than that like 4 000 or 500
47:03 - you don't know if splurging means 10
47:04 - or a thousand dollars yeah but if
47:08 - rental prices are really spread out um
47:10 - from that
47:11 - middle number then i could
47:14 - you know then it it makes sense for me
47:17 - to try to
47:18 - find something that that is a lot larger
47:21 - or a lot more expensive or a lot less
47:22 - expensive i have a lot more
47:24 - to choose from basically and so not
47:27 - knowing more than just the middle is
47:29 - helpful for me in understanding what
47:32 - these rental prices look like
47:34 - so um there are a bunch of different
47:36 - measures for
47:37 - spread which is really just measuring
47:41 - how wide this distribution is like how
47:43 - far
47:44 - away are we getting from this middle
47:46 - number um
47:48 - and and you can probably guess some of
47:50 - them i mean the simplest
47:52 - the simplest simplest one is to just
47:54 - look at the minimum and the maximum
47:57 - which we did in our um in our describe
48:01 - thing but uh i'll really quickly
48:05 - look at so yeah and again this is a
48:07 - these are statistics that can can be
48:09 - pretty impacted by outliers where
48:11 - if you have an outlier that is huge
48:13 - that's gonna
48:15 - that's gonna be the maximum uh but it's
48:16 - not really gonna tell you anything about
48:18 - the rest of the data or what how the
48:19 - rest of the data is represented
48:21 - um or in the in the case of the subway
48:24 - um right the the max would have been
48:26 - that 43 minutes but we don't really know
48:29 - what that represents
48:30 - right um so we see right it's between
48:34 - 1250 and 20 000. like alex said
48:38 - we've definitely got some outliers here
48:41 - um i think
48:42 - there's a couple of things we can do to
48:44 - try to understand to try to get rid of
48:46 - the outliers
48:47 - right we can we can start kind of doing
48:49 - the same thing that we did for the
48:51 - trimmed mean
48:52 - but we can just like trim off ten the
48:55 - bottom ten percent in the top ten first
48:57 - percent and report those numbers
49:00 - um so like the the cutoff for the bottom
49:03 - 10
49:04 - is called the 10th percentile the cutoff
49:06 - for the top 10
49:07 - is the 90th percentile um and so that
49:10 - would give us kind of like a
49:12 - a trim range if you will um
49:16 - we can actually see if we go back up to
49:19 - the describe
49:20 - output it gives us i think the
49:24 - 25th percentile and the 75th percentile
49:27 - so those are
49:28 - the kind of the cutoffs for the middle
49:30 - 50
49:31 - of the data and that can tell us
49:33 - something so
49:34 - for rent um that's between 2750
49:38 - and 5200. so basically
49:42 - the middle 50 of apartments are
49:44 - somewhere between 27.50 and 5200
49:48 - so um so that can give me some
49:50 - understanding of
49:52 - how much variation there is around this
49:54 - this median
49:56 - remember the middle the 50th percentile
49:58 - is the median it's the middle number
50:01 - um so look at that poor minimum for
50:04 - square footage
50:04 - of i think that's 250 uh minimum square
50:07 - feet uh
50:09 - oh yeah oh man i
50:13 - i have never i think the largest
50:16 - apartment i ever lived in was
50:18 - my second apartment in dc we had like
50:21 - 900 square feet
50:23 - in two people and it was super glorious
50:26 - and
50:27 - it will never happen again um
50:30 - there are some other ones like standard
50:32 - deviation i'm looking at the time
50:34 - and starting to realize that once again
50:37 - we've
50:37 - over scoped i always think we're gonna
50:39 - have extra
50:40 - we go on tangents but the tangents are
50:41 - worth it yeah we do
50:45 - um so standard deviation is another one
50:48 - that's a really
50:49 - uh common measurement for uh
50:53 - for the amount of variation around the
50:55 - mean um
50:57 - and i mean we can calculate it
51:01 - it's it's not a super interpretable
51:03 - statistic
51:05 - uh right off the bat
51:10 - but um but we can use it to kind of
51:14 - compare distributions
51:15 - and for distributions
51:18 - that are normal which we'll talk
51:22 - about what that means in a later um in a
51:24 - later
51:26 - live stream this gives us some sense of
51:29 - how far away you have to go from the
51:30 - mean
51:31 - in order to encapsulate like 65 of the
51:34 - data
51:35 - 95 percent of the data you can kind of
51:38 - uh calculate those things super easily
51:40 - so a smaller standard deviation means
51:42 - that the the data is more
51:44 - is closer together yeah we can actually
51:48 - like super super fast because why not
51:52 - um small standard deviation let's like
51:56 - create some data um so there's this
52:00 - function numpy
52:01 - random normal where we can create
52:05 - a data from a normal distribution
52:08 - um actually i'm gonna just really
52:11 - quickly
52:11 - pull up the
52:14 - uh the
52:23 - information about it so the parameters
52:27 - are the center and the standard
52:30 - deviation
52:31 - and then the number of values
52:34 - so let's say we center it at
52:38 - 10 and then we'll do a standard
52:41 - deviation of one
52:42 - and we'll grab a thousand values
52:46 - and then we'll get one
52:52 - with a standard deviation of
52:55 - 10 instead and then let's
52:59 - um plot histograms of them
53:03 - i'm gonna do the large one first because
53:08 - that way we can plot them on top of each
53:12 - other
53:16 - that just the alpha parameter is just
53:18 - going to make it um
53:25 - let's see if this works on the fly
53:35 - yeah nice so yeah so you can see
53:39 - i probably should have added a legend
53:40 - but this is the
53:42 - standard deviation of one and this is
53:44 - the standard deviation of 10.
53:46 - so you can see they're both centered at
53:48 - 10 which is what i said
53:50 - um but you can see that the one with the
53:52 - larger standard deviation is way wider
53:55 - than the one with the super small
53:56 - standard deviation
53:59 - cool okay we've got five minutes left so
54:02 - i am going to jump into
54:03 - categorical variables really fast so
54:05 - that they don't feel left
54:07 - out um i think the main thing to say
54:10 - about
54:10 - categorical variables and summarizing
54:12 - them is that
54:14 - you really shouldn't use the same
54:16 - summary statistics
54:17 - as you use for uh quantitative variables
54:21 - and this can happen a lot of times by
54:23 - accident um it can happen if you don't
54:26 - code your variables correctly so
54:28 - um if you have a categorical variable
54:31 - that's coded as
54:32 - values like one two three four like we
54:34 - saw last time
54:35 - um it can be then
54:39 - if you're using built-in packages or
54:41 - built-in functions
54:42 - um sometimes you'll miss
54:46 - categorical variables especially if
54:48 - you're working with a really large
54:49 - data set um but
54:53 - the other thing is like if you have a
54:56 - categorical variable that's ordered
54:57 - so i think the example we gave last time
55:00 - was
55:01 - something like responses to a question
55:03 - about agreement like
55:05 - strongly disagree disagree neutral agree
55:09 - strongly agree and you've coded those
55:13 - um in your data then it might be
55:15 - tempting
55:16 - let's say like everybody had to in their
55:19 - responses
55:20 - rate their agreement on a scale from one
55:22 - to five so you have those numbers
55:23 - it might be tempting to say okay well we
55:25 - can calculate
55:26 - mean agreement score um but that's
55:30 - really not a good idea because
55:33 - the difference between strongly disagree
55:36 - and
55:36 - disagree might be very different
55:40 - than the difference between disagree and
55:43 - neutral
55:44 - like it might be a large much larger
55:46 - amount of
55:47 - change in agreement between each like
55:50 - each step
55:52 - um and so to treat that as if it's
55:54 - numerical
55:55 - when it's really not is kind of
55:58 - misleading
55:59 - yeah you could even imagine a survey
56:00 - that was like created poorly where the
56:02 - options were like
56:03 - strongly disagree kind of disagree very
56:06 - barely disagree
56:07 - neutral and strongly agree where it's
56:09 - like the gap between all of those
56:11 - is not even right and so um taking the
56:14 - average there is not really fair to some
56:15 - of those uh
56:16 - to some of those responses yeah i'll
56:18 - pull up
56:19 - so i have i had some notes
56:23 - from a different data set that i was
56:26 - looking at
56:27 - and one of the examples was
56:31 - with travel time this so this is a data
56:33 - set of students
56:35 - um and one of the um
56:38 - one of the variables is travel time to
56:40 - school so this is like how far
56:42 - each student has to travel to get to
56:43 - school and so these are the categories
56:46 - they're
56:46 - less than 15 minutes 15 to 30 minutes 30
56:49 - minutes to an hour or more than an hour
56:52 - and so these are different amounts of
56:53 - time right like 15
56:55 - so this is a 15 minute window whereas
56:58 - this is like a 30 minute window and then
57:00 - these are like
57:00 - anything more than an hour anything less
57:03 - than 15 minutes
57:04 - um so i guess this is also a 15 minute
57:06 - window so but these are all
57:08 - kind of different sized buckets and so
57:11 - because they're different sized buckets
57:13 - to calculate a mean of these even though
57:15 - they're
57:15 - there is an ordering and we could like
57:17 - in the original data these are coded as
57:19 - one two three four
57:20 - um we still shouldn't calculate the mean
57:22 - of these
57:23 - i think it's more obvious with this kind
57:25 - of thing with an agreement scale people
57:27 - do it a lot because
57:29 - it feels normal but don't do it
57:33 - would you even say i'm sorry i know this
57:34 - is a tangent we have literally two
57:35 - minutes left but would you say that uh
57:36 - like
57:38 - if a survey is made properly and it's
57:40 - like the spaces are
57:41 - even would you still recommend not doing
57:43 - it because folks don't respond to
57:45 - surveys as if every bucket is
57:47 - is of equals how do you know you mean
57:49 - for something like this where the
57:51 - spacing is
57:52 - even between them like it was like zero
57:55 - to fifteen fifteen to thirty thirty
57:57 - if yeah yeah but but even with a survey
58:00 - where it's not
58:01 - like where you can't quantify size where
58:04 - if it is strongly disagree
58:05 - even if it's disagreeing you can't agree
58:07 - quantify size then you have no way of
58:09 - under
58:10 - if you can't if it's agreement and you
58:12 - can't quantify the size of a
58:14 - agreement then you have no way of
58:17 - knowing if they're
58:18 - if they're not equal yeah so you still
58:20 - shouldn't do it yeah yeah i guess that
58:22 - makes sense where it's like
58:23 - strongly disagree might be of smaller
58:25 - size because you
58:26 - like yeah it's actually like a whole
58:29 - area of statistics
58:31 - um like psychometrics
58:34 - and understanding i yeah i mean i know
58:38 - like
58:38 - i know a very rudimentary or i have a
58:42 - very
58:42 - rudimentary understanding of it but um
58:45 - but yeah it's like a whole area of
58:48 - statistics
58:49 - that we will not go into in this live
58:52 - stream
58:52 - but it is interesting um okay we have
58:55 - like
58:56 - two seconds i think the the first thing
58:58 - that i always do
58:59 - if i'm trying to understand a
59:00 - categorical variable is i just print out
59:03 - the frequencies of each category i'm
59:06 - gonna just
59:06 - demo that really quickly um i think
59:10 - the easiest one is the burrow uh
59:14 - variable and i'm just gonna do oops
59:17 - bro value counts gives us the
59:20 - frequencies
59:22 - um so we've got and
59:25 - by default it puts the most frequent one
59:27 - at the top so there are
59:29 - 3 500 apartments in manhattan
59:33 - 1013 in brooklyn 448 in queens
59:37 - we can also get these as proportions if
59:41 - we want
59:42 - um i think normalize equals true we'll
59:45 - calculate proportions
59:47 - there's no missing data in this data set
59:49 - but um
59:51 - if there were missing data like if i
59:53 - really quickly
59:54 - uh
59:59 - like create um
60:04 - some missing data and then
60:07 - oops
60:13 - oh
60:18 - what what am i doing wrong okay well
60:22 - if i were to create some missing data
60:25 - um we would see that it's not getting
60:28 - counted
60:29 - it might be i walk i don't know what
60:31 - lock versus oh oh it's because i
60:34 - did burrow twice um like if i do this
60:38 - and then i do normalize equals true
60:43 - you'll see that these numbers change
60:45 - slightly like
60:47 - .7078 went to
60:52 - 0.7077
60:53 - um because it's it's not counting that
60:56 - missing data
60:58 - in the um in the denominator so
61:01 - the denominator is now slightly smaller
61:04 - um so just
61:05 - like again these are all just little
61:07 - things to pay attention to
61:09 - and the last thing i'll demonstrate is
61:13 - how to create a
61:17 - power plot um
61:23 - which is super easy um and this looks a
61:27 - lot like a
61:27 - histogram but now because we've got
61:29 - categorical data
61:31 - um it's not like we have an ordered set
61:33 - of values on the x-axis we just have
61:37 - discrete values like manhattan
61:40 - queens and brooklyn um and so this gives
61:43 - us the same
61:43 - the same sort of sense the frequency of
61:45 - each category but now it's not
61:47 - a frequency of a range of values it's
61:49 - just the frequency of a category
61:51 - yeah and speaking to how uh these aren't
61:54 - in like ranges these are just like a
61:56 - single
61:57 - value right it's not a it's not a a
61:59 - bucket of a range it's just no these are
62:00 - all the ones that are manhattan as
62:01 - opposed to a histogram which was like
62:03 - hey this was apartment buildings
62:04 - that cost between a thousand and two
62:06 - thousand right
62:08 - awesome okay i'm gonna leave it there i
62:10 - know we're two minutes early or two
62:12 - minutes over
62:13 - um and at this point
62:17 - i think uh i've we've thrown a lot at
62:20 - you this is like
62:22 - probably a couple of classes of a
62:24 - regular statistics class to really dig
62:26 - into each of these summary statistics
62:28 - and really understand them this is
62:30 - multiple modules
62:31 - um in the master statistics path so
62:35 - if you're really interested in digging
62:37 - into this and you
62:39 - are a pro member i really suggest that
62:41 - you go through some of that
62:42 - uh that content there's actually a bunch
62:44 - of like
62:45 - non-coding content content which is
62:48 - really just focused on
62:49 - looking at histograms and reading them
62:51 - and trying to understand them
62:53 - and also goes through some other plots
62:55 - like box plots that we didn't have time
62:57 - to cover today
62:59 - um but this was really fun for at least
63:04 - one hour of time to kind of swing
63:07 - through
63:07 - all of this stuff so if we unplug what
63:09 - we're doing next week
63:10 - yeah so next week we're going to start
63:12 - looking at relationships between
63:14 - multiple variables
63:16 - so um again like if we're starting
63:19 - let's say our end goal is to try to
63:22 - predict or say what the what a fair
63:25 - price is for an apartment with
63:27 - specific attributes then we want to
63:29 - understand how rental price is related
63:32 - to other things like the number of
63:34 - bedrooms or
63:35 - the square footage or something like
63:37 - that so we want to understand how things
63:38 - are related to each other and that's
63:40 - what next week's class is going to be
63:42 - about
63:42 - and then we're going to move after that
63:45 - into
63:46 - my most favorite part of this whole
63:48 - series which is going to be
63:50 - uh hypothesis testing and inferential
63:52 - statistics which is really when things
63:54 - get fun
63:55 - um cool so tune in next time
63:59 - uh send us questions like if you
64:02 - if you have any follow-up questions or
64:04 - comments or feedback
64:05 - you can let us know um either my email
64:09 - is sophie codecademy.com um
64:12 - s-o-p-h-i-e i'll drop it in the chat uh
64:15 - but also like you can comment on the
64:17 - youtube video
64:18 - or um comment comment on the youtube
64:21 - video that'll help more people find it
64:22 - you know we can do the like
64:24 - youtuber thing of like hit the like
64:26 - button hit the subscribe button do all
64:28 - that stuff if you like these videos
64:29 - uh you know let's let's try to get more
64:31 - people watching them too yeah
64:33 - all right have