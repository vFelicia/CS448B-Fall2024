00:00 - stream series on linear regression
00:03 - um we're gonna be kind of reviewing a
00:07 - lot of the stuff that we covered in the
00:08 - last
00:09 - seven weeks in one day and we're just
00:12 - gonna kind of play with a data set
00:14 - and really go through a full process
00:17 - from start to finish
00:18 - of how do we get some data how do we
00:21 - clean it up a little bit
00:22 - how do we fit some models how do we
00:24 - compare them
00:25 - and maybe we'll come up with we'll see
00:27 - if we can come up with the best possible
00:29 - model
00:30 - that we can for predicting rental prices
00:33 - uh by the end of this session yeah i
00:36 - think this session in particular is kind
00:38 - of
00:39 - more open-ended than our last ones
00:40 - basically our plan is to play around
00:42 - with this data set and so
00:43 - definitely if you're watching this in
00:44 - the chat as we are doing this live
00:46 - um and you want us to like take a look
00:48 - at any of these features or want us to
00:51 - experiment with this in a particular way
00:53 - we are super open to
00:55 - uh kind of uh making this as interactive
00:58 - as possible
00:58 - so um yeah exactly yeah we want this to
01:02 - be kind of a fun way to kind of
01:04 - wrap everything up but also make it
01:07 - explorational explorative
01:10 - i don't know um okay cool
01:14 - so um one thing i do want to point out
01:17 - before we get started
01:18 - is that um if you're following along
01:21 - with the linear regression course
01:23 - on our on our site and i'll i'll go
01:27 - to codecademy really quickly right now
01:30 - and pull that
01:31 - up so um if you go to this linear
01:35 - regression in python
01:36 - course and take a look at the syllabus
01:39 - um
01:40 - the data set that we're using in a
01:42 - cleaned form
01:44 - exists in this um last project
01:47 - in the in the course this craigslist
01:50 - analysis
01:51 - so if you'd like to play around with
01:54 - this and you want to
01:55 - just get the data already loaded for you
01:58 - and
01:58 - see an example solution
02:01 - this is a little bit subseted and it's a
02:04 - little bit cleaned up
02:05 - but you can do that here if you have a
02:07 - codecademy subscription
02:09 - and even after this
02:12 - this stream if you want to play with it
02:14 - yourself but you don't want to set up
02:15 - your
02:15 - local environment this is a great place
02:18 - to come and um
02:19 - and play with it cool
02:22 - so i'm just going to show you i'm not
02:24 - going to go through the full process but
02:26 - i'm just going to show you
02:27 - where i downloaded the data from and
02:31 - the reason i'm showing you this is
02:33 - really just because
02:35 - if you're working on your own computer
02:36 - you're doing your own analysis you're
02:38 - just getting started
02:40 - um you might come to a place like kaggle
02:43 - or the uci machine learning repository
02:45 - that we've shown before
02:46 - and you might download your own data set
02:49 - and so i think it's
02:50 - it's useful to kind of see the process
02:52 - from the beginning
02:53 - um so this is the data set we're going
02:56 - to be working with i found it on kaggle
03:00 - just by clicking going to kaggle.com
03:02 - clicking data sets and then searching
03:04 - around within data sets
03:06 - um i like to use data sets that have a
03:08 - high usability rating so 10.0 is
03:11 - like the highest i think that you can
03:14 - get um
03:14 - so if you're just starting out it just
03:16 - means it's really easy to
03:18 - download this data and get started with
03:19 - it it's not super super messy although
03:21 - this is a pretty big data set and has
03:24 - um some columns that you might not care
03:27 - about
03:28 - but it's super easy to use it's got um
03:31 - it's got like descriptions of everything
03:34 - um it's got like an overview it's got
03:37 - easy download all of that so
03:41 - we can take a a brief look at it even on
03:44 - kaggle we see there's
03:45 - 22 columns 10 of them are printed out
03:48 - here
03:50 - so you can get a sense for what kind of
03:51 - information might be contained in here
03:55 - and then we can also get a little bit of
03:58 - of
03:58 - context and information about how this
04:00 - data set came
04:02 - to exist on kaggle um so it gives us
04:05 - this
04:06 - the sentence craigslist is the world's
04:07 - largest collection of privately sold
04:09 - housing options yet it's very difficult
04:11 - to collect all of them in the same place
04:13 - so this person built this data set as a
04:15 - means by which to perform experimental
04:17 - analysis on united states
04:19 - um as a whole and instead of isolated
04:22 - urban housing markets
04:24 - the data is scraped so web scraped every
04:26 - few months
04:27 - so somebody else has done all the web
04:29 - scraping for us which is nice
04:31 - um it contains almost all relevant
04:33 - information that craigslist provides
04:35 - on retail sales so um this is just data
04:38 - that has been web scraped from
04:40 - craigslist
04:41 - about housing listings yeah the license
04:44 - is kind of interesting
04:45 - in if you're like making a project for
04:47 - your portfolio or something that
04:49 - you're making for your own personal
04:50 - projects you might want to look at the
04:52 - license and just kind of like dive into
04:54 - um what you're allowed to do with the
04:56 - data set
04:57 - um obviously if it's on table like this
05:00 - um you'll probably be able to like
05:01 - download it and play around with it
05:02 - yourself but again
05:04 - you might want to look at the license
05:05 - when you think about hey how should we
05:07 - be
05:07 - like publishing something related to
05:09 - this this data set
05:11 - yeah exactly um
05:14 - okay so i think we can get started so
05:17 - all i did was i pressed this download
05:19 - button
05:20 - um you have to create a kaggle account
05:22 - oh you can't see my mouse
05:24 - but you can see that it's highlighted in
05:26 - the top right
05:28 - of this right hand corner of this page
05:30 - there's this download button
05:33 - and you have to create a kaggle account
05:35 - once you press download it will download
05:37 - onto your computer
05:38 - and then all i've done is i've moved
05:41 - this housing.csv file that i that
05:45 - was downloaded when i press that button
05:47 - into the folder
05:48 - where my jupyter notebook is or it could
05:51 - be the folder where you've got
05:53 - um like your script that you're writing
05:57 - as well and then i've just opened up
06:00 - this
06:00 - code dot ipynb
06:04 - um this jupiter notebook and this allows
06:06 - me to start playing with the data
06:08 - yeah and again if you aren't super
06:10 - familiar with how to work with jupyter
06:12 - notebooks we've got a lot of articles
06:13 - that kind of
06:14 - help you set that up it's slightly
06:16 - complicated to just like open the
06:18 - jupyter notebook to begin with i'm
06:20 - assuming you did it through
06:21 - terminal or did you do it through um
06:23 - like the jupiter
06:24 - app i did it through terminal um it's
06:26 - actually
06:27 - like once you have it downloaded um it's
06:31 - relatively simple to do via the terminal
06:34 - um i think you just
06:35 - you just type jupiter notebook into the
06:37 - terminal
06:38 - um but yeah it definitely takes a little
06:41 - setup on your computer first before you
06:43 - get that running
06:45 - um and i think we we provided some
06:48 - information on how to download
06:50 - everything on the first
06:52 - stream i believe cool um
06:56 - cool so um i'm just gonna load a bunch
06:59 - of libraries i don't know which ones
07:01 - we'll end up
07:01 - using and then i'm gonna go ahead and
07:04 - load this data
07:05 - um you'll notice normally i just do
07:09 - like this right read in the csv
07:13 - um but let's just print what that looks
07:15 - like
07:17 - um
07:21 - oh actually seems like it's fine i
07:23 - pulled this from somewhere else
07:25 - um you might notice though things that
07:27 - happen when you
07:28 - when you first load a data set
07:31 - um you might see like the header for
07:34 - example is getting
07:36 - pulled in as a row instead of um
07:39 - instead of like column names and so
07:42 - that's what i was intending to do in
07:44 - that first line of code but
07:45 - it looks like it's being read in okay so
07:48 - that's fine um and i've used the pandas
07:51 - library to read it in
07:53 - and so i think a good goal for today is
07:56 - to try to
07:58 - come up with a linear model that allows
08:00 - us to predict
08:02 - price and in the process we can start
08:05 - thinking about what the relationship
08:06 - between
08:07 - price and some of these other variables
08:11 - is so
08:12 - type of apartment square feet beds baths
08:15 - um but also all this other stuff got
08:17 - like parking options
08:18 - laundry options comes furnished
08:22 - um so we got lots of lots of information
08:26 - and then we're going to see if we can
08:28 - figure out the relationship between all
08:29 - of those things in price and see if we
08:31 - can predict
08:31 - price accurately and maybe we would be
08:34 - doing this
08:34 - if we are renting an apartment ourselves
08:38 - or maybe if we are the owners of an
08:42 - apartment and we're trying to figure out
08:44 - like what is the
08:45 - the best or the appropriate market value
08:49 - for this apartment
08:51 - okay so um let's take a look at a couple
08:54 - of things
08:54 - if we're trying to build a linear model
08:57 - for price
08:58 - um are there any columns that
09:01 - you think we should get rid of alex so
09:04 - there's all this kind of like
09:06 - metadata information like the url
09:09 - the id even i don't know if we
09:11 - necessarily need that
09:13 - um i mean it looks like all of these are
09:16 - from reno
09:17 - tahoe so like region url it seems like
09:20 - it's already represented in the region
09:22 - column um so i would probably get rid of
09:24 - region url
09:26 - um that's all i would get rid of from
09:28 - what i'm seeing here there might be more
09:30 - things to the right if we keep looking
09:31 - but uh
09:32 - so let's actually we'll prac we'll print
09:34 - out the info so that we can see
09:36 - first of all how many rows and columns
09:39 - there are and see all the names
09:42 - um looks like so we've got
09:45 - around 385
09:48 - 000 rows so a fair number of rows and
09:51 - then 22 columns we saw that before
09:54 - um i agree it that it makes sense to get
09:58 - rid of
09:59 - some at least id and url
10:02 - um it looks like id for example
10:06 - is being read in as an integer so we're
10:09 - thinking of this as like
10:11 - a numerical value but like an id
10:14 - that is a higher number like this is a
10:16 - higher number than this
10:18 - i believe if i'm looking this correctly
10:20 - but
10:22 - no i have to go back further this this
10:26 - bottom one actually is a larger number
10:28 - than the one above it but
10:30 - um that doesn't really mean anything
10:33 - meaningful about this apartment with
10:35 - respect to this one doesn't mean
10:36 - like it has more of something it just
10:39 - means that
10:41 - it has a different id yeah this might
10:43 - even be a good example of something that
10:44 - are like not correlated
10:46 - at all and so would not make for a good
10:48 - uh good linear regression where if we
10:49 - mapped like price to id
10:51 - the id is just kind of like a randomly
10:53 - generated number presumably or
10:55 - um or at least that's what i'm assuming
10:56 - and so you wouldn't really see any
10:58 - relationship there
10:59 - yeah and then for url
11:02 - um i mean i don't want to do it because
11:05 - i don't want to like
11:06 - create a problem well we can do it on a
11:08 - smaller scale but
11:10 - um what would happen alex if we if we
11:12 - just like
11:13 - put url in it looks like it's being
11:16 - saved as a string
11:17 - right um if we added url to
11:21 - our linear model what do you think would
11:23 - happen i mean so i think that that would
11:25 - try to break it that would consider that
11:26 - like categorical variable right of like
11:29 - hey there are
11:30 - 700 categories and none of them share
11:34 - you know uh category one is this url
11:36 - category two is this url so it's just
11:37 - seven under categories and they're all
11:39 - different because none of them share
11:40 - the same url presumably yeah so
11:44 - let's actually do this really fast
11:45 - because i think this is kind of a funny
11:47 - exercise
11:48 - let's um let's start by just
11:52 - making a um a smaller subset of this
11:55 - housing data
11:55 - [Music]
11:57 - where we'll just take um
12:02 - i always forget the syntax here is it
12:04 - like
12:05 - if you just want the first couple of
12:07 - rows
12:09 - and then you do like i think it's like
12:11 - zero to zero colon five won't give you
12:13 - the first five i think
12:15 - yeah i think let's see
12:32 - cool um okay so let's do
12:36 - like let's do
12:41 - 50 rows and then let's fit a model
12:46 - i'm gonna actually like grab my
12:52 - i'm gonna grab some code from somewhere
12:54 - random
12:55 - um again because i never memorized these
12:59 - uh
13:02 - these things so i think uh
13:06 - breaking down uh breaking down this line
13:10 - of code real quick
13:11 - so we are using the sm module which if
13:14 - we look at our
13:14 - import statement is statsmodel.api
13:19 - and then within that we are using if you
13:21 - scroll back down to your line of code
13:23 - within that we are using the uh
13:27 - ols what does ols stand for do you know
13:29 - ordinary least squares
13:31 - okay so this is remember at the
13:34 - beginning we kind of talked about
13:36 - um ways of fitting linear regression
13:39 - model
13:40 - and the way that we've been using is
13:42 - called ordinary least squares because
13:43 - we're minimizing
13:46 - the squared distance between
13:49 - each point and the line essentially okay
13:52 - and then we're saying we're building it
13:54 - from a formula and that formula
13:56 - is what uh based on predicting price
13:59 - based on url
14:00 - yeah and i'm gonna use housing
14:03 - underscore sub
14:04 - and then i'm just gonna print model
14:07 - one dot params
14:10 - and so like how many parameters do you
14:13 - expect to see
14:14 - i expect to see what 50 parameters
14:18 - because it's uh
14:19 - yes again this is the example where
14:21 - they're using a url as a category
14:23 - and there's presumably 50 different urls
14:26 - and so
14:27 - yeah yeah i think there might be fifth
14:31 - well there's going to be 50 unique urls
14:36 - and then um minus one
14:39 - reference group right so 49
14:42 - plus a intercept so yeah you're exactly
14:45 - right there's gonna be 50 of these
14:47 - and we end up with so here's the
14:49 - intercept and then you see
14:50 - we have a slope for url true dot
14:54 - this whole thing url and then we have a
14:57 - different one for url
14:58 - true dot this whole thing um
15:02 - and then this it keeps going right so
15:05 - pretty good example if
15:06 - you just like take your data set without
15:07 - thinking about it you're going to
15:10 - um run into something like this where
15:13 - suddenly we have
15:14 - yeah 49 parameters based on these these
15:16 - random urls that's definitely not what
15:18 - we're
15:19 - what we're interested in doing yeah and
15:21 - this is actually one of the things that
15:23 - um so we can talk a little bit about
15:26 - this if we have time
15:28 - a little later today but i think um a
15:31 - lot of times
15:32 - people end up using other packages i
15:35 - think the most common one
15:36 - is scikit-learn to fit models
15:40 - and when you fit a model in scikit-learn
15:44 - you can get the you can get these like
15:48 - uh parameters but they don't come with
15:51 - labels
15:53 - automatically and it's like
15:57 - it's a library that's really built
15:59 - around like
16:00 - building the model and making
16:01 - predictions not necessarily like
16:03 - inspecting the model output and so i
16:06 - think that that one of the reasons i
16:08 - like stats models
16:10 - is that when if i were to print out like
16:12 - just the standard model summary here
16:15 - like it's gonna give me i'm gonna have
16:17 - to like
16:18 - see that something crazy happened here
16:22 - um and then that hopefully gives me a
16:25 - clue that i need to like
16:27 - be more purposeful about my model um and
16:30 - it's not always
16:31 - the diagnosis that that has to happen
16:34 - isn't always like
16:36 - it's not always really detailed and
16:38 - small sometimes it's like
16:40 - a big mistake like this where
16:43 - you just threw something into a model
16:45 - and it didn't make sense
16:46 - um but you weren't thinking about it
16:49 - necessarily before
16:52 - so yeah we have a question on the
16:53 - facebook page of how do we deal with
16:54 - those categorical variables
16:56 - um do you so one we
16:59 - we touched on this in much greater
17:01 - detail in an earlier session so take a
17:04 - look at our youtube page
17:05 - and if you look at this series you'll
17:07 - find you'll find videos about dealing
17:08 - with
17:09 - category categorical variables but do
17:11 - you have like a quick answer of like
17:12 - how would you deal with something like
17:14 - this well so you can definitely include
17:16 - categorical variables
17:17 - in your model um but you need to be a
17:21 - little bit careful about it because
17:23 - depending on how many possible values
17:25 - there are
17:26 - of a particular variable you could end
17:29 - up with
17:30 - a whole lot of parameters in your model
17:32 - which means you have to estimate
17:34 - a lot of things and you've now created
17:36 - something that's super complicated
17:38 - um that may or may not really be
17:40 - improving the model so
17:42 - in our last session we talked a little
17:43 - bit about like what's the
17:46 - trade-off between like accuracy of a
17:50 - model
17:50 - and or predictive ability of a model
17:53 - and like complexity complexity
17:57 - and i think depending on the problem
17:59 - there's maybe a different
18:00 - balance there's no right or wrong answer
18:03 - but
18:04 - i definitely think like
18:07 - there's a limit to how much complexity
18:09 - you potentially want in your model
18:12 - so we'll we'll actually we'll talk about
18:15 - that a little more today too
18:16 - yeah so i'm curious what and this is
18:18 - maybe a tangent but
18:20 - in looking at the url and then i was
18:22 - also curious about the column um
18:23 - description like we scroll over to see
18:26 - the description on some of these just to
18:27 - get a sense
18:28 - of like what those look like um
18:32 - what would you think of doing something
18:34 - like
18:35 - creating a new column that's like you
18:37 - know is the word
18:39 - um stunning in either the url or the
18:42 - description right
18:43 - could we do some like natural language
18:45 - processing of taking these
18:47 - taking this information that's in you
18:50 - know
18:50 - paragraph form or natural language form
18:53 - and trying to create new categories
18:55 - um like that or like you know definitely
18:58 - imagine
18:58 - you know washer washer dryer is already
19:01 - a column in here but let's say that
19:02 - column didn't exist
19:04 - could we look in the description and try
19:05 - to like create our own variable of like
19:07 - oh
19:07 - the description mentions a washer dryer
19:10 - totally
19:11 - yeah so that's a really good point um a
19:14 - lot of times
19:15 - in your data pre-processing stage
19:18 - you might like right now i think for
19:21 - time's sake we'll probably just get rid
19:22 - of this description column
19:24 - although we could we could probably
19:26 - relatively quickly write some code to
19:28 - pull
19:29 - something out from here um but
19:33 - but yeah for for sure there are
19:35 - definitely
19:37 - places where you can take data that
19:39 - exists in a format that
19:41 - you're not going to use like you're not
19:42 - going to throw description into your
19:44 - linear model
19:45 - and turn it into usable information
19:48 - like i like your example of if the word
19:51 - stunning is in here like maybe
19:54 - or i'm trying to think of another yeah i
19:56 - was looking i was thinking that when i
19:58 - was looking at the
19:58 - urls if you scroll down to uh or maybe
20:01 - you got rid of it at this point but um
20:03 - if you if you look at those urls
20:07 - um which were showing up in the when you
20:10 - were showing all of them
20:12 - when you were showing all of the uh
20:14 - parameters
20:15 - but the urls are like you know
20:18 - craigslist.org stunning dash bedroom
20:22 - like three dash bedroom uh yeah so i was
20:24 - thinking oh like there's information in
20:25 - those urls that even though we don't
20:27 - want to use the urls themselves as
20:28 - categorical things we could like maybe
20:30 - parse those urls to get um
20:32 - some information out of them yeah or
20:33 - maybe like ocean
20:35 - view or something like that yeah they'll
20:37 - want
20:38 - oh yeah interesting comment in the
20:39 - youtube chat from alex of uh
20:41 - you could just get the length of the
20:42 - description as a comp uh as a column
20:45 - um yeah what if i do len
20:48 - description is that going to give me the
20:50 - number of characters yeah i think so
20:52 - assuming it's just a string
20:53 - let's uh let's actually we can try that
20:56 - um let's before we do that though let's
20:59 - go back
21:00 - and let's actually like pull out some of
21:02 - these things so
21:04 - um i i feel like we can safely say the
21:06 - id column is probably not giving us any
21:08 - information
21:10 - towards uh price i'm gonna get rid of
21:13 - url as well because i figure
21:15 - anything that's in the description
21:19 - is probably in the url also i'll get rid
21:22 - of region url like you said because that
21:24 - information is also contained in this
21:26 - region variable
21:28 - um image url uh number 17 there probably
21:32 - isn't super helpful
21:33 - okay yeah um and then
21:38 - can leave we can leave lat and lawn i
21:40 - assume those are
21:41 - latitude and longitude which might be
21:43 - interesting
21:44 - so let's go ahead and start with that
21:48 - um so actually
21:52 - let's think should i use i think there's
21:55 - like a dot
21:56 - drop that allows us to easily get rid of
22:00 - columns
22:01 - or we can use subsetting but let's uh
22:05 - let's try our let's see if i can find
22:07 - this
22:08 - drop columns
22:13 - um i'm gonna just show you guys my
22:15 - googling
22:16 - as well because i feel like this is
22:20 - a useful skill indeed
22:24 - um okay so this is
22:27 - the the labels parameter gives the index
22:30 - or column labels to drop
22:32 - and it looks like i can just input like
22:35 - a list of
22:36 - labels that i want to drop so
22:39 - i'm going to go ahead and grab those so
22:42 - we've got
22:42 - id url region url
22:55 - i wonder if it doesn't in place or if
22:56 - we'll have to do housing
23:00 - you are probably right let's check and
23:03 - then was there anything else
23:04 - image url maybe
23:08 - okay was it
23:12 - img uh full image
23:16 - girl okay let's see this might just give
23:20 - us
23:25 - uh maybe it takes index of uh 0
23:28 - 1 2 instead of the column names
23:34 - i'm gonna try labels
23:37 - equals this
23:42 - and then we might need to give it an
23:48 - axis
23:50 - okay from the
23:53 - index or the column so let's give it
23:56 - axis equals one because we want to drop
23:59 - the column
24:08 - okay and then this just gave us
24:11 - this exact same data set but with those
24:14 - columns dropped
24:16 - and so now if we want to resave it we
24:19 - either need to add
24:20 - in place equals true or housing equals
24:25 - so i'll do it this way
24:29 - and then let's just see so if i take
24:33 - len of housing dot description
24:38 - is that gonna just give me a bunch of
24:39 - numbers that give me
24:42 - so uh
24:46 - interesting i bet that's like the sum
24:48 - total of
24:49 - the description i'm guessing you want to
24:51 - call them right
24:52 - right um you might need to do like an
24:56 - apply
24:57 - of like um
25:01 - or housing of description of the column
25:07 - uh let's see
25:11 - right so let's just get the column right
25:12 - housing.description of
25:14 - um i don't know that that is what uh
25:17 - that is
25:17 - that is what yeah so um
25:21 - let's try apply function
25:25 - to a column in pandas um
25:29 - i know that's the dot apply but i just
25:31 - want to pull up the
25:33 - pull this up so we can see it so um
25:36 - this allows us to apply the same
25:39 - function
25:40 - to every value in a column
25:44 - um so axis along which the function is
25:47 - apply
25:49 - zero would apply it to each column or
25:52 - sorry one apply each function apply the
25:56 - function to each row
25:57 - so if we want to apply to each row we
25:59 - would set that equal to one
26:01 - so let's try this as well
26:04 - um let's do like housing
26:10 - dot and then i see on youtube someone's
26:14 - uh pointing us to dot shape
26:16 - as well which uh which might be a uh
26:20 - a good way to do that i'll look into
26:21 - that as you uh as you work on the fly
26:24 - okay um well
26:28 - let's see this is gonna apply it to
26:31 - every column um so i wonder if i can
26:35 - just
26:36 - i wonder if i can just pull out like
26:38 - housing dot
26:40 - um description
26:43 - dot apply and then do
26:46 - like and give it wayne
26:50 - that's the function one
26:56 - do i do i would just give it len and
26:58 - then
26:59 - like then we have to figure out the axis
27:02 - stuff but
27:11 - one takes no keyword arguments um
27:16 - interesting let's see if i do like
27:19 - len of hello
27:22 - i am sophie
27:27 - it does give me the number of characters
27:36 - let's see
27:43 - so if we if we get just the column uh
27:47 - so if you do um
27:50 - if you do data frame or so if you do
27:52 - housing dot description
27:54 - dot s dot len
27:57 - dot s dot one dot string sorry dot s t
28:01 - dot str.len
28:05 - len and get rid of yeah
28:09 - and then parentheses after plan as well
28:12 - what does the str do oh wow i believe it
28:15 - was saying
28:16 - treat it as a string we can try to get
28:18 - rid of it you can try to get rid of
28:20 - str um and see what happens i think we
28:23 - had oh we had applied
28:28 - interesting okay so if we apply len to
28:32 - the entire
28:34 - into the entire series then
28:38 - there is no there's no way to calculate
28:41 - the length of
28:42 - a series object so we need to turn the
28:44 - series into a string or something
28:46 - a set of strings yeah
28:50 - super interesting was that was that your
28:53 - own googling or is that
28:54 - uh yeah so i google panda get length of
28:57 - string in column and it was the first
28:59 - result
29:00 - nice okay well i wish you guys could
29:03 - also see alex's screen
29:05 - but this is fun okay so this gives us
29:07 - the
29:08 - the length of the description let's um
29:11 - let's add this
29:12 - into our into our
29:16 - initial data set so we'll say like
29:18 - housing
29:19 - dot um description
29:22 - length equals it's gonna give me a
29:28 - warning but it's gonna work
29:32 - um let's go ahead and take a look at
29:36 - housing.head again
29:40 - yeah probably good to like verify that
29:41 - this is doing what we expect
29:43 - where um description and description
29:46 - link is probably all the way at the very
29:48 - end
29:48 - you didn't see it actually oh did you
29:51 - reset housing equals
29:59 - let's try this again
30:01 - [Music]
30:02 - but i think
30:12 - oh there it goes there it is um okay
30:16 - so now we've got a description length
30:17 - the number of characters
30:19 - um and then i'm gonna now drop
30:23 - description i don't think we're gonna
30:25 - use anything else but
30:26 - again like alex said we could we could
30:28 - do something else here we could
30:30 - figure out um we could pull out some
30:32 - word that we think is gonna
30:34 - be um associated with
30:37 - particularly um high like
30:40 - high prices like exceptional or
30:44 - special maybe like places that have us
30:48 - some sort of one free month special or
30:50 - more likely
30:51 - yeah we could even look for like free
30:53 - month in there because i think that
30:54 - that's
30:54 - that's not a data that's represented uh
30:57 - in any of
30:58 - our other columns like oh this comes
31:00 - with a free month well that like changes
31:01 - the
31:02 - you know real price of the apartment
31:04 - exactly so we could definitely
31:06 - pull some things out of here and and
31:08 - create some more columns around
31:09 - description
31:10 - but i'll leave that for now um and then
31:14 - i'll just grab this
31:17 - drop again so you can drop description
31:29 - okay and then let's take a look at it
31:32 - again
31:36 - okay i feel like we're getting closer
31:40 - um do you have any other questions
31:42 - before we try to fit a model
31:43 - like anything else you think should one
31:45 - thing i would want to do is
31:47 - because we kind of like made this
31:48 - description length thing on the fly
31:51 - i would kind of want to verify that that
31:52 - is indeed an integer rather than a
31:54 - string because like who knows
31:56 - who knows what data type uh you know
31:59 - these random functions that we were just
32:00 - googling it was throwing out so i
32:02 - probably want to look at the info again
32:03 - just to make sure that we
32:05 - know for that uh description length cool
32:08 - it's a it's a float
32:10 - that looks good um the other thing
32:14 - i mean one thing that i might look at
32:17 - before we get started is um first of all
32:20 - for some of these categorical variables
32:24 - like parking options
32:26 - and region
32:29 - i'm i'm curious how many different
32:32 - values there are
32:34 - because if there's like a thousand
32:37 - different values we're still gonna
32:39 - end up with like a lot of parameters in
32:42 - this model
32:43 - right um so one thing i might do is just
32:46 - like we can even do
32:49 - um
32:54 - is it describe
33:04 - give us a little bit more or allow us to
33:08 - um like this so in the region
33:11 - there's 404 different categories
33:15 - right so we probably don't want to
33:19 - throw that into the model willy-nilly
33:21 - because we're going to end up with 403
33:25 - slopes um on region if we put that in it
33:28 - also looks like
33:31 - oh well actually that's an and for a
33:33 - reason
33:34 - there's 12 different types of apartments
33:37 - that's a little more reasonable
33:39 - um how would you go about
33:43 - um
33:46 - states yeah um yeah that's interesting
33:50 - we could look into that maybe you know
33:52 - puerto rico or something is in there
33:54 - um how would you go about kind of
33:57 - verifying that
33:58 - all of these that there's like no errors
34:01 - in the data
34:02 - of like oh maybe laundry options maybe
34:04 - one of the five is like
34:06 - capitalized differently than the others
34:08 - or like
34:10 - you i mean i would go through if
34:13 - if we were spending more time on this
34:15 - and i want to make sure we got some time
34:17 - for modeling but
34:19 - if i was spending more time on this i
34:20 - would definitely go in
34:22 - much more detail looking at each of
34:24 - these like so for example
34:27 - um let's look at type
34:30 - for a moment because this has kind of a
34:32 - lot of categories
34:34 - to just throw it into the model but not
34:36 - so many that we
34:38 - just want to like definitely throw it
34:41 - out
34:42 - um but so one thing i might do is first
34:45 - of all
34:46 - take a look at housing
34:50 - dot type dot value counts
34:55 - and so this gives me all of the
35:00 - values in this column so we've got
35:02 - apartment house townhouse condo
35:04 - duplex manufactured
35:08 - goes on so it looks like these are all
35:12 - at least their own thing right
35:15 - um we definitely have a couple
35:16 - categories like assisted living
35:18 - and land that don't seem very highly
35:22 - represented in the data
35:23 - and in fact like even even in law
35:26 - and flat and loft and cottage cabin
35:31 - are like oh there are a lot fewer
35:34 - of those um than all of the other ones
35:38 - so like one thing i might do is this is
35:41 - a lot of extra parameters for like not
35:43 - that much variation in the data
35:46 - i might collapse this into one category
35:48 - that's like
35:49 - other interesting um to just
35:52 - kind of cut this down a little bit the
35:55 - other thing i might do
35:57 - is take a look at a box plot of all
36:00 - these things
36:01 - and price so i might do like sns.box
36:04 - plot
36:07 - x equals type
36:11 - y equals price
36:14 - data equals housing
36:17 - [Music]
36:20 - and then
36:23 - show the plot
36:29 - okay so it's like there's one massive
36:33 - outlier this is actually okay
36:36 - let's let's investigate this further so
36:38 - like
36:39 - if i it might not even be one outlier it
36:42 - might be like
36:43 - a bunch and you're going to change
36:46 - somewhere so one thing i might also look
36:49 - at here is
36:52 - um
36:54 - guess i'll use
36:57 - i always forget if we're at his plot or
36:59 - his plot
37:01 - um seabourn keeps changing
37:05 - it's uh or disc plot like
37:09 - um and then do uh
37:13 - housing.price
37:21 - [Music]
37:29 - okay so we've got this like very very
37:31 - skewed um
37:32 - distribution here where we go from zero
37:36 - all the way up to like a million or more
37:39 - than
37:40 - it's the one with nine
37:43 - yeah so yeah 2.5 with nine zeros
37:48 - um so i think
37:51 - we wanna probably restrict this a little
37:54 - bit
37:55 - let's for the moment
37:58 - i'm gonna just kind of arbitrarily cut
38:00 - this down
38:01 - um what do you think is a good like
38:04 - cut-off point
38:05 - so we're gonna be removing that rose
38:07 - where price is over
38:09 - a million yeah or like
38:15 - so how would you go about deciding this
38:17 - in like the real world of
38:18 - like when can you just throw out data
38:21 - like this
38:22 - so um i guess
38:25 - there's again no clear answer um
38:29 - i'm sorry to say but i think
38:32 - in practice you kind of play around with
38:34 - this for a little while so
38:37 - let's like for example let's just start
38:40 - without making any
38:42 - permanent changes let's actually
38:45 - we can do this in here let's subset this
38:50 - right like within the plot and say we
38:53 - only want
38:56 - um values where price is less than
39:01 - 1 million um and then
39:04 - let's like replot this
39:07 - and it still just looks like super
39:09 - skewed like we still really can't
39:12 - see this variation um
39:15 - one thing we could do at this point is
39:16 - we could try we could even try
39:18 - like taking a log and see if like
39:21 - the log transform helps us get some
39:24 - things like
39:26 - oh and then here we're we're gonna have
39:30 - an issue with taking the log of anything
39:33 - um that's zero so that gives us a clue
39:35 - that there's actually some apartments
39:36 - where the price is
39:37 - zero so um now i'm gonna say like
39:42 - and housing.price
39:47 - greater than zero and i think i need to
39:49 - put these in
39:54 - parentheses
39:58 - and now okay so now we've got like
40:02 - a little bit more usable of a
40:05 - distribution
40:07 - um so this might be a clue that we want
40:10 - a log transform
40:11 - for our price variable
40:14 - um because i threw this log in here but
40:16 - we could also
40:18 - could we go in the other direction try
40:19 - like super
40:21 - like over cutting things and just say
40:22 - like hey what does it look like if we
40:24 - cut everything that's less than um you
40:27 - know
40:27 - two thousand or everything that's
40:29 - greater than two thousand um
40:31 - dollars and see yeah let's do like
40:34 - three thousand i feel like because these
40:35 - might be rental prices
40:38 - and this is yeah so that looks pretty
40:39 - good this is another thing that we could
40:41 - do instead of instead of just taking
40:43 - log we could just cut this down and say
40:46 - okay like
40:47 - realistically we really only care about
40:50 - this
40:51 - um when we're
40:54 - looking at apartments that are three
40:56 - thousand
40:57 - price range within our lowly price range
41:01 - um yeah and then i see somebody wrote in
41:05 - the chat can we see
41:06 - performance before and after including
41:08 - the outlier
41:09 - and definitely so um this comes back to
41:11 - a question that somebody asked on
41:13 - discord
41:14 - a couple weeks ago and i thought it was
41:16 - a great question which is
41:18 - like how do you compare models and i
41:21 - think
41:23 - totally if you have a data set where
41:25 - it's not
41:26 - super costly to run or time consuming to
41:28 - run the model
41:29 - you can definitely like run the same
41:32 - model
41:33 - with a bunch of different iterations
41:35 - getting rid of um
41:37 - getting rid of an outline liar or
41:40 - getting
41:41 - um or like taking a log transform not
41:43 - taking a log transform
41:45 - you can try all those things and refit
41:47 - the model it gets a little more
41:48 - difficult if you have like a huge data
41:50 - set
41:51 - then you might need to like take a
41:53 - subset of data to do
41:54 - that exploration um but yeah
41:57 - no no need to ignore your comment
42:00 - because it looked like it definitely
42:02 - looked like an one outlier in that plot
42:04 - that we saw
42:05 - um i think it was just like a
42:09 - weird artifact of the skew um
42:12 - but it definitely looked like an outlier
42:15 - um in that first plot
42:17 - so i would say based on looking at this
42:20 - um i feel like 3000 seems like a pretty
42:25 - good cutoff point i mean we could like
42:27 - look at this tail to ten thousand or
42:31 - five
42:31 - to like ten thousand and see if
42:34 - it looks like yeah
42:37 - i mean it kind of looks like to me this
42:40 - the cutoff is maybe like
42:42 - three or four thousand so
42:45 - my very um
42:49 - precise uh
42:51 - [Music]
42:53 - method for this i'm gonna cut it to 4
42:55 - 000
42:56 - or smaller okay
42:59 - so let's let's actually do that we'll do
43:02 - housing
43:03 - dot housing equals housing
43:07 - housing price uh actually
43:11 - totally override it uh
43:14 - i like housing
43:19 - affordable um
43:24 - okay and that kind of that kind of
43:27 - distinction is like if we
43:29 - saved over housing then it would be a
43:30 - little bit trickier to say oh let's like
43:32 - plug in
43:33 - the let's plug in the model before we
43:36 - we cut out the super expensive um houses
43:39 - it'd be a little bit trickier to do that
43:40 - or we'd have to like reload the data set
43:42 - and uh yeah for sure okay
43:47 - let's do that um okay so now let's just
43:51 - take a look at this
43:52 - again
43:58 - all right so we've got region price
44:01 - and then i'm reminding myself as well we
44:05 - looked at this before we saw
44:07 - region is a little bit overwhelming of a
44:11 - variable right here
44:12 - um maybe like again i
44:15 - assume that the region is giving us like
44:18 - specific cities basically um
44:22 - maybe there's a way that we can
44:25 - do some like combining of categories
44:27 - again where we just say like
44:30 - north west yeah southwest right maybe we
44:33 - could
44:35 - oh market in the south and new york is
44:38 - in the northeast
44:38 - yeah um so there's there's lots more
44:42 - i guess it's gonna feel a little
44:44 - unsatisfying because there's a lot more
44:46 - cleaning of this data set and
44:48 - producing like not just cleaning but
44:50 - creating
44:51 - of new features so like feature
44:54 - engineering
44:55 - um that we could do before fitting a
44:57 - model that would probably improve this
44:59 - model
45:00 - because to be fair like the relationship
45:02 - between
45:03 - price and square feet is probably
45:07 - different by region right like
45:10 - the prices in jacksonville are going to
45:12 - be different than the prices in new york
45:14 - city and
45:15 - the relationships between like price and
45:18 - other variables might also be different
45:20 - in different regions so i feel like
45:22 - region is probably important in some
45:25 - sense but like
45:27 - yes i mean so ultimately we could do the
45:29 - same thing of like depending on
45:30 - you know depending on the real-world
45:32 - application of this if we're renting an
45:34 - apartment in
45:35 - denver we don't really care what rent is
45:38 - like in new york and so we could take
45:40 - this data set and just like we cut out
45:41 - all the super expensive
45:42 - apartments we could cut out all the
45:44 - apartments that are not in denver
45:46 - um that's also true yes um but what if
45:49 - we have
45:49 - apartments that we want to rent out in
45:51 - like every major city in the u.s
45:53 - yeah that's a good thing what do we do i
45:55 - don't know
45:58 - um okay so let's let's see
46:01 - what should we include in our first
46:03 - model should we include
46:06 - i mean i think the most obvious things
46:08 - are like square footage bedrooms
46:10 - yeah okay let's um let's
46:14 - let's get a a model going let me
46:17 - grab and grab the same
46:26 - code from up above
46:31 - and i'm gonna go ahead and
46:35 - start adding we're gonna do this with
46:38 - housing
46:40 - affordable and let's start grabbing
46:43 - something so
46:43 - we want should we include we'll
46:47 - include type for now um
46:50 - square feet
47:04 - and remind me of like those are
47:06 - certainly
47:07 - should be treated like a category right
47:11 - yes yes no but it looks like right now
47:14 - they're treated
47:14 - as like zero one um so
47:18 - remember that here let's actually print
47:21 - out
47:22 - the info again so
47:26 - all of these are being stored it looks
47:28 - like as integers so it's just zero
47:31 - one um and
47:35 - remember that when we when we fit
47:38 - this model with say like uh type
47:42 - what's actually happening under the hood
47:44 - is we're getting
47:46 - a new um like a new
47:50 - design matrix here that has
47:54 - 11 new columns that are ones and zeros
47:58 - um i can actually like
48:01 - print that out let's see i think it's
48:04 - like
48:09 - patsy dot
48:16 - you know what i'm gonna do i'm gonna go
48:18 - back to
48:21 - i think it was
48:25 - here
48:30 - i may have done this
48:37 - so let me try one more place
48:57 - yeah okay so
49:01 - this is a different
49:10 - different data set but let's just for
49:14 - a moment do you like rent or it was
49:17 - price
49:19 - as a function of type and this is
49:23 - um housing affordable
49:27 - and return it as a data frame and so
49:35 - yeah so this is the new
49:38 - x matrix that we get and so it'll have
49:42 - like this one for an intercept
49:44 - column of one for the intercept and then
49:46 - it'll have like type
49:47 - t assisted living and this will be ones
49:49 - and zeros type t
49:51 - condo and this will be ones and zeros
49:53 - and so it
49:54 - basically just creates a bunch of dummy
49:55 - variables with ones and zeros
49:57 - for all of the um
50:01 - for all of the possible values of that
50:03 - categorical variable
50:05 - so back up here
50:08 - this column cataloud is basically
50:12 - the same is being treated in the same
50:14 - way as this type
50:16 - column it's just we're not seeing it
50:18 - when we use this os from formula we're
50:20 - we're just putting in the string but
50:22 - in order to fit the model it's
50:24 - separating that column
50:25 - into like 11 new columns
50:29 - got it but if we
50:32 - if we had three so that that's working
50:36 - because it's binary if it's either cat's
50:37 - lab or not
50:38 - but if this were if there was a third
50:41 - category for that we wouldn't want it to
50:43 - be numerical zero one two
50:45 - we would want it to be you know string
50:48 - zero one two which would then be treated
50:50 - as a category
50:51 - or or the you know the string the actual
50:54 - names of those categories whatever they
50:56 - might be yeah exactly
50:58 - um that's a good point so again if we
51:01 - were
51:02 - being more thorough here we'd probably
51:04 - want to verify
51:05 - that the only values are zeros and ones
51:08 - um
51:09 - i'm pretty sure i've taken a look at
51:11 - this before and and verified that
51:14 - um but definitely would want to it also
51:17 - seems like these
51:18 - all of the columns where it is like a
51:20 - binary thing like cast can be allowed or
51:22 - not
51:23 - it seems like those are ones and zeros
51:25 - and anything
51:26 - where the type like type where there's
51:30 - more than
51:30 - two options it seems to be recorded as
51:33 - strings but
51:34 - that's an assumption that i'm making
51:36 - that's not like something that we have
51:38 - checked
51:39 - right now um okay
51:43 - let's go back
51:46 - so we were here
51:50 - so we had
51:54 - up to smoking aloud in the model
51:58 - let's do wheelchair
52:03 - access i'm just adding everything i
52:05 - don't know
52:09 - there might be a shortcut to add
52:11 - everything
52:12 - and then take some things out
52:17 - you know an r there is but
52:20 - um and then let's do
52:22 - [Music]
52:23 - what do you think we'll add description
52:26 - we'll add latin law
52:28 - let's add description length to start
52:30 - and then we'll we'll come back and
52:32 - add some more things um
52:35 - and then the data is housing underscore
52:37 - affordable and we're going to fit this
52:39 - model
52:40 - and then let's print out the summary
52:42 - it's going to be
52:44 - big but
52:48 - it might take a minute okay
52:53 - so and i'm actually gonna do
52:55 - [Music]
52:58 - zoom out so that we can see all of this
53:00 - this is one thing i don't like about the
53:02 - summary output i
53:03 - would love if they would output it for
53:06 - me as a data frame
53:08 - um directly but instead we get this like
53:12 - crazy thing that is subject to whatever
53:15 - weird formatting
53:17 - um jupiter notebooks decides to use
53:20 - um okay so we can actually see all a lot
53:23 - of the things we've already talked about
53:25 - we can see
53:26 - that the r squared is 0.129
53:30 - and adjusted r squared is also 0.129 so
53:34 - i would interpret this as like this
53:36 - model is
53:37 - not super good at identify
53:41 - or at predicting price because we're
53:43 - really only
53:44 - um really only explaining about like 13
53:48 - of the variation in price for all of
53:51 - these apartments
53:52 - um but we could use these numbers to
53:55 - compare
53:56 - this to a new model so let's see
54:00 - what happens and you'll notice like we
54:02 - do have
54:03 - at least the type variable like we said
54:06 - we've got like 11
54:07 - different um values here right
54:13 - let's add in let's add in lat and lawn
54:16 - and just see
54:18 - if that improves it at all because right
54:21 - now we have nothing
54:23 - cutting down the region at all so maybe
54:26 - maybe that would be useful oh yeah live
54:29 - long is actually a great way to
54:30 - get the region uh
54:35 - but that's not going to be like linear
54:36 - right yeah it's hard to
54:39 - because it's not it's not like oh higher
54:41 - latitude is going to be more expensive
54:43 - yeah maybe we need like
54:47 - well okay let's add it in
54:54 - okay well it didn't prove it i feel like
54:58 - i mean for such a small number like this
55:01 - is only
55:03 - 0.129 so we're already up to like 15
55:06 - percent
55:07 - so that's not terrible um i bet that
55:10 - that
55:11 - if we ran like a um anova comparing
55:14 - these two models i bet the
55:16 - bigger model would like that added
55:19 - complexity would
55:20 - probably be worth it um
55:23 - other ideas or questions uh question
55:25 - from the youtube chat how do we
55:26 - calculate feature importance
55:28 - oh um that is a very loaded question
55:33 - um so one thing that we can do
55:36 - is if there's something called like uh
55:42 - what are they called standardized
55:44 - coefficients um
55:46 - another another thing that we can do
55:48 - that's slightly different but related is
55:50 - we
55:50 - if we standardize all of the
55:54 - um all the quantitative variables
55:58 - then they're all on the same scale then
56:00 - they're
56:01 - then the um coefficients at least are
56:05 - all on the same scale
56:06 - so they are comparable so we could
56:08 - actually if
56:09 - everything was standardized we could
56:11 - compare coefficients at least for the
56:14 - quantitative variables
56:16 - um another thing that
56:20 - people do and i guess this is really a
56:23 - plug for
56:24 - the feature engineering um content
56:26 - that's coming out
56:27 - hopefully in a couple of months um that
56:30 - nitia is working on
56:31 - because there there are a lot of other
56:34 - mechanisms that we can use to try to
56:36 - like
56:36 - pick out features based off of
56:40 - their relative importance in terms of
56:44 - this model um or there's also ways that
56:46 - we can
56:48 - iteratively try to build this model for
56:50 - example like
56:51 - we can use like forward and or backward
56:54 - selection
56:55 - to kind of tell the computer okay like
56:58 - start with
56:58 - a model that has everything in it and
57:02 - start deleting things and testing them
57:05 - out and then
57:06 - if something helps to delete it
57:09 - then keep it missing and then like
57:12 - continue the process from there or
57:13 - the reverse we could start with nothing
57:15 - and try like adding
57:17 - predictors one at a time and then
57:20 - see which one and do it which one
57:22 - improves the model the most and then
57:24 - only keep that one and then
57:25 - iteratively do that again um we can also
57:28 - use like
57:31 - uh like ridge or lasso regression
57:35 - to try to shrink some of the
57:37 - coefficients to zero if
57:39 - they're or close to zero if they're not
57:42 - super
57:42 - relevant um
57:44 - [Music]
57:46 - i see a question i wonder what why stats
57:48 - models doesn't include in summary
57:50 - metrics such as mean
57:51 - absolute error means squared error root
57:55 - mean squared error
57:58 - is r squared better that's a interesting
58:00 - question
58:02 - um so
58:09 - r squared is related to all of those
58:11 - things um
58:13 - because it is also based off of the
58:16 - um it included in that r squared
58:19 - calculation you're calculating like
58:22 - the error as part of it and seeing
58:26 - basically saying like how much of that
58:28 - error
58:29 - are you or how much of the variation
58:33 - are you accounting for by adding that
58:35 - line in
58:36 - um but
58:39 - i don't know i don't know why they chose
58:41 - not to include
58:43 - additional metrics um
58:46 - it's always interesting um
58:50 - okay we are like running quickly out of
58:53 - time and i know we haven't gotten super
58:54 - far
58:55 - but at least we've had a chance to test
58:57 - out some models and hopefully we've
58:59 - gotten
59:00 - you've all gotten a taste of what this
59:02 - might look like on your own
59:04 - um i think the fun of today
59:07 - is kind of we didn't have a clear
59:10 - direction of where we were gonna go with
59:11 - this today um but
59:13 - it was i think i hope useful to kind of
59:16 - see
59:17 - how one might get started on an analysis
59:19 - like this
59:20 - and my hope is that if you're interested
59:23 - in learning more about linear regression
59:25 - and if you're interested in practicing
59:27 - these skills a little bit more
59:28 - that you might take this and run with it
59:31 - and like try running your own model see
59:33 - if you can
59:34 - improve this even more another thing i
59:37 - would probably do
59:38 - is start subsetting the data to
59:41 - one region and see if that allows me to
59:44 - get
59:45 - at least a better a little bit better
59:49 - um yeah but i think this kind of work in
59:51 - general just will make you more
59:52 - comfortable
59:53 - using all these tools make you more
59:54 - comfortable with like kind of solving
59:56 - arbitrary tasks like we were doing like
59:58 - how do i find the length of
60:00 - the string in this column just like the
60:02 - more and more work that you
60:03 - do working with data frames working with
60:07 - um these libraries that are making these
60:08 - models i think that
60:10 - that will just help grow your skill in
60:12 - like being able to
60:13 - or being more confident in doing these
60:15 - with when
60:16 - you need to do it in a real world
60:18 - situation yeah
60:19 - and another thing that i will say is
60:22 - once you have learned
60:23 - more methods for creating models then
60:26 - you can even like you could extend this
60:29 - even further this doesn't have to be
60:30 - just a linear regression problem this
60:32 - can be like a
60:33 - how well can i predict price problem and
60:36 - you can try even more methods
60:38 - and use some of the methods that we
60:40 - discussed here to still compare
60:43 - compare models and also some of these
60:45 - metrics that
60:47 - alex in the chat just mentioned so like
60:50 - mean absolute error is um
60:53 - is a useful one to compare different
60:56 - types of models because
60:58 - you know like you don't want to
61:01 - you don't necessarily have a way of
61:03 - calculating like aic
61:05 - for um for like every model that you
61:09 - might
61:10 - every type of model that you might
61:11 - create but mean absolute error you
61:13 - definitely could
61:16 - cool well that's it that's it for linear
61:20 - regression
61:21 - um i hope that you guys found this
61:23 - helpful and definitely let us know
61:25 - if you have any more questions we'll try
61:27 - our best to answer them
61:30 - and feel free to write those questions
61:33 - as comments
61:34 - in the youtube video or also on discord
61:37 - if you have been on discord
61:39 - yeah uh we don't have any other live
61:42 - streams currently planned but hopefully
61:43 - we will do another
61:44 - kind of series like this um in the near
61:46 - future but
61:47 - yeah might uh might take a little a
61:49 - couple weeks break
61:50 - um so we'll see what happens but uh yeah
61:54 - keep an eye on the youtube channel for
61:55 - more stuff like this and
61:57 - yeah thanks for thanks for doing this
61:58 - sophie so he led the you know eight
62:00 - straight weeks of this
62:02 - um great work sophie it's all fun thanks
62:04 - for joining me on all of these alex
62:06 - this is a wild ride all right cool all
62:09 - right