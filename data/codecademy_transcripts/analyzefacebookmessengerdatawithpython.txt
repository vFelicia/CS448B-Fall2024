00:04 - hey everyone we're just getting started
00:07 - here live from the codecademy studio
00:10 - sorry for a little bit of the delay my
00:14 - name is Ian freed a curriculum developer
00:16 - here click add me we're just going to
00:19 - make sure that everything is looking
00:21 - good here in the studio with the live
00:25 - stream and yeah if you guys are able to
00:29 - hear well able to see me or run into any
00:32 - issues feel free to use the chat that we
00:35 - have going on the live stream awesome so
00:42 - we'll just give a little more time to
00:44 - make sure that everything is looking
00:46 - good
00:47 - and we're gonna go ahead and get started
00:48 - thank you all for joining us today we're
00:51 - really excited here to walk you through
00:53 - how to use a lot of the data that
00:56 - Facebook is collecting on all of us to
00:59 - get some interesting insight so I think
01:01 - it'll be a great session I've learned a
01:03 - lot about my past
01:05 - facebook Messenger history my relations
01:08 - with my friends it's been a journey and
01:10 - exciting and I hope all of you guys will
01:13 - get to do the same okay I think
01:19 - everything is sounding good everything
01:21 - is working well so let's go ahead and
01:25 - get started and for anyone who's just
01:27 - joining us once again my name is Ian
01:29 - freed I'm a curriculum developer here at
01:31 - code Academy and I work on our data
01:33 - science path team and we created a lot
01:37 - of the content that you'll see on code
01:38 - Academy related to data analysis and
01:40 - machine learning and I've been interest
01:44 - in this area for a little while and I
01:46 - wanted to see what I could do with the
01:49 - data that Facebook is collecting on US
01:52 - news a label it to all the data that it
01:56 - has
01:57 - and I knew it was collecting all this
01:59 - data so I wanted to go ahead and see
02:00 - what I could get from doubling that data
02:02 - I'm doing an analysis and so today well
02:05 - will show you how to download that
02:07 - Facebook data for yourself and then go
02:10 - ahead and do an analysis on your chat
02:12 - history in Jupiter notebooks which is a
02:15 - great data science tool for evaluating
02:18 - Culloden and displaying graphics and
02:22 - you'll learn some stuff about your
02:24 - Facebook friends so to go ahead and get
02:28 - started I'm gonna move over to Facebook
02:31 - and talk to you a little bit about how
02:34 - we can go ahead and download your data
02:37 - at any time throughout the chat feel
02:39 - free to comment you know any issues
02:43 - you're running into any questions that
02:44 - you have I want to try and make this
02:46 - conversation between me and you all here
02:48 - not just making kind of me talking at
02:51 - you so definitely let me know any
02:54 - questions that arise so I'm here on my
02:58 - facebook settings which you can access
03:01 - by going to your Facebook I'm just
03:03 - clicking the little arrow on the top
03:04 - right and then going down to the
03:07 - settings option I'm gonna bring you here
03:11 - and we see this banner right here along
03:13 - the top really alerting you to how to
03:16 - download your Facebook information
03:17 - Facebook wants you to know that it has
03:19 - it and that you can access it so you can
03:21 - get a look for yourself
03:22 - so hopefully you might have done this
03:25 - beforehand I did we send out some
03:26 - instructions and some of you others
03:28 - might be joining for the first time so
03:30 - feel free to follow me right now and you
03:32 - can go ahead and get that download
03:33 - started on your data so we'll click your
03:36 - Facebook information right here and then
03:39 - we can go down to this other section
03:40 - right here that says download your
03:42 - information and well click download a
03:48 - copy
03:53 - let it load for a second and Facebook
03:59 - will take you to this page right here
04:02 - and what this page is showing you is all
04:06 - the information that Facebook is
04:07 - collecting so as you can see it's a lot
04:11 - of information it might be shocking to
04:14 - you it might not be shocking to you it
04:16 - was a little bit shocking to me though
04:17 - they have everything from your posts
04:19 - your photos and videos comments all your
04:21 - friends the messages this is what we'll
04:23 - be looking at today information about
04:27 - advertisers and how they're looking at
04:29 - your profile and information they're
04:31 - getting your location history calls it's
04:35 - pretty pretty crazy so once I saw this I
04:39 - was like okay I need to do something
04:41 - with this myself rather than just having
04:42 - Facebook using it for there may be
04:45 - machine learning models or for
04:46 - advertisers and to go ahead and get this
04:51 - data what we're going to do is we're
04:52 - going to adjust some of the options I'm
04:54 - gonna start what we were to do it on my
04:56 - page right now since I've already had a
04:57 - request I got sent me but on your page
04:59 - what you'll want to do is go ahead and
05:02 - leave this date range to all of my data
05:06 - that is going to go ahead and just get
05:08 - all your data going back into your past
05:11 - we'll change this format right here it's
05:13 - highlight as an HTML but if you were to
05:15 - click on that little box right there
05:18 - you'll see the option for JSON and JSON
05:21 - is another format data that is more easy
05:24 - for us to analyze and so just change
05:27 - that up into JSON and then lastly you
05:30 - can go over to the media quality and
05:32 - change the media quality to low this
05:34 - will make sure that when you're
05:36 - downloading that information any sort of
05:38 - photos that have been in your chat
05:39 - history they'll get downloaded at the
05:41 - lowest resolution otherwise that file
05:44 - that you're requesting could be really
05:45 - really really big I mean for our
05:47 - analysis we're not going to be needing I
05:49 - need the photos or anything like that
05:52 - once you've made those selections you'll
05:55 - want to go ahead and click this deselect
05:58 - all button
05:59 - right here on the top this will make
06:01 - sure that we're not downloading every
06:02 - single piece of information Facebook has
06:03 - of us that's gonna be a huge huge huge
06:06 - file it'll take a long time to download
06:09 - and maybe maybe you want that so you can
06:12 - do that for yourself but I would
06:13 - recommend at least for this analysis to
06:15 - just deselect all by clicking right over
06:18 - there on the top right and then
06:20 - scrolling down to messages and you'll
06:23 - just click that messages checkbox and
06:25 - that will make sure that we're just
06:27 - getting the message that has three data
06:29 - once all the settings are configured
06:31 - you'll click create file right here on
06:33 - the top right and then you'll get an
06:36 - email and do notification from Facebook
06:39 - when that file is ready for you to
06:41 - access and it'll be available here in
06:44 - the available final section it might
06:46 - take a day or two for you it might be
06:49 - really quick I think it depends on how
06:52 - many requests Facebook is getting how
06:54 - busy their servers are it's a little bit
06:56 - of a mystery so you can go ahead and get
06:59 - that downloaded download started right
07:01 - now and if you don't have it already
07:04 - it's totally fine we have some sample
07:08 - data that you can use to follow along
07:10 - with us today and you can access that
07:13 - sample data by going to the bottom of
07:15 - the stream and the description area we
07:18 - have a link to that sample data it's
07:22 - gonna be in a zipped JSON file so you
07:25 - can just go ahead right now and click on
07:28 - that link download the zip file and
07:30 - unzip so just tracking quickly to see if
07:36 - anyone has any any questions seems like
07:39 - everyone's doing well I guess Facebook
07:41 - can be scary I agree but but
07:46 - you know it's a necessary evil maybe in
07:48 - our lives well I think a lot of us use
07:51 - it so cool okay so what you've gone
07:55 - ahead and submitted that request for
07:57 - your data let me show you what that
08:01 - might look like on your computer so I'm
08:07 - going to show you right here when you
08:11 - download your Facebook data you're going
08:14 - to get this folder that will save
08:17 - messages and so I'm going to zoom in
08:20 - maybe a little right here so everyone
08:22 - can see potentially okay not able to do
08:35 - but you'll see right here we have this
08:37 - messages folder and that's the contain
08:40 - every single conversation that you have
08:43 - ever had on Facebook so it doesn't
08:46 - matter if it's your best friend who
08:48 - you're talking to
08:49 - last week or it was that person you
08:51 - spoke to one time back in middle school
08:54 - you will have every single conversation
08:56 - that you've ever had in this messages
08:59 - folder so I say beware which messages
09:03 - you go into you might find things that
09:05 - you don't want to remember I would stick
09:08 - to maybe some friends who you've known
09:10 - for a really long time you have a lot of
09:12 - messages with each other and you'll be
09:13 - able to find out some interesting
09:15 - information there and inside that
09:19 - messages folder there will be a
09:21 - different folder for every single
09:22 - conversation that you had all those
09:26 - different friends conversations and
09:28 - inside that folder we have this message
09:30 - JSON file that message file is going to
09:34 - actually contain all the information for
09:36 - those messages that we've downloaded so
09:39 - that's the key file that we're going to
09:41 - be using to
09:42 - for our analysis and so once again don't
09:48 - worry if you don't have your own data
09:49 - downloaded you can use that sample data
09:53 - that we have provided in the description
09:55 - of the luxury okay so we have this data
10:04 - that we've downloaded from Facebook
10:05 - let's go ahead start taking a peek at
10:08 - what we have inside and figuring out
10:11 - what we can find about for this analysis
10:15 - we're going to be using a tool called
10:18 - Jupiter and notebook a new red notebook
10:21 - is a really useful tool that is used by
10:26 - data sciences a lot of people in the
10:28 - data analysis industry to look at data
10:31 - do a lot of exploration with it to see
10:33 - what's there and then also do their
10:35 - analysis and so to access Jupiter
10:39 - hopefully on your computer you have
10:41 - downloaded Conda anaconda is this great
10:45 - toolkit for data scientist that has
10:48 - Python included for you a lot of the
10:51 - great packages that are used for data
10:54 - analysis and it should also download
10:57 - Jupiter notebooks for you I mean so if
11:00 - you just open up your terminal on your
11:02 - computer you can just go ahead and type
11:05 - in Jupiter notebook just like this and
11:11 - it's going to go ahead and open up on
11:15 - your computer the Jupiter notebook
11:17 - interface and it's basically going to
11:21 - show through your web browser or just
11:23 - all the files that you have on your
11:24 - computer and you can navigate through
11:27 - these different folders to wherever you
11:31 - want to create this file for analysis so
11:34 - I'd recommend creating this notebook in
11:37 - the same location where you download
11:39 - that messages JSON file from Facebook
11:43 - and then once you're in the right
11:45 - location
11:46 - all you need to do to create a new file
11:48 - is go up to new here on the top right
11:50 - and click Python 3 and that's going to
11:56 - open up a new Python 3 notebook for you
12:00 - in Jupiter and so I'm just going to
12:02 - navigate back to my notebook over here
12:04 - just as I created this one in the right
12:07 - folder for where I'm going to be doing
12:09 - my analysis so we're here in Juvenal
12:12 - books and let's get started with looking
12:16 - at this data so what I'm going to do
12:20 - first is go back to my finder and I'm
12:22 - going to find that file that is the
12:24 - message to JSON file because we need
12:26 - that location of where the file is on
12:29 - your computer and order for us to open
12:31 - it up so I'm just going to right click
12:33 - double-click whatever it is on your
12:35 - computer and if any of your Mac or
12:37 - Windows and click get info on the
12:39 - message dot JSON file and I'm going to
12:43 - go to the location which is listed over
12:46 - here in this information page right
12:50 - click on that location and click copy
12:53 - and what that's going to do is just get
12:55 - that copy for me and that location for
13:02 - me and I'm just going to go back to my
13:04 - Jupiter notebook and I'm going to create
13:06 - a variable called path to file and I'm
13:12 - gonna put in a string that is this
13:14 - location that I've copied at at the end
13:17 - this is just the file location where
13:20 - that file is but I want to point
13:21 - directly to the specific message our
13:23 - JSON file so I'm going to add and right
13:26 - at the end a slash and then message dot
13:29 - JSON and when your integrator notebooks
13:32 - to run a certain cell you can just click
13:35 - shift and return or shift and enter and
13:38 - that will execute that single line of
13:40 - code and this is one of the great things
13:41 - about Jupiter I'm going to let you run
13:43 - blocks of code or lines of code on one
13:45 - at a time and so just looking here at
13:49 - the chat if you're having some issues
13:52 - finding finding the file I would say try
13:55 - downloading once
13:57 - again that message JSON file from the
14:01 - YouTube description and you should be
14:05 - able to in your I believe in Windows
14:07 - it's like Explorer and maggots finder
14:09 - find where that file is and then copy
14:12 - the location from there okay now that we
14:17 - have our location from the file let's go
14:26 - ahead and open it up so I'm going to say
14:29 - with open and then I'm going to put
14:35 - inside people bin the path to file and
14:39 - I'm going to call this as file right
14:43 - here I'm do a colon and then I'm going
14:46 - to store the contents of this JSON file
14:49 - in a variable called chat history and to
14:56 - load this file in I'm going to write
14:58 - JSON load my file and right here when
15:04 - you run this you might run into an error
15:06 - when typing in JSON download if you are
15:09 - seeing that I'm going to say that you
15:11 - should also import JSON right here on
15:13 - top this is just importing a Python
15:16 - package for working with JSON data so
15:19 - what import that will open up the file
15:21 - with this syntax outside shift-enter
15:24 - and this has now opened that file and
15:27 - pulled all the contents out into chat
15:29 - history and we can take a look at
15:31 - China's three right here and integrate
15:33 - notebooks to just look at a variable you
15:36 - don't need to print it out you can just
15:38 - run it that individual variable just run
15:44 - that self and you can see here we have
15:46 - China's three which is this Python
15:48 - dictionary and all trying
15:53 - a little here so you guys can see more
15:59 - and hopefully that will help okay
16:05 - great we got some zoom a little bit of
16:08 - lag but now you guys should be able to
16:13 - see the information and display a little
16:17 - more clearly so in order for us to get a
16:21 - better idea of all of this data that's
16:24 - in here I'm gonna go ahead and look at
16:26 - the keys of the dictionary so I'm going
16:28 - to say chant history donkeys and I can
16:34 - see that we have a few different keys
16:35 - here one of participants another is
16:38 - messages so the participants are going
16:42 - to be the people who are involved in
16:44 - this chat and so we can look here up on
16:48 - the top for participants we have Lewis
16:50 - Carroll and we have Jane Austen
16:52 - originally I had done a lot of analysis
16:54 - on my own facebook messenger data and I
16:57 - was horrified by some of the things that
16:59 - I saw and so I thought best to to use
17:04 - some other information for for this live
17:08 - chat so we grabbed some text from
17:11 - Project Gutenberg which is an open
17:13 - source natural language processing or
17:19 - novel novel database and grab text from
17:22 - two famous authors Lewis Carroll and
17:25 - Jane Austen and we replaced my actual
17:28 - message history with information or
17:31 - texts from their dolls so we're just
17:34 - mixing up a little bit here but if
17:35 - everything is exactly in the same format
17:37 - as it should be downloaded from Facebook
17:40 - so don't need to worry there so let's go
17:47 - ahead and take a look at a little bit
17:49 - more what we have here so we have these
17:51 - two participants Lewis Carroll
17:53 - and Jane Austen but then we also have we
17:57 - see messages and then we see these
17:59 - blocks and messages so we see the sender
18:01 - is Jane then we have a timestamp which
18:04 - is at time set of milliseconds and when
18:06 - that message was sent we also see we
18:08 - have content here which is the actual
18:10 - message content that was sent by that
18:15 - individual and and we also have other
18:24 - information here and like photos which
18:27 - will have oh like oh there are two
18:31 - things if you send a picture if you made
18:33 - a phone call that will also be listed
18:35 - there so that that will all be listed
18:41 - and I'm just taking a look at the chat
18:44 - here can you scroll back up and show
18:47 - with the open command of course sorry
18:49 - about that
18:51 - so I'll scroll back up here and we can
18:54 - see right here import JSON to make sure
18:58 - that we have that JSON package and then
19:00 - with open path to file as file and we're
19:04 - gonna load that up JSON file into
19:08 - Chinese 3 with JSON download of file
19:12 - I apologize for missing on that Sam and
19:16 - Chris hopefully you guys are able to now
19:18 - see how do you get that bottle loaded
19:24 - okay so we can see that we have a lot of
19:28 - good information here everything that we
19:31 - want is really stored inside the
19:34 - messages key of this dictionary so I'm
19:38 - gonna just go to Chinese 3 once again
19:42 - and then type in the key messages well
19:49 - let it run for a second and this is the
19:51 - information that we really want so how
19:54 - do we go ahead and take this list of
19:58 - basically messages that we
20:00 - out here and convert it into something
20:04 - that is useful for us how can we
20:06 - visualize this data in a useful way
20:08 - in order to do this we're going to use
20:10 - the Python package pandas and pandas is
20:14 - really good for data visualization
20:17 - helping us organize our data to data
20:20 - cleaning and ultimately managing our
20:22 - data with named Python so I'm going to
20:25 - import the pandas package right here and
20:31 - then I'm going to create what's called a
20:33 - data frame in Peppa's and a data frame
20:36 - is the main way that we store large
20:38 - amounts of data in so I'm going to name
20:43 - this data frame messages and I'm going
20:46 - to set it equal to a value that's going
20:49 - to be the data frame of our data and now
20:52 - when we're using pandas we usually have
20:55 - this shorthand notation for PD so I'm
20:58 - sort of just right away reporting pandas
20:59 - I'm going to import it as D and now I'm
21:02 - going to set messages to PD dot data
21:05 - frame of those that entire list of
21:09 - messages so I'll say chat history
21:16 - of messages
21:23 - and I'll run that and now we can take a
21:26 - look at messages right here I think we
21:31 - see that we get this data frame and on
21:36 - the top we have these different columns
21:38 - these columns represent the different
21:41 - aspects of an individual message so we
21:46 - have the content column which is showing
21:48 - the actual content of that specific
21:51 - message we have the sender name which is
21:54 - the name of the person you send that
21:55 - message we have some other interesting
21:58 - things like gifts that were posted if
22:01 - there was like a call associated with
22:02 - that message how long it lasted
22:05 - we have importantly the timestamp in
22:07 - milliseconds and this is the actual time
22:10 - that that message was sent
22:12 - and so those columns represent those
22:15 - different attributes of a message and
22:18 - then each row here is representing a
22:20 - different message that we have so 0 is
22:24 - representing one message from Jane
22:26 - Austen the started the family of
22:28 - Dashwood had long been settled the
22:31 - second message one was by Lewis Carroll
22:33 - and it starts down the rabbit hole so we
22:40 - can see we have all this information
22:42 - here and okay I'm seeing some questions
22:47 - so someone asks about line one so line
22:51 - one at the top is just going to be the
22:52 - path true that specific file on your
22:56 - computer and this is the message that
22:58 - JSON file that has all your message
22:59 - information we also have a question
23:02 - about what is a NAND mean so any any
23:07 - sense for not a number and whenever
23:10 - there's missing information for a
23:13 - specific column for a certain message a
23:16 - wool list and an
23:18 - automatically so for message zero right
23:21 - here there was no call duration so it
23:24 - was just a text message there are no
23:26 - gifts there were no reactions use those
23:28 - are those little you know reactions you
23:30 - can set to individual Facebook messages
23:32 - and so first if something's missing
23:35 - we'll say on an if we have data there we
23:38 - will be able to see the actual piece of
23:43 - content there okay awesome
23:49 - so we have this big data frame of all of
23:52 - our data now it's a good question your
23:55 - time to ask the question what are we
23:57 - going to do with this data what sort of
23:59 - insights can be find and so when I was
24:03 - looking at this I saw that we have all
24:04 - these this message content what what can
24:08 - I do with that message content and one
24:10 - thing that's really popular to do with
24:11 - text data is to do what's called a
24:14 - sentiment analysis and a sentiment
24:16 - analysis is essentially looking at
24:19 - different text data and assigning a
24:22 - positive or negative score to a piece of
24:25 - text to see if it is positive or
24:27 - negative was this something that was a
24:29 - happy or joyful or positive or is this
24:32 - something that was maybe a sad or or a
24:35 - negative or you know unhappy and by
24:38 - looking at the sentiment of let's say
24:41 - these messages over time we're able to
24:43 - see how the nature of your facebook chat
24:47 - has changed with your friend so maybe
24:49 - you guys started off you know in the
24:51 - beginning of your friendship having
24:52 - positive messages I maybe you guys hit
24:55 - some rough spots and the sentiment of
24:57 - those messages start to go down maybe
24:59 - got into a fight those the sentiment
25:01 - went really down or we can also even
25:04 - look at sentiment between each she's a
25:07 - big person in the conversation so maybe
25:09 - in your conversations with your friend
25:10 - your messages are gonna be positive your
25:13 - that upbeat person you're always trying
25:15 - to you know help your friend out and I'm
25:17 - in a good mood and or maybe maybe you're
25:19 - the kind of person who you know you're
25:21 - you've been down on your luck things
25:23 - have been tough and maybe the sentiment
25:26 - of your
25:26 - we'll indicate that so what I wanted to
25:29 - do was look at the sentiment of these
25:31 - messages these Facebook messages over
25:33 - time and see what I'm able to to find so
25:38 - that's kind of where this analysis is
25:40 - going to go you can see that we have a
25:42 - lot of other data you could go so many
25:43 - different ways with this but for today
25:46 - we're gonna stick with the sentiment
25:48 - analysis and since I wanted to do an
25:52 - analysis over time I wanted to take
25:55 - advantage of this time data that we have
25:58 - and so we can see that we have the
26:03 - timestamp in milliseconds this doesn't
26:06 - mean a lot to us this is the amount of
26:08 - time since some specific time when
26:11 - Facebook starts recording
26:13 - you know dates I believe for lots of the
26:16 - computer systems they track from like
26:17 - January 1st 1970 when they count up from
26:20 - their different systems use different
26:23 - time stamps so what we're going to do is
26:25 - go ahead and using pandas which is great
26:28 - once again for data manipulation convert
26:30 - this time stamp from milliseconds into
26:33 - something useful and so to go ahead and
26:36 - do that I'm going to create some helper
26:38 - functions so I'm gonna create a function
26:39 - call to convert time and it's going to
26:43 - take in a timestamp and what we're going
26:46 - to do is we're going to return that
26:50 - timestamp converted into what is called
26:53 - a pandas date/time and it's a specific
26:55 - got pandas data type that will give us
26:57 - the month the Jade year and our of a
27:00 - specific message and so to do that we're
27:04 - going to call cadiz and use it to
27:07 - date/time that's it and I'm just going
27:11 - to feed it that timestamp and I can go
27:15 - ahead and just you know save this
27:17 - function or define it right now but one
27:21 - thing that you would run into you just
27:22 - normally see
27:23 - is this function would not work properly
27:25 - I'm gonna think eyes we need to make a
27:27 - note of the time stamp which is
27:29 - specifically in milliseconds
27:30 - so we need to add that unit in to our
27:32 - daytime function so there is a keyword
27:35 - argument that we can add call it unit
27:37 - and I'm just going to have the unit's in
27:39 - milliseconds so I've created this
27:43 - function let's try it out I'm just going
27:45 - to copy this time stamp right here from
27:49 - this one message if my computer will let
27:52 - me copy it and I'm gonna call convert
27:55 - time on this time stamp and awesome now
28:03 - we can see here that this message was
28:07 - from October 5th 2018 at the 22nd power
28:11 - at 10:30 so really useful tool to able
28:17 - to make those time stamps into something
28:19 - that's useful to us so what I'm going to
28:21 - do is I'm going to go ahead and apply
28:22 - this convert time function that we just
28:25 - make here to every single message in our
28:30 - messages data frame so it's going to
28:32 - burn all these time stamps from
28:33 - milliseconds into date/time and then we
28:36 - can do this by saying that messages and
28:38 - we want to operate on the whoa so we're
28:43 - gonna operate on this time stamp
28:45 - milliseconds help but I'm going to
28:46 - create a new column so I'm gonna make a
28:47 - new column called date and I'm gonna set
28:53 - it equal to applying this function
28:56 - convert time to every single time stamp
29:00 - that we have so I'm gonna say messages
29:02 - of time stamp and milliseconds and we're
29:06 - going to use pandas apply and what apply
29:10 - will do is apply this convert time
29:11 - function that I have to every single
29:16 - message or at every single time stamp
29:18 - for every single message
29:22 - so I'm gonna run that and I'll show my
29:25 - data frame again and if I scroll over to
29:29 - the right on my data frame when I'll see
29:32 - that we have this new column called date
29:34 - which has that converted timestamp so
29:38 - our most recent message is going to be
29:40 - from October 6 2008 teen and if we
29:44 - scroll down all the way to the bottom
29:46 - and I didn't show this before but I
29:49 - believe we have 60 how many 6600 almost
29:59 - 6700 messages between Jane and Luis so
30:02 - they're pretty good friends you might
30:04 - find that for some of your friendships
30:07 - you have fewer messages if it's maybe
30:09 - your best friend you might find that you
30:11 - have 100,000 messages that might depend
30:14 - you know how the side of the file will
30:17 - change depending on that so you think
30:18 - might run slower I filled a lot of
30:20 - messages just things to look out for but
30:23 - you'll see here our earliest message is
30:25 - from August 29th 2017 so they've been
30:29 - friends for a little over a year at this
30:32 - point a year and three or four months
30:35 - great so we were able to pull out this
30:39 - date and I just want to make sure I
30:41 - don't have any questions yet I see some
30:45 - of my colleagues are being really
30:47 - helpful thank you guys for answering
30:48 - some questions too and great ok so we
30:52 - have this day time but we also want to
30:54 - pull out a specific month and year of
30:59 - that day time because when I'm doing the
31:02 - sentiment analysis I want to see on a
31:05 - month-by-month basis how sentiment is
31:08 - changing we can look at individual
31:10 - messages and you know compare the
31:12 - sentiments but we're gonna have 60 you
31:15 - know 860 700 message
31:18 - at that point if we shrink our space
31:20 - down to the let's say 13 or started 15
31:23 - months that Jane and Luis have been
31:25 - talking we can do like an aggregate
31:28 - analysis and see how that sentiment is
31:30 - changing over time so now I want to do
31:33 - go ahead and pull out that month in here
31:35 - and thankfully this is really easy with
31:38 - pendous daytime objects so I'm going to
31:42 - create two more helper functions right
31:43 - now one called yeah month and to take in
31:48 - a pandas date/time and we're just gonna
31:52 - return the date with a period and then
31:55 - the month that's it so if you have that
31:57 - pandas date/time object you just add a
32:00 - period after and then month I mean
32:02 - you'll get the month I'm going to add
32:04 - another function called get here which
32:07 - will also take a day time object and we
32:10 - can return date dot here and just like I
32:17 - did before I'm going to create two new
32:19 - columns in my data frame one will be
32:22 - called month and the other will be call
32:28 - here and I'm going to operate this time
32:31 - on the date column of messages so if I
32:34 - just put date in the brackets and
32:36 - messages were able to access that date
32:38 - column and we're going to use the apply
32:40 - method once again and we'll apply our
32:43 - get month function that we just defined
32:47 - and I'm gonna do the same thing for a
32:51 - year so I'll say messages of year
32:54 - creating this new messages call them and
32:56 - we're gonna operate it on a date column
32:59 - and we will apply our get year
33:06 - and you know if you're following along
33:07 - and you're familiar with Python then
33:10 - using pandas that's great
33:12 - if you're newer to Python or you haven't
33:14 - used it before you know don't don't
33:16 - worry this video is being accordion you
33:19 - it'll be saved you'll be able to access
33:21 - it later on and if there's any sort of
33:23 - syntax that you're not getting you'll be
33:26 - able to review and hopefully learn learn
33:28 - some of the syntax um during that review
33:30 - time so yeah don't get stressed just
33:33 - feel free to follow along for now and
33:35 - see what insights are able to find great
33:41 - let's go ahead and just inspect our
33:44 - messages data frame one more time and
33:46 - we'll be able to see those two new
33:48 - columns that we added right here on the
33:50 - right that have month and year so we've
33:58 - gone ahead and done a little bit of
34:01 - cleaning on the data that we downloaded
34:02 - from Facebook now I want to go ahead and
34:06 - get started on doing that actual
34:08 - sentiment analysis how do we see the
34:10 - content that we have and see the score
34:13 - or the sentiment of it I meant to do our
34:16 - sentiment analysis we're going to be
34:18 - using a package called penalty K which
34:22 - is the natural language toolkit this was
34:25 - a really useful Python package for
34:28 - natural language processing so if you're
34:30 - interested in text data what it says we
34:32 - can find with it I highly recommend
34:34 - looking into a penalty kick and with LT
34:37 - k we're going to use a specific tool
34:40 - called Vader and Vader is a sentiment
34:46 - analysis tool which is going to do the
34:49 - sentiment analysis for us so I'm going
34:52 - to say from NLT k dot sentiment dog
34:55 - Vader so this is specific part of adult
34:58 - DK we're going to import sentiment
35:02 - intensity analyzer
35:06 - and so we'll go ahead and import that if
35:11 - on your computer you don't have n LT k
35:13 - installed or let's say you run this cell
35:16 - and you see you know an error pop-up you
35:19 - can just go back to your terminal and I
35:23 - would open up a new window that's not
35:24 - Jupiter and if you have anaconda
35:26 - installed you can just do Conda install
35:29 - and LG k if you don't have content
35:32 - you're using pip you can do pip install
35:34 - l TK and that you download that package
35:36 - for you one last thing that we want to
35:41 - do with ml TK is we need to download the
35:44 - lexicon for mater and this is
35:46 - essentially a giant dictionary of words
35:48 - that are scored with positive or
35:51 - negative scores and this is what Vader
35:54 - will use in order to look at our text
35:57 - data and assign these scores to our
36:01 - messages so I'm going to say NLT Kay got
36:04 - download and in a string I'm going to
36:08 - say Vader underscore lexicon and when
36:13 - you run the cell it might take some time
36:15 - for me I've already downloaded it to my
36:17 - computer so it runs really quickly but
36:19 - for you and might take some time to
36:21 - download once that's downloaded let's go
36:25 - ahead and create a sentiment analyzer
36:29 - object so I'm going to say sentiment
36:32 - analyzer is equal to and I'm going to
36:36 - say sentiment intense
36:41 - [Music]
36:46 - with rocket at the end don't forget the
36:49 - parentheses that those creates that
36:51 - sentiment analysis object for us I let's
36:54 - go ahead and test this out on some data
36:58 - so I'm just gonna ask if someone can
37:01 - type in maybe a sentence that you want
37:03 - me to run a quick sentiment analysis on
37:05 - I'll just enter it into the chat and I
37:09 - will try out our sentiment and sudsy
37:12 - analyzer on your sentence and no one
37:15 - puts anything in I'll use one of your
37:17 - comments maybe I might we can give it a
37:18 - shot so in order to do this I'm gonna
37:20 - say sentiment analyzer dot polarity
37:24 - scores and polarity scores is this
37:27 - method that will score a sentence for us
37:31 - and I see someone saying that's sweet so
37:35 - I'm gonna say that meat and so I put
37:43 - that sentence into my analyzer and we
37:47 - can see we get to this dictionary of
37:49 - scores back and so the first is a
37:52 - negative score a neutral score and then
37:54 - a positive score and a compound score
37:56 - and so I think when whoever you are when
38:00 - you said that sweet you meant I had a
38:02 - great way you seems pretty positive and
38:04 - dander seems to agree it's giving a
38:06 - positive score point seven five and a
38:09 - compound or overall a score of 0.5 which
38:12 - is pretty positive and so those three
38:14 - scores negative neutral positive those
38:17 - will range from zero to one and the
38:19 - compounds will range from negative one
38:21 - meaning most negative to one meaning
38:23 - most positive so what I'm going to do is
38:26 - I'm going to try this one more time on a
38:29 - a few more sentences really quickly so
38:33 - we run with polarity scores on the
38:36 - machines will take over we can see what
38:40 - bigger studs okay
38:42 - Vader is saying the machines will take
38:44 - over is a neutral statement I don't know
38:48 - if I would agree with that but you know
38:52 - I'm a little worried about machines
38:54 - taking over it's not not the best thing
38:56 - to me that again Vader is you know part
39:00 - of the computer I guess so maybe it's
39:02 - you know not not the best to ask for
39:04 - this this kind of sentence so it's not
39:09 - perfect at everything you know there's
39:12 - clear positive or negative words in your
39:14 - sentence Vader will work pretty well
39:17 - other times it you know it doesn't and I
39:22 - saw that someone's asking about not to
39:26 - language processing in foreign languages
39:27 - so yeah I believe Vader it only works in
39:30 - English at this time however there are
39:34 - some natural language processing tools
39:36 - out there that are being trained on
39:39 - different languages so people are doing
39:42 - it I think there's a lot of unexplored
39:43 - territory there I think that hopefully
39:45 - will be a lot more research done there
39:47 - right this time
39:49 - looks like Vader is just just an English
39:54 - great okay so now that we have this
39:56 - let's create a helper function that will
39:59 - score each of our messages for us so I'm
40:03 - gonna say define a function called get
40:06 - polarity that's going to take in a piece
40:09 - of text and it's going to return our
40:13 - sentiment analyzer and we're going to
40:16 - use its polarity squares function and
40:22 - we're going to pass it our text data and
40:26 - what we're going to do is we're just
40:28 - going to excuse me access our compound
40:31 - score so I'm just going to access the
40:34 - compound key
40:37 - cheri compound great so now I'm going to
40:45 - go ahead and create a new column and
40:48 - messages called sentiment and I'm going
40:52 - to set it equal to applying this get
40:55 - polarity function to our message content
41:00 - column so I'm the same messages and I'm
41:04 - going to go back just to make sure what
41:05 - the name of that column is my school I
41:10 - can see what content so content is name
41:13 - of our column that has that actual text
41:15 - data so I'm gonna say messages of
41:17 - content and we're going to apply our get
41:22 - polarity function and I will run this
41:27 - this might take a little bit of time
41:29 - depending on how many messages you have
41:31 - it's just a little bit more intensive
41:33 - analysis than the big conversion reading
41:35 - before and then we look at messages once
41:39 - again and I scroll over to the right
41:43 - we're able to see these different
41:46 - sentiment scores so for these first two
41:47 - sentences banner saying you know zero
41:51 - sentiment meaning basically neutral but
41:54 - this one point eight four one nine a
41:56 - really positive message from from Jane
41:58 - so let's see Jamis saying their state
42:00 - was large and their residence was
42:02 - probably something grand or great so
42:05 - really really hyping about this
42:07 - residence or this estate so very
42:09 - positive so now we have all these
42:13 - sentiment score to see a negative 0.15
42:15 - here negative 0.5 nine pretty negative
42:18 - let's take a look at this one really
42:19 - quickly Alice was beginning to get very
42:21 - tired of sitting so yes Alice was
42:24 - getting bored she was tired she wanted
42:25 - to do something she was getting antsy so
42:28 - more and more negative so hopefully
42:31 - everyone was able to follow along with
42:33 - the sample data to get to this point
42:36 - where you have sentiment we're getting
42:38 - to the really cool part soon or right
42:40 - really where we get to see some results
42:42 - see some insights from our data so to
42:49 - get these insights first really quickly
42:51 - let's get a idea of who is the more
42:54 - positive and more negative person in
42:57 - this conversation is it Jane or is it
43:00 - Lewis and so in order to do this we're
43:02 - going to use our messages data frame and
43:04 - I'm going to use the group by method and
43:07 - what this will do is it will group all
43:11 - the rows in our data frame that are in
43:15 - the same rows where sender is Jane
43:18 - Austen or all of the rows where the
43:21 - sender is Lewis Carroll and squish them
43:23 - together and then we can go ahead and
43:26 - apply some sort of aggregate function to
43:30 - all of that content so let me let me
43:33 - write it out and I think it'll be a
43:34 - little easier in to explain so I'm gonna
43:37 - say group by sender name and I'm gonna
43:40 - take the mean I'm just gonna run this
43:43 - really quickly and so what I was doing
43:45 - was taking all the messages that Jane
43:47 - sent and finding the mean or the average
43:52 - of each of these different columns here
43:54 - so we can see if we look at the average
43:58 - sentiment we see point one six so all
44:01 - the messages that Jane sent her average
44:03 - overall sentiment was point one six we
44:06 - go down to Lewis we can see the average
44:08 - overall sentiment was point zero five so
44:11 - what this is showing to us is that Jane
44:15 - is almost three times as positive and
44:18 - her messages as Lewis overall so it's
44:22 - some pretty interesting interesting
44:23 - finding there I'm not a you know how to
44:26 - read a lot of James works so I'm not as
44:29 - familiar you know but very surprising to
44:33 - me and we from from my understanding of
44:35 - these two authors so hopefully you're
44:38 - getting some interesting insights on
44:40 - your data if you're following along with
44:43 - your own
44:44 - not hopefully in the future when you're
44:45 - able to do on your data you will
44:47 - automatically get some really
44:48 - interesting insights right from here
44:52 - now that we have this though let's go
44:54 - ahead and see if we can make our
44:55 - visualization over time and see see how
44:58 - that sentiment is changing so once again
45:02 - I'm going to take messages and I'm going
45:05 - to group by and this time instead of
45:09 - just agree by sender name I'm going to
45:10 - group by three different columns I'm
45:13 - gonna group by month
45:14 - I'm going to group by year and I'm going
45:17 - to group by sender name and what this
45:21 - will allow me to do is it will allow me
45:23 - to grab each combined unit a month year
45:30 - and sender so let's say for August 2018
45:33 - Jane Austen it's going to gather all her
45:35 - messages from that month you know August
45:39 - 2018 and then we could do that averaging
45:41 - of sentiment on all of those messages so
45:45 - once again I'm grouping by those three
45:47 - columns month year and sender name and
45:50 - then I'm going to take the mean and this
45:56 - time I'm gonna save this to a variable
45:58 - and I'm gonna call this year month since
46:02 - I'm grouping by year and month and then
46:06 - one less thing I'm going to do and tag
46:09 - out to the end is one more pandas method
46:11 - reset index and what this will do is it
46:16 - will reformat this information into a
46:20 - data frame once again so we're able to
46:22 - do some further analysis on it so I'm
46:25 - going to go ahead and run this and I'm
46:30 - gonna now display Year month and we can
46:32 - take a look at this and so what we can
46:38 - now see in this data frame is we have
46:40 - for January 2018 we have Jane Austen and
46:44 - we have her average sentiment as point
46:47 - one seven for the same month January
46:49 - 2018 we have Lewis Carroll and then we
46:51 - have her average
46:53 - at point zero five and this continues
46:55 - for every single month of the year
46:58 - grouping in pandas is a really really
47:01 - powerful tool it helps us do a lot of
47:03 - interesting things it can be a little
47:06 - complicated at first when you haven't
47:08 - done it before but once again you'll
47:11 - have access to this recording later on
47:13 - will also provide the code to you and
47:16 - you can look at it a little bit deeper
47:17 - but ultimately we're just saying let's
47:19 - look at each month over the course of
47:22 - their friendship and get that average
47:23 - sentiment for each of the different
47:26 - people in that chat so now that I have
47:31 - this new data frame I want to go ahead
47:34 - and grab all of those average sentiments
47:37 - over time for each Center for each
47:41 - person in this chat because once I grab
47:43 - all that data I can go ahead and then
47:45 - plot that over time so what I'm going to
47:50 - do is I'm going to create a new variable
47:53 - gene and this is going to store all of
47:56 - the sentiment values for Jane and when
47:59 - I've used that year month data frame
48:01 - that I created earlier or just before
48:04 - and what I'm going to do is I'm going to
48:07 - just get all the rows or all the
48:10 - messages that belong to Jane and the way
48:14 - we're gonna do this is with what's
48:15 - called boolean indexing and basically it
48:18 - just lets us select all those roles all
48:21 - those rows in this data frame where Jane
48:23 - is the sender so I'll type out how you
48:26 - this here and then I can explain to you
48:29 - so we're gonna say Year month of sender
48:32 - mean and we want it to be equal to
48:39 - so what this is doing is it's saying go
48:43 - to every single row look at the sender
48:47 - name and the second name is Jane Austen
48:49 - give me give me that whole row give me
48:52 - that information
48:55 - but specifically I'm really just
48:56 - interested in the sentiment I don't care
48:58 - about the timestamp I don't care about
49:01 - the call duration I just want that
49:04 - sentiment so I'm gonna then just access
49:08 - that sentiment by using the the bracket
49:11 - notation and then sign two minute and
49:15 - one last thing I'm just going to tack on
49:16 - to the end just because I want the raw
49:18 - data I don't want it in in some you know
49:22 - pandas objects I just want the raw
49:24 - values I'm just going to say dot values
49:26 - right here let me just zoom out a little
49:29 - bit just so you're able to see this
49:31 - whole line of code altogether I think
49:34 - that'll be really useful for you we'll
49:39 - take a second and resize great and now I
49:45 - will run this line of code and if I look
49:48 - at Jane we now see that Jane is just
49:50 - this array of sentiments and so this
49:54 - first value zero point one seven is
49:56 - James first sentiment from the first
49:59 - month that we have in record point one
50:02 - five three as for sentiment on average
50:04 - for the second month and then point one
50:07 - four here at the end is her sentiment
50:09 - for the last month
50:10 - so I'm going to go ahead and do the same
50:13 - thing now for Lois I'm going to say
50:15 - Lewis is equal to and I'm gonna just
50:18 - grab only the rows from our data frame
50:22 - where the sender name is equal to
50:29 - [Applause]
50:30 - and once again we're just gonna grab
50:33 - those sentiment scores from there and
50:36 - we're just gonna grab the values and
50:40 - let's see I'm running into an error
50:43 - series object has no attribute value so
50:47 - let's see so I just value instead of
50:53 - values so we'll go ahead and update that
50:56 - and now we have none and I can run this
51:00 - cell below and now we have all the
51:02 - sentiment scores for Luis over time from
51:06 - the beginning of their Facebook
51:08 - friendship to the act now that we have
51:11 - the values we can finally get to our
51:14 - graphic and this will allow us to
51:16 - visualize how these sentiments between
51:18 - the two have changed over time so to do
51:21 - this we're going to import map map on
51:24 - Lib matplotlib is the plotting package
51:31 - from Python so in order to import that
51:35 - we'll say import matplotlib PI plot as P
51:38 - LT peel T is just that shorthand
51:41 - notation we'll be using to access my
51:43 - public and to go ahead and do a plot all
51:47 - we have to do is type in p LT not plot
51:50 - then I can just put in Jane and then
51:55 - below that I can just say PLT dot plot
51:58 - and just put in the bluest and before I
52:02 - run this I want to make sure that this
52:05 - plot is big that we can all see it so
52:07 - I'm gonna say PLT dot figure and this
52:11 - just lets me access the figure size that
52:14 - we're going to be making these plots on
52:16 - and I'll say big size is equal to and I
52:20 - just give it a width by I'd I'm gonna
52:24 - take 20 my time I want it to be longer
52:26 - so go ahead and
52:30 - run this and there we go now we have our
52:36 - graph of sentiment over time really cool
52:40 - but you're asking okay who is who which
52:42 - one is Jane which one is Lewis which one
52:45 - it's me which one is my friend so we can
52:48 - quickly go back to our previous cell
52:49 - then we could just add in a label which
52:53 - is going to be gene for a gene and then
52:56 - for Lewis the label is going to be Lewis
52:59 - and then just below this I'm going to
53:03 - use map ellipse legend tool which one I
53:05 - had a legend to our plot and show us
53:09 - which line belongs to which Center so
53:13 - here we go we have our final product we
53:16 - have this plot over time for sentiment
53:19 - of Jane and Lewis we can see that their
53:23 - friendship starts out at kind of a a mid
53:25 - level of sentiment you know they're just
53:27 - getting to know each other perhaps we
53:30 - then see a few months into the
53:31 - friendship there's a divergence Jane
53:34 - gets really positive in her sentiment
53:37 - Lewis goes down in her sentiment we can
53:41 - guess about why this happens maybe maybe
53:43 - Lewis was going through some rough
53:45 - things in life and Jane was trying to
53:47 - cheer up her friend this is the part of
53:49 - the analysis where things that
53:51 - interesting on your end when you're
53:52 - looking at your Facebook message history
53:55 - you get to add that context to the data
53:58 - and this is one of those things that's
54:00 - important in data science overall is
54:03 - having kind of a domain knowledge or
54:06 - like an understanding of the data that
54:07 - you're working with and you're able to
54:09 - provide that for your Facebook message
54:11 - data so even the Facebook has it they
54:13 - don't know what's going on your light
54:14 - outside
54:14 - besides it's hopping on Facebook but you
54:16 - have all that other knowledge to put
54:19 - that game in context so if you're
54:21 - looking at your sentiment over time and
54:22 - you're seeing things happen you're able
54:25 - to infer oh yeah like this was when I
54:28 - was going to
54:29 - break up or this was a really great time
54:31 - in life I just got this great promotion
54:33 - and having that context really adds
54:35 - another dimension to to your analysis so
54:38 - I urge you to have that mindset when you
54:41 - go into analyzing your sentiment but you
54:44 - should be able to get a graph like this
54:45 - showing how how that sentiment changes
54:49 - over time and I think you'll find a lot
54:51 - of interesting insights I really enjoy
54:53 - doing this with my data and I think you
54:56 - guys will have a lot of fun too so I
54:59 - hope you're able to follow along with us
55:02 - as we went through this process I know
55:05 - some of you might run two issues here
55:07 - and there but once again this video will
55:10 - be up for you to review and as well the
55:13 - code we're really eager to see what you
55:16 - guys are able to do with your data and
55:18 - plugging out your sentiment or finding
55:21 - even other things hidden in the data if
55:24 - you do any of this analysis on your own
55:26 - we'd love to see it so feel free to
55:28 - tweet at us we're at Coke add me on
55:31 - Twitter
55:32 - send us the septum analysis that you do
55:35 - a lesson on the insights that you find I
55:37 - think it will be a lot of fun that you
55:40 - guys will enjoy I know I enjoy going to
55:44 - mind it even at times we did get scary I
55:46 - will warn you there your past can be
55:48 - scary but anyway so thank you guys all
55:54 - for joining I hope you enjoyed get out
55:59 - there take advantage of all the data
56:01 - that you can find learn some new things
56:04 - and once again if you're interested in
56:08 - kind of digging into data science a
56:09 - little bit further we have a lot of
56:10 - options here at code Academy happy to
56:12 - help you learn some more okay well thank
56:17 - you all for joining us joining us today
56:20 - we really appreciate you all and enjoy
56:24 - the rest day