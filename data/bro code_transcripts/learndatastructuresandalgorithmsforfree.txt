00:00 - hey y'all what's going on everybody it's
00:02 - you bro hope you're doing well and in
00:03 - this video we're going to discuss data
00:05 - structures and algorithms so sit back
00:08 - relax and enjoy the show
00:12 - if you wouldn't mind please like comment
00:14 - and subscribe one like equals one prayer
00:17 - for the youtube algorithm here's a few
00:19 - of the topics that we're going to cover
00:21 - in the series first we will learn about
00:23 - basic data structures followed by big o
00:26 - notation then searching algorithms
00:28 - sorting algorithms graphs and trees be
00:32 - sure to look for the timestamps in this
00:34 - video's description
00:35 - well what's a data structure simply put
00:38 - a data structure is a named location
00:41 - that can be used to store and organize
00:43 - data here's a real life example think of
00:45 - a family tree it's a hierarchy of family
00:48 - relationships we need some way to
00:50 - organize all of this data a tree is a
00:53 - data structure what about arrays we have
00:55 - a little bit of experience with those an
00:57 - array is a collection of elements stored
01:00 - at contiguous memory locations that's
01:02 - another example of a data structure it's
01:05 - basically a named location that can be
01:07 - used to store and organize data
01:09 - different data structures store and
01:11 - organize data in different ways each of
01:14 - them has their own pros and cons well
01:16 - what's an algorithm an algorithm is a
01:19 - collection of steps to solve a problem
01:21 - it's the steps to a solution here's an
01:24 - example i'm hungry and i need a pizza
01:26 - that's my problem an algorithm to bake a
01:28 - pizza could be one heat the oven two
01:31 - knead the dough three add the toppings
01:33 - and then eventually you'll solve your
01:35 - problem of not being hungry anymore so
01:37 - that's a real life example of an
01:39 - algorithm one algorithm that you're
01:41 - probably familiar with is a linear
01:43 - search suppose we have an array of
01:45 - elements and i need to find a value
01:47 - using a linear search we would one by
01:49 - one examine the elements of an array to
01:51 - find a value a linear search is an
01:54 - example of a searching algorithm i know
01:56 - what you're probably thinking why do i
01:58 - need to learn data structures and
01:59 - algorithms what's the point well let me
02:01 - tell you you'll write code that is both
02:03 - time and memory efficient and two
02:06 - commonly asked questions involve data
02:08 - structures and algorithms and coding job
02:10 - interviews do you know how to reverse a
02:12 - doubly linked list no no you don't do
02:14 - you so you should probably watch this
02:16 - series so without further ado ladies and
02:18 - gentlemen let's begin
02:20 - hey what's going on everybody it's you
02:22 - bro hope you're doing well in this video
02:24 - i'm going to explain the stack data
02:26 - structure so sit back relax and enjoy
02:29 - the show
02:32 - all right everybody stacks a stack is a
02:35 - lifo data structure lifo is an acronym
02:38 - for last in first out this allows us to
02:42 - store objects into a sort of vertical
02:44 - tower like a stack of books or a stack
02:47 - of cds we use the push method to add
02:50 - objects to the top of the stack and the
02:52 - pop method to remove objects from the
02:55 - top so here's a real-life example of us
02:58 - creating a stack data structure after
03:00 - declaring our stack we can add objects
03:03 - to our stack the objects that we'll be
03:04 - adding are various video games to add an
03:07 - object to our stack we use the push
03:09 - method like we're pushing something onto
03:12 - our stack so let's push minecraft onto
03:14 - the bottom of our stack and then next
03:17 - we'll add skyrim then maybe some doom
03:21 - then borderlands and then final fantasy
03:23 - vii so this is a lifo data structure
03:26 - last in first out to access objects on
03:30 - the bottom of our stack we first need to
03:32 - pop or remove objects from the top of
03:34 - our stack if i need to access minecraft
03:36 - on the bottom i don't want to pull this
03:38 - from the bottom because everything on
03:40 - the top is going to fall right so that's
03:42 - kind of the concept so in order to
03:44 - access minecraft on the bottom i need to
03:46 - pop or remove objects from the top in
03:49 - order to access objects on the bottom so
03:52 - in order to access minecraft i need to
03:54 - remove all the games on the top of my
03:55 - stack so that's kind of the point of a
03:57 - stack data structure
03:59 - now let's create a stack the language
04:01 - you use really doesn't matter for the
04:03 - most part you can implement a stack in
04:05 - java python c plus plus c is sharp the
04:09 - language really doesn't matter however
04:11 - the syntax might be slightly different i
04:13 - just so happen to be using java so i'll
04:15 - stick with that now to push objects onto
04:18 - our stack we first need to declare our
04:20 - stack and instantiate it so let's do
04:22 - that stack then we will list the data
04:24 - type of the objects that we will be
04:26 - adding to our stack luckily strings are
04:29 - a type of object and they're fairly
04:31 - simple so we will create a stack that
04:33 - can store strings the names of video
04:35 - games and i will name the stack step to
04:38 - keep it simple stack stack equals new
04:41 - stack and i will list the data type and
04:44 - then add a constructor okay so we now
04:46 - have a stack data structure now with
04:49 - stacks they have five unique methods
04:52 - let's take a look now with stacks we can
04:54 - push an item onto the top of our stack
04:57 - we can pop an item from the top of the
04:59 - stack we can peek at the item at the top
05:02 - of the stack we can check to see if our
05:04 - stack is empty
05:06 - and we can search our stack for an item
05:09 - so those are five unique methods
05:11 - available to stacks let's first check to
05:13 - see if our stack is empty so let's use
05:16 - system.out.printline stack dot empty
05:19 - method
05:20 - and currently our stack is empty that is
05:23 - true but that'll change soon so let's
05:25 - push some games onto the top of our
05:27 - stack so we type the name of our stack
05:29 - stack and use the push method then pass
05:32 - in our object so we will pass in a
05:35 - string the name of a video game let's
05:37 - add minecraft first
05:40 - then let's add
05:44 - skyrim
05:47 - then doom
05:49 - you can pick your own games if you want
05:50 - i don't care then borderlands
05:56 - and then final fantasy vii
05:59 - ffv iii
06:02 - okay let's check to see if our stack is
06:04 - empty now so let's move this line of
06:06 - code to the bottom
06:09 - and run it
06:10 - so our stack is no longer empty this
06:13 - returns false
06:14 - so with the stack we can at least take a
06:17 - look at the items within the stack
06:19 - without removing them so let's use
06:22 - system.out.printline and i will print my
06:24 - stack
06:25 - so this will print all of the objects
06:28 - within my stack beginning with minecraft
06:30 - then skyrim doom borderlands and final
06:32 - fantasy vii however by printing our
06:35 - stack we're not removing items from the
06:37 - stack so that's completely fine
06:39 - so this time let's pop the topmost item
06:43 - from our stack by using the pop method
06:45 - stack
06:46 - dot pop and we don't need to list an
06:49 - object because pop will always remove
06:51 - the top most object from our stack so
06:54 - now final fantasy will be missing from
06:57 - the top of our stack so if i was to
06:59 - repeat this again
07:01 - well then this will remove borderlands
07:04 - and then doom
07:07 - then skyrim
07:09 - and then lastly minecraft
07:11 - then if you pop again this is what
07:13 - happens
07:15 - an exception empty stack exception so
07:17 - that's something to keep in mind now
07:19 - when you pop the topmost object from a
07:22 - stack this will actually return that
07:24 - object if you need the object from the
07:27 - top of the stack you can pop it and then
07:29 - assign it to something so let's say
07:32 - string
07:33 - my fav
07:35 - game equals stack dot pop and then i
07:38 - will print my fave game and this will
07:42 - print final fantasy 7
07:44 - and it should be missing from the top of
07:46 - my stack if i was to print my stack
07:49 - so final fantasy vii is no longer in my
07:52 - stack because i popped it then assigned
07:54 - it to the string variable of my favorite
07:57 - game so in order to take the top most
08:00 - object from a stack you use pop and then
08:02 - assign it to something if you ever need
08:04 - to take a look at what the top most
08:06 - object is within your stack you do not
08:08 - want to use pop because that will remove
08:10 - it you can instead use the peak method
08:13 - of stacks so what i'll do is
08:15 - system.out.printline
08:17 - stack.peak and then immediately follow
08:20 - this with printing my stack so let's run
08:23 - this so the topmost item within my stack
08:26 - after using the peak method is final
08:29 - fantasy 7 then after printing my stack
08:32 - final fantasy 7 is still at the top of
08:34 - my stack so if you ever need to take a
08:36 - look at the object that is at the top of
08:38 - the stack without removing it you can
08:41 - simply use the peak method if you need
08:43 - to search for an object within the stack
08:46 - you can use the search method let's
08:48 - print stack dot search and we pass in
08:51 - our object as an argument so the very
08:54 - first item in a stack will have a
08:56 - position of one you would think it's
08:58 - zero but it's actually one now let's
09:01 - find borderlands that will be in
09:03 - position two from the top
09:05 - then doom would be of course
09:08 - three skyrim is four then minecraft is
09:12 - five so if you search for an item that
09:14 - is not within the stack like fallout 76
09:18 - well then this will return negative one
09:21 - if search does not find this object
09:23 - within your stack it will return
09:25 - negative one so with stacks we can
09:27 - actually run out of memory so let's add
09:30 - like a billion copies of fallout 76 we
09:33 - find bethesda's warehouse of unsold
09:35 - fallout 76 games so we'll create a stack
09:38 - of a billion copies of fallout 76 we can
09:41 - do this using a for loop didn't i
09:44 - equals zero
09:45 - as long as i is less than like 1 billion
09:50 - that's probably close enough
09:52 - increment i then we will push a copy of
09:55 - fallout 76 to our stack
09:58 - so stack dot push
10:01 - fallout 76
10:03 - and this is what happens
10:05 - a few moments later so we will run into
10:09 - an out of memory error due to the java
10:12 - heap space so it is possible to run out
10:14 - of memory when adding objects to our
10:17 - stack now this is an interesting feature
10:19 - that may or may not happen to you if you
10:21 - add a billion copies of skyrim onto a
10:24 - stack
10:25 - [Music]
10:31 - [Music]
10:33 - damn it todd howard you did it again
10:36 - now what are some uses of stacks here's
10:38 - a few examples one we can use them in
10:40 - undo redo features in text editors like
10:43 - when we go to edit then undo typing or
10:46 - redo typing we can move back or forward
10:49 - through browser history by hitting the
10:51 - back or forward button in the top left
10:53 - corner of most web browsers we can
10:55 - implement them in back tracking
10:57 - algorithms if we need to navigate a maze
11:00 - or search through file directories and
11:02 - another is that we use them when calling
11:05 - functions whenever we call a function we
11:07 - add what is known as a frame to the call
11:09 - stack but i'll probably save this topic
11:12 - for another video so in conclusion
11:14 - everybody a stack is a lifo data
11:17 - structure last in first out it stores
11:20 - objects into a sort of vertical tower a
11:23 - stack of objects you use the push method
11:26 - to add objects to the top and the pop
11:28 - method to remove objects from the top so
11:31 - those are stacked data structures in the
11:33 - next video we'll be covering cues which
11:35 - work a little bit different so if you
11:37 - can give this video a thumbs up and drop
11:39 - a random comment down below but yeah
11:41 - that is a stack data structure and well
11:44 - computer science
11:45 - what's going on everybody it's bro hope
11:47 - you're doing well in this video i'm
11:49 - going to explain cues in computer
11:51 - science so sit back relax and enjoy the
11:54 - show
11:56 - if you find this video helpful please
11:58 - remember to like comment and subscribe
12:01 - your support will help keep this channel
12:03 - running whoa hey we're talking about
12:05 - cues today so cues are a fifo data
12:08 - structure not to be confused with fifa
12:11 - fifo as in first in first out an example
12:14 - would be a line of people it's first
12:15 - come first serve whoever's there first
12:17 - gets helped first so this is a
12:19 - collection designed for holding elements
12:21 - prior to processing it's a linear data
12:24 - structure here's an example imagine that
12:26 - we're a cashier and we're selling
12:27 - tickets to a concert we will help
12:29 - whoever approaches our register first
12:31 - it's first come first serve in other
12:33 - words it's fifo first in first out
12:36 - unfortunately a wild karen appears and
12:39 - she would like to talk to the manager
12:40 - now karen is going to waste everybody's
12:42 - time so it's going to be a while in the
12:44 - meantime chad enters the line he will
12:46 - enter in at the back and wait his turn
12:48 - followed by steve and then harold so
12:51 - karen is at the head of our queue our
12:53 - line and harold is all the way at the
12:55 - back our tail as you can see harold is
12:58 - very anxious right now once karen has
13:00 - been defeated chad will move to the head
13:02 - of our queue
13:03 - so chad will now be helped next and
13:06 - harold will still be at the tail until
13:08 - somebody else enters the line once chad
13:10 - has been helped he will move out of our
13:12 - queue and steve is at the head now
13:15 - and the tail will move as well
13:17 - and then once steve has been held
13:19 - harold is the only one currently waiting
13:21 - in line for his tickets he will be at
13:23 - both the head and the tail of our queue
13:25 - so that's kind of how a queue works it's
13:27 - first come first serve fifo first in
13:30 - first out if you were here first then
13:32 - you will be helped first now the
13:34 - concepts of adding and removing objects
13:36 - from a queue are known as both enqueuing
13:38 - and dequeuing enqueuing is the concept
13:41 - of adding an object to the end of our
13:44 - cue our tail and dequeueing is when we
13:47 - remove an object from the head of our
13:49 - queue now what methods do you need to
13:51 - enqueue and dequeue from a queue well it
13:54 - really depends on the programming
13:55 - language that you're working with a lot
13:57 - of you voted for data structures and
13:59 - algorithms using java so i'll probably
14:00 - stick with java then here's an example
14:02 - now to enqueue and dq we need well a
14:05 - queue so let's create one
14:07 - q
14:08 - then we need to list the data type of
14:10 - the objects that we're going to add to
14:12 - this queue
14:13 - let's work with strings because strings
14:14 - are objects and they're fairly simple
14:16 - and i will name this queue q
14:18 - equals new now check this out i will
14:21 - attempt to create a queue object and
14:23 - instantiate it and the data type is
14:25 - string and i will add a constructor now
14:27 - we cannot instantiate the type q here's
14:30 - the reason why so if we take a look at
14:32 - the queue class
14:35 - right here q is actually an interface
14:37 - and not a class and based on our lesson
14:39 - on interfaces we cannot create an
14:42 - instance of an interface an interface is
14:44 - meant to be more of like a template that
14:46 - we can apply to another class so to
14:48 - utilize q technology we need a class
14:52 - that implements q now taking a look at
14:54 - our java collections hierarchy there are
14:57 - two classes that implement cues one of
14:59 - which is the linked list and the other
15:02 - is the priority queue now priority
15:04 - queues they will rearrange their
15:06 - elements based on a certain priority so
15:08 - they wouldn't be a good example so to
15:10 - utilize the features of a queue we are
15:12 - going to create a linked list because we
15:14 - cannot instantiate a queue itself
15:16 - because it's an interface with that
15:18 - being said let's change equals new queue
15:21 - to equals new linked list
15:24 - we won't be focusing on any of the
15:26 - features of linked lists we'll save that
15:28 - for another video maybe the next one i
15:30 - haven't decided yet so we will only
15:33 - cover the features that linked lists
15:35 - will utilize via the q interface now
15:39 - with the q interface there are three
15:41 - methods that we inherit from the
15:43 - collection's parent class add remove and
15:46 - element these will do approximately the
15:48 - same thing as enqueuing dequeuing and
15:51 - peaking at the top most element however
15:54 - they throw exceptions and according to
15:56 - the documentation it's actually better
15:58 - to use this column of methods instead
16:01 - these will return a special value we
16:03 - have offer which will enqueue or add an
16:06 - element to the tail of our queue poll
16:09 - will dequeue it will remove the head of
16:12 - our current queue and then there is also
16:15 - peak it will not remove the head but it
16:17 - will examine it and return it so those
16:20 - are the three dedicated methods that we
16:23 - implement via the queue interface there
16:25 - are some additional methods too which
16:26 - we'll cover later which we inherit from
16:28 - the collections class so let's begin by
16:31 - offering a bunch of people to our queue
16:34 - this is how we enqueue or add elements
16:36 - to our queue so first in our line we
16:38 - have karen so to add her to the queue we
16:41 - would type the name of our queue q dot
16:44 - offer
16:46 - and we will offer karen to the front of
16:49 - our queue and she would like to complain
16:51 - to the manager
16:52 - next we have chad q
16:54 - dot offer
16:57 - chad
16:58 - then we have steve
17:02 - and lastly harold
17:07 - okay now if we were to display our cue
17:11 - we'll place it within a print line
17:12 - statement
17:13 - we will pass in q as an argument and
17:16 - this should display our queue in order
17:18 - first we have karen at the head then
17:20 - chad steve then harold at the tail one
17:23 - method that we implement from the q
17:25 - interface is the peak method we can use
17:28 - the peak method to examine and take a
17:31 - look at the object at the head of our
17:33 - cue we're not actually going to remove
17:35 - the object at the head of the cube but
17:37 - take a look at it so within a print line
17:40 - statement i will use
17:41 - q
17:42 - dot peak method
17:44 - and after peaking over our cash register
17:47 - unfortunately karen is at the front of
17:49 - the line now to dq we can use the pull
17:52 - method so type
17:55 - q
17:56 - dot poll
17:57 - and this will dq your q
18:01 - so karen is no longer at the head of our
18:03 - queue so let's pull a couple more times
18:05 - for practice
18:07 - so after pulling twice
18:09 - karen and chad are now gone
18:11 - using pull a third time will remove
18:14 - steve
18:16 - and lastly harold
18:18 - now the nice thing about the pull method
18:20 - is that it will not cause an exception
18:23 - according to the java documentation you
18:26 - can use element to peak but it will
18:27 - throw an exception so if i was to use
18:31 - element at the end
18:32 - this would cause an exception then
18:34 - so usually it's preferable to use this
18:37 - column of methods even though they're
18:39 - similar these will not throw exceptions
18:41 - offer pull and peek now with cues there
18:44 - are some additional methods that you
18:45 - might be interested in the q class will
18:48 - extend the collection class so the q
18:50 - class inherits everything that the
18:52 - collection class has and there's some
18:54 - pretty useful methods within here i'll
18:56 - show you a few so the first is that we
18:58 - can check to see if our queue is empty
19:00 - so within a print line statement i will
19:02 - type
19:03 - q
19:04 - dot is
19:06 - empty and if our queue is empty this
19:08 - will return true so if i move this line
19:11 - of code after we offer a bunch of
19:13 - strings a bunch of people to our queue
19:16 - if we check to see if our queue is empty
19:18 - that will be false there are people
19:20 - currently within our line
19:22 - so that is the is md method we can also
19:25 - check to see the size of our queue how
19:27 - long is our line system.out.printline
19:30 - q
19:31 - dot size
19:33 - and the size of our line our q
19:35 - is four four objects we have four people
19:38 - waiting in line to be helped so that is
19:40 - the size method and another is the
19:43 - contains method we can check to see if
19:45 - our queue has a certain object that
19:47 - we're looking for so within a print line
19:49 - statement i will type q
19:52 - dot contains and then pass in an object
19:55 - let's check to see if harold is at the
19:57 - back of the line he's been waiting here
19:58 - patiently q dot contains herald
20:02 - and according to the contains method
20:04 - harold is in fact within our line that
20:06 - is true however this method does not
20:09 - give the index the position in which
20:11 - harold is in imagine that we yell out
20:13 - hey harold are you there and he yells
20:15 - back yeah or true that's kind of how the
20:18 - contains method works so these are three
20:21 - useful methods that cues inherit from
20:23 - the collection class now where could
20:25 - cues be useful they're actually used in
20:27 - quite a number of things but here's a
20:29 - few examples that came to mind so one
20:31 - that can be used in keyboard buffers
20:33 - have you ever typed so fast that the
20:35 - computer couldn't render all the
20:37 - characters onto the screen and all of
20:38 - those characters were added to a buffer
20:41 - they were all displayed in sequence and
20:42 - they were waiting for their turn to be
20:44 - displayed so with a keyboard buffer
20:46 - letters should appear on the screen in
20:48 - the order that they're pressed right
20:50 - first in first out they're also used in
20:53 - printer cues print jobs should be
20:55 - completed in order so we would start
20:57 - with page one then page two and just
20:59 - follow that pattern and as we discussed
21:01 - before they're used in linked lists
21:03 - priority queues as well as breadth first
21:07 - search algorithms well everybody in
21:09 - conclusion a queue is a fifo data
21:12 - structure first in first out it's a
21:14 - collection designed for holding elements
21:17 - prior to some sort of processing it's a
21:20 - linear data structure imagine a bunch of
21:22 - people waiting in line to enqueue that
21:25 - means to add an element to the tail of
21:28 - our queue to the end we can use the
21:30 - offer method in java to dequeue we would
21:34 - use the polled method in java that will
21:36 - remove the element at the head of our
21:38 - queue so that is the queue data
21:41 - structure in the comments down below let
21:43 - me know what you would tell karen if she
21:44 - asked to speak with your manager and
21:46 - that ladies and gentlemen is the q data
21:49 - structure in the field of computer
21:51 - science
21:52 - hey yeah everybody it's you bro hope
21:54 - you're doing well and in this video
21:56 - we're going to discuss priority queues
21:58 - in computer science so sit back relax
22:01 - and enjoy the show
22:04 - hey y'all what's going on everybody so
22:07 - priority queues a priority queue is a
22:09 - fifo data structure first in first out
22:13 - however a major difference with priority
22:15 - queues is that before we start pulling
22:18 - and serving elements we put them in some
22:20 - sort of order higher priority elements
22:22 - are served first before elements with
22:25 - lower priority so here's an example
22:27 - let's say that we have some student gpas
22:30 - and we'll place them all within a queue
22:32 - and then a priority queue and take a
22:33 - look at the differences so let's create
22:35 - a queue the data type is q
22:38 - and we are going to insert some doubles
22:40 - some gpas i'll name this queue equals
22:44 - new now queues are actually interfaces
22:47 - and we can't implement them directly so
22:49 - we need to use a class that utilizes the
22:52 - queue interface one that i know of is a
22:55 - linked
22:56 - list
22:58 - then finish instantiating it
23:00 - now to add data to a queue we use the
23:03 - offer method q
23:05 - dot offer and then pass in a gpa let's
23:09 - make sure that these are not in order
23:11 - uh how about a 3.0
23:14 - then the next student has a let's say
23:17 - 2.5
23:19 - then a 4.0
23:23 - 1.5
23:25 - and 2.0
23:27 - okay now let's display the elements of
23:29 - our queue
23:30 - one easy way in which we can do that is
23:32 - to use a while loop
23:34 - and this is our condition we'll continue
23:36 - this while
23:38 - not logical operator q
23:41 - dot
23:41 - is
23:42 - empty method while our q is not empty
23:47 - pull each element and display it so
23:50 - within a print line statement let's use
23:53 - queue dot poll to display and remove
23:56 - each element beginning with the first
23:58 - one and then working our way to the end
24:00 - so this will display the elements within
24:02 - my queue so these are in first come
24:05 - first serve order whatever element we
24:08 - were offered first we are serving that
24:10 - element first if an element was added
24:12 - last well then it's served last so
24:14 - that's a standard queue now with a
24:17 - priority queue we're going to arrange
24:18 - them in some sort of order before we
24:20 - actually pull these elements so let's
24:23 - change our linked list to a priority
24:25 - queue
24:27 - and run this again and let's take a look
24:29 - at the differences
24:31 - okay these are all in order now so if
24:33 - we're working with numbers like doubles
24:36 - these are arranged in ascending order
24:38 - beginning with the smallest element
24:40 - let's pretend that these are grade point
24:42 - averages gpas why might we want to put
24:44 - these in order let's say whatever
24:46 - student performed the worst gets two
24:49 - hours of free tutoring whichever student
24:51 - performed second worst gets one hour of
24:53 - free tutoring and third gets half an
24:56 - hour free tutoring now if you need these
24:58 - in descending order well there's one
25:00 - change that we're going to make within
25:02 - the constructor we can pass in a
25:04 - comparator but that's a little advanced
25:06 - for us and we haven't discussed that yet
25:08 - so there is a default comparator that we
25:10 - can use found within collections
25:12 - collections dot
25:14 - reverse
25:15 - order method so these elements will be
25:19 - in reverse order then
25:22 - in this example let's say that whichever
25:24 - student has the best gpa will receive i
25:27 - don't know a gold medal like the
25:28 - olympics and the second best student
25:30 - will receive a silver medal then a
25:32 - bronze medal and then every student
25:34 - after that i guess will receive nothing
25:37 - but that's okay though they got some
25:38 - free tutoring in the last example okay
25:40 - now let's change the data type let's say
25:42 - that these are now strings and let's put
25:45 - these in standard order so let's change
25:48 - these two strings maybe this student has
25:50 - a b
25:52 - this one has a c
25:55 - a
25:58 - f
25:59 - and
26:00 - what are we missing d
26:02 - if we have a priority queue of strings
26:04 - well then these elements will be in
26:06 - alphabetical order so if we need these
26:09 - in reverse alphabetical order again we
26:11 - can pass in a comparator and then one
26:13 - that we can use is from collections
26:16 - collections dot reverse
26:19 - order
26:20 - and these elements within our priority
26:23 - queue are now within reverse
26:25 - alphabetical order so yeah that's a
26:27 - priority queue think of it like a queue
26:29 - however we first sort these elements
26:32 - based on a certain priority we will pull
26:35 - and serve the elements with the highest
26:37 - priorities first and work our way to
26:39 - elements with lower priority so yeah
26:42 - that's a priority queue if you would
26:44 - like a copy of this code i'll post this
26:45 - to the comment section down below and
26:47 - well yeah those are priority queues in
26:49 - computer science
26:52 - hey what's going on everybody it's you
26:54 - bro hope you're doing well in this video
26:56 - we're going to discuss linked lists and
26:58 - computer science so sit back relax and
27:00 - enjoy the show
27:03 - now before we dive straight into linked
27:05 - lists we're going to take a closer
27:07 - examination of arrays and array lists we
27:09 - will see what disadvantages that these
27:11 - data structures have where linked lists
27:13 - excel at so we'll compare and contrast
27:16 - the differences between the two with
27:17 - what we understand with arrays and array
27:19 - lists these data structures store
27:21 - elements in contiguous memory locations
27:24 - in this demonstration i'm storing
27:25 - letters of the alphabet suppose that the
27:27 - first element of my array has a memory
27:29 - address of one two three fake street
27:31 - obviously these are not real memory
27:33 - addresses but this is how i like to
27:35 - think about things if this was a memory
27:37 - address then the next element in my
27:39 - array may have an address of one two
27:41 - five fig street then one two seven fake
27:44 - street one two nine fake street and then
27:46 - you just continue on in that pattern now
27:48 - arrays are fantastic at randomly
27:49 - accessing elements because they have an
27:51 - index but they're not so great at
27:53 - inserting or deleting elements
27:54 - especially when those elements are
27:55 - closer to the beginning of the array
27:57 - here's an example suppose i need to
27:59 - insert a new element at index three
28:01 - since this element is already occupied
28:03 - with a value i would need to shift my
28:04 - elements to the right in order to
28:06 - accommodate room for this new element so
28:08 - the process of shifting is cumbersome
28:11 - but once this element is empty then i
28:13 - can insert a new value so it's not that
28:15 - big of a deal if i have a small data set
28:17 - but imagine if i had one million
28:20 - elements i would need to shift my data
28:22 - up to that many times depending on the
28:24 - location of the insertion and the same
28:26 - concept applies with deletion as well we
28:28 - would shift our elements to the left to
28:29 - close the gap
28:32 - you're probably thinking dude why are
28:33 - you talking about arrays in a video
28:35 - about linked lists well where arrays
28:37 - have difficulty inserting and deleting
28:39 - linked lists actually have the advantage
28:41 - here is a representation of a linked
28:43 - list a linked list is made up of a long
28:45 - chain of nodes each node contains two
28:48 - parts some data that we need to store
28:50 - and an address to the next node in line
28:52 - also referred to as a pointer linked
28:54 - lists do not have an index the same way
28:56 - that arrays do but each node contains an
28:58 - address to where the next node is
29:00 - located so these nodes are
29:02 - non-contiguous they can really be
29:03 - anywhere within your computer's memory
29:05 - if our initial node has a memory address
29:08 - of one two three fake street like our
29:10 - array example then the next node in our
29:12 - linked list could have a memory address
29:14 - of maybe 101 help boulevard and another
29:18 - could be 404 nowhere lane then 666 crime
29:21 - circle each node knows where the next
29:24 - node resides i imagine this as if we're
29:26 - following a scavenger hunt or a series
29:28 - of clues to find the end of the linked
29:30 - list the tale each node has an address a
29:33 - clue as to where the next note is we
29:35 - begin at the head and work our way
29:37 - towards the tail following each clue
29:40 - each memory address found in each node
29:42 - then we know when we reach the end of
29:43 - our linked list when we check that
29:45 - address our pointer and it has a value
29:48 - of null that means we're at the tail
29:49 - we're at the end of our linked list
29:51 - inserting a node is easy in a linked
29:52 - list there's no shifting of elements
29:54 - involved wherever we need to place a new
29:56 - node we take the address stored in the
29:58 - previous node and assign the address of
30:01 - our new node with the address from the
30:04 - previous node so that our new node is
30:06 - pointing to the next node in line then
30:08 - we can take and replace the address in
30:10 - the previous node with an address that
30:12 - points to our new node it's as simple as
30:14 - that and we're completing our chain
30:16 - simply by inserting a node at a given
30:18 - location there's only a few steps
30:20 - involved no shifting of elements
30:21 - required deleting nodes are easy too
30:23 - wherever we need to delete a node we
30:25 - have the previous node point instead to
30:27 - the next node in line again no shifting
30:29 - of elements is necessary now this is
30:31 - where linked lists tend to be inferior
30:33 - to arrays they are bad at searching we
30:35 - can randomly access an element of an
30:37 - array because we have an index with a
30:39 - linked list that is not the case to
30:41 - locate an element we need to begin at
30:43 - the head and work our way towards the
30:45 - tail until we find the element that we
30:47 - are looking for this itself takes time
30:49 - in fact it would take linear time but
30:51 - making the insertion or deletion of a
30:53 - node is constant this variation of a
30:55 - linked list is a singly linked list
30:58 - there are single links to each node
31:00 - however there's another variation called
31:02 - a doubly linked list a doubly linked
31:05 - list requires even more memory to store
31:07 - two addresses in each node not just one
31:09 - which is the case with a singly linked
31:11 - list one address for the next node and
31:13 - another for the previous node in our
31:15 - chain the benefit of a doubly linked
31:17 - list is that we can traverse our doubly
31:20 - linked list from head to tail or from
31:22 - tail to head in reverse each node knows
31:24 - where the next and previous note is but
31:27 - the downside is that a doubly linked
31:29 - list uses even more memory than a singly
31:32 - linked list so how about we create a
31:33 - linked list in real life now let's do it
31:36 - alright ladies and gentlemen with all
31:37 - that out of the way let's create a
31:39 - linked list linked list list the data
31:42 - type of the objects we'll be storing
31:43 - within this linked list just strings
31:45 - because they're easy and i will name
31:47 - this linked list linked list linked list
31:49 - equals new linked list and list the data
31:53 - type again we are storing strings add a
31:54 - constructor boom you got yourself a
31:56 - linked list now if i was to take my
31:58 - cursor and hover over my linked list
32:00 - declaration there's a note here that
32:02 - says this is a doubly linked list each
32:05 - node knows where the previous and next
32:08 - nodes are now if we head to the linked
32:09 - list class itself there's a few things i
32:11 - need to mention here our linked list
32:13 - stores the memory location of our first
32:16 - and last nodes these are effectively the
32:19 - head and the tail of our linked list and
32:21 - there's also an inner class named node
32:23 - each node knows the memory address of
32:26 - the next and previous nodes within this
32:28 - linked list now taking a look at our
32:30 - linked list class definition our linked
32:32 - list class implements the dac interface
32:35 - and a deck is more or less a
32:36 - double-ended queue so with the deck
32:39 - interface we implement 12 additional
32:42 - methods so here's just a few of them so
32:44 - we can add to the head add to the tail
32:47 - remove the head remove the tail peek at
32:49 - the head peek at the tail some will
32:51 - throw exceptions some will return a
32:52 - special value so you can use any
32:54 - combination of these really and not only
32:56 - do we have these 12 methods but we can
32:58 - treat our linked list as either a stack
33:00 - or a queue we can push we can pop we can
33:02 - pull then we can offer so just to
33:04 - demonstrate let's first treat our linked
33:06 - list as a stack so linked lists do have
33:09 - a push method as well if we need to push
33:12 - an element onto our linked list as if it
33:14 - were a stack so let's push the letter a
33:17 - and then i will display my linked list
33:19 - with a print line statement
33:21 - system.out.printline linked list and of
33:23 - course we have the letter a so let's
33:25 - push another letter onto our stack what
33:27 - about b so at the bottom of our linked
33:29 - list we have a and then on top we have b
33:31 - let's add a couple more letters let's
33:33 - represent a typical grading scale we
33:35 - have c d and f a b
33:38 - c
33:39 - d f notice that i'm intentionally
33:42 - leaving out e we're going to insert that
33:44 - later so within our linked list that's
33:47 - behaving as a stack we have f on top
33:49 - then d c b and a so we also have access
33:52 - to a pop method as well linked list dot
33:56 - pop and this will pop the top of my
33:58 - linked list so f should no longer be
34:01 - here it's d c b and a so we can treat a
34:04 - linked list as a stack we can also treat
34:07 - it as a queue as well and just to save
34:09 - some time i'm going to copy these lines
34:11 - of code to add an element to a queue we
34:13 - do not use push we use offer so
34:16 - linkedlist.offer
34:19 - and we will keep the order
34:20 - so i'm not going to pop it quite yet
34:23 - so we have a b c d f a is at the head f
34:26 - is at the tail and to remove the head of
34:29 - our q we do not use pop we use pole
34:33 - and a is no longer in here we have bcdf
34:36 - so you can use a linked list to mimic a
34:38 - stack or a queue before we move on to
34:40 - the next section i'm going to get rid of
34:42 - this pull method so we have a typical
34:44 - grading scale a b c d f where linked
34:48 - lists are really good is the insertion
34:50 - and deletion of nodes let's say for this
34:52 - example i need to add a node between d
34:55 - and f that contains the letter e so
34:57 - that's really easy to do with the linked
34:58 - list we would type the name of our
35:00 - linked list dot add
35:02 - list and index like for
35:04 - and our object e and then to remove a
35:07 - node we would type the name of our
35:09 - linked list dot remove then list the
35:11 - object
35:12 - e so e should no longer be within my
35:15 - linked list so where linked lists tend
35:17 - to have an advantage over arrays and
35:19 - array lists is the insertion and
35:21 - deletion of nodes however there's one
35:23 - catch to this with a linked list we
35:26 - still need to traverse the entire linked
35:28 - list to find where we need to go unlike
35:30 - with arrays and array lists there's no
35:32 - random access to a linked list searching
35:34 - for an element is fairly straightforward
35:36 - too so within a print line statement i'm
35:38 - going to use the index of method of a
35:41 - linked list
35:43 - index of let's look for f
35:46 - so that would be
35:47 - at index four and before we wrap things
35:51 - up here here's a few methods related to
35:53 - linked lists that you might be
35:54 - interested in we can peek at the head or
35:56 - the tail node of our linked list so
35:59 - within a print line statement i'm going
36:00 - to print linked list dot and then use
36:03 - the peak first method so the first node
36:07 - within my linked list contains the
36:09 - letter a so we can peek last as well
36:12 - linkedlist.peak
36:14 - last and the last node of my linked list
36:17 - contains the letter f we can add new
36:19 - nodes at the head or the tail of our
36:21 - linked list by using the add first
36:23 - method for the head so maybe i need to
36:27 - add maybe zero because i don't really
36:29 - know what comes before a and the
36:30 - alphabet so zero would be a good bet i
36:32 - guess
36:33 - or we could add to the tail by using add
36:36 - last
36:38 - and after f comes g
36:41 - and we now have g at the tail of our
36:44 - linked list we can remove first and
36:46 - remove last you can also store them
36:48 - within a variable two let's say string
36:51 - first equals linked list dot
36:54 - remove first then to remove the last
36:57 - node
36:58 - we could just use remove
37:00 - last then
37:03 - and let's store this within a different
37:04 - variable remove last
37:07 - so yeah those are a few useful methods
37:09 - related to linked lists in conclusion
37:12 - everybody a linked list is a data
37:14 - structure that stores a series of nodes
37:16 - each node contains two parts some data
37:19 - and an address nodes are stored in
37:21 - non-consecutive memory locations each
37:24 - node can be really anywhere within your
37:26 - computer's memory and elements are
37:28 - linked via these pointers they contain
37:30 - an address for where the next node is
37:32 - we've discussed two varieties of linked
37:34 - lists a singly linked list as well as a
37:37 - doubly linked list with a singly linked
37:39 - list each node is made up of two parts
37:42 - some data and an address to traverse a
37:44 - singly linked list we would begin at the
37:47 - head node and use the address as a sort
37:49 - of clue to find where the next node is
37:51 - located within our computer's memory
37:53 - with a doubly linked list each node is
37:56 - made up of three parts some data and two
37:58 - addresses one address for the next node
38:01 - and another address for the previous
38:02 - node and it behaves the same way and to
38:04 - traverse a doubly linked list we could
38:06 - begin at the head and work our way
38:08 - towards the tail or we could begin at
38:10 - the tail and work our way towards the
38:12 - head depending on which way is closer to
38:14 - where we need to be within our linked
38:15 - list what are some of the advantages of
38:17 - a linked list one they're a dynamic data
38:19 - structure they can allocate needed
38:21 - memory while their program is currently
38:23 - running two insertion and deletion of
38:25 - nodes is really easy if you're familiar
38:27 - with big-o notation this would be in
38:29 - constant time there's only a few steps
38:31 - regardless of the size of our data set
38:34 - and three there is no to low memory
38:36 - waste what are some disadvantages one
38:39 - there is greater memory usage because we
38:41 - have to store an additional pointer each
38:43 - node also stores the address for where
38:45 - the next node is located and even more
38:48 - so with a doubly linked list this will
38:50 - use a lot more memory because we need
38:52 - two addresses for each node
38:54 - now two there's no random axis of
38:56 - elements within a linked list to find an
38:59 - element we need to begin at one end and
39:01 - work our way towards the other end and
39:04 - three accessing and searching of
39:06 - elements is more time consuming this is
39:08 - done in linear time this is where arrays
39:11 - and array lists have an advantage since
39:13 - they use indexing they can randomly
39:15 - access an element of an array or an
39:17 - array list with a linked list we have to
39:19 - manually traverse the entire linked list
39:22 - to get to a particular index since we
39:24 - don't have random access now what are
39:26 - some uses of linked lists one they could
39:28 - implement stacks or queues if you need a
39:30 - stack or queue for anything you could
39:32 - also use a linked list two maybe gps
39:35 - navigation so let's say you have a
39:37 - starting position and a final
39:38 - destination each step or stop along the
39:41 - way is kind of like a node and if you
39:42 - need to take a detour you can easily
39:44 - change insert or delete a node and
39:46 - recalculate how to get to your final
39:48 - destination three what about a music
39:50 - playlist so each song within a playlist
39:52 - might not necessarily be next to each
39:54 - other within your computer's memory you
39:56 - want your playlist to follow a certain
39:58 - order of songs so that could be another
40:00 - use of a linked list so those are linked
40:02 - lists if you would like a copy of all my
40:05 - notes here and my code i will post this
40:07 - to the comments section down below if
40:08 - you can give this video a thumbs up drop
40:10 - a random comment down below and well
40:12 - yeah those are linked lists
40:16 - what the heck is a dynamic array a
40:18 - dynamic array is an array with a
40:21 - resizable capacity if we need extra room
40:23 - for elements we can increase the
40:24 - capacity which we cannot normally do
40:26 - with a standard typical fixed size array
40:29 - dynamic arrays are also known as
40:31 - arraylist in java vectors in c plus plus
40:35 - arrays in javascript and lists in python
40:37 - here's an example of a static array and
40:39 - then we'll take a look at a dynamic
40:41 - array a static array has a fixed
40:43 - capacity we determine that capacity at
40:45 - compile time and we can't change it
40:46 - later normally in this example i have a
40:49 - static array with a capacity of six
40:51 - elements and a size of five elements
40:53 - that are currently occupied the last
40:55 - element is open so it's null each
40:57 - element has a memory address obviously
41:00 - these are not real memory addresses but
41:01 - this is how i like to think about things
41:03 - imagine that all of these memory
41:04 - addresses are houses and they're all
41:06 - next to each other now accessing an
41:08 - element is easy because we have index
41:10 - numbers to work with we can randomly
41:11 - access an element in o of one constant
41:14 - time the size of our data set doesn't
41:16 - matter however searching for a stored
41:18 - value still takes time because we need
41:19 - to begin at index zero and iterate over
41:22 - our array until we reach our value or
41:24 - the end in case we don't find it this is
41:26 - done in ofn linear time the larger the
41:29 - data set the time to finish will
41:31 - increase linearly and in the case of
41:33 - inserting or deleting that takes a
41:35 - linear time unless done at the end no
41:37 - shifting of elements is required however
41:39 - the closer we need to insert or delete
41:41 - to index zero we need to shift all
41:44 - elements that follow in order to make
41:46 - room for insertion or close any gaps in
41:48 - the case of deletion so if i need to
41:50 - insert a value at let's say index zero i
41:53 - have to shift all elements to the right
41:55 - by one to make room for this insertion
42:00 - and then we can insert a value now
42:02 - currently with our static array we're at
42:04 - capacity our array is full our size is
42:06 - equal to our capacity then in the case
42:08 - of deleting an element we need to shift
42:11 - all elements that follow after this
42:13 - index where we're making the deletion
42:15 - and shift everything once to the left so
42:17 - that would look like this
42:21 - and our size is back to five so there is
42:23 - one element that is open a major
42:25 - disadvantage of static arrays is that
42:28 - they have a fixed capacity we can't
42:30 - increase the capacity once the size of
42:32 - the elements reaches capacity in this
42:34 - separate example i have an array with a
42:36 - capacity of five elements and a size of
42:39 - five elements and it's completely full i
42:41 - can't decrease the capacity because the
42:43 - next memory block contains i don't know
42:45 - pictures of cats or something you do you
42:47 - i guess a dynamic array has its own
42:49 - inner static array with a fixed size
42:52 - once the inner static array of our
42:54 - dynamic array reaches capacity our
42:56 - dynamic array will declare and
42:58 - instantiate a new array with an
43:00 - increased capacity usually the amount
43:02 - that we increase the capacity by really
43:04 - varies depending on the programming
43:06 - language it's usually between 1.5 and 2.
43:09 - i just picked capacity times 2 for extra
43:11 - emphasis
43:12 - so what we'll do now is copy the
43:14 - elements over to our new array and these
43:17 - have different memory addresses than our
43:18 - original array so that would look
43:20 - something like this
43:25 - we now have a new array with double the
43:27 - capacity but like i said it really
43:29 - depends on the language that you're
43:30 - working with it's usually between 1.5
43:32 - and 2. this array has a size of 5
43:35 - elements that are full and a total
43:37 - capacity of 10 then if you need to
43:40 - shrink the capacity like if you're not
43:41 - using a lot of elements you can always
43:43 - just do the reverse process of what we
43:45 - did to increase it now with this new
43:48 - inner array the insertion and deletion
43:50 - of elements is really the same as a
43:52 - static array so you just shift all the
43:54 - elements to the right by one to insert a
43:56 - new element or shift all the elements to
43:58 - the left to delete an element what are
44:00 - some of the advantages of dynamic arrays
44:02 - one there is random axis of elements
44:04 - that is done in o of one constant time
44:07 - we can randomly access an element by an
44:09 - index number and retrieve the value 2.
44:12 - there is good locality of reference and
44:14 - data cache utilization because all of
44:16 - these memory addresses are contiguous
44:18 - they're right next to each other unlike
44:20 - with linked lists you have to jump
44:21 - around a lot because all of the memory
44:23 - addresses are kind of random and three
44:26 - it's easy to insert and delete elements
44:28 - at the end because there's no shifting
44:30 - of elements required
44:31 - and for the disadvantages a dynamic
44:33 - array wastes more memory than a linked
44:35 - list because we need to increase the
44:37 - capacity to accommodate more elements if
44:40 - we need the extra room and we may not
44:42 - necessarily need all of this extra room
44:45 - so a dynamic array wastes more memory
44:47 - than a linked list
44:49 - two shifting of elements is time
44:51 - consuming the closer we need to insert
44:53 - or delete closer to index zero we have
44:56 - to shift all elements that follow
44:58 - afterwards to the right in case of an
45:00 - insertion or to the left in case of a
45:03 - deletion and three expanding or
45:05 - shrinking the array is time consuming
45:07 - because we have to copy all the elements
45:09 - over to a new array with a different
45:11 - capacity and that's the basics of a
45:14 - dynamic arrays let's create our own
45:16 - dynamic array for practice all right
45:18 - welcome back we're going to create our
45:20 - own dynamic array using java in the
45:22 - future if you ever do need a dynamic
45:24 - array you might as well just use an
45:26 - array list according to the description
45:28 - it's a resizable array implementation of
45:30 - the list interface and it's pre-built so
45:32 - you might as well use it i thought we
45:34 - would create our own dynamic array just
45:35 - for learning purposes in practice but
45:38 - let's take a look at the arraylist class
45:40 - within this class there are a few
45:42 - defined members there's a default
45:44 - capacity set to 10. there are overloaded
45:46 - constructors within this arraylist class
45:49 - we can set our own initial capacity or
45:51 - we can use the default by not passing in
45:53 - an initial capacity there is a size to
45:56 - keep track of how many elements are
45:58 - filled within our array list and our
46:01 - arraylist does have its own inner static
46:04 - fixed size array and if we ever need to
46:06 - expand the size of this array we just
46:08 - copy the elements over to a new inner
46:11 - array so let's begin let's create a new
46:13 - class named dynamic array and i'll get
46:16 - rid of this so file new class
46:19 - and this will be named dynamic
46:22 - array
46:23 - then finish
46:24 - okay let's declare a few members let's
46:26 - create int size
46:29 - int capacity this will be the initial
46:31 - capacity i'll set this to 10 but feel
46:33 - free to pick whatever value that you
46:35 - want as well as an array of objects
46:38 - named array i will declare this but not
46:40 - yet instantiate it so you can make these
46:43 - private however i think that'll make our
46:45 - code a little more complex and difficult
46:47 - to understand although it'd be more
46:49 - secure i'm just going to use the default
46:51 - visibility for these members here all
46:53 - right let's create some overloaded
46:55 - constructors so public
46:57 - dynamic
46:59 - array
47:01 - and within here we will instantiate a
47:03 - new fixed size array
47:06 - this dot array
47:08 - equals new
47:11 - array of objects with a capacity of
47:14 - whatever capacity the default is so it's
47:17 - going to be 10 by default and we'll
47:19 - create an overloaded constructor just in
47:21 - case the user passes in their own
47:23 - capacity that they would like to set so
47:26 - int
47:27 - capacity
47:28 - this dot capacity equals
47:32 - whatever capacity that we pass in
47:34 - okay let's instantiate a new dynamic ray
47:38 - dynamic
47:40 - make sure to spell it right dynamic
47:42 - array i'll call this dynamic
47:45 - array with a lowercase d equals new
47:48 - dynamic
47:50 - array
47:51 - so i'm not going to pass in an initial
47:54 - capacity
47:55 - and let's print whatever the capacity is
47:57 - of our dynamic array dynamic array dot
48:01 - capacity
48:02 - and this should be 10. okay now let's
48:05 - pass in maybe
48:06 - a capacity of five
48:08 - and this should be five yep cool so it
48:11 - seems like that works all right let's
48:12 - head back to our dynamic array and
48:14 - declare all of the methods that we'll
48:16 - need let's create an add method public
48:19 - void add and there will be one parameter
48:22 - of object
48:24 - data
48:27 - next method insert
48:29 - public
48:31 - void
48:32 - insert
48:34 - the two parameters are
48:36 - int index
48:39 - object data okay next method we have
48:42 - delete
48:44 - public
48:45 - void
48:46 - delete
48:48 - there is one parameter of object data
48:53 - then we have search
48:56 - public and we're going to return it
48:58 - index
48:59 - search
49:04 - and we will need object data
49:07 - and let's return negative 1 for now then
49:11 - we have private
49:13 - void
49:14 - grow
49:15 - to expand the size of our array
49:18 - private
49:20 - void
49:21 - shrink
49:24 - then we'll need an is empty method
49:27 - public
49:28 - we will return a boolean value
49:31 - is
49:32 - empty
49:33 - and we might as well fill this in right
49:34 - away because there's only one line
49:36 - return size is equal to zero
49:40 - if our size is anything but zero we will
49:43 - return false
49:45 - and lastly tostring
49:48 - public
49:49 - string
49:50 - to
49:52 - string
49:53 - and i need to type in something i'm just
49:55 - going to return null for the time being
49:57 - until we return something
49:59 - okay let's begin by filling in the add
50:01 - method first we'll want to check to see
50:03 - if we're at capacity if
50:06 - our size is greater than or equal to our
50:09 - capacity then we better call the grow
50:12 - method to expand the size of our array
50:15 - so if there is room we will take our
50:17 - array
50:18 - at index of size that should be the end
50:21 - of our array
50:22 - equals
50:23 - data then we will increase our size by
50:27 - one now let's head all the way down to
50:29 - the tostring method to display the
50:31 - elements of this array and we just need
50:33 - to iterate over these let's declare
50:36 - a local variable of string string string
50:39 - and i will set this equal to an empty
50:41 - string and we will fill in the elements
50:43 - when we iterate over it
50:45 - so let's iterate over the elements of
50:47 - our array so let's create a for loop for
50:50 - int i equals zero
50:53 - and then i will continue this for loop
50:55 - as long as i is less than our size you
50:59 - can do capacity too if you want to see
51:01 - the entire array but let's begin with
51:03 - size i is less than size and i will
51:06 - increment our index i by one
51:09 - so i'm going to take our string and
51:12 - append it string plus equals
51:15 - our array at index of i
51:18 - that's one i
51:20 - plus maybe i'll add a comma then a space
51:23 - then we should return our string string
51:26 - okay this isn't perfect yet but let's at
51:29 - least test it let's head back to our
51:30 - main java file and add a few elements to
51:33 - our array using the add method so let's
51:36 - use the default capacity of 10 so we
51:38 - don't necessarily need to pass in
51:40 - anything so to add to our array we can
51:42 - use dynamic array dot and we declared an
51:46 - add method at the top so let's add maybe
51:50 - some letters i will add the letter a
51:53 - then b
51:54 - then c
51:55 - that should be good
51:56 - so a
51:57 - b
51:59 - and c
52:00 - and then let's call the tostring method
52:03 - system.out.printline and with the
52:05 - tostring method we only have to type in
52:07 - the name of what we would like to
52:09 - display the elements of so dynamic array
52:12 - and we don't necessarily need to type
52:13 - tostring so this should display
52:16 - a b and c now let's format this and
52:19 - clean it up a little bit like i would
52:20 - like to get rid of the last comma here
52:23 - and maybe enclose all of these elements
52:24 - within a set of square brackets so this
52:27 - is what we can do within the tostring
52:29 - method
52:30 - so after the for loop let's check to see
52:33 - if
52:35 - our string
52:36 - does not equal
52:38 - an empty string
52:39 - if that is the case if there are
52:41 - elements to display
52:43 - let's take our string
52:45 - then i'm going to create a substring and
52:46 - get rid of these last two characters the
52:49 - comma and the space so string equals
52:52 - string dot substring
52:55 - and the length is going to be beginning
52:57 - at index 0 and i will continue this
53:00 - until
53:01 - string
53:02 - dot length method
53:05 - minus two then after running this one
53:07 - more time the common space at the end
53:09 - should no longer be there because we
53:11 - created a substring to end at the last
53:13 - element then let's enclose all of these
53:15 - elements within a set of square brackets
53:17 - so i'll use some string concatenation so
53:20 - i'll add a
53:21 - left square bracket
53:24 - and then at the end add a right square
53:27 - bracket
53:30 - and then these should be within square
53:32 - brackets now and that looks a lot better
53:35 - now what if our string is empty let's
53:38 - return it just a set of square brackets
53:40 - using an else statement else
53:43 - we will set our string
53:45 - equal to
53:47 - a set of square brackets and that's it
53:50 - so let's head back to our main java file
53:53 - and comment these lines of code out
53:55 - where we add elements to our dynamic
53:57 - array
53:58 - so let's run this and we should have an
54:00 - empty set of square brackets actually
54:02 - this would be a good opportunity to test
54:04 - our is empty method so let's check that
54:07 - so within a print line statement
54:09 - system.out.printline
54:11 - i will take my dynamic array
54:14 - and use the is empty method
54:17 - then i'm just going to use some string
54:18 - concatenation
54:21 - empty colon space
54:23 - plus dynamic array is empty method and
54:27 - our dynamic array is currently empty
54:29 - that is true then let's fill this with
54:31 - elements a b and c
54:34 - so this should iterate and display the
54:37 - elements of our array and let us know if
54:39 - our array is empty which is false since
54:42 - we're here let's display the size and
54:43 - the capacity of our array too so
54:46 - system.out.printline dynamic array
54:49 - dot size
54:51 - and i'll use some string concatenation
54:53 - here too so size colon space
54:56 - plus dynamic array dot size and the
54:59 - capacity as well so
55:02 - capacity
55:03 - plus dynamic array dot capacity
55:07 - so this dynamic array
55:10 - has a size of three three elements are
55:12 - filled in and a capacity of ten for fun
55:16 - just to see the entire array let's go to
55:18 - the tostring method and change size to
55:20 - capacity so we can see all of the
55:23 - elements that are filled in and not
55:24 - filled in so after running this we can
55:27 - see our entire array at its full
55:28 - capacity so we have a size of three
55:31 - three elements are filled in but we have
55:32 - a total capacity of 10. the rest of the
55:34 - elements are null so if we were to count
55:37 - all of these they should be 10 so we
55:38 - have one two three four five six seven
55:42 - eight nine ten nice
55:44 - so you can change that back to size or
55:46 - you can keep it as capacity i'll just
55:47 - keep it as capacity for teaching
55:49 - purposes now let's fill in the insert
55:52 - method there's not a whole lot left to
55:54 - do
55:54 - first let's check to see if our size is
55:57 - greater than or equal to our capacity if
55:59 - so then we'll need to grow our array so
56:02 - size is greater than or equal to our
56:06 - capacity if that is the case call the
56:08 - grow method what we're going to do at
56:10 - this point is shift all of the elements
56:12 - that are filled in to the right in order
56:14 - to make room for the insertion so let's
56:16 - use a for loop and iterate over our
56:19 - filled elements in reverse order i will
56:21 - set into i art index
56:24 - equal to our size and then i will
56:27 - continue this as long as i is less than
56:31 - our index then decrement i by one so i'm
56:35 - going to take our array
56:37 - at i and set this equal to array
56:41 - at index of i minus one this will shift
56:44 - all of the elements over to the right to
56:46 - make room for the insertion so we will
56:48 - take our array
56:51 - at index
56:52 - equals whatever data we want to set then
56:55 - increase our size by one so then if we
56:58 - head back to our main java file we can
57:00 - insert a value at a given index so let's
57:04 - take our dynamic array
57:06 - dot use the insert method
57:08 - let's say at index 0 i would like to
57:10 - insert an x
57:12 - so let's try it
57:14 - cool we have x a b c the size is now 4
57:17 - and the capacity is still 10. now let's
57:19 - work on the delete method within here
57:21 - we're going to iterate over the elements
57:23 - of our array beginning from left to
57:25 - right so this is fairly easy int i
57:27 - equals zero we will continue this as
57:30 - long as i is less than our size and
57:33 - increment i by one after each iteration
57:36 - so during each iteration we will check
57:38 - to see if our array at index of i
57:41 - is equal to the data that we pass in as
57:44 - an argument so if that is the case we
57:46 - need to shift all of the elements to the
57:49 - left then so we'll need a nested for
57:52 - loop for that then we will need an index
57:54 - of j because i is already taken we're
57:56 - within a nested for loop int j
57:59 - equals zero and i will continue this
58:02 - nested for loop as long as j
58:04 - is less than our size minus i
58:08 - minus one
58:10 - and then we are going to increment our
58:12 - index j by one during each iteration so
58:15 - basically wherever we make the deletion
58:17 - we're going to shift all of the elements
58:19 - afterwards one spot to the left so we
58:22 - will take our array at index of i plus j
58:27 - and set the sequel to our array at index
58:30 - of i
58:31 - plus j the same as before but add plus
58:33 - one so that will target the next element
58:36 - that comes afterwards
58:37 - so after we escape this for loop we will
58:41 - take our array at index of size
58:44 - -1
58:45 - and set the sequel to null
58:48 - and then we will decrement our size by
58:51 - one and actually here would be a good
58:54 - place to shrink our array so let's write
58:56 - an if statement and check to see if our
58:58 - size falls below a certain criteria so
59:01 - let's say that if our size is less than
59:04 - or equal to a third of the capacity so
59:07 - capacity
59:09 - divided by three we don't want to shrink
59:11 - too often just because that's time
59:13 - consuming and then you may want to cast
59:16 - this as an int because it may not divide
59:18 - evenly
59:19 - so if our size is underneath a third of
59:21 - the capacity let's call the shrink
59:24 - method and we will shrink our array by
59:26 - maybe half but we'll get to that later
59:29 - so then we want to break to escape this
59:31 - for loop then okay let's try this then
59:35 - so after making the insertion let's
59:37 - delete what about a so dynamic array
59:41 - dot delete
59:43 - and i do not need to pass in an index
59:45 - just the data that i'm looking for
59:48 - all right so
59:49 - a is no longer in here we have x b and c
59:52 - the size is three and the capacity is
59:54 - still ten all right i promise we're
59:56 - almost finished let's fill in the search
59:58 - method next and this one is fairly short
60:00 - so we just need to iterate over the
60:02 - elements of our array beginning at index
60:04 - zero four and i
60:06 - equals zero i will continue this as long
60:09 - as i is less than the size of our array
60:12 - increment i by one
60:14 - if our array
60:16 - at index of i
60:19 - is equal to the data that we're looking
60:21 - for the data that we pass in as an
60:23 - argument then we will return whatever i
60:26 - is our index if we do not find it we
60:29 - return negative one that's kind of like
60:31 - a sentinel value that means we did not
60:33 - find the value that we're looking for
60:35 - okay so let's search for maybe c
60:38 - dynamic
60:40 - array dot search
60:43 - and i will pass in the data that i'm
60:45 - looking for i am looking for c so that
60:47 - should be 0 1 2 assuming we insert and
60:51 - delete some values later and then i'm
60:53 - going to place this within a print line
60:55 - statement so dynamic ray
60:58 - dot search and i am searching for c
61:02 - and our result is that c is at index two
61:06 - zero one two all right we're near the
61:08 - end let's grow and shrink our ray and
61:10 - i'll turn these lines into comments
61:13 - now for the grow method we're going to
61:15 - instantiate a new array but we'll
61:17 - increase the capacity first int
61:20 - new capacity
61:22 - equals
61:23 - our old capacity which is just named
61:25 - capacity and let's say we want to
61:27 - increase the capacity by two
61:30 - and then i will just cast this as an end
61:32 - okay so after we create a new capacity
61:35 - we will instantiate a new array then we
61:38 - need to copy the elements over so we'll
61:40 - have an array of objects named new
61:43 - array equals
61:45 - new
61:47 - array of objects with a capacity of our
61:50 - new capacity
61:52 - and then we need to copy the elements
61:54 - over to our new array and that's kind of
61:56 - time consuming but necessary
61:58 - so we begin at index zero for int i
62:02 - equals zero we will continue this as
62:04 - long as i is less than
62:07 - our size
62:08 - and i will increment this by one after
62:10 - each iteration so we will take our new
62:14 - array
62:15 - at index of i and set this to our old
62:19 - array just named array at index of i
62:23 - and then we will change the capacity
62:26 - to whatever new capacity is
62:29 - then lastly we will set our array to
62:32 - equal our new array
62:35 - then let's test it
62:37 - so i'm going to maybe add a bunch of
62:39 - elements i'll keep that as a comment
62:42 - so let's change the capacity of our
62:45 - array to five i'll pass in five into the
62:47 - constructor so we have less elements to
62:49 - work with so the size is three and the
62:50 - capacity is five i'm going to add
62:52 - another element
62:54 - let's try d
62:56 - so size four capacity five
63:00 - let's add e
63:03 - okay so currently our array is full now
63:06 - let's try to increase the size past the
63:09 - capacity so i will add f
63:11 - and this should increase and grow the
63:14 - size of our array so we have a capacity
63:16 - of 10 now and we have a bunch of empty
63:18 - elements and lastly we just need to
63:20 - shrink this array and this next part is
63:22 - super simple for the shrink method copy
63:24 - everything from the grill method and
63:27 - paste it within the shrink method but
63:29 - change capacity at times to to capacity
63:32 - divided by two now we will call the
63:35 - shrink method automatically when the
63:38 - size falls below a third of the capacity
63:40 - that means we have a lot of wasted
63:42 - memory now let's begin deleting elements
63:46 - so i will type dynamic ray dot delete a
63:49 - then maybe b
63:51 - so when the size is a third of the
63:53 - capacity that's when it should shrink so
63:56 - we're not there yet let's delete maybe
63:58 - one or two more times
63:59 - so let's delete c
64:03 - and there we go so the size is three and
64:05 - the capacity is now five
64:07 - well all right that's a very basic
64:09 - dynamic array if you're using java
64:11 - instead of just building your own
64:12 - dynamic array you might as well just use
64:14 - an array list because it's more
64:16 - efficient and well it's already coded
64:17 - for you but i think this was good
64:19 - practice for us just to understand how
64:21 - dynamic arrays work so if you would like
64:24 - a copy of all this code of course i will
64:26 - post this to the comment section down
64:27 - below if you made it all the way to the
64:29 - end please give this video a thumbs up a
64:31 - random comment down below and well yeah
64:33 - those are dynamic arrays and well
64:35 - computer science
64:37 - hey what's going on everybody it's bro
64:39 - hope you're doing well and in this video
64:41 - we're going to run some tests on both
64:43 - array lists and linked lists in java so
64:46 - sit back relax and enjoy the show
64:50 - all right let's begin we'll need both a
64:52 - linked list and an array list so linked
64:54 - list the data type will be integers and
64:57 - i will name this linked
64:59 - list equals new linked list again the
65:02 - data type is integer add a constructor
65:05 - boom we got ourselves a linked list and
65:07 - now we'll need an arraylist change
65:09 - linked to array
65:11 - this will be named arraylist
65:15 - make sure to pay attention the
65:16 - capitalization equals new arraylist and
65:20 - we'll need to keep track of the time
65:21 - we'll need a start time and time and
65:23 - elapsed time and these will be of the
65:26 - long data type we'll be working with
65:27 - nanoseconds so a long is preferable to
65:30 - an end a long is really just a really
65:32 - long integer think of it that way so we
65:34 - have start time
65:36 - end time
65:39 - and elapsed time
65:44 - okay and now we'll need to populate both
65:46 - our linked list and our array list and
65:48 - we can do that with a for loop we'll
65:50 - need a large sample size maybe one
65:52 - million elements that'll be good so int
65:54 - i equals zero we'll continue this as
65:57 - long as i is less than a million and i
66:00 - believe that's about a million yeah
66:02 - we're good okay then increment i by one
66:04 - during each iteration so i'm going to
66:06 - add i to both my linked list and my
66:09 - arraylist
66:10 - so we can add primitives because java
66:13 - has an autoboxing feature so even though
66:15 - these are integer objects we can still
66:17 - add primitives and be sure to add to our
66:19 - arraylist as well so arraylist.ad i okay
66:23 - let's move on now we'll keep track of
66:25 - the time we'll need to assign the start
66:27 - time of value and end time of value
66:29 - before and after our operation so let's
66:32 - say that start time
66:34 - equals and we can get the current time
66:36 - of our jvm by using system dot nano time
66:41 - method and let's take a look
66:43 - so this returns the current value of the
66:46 - running java virtual machine's time
66:48 - source in nanoseconds so we will get the
66:51 - start time after we do something i'll
66:53 - just add a note here do
66:55 - something then we will get the end time
66:58 - so end time equals
67:01 - system dot nano time method
67:04 - and to calculate the elapsed time that
67:06 - will be elapsed time equals end time
67:10 - minus start time
67:12 - and then we'll probably want to display
67:13 - this so let's do that
67:15 - system.out.printline
67:17 - and let's say
67:19 - linked
67:20 - list
67:21 - i'll add a tab too
67:24 - plus
67:25 - elapsed
67:26 - time plus
67:29 - ns4 nanoseconds
67:31 - and let's just run this to test it so
67:33 - this portion of code really took
67:35 - right between here took 400 nanoseconds
67:38 - okay let's actually do something now so
67:41 - let's get the first element within our
67:43 - linked list linked list
67:45 - dot get zero
67:48 - and let's see how long this takes
67:50 - okay thirteen thousand two hundred
67:52 - nanoseconds now with an arraylist let's
67:55 - copy everything that we have for our
67:57 - linked list
67:58 - and change linkedlist to arraylist
68:03 - and be sure to do that here as well
68:05 - array
68:06 - list and let's compare these so we are
68:08 - getting the index of zero for both our
68:10 - linked list and arraylist and we will
68:12 - compare them
68:14 - and it appears that our arraylist is
68:16 - slightly faster our linked list took 11
68:19 - 800 nanoseconds and our arraylist took
68:22 - 6700
68:24 - so it looks like getting the first
68:26 - element of our linked list is going to
68:28 - be a little bit slower than our
68:29 - arraylist this time let's get something
68:31 - right in the middle of our linked list
68:33 - and arraylist so i'm going to turn this
68:35 - line to a comment and we will get the
68:38 - index of 500 000 so that's right in the
68:42 - middle and do the same thing with
68:44 - arraylist as well
68:45 - so arraylist
68:47 - dot get
68:48 - 500
68:50 - 000. and let's see how long this takes
68:54 - okay so our linked list took way longer
68:57 - than our array list our linked list took
69:00 - 7.5 million nanoseconds compared to our
69:02 - arraylists 6900 nanoseconds okay so
69:06 - that's still way longer with a linked
69:08 - list now let's try something near the
69:10 - end of our linked list and array list
69:12 - what about the last element
69:15 - linked list die get nine hundred ninety
69:17 - nine thousand nine hundred ninety nine
69:19 - so that is the last index in our linked
69:22 - list and array list we have one million
69:24 - elements and this is exclusive and the
69:26 - first element has an index of zero so
69:29 - let's do the same thing with arraylist
69:33 - arraylist die get 999 99999 and here are
69:38 - the results our linked list took 63 000
69:41 - nanoseconds compared to our arraylist 17
69:44 - 000. now the reason that our linked list
69:46 - took less time to retrieve and index at
69:49 - the end is because this linked list is a
69:52 - doubly linked list so we can begin at
69:54 - the head and work our way towards the
69:55 - tail or begin at the tail and work our
69:57 - way backwards to the hud so since this
69:59 - index is right at the end it's actually
70:01 - going to be very easy to retrieve this
70:04 - index whereas in the middle is actually
70:06 - going to be the worst possible spot for
70:08 - a linked list because we can start at
70:10 - either end but it's still going to take
70:11 - the same amount of time to get to the
70:12 - middle so it appears that accessing an
70:14 - element from an arraylist is always
70:16 - faster than a linked list and that's to
70:18 - be expected because with an arraylist we
70:20 - have a random access of elements unlike
70:22 - with a linked list we have to begin at
70:24 - one end of our linked list and traverse
70:26 - our linked list until we get to the
70:28 - index that we're looking for now let's
70:30 - add or remove an element from our linked
70:32 - list and array list maybe just remove
70:34 - because it's going to take really the
70:35 - same amount of time so linked list
70:38 - dot remove and let's remove the first
70:41 - element so index zero and do the same
70:44 - thing with arraylist array
70:47 - list dot remove
70:49 - index zero and let's take a look again
70:53 - okay so it appears that our linked list
70:56 - was actually faster this time 17 000
70:58 - nanoseconds compared to
71:00 - 2.2 million nanoseconds of an arraylist
71:04 - and the reason that our arraylist took
71:06 - longer is because we need to shift all
71:08 - elements to the left after removing an
71:10 - element so we had to shift one million
71:12 - elements after removing the first
71:14 - element now let's remove something near
71:16 - the middle so let's remove index number
71:19 - five hundred thousand
71:21 - so linked list dot remove five hundred
71:24 - thousand and do the same thing with the
71:26 - arraylist
71:27 - be sure to comment this line out
71:30 - arraylist dot remove
71:32 - 500
71:33 - 000.
71:34 - and here are the results
71:36 - so looks like our linked list took a lot
71:38 - longer this time 7 million nanoseconds
71:41 - compared to 1.6 million nanoseconds for
71:44 - an arraylist so with our arraylist there
71:46 - were less elements to shift this time
71:49 - because we were right in the middle but
71:50 - with our linked list we still had to
71:52 - navigate to the middle to remove one of
71:54 - these elements and then let's remove the
71:57 - element at the end of both our linked
71:59 - list and our array list
72:00 - linked list that remove
72:03 - 999 999 and do the same thing with
72:06 - arraylist arraylist dot remove
72:10 - 999
72:12 - 99999
72:14 - and this time our linked list is just
72:17 - slightly slower than our arraylist
72:20 - adding or removing elements near the end
72:21 - of an array list is actually fairly easy
72:24 - the closer that we insert or delete near
72:26 - the end the less time it's going to take
72:28 - because we have to shift less elements
72:30 - and with a linked list well this is a
72:32 - doubly linked list so accessing the last
72:35 - element doesn't really take too long if
72:37 - it's near the middle it's going to take
72:38 - forever actually so it seems that in
72:40 - most situations an arraylist is going to
72:43 - be better than a linked list however if
72:45 - you have to do a lot of inserting or
72:47 - deleting especially if it's a large set
72:49 - a linked list might be better but it
72:51 - seems that an arraylist is going to be
72:53 - more flexible for most applications so
72:56 - that is linked lists versus arraylists
72:58 - if you would like a copy of all this
73:00 - code i will post this to the comments
73:02 - section down below and well yeah that's
73:04 - linked lists versus arraylists and
73:06 - computer science
73:08 - hey what's going on everybody it's you
73:10 - bro hope you're doing well and in this
73:12 - video i'm going to describe the very
73:14 - basics of big o notation in well
73:16 - computer science so sit back relax and
73:19 - enjoy the show
73:22 - oh yeah big o notation so a common
73:25 - phrase used with big o notation is how
73:28 - code slows as data grows it describes
73:31 - the performance of an algorithm as the
73:33 - amount of data increases and really this
73:35 - notation is machine independent what
73:38 - we're really focusing on is the number
73:40 - of steps to complete an algorithm
73:42 - because some machines are going to run
73:43 - certain algorithms faster than others
73:46 - and three we tend to ignore smaller
73:48 - operations if we had some task that took
73:51 - n plus one we would just reduce it to
73:54 - just n because that plus one really
73:56 - isn't going to make a difference so
73:57 - here's a few examples of big-o notation
74:00 - we have o of 1 o of n o of log n and o
74:05 - of n squared and n is really just the
74:07 - amount of data that we're passing in
74:09 - it's a variable like x what we're
74:11 - focusing on is the performance of an
74:13 - algorithm as the amount of data
74:15 - increases and n is a representation of
74:18 - the amount of data that we're passing in
74:20 - here's a more concrete example i have a
74:22 - function named add up we will add up to
74:24 - a certain number depending on what we
74:26 - pass in as an argument the for loop
74:28 - within here will iterate once up to
74:30 - whatever number that m is add i to sum
74:33 - then return it what if n was a large
74:36 - number like a million well it's going to
74:38 - take just above a million steps to
74:40 - complete this function this function is
74:42 - said to have a runtime complexity of oh
74:45 - then linear time as the amount of data
74:48 - increases it's going to increase the
74:50 - amount of steps linearly or
74:52 - proportionally now another way in which
74:54 - we could write the same function is to
74:57 - take sum equals n times n plus one
75:00 - divided by two we will get the exact
75:03 - same sum so if n was let's say a million
75:06 - this is still only going to take a
75:07 - couple steps three steps not a million
75:10 - steps so this function is going to have
75:13 - a runtime complexity of o of one the
75:15 - input size doesn't matter the amount of
75:17 - data that we have really doesn't matter
75:19 - it's going to be completed in the same
75:21 - amount of steps so this function is way
75:24 - better than this previous one three
75:26 - steps is better than a million steps and
75:29 - the reason that this isn't o of three
75:31 - because this takes three steps is that
75:33 - we really don't care about smaller
75:34 - operations in the grand scheme of things
75:36 - they really won't make much of a
75:38 - difference so we would just shorten this
75:39 - and say that this is o of one constant
75:41 - time the input size doesn't matter
75:43 - here's a graph that i made to represent
75:45 - the different runtime complexities we
75:47 - have data on the x-axis represented by n
75:51 - so data will increase as we go more to
75:53 - the right and we have time on the y axis
75:57 - i did add an asterisk next to time
75:59 - because some of these times can vary
76:00 - depending on what machine you're using
76:02 - so these can vary another way to look at
76:04 - this is number of steps so as we
76:07 - increase on the y-axis these algorithms
76:10 - are going to perform with increasingly
76:12 - more time they're going to get slower so
76:14 - let's begin with o of one constant time
76:17 - anything that has a runtime complexity
76:19 - of o of 1 will take the same amount of
76:21 - time regardless of the data size a few
76:23 - examples would include the random access
76:26 - of an element within an array and
76:28 - inserting at the beginning of a linked
76:29 - list so o of 1 is extremely fast next we
76:33 - have o of log n also known as
76:36 - logarithmic time one example is binary
76:38 - search we haven't talked about this yet
76:40 - we'll talk about a few of these
76:41 - algorithms in future videos but anything
76:44 - that has a runtime complexity of o of
76:46 - log n will actually take i don't want to
76:49 - say less and less time but increasingly
76:51 - less time to complete so as the data
76:54 - size increases this algorithm is going
76:56 - to be more and more efficient compared
76:58 - to the early stages with a small data
77:00 - set of n is also known as linear time as
77:04 - the amount of data increases the time it
77:06 - takes to complete something will
77:08 - increase proportionally linearly that
77:10 - would include looping through the
77:12 - elements in an array and searching
77:14 - through a linked list next we have o of
77:16 - n log n also known as quasi-linear time
77:20 - this would include a quick sort merge
77:22 - sort and heap sort we'll talk about
77:24 - these concepts in future videos so for
77:26 - the most part this is very similar to
77:28 - linear time unless we're working with a
77:30 - large data set then anything using o of
77:33 - n log n is going to start to slow down
77:36 - when we work with larger data sets hence
77:39 - this curve here then we have o of n
77:41 - squared also known as quadratic time a
77:44 - few examples would include insertion
77:46 - sort selection sort and bubble sort as
77:48 - the amount of data increases it's going
77:50 - to take increasingly more and more time
77:52 - to complete anything that has a runtime
77:55 - complexity of o of n squared so just to
77:58 - compare linear time and quadratic time
78:01 - with linear time if our data set was a
78:03 - thousand if n equals a thousand it's
78:05 - going to take a thousand steps they're
78:07 - proportional they're linear but if we
78:09 - were using quadratic time if n was a
78:11 - thousand then a thousand squared would
78:14 - be a million so if our data set was a
78:16 - thousand if we're using quadratic time
78:18 - it's going to take a million steps then
78:21 - way more than linear time so quadratic
78:23 - time is extremely slow with large data
78:26 - sets but in the case of a small data set
78:28 - it could actually be faster as you can
78:30 - see by the graph here and lastly we have
78:32 - o of n factorial also known as factorial
78:35 - time and one place where this is used is
78:38 - with the traveling salesman problem
78:40 - which i might discuss in a future video
78:42 - so this is extremely slow so if i had to
78:45 - give a letter grade to each of these
78:47 - runtime complexities when working with
78:49 - large data sets with constant time this
78:52 - would be an a like an a plus logarithmic
78:54 - time would be a b it's pretty good
78:56 - linear time is c it's okay quasi-linear
79:00 - time is a d it's just barely passing
79:02 - quadratic time would be an f and
79:04 - factorial time well you get expelled
79:06 - from school although some of these
79:08 - runtime complexities are actually faster
79:11 - when working with a smaller data set so
79:13 - keep that in mind well that's the very
79:16 - basics of big o notation it's notation
79:19 - used to describe the performance of an
79:21 - algorithm as the amount of data
79:23 - increases so if you can give this video
79:25 - a thumbs up drop a random comment down
79:27 - below and well that's big o notation in
79:30 - computer science
79:34 - all right everybody linear searches we
79:37 - need to talk about linear searches
79:38 - because this wouldn't be a complete
79:40 - course without them with a linear search
79:42 - we iterate through a collection one
79:45 - element at a time the runtime complexity
79:48 - of a linear search is big o of n the
79:50 - larger the data set the number of steps
79:52 - to complete that search will increase
79:54 - proportionately the disadvantages of a
79:57 - linear search is that they are slow for
80:00 - large data sets but with the advantages
80:02 - they are fast for searches of small to
80:05 - medium sized data sets and they don't
80:07 - need to be sorted that's a huge benefit
80:10 - over binary searches and interpolation
80:12 - searches and they are useful for data
80:15 - structures that do not have random
80:17 - access such as linked lists so let's
80:19 - begin let's create a basic array of
80:21 - integers
80:23 - int array
80:25 - and to make up some numbers they don't
80:26 - necessarily need to be in order
80:31 - all right then let's find an index int
80:33 - index equals and we will invoke a linear
80:37 - search function which we still need to
80:39 - declare we will pass in our array and
80:42 - some value we would like to search for
80:44 - uh let's search for the number one okay
80:47 - so then let's declare this function
80:49 - create method linear search private
80:51 - static and linear search so we have two
80:54 - parameters an integer array and an
80:57 - integer i'm going to rename i as value
80:59 - so it's more descriptive
81:01 - okay with a linear search all we need to
81:04 - do is loop through our array one element
81:07 - at a time so we can do that with the for
81:09 - loop
81:11 - so let's set
81:13 - into i our index equal to zero we will
81:17 - continue this as long as i is less than
81:20 - our arrays
81:21 - length then increment i by one what
81:24 - we're checking with an if statement is
81:26 - to see if our array at index of i
81:31 - is equal to the value that we're
81:33 - searching for this parameter
81:35 - if it is then let's return whatever our
81:37 - index is i
81:39 - if we do not find it after iterating
81:41 - through our entire array let's return it
81:44 - negative 1 as a sentinel value and
81:47 - that's all there is to it to our linear
81:48 - search function so back within our main
81:51 - function let's check to see if the value
81:53 - returned does not equal negative one
81:56 - that means that we found our value so
81:58 - with an if else statement
82:02 - let's check to see if index does not
82:05 - equal negative one that means that we
82:07 - have found our element so let's print
82:10 - element found at
82:13 - index
82:14 - plus index
82:17 - else let's print element not found
82:20 - system.out.printline
82:22 - element not found
82:24 - so if we're searching for the number one
82:27 - we would find that at index one
82:30 - zero one if we search for five
82:34 - that is found at index eight
82:36 - zero one 2 3 4 5 6 7 8. if there's some
82:40 - number that's not in here like 10
82:42 - then this will print element not found
82:45 - so yeah that's the idea behind a linear
82:47 - search we iterate through some
82:49 - collection one element at a time it's
82:51 - slow for large data sets but it's fast
82:54 - for small to medium data sets this would
82:56 - be a small data set and they do not need
82:58 - to be sorted that's a huge advantage so
83:01 - yeah everybody that is a linear search
83:04 - if you would like a copy of this code
83:05 - i'll post this to the comment section
83:07 - down below and well yeah that is a basic
83:10 - linear search in computer science i
83:12 - guess
83:13 - hey everyone it's you bro hope you're
83:15 - doing well and in this video i'm going
83:17 - to explain the binary search algorithm
83:20 - in computer science so sit back relax
83:23 - and enjoy the show
83:26 - all right binary search it's a search
83:29 - algorithm that finds the position of a
83:31 - target value within a sorted array or
83:34 - other collection in order for a binary
83:36 - search to work whatever we're searching
83:38 - through it needs to be sorted and
83:40 - basically what we do is we take half of
83:42 - the array and eliminate it during each
83:44 - step and we will whittle down our
83:45 - collection until there is only one
83:47 - element remaining
83:49 - in this example i have an array of 11
83:51 - elements each element contains a letter
83:53 - and they're all sorted alphabetically
83:55 - let's say we are looking for the letter
83:57 - h and i need the index what we would do
83:59 - with a binary search is that we always
84:01 - begin in the middle we first check to
84:03 - see if our target value is equal to this
84:06 - middle value if these are equal then we
84:08 - can return this index but odds are
84:10 - they're probably not going to be equal
84:12 - on the very first turn the very first
84:14 - step so these are not equal then we will
84:17 - check to see if our target value is
84:20 - greater than or less than this middle
84:22 - value since h is greater than f we can
84:25 - disregard this entire first half of our
84:28 - array because since this is sorted our
84:30 - target value could not possibly be
84:32 - within this first portion and then we
84:35 - begin step two or phase two it's the
84:37 - same process as before so again we begin
84:39 - in the middle check to see if our target
84:42 - value is equal to the middle value
84:44 - they're not check to see if our target
84:46 - value is greater than or less than the
84:49 - middle value this time h is less than i
84:52 - we would delete the second half of this
84:54 - portion we're not actually deleting
84:56 - these values we're disregarding them and
84:58 - then we can move on to step three we're
85:00 - repeating the same process as before and
85:03 - this time these elements divide evenly
85:05 - so we would just round down and begin
85:07 - and say that this is the middle so h is
85:10 - greater than g we would disregard this
85:13 - element and we only have h remaining so
85:16 - we would return this index because these
85:18 - values are equal and that's a binary
85:21 - search now a binary search isn't too
85:23 - efficient when working with small data
85:25 - sets however if you're working with a
85:27 - large data set like 1 million elements
85:31 - well then a binary search is actually
85:32 - fantastic because we're eliminating half
85:35 - of the elements we are searching through
85:37 - during each phase or turn so if we had a
85:40 - million elements after the first phase
85:42 - we can already disregard like half a
85:44 - million elements and then we just repeat
85:46 - the process until there's only one left
85:49 - so if this was an iterative approach we
85:51 - would need to search through these
85:52 - linearly beginning with you know index
85:55 - zero and going all the way to a million
85:57 - so a binary search is fantastic with
86:00 - large data sets the runtime complexity
86:02 - of a binary search is o of log n the
86:05 - larger the data set a binary search
86:07 - becomes more and more efficient compared
86:09 - to other search algorithms alright let's
86:12 - perform a binary search in real life now
86:14 - we'll use the built-in binary search
86:16 - method of arrays to begin with and then
86:18 - later on we'll build our own binary
86:20 - search function
86:21 - so we'll need an array to work with
86:23 - let's say we have an array of integers
86:25 - named array int array and the size of
86:28 - this array will be 100 we'll increase
86:30 - the size later for demonstration
86:32 - purposes and we'll need a target value
86:34 - that we're searching for i'll just name
86:36 - this target and target equals what about
86:38 - 42 we'll search for the number 42 and
86:41 - we'll need to populate our array so we
86:43 - can do so using a for loop int i equals
86:46 - zero we will continue this for loop as
86:49 - long as i is less than array dot length
86:53 - and increment i by one during each
86:55 - iteration then we will fill array at
86:59 - index i with whatever i is our index
87:03 - okay so
87:05 - the cheap way of using a binary search
87:07 - is to use the built-in binary search
87:09 - method of arrays
87:11 - let's say int
87:13 - index
87:14 - equals
87:16 - arrays
87:17 - dot binary
87:19 - search
87:20 - and taking a look at this binary search
87:23 - method there's two arguments that we
87:25 - have to pass in an array and whatever
87:27 - we're searching for so we will pass in
87:30 - our array and our target then return the
87:33 - index and let's display that
87:35 - so let's check to see if our index
87:39 - is equal to negative one if our target
87:42 - is not found then that means negative
87:45 - one will be returned from our binary
87:47 - search method so let's print something
87:49 - system.out.printline
87:51 - uh what about element
87:53 - not found
87:55 - actually better yet target not found let
87:58 - me change that
87:59 - target
88:00 - plus not found okay then else
88:06 - else we will display
88:07 - system.out.printline
88:11 - element
88:13 - found at
88:14 - colon space
88:16 - plus
88:17 - index
88:18 - all right let's try it
88:21 - okay element found at 42.
88:24 - cool
88:24 - now let's create our own binary search
88:26 - function for practice i'll turn this
88:28 - line into a comment copy it
88:30 - paste it and get rid of this erase
88:32 - portion
88:34 - okay then i'm just going to use a
88:35 - shortcut to generate this function
88:38 - okay so private static int binary search
88:42 - there are two parameters an array of
88:44 - integers named array and int target so
88:48 - we'll return negative one that acts as a
88:50 - sentinel value negative one means that
88:53 - the value is not found
88:55 - now what we'll need at this point is the
88:57 - beginning and ending index of our array
89:00 - so let's say int low will be the
89:01 - beginning
89:03 - and int high is the end array
89:06 - dot length minus one
89:09 - so we have a low index and high index
89:12 - and we'll create a while loop
89:15 - while
89:16 - low is less than or equal to high we'll
89:18 - continue this while loop and keep on
89:21 - searching through our array so first we
89:23 - need the middle index
89:25 - int
89:26 - middle and here's the formula for that
89:29 - low
89:30 - plus
89:31 - high
89:32 - minus low
89:33 - divided by two
89:35 - so we have our middle index we will take
89:39 - int
89:39 - value
89:41 - equals our array
89:43 - at index of middle so this will extract
89:46 - that value found within
89:48 - this element
89:49 - okay so this portion is optional i'm
89:51 - just going to display whatever this
89:53 - value is so we can count the amount of
89:54 - steps it's going to take to find a value
89:57 - so let's say middle colon space whatever
90:00 - this value is
90:02 - this line of code is optional i'm just
90:03 - doing this for educational purposes okay
90:06 - now we need to check to see if our value
90:09 - is less than or greater than our target
90:12 - or equal to
90:13 - if
90:15 - value
90:16 - is less than our target
90:21 - low
90:22 - equals middle
90:24 - plus one
90:25 - and actually i'm going to get rid of
90:27 - these curly braces if you have an if
90:28 - statement and you only have like one
90:30 - line of code you don't really need the
90:32 - curly braces i'm just doing this so it's
90:34 - easier to read
90:36 - okay else
90:37 - if
90:40 - value
90:41 - is greater than
90:42 - target
90:43 - we will set our high index high equals
90:47 - middle minus one
90:51 - else that means we have found our target
90:53 - else return
90:55 - middle
90:57 - so this means that target is found
91:02 - and by returning negative one that means
91:04 - target not found
91:06 - and that is our binary search function
91:08 - let's try it
91:10 - okay element found at 42 so it took us
91:13 - let's see one two three four four steps
91:16 - to find the number 42 within this array
91:18 - of 100 elements now let's increase the
91:21 - size because binary searches do
91:23 - extremely well with large data sets
91:26 - so let's say we have 1 million elements
91:28 - and let's change this target what about
91:31 - seven hundred seventy seven thousands
91:35 - whatever that number is okay so let's
91:37 - search for it
91:39 - and let's count the steps
91:41 - uh so there's quite a number of steps
91:43 - here but let's count them 1 2 3 4 5 6 7
91:48 - 8 9 10 11 12 13 14 15 16 17 18 19 20 20
91:54 - steps now imagine if we took a linear
91:57 - approach where we began at index 0 and
91:59 - looped through this entire array looking
92:01 - for this index and in that case looping
92:04 - through an array would have a runtime
92:05 - complexity of of n to find this number
92:08 - it's going to take over 700 000 steps
92:11 - because we're iterating once for each
92:13 - element within this array compared to a
92:15 - binary search where it only took 20
92:17 - steps well then in conclusion a binary
92:19 - search is a search algorithm that finds
92:22 - the position of a target value within a
92:25 - sorted array half of the array is
92:27 - eliminated during each step or phase so
92:31 - that's a binary search algorithm if you
92:33 - would like a copy of all this code of
92:35 - course i will post this to the comment
92:36 - section down below and that is the
92:39 - binary search algorithm in computer
92:41 - science
92:44 - all right what's going on everybody
92:46 - interpolation searches these are an
92:48 - improvement over binary searches that
92:50 - are best used for uniformly distributed
92:53 - data basically speaking we're going to
92:55 - make a guess and i'm saying that within
92:57 - quotes we're guessing where a value
93:00 - might be based on calculated probe
93:02 - results if our probe is incorrect our
93:05 - search area is narrowed and a new probe
93:08 - is calculated basically we're guessing
93:10 - where a value is going to be and return
93:12 - the index
93:13 - so using an interpolation search this
93:16 - has an average runtime complexity of big
93:19 - o of log log n and in a worst case
93:22 - scenario where our values within our
93:24 - collection increase exponentially this
93:27 - can have a runtime complexity of big o
93:30 - of n so to demonstrate this let's create
93:32 - an array of integers
93:34 - int array we'll name this array equals
93:38 - and assign this some numbers all
93:39 - uniformly distributed let's say the
93:41 - numbers one through nine all in order
93:43 - this would be i would say a best case
93:45 - scenario
93:47 - then let's find an index int index
93:50 - equals and we will invoke an
93:52 - interpolation search function so
93:55 - interpolation search we will pass in our
93:58 - ray
93:59 - and a value we would like to search for
94:01 - let's search for the number maybe eight
94:04 - okay then let's declare this function
94:08 - private static and interpolation search
94:11 - there are two parameters an array of
94:13 - integers
94:14 - and our value we are searching for i'm
94:16 - going to rename this parameter as value
94:18 - so it's more descriptive
94:20 - the first thing that we're going to do
94:21 - is calculate the upper bound and the
94:24 - lower bound of our searchable area
94:26 - so int high will be the higher bound of
94:28 - our searchable area and this will be our
94:31 - arrays length minus one
94:33 - and the lower bound is well the first
94:35 - index and low and i will set this equal
94:38 - to zero using a while loop we will
94:40 - continue probing the condition within
94:43 - our while loop is while our value is
94:46 - greater than or equal to
94:48 - our array at index of low the lower
94:51 - bound
94:52 - and
94:53 - our value
94:54 - is less than or equal to
94:57 - our array at index of higher bound
95:01 - and our sizable search area is going to
95:04 - shrink after each iteration
95:06 - so while our value is within the new
95:08 - searchable area keep on probing keep on
95:11 - searching i'm going to add another
95:12 - condition too
95:14 - low is less than or equal to high
95:18 - after each iteration our searchable area
95:20 - is going to shrink once our searchable
95:22 - area is zero elements well we can't
95:24 - search anymore so we might as well exit
95:26 - now here's the formula to calculate
95:28 - where our value is probably going to be
95:30 - and our guess will be referred to as our
95:33 - probe int probe equals and here's the
95:36 - formula
95:38 - high minus low
95:41 - times
95:43 - our value
95:44 - minus array at index of low our lower
95:48 - bound divided by
95:51 - array at index of high
95:55 - minus array
95:57 - at index of low just going to add low to
96:00 - the front of this and i'm just going to
96:01 - make this a little bit more readable for
96:03 - us here's the formula to calculate where
96:06 - our value is likely going to be it's a
96:08 - little complex to read this a few of the
96:10 - contributing factors are the size of our
96:13 - current searchable area high minus low
96:16 - so to begin with we have nine elements
96:19 - times the value we're searching for
96:21 - minus the value at the lower bound so 8
96:24 - minus 1 divided by the value at the
96:27 - higher bound minus the array at the
96:29 - lower bound then at the end we're just
96:31 - tacking on whatever our lower bound
96:33 - currently is
96:35 - so it's a complex formula all you have
96:37 - to do is just copy this so during each
96:39 - iteration i'm going to display our probe
96:43 - this is going to be essentially our
96:45 - guess
96:47 - and let's check to see if our probe is
96:49 - equal to our value
96:51 - so using an if statement let's check to
96:53 - see if array
96:55 - at index of probe
96:58 - is equal to
96:59 - our value that we're searching for
97:01 - if it is let's return our probe this is
97:04 - the correct index else if our guess our
97:07 - probe is incorrect we'll need to narrow
97:09 - down our search area so using an else if
97:12 - statement
97:14 - let's check to see if array
97:16 - at index of
97:18 - probe is less than our value if that's
97:22 - the case we will need to set the new
97:24 - lower bound low equals probe plus one
97:30 - else
97:31 - high equals probe
97:33 - minus one
97:35 - now here's the deal with these
97:36 - statements
97:37 - we're currently searching for the number
97:38 - eight
97:39 - if we guess our probe is at let's say
97:41 - five and our value is greater than this
97:44 - probe we can disregard this portion of
97:47 - the searchable area so we're moving the
97:50 - new lower limit the new lower bound to
97:53 - just after where our probe was six so
97:55 - this is the new searchable area and then
97:57 - we'll calculate a new probe based on
97:59 - this data if we're looking for let's say
98:02 - two and our probe says it's likely here
98:04 - at five well then since two is less than
98:07 - our probe we can disregard all of this
98:09 - data and this would be our new
98:11 - searchable area we would move the higher
98:13 - bound of our data down to just below our
98:16 - probe so that's kind of the idea
98:18 - so at the end if we do not find our
98:21 - value let's return negative 1 as a
98:23 - sentinel value
98:24 - so back within our main function let's
98:26 - check to see if the value returned does
98:28 - not equal negative one using if else
98:31 - statements
98:33 - if index does not equal negative one
98:37 - then let's print a message
98:39 - system.out.printline
98:42 - element found at index
98:46 - plus index
98:49 - else
98:51 - element not found okay so this should
98:54 - work let's run it
98:56 - so we are searching for eight so this
98:58 - formula calculated that this value is
99:01 - likely at probe seven index seven zero
99:05 - one two three four five six seven and
99:08 - this was the first iteration of our
99:10 - while loop we were able to find our
99:12 - value 8 on the first iteration so let's
99:14 - find a different value how about one
99:17 - element found at index 0 that's the
99:19 - first element so interpolation searches
99:22 - work very well with uniformly
99:24 - distributed data these numbers are all
99:27 - increasing by one so this is a little
99:29 - too easy for our interpolation search
99:32 - it's guessing the likely index on the
99:33 - first try so let's create a more
99:35 - difficult data set
99:37 - so with our new data set let's say that
99:39 - we will start with one and then double
99:41 - the number of the previous element so 1
99:44 - 2
99:45 - 4
99:46 - 8 16
99:48 - 32
99:49 - 64
99:50 - 128
99:52 - 256 512
99:55 - and 1024
99:57 - and let's search for the number what
99:59 - about 256
100:01 - let's see how many probes this is gonna
100:02 - take
100:05 - all right so here's the results
100:07 - we iterated our while loop five times we
100:09 - had five different guesses after the
100:12 - first guess this was not correct so we
100:14 - narrowed down our search area then we
100:16 - probed again this still wasn't the
100:18 - correct answer so we probed again add
100:20 - again and again until we got the right
100:22 - value so that's an interpolation search
100:25 - it's an improvement over a binary search
100:27 - that is best used for uniformly
100:29 - distributed data it guesses where a
100:32 - value might be based on a calculated
100:35 - probe result if the probe is incorrect
100:38 - the search area is narrowed and a new
100:40 - probe is calculated the interpolation
100:42 - search has an average runtime complexity
100:45 - of big o of log log n and a worst case
100:48 - runtime complexity of big o of n this
100:51 - would be if our values increase
100:52 - exponentially so yeah that is the
100:55 - interpolation search if you would like a
100:57 - copy of this code i'll post this to the
100:58 - comment section down below and well yeah
101:01 - that is the interpolation search in
101:03 - computer science
101:06 - oh yeah
101:08 - all right bubble sort bubble sort is a
101:11 - sorting algorithm that compares adjacent
101:13 - elements and checks to see if they're in
101:15 - order if not these elements are switched
101:17 - then the next pair of adjacent elements
101:19 - is compared and we continue on in that
101:21 - pattern until all elements are in order
101:23 - while using bubble sword i like to
101:25 - imagine that our collection is filled
101:27 - with water heavy things like rocks
101:30 - sediment will sink to the bottom
101:32 - anything light such as air wood bubbles
101:35 - anything light will flow to the top
101:37 - here's an array of nine unordered
101:39 - elements we will use bubble sort to
101:41 - manually sort these elements in
101:43 - ascending numeric order alright so i
101:45 - reset this array we're going to perform
101:48 - a total of nine laps but don't worry
101:50 - i'll fast forward through this footage
101:52 - so let's just walk through the first few
101:54 - steps we will compare these first two
101:56 - adjacent elements we'll check to see if
101:59 - the first element is greater than the
102:01 - second element if it is we will move
102:03 - this element to a variable which will
102:06 - probably be named temp short for
102:08 - temporary check the video in the java
102:10 - playlist on how to swap variables i
102:12 - think it's a pretty good video but i
102:14 - might be biased with my opinion we'll
102:16 - take the next element and place it where
102:18 - the first element was then move this
102:20 - variable within temp into the spot that
102:23 - one was in the previous element okay so
102:26 - then we will check these next two
102:28 - adjacent elements this first element is
102:30 - greater than the second element so we
102:32 - will move our first element
102:34 - into temp move the second element to
102:37 - where element one was
102:40 - and then move temp to where element two
102:43 - is and then we would just repeat this
102:45 - process
102:46 - so we will lap through this array once
102:48 - for each element that is available so
102:51 - let's just speed up the footage here and
102:53 - i will show you manually a bubble sort
102:59 - [Music]
103:58 - as you may have noticed the bubble sort
104:00 - algorithm really isn't that efficient
104:01 - even when working with smaller data sets
104:04 - in most real world applications you'll
104:06 - probably use a different sorting
104:08 - algorithm but this is still a good thing
104:10 - to learn so the bubble sort algorithm
104:12 - has a runtime complexity of o of n
104:15 - squared it runs in quadratic time so the
104:18 - larger the data set the more and more
104:20 - inefficient that this sorting algorithm
104:22 - is going to be with a small data set
104:24 - it's not horrible but there's definitely
104:26 - better algorithms out there so for
104:28 - practice let's create our own bubble
104:30 - sort algorithm
104:32 - all right let's create an array of
104:34 - integers and assign some random numbers
104:36 - make sure that they're not in order
104:38 - because well then that would defeat the
104:40 - purpose of this program so at the end
104:42 - we'll just display all the elements of
104:44 - this array using an enhanced for loop
104:46 - for i in array
104:49 - we will display with a print statement
104:52 - not print line
104:53 - whatever i is during each iteration so
104:56 - let's just test this
104:58 - so i have all the elements in my array
105:00 - printed and they're currently not in
105:02 - order so we'll need to declare a bubble
105:04 - sort method bubble
105:07 - sort
105:08 - and then we will need to define this
105:10 - outside of our main method
105:12 - so public
105:15 - static
105:16 - void we're not returning anything
105:19 - bubble
105:20 - sort
105:21 - and then we will need to pass in an
105:23 - array so
105:25 - that will be the argument array
105:27 - and we will accept
105:29 - int
105:30 - array
105:32 - okay so this is actually really easy to
105:35 - write even though the bubble sort
105:37 - algorithm really isn't too efficient
105:39 - so i guess that's one benefit so we'll
105:41 - need nested for loops
105:44 - and then the outer for loop will be
105:47 - int i
105:48 - equals zero
105:50 - i
105:52 - is less than
105:53 - array dot length
105:56 - minus one i
105:59 - plus plus
106:00 - and let's do the same thing with the
106:02 - inner for loop but we can copy what we
106:04 - have
106:05 - change i to j
106:11 - and this is going to be array length
106:13 - minus i
106:15 - minus 1.
106:16 - so we're going to check to see if
106:19 - array
106:22 - at index of j
106:25 - is greater than
106:26 - that will be for ascending order
106:29 - is greater than array at index of j
106:33 - plus one so that would be the next
106:36 - adjacent element so if this number is
106:39 - greater than this one we should switch
106:41 - these two elements around and we'll need
106:43 - the assistance of a temporary variable
106:46 - so let's declare int temp
106:48 - equals
106:49 - array at index of j
106:54 - then we will take array
106:56 - at index of j set this equal to array at
107:00 - index of j
107:02 - plus one
107:04 - and then lastly we have array
107:06 - at index of j
107:08 - plus one
107:10 - equals whatever is stored within temp
107:13 - and honestly that's all there is to it
107:15 - so this should sort our array
107:18 - and it's in ascending order so if you
107:20 - need this in descending order we would
107:22 - just swap this greater than sign with a
107:24 - less than sign
107:26 - and now this is in descending order so
107:29 - even though the bubble sort algorithm
107:31 - really isn't too efficient it's actually
107:33 - extraordinarily easy to write if you
107:34 - just need something really simple so i
107:36 - guess that's one benefit all right
107:38 - everybody that is the bubble sort
107:41 - algorithm it compares pairs of adjacent
107:44 - elements and checks to see if they're in
107:46 - order if they're not they're swapped and
107:48 - this process will repeat once for each
107:50 - element in an array or other collection
107:53 - so this algorithm has a runtime
107:55 - complexity of o of n squared so it's
107:59 - okayish for small data sets and please
108:02 - do not use this for any large data sets
108:05 - so if you would like a copy of this code
108:07 - i will post this to the comment section
108:09 - down below and well that's the bubble
108:11 - sort algorithm in computer science
108:14 - hey everyone it's me again and in this
108:16 - video i'm going to explain the selection
108:18 - sort algorithm and computer science as
108:20 - always sit back relax and enjoy the show
108:25 - okay everybody selection sort selection
108:28 - sword is an in-place comparison sorting
108:31 - algorithm that keeps track of the
108:32 - minimum value during each iteration and
108:35 - at the end of each iteration all we do
108:37 - is swap variables how i like to imagine
108:39 - this is that let's say that our array is
108:42 - a bunch of different closed boxes and
108:44 - each box contains a number and they're
108:46 - all out of order so what we'll do is we
108:49 - have a flashlight too because we're in
108:50 - the attic and it's really dark so we
108:52 - have a flashlight we will open each box
108:54 - beginning with the beginning of our
108:56 - array and we'll take a look at the value
108:58 - inside okay nine is a fairly small
109:00 - number i guess right so we will move
109:03 - this value to some temporary storage
109:05 - we'll keep track of the index of the
109:07 - minimum value nine is the new minimum
109:10 - let's open the next box and holy crap
109:12 - it's a one one is a really small number
109:15 - so that will be the new minimum
109:17 - then we'll open the next box which is an
109:19 - eight one is still less than eight let's
109:21 - move on the next box is two seven three
109:24 - six four and five one was the minimum
109:27 - value during this iteration and we need
109:29 - to move this value to the place that we
109:32 - started during this iteration index 0 so
109:35 - we'll have to do some good all variable
109:37 - swapping so we will take 9 place it
109:39 - within some temporary storage
109:42 - then take one and place it where nine
109:45 - was
109:46 - then take nine
109:48 - and place it where one was
109:52 - and that is the first iteration let's
109:54 - move on to iteration two and we'll clear
109:56 - min and temp okay so this portion is
109:59 - done now we're not worried about it this
110:02 - is the new beginning of the next
110:03 - iteration and well nine is a low number
110:06 - i guess we'll move that to min
110:09 - but eight is even lower than nine so
110:11 - we'll move that to min
110:13 - holy crap it's a two two is the new min
110:15 - for sure
110:17 - and then we just repeat this process
110:18 - over and over again two is the current
110:21 - minimum of this iteration one was the
110:23 - minimum of the first iteration but we're
110:25 - not concerned with that it's already
110:27 - sorted so we need to move two to where
110:29 - we began this iteration at index one and
110:33 - currently there's a nine in there so
110:34 - we're going to evict this nine place it
110:37 - within some temporary shelter take two
110:40 - place it within where we started at
110:42 - index one
110:44 - then take nine
110:46 - place it where two was and that is the
110:48 - second iteration now that we kind of
110:49 - know how this process works i'll speed
110:51 - up the rest of the video for this
110:52 - demonstration so let's begin at index 2.
111:01 - [Music]
111:23 - [Music]
111:31 - [Music]
111:39 - [Music]
111:48 - [Music]
112:08 - [Music]
112:14 - [Music]
112:22 - and that is the selection sort algorithm
112:25 - the selection sort algorithm has a
112:27 - runtime complexity of big o of n squared
112:30 - the larger the data set the more and
112:32 - more inefficient that using the
112:33 - selection sort algorithm is going to be
112:35 - although it's okay with smaller data
112:38 - sets now let's create our own selection
112:40 - sort algorithm okay let's implement a
112:43 - selection sort we'll need an array or
112:45 - other collection to work with let's
112:46 - create an array of integers because i
112:48 - want to make this as easy as possible so
112:50 - integer array and make up some random
112:53 - numbers make sure that they're not in
112:54 - order what about
112:56 - eight
112:57 - seven
112:58 - nine two three
113:01 - one five four
113:03 - six i guess
113:04 - then let's use a for each loop to
113:07 - iterate over the elements of this array
113:09 - four and i
113:11 - in array we will display each element
113:14 - with a print statement
113:17 - system.out.print
113:19 - i and let's run this once just to be
113:21 - sure that everything is working fine so
113:23 - 879
113:25 - five four six everything is working as
113:27 - it should
113:28 - so before we display the elements of our
113:30 - array let's invoke a selection sort
113:33 - function which we still need to declare
113:36 - so selection sort and we will pass our
113:39 - array as an argument because well that's
113:41 - what we want to sort right so selection
113:44 - sort and we'll need to create this
113:46 - method i will cheat and use the shortcut
113:48 - so outside of our main method let's
113:50 - declare private static void selection
113:53 - sort there is one parameter an array of
113:56 - integers we'll need a pair of nested
113:58 - loops to iterate over our array so let's
114:00 - work on the outer loop for int i equals
114:04 - zero we will continue this as long as i
114:06 - is less than array's length property
114:10 - minus 1 then increment i by 1 during
114:13 - each iteration
114:14 - then there is a nested loop within here
114:17 - change i to j
114:19 - so j equals
114:21 - i plus one
114:24 - j is less than array length
114:27 - and j plus plus
114:30 - so we'll need to keep track of the
114:31 - minimum so we'll do that outside of our
114:34 - nested loop int
114:36 - min
114:37 - equals i so that is the current minimum
114:41 - and within the nested for loop we will
114:44 - check to see
114:45 - if our array at index of min
114:50 - is greater than array
114:53 - at index of j
114:56 - if it is we will change our min to equal
114:59 - j
115:00 - then outside of our nested loop but
115:03 - within the outer loop we will do some
115:05 - good old variable swapping so int temp
115:09 - equals array at index of i to store this
115:13 - element
115:14 - array
115:15 - at index of i equals array
115:19 - at index of min
115:22 - then lastly
115:24 - array
115:25 - at index of min
115:27 - equals
115:29 - temp
115:30 - and that's all there is to it so after
115:32 - running this program
115:34 - our array is now sorted via the
115:36 - selection sort algorithm then of course
115:39 - if you need your array or collection
115:40 - sorted in descending order currently
115:43 - it's in ascending order all we do is
115:45 - swap this greater than sign with a less
115:48 - than sign and this will now be sorted in
115:51 - descending order depending on what you
115:52 - need
115:53 - well okay then everybody that is the
115:56 - selection sort algorithm it will search
115:58 - through an array and keep track of the
116:01 - minimum value during each iteration at
116:04 - the end of each iteration we swap
116:06 - variables and that's all there is to it
116:08 - it runs in quadratic time big o of n
116:11 - squared it's okay with small data sets
116:13 - even more so than bubble sort and it's
116:15 - pretty terrible with large data sets the
116:17 - larger the data set the more and more
116:19 - inefficient that this selection sort
116:21 - algorithm is gonna be so that is the
116:24 - selection sort algorithm if you learn
116:25 - something new give this video a big fat
116:27 - thumbs up drop a random comment down
116:29 - below and well that is the selection
116:32 - sort algorithm and i guess computer
116:34 - science
116:35 - what's going on people it's bro hope
116:37 - you're doing well and in this video
116:39 - we're going to discuss the insertion
116:40 - sword algorithm in computer science as
116:42 - always sit back relax and well enjoy the
116:46 - show
116:48 - all right everybody insertion sword now
116:50 - what we do with insertion sort is that
116:52 - we begin at index one we take the value
116:55 - found within move it to some temporary
116:56 - storage like a variable named temp to
116:59 - temporarily hold it we examine elements
117:02 - to the left if any elements are larger
117:04 - than what's within temp we shift those
117:06 - elements to the right so six is larger
117:08 - than one we shift it to the right if
117:11 - it's less than whatever's within temp we
117:13 - stop or until we run out of elements so
117:16 - we have run out of elements we take this
117:18 - value found within temp and place it at
117:20 - this opening here that was the first
117:22 - iteration let's move on to iteration two
117:24 - take this value place it within temp
117:26 - examine the elements to the left if this
117:28 - element is greater than temp then we
117:31 - shift it to the right it's not so we
117:32 - stop here and place this value back
117:35 - where it came from so that was the
117:37 - second iteration iteration three take
117:39 - this value place it within temp examine
117:41 - the elements to the left if they're
117:43 - greater than four we shift them to the
117:44 - right seven is larger than four shift it
117:46 - to the right six is larger than four
117:48 - shift it to the right one is not larger
117:51 - than four so we stop here take
117:53 - whatever's within temp this value four
117:55 - and insert it here into this opening so
117:57 - that was the first three iterations we
117:59 - repeat this process until we run out of
118:01 - elements so i'll speed up the footage so
118:04 - we are currently on index four
118:07 - [Music]
118:25 - [Music]
118:55 - do
118:57 - [Music]
119:13 - and that is your visual representation
119:15 - of the insertion sword algorithm i like
119:17 - to think of it like a jigsaw puzzle we
119:20 - have some pieces that are connected they
119:22 - fit together and we will move whole
119:24 - sections of pieces together to make room
119:26 - for a piece that fits so let's create
119:28 - our own insertion sort algorithm in java
119:30 - now we'll need to create an array let's
119:33 - create an array of integers int array
119:36 - and make up some numbers put whatever
119:38 - numbers you want within here uh maybe
119:40 - nine how about a one and an eight and a
119:44 - two and a seven three six five four that
119:49 - sounds good to me okay then let's
119:51 - display the elements of this array we'll
119:53 - use a for each loop four
119:56 - and i in array we will display each
120:00 - element within this array with a print
120:03 - statement so let me get rid of that
120:05 - print ln and just have print so i will
120:08 - print i i think i'm going to add a space
120:10 - afterwards i didn't do that in the
120:12 - previous two videos so i think i better
120:14 - do that okay let's just run this just to
120:16 - test it
120:18 - nine one eight two seven three six five
120:21 - four and before we display the elements
120:23 - of our array let's invoke a function
120:25 - which we still need to declare called
120:27 - insertion sort so
120:29 - insertion
120:31 - sort
120:32 - then we will pass in our array as an
120:35 - argument
120:36 - and we'll need to declare this so i'm
120:38 - going to cheat and create this
120:39 - automatically so outside of our main
120:41 - method create a method named insertion
120:44 - sort private static void insertion sort
120:47 - there is one parameter an array of
120:49 - integers okay
120:50 - now the first thing we'll do is create a
120:53 - for loop to iterate over each element of
120:55 - our ray but it begins at one not zero
120:58 - so that would be four
121:00 - then we will set
121:02 - int i
121:03 - to equal one not zero pay attention to
121:06 - that we will continue this as long as i
121:09 - is less than array
121:11 - dot link and we will increment i by one
121:15 - during each iteration
121:17 - now we need to take
121:18 - our value found within i and place it
121:21 - within temp so let's declare a temporary
121:24 - variable named temp int temp equals
121:27 - array at index of i
121:31 - and now we'll create a variable named j
121:34 - int j equals i minus 1.
121:37 - so this will keep track of what value
121:40 - we're comparing to the left of whatever
121:42 - i is
121:44 - so then we need to create a while loop
121:47 - we will continue comparing values to the
121:49 - left of i and our condition is going to
121:52 - be while j
121:54 - is greater than or equal to zero
121:58 - and
122:00 - array
122:01 - at index of j is greater than
122:05 - temp
122:07 - so if we need to shift an element to the
122:10 - right we would say array
122:14 - at index of j
122:16 - plus one
122:18 - equals
122:19 - array
122:21 - at index
122:22 - of j so that will shift an element to
122:24 - the right
122:26 - then we will decrement j by one j minus
122:29 - minus and the last thing to do is to
122:32 - insert the value found within temp into
122:34 - that opening so that would be array at
122:37 - index of j
122:39 - plus one
122:40 - equals temp
122:42 - and that's all there is to it so let's
122:44 - run this
122:46 - boo yeah one two three four five six
122:49 - seven eight nine
122:50 - well people in conclusion the insertion
122:53 - sort algorithm compares elements to the
122:55 - left and it will shift elements to the
122:57 - right to make room to insert a value the
122:59 - insertion sort algorithm has a runtime
123:02 - complexity of big o of n squared runs in
123:05 - quadratic time it's decent with small
123:07 - data sets but bad with large data sets
123:10 - and using insertion sort tends to be
123:12 - preferable to both bubble sort and
123:14 - selection sort it uses less steps than
123:17 - bubble sort and in the best case
123:19 - scenario insertion sort can run in o of
123:22 - n linear time compared to selection sort
123:25 - which the best case scenario is of n
123:27 - squared all right people so that is the
123:30 - insertion sword algorithm if you can
123:32 - destroy that like button leave a random
123:34 - comment down below and well yeah that's
123:36 - the insertion sword algorithm in
123:38 - computer science
123:42 - well howdy y'all we're talking about
123:44 - recursion now recursion is when a thing
123:47 - is defined in terms of itself i stole
123:50 - that definition from wikipedia really
123:52 - doesn't make too much sense a thing is
123:54 - defined in terms of itself so basically
123:56 - with recursion we apply the result of a
123:59 - procedure to a procedure a recursive
124:02 - method is one that calls itself and this
124:04 - can be a substitute for iteration
124:07 - there's a lot of overlap where you could
124:08 - use either recursion or iteration with
124:11 - recursion we divide a problem into
124:14 - sub-problems of the same type as the
124:16 - original and recursion tends to be used
124:18 - within advanced sorting algorithms and
124:21 - navigating trees some of the benefits of
124:23 - recursive code is that it's easier to
124:25 - read and write and it's easier to debug
124:28 - but the disadvantages is that it's
124:29 - sometimes slower and uses more memory
124:32 - let's begin with a very simple example
124:34 - let's create a method to simulate
124:36 - walking we'll do this both iteratively
124:38 - and recursively then take a look at the
124:40 - differences between the two so let's
124:42 - write an iterative walk method so we
124:45 - will need to invoke this method and then
124:47 - we will pass in the amount of steps that
124:49 - we would like to walk like five steps
124:52 - and then we'll need to declare this
124:53 - method private static void walk and i'll
124:56 - change i to maybe steps just so that
124:59 - it's more descriptive
125:00 - now let's use an iterative approach so
125:03 - iteration is the repetition of an
125:06 - internal process
125:07 - so we can use a for loop
125:09 - that will repeat a process and let's say
125:11 - that int i equals zero we will continue
125:14 - this as long as i is less than steps and
125:17 - then let's increment i by one during
125:19 - each iteration
125:21 - and then during each iteration i will
125:23 - print
125:24 - you take a step and really that's it
125:28 - and after running this code
125:30 - we have walked five steps now let's
125:32 - write the same method recursively
125:34 - recursion is the repetition of a
125:36 - procedure iteration is the repetition of
125:39 - a process
125:40 - so with recursion we need a base case
125:43 - that's what we do when we would like to
125:45 - stop and a recursive case what do we do
125:47 - when we would like to continue so our
125:50 - base case is going to be
125:52 - if
125:52 - steps is less than one then we will
125:56 - return
125:57 - and if you only have one statement
125:59 - within your if statement you don't
126:01 - really need these curly braces so i'm
126:03 - just going to omit those
126:06 - all right then we will print
126:09 - you take a step
126:12 - so this is our base case
126:15 - this is what we do when we would like to
126:16 - stop then our recursive case is what
126:19 - we're going to do when we would like to
126:20 - continue we will invoke the walk method
126:23 - within itself but we will pass in steps
126:26 - minus one
126:28 - and this is our recursive case
126:31 - and this will do the same thing however
126:33 - it's just written a little bit different
126:35 - now one thing that you should know is
126:37 - that programs have a data structure
126:39 - called a call stack a call stack keeps
126:42 - track of the order in which our program
126:44 - needs to function so with the main
126:47 - method we call the main method first and
126:49 - that's added to the bottom of our call
126:51 - stack so in order to complete our
126:54 - program we have to complete the main
126:56 - method and get to the end of it however
126:58 - when we took an iterative approach we
127:00 - invoked the walk method and that was
127:03 - added to the top of our stack remember
127:05 - that video on stacks it's a lifo data
127:07 - structure last in first out we have to
127:10 - take care of anything at the top of our
127:12 - stack first and then work our way down
127:14 - with a recursive approach we're adding
127:17 - multiple frames onto our call stack
127:20 - because one we're calling the main
127:21 - method then we're calling the walk
127:23 - method passing in five as an argument
127:26 - then we're calling the walk method again
127:28 - passing in four as an argument then
127:30 - three then two then one then zero then
127:33 - we return and we have to solve this in a
127:35 - lifo order last in first out we begin at
127:38 - the top and remove frames from the top
127:40 - until we reach the end so that's why
127:42 - using recursion is sometimes slower and
127:44 - uses more memory we're adding more
127:46 - frames to the call stack there's more
127:48 - methods that we have to keep track of
127:50 - now check this out what if we take 1
127:52 - million recursive steps we're going to
127:54 - call this walk method a million times
127:57 - and that's going to be a problem and we
127:59 - ran into an exception let's take a look
128:01 - at this
128:02 - so we encountered an exception a stack
128:04 - overflow error it's kind of like that
128:06 - one website when working with recursion
128:09 - it is possible to run out of memory
128:11 - although this is sometimes slower and
128:13 - uses more memory recursive code tends to
128:15 - be easier to read and write and easier
128:17 - to debug for a small method like this i
128:20 - would probably stick with an iterative
128:22 - approach just because it already is
128:24 - fairly simple but it's going to really
128:26 - come in handy when we get two topics on
128:28 - advanced sorting algorithms so let's try
128:31 - something a little more complex let's
128:33 - create a program to find the factorial
128:35 - of a number so let's create a factorial
128:38 - method and we'll write this recursively
128:41 - so let's find factorial what about 7 and
128:44 - then we'll need to define this method
128:46 - and we no longer need our walk method so
128:49 - if we're taking a recursive approach
128:51 - finding the factorial of a number let's
128:53 - say that i is num
128:56 - so we'll need a base case
128:58 - if
128:59 - num
129:00 - is less than one we will return one
129:04 - this is our base case
129:07 - then we need a recursive case
129:10 - we will return
129:12 - num
129:12 - times
129:14 - factorial
129:15 - then pass in
129:17 - num minus one and then eventually we'll
129:21 - hit our base case because num is going
129:23 - to be zero and this is our recursive
129:26 - case
129:27 - oh then change void to end because i
129:29 - forgot okay then let's display factorial
129:33 - seven within a print line statement
129:35 - factorial seven
129:38 - and the factorial of seven
129:41 - is five thousand forty so you can see
129:43 - that this was fairly easy to write it
129:45 - only took two lines of code by the way i
129:48 - found a great example of recursion on
129:49 - its wikipedia page and that's by the
129:51 - process of creating refreshed sourdough
129:54 - so the recipe calls for some sourdough
129:57 - left over from the last time the same
129:59 - recipe was made
130:00 - i thought that was a fairly descriptive
130:02 - example of recursion let's move on to
130:04 - level three let's create a recursive
130:06 - method to find a base raised to a given
130:08 - power let's create a power method and we
130:11 - need a base and an exponent let's find
130:14 - it two to the power of eight and then
130:15 - we'll need to define this method private
130:18 - static let's change void to int
130:21 - i to base and j to exponent
130:26 - we need a base case and a recursive case
130:29 - the base case will be
130:31 - if exponent
130:33 - is less than one we will return one
130:38 - this is our base case
130:42 - our recursive case will be return
130:45 - base
130:46 - times
130:47 - invoke the power method
130:50 - pass in
130:51 - base
130:52 - and exponent
130:54 - minus one
130:57 - and this is our recursive case
131:01 - and then we will need to display the
131:03 - result
131:05 - system.out.printline
131:08 - two to the power of 8 which is 256.
131:13 - all right everybody so that's recursion
131:15 - it's when a thing is defined in terms of
131:18 - itself we apply the result of a
131:20 - procedure to a procedure and a recursive
131:23 - method is one that calls itself and this
131:26 - can be a substitute for iteration we
131:28 - divide a problem into subproblems of the
131:31 - same type as the original and recursion
131:34 - is commonly used within advanced sorting
131:37 - algorithms and navigating trees some of
131:39 - the advantages is that recursive code is
131:42 - easier to read and write and easier to
131:44 - debug however it's sometimes slower and
131:47 - uses more memory so yeah that is
131:49 - recursion if you learn something new be
131:51 - sure to smash that like button leave a
131:53 - random comment down below and well yeah
131:55 - that's recursion in computer science
131:58 - hey what's going on everybody it's you
132:00 - bro hope you're doing well and in this
132:02 - video we're going to discuss the merge
132:04 - sort algorithm in computer science so
132:06 - sit back relax and enjoy the show
132:11 - all right what's going on people merge
132:13 - sword merge sword is a divide and
132:15 - conquer algorithm basically what we do
132:17 - is that we will pass an array as an
132:20 - argument to a merge sort function this
132:22 - function is going to divide our array in
132:24 - two we have a left array and a right
132:27 - array these will be sub arrays and we
132:29 - will copy the elements over from our
132:31 - original array to our two new subarrays
132:35 - and merge sort is a recursive function
132:37 - so at the end of merge sort we will call
132:39 - merge sort again and pass in our
132:42 - subarrays that we create and again the
132:44 - merge sort function is going to divide
132:46 - our arrays in two by creating a two new
132:48 - subarrays and then copy the elements
132:50 - over and we will stop when our arrays
132:53 - only have a size of one and that's where
132:56 - sorting then merging come in and with
132:58 - the process of merging and sorting we
133:00 - will create a second helper function
133:03 - named merge merge will accept a total of
133:05 - three arguments our left subarray our
133:08 - right subarray and the original subarray
133:11 - in which these elements came from merge
133:13 - is going to take these elements and put
133:15 - them back into their original array
133:17 - which they came from in order and we
133:19 - will do the same thing with the next
133:21 - grouping of arrays until all of these
133:23 - elements are merged back into their
133:25 - original array in which they came from
133:28 - all in order now in practice when we do
133:31 - execute this merge sort function instead
133:33 - of tackling all of these sub-arrays like
133:35 - one layer at a time we will tackle them
133:38 - by one branch at a time so it's going to
133:41 - look a little something like this where
133:43 - we will start with the leftmost branch
133:45 - and then work our way towards the right
133:47 - so i'll speed up the footage and just
133:49 - give you a rundown of how this works in
133:51 - practice
133:53 - [Music]
134:38 - [Music]
135:12 - so
135:18 - [Music]
135:29 - [Music]
135:40 - [Music]
135:48 - [Music]
136:09 - and that ladies and gentlemen is the
136:11 - merge sort algorithm the merge sort
136:14 - algorithm has a runtime complexity of
136:17 - big o of n log n it runs in quasi-linear
136:20 - time along with quick sort and heap sort
136:23 - which we still need to talk about so
136:24 - when working with large data sets merge
136:27 - sort is faster than insertion sort
136:29 - selection sort and bubble sort but on
136:32 - the other hand the emerge sword
136:33 - algorithm uses more space than a bubble
136:36 - sword selection sword and insertion sort
136:38 - because we need to create new sub-arrays
136:40 - to store elements whereas bubble sort
136:43 - selection sword and insertion sort can
136:45 - sort in place so they use a constant
136:47 - amount of space to do their sorting
136:49 - unlike with merge sort now let's move on
136:51 - to the hands-on portion of this video
136:53 - and create a merge sort function in code
136:56 - now all right well let's get started
136:58 - we'll need an array to work with make up
137:00 - some numbers make sure that they're not
137:02 - in order as well as a for loop to
137:04 - iterate over the elements of our array
137:06 - so currently our array is not in order
137:08 - but that's going to change soon let's
137:10 - invoke a merge sword method that we
137:12 - still need to declare this is going to
137:14 - be a recursive method and we will pass
137:17 - in an array and each time that we invoke
137:19 - this method we will split our array in
137:22 - half create two subarrays and then copy
137:24 - the elements over so let's create and
137:27 - declare this method
137:29 - private static void merge sort and we'll
137:32 - need a helper method too
137:34 - and we'll name this merge a helper
137:35 - method is just a method that helps
137:37 - another method basically so private
137:40 - static void merge
137:43 - and there's going to be three parameters
137:45 - within our merge method int
137:49 - left array
137:52 - int
137:53 - right array
137:56 - and int
137:58 - array
137:59 - remember that these are arrays of
138:01 - integers the first thing that we're
138:03 - going to do within our merge sort method
138:05 - is that we need to get the length of our
138:06 - array
138:07 - so let's cache that within a local
138:10 - variable named length
138:12 - int length
138:13 - equals array dot length
138:17 - and we'll need a base case too when do
138:19 - we stop recursion
138:21 - if
138:23 - length is less than or equal to one then
138:27 - we shall return
138:28 - and this is our base case basically with
138:30 - this merge sort method we're dividing
138:32 - our ray in two each time if the length
138:35 - is one there's no longer a need to
138:36 - divide our array further
138:39 - and we'll need to find the middle
138:40 - position of our array
138:44 - int middle equals length divided by two
138:48 - and we'll create two new sub arrays int
138:52 - array
138:53 - left array
138:55 - equals new
138:57 - integer array and the size is middle and
139:01 - we'll create a write array
139:03 - integer array right array the size is
139:07 - length
139:08 - minus middle
139:10 - okay now we need to copy the elements of
139:12 - our original array to our left and right
139:14 - arrays
139:15 - so we'll need two indices int i equals
139:19 - zero
139:20 - this will be for our left array
139:25 - and int
139:26 - j
139:28 - equals zero
139:30 - and this is for our right array
139:36 - then let's create a for loop
139:38 - we don't necessarily need to declare a
139:40 - new index here we can just use i so i'm
139:43 - going to add a semicolon
139:45 - our condition is i is less than length
139:50 - then increment i by one during each
139:52 - iteration so our condition
139:55 - is with an if statement if i is less
139:58 - than middle
140:00 - then we will copy an element from our
140:03 - original array to our left array
140:06 - left
140:07 - array
140:08 - at index of i
140:10 - equals array
140:12 - at index of i
140:16 - else
140:18 - we will copy that element to our right
140:21 - array
140:23 - else
140:24 - right array at index of j remember that
140:28 - this index is for the right array
140:30 - equals array at index of i
140:33 - then let's increment j by one
140:37 - okay this is where recursion comes in so
140:39 - outside of this for loop we will call
140:42 - merge sword again
140:44 - and pass in our left array
140:47 - so we'll consistently divide our array
140:50 - in half we'll begin by dividing the left
140:52 - array then the right array
140:54 - with a separate recursive call so select
140:57 - array then right array and then call
140:59 - merge
141:01 - so with merge we have to pass in our
141:03 - left array right array and our original
141:05 - array because we'll put the elements
141:08 - back in order
141:09 - left array right array and our original
141:12 - array that we received as an argument so
141:15 - that is it for the merge sort function
141:17 - let's work on merge next first thing
141:19 - that we're going to do within the merge
141:21 - method is cache the size of our left
141:23 - array and right array within some local
141:25 - variables
141:26 - int left size
141:28 - equals array
141:30 - dot length divided by two
141:33 - and right size
141:35 - equals array
141:38 - dot length
141:39 - minus left
141:41 - size
141:42 - and then we'll need three indices
141:45 - int i
141:46 - equals zero
141:48 - this is for our original array to keep
141:50 - track of the position
141:52 - l will be in charge of our left array
141:55 - and r will be in charge of our right
141:57 - array
141:58 - and these will be the indices that we're
142:00 - using
142:02 - okay the next part we're going to check
142:04 - the conditions for merging
142:08 - and we can do this with a while loop
142:11 - so our condition is going to be while l
142:15 - is less than
142:16 - left
142:17 - size
142:18 - and r is less than right size
142:23 - so basically while there's elements
142:25 - within both our left array and right
142:28 - array we will continue adding elements
142:30 - to our original array
142:32 - and we'll need to check to see which
142:33 - element is smaller if
142:37 - left array
142:39 - at index of l
142:42 - is less than
142:44 - right array
142:46 - at index of r
142:48 - then we will copy the element from our
142:50 - left array to our original array
142:53 - so we're basically comparing the number
142:54 - on the left to the right and adding
142:56 - whatever number is smaller back to our
142:58 - original array
143:00 - so array
143:01 - at index of i
143:04 - equals left
143:06 - array
143:07 - at index of l
143:10 - then we can increment i
143:12 - increment l
143:15 - so if the number on the left is not
143:18 - smaller than the number on the right we
143:19 - have to copy the element in our right
143:21 - array to our original array and we can
143:24 - use an else statement
143:26 - else
143:27 - array
143:28 - at index of i
143:30 - equals right array
143:33 - at index of our
143:35 - increment i increment r
143:39 - so there's probably going to be one
143:40 - element remaining that we cannot compare
143:42 - to another element because there's only
143:44 - one left so let's write a while loop for
143:47 - that condition
143:49 - while l is less than left size
143:55 - then we will take array at index of i
143:59 - equals left array
144:01 - at index of l
144:04 - increment i
144:06 - increment l
144:10 - then we'll need another while loop
144:12 - if r
144:13 - is less than right size
144:17 - we will copy the last right element over
144:20 - array at index of i
144:23 - equals
144:24 - right array
144:26 - at index of r
144:28 - increment i
144:30 - increment r
144:32 - and that should be it
144:34 - let's run this
144:37 - and our array is now sorted
144:39 - in conclusion everybody the merge sort
144:41 - algorithm
144:42 - recursively divides an array in two
144:45 - sorts them and then recombines them the
144:47 - merge sort algorithm has a runtime
144:49 - complexity of big o of n log n and a
144:53 - space complexity of big o of n
144:56 - so that is the merge sort algorithm if
144:59 - you would like a copy of this code i
145:00 - will post this to the comments section
145:02 - down below and well yeah that is the
145:05 - merge sword algorithm in computer
145:07 - science
145:08 - hey uh it's you bro hope you're doing
145:10 - well and in this video i'm going to
145:11 - explain the quick sword algorithm make
145:13 - computer science yep so uh sit back
145:16 - relax and enjoy the show
145:20 - all right quick sort here's how quick
145:23 - sort is going to work we need an array
145:25 - or some other type of collection i have
145:27 - a simple unordered array what we do is
145:30 - that we'll pass our array as an argument
145:32 - into a quick sort function after passing
145:36 - our array as an argument to the quick
145:38 - sort function we need to pick a pivot
145:41 - there's different variations of quick
145:42 - sort we can either pick a pivot at the
145:45 - beginning the middle or at the end but
145:47 - with most standard quick sort algorithms
145:50 - we will set the pivot to be at the end
145:52 - of our array what we're trying to
145:54 - accomplish is that we need to find the
145:56 - final resting place of our pivot where
145:59 - is this value going to be and taking a
146:01 - look at this
146:03 - that would be right about here but we
146:05 - don't know that yet so to find the final
146:08 - resting place of this value our pivot
146:11 - here's what we can do we will declare
146:13 - and use two indices j and i j will begin
146:17 - at the start of our array i will be one
146:20 - less than the beginning of our array and
146:22 - that's going to be important later and
146:23 - we'll need the help of a temporary
146:25 - variable so we can swap some values all
146:28 - we're doing is checking to see if the
146:30 - value at j is less than our pivot if
146:33 - it's greater than our pivot or equal to
146:35 - our pivot we ignore it eight is greater
146:37 - than five we ignore this value and then
146:40 - increment j by one
146:43 - so during that last iteration i did not
146:45 - come into play yet but it will this
146:47 - round again we check to see if this
146:49 - value is less than our pivot which it is
146:52 - what we do now is increment i
146:55 - then what comes next is that we swap
146:58 - these two values i and j and we'll need
147:01 - the help of a temporary variable so take
147:03 - the value at i
147:05 - assign it to temp
147:07 - take the value at j
147:09 - assign it to i
147:12 - take the value within temp
147:15 - assign it to j
147:16 - then we can move on to the next
147:18 - iteration increment j
147:22 - we check to see if the value at j is
147:24 - less than our pivot which it is if that
147:26 - is the case we increment i
147:30 - swap these two values
147:36 - we'll repeat this process until j
147:39 - reaches our pivot then we can move on to
147:41 - the next step
148:13 - here's the next step after our index j
148:16 - reaches our pivot we now know where the
148:19 - final resting place of our pivot is
148:21 - gonna be it's i incremented by one so we
148:24 - will increment i
148:26 - then swap the value at index of i with
148:30 - the value at our pivot
148:36 - our pivot is now within the correct
148:38 - place an easy way to tell is that all
148:41 - elements to the left of our pivot should
148:43 - be at less than our pivot but they're
148:46 - not necessarily going to be in order and
148:48 - that's fine they'll be organized later
148:50 - all elements to the right of our pivot
148:52 - should be greater than or equal to our
148:55 - pivot and like i said before they
148:57 - probably will not be in order the
148:59 - important thing is that elements to the
149:01 - left should be less than our pivot
149:04 - elements to the right should be greater
149:05 - than that's how we know the pivot's in
149:07 - the correct place now the next step
149:09 - we're going to create two sections two
149:12 - partitions
149:13 - the first partition will be all the
149:16 - elements from the beginning of our array
149:18 - up until our pivot but not including the
149:20 - pivot and our second partition will be
149:23 - all the elements after our pivot until
149:26 - the end of our array quicksort is a
149:28 - recursive algorithm we need to pass
149:31 - these partitions as arguments into the
149:34 - quick sort function remember that the
149:37 - quick sort algorithm is a recursive
149:40 - divide and conquer algorithm but unlike
149:42 - with merge sort with merge sort we
149:44 - create new sub arrays with quick sort we
149:47 - will be sorting these arrays in place
149:50 - but we need to keep track of the
149:52 - beginning and ending indices of these
149:54 - partitions
149:56 - and then it's just a matter of repeating
149:57 - the same steps over again but we're
149:59 - going to instead use these partitions
150:02 - sections of our array i'll give you a
150:04 - quick demonstration of what the
150:05 - quicksort algorithm is going to look
150:07 - like to completion
150:08 - [Music]
151:33 - do
151:35 - [Music]
152:45 - and that ladies and gentlemen was your
152:47 - visual representation of the quicksort
152:49 - algorithm let's code our own quicksort
152:52 - algorithm just to solidify our
152:54 - understanding of this topic all right
152:56 - people let's create a quick sort
152:58 - function you'll need an array to work
153:00 - with place some random numbers within
153:01 - that array and then some way to iterate
153:04 - and display the elements of your array
153:05 - i'm just using a symbol for each loop so
153:08 - after running this of course our array
153:10 - is not yet sorted so before we display
153:12 - the elements of our array let's invoke a
153:15 - quick sort function which we still need
153:17 - to declare defined there will be three
153:19 - arguments
153:20 - our array
153:21 - and the beginning and ending indices of
153:24 - our array so that would be zero for the
153:27 - beginning then to find the ending you
153:29 - can just say array dot length
153:32 - minus one let's declare this function
153:35 - private static void quick sort
153:37 - and let's rename some of these
153:39 - parameters we have our array of integers
153:41 - named array
153:42 - and this will be the starting index and
153:45 - this parameter will be the ending index
153:48 - so we have indices start and end now the
153:51 - base case this will use recursion
153:54 - will be
153:55 - if
153:57 - end
153:58 - is less than or equal to start
154:00 - then we will return
154:03 - and this is our base case eventually we
154:07 - won't be able to divide our array any
154:09 - further so that's when we stop and
154:11 - return with our quick sort function
154:13 - we'll need the assistance of a helper
154:15 - function that we will name partition
154:18 - let's copy our function declaration
154:20 - paste it and make a few changes
154:23 - so this will return an int the location
154:25 - of our pivot and the function name will
154:28 - be partition and the parameters are the
154:30 - same at the end of our partition helper
154:33 - function we'll return i i will be the
154:35 - location of our pivot but we'll get to
154:38 - that later okay so within our quick sort
154:40 - function we'll need to find the location
154:43 - of where to pivot
154:44 - int pivot and the partition function
154:47 - will be in charge of that
154:48 - partition is going to sort our array and
154:52 - find the pivot so all elements to the
154:54 - left will be smaller than our pivot all
154:56 - elements to the right will be larger so
154:58 - pass in our array
155:00 - we're sorting our array in place there's
155:02 - no need to create any sub-arrays we'll
155:04 - just pass in our original array as well
155:06 - as the start and end after we figure out
155:09 - where our pivot's going to be we can
155:11 - pass in each partition recursively back
155:14 - into the quick sort function
155:16 - so again we will invoke quick sort
155:20 - pass in our array
155:22 - the start of our left partition
155:24 - and the ending of our left partition and
155:27 - that is where pivot is minus one we do
155:29 - not want to include our pivot and then
155:31 - we will need to use quick sort on the
155:33 - right partition change start to pivot
155:37 - plus one
155:38 - because the pivot is already in place
155:41 - and then the end of our array in this
155:44 - variation of the quick sort function we
155:46 - will say that pivot is at the end it
155:48 - will always be at the end to begin with
155:50 - int pivot equals array at index
155:53 - of n we'll need two indices i and j
155:56 - we'll create index i
155:58 - equals start minus one
156:01 - and then we will iterate through our
156:03 - array and this is where we will declare
156:05 - int j our second index int j equals
156:09 - start and we will continue this for loop
156:11 - as long as j is less than or equal to
156:15 - the end of our array minus one then
156:18 - increment j by one what we're going to
156:20 - check is if array
156:23 - at index of j
156:25 - is less than our pivot
156:28 - if one of these elements is less than
156:30 - our pivot we want it on the left hand
156:32 - side of our pivot
156:33 - any numbers larger than our pivot should
156:35 - be on the right hand side
156:37 - so we will increment i by 1 and do a
156:40 - basic variable swap and we'll need the
156:42 - help of a temporary variable int temp
156:45 - equals array at index of i
156:48 - array at index of i
156:51 - equals array at index of j
156:55 - lastly array at index of j equals temp
156:59 - this is just a basic variable swap once
157:02 - all elements that are less than our
157:04 - pivot are on the left hand side and all
157:06 - elements that are larger than our pivot
157:08 - are on the right hand side what we will
157:10 - do now is increment i by one
157:13 - and then insert our pivot into its final
157:16 - resting place with another basic
157:18 - variable swap so let's copy this code
157:21 - then paste it and make a few changes int
157:24 - temp equals array at index of i that's
157:27 - the same
157:28 - array at index of i equals array at
157:32 - index of end
157:33 - array at index event equals temp and
157:37 - that's it and then make sure you return
157:39 - i at the end that is the location of our
157:41 - pivot
157:42 - then after running this
157:44 - our array is now sorted via the quick
157:47 - sort algorithm in conclusion the
157:50 - quicksort algorithm moves smaller
157:52 - elements to the left of a pivot we
157:54 - recursively divide our array into two
157:57 - partitions and pass those partitions as
158:00 - arguments recursively into the quick
158:03 - sort function the runtime complexity of
158:05 - the quicksort algorithm actually varies
158:08 - in its best and average cases it runs in
158:11 - big o of n log n however in its worst
158:14 - case it can run in big o of n squared
158:17 - this is rare and it occurs if the array
158:19 - is already sorted or close to being
158:22 - sorted but most of the time it will run
158:24 - in big o of n log n and the space
158:26 - complexity of the quicksort algorithm is
158:29 - big o of log n this is due to recursion
158:32 - it uses more space than bubble sort
158:35 - selection sort and insertion sort even
158:37 - though it sorts in place that's because
158:39 - the quicksort algorithm uses recursion
158:42 - we're adding frames to the call stack
158:44 - which takes memory so yeah that is the
158:46 - quick sort algorithm if you found this
158:48 - video helpful please be sure to smash
158:50 - that like button leave a random comment
158:52 - down below and subscribe if you'd like
158:54 - to become a fellow bro
158:58 - all right what's going on everybody hash
159:00 - tables a hash table is a collection of
159:04 - key value pairs each key value pair is
159:07 - known as an entry we have two pieces of
159:09 - data
159:10 - the first is the key and the second is
159:12 - the value in this example let's pretend
159:15 - that we're a teacher and we need to
159:17 - create a hash table of all of our
159:19 - students each student has a name and a
159:22 - unique student id number but these can
159:24 - be of any data type that you would like
159:27 - in this example the key will be an
159:28 - integer and the value will be a string
159:31 - so how do we know at which index to
159:33 - place each of these entries well what we
159:35 - could do is take each key and insert it
159:38 - into a hashcode method the hashcode
159:41 - method will take a key as input plug it
159:44 - into a formula and spit out an integer
159:46 - this integer is known as a hash now if
159:49 - we're finding the hashcode of an integer
159:51 - in java that's actually really easy the
159:54 - formula is the number itself so the hash
159:56 - of 100 is 100 123 is 123. so on and so
160:02 - forth with the other keys so after
160:04 - finding the hash of your key now what
160:07 - can we do these numbers are way too
160:09 - large and the size of our hash table is
160:11 - only 10 elements what we'll do next is
160:14 - take each of these hashes and divide
160:16 - each of them by the capacity the size of
160:20 - our hash table we have 10 elements so
160:23 - take each hash divided by the capacity
160:26 - of our hash table whatever the remainder
160:28 - is we will use the remainder as an index
160:31 - and to find that we will use the modulus
160:34 - operator so 100 divides by 10 evenly the
160:38 - remainder is zero so 100 modulus 10
160:42 - gives us a remainder of zero so
160:44 - spongebob's entry we will place at index
160:47 - 0 within our hash table so the hash of
160:50 - patrick's key is 123. 123 modulus 10
160:55 - gives us a remainder of 3. patrick's
160:58 - entry will be inserted at index 3 of our
161:01 - hash table so here's a little shortcut
161:03 - if you have a number modulus 10 you
161:05 - could find the remainder and that is the
161:07 - last digit 321 modulus 10 will give us a
161:10 - remainder of 1. sandy's entry will be
161:13 - placed at index one then squidward's
161:16 - entry will be placed at index five
161:19 - 555 modulus 10 is five and gary's will
161:23 - be at index seven following the same
161:25 - pattern now here's one situation what if
161:28 - two hashes are calculated to be the same
161:31 - value that is known as a collision and i
161:34 - can best demonstrate that with a
161:36 - separate example
161:37 - in this next example let's say that each
161:40 - key is now a string each entry is a pair
161:43 - of strings
161:44 - we will first need to find the hash of
161:47 - each of these keys so the hash code of a
161:49 - string uses a different formula
161:52 - basically speaking we're going to take
161:54 - the ascii value of each character within
161:57 - the string and plug it into this formula
161:59 - i went ahead and calculated the hash of
162:02 - each of these strings using this formula
162:04 - of the string hash code method and the
162:07 - next steps are the same as before take
162:09 - each hash divided by the capacity of our
162:12 - hash table and find the remainder so
162:15 - beginning with the first hash
162:18 - 48625 modulus 10 gives us a remainder of
162:22 - 5. spongebob's entry is now at index
162:26 - five within our hash table
162:28 - now patrick's will be at index zero now
162:31 - here's sandy's sandy's will also be zero
162:34 - we have a collision both of these
162:36 - entries will be located at the same
162:39 - index so what do we do each of these
162:42 - storage locations is also known as a
162:44 - bucket and the most common resolution
162:47 - for a collision in a hash table is that
162:49 - we will turn each bucket into a linked
162:52 - list if this bucket already has an entry
162:55 - within the first entry we will also add
162:58 - an address to the location of the next
163:01 - entry and keep on adding more if there's
163:04 - more entries within this bucket so in
163:06 - this way this becomes a linked list if
163:09 - we're looking up a key we first go to
163:11 - the index in which it's located if
163:13 - there's more than one entry we will
163:15 - search linearly through this bucket as
163:18 - if it were a linked list until we find
163:20 - the key that we're looking for
163:22 - so that's the most common resolution
163:24 - when there is a collision but ideally
163:27 - you would want each of these entries to
163:28 - be within their own bucket based on the
163:31 - hash of squidward's key squidward's
163:33 - entry has an index of nine and gary gary
163:37 - has an index of five and there's another
163:39 - collision we will add the address of
163:42 - gary's location to the first entry and
163:45 - this bucket becomes a separate linked
163:47 - list this process is known as chaining
163:50 - the less collisions that there are the
163:52 - more efficient that this hash table is
163:54 - going to look up a value ideally you
163:57 - would want each entry to have their own
163:59 - bucket but collisions are possible to
164:01 - reduce the number of collisions you can
164:03 - increase the size of the hash table but
164:06 - then again the hash table is going to
164:07 - use up more memory then so people
164:09 - usually find a balance between the two
164:11 - so yeah those are hash tables in theory
164:14 - let's create our own now alright
164:16 - everybody so let's implement a hash
164:18 - table in java so we will need to declare
164:20 - this hash table and list the data types
164:24 - of our key value pairs if we need to
164:26 - store primitive data types we can use
164:28 - the appropriate wrapper class so let's
164:30 - store integers and strings
164:33 - integer and string the integers will be
164:36 - the key is the strings will be the
164:37 - values we'll map student id numbers and
164:40 - student names and i'll name this hash
164:43 - table just simply table
164:45 - equals new
164:47 - hash table
164:50 - there we go so in java when we create a
164:52 - hash table these have an initial
164:54 - capacity of 11 elements and a load
164:57 - factor of 0.75 so once 75 of our
165:01 - elements are filled this hash table will
165:04 - dynamically expand to accommodate more
165:06 - elements now you can set a different
165:08 - capacity for your hash table instead of
165:10 - 11 let's say 10 to be consistent with
165:12 - our example in the previous part of this
165:14 - lesson and if you would like to change
165:16 - the load factor you can add that as well
165:18 - instead of 75 let's say 50
165:21 - so we would pass in a floating point
165:23 - number
165:23 - so 0.5 then add an f at the end for
165:26 - floating point numbers but in this
165:28 - example let's just keep the load factor
165:30 - consistent so let's start adding some
165:32 - key value pairs to add an element to
165:35 - your hash table use the put method so
165:38 - table dot put and then we will pass in
165:41 - an integer as the key and a string as
165:44 - the value so our first student is
165:47 - spongebob he has a student id of let's
165:49 - say 100
165:51 - and let's pass in a string for the value
165:54 - spongebob
165:55 - okay that is our first student so let's
165:57 - add a couple more from the previous
165:58 - example
166:00 - so we have spongebob
166:02 - patrick with an id of one two three
166:05 - sandy with an idea of three two one
166:08 - squidward with an id of 555
166:13 - and gary with an id of 777
166:17 - now to access one of these values you
166:19 - can use the get method of tables
166:21 - so i'll display this within a print line
166:23 - statement
166:24 - table dot get and i will pass in a key
166:28 - let's get the value at key number 100 so
166:31 - this student is spongebob how can we
166:34 - display all of the key value pairs of a
166:36 - table well we could use a for loop
166:39 - so i'm going to create a for loop
166:42 - and place this within it
166:45 - so to iterate over the keys of our table
166:48 - this is what we can write we can use an
166:50 - enhanced for loop so we are iterating
166:52 - over integers
166:54 - so the data type is integer
166:56 - key
166:57 - colon so to make our hash table iterable
167:00 - we can get all of the keys from our
167:02 - table and put them within a set a set is
167:05 - iterable so we will iterate over table
167:08 - dot key set method
167:11 - this will take all of our keys and
167:13 - return a set and a set is something we
167:15 - can iterate over and within our print
167:17 - line statement let's print each key
167:20 - key
167:21 - plus
167:22 - then maybe i'll add a tab to separate
167:23 - these
167:25 - plus
167:26 - table
167:27 - dot get
167:29 - then pass in whatever our key is okay so
167:32 - after running this this will display all
167:35 - of our key value pairs and if you need
167:37 - to remove an entry well there's a remove
167:39 - method table dot remove then pass in a
167:43 - key let's remove gary
167:45 - so remove the entry with this key 777
167:49 - and gary is no longer within our table
167:52 - but we'll keep them in i'll turn this
167:53 - line into a comment now just to get a
167:55 - better understanding of where these key
167:57 - value pairs are being placed let's also
168:00 - display each hash code for each of these
168:02 - elements
168:03 - so preceding our key let's display each
168:05 - hash code i'll precede our key with a
168:08 - tab
168:11 - and let's display each key's hash code
168:15 - key dot
168:16 - hashcode method if we're using the
168:19 - hashcode of integers this will return
168:21 - the primitive integer value represented
168:24 - by the key that we're passing in if
168:26 - we're using the hashcode method of
168:29 - integers well the hash is going to end
168:31 - up being the same integer so you can see
168:33 - that these numbers are the same to
168:34 - calculate an index we can follow the
168:36 - hash with modulus operator then the size
168:40 - of our table
168:41 - we set this to originally be 10.
168:44 - so we have gary at index 7 squidward at
168:47 - index 5 patrick at 3 sandy at 1
168:51 - spongebob at 0. now if our data type was
168:53 - strings we would use a different hashing
168:55 - formula so let's change the data type to
168:58 - string and all of these keys are now
169:00 - strings
169:04 - then let's remove modulus 10
169:06 - and change the data type of our for loop
169:09 - to strings
169:10 - string key
169:13 - okay these are the new hashes for each
169:15 - of our keys
169:16 - this key has this hash number this key
169:19 - has this hash number so on and so forth
169:21 - so different data types will have
169:23 - different hash code formulas now let's
169:26 - calculate the element in which each of
169:28 - these entries is going to be placed by
169:29 - adding modulus the size of our hash
169:32 - table 10.
169:35 - so here are the elements and we actually
169:37 - have two collisions we have a collision
169:40 - with these two keys they're both within
169:42 - bucket five as well as these two entries
169:44 - so both of these will be placed into
169:46 - bucket zero since there's more than one
169:48 - entry within the same element we will
169:50 - treat this bucket as a linked list and
169:52 - just iterate over it linearly until we
169:55 - reach the key that we're looking for now
169:57 - one way in which we can avoid collisions
169:59 - is to increase the size of our hash
170:01 - table if we set this to the default of
170:03 - 11 and change this to modulus 11 well
170:07 - then these will be placed within
170:08 - different buckets and you can see that
170:09 - they have changed however we still have
170:11 - a collision with spongebob and squidward
170:14 - so what if we increase this to 21.
170:17 - do we have any collisions then
170:20 - nope we do not these keys are within
170:22 - their own buckets
170:24 - all right everybody so in conclusion a
170:26 - hash table is a data structure that
170:28 - stores unique keys to values when you
170:32 - declare a hash table you state the data
170:34 - types of what you're storing and these
170:36 - are reference data types each key value
170:39 - pair is known as an entry and a benefit
170:42 - of hash tables is that they have a fast
170:45 - insertion lookup and deletion of key
170:47 - value pairs but they're not ideal for
170:50 - small data sets since there's a lot of
170:52 - overhead but they're great with large
170:54 - data sets hashing in the context of a
170:56 - hash table takes a key and computes an
171:00 - integer it utilizes a formula which will
171:02 - vary based on the key as input and its
171:05 - data type then to calculate an index we
171:07 - follow this formula we take a hash
171:10 - modulus the capacity of our table to
171:13 - calculate an index each index is also
171:16 - known as a bucket it's an indexed
171:18 - storage location for one or more entries
171:21 - they can store more than one entry in
171:23 - the case of a collision and in case that
171:25 - happens we treat each bucket as a linked
171:28 - list each entry knows where the next
171:31 - entry is located and as we discussed a
171:33 - collision is when a hash function
171:35 - generates the same index for more than
171:37 - one key less collisions equals more
171:40 - efficiency and the runtime complexity of
171:42 - a hash table varies if there are no
171:44 - collisions the best case would be a
171:47 - runtime complexity of big o of one it
171:50 - runs in constant time in case there are
171:52 - exclusively collisions as in we place
171:55 - all of our entries within the same
171:56 - bucket it's going to be one giant linked
171:59 - list and the runtime complexity of a
172:01 - linked list is big o of n it runs in
172:04 - linear time on average the runtime
172:06 - complexity of a hash table will be
172:08 - somewhere within this range so yeah
172:10 - everybody those are hash tables if you
172:13 - would like a copy of all my notes here
172:14 - i'll post them in the comments section
172:16 - down below and well yeah those are hash
172:19 - tables in computer science
172:24 - all right what's going on everybody
172:25 - graphs a graph is a non-linear
172:28 - aggregation of nodes and edges a node
172:32 - also known as a vertex may contain some
172:34 - piece of data and an edge is a
172:36 - connection between two nodes there are
172:39 - two types of graphs we're going to
172:40 - discuss undirected and directed an
172:43 - example of an undirected graph could be
172:45 - a social network like facebook each node
172:48 - could represent a user and if one user
172:51 - is friends with another user well we
172:53 - could establish a friendship and edge a
172:55 - connection between these two nodes if
172:58 - two nodes are connected they have what
172:59 - is known as adjacency in this example
173:03 - larry is friends with patrick and sandy
173:05 - so larry has adjacency to patrick and
173:08 - sandy patrick is friends with larry
173:11 - sandy spongebob and spongebob is friends
173:13 - with sandy patrick and squidward and
173:16 - squidward is adjacent to only one
173:17 - neighbor spongebob so a social network
173:20 - could be an example of an undirected
173:22 - graph the other type of graph is a
173:24 - directed graph a director graph contains
173:27 - edges that will link one node to another
173:29 - however these are one-way connections in
173:32 - this example node a would have adjacency
173:35 - to node b but not the other way around
173:38 - however it is valid to have one node
173:40 - pointing to another node and that node
173:42 - could point back to the previous node an
173:44 - example of a directed graph could be a
173:46 - street map let's say you're working on a
173:48 - travel app and each node is a possible
173:51 - destination
173:52 - these single edges could be one-way
173:54 - streets and these double edges could be
173:56 - two-way streets you can move back and
173:58 - forth between these two destinations
174:01 - there are two popular ways to represent
174:03 - a graph an adjacency matrix and an
174:06 - adjacency list with an adjacency matrix
174:09 - we could create a 2d array or 2d array
174:12 - list one row and one column for each
174:14 - node if we need to check to see if there
174:16 - is adjacency between two nodes we would
174:19 - first find the index of the node we're
174:21 - beginning at let's say a so we would go
174:23 - to node a and then find the index of the
174:26 - node we're trying to travel to so b so
174:29 - row a column b if there are no edges
174:32 - this would be zero if there is an edge
174:34 - this would be one so since there's one
174:36 - here within row a column b well there's
174:39 - adjacency from node a to node b but if
174:42 - we take a look at row a column c this is
174:45 - zero so there's no adjacency between a
174:47 - to c but if there was well we would
174:49 - replace the zero with one then now there
174:52 - are pros and cons with the matrix one of
174:54 - the benefits is that the runtime
174:56 - complexity to locate an edge is big o of
174:59 - one it's constant all we have to do is
175:01 - find two indices so we have to find the
175:04 - row and the column however the space
175:06 - complexity to store a matrix is big o of
175:10 - v squared v as in the number of vertices
175:13 - that we have but you could also think of
175:14 - that as n and for the number of nodes
175:17 - big o of n squared so since we have five
175:19 - nodes and five columns we would have a
175:22 - total of 25 spaces so the benefits of a
175:25 - matrix is that it's very quick to look
175:27 - up an edge however a matrix uses a lot
175:30 - of room so it tends to suit graphs that
175:33 - have a lot of edges
175:34 - on the other hand we have an adjacency
175:37 - list an adjacency list is an array or
175:40 - array list of linked lists each element
175:43 - is a separate linked list and each
175:46 - header within the linked list would
175:48 - contain the address of a node if there's
175:50 - adjacency between one node and another
175:53 - we would add the adjacent node to our
175:55 - linked list so to find adjacency between
175:57 - two nodes we would find the node that
176:00 - we're starting at let's see if b is
176:02 - adjacent to e so we would locate index b
176:05 - and travel this linked list until we
176:07 - find the node that we're looking for
176:09 - that means there's adjacency between
176:11 - nodes b and e even if there's a node
176:14 - that is not adjacent to any neighbors we
176:15 - would still want to add it to our
176:17 - adjacency list just in case we do update
176:19 - it here are the pros and cons of an
176:21 - adjacency list the time complexity to
176:23 - locate an element is big o of v v as in
176:26 - the number of vertices you can also
176:29 - think of this as n so this would be big
176:31 - o of n to locate an edge we would first
176:34 - access the node that we're beginning at
176:36 - by an index so let's begin at b and we
176:38 - are looking for adjacency between b and
176:41 - e since each element is a linked list we
176:44 - need to traverse this linked list
176:46 - linearly until we find the node that
176:48 - we're looking for
176:49 - so in that way it's linear however a
176:51 - benefit of a list over a matrix
176:54 - is that they use less space the space
176:56 - complexity of an adjacency list is big o
176:59 - of v plus e v for the number of vertices
177:02 - aka nodes and e for the number of edges
177:06 - so yeah everybody those are graphs a
177:08 - graph can be used to model a network
177:11 - each node is a piece of data within our
177:13 - network and an edge connects nodes so
177:15 - like i said it's a popular way to model
177:18 - networks which don't necessarily have
177:20 - any sort of order so yeah that's an
177:22 - intro to graphs and in the next two
177:24 - topics we'll create our own adjacency
177:26 - matrix and adjacency list hey if you
177:28 - enjoyed this video give it a thumbs up
177:30 - if you have any ideas of where else you
177:32 - could implement a graph let me know in
177:34 - the comment section and of course
177:36 - subscribe if you'd like to become a
177:37 - fellow bro
177:39 - hey yeah everybody it's you bro hope
177:41 - you're doing well and in this video i'm
177:43 - going to show you one way in which we
177:45 - can create an adjacency matrix and
177:47 - computer science so sit back relax and
177:50 - enjoy the show
177:51 - alright people so an adjacency matrix an
177:54 - adjacency matrix is a 2d array to store
177:58 - ones and zeros to represent edges
178:00 - between nodes there are different
178:02 - variations you could use booleans so you
178:04 - could say true or false depending if
178:07 - there's an edge or not and basically the
178:09 - number of rows and columns in an
178:11 - adjacency matrix is equal to the number
178:14 - of unique nodes the runtime complexity
178:16 - to check an edge is big o of one it's
178:19 - constant and the space complexity is big
178:21 - o of v squared so it uses up a lot of
178:23 - space now let's create our own adjacency
178:25 - matrix they're actually fairly simple so
178:28 - i'm going to create two classes graph
178:30 - and node so file new
178:33 - class i will name this graph
178:36 - finish
178:37 - file new class and i will name this
178:40 - class node
178:42 - so let's say that each node has some
178:44 - data maybe a single character char data
178:47 - and i'll create a constructor
178:50 - node we'll pass in some data when we
178:52 - create a node
178:53 - char
178:54 - data
178:55 - this dot data equals data within the
178:58 - graph class i'm going to declare a 2d
179:01 - array of integers
179:03 - so integer 2d array and i will name this
179:06 - matrix
179:07 - and within the graph constructor
179:09 - we will instantiate our matrix
179:13 - matrix equals
179:15 - new int
179:17 - now we need to declare a size of this
179:19 - matrix when we construct a graph object
179:22 - let's set up a parameter so int size
179:25 - size will be the amount of notes that we
179:27 - have and the size of this 2d array will
179:30 - be size and size that's why the space
179:33 - complexity is big o of v squared it's
179:35 - the number of vertices squared if we
179:37 - have five nodes well then the size is
179:39 - going to be a total of 25 elements now
179:42 - let's declare a few methods
179:43 - public void
179:45 - add node
179:47 - and then we will pass in a node
179:50 - node node
179:52 - and add edge
179:54 - public void add edge then we need two
179:58 - indices a source and a destination and
180:01 - source int destination dst for short
180:04 - we'll need a method to check an edge
180:08 - public and this will return a boolean
180:10 - value boolean
180:12 - check
180:12 - edge
180:14 - and we'll need a source and destination
180:16 - for parameters
180:18 - and let's create a print method
180:20 - void print
180:23 - okay we'll fill in add a node a little
180:25 - bit later so let's fill in add edge we
180:27 - will pass in a source and a destination
180:30 - two indices
180:31 - so source will be the row destination
180:33 - will be the column so what we're gonna
180:35 - do is take our matrix at index of source
180:39 - and destination and set whatever value
180:42 - is in here which will be zero equal to
180:45 - one that means there's an edge between
180:47 - two nodes and that's really it within
180:49 - the check edge method we're going to
180:51 - check within our matrix if a given value
180:54 - is equal to one if it is return true if
180:57 - not return false
180:58 - so using an if statement let's check to
181:00 - see if matrix at index of source and
181:04 - destination is equal to one that means
181:07 - there's an edge if there is an edge
181:08 - let's return true
181:11 - else we will return false and that's it
181:15 - okay now before we actually print our
181:17 - graph let's actually create a graph
181:20 - so let's get rid of this
181:22 - graph
181:24 - graph equals new graph
181:28 - and we can add some nodes although this
181:30 - method doesn't do anything quite yet but
181:32 - it will in the future
181:33 - and we need to pass in a size uh so
181:35 - let's pass in five we'll create five
181:37 - nodes
181:38 - so to add a node type graph dot add node
181:42 - and this is a method that we created
181:44 - and i will pass in an anonymous node
181:47 - or you can use a named one
181:49 - so we have new node
181:51 - and then to create a node we need to
181:52 - pass in some data because that's what we
181:54 - decided on so let's pass in the letter a
181:58 - so this will be node a
181:59 - and we'll create a few more nodes
182:02 - b c d and e
182:08 - and we also can add edges between these
182:10 - nodes to represent adjacency so to add
182:12 - an edge type graph dot add
182:15 - edge
182:16 - so what we're doing in this example
182:18 - think of each node as having an index
182:20 - number to create an edge between two
182:22 - nodes we will pass in the index number
182:25 - of each node if i need an edge between
182:27 - nodes a and b while each of these has an
182:30 - index number within our matrix so this
182:32 - first node would have an index of zero
182:34 - and the second node would have an index
182:36 - of one if i need an edge between these
182:38 - two i will add edge between zero and one
182:41 - and let's create a few others
182:43 - so how about b and c
182:46 - so c would have an index of two and c
182:48 - will have two edges based on the
182:50 - previous video
182:52 - so c will be connected to d
182:54 - so two to three
182:57 - as well as e
182:58 - so two and four
183:01 - d won't have any edges this is a
183:04 - directed graph in this example
183:06 - and e has two edges we have e to a that
183:09 - would be four two zero and e to c four
183:13 - and two
183:15 - now let's print our graph graph dot
183:17 - print and we need to fill in this method
183:19 - within the print method of the graph
183:21 - class we just have to print our 2d array
183:24 - and we can use for loops for that
183:26 - so this will be the outer for loop and i
183:29 - equals zero we will continue this as
183:31 - long as i is less than the length of our
183:34 - matrix matrix dot length then increments
183:37 - i by one
183:39 - so this will iterate over all of the
183:40 - rows in our matrix and then we need a
183:43 - nested for loop so change i to j within
183:46 - the nested loop
183:49 - whatever index we're on we will take
183:51 - matrix at index of i dot length this
183:54 - time as the stopping condition
183:56 - and during each iteration of the inner
183:58 - for loop i will use a print statement
184:01 - and i will print
184:03 - matrix
184:04 - at indices of i
184:06 - and j then i'll add a space between each
184:09 - of these oh then when you exit the inner
184:11 - for loop let's print a new line
184:13 - okay and here's our adjacency matrix
184:16 - each row corresponds to a node as well
184:18 - as each column if there's adjacency
184:20 - between two nodes well then there will
184:22 - be a one at that row and column this
184:25 - next part really isn't necessary this is
184:27 - kind of the general idea of an adjacency
184:29 - matrix but let's add some headers to the
184:31 - rows and columns within the graph class
184:33 - at the top let's create an arraylist
184:35 - array list and the data type will be
184:38 - node
184:40 - and i will name this nodes
184:42 - when we construct a graph object let's
184:44 - instantiate our nodes arraylist
184:47 - nodes equals new array
184:50 - list
184:53 - and when we add a node we just take our
184:55 - nodes arraylist dot add
184:58 - node
184:59 - and within the print method let's make a
185:01 - few changes
185:03 - so preceding our nested for loops let's
185:06 - print the data found within each node so
185:08 - it serves as a header and i can use it
185:11 - for each loop for this
185:12 - for every node node
185:15 - in
185:16 - nodes
185:18 - then let's print
185:21 - the node's data then maybe i'll add a
185:24 - space
185:25 - oh then add a new line
185:27 - okay there we go each column has the
185:30 - data found within each node and let's
185:32 - also do the same thing with each of
185:33 - these rows i think it would look cool
185:35 - before the inner for loop let's do the
185:38 - same thing
185:39 - let's copy this line
185:42 - but this would be it nodes dot get
185:46 - index of i
185:48 - dot data and let's see how that looks so
185:51 - far
185:51 - okay not too bad but let's add a few
185:54 - spaces
185:55 - so system.out.print
185:57 - and i'll just print two spaces all right
186:00 - there's our adjacency matrix if you're
186:02 - working with more complex data let's say
186:04 - city names i would consider using printf
186:07 - statements instead because you can align
186:09 - things properly and if you do want to
186:11 - check an edge we did create a check edge
186:13 - method
186:14 - so within a print line statement let's
186:16 - invoke the graph dot check edge method
186:21 - and then pass in two indices
186:23 - so let's see if there's an edge between
186:25 - nodes a and b so a has an index of zero
186:29 - b has an index of one there is an edge
186:31 - between these two so this will return
186:33 - true there is an edge this time let's
186:35 - check to see if there's an edge between
186:37 - d
186:38 - and c so d has an index of zero one two
186:42 - three
186:45 - and c has an index of two
186:48 - and this returns false there is no edge
186:51 - all right people so that's an adjacency
186:53 - matrix it's an array to store ones and
186:56 - zeros to represent edges the number of
186:59 - rows and number of columns is equal to
187:01 - the number of unique nodes the runtime
187:03 - complexity to check an edge is big o of
187:06 - one all we need are two indices however
187:08 - the space complexity for an adjacency
187:10 - matrix is big o of v squared take the
187:13 - number of nodes you have and square it
187:15 - so if i have five nodes five squared is
187:18 - 25 we have 25 elements alright so that's
187:21 - an adjacency matrix if you would like a
187:23 - copy of this code i'll post this to the
187:25 - comment section down below and well yeah
187:27 - that's an adjacency matrix in computer
187:29 - science
187:31 - hey what's going on everybody it's you
187:33 - bro hope you're doing well and in this
187:34 - video i'm going to explain adjacency
187:36 - lists in computer science so sit back
187:39 - relax and enjoy the show
187:41 - all right what's going on everybody
187:43 - adjacency list an adjacency list is an
187:45 - array or an array list made up of linked
187:48 - lists each element is a separate linked
187:51 - list and each linked list can contain
187:53 - nodes and each linked list has a unique
187:56 - node at the head and basically speaking
187:59 - to represent a graph all adjacent
188:01 - neighbors to a node are added to that
188:03 - node's linked list if we add an edge we
188:06 - just add the address of that node to the
188:08 - tail so let's begin let's create two
188:10 - classes graph and node
188:13 - file new class this will be graph
188:16 - finish
188:17 - then file new class
188:20 - node
188:22 - let's say we have some data maybe just a
188:24 - single character char data
188:27 - and we'll create a constructor
188:29 - and then pass in some data
188:31 - char data
188:33 - this dot data equals data
188:36 - within our graph class we need to create
188:38 - an array list of linked lists
188:41 - array
188:42 - list and the data type of what's going
188:44 - to be stored are linked
188:47 - now we need a data type for our linked
188:49 - lists what are the linked lists going to
188:51 - store well they're going to store nodes
188:54 - let's name this array list just a list
188:57 - for adjacency list
188:59 - and let's create a graph constructor
189:02 - and we will instantiate our adjacency
189:05 - list a list equals new
189:08 - array
189:09 - list
189:11 - okay let's declare some methods we'll
189:13 - need an add node method so public void
189:16 - add node
189:18 - and there is one parameter a node of the
189:21 - node data type
189:23 - we'll need to add an edge
189:26 - public void add edge
189:28 - we'll need two indices a source and a
189:31 - destination
189:32 - into source
189:34 - int destination
189:36 - we'll be able to check an edge
189:38 - public void
189:40 - check edge
189:42 - again we'll need a source and
189:43 - destination indices
189:45 - and let's print our graph
189:47 - public void print
189:51 - all right now let's head to our main
189:52 - class and instantiate our graph
189:55 - graph
189:56 - graph equals new graph
189:59 - we're going to reuse a lot of the same
190:00 - code from the previous topic on
190:02 - adjacency matrices so let's add some
190:05 - nodes graph dot add node and i will pass
190:09 - in an anonymous node new node
190:13 - and we need some data
190:15 - so let's pass in the letter a
190:17 - let's copy this and create four
190:18 - additional nodes
190:21 - so
190:22 - b
190:23 - c
190:24 - d
190:24 - and e
190:26 - and we need to create some edges again
190:28 - i'm using the edges from the previous
190:30 - topic so graph dot add edge each of
190:33 - these nodes has an index number the
190:35 - first will be zero the second will be
190:37 - one then two
190:39 - three four if i need an edge between
190:42 - nodes a and b the index in node a is
190:44 - zero that's the source and the
190:46 - destination is b that has an index of
190:49 - one so zero one
190:51 - and let's fill in a few others how about
190:54 - one
190:55 - and two
190:58 - one and four
191:02 - two
191:02 - three
191:06 - two
191:07 - four
191:09 - four
191:10 - zero
191:12 - and four
191:14 - two
191:16 - okay so then at the end let's print our
191:18 - graph but we have not yet filled in the
191:19 - methods so graph dot print
191:22 - okay so let's head to our graph class
191:24 - and fill in some of these methods let's
191:26 - begin with add node so in order to add a
191:29 - node we first need to create a new
191:31 - linked list
191:32 - so linked
191:33 - list the data type of this linked list
191:36 - is nodes
191:37 - and let's name this current list
191:40 - equals new
191:42 - linked list
191:45 - after we create this new linked list we
191:47 - can add a node to the linked list
191:49 - current list dot add node
191:53 - whenever we create a new node we will
191:55 - also create a new linked list and the
191:58 - new node will be at the head of the
191:59 - linked list and lastly we just need to
192:02 - add this linked list to our array list
192:04 - alist
192:06 - dot add
192:07 - current list
192:09 - okay then let's fill in the add edge
192:11 - method i will declare a linked list and
192:13 - i'll just copy this linked list current
192:16 - list
192:16 - equals to add an edge to our adjacency
192:19 - list we first need to get a linked list
192:21 - from the arraylist
192:23 - so let's store that within currentlist
192:25 - currentlist equals to access our
192:28 - adjacency list we will type
192:31 - dot get
192:32 - and then an index and that will be
192:34 - source
192:35 - this will return a linked list it's kind
192:38 - of like it's in two layers we'll also
192:40 - need to know our destination node
192:42 - so let's say node
192:44 - destination node equals
192:47 - then we'll need to find the array list
192:49 - that this node is located at the head
192:51 - adjacency list dot get
192:54 - our destination index
192:56 - then follow this with get zero that is
192:59 - the head of our linked list so this is
193:02 - the address of the node we would like to
193:04 - link to and now we just need to add this
193:06 - node to the tail of our current list
193:09 - current list dot add destination node
193:14 - and that's it we're taking a node and
193:16 - adding it to the tail of a linked list
193:19 - you can shorten this code if you would
193:20 - like to do so in less steps you would
193:22 - just take this portion
193:25 - and replace current list with
193:27 - alist.getsource then you technically
193:29 - don't need this line but it's a little
193:31 - more difficult to read so you do you
193:33 - okay this time let's check an edge so we
193:35 - can copy these two lines of code
193:38 - paste it what we're going to do is
193:40 - iterate over our current linked list and
193:42 - see if there's a match between a node
193:44 - and our destination node so let's use a
193:47 - for each loop and we will iterate over
193:49 - all of the nodes within our current
193:51 - linked list
193:53 - so the data type is node
193:55 - node
193:56 - in our current list with an if statement
194:00 - let's check to see if the current node
194:03 - that we're looking at is equal to our
194:05 - destination node are these addresses the
194:07 - same
194:08 - if so then return true if we escape the
194:11 - for loop that means we did not find the
194:13 - node we were looking for so let's return
194:15 - false and the return type of this method
194:17 - is going to be boolean okay we have one
194:19 - more method we just need to print our
194:21 - adjacency list we'll use nested for each
194:24 - loops
194:26 - so we need to iterate over all of the
194:29 - linked lists within our array list
194:32 - so for
194:33 - linked list
194:34 - the data type is node
194:36 - and let's name this current
194:39 - list
194:40 - iterate over every linked list in our
194:42 - array list
194:44 - and then we'll need another for each
194:45 - loop
194:46 - for every node node in current list then
194:52 - let's use a print statement
194:55 - print the node's data
194:57 - then maybe i'll add an arrow for flavor
194:59 - to represent a linked list
195:02 - then outside of our inner for loop let's
195:04 - print a new line and that should be
195:06 - everything that we need so let's run
195:08 - this
195:09 - all right so there is our adjacency list
195:12 - basically an adjacency list is an array
195:14 - or an array list made up of linked lists
195:17 - each linked list has unique node at the
195:19 - head and all adjacent neighbors to that
195:21 - node are added to the nodes linked list
195:24 - at the tail the runtime complexity to
195:26 - check an edge is big o of the v for the
195:29 - number of vertices it's because we need
195:31 - to traverse a linked list linearly to
195:34 - find a matching node and the space
195:35 - complexity for an adjacency list is big
195:38 - o of v plus e v as in the number of
195:40 - vertices e as in the number of edges
195:43 - so yeah that's an adjacency list it's an
195:46 - array or an array list made up of linked
195:48 - lists it's used to represent a graph if
195:50 - you would like a copy of this code i'll
195:52 - post this to the comment section down
195:54 - below and well yeah those are adjacency
195:56 - lists and computer science
195:59 - alright everybody depth first search
196:02 - depth first search is a search algorithm
196:04 - for traversing a tree or graph data
196:07 - structure we can break this down into
196:08 - three steps when navigating a graph we
196:11 - will pick a route and we will keep on
196:12 - going until we reach a dead end or a
196:15 - previously visited node if we do then we
196:17 - will move on to step three we will
196:19 - backtrack to the last node that has
196:22 - unvisited adjacent neighbors let's
196:24 - navigate this maze using a depth first
196:26 - search approach here's the entrance and
196:29 - here's the exit so the general concept
196:31 - of a death first search is that when we
196:33 - reach more than one adjacent neighbor
196:35 - we're just going to pick a route let's
196:37 - say we prefer right turns over left
196:39 - turns but you can really set any rule
196:40 - that you want when faced with more than
196:43 - one adjacent neighbor i'm just going to
196:44 - pick her out and keep on going
196:47 - and if we reach a dead end then we're
196:49 - just going to backtrack to a node that
196:50 - has some unvisited adjacent neighbors
196:53 - so there's no more place we can go so
196:55 - we're going to backtrack all the way
196:57 - back to
196:58 - this intersection right here this route
197:00 - is unvisited so we will continue going
197:04 - not backtracking yet there's no dead
197:05 - ends but now there is so we backtrack
197:08 - and keep on going
197:11 - so this is what this looks like sped up
197:26 - and we have reached the end
197:28 - let's use a depth first search approach
197:30 - to navigate this graph maybe we're at
197:32 - node a and we need to travel to i but we
197:34 - don't know where i is one way in which
197:36 - we can keep track of our position is to
197:38 - use a stack or in the case of recursion
197:41 - we can use a call stack whenever we
197:43 - visit a node we will push it onto the
197:44 - stack we can either travel to nodes b or
197:47 - d so we will mark d as visited and push
197:50 - this node onto the stack then we can
197:52 - either go to e or g let's go to g
197:55 - we'll push that onto the stack
197:58 - then h
197:59 - e
198:00 - and now we circle the round back to d
198:03 - but d is already marked as visited so
198:05 - we're going to backtrack to a node that
198:07 - has unvisited adjacent neighbors which
198:09 - is a
198:10 - so we will pop all of these nodes off of
198:12 - the stack all the way back to a which
198:14 - has unvisited adjacent neighbors and
198:17 - this time we will go down this route
198:19 - and push all of these nodes onto our
198:21 - stack
198:23 - and we have reached the end using a
198:25 - depth first search approach when
198:27 - simplified you pick a route you keep
198:29 - going when you reach a dead end or a
198:31 - node you already visited you backtrack
198:33 - to a node that has unvisited adjacent
198:36 - neighbors and you repeat steps one
198:37 - through two all right well let's
198:39 - implement this in code now
198:42 - and here we are people so i'm going to
198:44 - be using a graph that utilizes an
198:46 - adjacency matrix if you're using an
198:49 - adjacency list the code's just going to
198:51 - be a little bit different but the
198:52 - concept is really still the same i'm
198:54 - using code from the previous few videos
198:56 - we have a node class a graph class and
198:59 - in the previous videos we've already
199:01 - populated this graph with nodes and
199:03 - edges and then i'm just going to print
199:05 - this this is my adjacency matrix now
199:08 - within the graph class we're going to
199:09 - create a depth first search method and a
199:12 - helper method so this will be public
199:15 - void and we will name this depth
199:18 - first
199:19 - search
199:20 - and there will be one parameter an index
199:23 - of where we would like to begin
199:25 - to keep track of the nodes that we've
199:27 - already visited what some people like to
199:29 - do is that they will create a boolean
199:31 - within their node class such as boolean
199:34 - visited and they'll just mark it as
199:36 - false or true however it's very easy to
199:38 - forget to change these back to false
199:40 - when you exit the depth first search so
199:42 - what i'm going to do instead is create
199:44 - an array of booleans and the size will
199:46 - be equal to the length of the matrix
199:48 - so let's create a boolean array and i
199:51 - will name this visited
199:53 - equals new boolean
199:56 - and the size of our array is the length
199:58 - of our matrix
200:01 - then lastly we will implement a helper
200:03 - function so let's name this dfs
200:07 - helper
200:08 - and we will pass in our source and our
200:10 - boolean array visited then we just need
200:13 - to create this method
200:15 - private void dfs helper and there are
200:18 - two parameters int source and an array
200:20 - of booleans named visited you can either
200:23 - implement a depth first search
200:24 - iteratively using a stack or you can
200:27 - utilize the call stack if you use
200:28 - recursion in this example we're going to
200:30 - use recursion when we invoke this helper
200:33 - function we're going to check to see if
200:35 - the current node that we're on is
200:36 - visited or not and we can use an if
200:38 - statement
200:40 - if
200:41 - our visited array at index of source is
200:46 - equal to true or you could write the
200:48 - shorthand and just say if visited at
200:50 - index of source since this returns a
200:52 - boolean value if we've already visited
200:55 - this node we're going to return else we
200:57 - will mark this node as visited
200:59 - else
201:00 - visited at index of source
201:04 - equals true
201:05 - if you would like although this part's
201:07 - not necessary within my console i'm just
201:09 - going to print that we visited this node
201:11 - so within a print line statement
201:13 - i do have my nodes within an arraylist
201:15 - i'm just going to access the data of one
201:17 - of these
201:19 - nodes dot get
201:21 - source
201:22 - dot data
201:24 - plus
201:26 - equals visited this part technically
201:29 - isn't necessary but it's going to help
201:30 - with my demonstration in this example
201:32 - maybe we start at node a we need to find
201:35 - any adjacent neighbors if we're using an
201:37 - adjacency matrix we need to iterate over
201:40 - this row we can use a for loop for that
201:45 - so into i equals zero
201:47 - we will continue this as long as i is
201:50 - less than
201:51 - our matrix
201:52 - at index of source
201:55 - dot length this equals the length of a
201:58 - row and then increment i by one we're
202:00 - checking to see if one of these elements
202:02 - is a one that means that's an adjacent
202:04 - neighbor that we can travel to
202:07 - using an if statement
202:09 - if matrix at indices
202:12 - of source
202:13 - and i
202:15 - source is the row that we're working
202:16 - with i is the column if this is equal to
202:19 - 1
202:20 - then we will invoke the dfs helper
202:23 - method again so this is recursive we
202:25 - will pass in i
202:27 - as well as our boolean array named
202:29 - visited if we successfully search
202:32 - through an entire row outside of the for
202:34 - loop let's return and that's it so let's
202:37 - run this within my main class i will
202:39 - call the graphs
202:40 - depth first search method and pass in an
202:43 - index of a starting node so let's begin
202:45 - at zero
202:47 - we visit a first then b c d e let's try
202:51 - b which has an index of one
202:54 - b c d e a
202:57 - c which has an index of two
203:00 - c d e a b okay now pay attention to this
203:03 - this is a directed graph and we're
203:05 - beginning at d
203:06 - there's no place that we can go so we're
203:08 - stuck at d we only visit d
203:11 - and lastly we have e
203:12 - which has an index of four
203:15 - e a b c d
203:17 - alright everybody that is the depth
203:19 - first search algorithm you pick a route
203:21 - you keep going if you reach a dead end
203:24 - or an already visited node you backtrack
203:26 - to a previous node with unvisited
203:28 - adjacent neighbors if you would like a
203:30 - copy of this code i'll post this to the
203:31 - comment section down below don't forget
203:33 - to give this video a thumbs up leave a
203:35 - random comment down below and subscribe
203:37 - if you'd like to become a fellow bro
203:40 - all right what's going on people breath
203:42 - first search breadth first search is a
203:45 - search algorithm for traversing a tree
203:48 - or graph data structure this is done one
203:51 - level at a time rather than one branch
203:53 - at a time like what you see with depth
203:55 - first search here's an example in the
203:58 - previous topic on depth first search we
204:00 - would navigate a graph one branch at a
204:03 - time but in a breadth-first search
204:05 - approach we would navigate this graph
204:07 - one level at a time so let's begin at
204:09 - node a and we are attempting to travel
204:11 - to node i instead of a stack we'll use a
204:14 - queue all unvisited nodes we will add to
204:16 - the queue so we're currently at a we'll
204:19 - add that to the queue then add any
204:21 - unvisited adjacent neighbors to the
204:23 - queue as well we have both b and d as
204:25 - neighbors
204:27 - so we will add these to the q
204:30 - then add any unvisited adjacent
204:32 - neighbors of nodes b and d
204:34 - that means we will add c
204:38 - e and g
204:41 - and for the next level we have f
204:44 - and h
204:46 - and lastly i
204:49 - so that is a breadth first search
204:51 - approach we will navigate a graph one
204:54 - level at a time using a queue rather
204:56 - than one a branch at a time using a
204:58 - stack like with depth first search let's
205:01 - implement this in code now
205:03 - okay everyone so in my graph i'm
205:05 - utilizing an adjacency matrix we're
205:08 - reusing the same code from the previous
205:10 - few videos and we have a node class that
205:13 - contains some data and i went ahead and
205:15 - populated this graph already it's the
205:17 - same data from the previous few topics
205:20 - heading to our graph class let's create
205:22 - a breadth first search method
205:24 - public void
205:26 - breadth
205:28 - first
205:29 - search
205:31 - and we will take an integer name source
205:35 - this will be the index of the node we
205:37 - would like to begin searching at and
205:39 - with a breadth first search we can
205:40 - utilize a q
205:44 - so q
205:45 - we will store integers these will be
205:47 - indices
205:49 - and i will name this q
205:52 - equals new
205:54 - now queues are actually interfaces we
205:56 - need to use a data structure that
205:58 - utilizes the queue interface one of
206:00 - which is a linked list
206:05 - okay so we have our queue and we're
206:07 - going to create an array of booleans to
206:10 - mark if a node has been visited or not
206:12 - so let's create a boolean array
206:15 - named visited
206:17 - equals new boolean
206:20 - and the size will be matrix dot length
206:25 - with the node that will begin at let's
206:27 - add that to the queue q
206:29 - dot you can use add
206:31 - or offer
206:34 - and then pass in the index of the
206:36 - starting node
206:37 - then within the boolean array of visited
206:40 - we will mark this as true at index of
206:43 - source
206:44 - equals true and now we'll need a while
206:47 - loop
206:49 - our condition is that we'll continue
206:51 - this while loop
206:53 - while the q's
206:55 - size method
206:56 - does not equal zero we'll assign our
206:59 - source equal to whatever is at the front
207:02 - of the queue
207:03 - q dot pull
207:05 - to remove an element with this code that
207:07 - i've written i've already went ahead and
207:08 - created an array list of the nodes to
207:10 - access the data whenever we pull a node
207:13 - i'm going to display the data so this
207:15 - part technically isn't necessary
207:18 - nodes dot get source
207:20 - dot data
207:22 - plus
207:24 - equals visited let's say that we're at
207:27 - node a we're going to iterate over this
207:29 - row and look for any adjacent neighbors
207:31 - so let's use a for loop for that
207:35 - four and i
207:37 - equals zero
207:38 - we will continue this as long as i is
207:41 - less than our matrix at index of source
207:45 - dot length
207:47 - this means the length of the row then i
207:50 - plus plus
207:51 - during each iteration let's check to see
207:53 - if this value is 1
207:56 - and the node that we're trying to visit
207:58 - has not already been visited if
208:02 - then
208:03 - matrix
208:05 - at indices
208:06 - of source
208:08 - and i
208:09 - is equal to one
208:11 - and visited
208:13 - at index of i
208:16 - is not true so we can use the not
208:18 - logical operator if we have an adjacent
208:20 - neighbor that's not been visited then we
208:23 - will add the index to the queue and that
208:25 - node is going to wait in line
208:27 - q
208:28 - dot offer
208:31 - i and i is an index
208:33 - and then mark this note as visited so
208:35 - take our boolean array visited at index
208:38 - of i
208:39 - set the sequel to true and there we go
208:42 - let's invoke the breadth first search of
208:45 - our graph class then pass in an index of
208:48 - a node we would like to begin at in this
208:50 - example node a has an index of zero b is
208:54 - one c is two so on and so forth so let's
208:57 - perform a breath first search beginning
208:59 - at node a
209:02 - we will cover these nodes in this order
209:04 - a b c e d let's change this to 1 that
209:08 - would be node b
209:10 - b c e d a
209:12 - 2 is c
209:14 - c d e a b
209:16 - 3 is d
209:17 - we can't go anywhere from node d
209:20 - so only d is visited
209:22 - and e
209:24 - e a c b d
209:27 - now before we wrap things up here are
209:28 - the differences between breadth and
209:30 - depth-first searches breadth traverses a
209:33 - graph level by level depth traverses a
209:36 - graph branch by branch
209:38 - breadth utilizes aq depth utilizes a
209:42 - stack breadth tends to be better if the
209:44 - destination is on average close to the
209:46 - start and depth tends to be better if
209:49 - the destination is on average far from
209:51 - the start in a breath first search
209:53 - siblings are visited before children in
209:56 - a depth first search children are
209:58 - visited before siblings and if you ever
210:00 - plan on creating video games depth first
210:02 - searches tend to be more popular than
210:04 - breadth first searches alright everybody
210:07 - that is the breadth first search if you
210:09 - would like a copy of this code i'll post
210:10 - this to the comment section down below
210:12 - don't be afraid to give this video a
210:14 - thumbs up drop a random comment down
210:16 - below and subscribe if you'd like to
210:17 - become a fellow bro
210:20 - all right everybody here's a quick
210:22 - introduction to trees in this video
210:25 - we're only going to cover some of the
210:26 - terminology a tree is a non-linear data
210:29 - structure where nodes are organized in a
210:31 - hierarchy this could be one example of a
210:34 - tree it's made up of nodes and edges
210:36 - nodes could be some piece of data and
210:38 - edges represent a relationship between
210:40 - two nodes a real-life example of a tree
210:43 - could be a family tree maybe we're at no
210:46 - d that's us or me or you b is a parent e
210:50 - is your brother or sister a is a
210:52 - grandparent
210:54 - c could be an aunt or uncle and f is
210:56 - your cousin a few examples of where a
210:58 - tree data structure would be used in
211:00 - programming or technology would be file
211:02 - explorers databases domain name servers
211:06 - and the html document object model the
211:08 - top of the tree is known as the root
211:11 - node the root node doesn't have any
211:13 - incoming edges only outgoing edges
211:16 - any nodes at the bottom of a tree are
211:18 - known as leaf nodes they do not have any
211:21 - outgoing edges but they do have incoming
211:23 - edges and branch nodes are in the middle
211:26 - they have incoming and outgoing edges
211:28 - here's a few other terms any nodes with
211:31 - outgoing edges are also known as parents
211:34 - any node with an incoming edge is known
211:36 - as a child node and nodes can be both
211:39 - parents and children if they have both
211:41 - incoming and outgoing nodes any two
211:43 - children that share the same parent are
211:46 - known as siblings nodes d and e both
211:49 - share the same parent of b so d and e
211:52 - are both siblings the same thing goes
211:54 - with b and c nodes b and c share a as a
211:57 - parent so these two nodes are siblings
212:00 - it's just like a family tree a sub tree
212:02 - is a smaller tree held within a larger
212:05 - tree you could say that this portion is
212:07 - a sub-tree
212:08 - the size of a tree is equal to the
212:10 - number of nodes so what's the size of
212:12 - this tree
212:14 - we have one node two
212:16 - three four five six
212:19 - the size of this tree is six six nodes
212:22 - the depth of a node is the number of
212:24 - edges below the root node our root node
212:27 - would have a depth of zero
212:29 - then as we move down levels b and c have
212:32 - a depth of one
212:33 - d e and f have a depth of two
212:36 - and lastly the height of a node is the
212:38 - number of edges above the furthest leaf
212:41 - node all of these leaf nodes are the
212:43 - same distance away from the root node
212:45 - these would all have a height of zero b
212:48 - and c have a height of one and a has a
212:51 - height of two
212:52 - well all right then everybody that is a
212:54 - quick introduction to some of the
212:56 - terminology around trees it's a
212:58 - non-linear data structure where nodes
213:00 - are organized in a hierarchy and in the
213:03 - next few topics we'll get a little more
213:04 - in depth with trees if you found this
213:07 - video helpful please help me out by
213:08 - smashing that like button leave random
213:10 - comments down below and subscribe if
213:12 - you'd like to become a fellow bro
213:14 - all right what's going on everybody in
213:16 - this video we're going to discuss binary
213:19 - search trees but first we'll need to
213:21 - know what a binary tree is a binary tree
213:24 - is a tree where each node has no more
213:27 - than two children that's why it's binary
213:30 - in this example node one has two
213:32 - children two and three and each of these
213:34 - children have their own children they
213:36 - have no more than two children nodes
213:39 - four and five are the children of two
213:41 - and nodes six and seven are the children
213:43 - of three in the similar example this
213:46 - would also be a binary tree node 3 only
213:49 - has one child node 6 but with a binary
213:52 - tree no node has more than 2 children
213:55 - since node 3 has one child that's fine
213:58 - but if node 3 had three children well
214:01 - that would no longer be a binary tree
214:03 - now what makes a binary search tree
214:05 - different from a binary tree is that the
214:08 - values are arranged in a certain order
214:10 - here's the order the root node should be
214:13 - greater than the left child but less
214:16 - than the right child four is greater
214:18 - than two and four is less than six
214:21 - that's the pattern that this data
214:23 - structure is built around if we take a
214:25 - look at some of the sub trees let's say
214:26 - this one two is greater than one but
214:29 - less than three
214:31 - and in this subtree
214:32 - six is greater than five but less than
214:35 - seven
214:36 - if done correctly starting at the root
214:38 - node the left most child should be the
214:41 - least value and again starting from the
214:44 - root node the rightmost child should be
214:46 - the greatest value the reason that these
214:48 - nodes are arranged this way is for quick
214:50 - lookup let's say we're looking for five
214:53 - what we'll do is compare five to the
214:55 - root node if 5 is equal to the root node
214:57 - well then we found our answer if 5 is
215:00 - greater than our root node we will move
215:02 - down to the right branch and do the same
215:04 - thing again is 5 equal to 6 no it's not
215:07 - however it's less than 6 so we know to
215:09 - go down at this branch the runtime
215:11 - complexity to find a value within a
215:14 - binary search tree is big o of log n in
215:17 - its best case so let's code a binary
215:19 - search tree now
215:20 - okay let's begin let's create a node
215:23 - class file new class i will name this
215:26 - node
215:27 - and finish
215:28 - nodes will have some data maybe an
215:30 - integer this time int data and the node
215:33 - class should contain at most two nodes
215:37 - node left for the left child and node
215:41 - right for the right child if a node is a
215:44 - leaf node well then these will be null
215:46 - but we would still like to allocate
215:47 - space to hold children
215:50 - and within the constructor we will pass
215:52 - in some data
215:54 - int data and assign it
215:57 - this dot data equals data
216:00 - now let's create a binary search tree
216:02 - class file new class
216:06 - binary
216:07 - search
216:08 - tree
216:10 - and within our main class let's
216:12 - instantiate a binary search tree object
216:14 - binary search tree i'll name this tree
216:18 - equals new binary search tree
216:21 - and within the binary search tree class
216:23 - each binary search tree should have a
216:26 - root node
216:27 - node
216:28 - root
216:29 - here are some methods we'll need an
216:31 - insert method to insert nodes public
216:34 - void
216:35 - insert
216:37 - and there is a parameter of a node
216:40 - node
216:41 - node
216:42 - now let's create a second method this
216:43 - next method will be a helper method of
216:46 - the insert method so the insert method
216:48 - is going to call its helper method and
216:50 - this will be private
216:53 - the return type is node
216:56 - insert
216:57 - helper
216:58 - we need a root node node
217:01 - root
217:03 - and a node
217:04 - node
217:05 - node
217:07 - in order for a program to compile we do
217:09 - need to return something so just as a
217:11 - placeholder i'm going to return null
217:14 - okay for the next method we need a
217:16 - display method
217:18 - public
217:19 - void
217:20 - display
217:23 - and helper method
217:25 - private
217:26 - void
217:27 - display
217:30 - helper
217:32 - and this has one parameter a root node
217:34 - node root next method is a search method
217:38 - public
217:40 - boolean we're returning a boolean
217:42 - search
217:44 - and the parameter is
217:47 - int
217:48 - data
217:49 - and we will need a helper method
217:53 - private
217:55 - boolean
217:56 - search
217:58 - helper
218:01 - there are two parameters
218:03 - node root
218:05 - and int
218:06 - data
218:07 - again we do need to return something i'm
218:09 - going to return false just as a
218:12 - placeholder
218:14 - okay next method is a remove method
218:17 - public
218:18 - void
218:19 - remove
218:22 - and the parameter is int
218:24 - data and we'll need a helper method
218:28 - the return type is node
218:31 - remove
218:32 - helper the two parameters are node root
218:36 - and int data
218:38 - we do have to return something so as a
218:40 - placeholder i'm going to return null
218:42 - our remove helper method is also going
218:44 - to rely on two separate methods one to
218:47 - find a successor and another to find a
218:49 - predecessor this is in case we're
218:51 - deleting nodes we have to shift nodes
218:53 - around so this will be private
218:56 - int
218:57 - successor
219:00 - and we will pass in
219:02 - node
219:03 - root
219:05 - and we need to return something let's
219:07 - return 0 as a placeholder and let's copy
219:10 - this paste it and this will be method
219:13 - predecessor
219:16 - and that will be it so let's close these
219:18 - for now and begin with the insert method
219:22 - so with the insert method we will assign
219:24 - our root node equal to insert
219:28 - helper
219:30 - method
219:31 - pass in our root node
219:34 - and node
219:35 - so the reason that we're using helper
219:37 - methods is that we'll be using recursion
219:39 - so it's a lot easier to use recursion if
219:41 - you have a helper method within the
219:43 - insert helper method
219:45 - let's declare int data
219:48 - equals
219:49 - our nodes data node is the node that
219:52 - we're passing in to insert
219:54 - so let's check to see if our root node
219:56 - is assigned or not
219:58 - so if
220:00 - our root is equal to null
220:02 - then we should probably assign this node
220:04 - to the root node because well this is
220:06 - the first node
220:08 - root equals node
220:10 - return root
220:12 - if root is not null we have to compare
220:14 - the data to see if it's less than our
220:16 - root or greater than our root else if
220:20 - our data
220:22 - is less than
220:23 - the data of the root node
220:26 - then we are going to assign this node as
220:29 - the left child of our root node
220:31 - are roots left child root dot left
220:34 - equals
220:35 - and we will use recursion
220:37 - insert
220:38 - helper method
220:41 - pass in our roots left node
220:45 - and our node
220:46 - the root node is going to change with
220:47 - recursion at first it's going to be the
220:50 - root node of the entire tree after
220:52 - recursion we're examining the root node
220:55 - of a subtree
220:56 - so if we're passing in the left child of
220:59 - the original root node well then that
221:02 - left child is now the root node of a
221:04 - subtree that we're currently working
221:05 - with so else if the data is less than
221:08 - the current root node we go left if it's
221:11 - greater than the root node then we will
221:13 - go right
221:15 - else
221:16 - root dot right that's the right child
221:20 - equals
221:21 - insert
221:22 - helper method
221:24 - pass in
221:26 - root
221:26 - dot write
221:28 - and our node and it's the same process
221:30 - all over again
221:32 - and at the end we will return the
221:34 - current root node
221:36 - okay that is it for the insert method
221:38 - and the insert helper method
221:40 - so let's insert a few nodes
221:42 - although we can't yet display them
221:44 - let's insert some anonymous nodes
221:47 - tree dot insert and we can pass in a
221:50 - node either a node name or an anonymous
221:53 - node
221:54 - so let's pass in some anonymous nodes
221:58 - uh let's pass in the number five or some
222:00 - other number of your choosing
222:03 - and let's pass in a bunch of numbers
222:05 - how about one and nine make sure these
222:08 - are not in order seven and three
222:13 - six
222:14 - four and maybe one more
222:17 - eight if you run and compile this there
222:19 - is nothing obvious that really happens
222:21 - so let's work on a display method next
222:24 - so with the display method we will
222:25 - invoke our helper method display helper
222:29 - pass in the root node within the display
222:32 - helper method we're going to check to
222:34 - see if the root node of our subtree does
222:36 - not equal no
222:38 - so if
222:40 - root does not equal no
222:43 - if you would like to display these in
222:45 - order we can use in order traversal and
222:48 - that uses recursion
222:49 - so invoke the display helper method
222:53 - pass in the root child's left node
222:57 - root.left if we're using recursion the
223:00 - first piece of data that's displayed is
223:03 - the least value and these values will be
223:06 - displayed in increasing order
223:08 - technically the term is non-decreasing
223:10 - but think of it as ascending order so
223:12 - the very first value is going to be the
223:14 - least one followed by the root node of
223:17 - this subtree
223:19 - system.out.printline root.data
223:22 - in our first subtree the data all the
223:24 - way to the left is one and the root node
223:27 - of that subtree is two then we need the
223:29 - right child which should theoretically
223:31 - be three so again we'll invoke the
223:33 - display helper method and pass in the
223:36 - right child so this is in order traverso
223:39 - all of the nodes will be displayed in
223:41 - non-decreasing order so let's try this
223:44 - at the end of our main class let's
223:46 - invoke tree dot display
223:50 - and see what happens
223:52 - yeah there we go
223:55 - all of the nodes within our binary
223:57 - search tree are now in order if you
223:59 - would like this in reverse order you can
224:00 - just change these methods around
224:02 - replace left with right
224:05 - and right with left
224:07 - and these are now in decreasing order
224:10 - but let's change that back
224:12 - okay up next we have the search methods
224:15 - okay search will return
224:18 - then invoke the search
224:20 - helper method
224:22 - pass in our root node
224:24 - as well as some data within the search
224:27 - helper class
224:29 - we'll check to see if root is equal to
224:31 - null
224:33 - that means our tree is empty so of
224:35 - course we can't search for anything
224:38 - return
224:39 - false
224:40 - then add
224:42 - else if our root dot data is equal to
224:47 - data that means we found the data that
224:49 - we're looking for there's a match then
224:51 - we're going to return true
224:54 - we found what we're looking for
224:56 - else
224:57 - if our roots
224:59 - data
225:00 - is greater than our current data that
225:03 - we're looking for that means we need to
225:05 - go left
225:06 - so return
225:07 - then invoke the search helper method
225:11 - pass in the left child root dot left and
225:15 - the left child is now the root node of
225:17 - the subtree
225:19 - and then you need to pass in data as
225:20 - well
225:21 - else we go right
225:24 - so copy this paste it
225:27 - return search helper root dot write and
225:32 - data and we can get rid of this return
225:34 - statement at the bottom
225:35 - okay let's try this
225:37 - so at the end of my main class let's use
225:40 - a print line statement and i will type
225:42 - tree dot search
225:45 - and let's search for xero none of my
225:48 - nodes have zero as a piece of data so
225:50 - this should return false
225:53 - let's search for one now
225:55 - that returns true there's a one within
225:57 - one of my nodes
225:59 - what about nine
226:01 - that is also true
226:03 - and let's try ten
226:05 - and that is false
226:07 - okay so those are the search methods
226:10 - and lastly we have the remove methods as
226:14 - well as successor and predecessor now
226:17 - these methods are going to be a little
226:18 - bit tough but i'll try my best to
226:19 - explain it so within an if statement
226:22 - let's check to see if this data even
226:24 - exists first so let's invoke the search
226:26 - method and pass in our data so this
226:30 - returns a boolean true or false if we do
226:33 - find the data that we're looking for
226:35 - then let's invoke the remove helper
226:37 - method
226:39 - pass in our root node as well as our
226:42 - data
226:43 - then within an else statement and this
226:45 - part's optional
226:46 - let's let the user know that we can't
226:48 - find that data
226:50 - data plus
226:53 - could not be found
226:55 - so just to test things real quick i'm
226:57 - going to remove some data that doesn't
226:59 - exist
227:00 - tree dot remove
227:02 - zero
227:04 - zero cannot be found now let's move on
227:07 - to the remove helper method
227:10 - the first thing that we'll do is check
227:11 - to see if our root node is equal to null
227:15 - if
227:16 - our root node is equal to null
227:19 - then let's return the root node
227:23 - then within an else if statement let's
227:26 - check to see if the data that we're
227:28 - trying to remove
227:29 - is less than the data of our root node
227:33 - if it is we need to go left down the
227:35 - binary tree
227:37 - root dot left
227:39 - equals
227:40 - then invoke the remove helper method
227:43 - pass in the left child of the root node
227:45 - root.left as well as our data so we're
227:49 - going to go as far left as we can then
227:51 - add an else if statement
227:53 - else
227:54 - if
227:55 - data is greater than the data of the
227:58 - root node then we will go right so let's
228:01 - copy this paste it root dot write equals
228:05 - remove helper pass in root dot right as
228:09 - well as our data then add an else
228:12 - statement if we reach the else statement
228:14 - that means that we have found our node
228:16 - and i'll just add a note to explain that
228:18 - if the node we're trying to remove has
228:20 - children that kind of complicates things
228:22 - then we have to shift the nodes around
228:24 - but first let's check to see if it's a
228:26 - leaf node then that's really easy within
228:28 - an if statement we will check to see if
228:31 - the left child
228:33 - is equal to null
228:35 - and
228:36 - the right child
228:37 - is also equal to null
228:40 - that means that the node that we're
228:41 - trying to remove is a leaf node and
228:44 - that's really simple we don't need to
228:45 - shift any nodes around we can just set
228:47 - this current root node equal to null
228:50 - however if the node we're attempting to
228:53 - delete has a right child will have to
228:55 - shift those nodes around and find a
228:57 - successor
228:59 - else if
229:01 - root dot right does not equal no
229:05 - that means there's a right child
229:07 - and we need to find a successor
229:10 - to replace
229:12 - this node
229:14 - so we will assign
229:16 - root
229:16 - dot data
229:18 - is equal to the successor method which
229:21 - will find a successor for us and pass in
229:24 - the current root node then take root dot
229:28 - right
229:29 - equals and invoke the remove helper
229:32 - method
229:33 - pass in the right child root.right
229:37 - as well as root dot data
229:40 - so when we delete a node that will
229:41 - create a gap and if there's a child well
229:44 - we don't want that child to be lost we
229:46 - don't want that child to become an
229:47 - orphan so we will link a child to that
229:50 - spot where we deleted a node now if
229:52 - there's a left child we have a slightly
229:55 - similar procedure so we can use an else
229:57 - statement
229:58 - take root.data
230:01 - and invoke the predecessor method
230:03 - take this line of code paste it root dot
230:07 - left equals remove helper method pass in
230:11 - the left child of the root node
230:14 - this else statement will find a
230:16 - predecessor
230:19 - to replace this node
230:22 - and at the end we will return root
230:26 - so we can close out of these two methods
230:28 - and we can open up the successor and
230:31 - predecessor methods
230:33 - what we're doing with the successor
230:34 - method is that we're attempting to find
230:37 - the least
230:38 - value
230:40 - below the right
230:42 - child
230:43 - of this root node
230:46 - we will assign the current root node
230:48 - equal to root dot right
230:52 - then within a while loop
230:54 - we will take root
230:56 - dot left and check to see if it is not
230:59 - equal to null
231:01 - while this condition is true
231:03 - take root
231:05 - set the sql to root dot left
231:08 - and at the end return
231:10 - root dot data
231:12 - so what's happening here within the
231:14 - remove helper method
231:16 - if the node we're trying to delete has a
231:18 - right child we need to find a successor
231:21 - to fill in that gap and that successor
231:24 - should have the least value
231:26 - so we will go right and look as far left
231:29 - as we can because values on the left are
231:31 - less than the root numbers on the right
231:33 - are greater than our root after going
231:35 - right we will go as far left as we can
231:38 - to get the lowest value then return it
231:41 - so that's what we're doing when we're
231:42 - assigning a successor to fill in that
231:44 - gap when we delete a node that has
231:46 - children and then the predecessor method
231:48 - is going to be very similar we will find
231:51 - the greatest value below the left child
231:55 - of this root node so we can copy all of
231:58 - this
231:59 - paste it
232:00 - this time we will go left
232:02 - then go as far right as we can go
232:05 - i do apologize this is a lot to take in
232:07 - recursion can be very confusing but yeah
232:10 - that's everything let's attempt to
232:11 - remove a node
232:13 - so before we display our tree let's take
232:15 - tree dot remove
232:18 - then pass in the data of a node you
232:19 - would like to remove let's remove one
232:22 - and here are the remaining nodes we have
232:24 - two through nine all in order
232:27 - let's remove five okay one two three
232:30 - four six seven eight nine let's remove
232:32 - nine
232:34 - nine is no longer there and let's remove
232:36 - a node that is not within here like zero
232:40 - zero cannot be found
232:42 - alright then everybody that is a binary
232:44 - search tree just so you're aware the
232:46 - order in which you insert nodes into a
232:48 - binary search tree does matter if it's
232:51 - unbalanced technically my tree follows
232:54 - all the rules however it's fairly
232:56 - lopsided there is a way to balance
232:58 - binary trees but that's a topic for avl
233:01 - trees that's why the runtime complexity
233:03 - has a best case scenario of big-o of log
233:06 - n but a worst case scenario of big o of
233:09 - n depending on how balanced it is but
233:11 - basically a binary search tree is a tree
233:14 - data structure where each node is
233:16 - greater than its left child but less
233:19 - than its right the benefit of a binary
233:21 - search tree is that it's easy to locate
233:23 - a node when they are in order so yeah
233:26 - those are binary search trees if you
233:28 - would like a copy of this code i'll post
233:30 - this to the comment section down below
233:31 - don't be afraid to smash that like
233:33 - button leave random comments down below
233:35 - and subscribe if you'd like to become a
233:37 - fellow bro
233:39 - hey y'all everybody so let's talk about
233:41 - tree traversal tree traversal is the
233:43 - process of visiting all the nodes of a
233:45 - tree we don't have random access so we
233:47 - begin at the root node and follow a
233:49 - certain procedure to visit all of the
233:51 - nodes of a tree and there's three that
233:53 - we're going to talk about in order post
233:55 - order and pre-order traversal in this
233:58 - example we will be working with a binary
234:00 - tree not a binary search tree because
234:03 - these values are not in binary search
234:06 - tree order so each node has at most two
234:09 - children a left node and a right node
234:12 - let's begin with an in-order traversal
234:14 - here's a recursive method that will
234:16 - navigate a tree in order using recursion
234:20 - we visit as many left nodes as we can
234:23 - followed by the root node and then any
234:25 - right nodes the entry point is always
234:28 - the root and using recursion we will go
234:30 - as far left as we can so we go from a to
234:33 - b to d
234:35 - so we will mark this node as visited
234:38 - then visit the root node next
234:40 - and then the right node since we can't
234:42 - go left anymore we're going to visit the
234:44 - root node a
234:46 - all that's left is to go right so we
234:48 - will go to c
234:50 - however we can go left so we will visit
234:52 - f first
234:54 - then c
234:55 - then g
234:56 - so this is in order traversal when used
234:59 - in the context of a binary search tree
235:01 - we will visit all of these nodes in
235:03 - non-decreasing order so again that was
235:07 - d
235:08 - b
235:09 - e
235:11 - a
235:12 - f c g
235:15 - we go left root right and here's that
235:18 - method again
235:19 - and here's post order traversal it's
235:22 - used to delete a tree from leaf to root
235:25 - and we follow this pattern left right
235:28 - root
235:29 - we will use recursion to visit the left
235:31 - children then the right children then
235:33 - the root so a is our entry point we are
235:36 - going as far left as we can go from a to
235:40 - b to d
235:41 - mark d is visited then we go right using
235:44 - recursion go from d
235:46 - to e
235:47 - then root
235:48 - left right root so we go back up go as
235:53 - far right as we can unless we encounter
235:55 - a left child which we did so we will
235:57 - mark f as visited then g
236:00 - right
236:01 - root
236:02 - left right root and all that's left is
236:06 - the root node
236:07 - so that is post order traversal it's
236:10 - used to delete a tree from leaf to root
236:13 - and lastly we have pre-order traversal
236:16 - pre-order traversal is used to create a
236:19 - copy of a tree we visit any root nodes
236:22 - first
236:22 - then left then right
236:24 - so we will mark a as visited our entry
236:27 - point then visit the left child and we
236:30 - will continue to go left
236:32 - then go right
236:33 - so we can't go left anymore we're going
236:35 - to go right
236:36 - mark the root as visited
236:38 - then left then right this is used to
236:41 - create a copy of a tree but in order to
236:44 - create a copy of a tree you need to
236:45 - create a root node and a branch node to
236:47 - hold the leaves
236:49 - so again that's
236:50 - root left we're still going left
236:54 - right
236:55 - root
236:56 - left
236:57 - right
236:58 - a b d
237:00 - e
237:01 - c f g
237:03 - that is pre-order traversal it's used to
237:05 - create a copy of a tree well okay then
237:08 - everybody that is a few ways in which we
237:10 - can traverse a tree like i said normally
237:13 - we don't have random access so the entry
237:15 - point is the root node and you can
237:17 - either follow an in order in-order
237:19 - post-order or pre-order traversal
237:21 - depending on what you need to do with
237:22 - this tree so that's tree traversal and
237:25 - computer science if you liked this video
237:27 - please be sure to give it a thumbs up
237:29 - leave a random comment down below and
237:31 - subscribe if you'd like to become a
237:32 - fellow bro
237:34 - hey everybody in this video i thought i
237:36 - would show you all how you can get the
237:38 - runtime of a program i thought it might
237:40 - be useful now that we know how data
237:42 - structures and algorithms work for the
237:43 - most part so within the entry point of
237:46 - your program you'll need the start time
237:48 - and we'll store this within a long data
237:50 - type i'll name the start equals then
237:53 - type system dot nano
237:57 - time
237:58 - a nano second is a billionth of a second
238:01 - and then we have our program
238:03 - whatever it is
238:06 - and then at the end of our program we
238:08 - will get the duration
238:10 - this will be of the long data type
238:12 - duration
238:14 - equals and then we need the current time
238:17 - system dot nano time
238:20 - minus our start time
238:23 - this returns a time in nanoseconds if
238:25 - you need milliseconds you can divide
238:27 - this number by 1 million
238:30 - okay then let's print the duration
238:32 - system.out.print line
238:34 - duration
238:35 - plus
238:37 - milliseconds or some other unit to
238:39 - measure time okay so with our program
238:41 - let's just have the main thread sleep
238:43 - for maybe three seconds
238:46 - whatever it is you want to do
238:48 - and we will sleep for three thousand
238:50 - milliseconds
238:52 - and here we go so we will wait for three
238:55 - seconds
238:56 - and we are done this program took 3014
238:59 - milliseconds to complete
239:01 - when you enter your program get the
239:03 - start time at the end of your program
239:05 - get the current time minus the start
239:07 - time then depending on the units you're
239:09 - using you may need to divide this number
239:11 - by a million or if you need this in
239:12 - seconds you would divide this by 1
239:14 - billion so yeah that's how to calculate
239:16 - the run time of a program if you found
239:18 - this video helpful please be sure to
239:20 - smash that like button leave a random
239:22 - comment down below and subscribe if
239:23 - you'd like to become a fellow bro
239:27 - hey you yeah i'm talking to you if you
239:30 - learned something new then help me help
239:32 - you in three easy steps by smashing that
239:35 - like button drop a comment down below
239:37 - and subscribe if you'd like to become a
239:39 - fellow bro
239:43 - [Music]
240:14 - you