With timestamps:

00:00 - Everyone and welcome to this Python and open
CV course. In this course, we'll be talking
00:05 - about everything you need to know. To get
started with open CV in Python, we're going
00:09 - to start off with the very basics that is
reading images and video, manipulating those
00:14 - media files with image transformations, and
how to draw shapes and put text on those files.
00:20 - Then we're going to move on to the most advanced
parts of open CV that is switching between
00:25 - color spaces bitwise operators, masking, histograms,
edge detection and thresholding. And finally,
00:32 - to sum things up, we'll be talking about face
detection and face recognition in open CV,
00:38 - so how to detect and find faces in an image
and how to recognize them using inbuilt methods.
00:45 - In the last video, we'll be building a deep
computer vision model to classify between
00:50 - the characters in The Simpsons based off some
images. All material discussed will be available
00:56 - on my GitHub page, and all relevant links
will be put up in the description below. If
01:01 - that sounds exciting, don't forget to head
over and subscribe to my channel. And I'll
01:05 - see you in the course. Hey, everybody, and
welcome to this Python and urban TV coast.
01:13 - Over the next couple of videos, we're going
to be talking about using the open CV library
01:17 - to perform all sorts of image and video related
processing and manipulations. Now I won't
01:23 - be delving into what open CV is really is.
But just be brief. It is a computer vision
01:28 - library that is available in Python, c++ and
Java. A computer vision is an application
01:35 - of deep learning that primarily focuses on
deriving insights from media files, that is
01:41 - images and video. Now, I'm going to assume
that you already have Python installed on
01:46 - your system. And a good way to check this
is by going to terminal and typing Python
01:51 - dash dash version. Now make sure you're running
a version of Python of at least 3.7 above
01:57 - whatever we do in this post wonderly work
in some older versions of Python, and especially
02:02 - Python two, so just make sure that you have
the latest version installed, go ahead to
02:06 - python.org and download the latest version
from bet. Now assuming that you've done this,
02:11 - we can proceed to installing the packages
that we require in this course. The first
02:16 - one is open C. So go ahead and do a pip install
Open CV dash contrib dash Python. Now sometimes
02:26 - you may find people telling you to install
just open CV dash Python. Well, this open
02:31 - team dash Python is basically the main package
the main module of open CV, open CV dash contract
02:38 - dash Python includes everything in the main
module, as well as a contribution modules
02:43 - provided by the community. So this is something
I recommend you install as it includes all
02:48 - of open CV functionality. You may also notice
that urgency, we tried to install the NumPy
02:53 - package. Now NumPy is kind of a scientific
computing package in Python, that's extensively
02:59 - used in matrix an array manipulations, transformations,
reshaping and things like that. Now, we'll
03:05 - be using NumPy in some of the videos in this
course. But don't worry if you've never used
03:09 - them before. It's simple and relatively easy
to get started with. Now the next package,
03:14 - I'd like you to install a sphere. So go ahead
and do pip install seer. Now, slight disclaimer,
03:22 - this is a package that I built to basically
help you to speed up your workflow. Sierra
03:27 - is basically a set of utility functions that
will prove super useful to you in your computer
03:31 - vision journey. It has a ton of super useful
helper functions that will help speed up your
03:36 - workflow. Now, although we're not going to
be using this for a good part of this course,
03:40 - in fact, we'll only begin to use this in our
last video of this course when we're building
03:44 - a deep computer vision model. I recommend
you install it now so that you don't have
03:49 - to worry about the installation process later
on. If you're interested in contributing to
03:54 - this package, or just simply want to explore
the codebase I'll leave a link to this GitHub
03:58 - page in the description below. Okay, that's
it for this video. In the next video, we'll
04:04 - be talking about how to read images and video
in open CV. So I'll see you guys in the next
04:11 - video. Hey everybody, and welcome back to
another video. In this video, we're going
04:18 - to be talking about how to read images and
video in open CV. So I have a bunch of images
04:25 - in this photos folder, and a couple of videos
in this videos folder. In the first half of
04:32 - this video, we'll be talking about how to
read in images in open CV, and towards the
04:37 - end we'll be actually talking about how to
read in videos. So let's start off by creating
04:42 - a new file and call this reader dot p y. And
the first thing we have to do is actually
04:48 - input CV two as CV. So the way we read in
images in open CV is by making use of the
04:56 - cv.im read method. Now this method basically
takes in a path to An image and returns that
05:01 - image as a matrix of pixels. Specifically,
we're going to be trying to read this image
05:06 - of a cat here. So we're going to say photos
slash cat dot jpg. And we're going to capture
05:13 - this image in a variable called IMG. Now you
can also provide absolute paths. But since
05:20 - this photos folder is inside my current working
directory, I'm going to reference those images
05:25 - relatively. Now once we've read in our image,
we can actually display this image by using
05:31 - the cv.rm show method. Now this method basically
displays the image as a new window. So the
05:38 - two parameters we need to pass into this method
is actually the name of the window, in this
05:44 - case is going to be kept and the actual matrix
of pixels to display, which in this case is
05:49 - IMG. And before we actually move ahead, I
do want to add an additional line a CV dot
05:56 - wait key zero. Now the CV or wiki zero is
basically a keyboard binding function, it
06:03 - waits for a specific delay, or time in milliseconds
for a key to be pressed. So if you pass in
06:11 - zero, it basically waits for an infinite amount
of time for a keyboard key to be pressed.
06:17 - I didn't worry too much about this, it's not
really that important for this course. But
06:22 - we will be discussing some parts of it towards
the end of this video. So let's actually save
06:28 - this and run by saying Python, read dot p
y, and the image is displayed in a new window.
06:36 - Cool. Now this was a small image, this was
an image of size 640 by 427. Now we're going
06:46 - to try and read in this image of the same
cat, but a much larger version, a 2400 by
06:52 - 1600 image. So we're gonna say Cat on a school
large dot jpg. Let's save that and run. And
07:01 - as you can see, this image goes way off screen.
The reason for this is because the dimensions
07:07 - of this image were far greater than the dimensions
of the monitor that I'm currently working
07:12 - on. Now currently, open CV does not have an
inbuilt way of dealing with images that are
07:18 - far greater than your computer screen. There
are ways to mitigate this issue. And we'll
07:24 - be discussing them in the next video when
we talk about resizing and rescaling frames
07:29 - and images. But for now, just know that if
you have images, if you have large images,
07:35 - it's possibly going to go off screen. So that's
it for reading images, we can then move on
07:43 - to reading videos in open CV. So that's called
reading videos. So what we're going to do
07:51 - is we're actually going to read in this video
of a dog, and the way we read in videos is
08:00 - by actually creating a capture variable and
setting this equal to CV dot video capture.
08:07 - Now this method either takes an integer arguments
like 0123, etc, or a path to a video file.
08:17 - Now you would provide an integer argument
like 012, and three, if you are using your
08:23 - webcam or a camera that is connected to your
computer. In most cases, your webcam would
08:29 - be referenced by using the integer zero. But
if you have multiple cameras connected to
08:34 - your computer, you could reference them by
using the appropriate argument. For example,
08:39 - zero would reference your webcam, one would
reference the first camera that is connected
08:44 - to your computer to would reference the second
camera and so on. But in this video, we'll
08:49 - be actually looking at how to read an already
existing videos from a file path. Now specifically,
08:55 - we'll be reading this dog, this video for
dog here. And the way we do that is by providing
09:01 - the path so videos, slash dog dot mp4. Now,
here's where reading videos is kind of like
09:10 - different from reading images. In the case
of reading and videos, we actually use a one
09:16 - loop and read the video frame by frame. So
we're going to say while true. And the first
09:23 - thing we want to do inside this loop is say
is true. And frame is equal to capture dot
09:30 - read. Now this capture dot read basically
reads in this video frame by frame, it returns
09:38 - the frame and a Boolean that says whether
the frame was successfully read in or not.
09:43 - Do you display this video we can actually
display an individual frame. So we do this
09:49 - by saying TV on show and we call this video
and we pass in the frame and finally for some
09:58 - way to stop the Do from playing indefinitely
is by saying if CV don't wait, Ki 20 and 0x
10:08 - ff is equal to equal to Ord of D. There we
want to break out of this while loop. And
10:16 - once that's done, we can actually release
the capture pointer. And we can destroy all
10:24 - windows. And we can get rid of this. So basically
just to recap, the capture variable is an
10:35 - instance of this video capture clause. Inside
of while loop, we grab the video frame by
10:41 - frame. By utilizing the captured read method,
we display each frame of the video by using
10:49 - the CV dot m show method. And finally, for
some way to break out of this while loop,
10:54 - we say if See, we don't wait ki 20 if and
0x f f is equal to or D, which basically says
11:02 - that if the letter D is pressed, then break
out of this loop and stop displaying the video.
11:08 - And finally, we release the capture device
and we destroy all the windows since we don't
11:12 - need them anymore. So let's save that and
run. And we get a video displayed in a window
11:20 - like this. But once it's done, you will notice
that the video suddenly stops and you get
11:31 - this error. More specifically a negative 215
assertion failed error. Now if you ever get
11:37 - an error like this negative 215 assertion
failed. This would mean in almost all cases
11:42 - is that open CV could not find a media file
at that particular location that you specified.
11:49 - Now, the reason why it happened in the video
is because the video ran out of frames, open
11:55 - CV could not find any more frames after the
last frame in this video. So it unexpectedly
12:01 - broke out of the while loop by itself by raising
a CV to error. And now you're gonna get the
12:07 - same error. If we comment this out, we uncomment
this out. And we specify a wrong path to this
12:17 - image. So I see me Oh wait, wait key, zero,
save that and run and we get the exact same
12:26 - error. This basically again says that open
CV could not find the image or the video frame
12:32 - at a particular location basically, it could
not be ready. That's what it's saying. So
12:39 - that's pretty much it. For this video, we
talked about how to read any images in open
12:44 - CV and how to read in videos using the video
capture class. In the next video, we'll be
12:50 - talking about how to rescale and resize images
and video frames in open CV. So see you then.
12:58 - Hey, everyone, and welcome back. In this video,
we're going to be talking about how to resize
13:06 - and rescale images and video frames in open
CV. Now, we usually resize and rescale video
13:13 - files and images to prevent computational
strain. Large media files tend to store a
13:19 - lot of information in it and displaying it
takes up a lot of processing needs that your
13:24 - computer needs to assign. So by resizing and
rescaling, we're actually trying to get rid
13:29 - of some of that information. rescaling video
implies modifying its height and width to
13:36 - a particular height and width. Generally,
it's always best practice to downscale or
13:42 - change the width and height of your video
files to a smaller value than the original
13:49 - dimensions. The reason for this is because
while most cameras your webcam included, do
13:54 - not support going higher than its maximum
capability. So for example, if a camera shoots
14:00 - in 720 P, chances are it's not going to be
able to shoot in 1080 P or higher. So to rescale
14:06 - a video frame or an image, we can create a
function called def rescale frame. And we
14:14 - can pass in the frame to be resized and scale
the value which by default we're going to
14:21 - set as point seven five. So what I'm going
to do next is I'm going to say with is equal
14:27 - to frame dot shape of one of one times scale.
And I'm going to copy this and do the same
14:39 - thing for the height. Now remember frame no
shape of one is basically the width of your
14:46 - frame or your image and frame note shape of
zero is basically the height of the image.
14:53 - Now since width and height are integers, I
can actually convert these floating point
14:58 - values to an integer by converting it to an
iron T. And what we're going to be doing is
15:04 - we're going to create a variable called dimensions,
and set this equal to a table of width, comma
15:10 - height. And we can actually return CV don't
resize the frame, the dimensions, and we can
15:20 - pass in it interpolations of CV dot into on
the school area. Now we'll be talking about
15:28 - CV dot resize in an upcoming video. But for
now, just note that it resizes the frame to
15:33 - a particular dimension. So that's all a function
does, it takes in the frame, and it scales
15:40 - that frame by a particular scalar value, which
by default is point seven, five. So let's
15:45 - actually try to see this in action. Let's
go back to this readout p y, and grab this
15:51 - code. And we can paste there, we don't need
us for now. uncomment these out. Now what
16:05 - I'm going to do is after I've read in the
frame, I'm going to create a new frame call
16:09 - frame on this go resized, and set this equal
to rescale frame of frame. And let's leave
16:20 - the scale value is point seven, five. And
we can actually display this video resized
16:29 - by passing a frame on the scope resized. Resize.
So let's save that and run Python rescale
16:42 - del p why that was an error. Okay, we don't
need this. Let's close that out, Save and
16:51 - Run. And this was our original video. And
this is actually a resize the video with the
17:00 - video resize by point seven 570 5%. We can
modify this by changing the scale of value
17:08 - to to maybe point two, so we rescaling to
20%. And we get an even smaller video in a
17:20 - new window. So let's close that out. Now you
can also apply this on images. So let's uncomment
17:33 - that out, change that to cat dot jpg. And
we can do receive our show. Image and pawson
17:45 - the resized image. And we can create a resize
image by calling rescale frame and we could
17:59 - pass in the IMG. So let's see that in Rome.
And this is a small videos we're not concerned
18:07 - with that. This is actually the big image
the large image. And this is the recent version
18:14 - of this image. So let's close that out. Now
there is another way of rescaling or resizing
18:19 - video frames specifically. And that's actually
using the capture dot set method. Now this
18:25 - is specifically for videos, and will work
for images. So let's go ahead and try to do
18:32 - that. Let's call this depth change rez. So
we're changing we're changing the resolution
18:37 - of the image of video. And we can pass in
a width and a height. And what we're going
18:45 - to do is we're going to say capture, don't
set three comma with and we're going to do
18:53 - the same thing with capture dot set four comma
height. Now three info basically stands for
19:00 - the properties of this capture class. So three
references the width and full references the
19:06 - height. You can also expand this to maybe
change the brightness in the image. And I
19:12 - think you can reference that by setting this
to 10. But for now we're going to be interested
19:17 - in the width and the height. Now, I do want
to point out this, this method will work for
19:24 - images, videos, and live video. Basically,
for everything you can use this rescale frame
19:32 - method. But the changes function only works
for live video. That is video you read in
19:38 - from an external camera or your webcam for
instance. So video that is going on currently,
19:44 - this is not going to work on standalone video
files, video files that already exist. It
19:51 - just doesn't work. So if you're trying to
change the resolution of live video, then
19:56 - go with this function if you're trying to
change the resolution of an old already existing
20:00 - video, then go with this function. So that's
pretty much it for this video that we talked
20:07 - about how to resize and rescale video frames
and images in open CV. In the next video,
20:12 - we'll be talking about how to draw shapes,
and write text on an image. So that's everything.
20:18 - I'll see you guys in the next video. Hey,
everyone, and welcome back to another video.
20:26 - In this video, we're going to be talking about
how to draw and write on images. So go ahead
20:31 - and create a new file and call this draw dot
p y. We're going to input CV two and CV, we're
20:39 - going to input the NumPy package that open
CV had installed previously. And we're going
20:44 - to input that as MP, we will read in an image
by saying OMG is equal to cv.rm, read person
20:53 - photos, photos slash cat dot jpg, we can display
that image in a new window. And we can do
21:01 - receive out of weight key zero. Now there
are two ways we can draw on images by actually
21:08 - drawing on standalone images like this image
of a cat to or we can create a dummy image
21:13 - or a blank image to work with. And the way
in which we can create a blank image is by
21:18 - saying blank is equal to NP dot zeros of shape
500 by 500. And give it a data type of ui
21:27 - 98. You ID eight is basically an image the
datatype of an image. So if you want to try
21:35 - and see this image, see what this image looks
like. We can say blank, and we can pass in
21:42 - like save that and run Python drawed or p
y. And this is basically the blank image that
21:51 - you can draw on. So we're going to be using
that in instead of drawing this cat image.
21:59 - But feel free to use this cat image if you'd
like. So the first thing we're going to do
22:05 - is try to paint is trying to paint the image
a certain color. And the way we do this is
22:13 - by saying blank and reference all the pixels
and set this equal to zero comma 255 comma
22:20 - zero. So by painting the entire image green,
and we can display this image by saying green
22:26 - in passing the blank image, save that and
run. Can I broadcast Yeah, okay, you need
22:35 - to give it a shape of three, basically, we
are giving the shape of height, width, and
22:41 - the number of color channels. So just keep
that in mind save up. And this is the green
22:46 - image that we get cool, we can even change
this and try to change this to red zero comma
22:54 - 255. Save that. And we get a red image over
here. Now you can also call a certain portion
23:03 - of the image by basically giving it a range
of pixels. So we can say 200 to 300. And then
23:11 - from 300 to 400. Save that and run and you
got a Red Square in this image. The next thing
23:22 - we're going to do is we're going to draw a
rectangle. And the way we do this is by using
23:28 - the CV don't rectangle method. This method
takes in an image to draw the rectangle over,
23:35 - which in this case is blank. And it takes
in point 1.2, color, thickness and a line
23:42 - type if you'd like. So the point one will
specifically be zero comma zero, which is
23:48 - the origin. And we can go all the way across
to 250 comma 250. Let's give it a color of
23:55 - zero comma 255 comma zero, which is green,
give it a thickness of let's say two, which
24:02 - is basically saying the thickness of the borders.
And once that's done, we can display this
24:09 - image by saying let's call this rectangle
in passing and passing the blank image. We
24:19 - can comment this out since we don't need this
anymore. And we get a green rectangle that
24:27 - goes all the way from the origin to 250 comma
250. You can play around with it if you like
24:35 - so we can go from 250 to maybe 500. And it
goes all the way across the image. So you
24:44 - basically divide the image in half. Now there
is a way of filling in this image a certain
24:51 - color. And the way we do this is instead of
saying thickness is equal to two, we say thickness
24:57 - is equal to CV dot field. That basically fills
in the rectangle to get this green rectangle.
25:05 - Now Alternatively, you can also specify this
as negative one, negative one. And we get
25:15 - the same result, what we can also do is, instead
of giving it fixed values like 250, and 500,
25:21 - what we could do is we could say, IMG done
shape of zero, of one divided by divided by
25:30 - two, and image dome shape of zero, divided
by divided by two. Let's save that and run.
25:37 - image is not in fact, God, this is blank,
this is blank, save that and run. And we get
25:47 - a nice little rectangle, or square, if you
will, in this image, what it basically did
25:54 - is it scaled the rectangle from instead of
being these, this entire square, this rectangle
26:01 - basically has dimensions, half of that of
the original image. So moving on, let's try
26:10 - and draw a circle. Draw circle. This is also
fairly straightforward, we do a CV dot circle.
26:22 - And we pass in the blank image. And we give
it a center, which basically the coordinates
26:27 - of the center for now let's set this to the
midpoint of this image by saying 250 comma
26:34 - 250. Alternatively, you could also get this
let's give it a radius of 40 pixels, give
26:46 - it a color of zero comma, zero comma 255,
which is red BGR. And give it a thickness
26:54 - of let's say three. We can display this image,
say, circle is equal to blank. And we get
27:10 - a nice little circle over here, that has its
center at 250 km 250, and radius of 40 pixels.
27:19 - Again, you can also fill in this image by
giving a thickness of negative one. Here,
27:26 - we get a nice little dot here in the middle.
Cool. Now there's something else that I forgot.
27:33 - And that is how to draw a line a standalone
line on the image. That again, is fairly straightforward,
27:41 - say draw a line, we use a cv.in line method.
And this takes in the image to draw the line
27:48 - on and two points, that's just copy these
points, basically everything. And this basically
27:58 - draws a point from zero comma zero to half
these image dimensions. So that's 252 50.
28:04 - And then it draws a line of color zero comma
255, comma zero. Let's set this to full white
28:13 - 2255, d 5255. And it's green thickness you
can specify as three. And we didn't display
28:23 - this image. See you don't on show colas line,
rule the line, blank image, and we get a line
28:38 - that goes all across from zero comma, zero
comma zero to 250, comma 250. Let's try and
28:46 - play around with this. And let's draw a line
from 100 to maybe 250. And then it goes all
28:56 - the way to 300 to 400, save that. And you've
got a line that goes from 100 100 to 300,
29:13 - comma 400. Cool. And finally, the last thing
that we will discuss in this video is how
29:19 - to write text on an image that that's right
text on an image. Now, the way we do this
29:30 - is very straightforward. We see we do a CV
dot put text. And this will put text on the
29:38 - blank image. We specify what we want to put
on. So let's say hello. We can give it an
29:46 - origin, which is basically where do we want
to draw the image from? Let's set this to
29:50 - 225 and 225. And we can also specify font
face. Now open CV comes with inbuilt fonts.
30:00 - And we will be using the CV dot font unschool
Hershey ns go. We'll be using the triple x,
30:08 - you have complex you have duplex you have
plain. You have script simplex and a lot of
30:15 - inbuilt phones. But for now, let's use a triplex.
Let's give this a font scale, which is basically
30:21 - how much do you want to scale the font by,
let's set this to 1.0. We don't want to scale
30:26 - a font, let's give it a color of zero comma
255, comma zero, and give it a thickness of
30:33 - two. Commit that out. And we can display this
image. So you don't I'm show let's call this
30:46 - text and pass in the blank image. And we get
some text that is placed on the image. You
30:57 - play around with it and say, Hello, my name
is Jason. Save and Run. And it goes off screen.
31:08 - I when we're dealing with large images, but
we can there's no way of actually handling
31:14 - this except for maybe changing the margins
here a bit, too, we can do that by saying
31:20 - let's say it's zero comma two to five. And
it sounds from zero and says Hello, my name
31:31 - is yes. So that's it. For this video, we talked
about how to draw shapes, how to draw a lines,
31:39 - rectangles, circles and how to write text
on an image. Now in the next video, we'll
31:44 - be talking about basic functions in open CV,
that you're most likely going to come across
31:48 - whatever project in computer vision you end
up doing. So if that's it, I'll see you guys
31:53 - in the next video. Hey, everyone, and welcome
back to another video. In this video, we're
32:01 - going to be talking about the most basic functions
in open CV that you're going to come across
32:06 - in whatever computer vision project you end
up building. So let's start off with the first
32:11 - function. And that is converting an image
to grayscale. So we've written an image, and
32:20 - we've displayed that image in a new window.
And currently, this is a BGR image, a three
32:26 - channel blue, green and red image. Now there
are ways in open CV to essentially convert
32:33 - those BGR images to grayscale so that you
only see the intensity distribution of pixels
32:39 - rather than the color itself. So the way we
do that is by saying gray is equal to CV dot
32:46 - CBT color, we pass in the image that we want
to convert from, which is IMG, and we specify
32:52 - a color code. Now this kind of code is CV
dealt kind of unskilled BGR. To great, since
33:01 - we're converting a BGR image to a grayscale
image. And we can go ahead and display this
33:07 - image by saying CV don't show passing gray
and pass in the gray image. Save that and
33:15 - run your Python basic.pi. And this was the
original image. And this is the grayscale
33:29 - image. Let's try this with another image.
Slide with no this is the image of a park
33:37 - in Boston save and maybe change that to Boston.
And this is the BGR image in open CV, and
33:47 - this is its corresponding grayscale image.
So nothing too fancy. We've just converted
33:53 - from a BGR image to a grayscale image. The
next function we're going to discuss is how
34:00 - to blur an image. Now blurring an image essentially
removes some of the noise that exists in an
34:07 - image. For example, in an image, there may
be some extra elements that were there because
34:12 - of bad lighting when the image was taken,
or maybe some issues with the camera sensor
34:18 - and so on. And some of the ways we can actually
reduce this noise is by applying a slight
34:25 - blur. There are way too many blurring techniques
which we will get into in the advanced part
34:30 - of this goes. But for now we're just going
to use the Gaussian Blur. So what we're going
34:34 - to do is we're going to create a blurred image.
I think blur is equal to CV dot Gaussian Blur.
34:43 - And this image will take an associate image
which is the IMG it will take in a kernel
34:47 - size, which is actually a two by two tuple
which is basically the window size that open
34:53 - CV uses to compute the blown the image. We'll
get into this in the advanced part of the
34:59 - scope so don't worry too much about this,
just know that this kernel size has to be
35:03 - an odd number. So So let's start a real simple
and keep the kernel size to three by three.
35:09 - And another thing that we have to specify
is CV dot border on school default. So go
35:16 - ahead and try to display this image, the same
blur, and pawson blue. Now, you will be able
35:24 - to notice some of the differences in this
image. And that is because of the blur that
35:30 - is applied on it. Right this people in the
background are pretty clear on this image.
35:35 - And over here, they're slightly blurred. To
increase a blind his image, we can essentially
35:42 - increase the kernel size from three by three
to seven by seven, save that and run. And
35:49 - this is the image that is way more blurred
than the previous image. So that's it. The
35:56 - next function we're going to discuss is how
to create an edge cascade, which is basically
36:01 - trying to find the edges that are present
in the image. Now again, there are many edge
36:06 - cascades that are available. But for this
video, we're going to be using the canny edge
36:11 - detector, which is pretty famous in the computer
vision world. Essentially, it's a multi step
36:16 - process that involves a lot of blurring and
then involves a lot of grading computations
36:22 - and stuff like that. So we're gonna say, Kenny,
Kenny is equal to CV dot Kenny, we pass in
36:30 - the image, we pass in to threshold values,
which for now I'm going to say 125 and 175.
36:38 - Let's go ahead and try to display this image,
get the Kenny images. And we can pass in county.
36:46 - Save that and run. And these were the edges
that were found in this image. As you can
36:54 - see that it hardly any edges found in the
sky. But a lot of features in the trees and
36:59 - the buildings. And quite a few, you know features
and edges in the grass and stuff. We can reduce
37:07 - some of these edges by essentially blurring
the image. And the way we do that is instead
37:12 - of passing the IMG, we pass in the blur. See
that run. And as you can see that there were
37:19 - far less edges that were found in the image.
And this is a way you can basically reduce
37:23 - the amount of edges that were found by a lot
by applying a lot of blur, or get rid of some
37:29 - of the edges by applying a slight blur. Now
the next function we're going to discuss is
37:35 - how to dilate an image using a specific structuring
element. Now the structuring element that
37:42 - we are going to use is actually these edges,
the canny edges that were found, so we're
37:49 - gonna say dominating the image. And the way
we do that is by saying dilated is equal to
37:55 - CV dot dilate. And this will take in the structuring
element, which is basically the canny edges.
38:02 - And we'll take a kernel size, which we'll
specify as three by three for now. And it
38:08 - will also take n iterations of one. Now, dilation
can be applied using several iterations of
38:14 - the time, but for now, we're just going to
stick with one. So go ahead and try to display
38:19 - this image by saying CV dot m shope. Call
this dilated. And we can pass in David. Save
38:27 - that and run. And if these were, if these
were edges, these are the dilated edges, we
38:37 - can maybe increase the kernel size to maybe
seven by seven and tried to see what that
38:44 - does hold on. And nothing much was done. Not
much difference was that let's try to increase
38:52 - the number of iterations to maybe three. And
it's definitely way thicker. But you're gonna
39:01 - see subtle differences with the amount of
features and edges that you find. Now there
39:06 - is a way of eroding this dilated image to
get back this structuring element. Now, it's
39:12 - not going to be perfect, but it will work
in some cases. So we're gonna say, call this
39:20 - roading and we call this eroded is equal to
CV don't erode, it will take in the dilated
39:28 - image, pass and dilated, it will take a kernel
size of let's start off with three by three
39:36 - and given n iterations of one just for now.
And we didn't display this image show coolest
39:48 - clothes eroded, eroded and if this was your
structuring element, and this was your dilate
40:00 - image, this is basically the result you get
from eroding this image. Now, it isn't the
40:06 - same as a structural element. But you can
just about to make the features that. But
40:11 - you can see that between this and this, there
is a subtle change in the edges and the thickness
40:16 - of these edges, we can maybe try to match
these values, so that we attempt so that there
40:24 - is an attempt to get back this edge cascade.
And yes, we got the edges back there, as you
40:35 - can see that you compare these two, they look
pretty much the same. And the edges are the
40:43 - same. So essentially, if you follow the same
steps, you can, in most cases, get back the
40:48 - same edge cascade. And probably the last function
that we're going to discuss is how to resize
40:56 - and crop an image. So we're going to start
with resize. So we come to resizing video
41:02 - frames and images in the previous video in
one of the previous videos. But we're just
41:08 - going to touch on the CBO resize function
just a bit. So we're going to say resized,
41:13 - resized equal to CV dot resize, this will
take an image to be resized, and it will take
41:22 - in a destination size, which let's set this
to 500 by 500. And so this essentially takes
41:30 - in this image of the park, and resize that
image to 500 by 500, ignoring the aspect ratio.
41:40 - So we display this image by saying saved out
I'm sure resized and resized. Save that and
41:47 - run. And let's go back to this image. If this
is the original image, this is the image that
41:55 - was resized to 500 by 500. Now by default,
there is an interpolation that occurs in the
42:02 - background, and that is CV dot into on the
scope area. Now this interpolation method
42:08 - is useful if you are shrinking the image to
dimensions that are smaller than that of the
42:13 - original dimensions. But in some cases, if
you are trying to enlarge the image and scale
42:19 - the image to a much larger dimensions, you
will probably use the inter underscore linear
42:26 - or the inter on scope cubic. Now cubic is
the slowest among them all. But the resulting
42:33 - image that you get is of a much higher quality
than the inter on scope area or the inter
42:38 - underscore linear. So let's touch on cropping.
And that's basically by utilizing the fact
42:44 - that images are arrays. And we can employ
something called Array Slicing, we can select
42:50 - a portion of the image on the basis of your
pixel values. So we can say cropped is equal
42:58 - to the image. And we can select a region from
50 to 200. And from 200 to 400. And we can
43:09 - display this image Cole is cropped, possibly
cropped. And this is a cropped image of let's
43:21 - go back here of this original image, you try
to superimpose them, it's probably going to
43:29 - be you. Yeah, it's basically this portion.
So that's pretty much it. For this video,
43:40 - we talked about the most basic functions in
open CV, we talked about converting an image
43:44 - to grayscale by applying some blur by creating
an edge cascade by dilating the image by eroding
43:51 - that dilated image by resizing an image and
trying to crop an image using Array Slicing.
43:59 - In the next video, we're going to be talking
about image transformations in open CV, that's
44:03 - translation, rotation, resizing, flipping
and cropping, so if you have any questions,
44:08 - leave them in the comments below. Otherwise,
I'll see you guys in the next video. Hey,
44:15 - everyone, and welcome back to this Python
and open CV course. In this section, we're
44:21 - going to cover basic image transformations.
Now these are common techniques that you would
44:25 - likely apply to images, including translation,
rotation, resizing, clipping and cropping.
44:34 - So let's start off with translation. Translation
is basically shifting an image along the x
44:43 - and y axis. So using translation, you can
shift an image up, down, left, right, or with
44:49 - any combination of the above. So so to translate
an image, we can create a translating function,
44:55 - we're gonna call this def translate This translation
function will take in an image to translate
45:04 - and take an x and y, x and y basically stands
for the number of pixels, you want to shift
45:09 - along the x axis and the y axis respectively.
So do translate an image, we need to create
45:14 - a translation matrix. So we're going to call
this transmit is equal to NP dot float 32.
45:23 - And this will take in a list with two lists
inside of it. And the first list we're going
45:29 - to say, one comma zero comma x, and zero comma
one comma y. And since we're using NumPy,
45:38 - we can import NumPy, import NumPy as NP. And
once we've created our translation matrix,
45:47 - we can essentially get the dimensions of the
image saying dimensions, which is a tuple
45:53 - of image don't shave off one, which is the
width an image dot shape of zero, which is
46:00 - the height. And we can return CV dot warp
a fine. This will take in the image matrix
46:10 - to trans MIT animal taking the dimensions.
And with that data, we can essentially translate
46:18 - our image. And before we do that, I do want
to mention that if you have negative values
46:22 - for x, you're essentially translating the
image to the left, negative negative y values
46:30 - implies shifting up positive x values implies
shifting to the right. And as you guessed,
46:37 - positive y values shifted down. So let's create
our first translated image. We're setting
46:46 - this equal to translate, we're going to pass
in the image, the image and we're going to
46:53 - shift the image right by 100 pixels, and down
by 100 pixels. That's to receive it on on
47:00 - the show, translated and translate tip. Save
that and run Python krones formations dot
47:13 - p y. And this is your translated image, it
was shifted down by 100 pixels and shifted
47:20 - to the right by 100 pixels. So let's change
that. Let's shift the image left by 100 pixels
47:28 - and down by 100 pixels. So we pass in negative
values for x and it moved to the left. Feel
47:39 - free to play around with these values as you
see fit. Just know that negative x shifts
47:45 - to the left, negative y shoves it up, x shifted
to the right and positive y values shifted
47:51 - down. Moving on, let's talk about rotation.
rotation is exactly what it sounds like rotating
48:00 - an image by some angle. Open CV allows you
to specify any point any rotation point that
48:06 - you'd like to rotate the image around. Usually
if the center but but with open CV, you could
48:12 - specify any arbitrary point it could be any
corner, it could be 10 pixels to the right
48:17 - 40 pixels down, and you can shift the image
around that point. So to draw to rotate the
48:22 - image, we can create a rotating function,
let's call this dev rotate. This will take
48:27 - an image angle to rotate around and a rotation
point which we're going to say which we're
48:35 - going to set is not so we're going to grab
the height and width of the image by pressing
48:44 - by setting this equal to IMG dot shape of
the first two values. Basically, if the rotation
48:51 - point is none, we are going to assume that
we want to rotate around the center. So we're
48:58 - going to say rot point is equal to width divided
by two divided by two in height divided by
49:07 - divided by two. And we can essentially create
the rotation matrix like we did with the translation
49:14 - matrix. By setting this equal to rot met is
equal to CV dot get rotation matrix 2d. We're
49:23 - going to pass in the center the rotation point
and angle to rotate around which is angle
49:32 - and a scale value. Now we're not interested
in scaling the image when we've rotated so
49:37 - we can set this to 1.0. value we can set a
dimensions variable equal to the width and
49:45 - the height and we can return the rotated image
which is a CV don't warp a fine image rot
49:57 - met the destination size which is dimensions.
And that's it. That's all we need for this
50:04 - function. So we can create a rotated image
by setting this equal to rotate, and we can
50:12 - rotate the original image by 45 degrees. So
let's display this image, call this rotated,
50:22 - and pass and rotated. Save that in rock. And
this is your rotated image. As you can see,
50:31 - it was rotated counterclockwise by 45 degrees.
If somehow you wanted to rotate this image
50:36 - clockwise, just specify negative values for
this angle, and it will rotate the image around
50:47 - rotated clockwise. Now you can also rotate
a rotated image that is take this image and
50:54 - rotated by 45 degrees further. So let's call
this rotated, rotated rotated is equal to
51:05 - rotate or rotate tid. And we can rotate this
image by another 45 degrees. So we're rotating
51:14 - it clockwise. And we can see the.on show called
is rotated, rotated. And we can pause and
51:24 - rotated, rotated, whatever, rotate it. And
this is your rotate rotated image. Now the
51:32 - reason why these black lines were included
is because if there's no image in it, if there's
51:38 - no part of the image in it, it's going to
be black by default. So when you took this
51:43 - image and rotated it by 45 degrees, you essentially
rotated the image, but introduce these black
51:50 - triangles. Now if you tried to rotate this
image further by some angle, you are also
51:56 - trying to rotate these black triangles along
with it. So that's why you get these kind
52:01 - of a skewed image. So there's additional triangles
are included over here. But save yourself
52:07 - the trouble and basically add up these angles
and you will get the final angle. So we can
52:14 - change that to 90 and retake the original
image by negative 90. And this is essentially
52:22 - the image that we were trying to go for, take
this image rotated 45 degrees clockwise and
52:29 - rotate this 45 degrees image by further 45
degrees, save yourself the trouble and add
52:35 - those two angle values. So so far, we've covered
two image transformations, translation and
52:41 - rotation. Now we're going to explore how to
resize an image. Now this is nothing too different
52:46 - from what we've discussed previously. But
let's touch on adjust a bit resizing. And
52:53 - we can create a resized variable and set this
equal to CV don't resize, we can pass in the
52:59 - image to resize and the destination signs
of maybe 500 by 500. And by default the interpolation
53:06 - is CV dot inter underscore area. You can maybe
change this to into underscore linear or inter
53:14 - underscore cubic. Definitely a matter of preference
depending on whether you're enlarging or shrinking
53:21 - the image. If you're shrinking the image,
you will probably go for into underscore area
53:26 - or stick with default. If you're enlarging
the image, you could probably use the inter
53:30 - underscore linear or the dansko cubic cubic
is slower, but the resulting image is better
53:37 - with over high quality. Again, I think it's
you different from what we discussed before.
53:41 - So we can display this image. I can resize
and passing and resized. Save that run and
53:54 - we've got a resized image. Next up we have
flipping how to flip an image. So we don't
54:04 - need to define a function for this, we just
need to create a variable and set this equal
54:09 - to CV dot flip. This will take in an image
and a flipped code. Now this flip code could
54:16 - either be 01 or negative one. Zero basically
implies flipping the image of vertically that
54:24 - is over the x axis one specifies that you
want to flip the image horizontally or over
54:30 - the y axis and negative one basically implies
flipping the image both vertically as well
54:37 - as horizontally. So let's start off with zero
claiming it vertically. I'm show call this
54:44 - flip in Parson boop, Save and Run. And this
is the image that was clipped vertically.
54:53 - Let's try out a horizontal clip how we get
a horizontal Flip, surely see whether it was
55:01 - a horizontal flip, we can bring these two
images together. And if they looked like mirror
55:09 - images, then it was flipped horizontally.
This is a kind of a symmetric image. So it's
55:14 - not that obvious, but bring them together
and you can maybe find out the difference.
55:20 - We could also try to flip the image vertically
and horizontally by specifying negative one
55:28 - as a flip code. And the image was flipped
both vertically, as well as horizontally mirror
55:34 - images, but reverse mirror images. And the
last method is cropping now being discussed
55:42 - cropping again, I'm just going to touch on
it, we can create a variable called corrupt
55:47 - and set this equal to IMG and perform some
Array Slicing. So 200 to 403 100 to 400. Save
55:58 - that and run. We didn't display the search.
Even though I'm show it's cool as cropped,
56:10 - past and cropped, Save and Run. And this is
the cropped image we try to bring this together
56:16 - can be brought together, cutting gram holders.
Okay. So that's pretty much it. For this video,
56:26 - we talked about translating an image, rotating
that image, resizing an image, flipping an
56:34 - image and cropping those images, we are basically
just covering the basics, basic image transformations.
56:40 - There are of course, way mo transformation
that you could possibly do with open CV. But
56:46 - just to keep this go simple and beginner friendly,
I'm only covering the basic transformations.
56:52 - So that's it for this video. In the next video,
we're going to be talking about how to identify
56:56 - countries in an image. So if you have any
questions, leave them in the comments below.
57:02 - Otherwise, I'll see you guys in the next video.
Hey everyone, and welcome back to another
57:10 - video. In this video, we're going to be talking
about how to identify contours in open CV.
57:17 - Now contours are basically the boundaries
of objects, the line or curve that joins the
57:23 - continuous points along the boundary of an
object. Now from a mathematical point of view,
57:29 - they're not the same as edges. For the most
part, you can get away with thinking of contours
57:34 - as edges. But from a mathematical point of
view, contours and edges are two different
57:39 - things. contours are useful tools when you
get into shape analysis and object detection
57:46 - and recognition. So in this video, I sort
of want to introduce you to the idea of contours
57:52 - and how to identify them in open CV. So the
first thing I've done is I've read in a file,
58:00 - an image file, and I've displayed that image
using the cv.rm show method. Then next thing
58:06 - I want to do is convert this image to grayscale
by saying gray is equal to CV dot CVT color
58:13 - IMG CV dot color on this go BGR to great,
and we can display this. So just know that
58:24 - we're on the same footing. I'm going to run
this Python, Cantu's down p y. And we get
58:34 - a gray image over here. Now after this, I
want to essentially grab the edges of the
58:40 - image using the canny edge detector. So I'm
going to say Kenny is equal to CV Kenny, we're
58:45 - going to pass in the IMG and we're going to
give it to threshold values. So 125 and 175.
58:55 - And we can display this image calling this
Kenny edges passing Kenny. I save that and
59:02 - run it I didn't save it, save it in ROM and
these are the edges that were there in the
59:10 - image. Now, the way we find the contours of
this image is by using the find contours method.
59:19 - Now this method basically returns two things,
contours and higher keys. And essentially
59:27 - this is equal to CV dot find Cantu's. This
takes in the edges. So Kenny, it takes in
59:35 - a mod in which to find the contents now this
is either CV dot retter on a scope tree, if
59:45 - you want all the hierarchical contours, or
the rhetoric external if you want only the
59:50 - external countries, or, or retter list if
you want all the cartoons in the image. The
59:57 - next method we pass in is actually the cone
to approximation method for now we're going
60:00 - to set this to CV dot chain, unscrew approx
ns go numb. So let's, let's just have a top
60:10 - down look at what this function does. So essentially,
the CBO fund contours method looks at the
60:17 - structuring element or the edges of a found
in the image and returns to values, the contours,
60:24 - which is essentially a Python list of all
the coordinates of the contours that were
60:29 - found in the image. And hierarchies, which
is really out of the scope of this course.
60:35 - But essentially, it refers to the hierarchical
representation of contours. So for example,
60:41 - if you have a rectangle, and inside the rectangle,
if you have a square, and inside of that square,
60:46 - you have a circle. So this hierarchy is essentially
the representation that open CV uses to find
60:53 - these courtrooms. This even retinal list essentially
is a mod in which this fine contries method
61:01 - returns and finds the cuantos. Read a list
essentially returns all the quantities that
61:06 - find in the image. We also have Reto external
that we discussed radix download retrieves
61:13 - only the external conduits to all the ones
on the outside, it returns those revenue underscore
61:20 - tree returns all the hierarchical contours,
all the contours that are in a hierarchical
61:26 - system that is returned by record underscore
tree. For now, I'm just going to set this
61:31 - to will list to return all the contours in
the image. The next one we have is the contour
61:38 - approximation method. This is basically how
we want to approximate the contour. So chain
61:44 - approx none does nothing, it just returns
all of the contracts. Some people prefer to
61:51 - use red chain approx symbol, which essentially
compresses all the quantities that are returned
61:58 - in the simple ones that make most sense. So
for example, if you have a line in an image,
62:03 - if you use chain approx none, you are essentially
going to get all the contours all the coordinates
62:09 - of the points of that line, chain approx simple
essentially takes all of those points of that
62:15 - line, compresses it into the two end points
only. Because that makes the most sense, a
62:21 - line is defined by only two end points, we
don't want all the points in between. That,
62:25 - in a nutshell is what this entire function
is doing. So since cartoons is a list, we
62:33 - can essentially find the number of cartoons
that were found by finding the length of this
62:38 - list. So we can print print length of this
list. And we can say fair, we can say 
62:56 - we can say these many contused. Found. Okay,
so let's say that and Ron. And we found 2794
63:12 - quantos in the image. And this is huge. This
is a lot of code who's ever found in the image.
63:20 - So let's do a couple of things. Let's try
to change this chain approx symbol to chain
63:26 - approx none, and see what that does. See how
that affects our length. Now there isn't any
63:33 - difference between those two, because I'm
guessing that there were no points to compress
63:38 - and sin there are a lot of edges and points
in this image. So there wasn't a lot of compression.
63:44 - So let's change the back to symbol. And actually,
what we want to do is I want to blow this
63:50 - image before I find the edges. So let's do
this. Let's do a blue is equal to CV dot Gaussian
63:57 - Blur can pass in the gray image. And we can
give the kernel size of let's let's do a lot
64:04 - of blur. So five by five. And maybe we can
give it by the default of CV dot border on
64:13 - disko default. And we can if you want to,
and we can display this image, call this blur
64:22 - and pass an error we can find the edges on
this blurred image. So let's close below.
64:35 - And as you can see this significant reduction
in the number of Quorn twos that were found
64:41 - just by blurring the image. So it went all
the way from 2794 to 380. That's closest seven
64:50 - times just by blurring the image with the
kernel size of five by five. Okay, now there
64:56 - is another way of finding the corner shoes
is that it's stead of using this canny edge
65:01 - detector, we can use another function in open
CV, and that is threshold. So I'm just going
65:09 - to comment this out. And down here, what I'm
going to do is I'm going to say, ret Thresh
65:16 - is equal to CV don't threshold, this will
take in the gray image, and we've taken a
65:22 - threshold value of 125 and a maximum value
of 255. I don't worry too much about thresholding.
65:30 - For now, just know that threshold essentially
looks at an image and tries to binarize that
65:36 - image. So if a particular pixel is below 125,
if the density of that pixel is below 125,
65:42 - it's going to be set to zero or blank. If
it is above 125, it is set to white or two
65:48 - by five. That's all it does. And in the find
quantities method, we can essentially pass
65:53 - in the thrush value. So let's save that. Let's
close this out and try to run that. Type.
66:00 - Okay. threshold missing. Okay, I think I forgot
one part, where to specify a threshold and
66:10 - type. So this is CV dot Thresh. On this go,
binary, binary raising the image basically.
66:18 - Okay, let's run that. And there were 839 contours
that were found, we can visualize that let's
66:29 - print ad to display this Thresh. Image, passing
Thresh. Same that run. And this was the thresholded
66:40 - image you're using 125. close this out, using
125 as our threshold value, and 255 as a maximum
66:51 - value, we got this thresholded image. And
when we tried to find the current use on this
66:56 - image, we got 839 concepts. Now don't worry
too much about this thresholding business,
67:04 - we'll discuss this in the advanced section
of this goes more in depth just know that
67:08 - thresholding attempts to binarize an image,
take an image and convert it into binary form
67:13 - that is either zero or black, or white, or
to Vi five. Now what's cool in open CV is
67:20 - that you can actually visualize the contours
that were found on the image by essentially
67:26 - drawing over the image. So what do we do real
quick is actually input NumPy NumPy as NP
67:40 - and after this, I'm going to create a blank
variable and set this equal to NP dot zeros
67:48 - of image dot shape of the first two values,
and maybe give it a data type of I know you
67:56 - are 28 we can display this image because blank
pawsome blank, just to visualize and have
68:09 - a blank image to work with. Let's save that
and go to a blank image. This is of the same
68:16 - dimensions as our original accounts image.
So what I'm going to do is I'm going to draw
68:23 - these contours on that blank image so that
we know what kind of contours that open CV
68:29 - found. So the way we do that is by using the
CV dot draw contours method, it takes in an
68:37 - image to draw over fill blank, it takes in
the contours, which has to be a list, which
68:43 - in this case is just the quantities list.
It takes an account to index which are basically
68:49 - how many countries do you want in the image.
Since we want all of them since we want to
68:54 - draw all of them, we can specify a negative
one, give it a color, let's add this to BGR.
69:00 - So let's set this to red zero comma zero comma
255. And we can give it a thickness of maybe
69:06 - two. And we can display the blank image. So
let's call this contused join. And we can
69:15 - pass in blank. Save that and run. Okay, there
was an error I think this has to be shaped.
69:28 - Okay, so these were the cartoons that would
draw on the image. If you take a look at the
69:34 - threshold value thresholded image, it's not
the same thing. What I believe it attempted
69:42 - to do is instead it found the edges of this
image all the edges of this image and attempted
69:48 - to draw it out on this blank image. Let's
set this so let's set the thickness to maybe
69:56 - one so that we have a crisper view Okay, so
these were the quantities that were drawn
70:07 - in the image. And in fact, if you try to visualize
it with Kenny, let's actually visualize that
70:13 - with Kenny uncomment. That out, run. blows
on the point undefined. Okay, that has to
70:22 - be an image. Okay, let's look at Kenny, let's
look at this. Okay, it's not the same thing.
70:35 - And that makes sense, because our firing coaches
method and use Kenny, as the basis of detecting
70:41 - and finding the controls. But we can do that.
Let's not use a thresholding method. And instead,
70:49 - let's use Kenny. So we can pass in Kenny here.
Save that and run. And, okay, that pretty
70:58 - much the same thing, right? It's basically
a mirror image of these two, like I said,
71:05 - you can get away with thinking of contours
as edges. They're not the same thing. But,
71:09 - but you can think of them as edges. Because
from a programming point of view, they kind
71:14 - of like the edges of the image. Right? The
other boundaries, they are curves that join
71:19 - the points along the boundary, those are basically
edges. So let's try to blow that image. Let's
71:25 - uncomment that out. Let's see what that does.
I don't think that had any effect because
71:32 - we didn't pass in blood. Okay, 380 countries
have found and mirror images of each other.
71:44 - So generally, what I recommend is that you
use scanning method first, and then try to
71:50 - find the corn who's using that, rather than
try to threshold the image and then find the
71:56 - contours on that. Because like we will discuss
in the advanced section, this type of thresholding.
72:01 - The simple thresholding has its disadvantages.
Maybe because we're passing in a simple, just
72:08 - one value, dread binarize the image using
this threshold value, right? It's not the
72:12 - most ideal, but in some cases, in most cases,
it is most favored kind of thresholding because
72:19 - it's the simplest, and it does the job pretty
well. So that's pretty much it. For this video,
72:25 - we talked about how to identify quantities
in open CV. But in two methods first trying
72:31 - to find the edge cascades of the image using
the canny edge detector, and try to find the
72:35 - quantities using that and also trying to binarize
that image using the CV dot threshold and
72:41 - finding the contours on that. So if you have
any questions, leave them in the comments
72:47 - below. I'll be sure to check them out. Otherwise,
as always, I'll see you guys in the next video.
72:53 - Hey, everyone, and welcome back to another
video. We are now at the advanced section
73:00 - of this course, where we are going to discuss
the advanced concepts in open CV. So what
73:06 - we're going to be doing in this video is actually
discussing how to switch between color spaces
73:10 - in urgency. Our color spaces, basically a
space of colors, a system of representing
73:18 - an array of pixel colors. RGB is a kind of
space grayscale is color space. We also have
73:25 - other color spaces like HSV, lamb, and many
more. So let's start off with trying to convert
73:33 - this image to grayscale. So we're going to
convert from a BGR image which is open CV
73:38 - is default way of reading and images. And
we're going to convert that to grayscale.
73:44 - So the way we do that is by saying gray is
equal to CV dot CBT color. We pass in the
73:51 - image and we specify a color code, which is
CV dot color, underscore BGR to to grip since
74:01 - we're converting from a BGR image format to
grayscale format, and we can display this
74:06 - image I st gray and passing in grip. Let's
save that and run Python spaces dot p y. We
74:17 - had a problem as a comma, Save and Run. And
this is the grayscale version of this BGR
74:27 - image. Cool pretty cool. grayscale images
basically show you the distribution of pixel
74:33 - intensities at particular locations of your
image. So let's start off with trying to convert
74:40 - this image to an HSV format. So from Jeff
from vgr to HSV. HSV is also called hue saturation
74:49 - value and is kind of based on how humans think
and conceive of color. So the way we conduct
74:57 - that is by saying HSV is equal to CV dot CBT
color, we pass in the IMG variable. And we
75:06 - specify a color code, which is CV dot color,
undergo BGR to HSV. And we can display the
75:14 - syringe called as HSV and pass in HSV. Let's
save that. And this is the HSE version of
75:23 - this BGR image. As you can see that there
was a lot of green in this era and the skies
75:31 - are reddish. Now we also have another kind
of color space. And that is called the LA
75:40 - be color space. So we're going to convert
from BGR to L A, B. This is sometimes represented
75:46 - as L times A times B, but but v free to use
whatever you want. So lb is equal to CV dot
75:54 - CVT color, we pass the MG and the color on
the scope of BGR. to AB see that I'm sure
76:05 - colas lamb pass and lamb is wrong that and
this is the LGB version of this BGR image.
76:15 - This kind of looks like a washed down version
of this BGR image. But hey, that's the lamb
76:21 - format is more tuned to how humans perceive
color. Now when I started off with this goes,
76:26 - I mentioned that open CV reads in images in
a BGR format that has blue, green and red.
76:32 - And that's not the current system that we
use to represent colors outside of open CV.
76:38 - Outside of open CV, we use the RGB format,
which is kind of like the inverse of the BGR
76:43 - format. Now if you try to display this IMG
image in a Python library that's not open
76:49 - CV, you're probably going to see an inversion
of colors. And we can do there real quick.
76:54 - Let's try to input mat plot lib dot pie plot
as PLT. And we can can, we can basically uncomment
77:06 - commented that out. And we can try and display
this image variable. So we're gonna say PLT
77:11 - dot, I am show pass in the image. And we could
say a peak, or we could say PLT dot show,
77:19 - maybe let's comment this out, save that and
run. And this is the image you get. Now, if
77:25 - you compare with the image that open CV read,
this is completely different, these two are
77:30 - completely different images. And the reason
for this is because this image is a BGR image
77:37 - and open CV displays BGR images. But now if
you tried to take this BGR image and try to
77:43 - display it in matplotlib, for instance, matplotlib
has no idea that this image is a BGR image
77:50 - and displays that image as if it were an RGB
image. So that's why you see an inversion
77:56 - of color. So where there's red over here,
you see a blue, where there's blue over here
78:00 - you see a red, and there are ways to convert
this from BGR to RGB. And that is by using
78:08 - open CV itself. So let's comment that out.
And let's uncomment this all out. And right
78:17 - over here, let's say BGR to RGB. And what
we're going to say is our RGB is equal to
78:26 - CV dot CVT color, we can pass in the BGR image
oopsie, we can pass in the br image. And what
78:35 - we're going to do is specify a color code,
which you see without color on the scope BGR
78:41 - to RGB. And we can try to display this image
in in open CV and see what that displays RGB.
78:51 - And we can also display this in matplotlib.
So I've passed in the RGB. And we can do PLT
79:01 - dot show, save that and go here it is you
Python spaces dot p y. What I'm most interested
79:14 - in is this. And this. Now again, you see an
inversion of colors, but this time in open
79:22 - CV because now you provided open CV and RGB
image. And it assumed it was a BGR image.
79:29 - And that's why there's an inversion of colors.
But we pass in the RGB image to matplotlib
79:34 - and matplotlib is default is RGB. So that's
why I displayed the proper image. So just
79:40 - keep this in mind when you're working with
multiple libraries, including open CV and
79:44 - matplotlib for instance, because do keep in
mind the inversion of colors that tends to
79:51 - take place between these two libraries. So
now another thing that I want to do is we've
80:01 - essentially converted the BGR to grayscale,
we've essentially converted BGR, HSV BGR to
80:07 - RGB BGR to RGB, what we can do is we can do
the inverse of that, we can convert a grayscale
80:14 - image to BGR, we can convert an HSV to BGR,
we can convert an LNB to BGR, and RGB to be
80:23 - GL, and so on. But here's one of the downsides.
You cannot convert grayscale image to HSV
80:31 - directly. If you wanted to do that, what do
you have to do is convert the grayscale to
80:35 - BGR. And then from video to HSV. So we're
gonna do that real quick. So we're gonna say
80:41 - HSV, two BGR. Okay, so the first thing we
do is HSV, underscore vgr. Basically, converting
80:50 - from HSV to BGR is equal to CV dot CVT color,
this will take in the HSV image. And the color
80:58 - code will be color on Cisco HSV, two BGR.
And we can display this image, let's call
81:07 - this HSV, two BGR and pass in HD on the scope
BGR. On screw VR, save that and run. Okay,
81:22 - we're not interested in this. So let's close
this out. But essentially, this is the HSV,
81:28 - two BGR image. If this was the HV image, we
converted this image to BGR. And we can try
81:36 - this with lamb. So let's call this lamb to
lamb, and of course, lamb. And let's copy
81:45 - this and paste that. We can get rid of Mapplethorpe's
it's been addressed in an email. So go out
81:51 - and run. Okay, that was a mistake. We said
HSV, lamb to L baby to BGR. That was my mistake.
82:04 - Cool. So if this was the lamb version, this
is the lamb to BGR version back from BGR to
82:13 - lamb and from lamb to BGR. So that's pretty
much it. For this video, we discussed how
82:21 - to convert, we discussed how to convert between
color spaces from BGR to grayscale, HSV, LGB,
82:29 - and RGB. And if you want to convert from grayscale
to nav, for instance, note that there is no
82:37 - direct method, what you could do is convert
that grayscale to BGR. And then from BGR to
82:43 - and maybe that's possible. By directly. I
don't think there was a way to do that, if
82:49 - open CV could come up with the feature like
that, it would be good, but it's not gonna
82:52 - hurt you to write extra lines of code, at
least two or three lines of code extra, moderately
82:57 - hard. In the next video, we will be talking
about how to split and merge color channels
83:01 - in open CV. If you have any questions, leave
them in the comments below. Otherwise, I'll
83:06 - see you guys in the next video. Everyone and
welcome back to another video. In this video,
83:15 - we're going to be talking about how to split
and merge color channels in open CV. Now,
83:20 - a color image basically consists of multiple
channels, red, green, and blue. All the images
83:27 - you see around you all the BGR or the RGB
images are basically these three color channels
83:34 - merged together. Now open CV allows you to
split an image into its respective color channels.
83:40 - So you can take a BGR image and split it into
blue, green and red components. So that's
83:48 - what we're going to be doing in this video,
we're going to be taking this image of the
83:52 - park that we had seen in previous videos,
and we're going to split that into its three
83:57 - color channels. So the way we do that is by
saying b comma g comma r, which stands for
84:02 - the respective color channels, and set this
equal to CV dot split split of the image.
84:09 - So the CV dot split basically split the image
into blue, green and red. And we can display
84:15 - this image by saying CV dot I'm sure, let's
call this blue and pass in blue. And let's
84:22 - do the same for green image and pass in G
and we can do the same for the red part two
84:31 - are and we can actually visualize the shape
the shapes of these images. So let's first
84:38 - print the image node shape, and then print
the bead on shape. And then print the genome
84:44 - shape and then print the our dot shape. Basically,
we're printing the shapes and dimensions of
84:52 - the image and the blue, green and red and
we're also displaying these images. So let's
84:57 - run Python split merge dot p Why. And these
are the images that you get back. This is
85:05 - the blues, the blue image, this is the green
image. And this is the red image. Now these
85:11 - are depicted and displayed as grayscale images
that show the distribution of pixel intensities.
85:19 - regions where it's lighter showed that there
is a far more concentration of those pixel
85:24 - values and regions where it's darker, represented
a little or even no pixels in that region.
85:31 - So take a look at the blue pick the blue channel
first. And if you can, if you compared with
85:38 - the original image, you will see that the
sky is kind of almost white, this basically
85:45 - shows you that there is a high concentration
of blue in the sky, and not so much in the
85:51 - the trees or the grass, let's take a look
at the green. And there is a fairly even distribution
85:56 - of pixel intensities between the between the
grass, the trees, and some parts of the sky.
86:04 - And take a look at the red color channel.
And you can see that parts of the trees that
86:09 - are red are whiter and the grass in the sky
are not that white in this red image. So this
86:17 - means that there is not much red color in
those regions. Now coming back, let's take
86:22 - a look at the shapes of the image. Now this
stands for the original image, the BGR image,
86:29 - the additional elements in the tuple here
represents the number of color channels, three
86:34 - represents three color channels blue, green,
and red. Now if we proceeded to display the
86:40 - shapes of BG and our components, we don't
see a three in the tuple. That's because the
86:46 - shape of that component is one. It's not mentioned
here, but it is one. That's why when you try
86:52 - to display this image using see even if I'm
show it displays it as a grayscale image,
86:57 - because grayscale images have a shape of one.
Now, let's try and merge these color channels
87:05 - together. So the way we do that is by seeing
the merge image, merged images equal to CV
87:12 - dot merge. And what we do is we pass in a
list of blue of blue comma g comma r, I'd
87:20 - save that in let's display that things either
on show call this them call this the merged
87:28 - image. And we can pass in merged. So let's
save that and run. And we get back the merged
87:35 - image by basically merging the three individual
color channels red, green, and blue. Now there
87:44 - is a way an additional way of looking at the
actual color there is in that channel. So
87:49 - instead of showing you grayscale images, it
shows you the actual color involved. So for
87:54 - the blue image, you get the blue color channel
for the red channel, you get the red color
88:02 - for that channel. And the way we do that is
we actually have to reconstruct the image.
88:08 - The shapes of these images are basically grayscale
images. But what we can do is we can actually
88:14 - create a blank image, a blank image using
NumPy. And essentially, what we're going to
88:20 - do is we're going to say blank is equal to
NP dot zeroes. And we're going to set this
88:24 - to the shape of the image, but only the first
two values. And we can give it a data type
88:31 - of you iemt, eight, eight, which basically
are for images. And to print the blue color
88:39 - channel, what we're going to do is we're going
to say, down here, we're going to say blue
88:43 - is equal to CV dot image, we're going to pass
in the list of b comma, blank comma blink.
88:53 - And we're going to do the same thing for green
and set is equal to CV dot merge of blank
89:00 - comma g comma blank. And we're going to do
the same thing for red by setting this equal
89:06 - to CV dot merge of blank comma blink, comma,
comma red. Basically, what I've done is this
89:15 - blank image basically consists of the height
and the width, not necessarily number of color
89:23 - channels in the image. So by essentially merging
the blue image in its respective compartment,
89:29 - so blue, green and red, we are setting the
green and the red components to black and
89:35 - only displaying the blue channel. And we're
doing the same thing for the green by setting
89:39 - the blue and the red components to black.
And the same thing for red by setting the
89:44 - blue and the green components to black. And
we can display this by saying blue, green,
89:52 - and red. Let's save that and run and now you
actually get the color in its respective color
90:03 - channels. Take a look at this, you now be
able to visualize the distribution much better.
90:08 - Here you can see lineup later portions represent
a high distribution. Lighter portions here
90:14 - represent the high distribution of red and
higher and wider regions represent a high
90:18 - distribution of green. So essentially, if
you take these three images of these color
90:24 - towns and merging them together, you essentially
get back the merged image. That's the merged
90:32 - image. So that's pretty much it. For this
video, we discuss how to split an image into
90:39 - three respective color channels, how to reconstruct
the image to display the actual color involved
90:46 - in that channel, and how to merge those color
channels back into its original image. In
90:51 - the next video, we'll be talking about how
to smooth and blur an image using various
90:56 - blurring techniques. If you have any questions,
leave them in the comments below. Otherwise,
91:01 - I'll see you guys in the next video. Hey,
everyone, and welcome back to another video.
91:08 - In this video, we're gonna address the concepts
of smoothing and blurring in urban CV. Now,
91:14 - before I mentioned that we generally smooth
and image when it tends to have a lot of noise,
91:20 - and noise that's caused from camera sensors
are basically problems in lighting when the
91:26 - image was taken. And we can essentially smooth
out the image or reduce some of the noise
91:31 - by applying some blurring method. Now Previously,
we discussed the Gaussian Blur method, which
91:39 - is kind of one of the most popular methods
in blurring. But generally, you're going to
91:44 - see that Gaussian Blur won't really suit some
of your purposes. And that's why there are
91:49 - many blurring techniques that we have. And
that's what we're going to address in this
91:54 - video. Now, before we actually do that, I
do want to address a couple of concepts. Well,
91:59 - let's actually go to an image and discuss
what exactly goes on when you try to apply
92:04 - blur. So essentially, the first thing that
we need to define is something called a kernel
92:10 - or window. And that is essentially this window
that you draw over an image that has two lines
92:21 - here. Let's draw another line. So this is
essentially a window that you draw over a
92:28 - specific portion of an image. And something
happens on the pixels in this window. Let's
92:35 - change it to blue. Yeah. So essentially, this
window has a size, this size is called a kernel
92:44 - size. Now kernel size is basically the number
of rows and the number of columns. So over
92:50 - here, we have three columns and three rows.
So the kernel size for this is three by three.
92:55 - Now, essentially, what happens here is that
we have multiple methods to apply some blue.
93:02 - So essentially, blur is applied to the middle
pixel as a result of the pixels around it,
93:09 - also called the surrounding pixels. Let's
change that to a different color. So something
93:18 - happens here as a result of the pixels around
the surrounding pixels. So with that in mind,
93:23 - let's go back and discuss the first method
of blurring which is averaging. So essentially,
93:31 - averaging is we define a kernel window over
a specific portion of an image, this window
93:37 - will essentially compute the pixel intensity
of the middle pixel of the true center as
93:43 - the average of the surrounding pixel intensities.
So if this was to green, suppose if this pixel
93:51 - intensity was one, this was maybe two, this
is 345678, you get the point. Essentially,
93:58 - the new pixel intensity for this region will
be the average of all the surrounding pixel
94:05 - intensity. So that's summing up one plus two
plus three plus four plus five plus six plus
94:08 - seven plus eight, and dividing that by eight,
which is essentially the number of surrounding
94:13 - pixels. And we essentially use that result
as the pixel intensity for the middle value,
94:19 - or the true center. And this process happens
throughout the image. So this window basically
94:24 - slides to the right. And once that's done,
it slides down, and computed basically for
94:30 - all the pixels in the image. So let's try
to apply and see what this does. So what we're
94:36 - going to do is we're going to say average,
is equal to CV don't blur. The CV or blow
94:42 - method is a method in which we can apply averaging
blur. So we define the source image which
94:48 - is IMG, we give it a kernel size of let's
say three by three. And that's it. We can
94:53 - display this image called as average, average
blur. Save that and run Python smoothing dot
95:03 - p y net Gosh, we have to pass an average,
save that and run. And this is basically the
95:15 - average blow that's applied. So what the algorithm
did in the background was essentially define
95:20 - a candle window of a specified size three
by three. And it computed the center value
95:26 - for a pixel using the average of all the surrounding
pixel intensities. And the result of that
95:32 - is we get a blurred image. So the higher kernel
size we specified, the more blur there is
95:38 - going to be in the image. So let's increase
that to seven by seven and see what that does.
95:45 - And we get an image with way more blur. So
let's move on to the next method, which is
95:52 - the Gaussian Blur. So Gaussian basically does
the same thing as averaging, except that instead
96:00 - of computing the average of all of this running
pixel intensity, each running pixel is given
96:07 - a particular weight. And essentially, the
average of the products of those weights gives
96:12 - you the value for the true center. Now using
this method, you tend to get less blurring
96:18 - than compared to the averaging method. But
the Gaussian Blur is more natural as compared
96:24 - to averaging. So let's print that out. Let's
call this Yes. And set this equal to CV dot
96:32 - Gaussian Blur. And this will take in the source
image, so IMG kernel size of seven by seven,
96:40 - just to compare with the averaging. And another
parameter that we need to specify is sigma
96:45 - x, or basically the standard deviation in
the x direction, which for now, just going
96:51 - to set as zero. And we can put that out, call
this Gaussian Blur and pass in gaps, save
97:00 - that and run. If you can bear with this, you
see that both of them use the same code size,
97:09 - but this is less blurred as compared to the
average method. And the reason for this is
97:14 - because a certain weight value was added when
computing the blur. Okay, so let's move on
97:21 - to the next method. And that is median blur.
So let's go back to our image. And medium
97:30 - blurring is basically the same thing as averaging,
except that instead of finding the average
97:36 - of the surrounding pixels, it finds the median
of the surrounding pixels. Generally, medium
97:43 - blurring tends to be more effective in reducing
noise in an image as compared to averaging
97:48 - and even Gaussian Blur. And it's pretty good
at removing some salt and pepper noise that
97:53 - may exist in the image. In general, people
tend to use this image in advanced computer
97:59 - vision projects that tend to depend on the
reduction of substantial amount of noise.
98:04 - So let's go back here. And the way we apply
the blur is by saying, let's call this median
98:11 - and set the Z and set this equal to CV dot
median, blue, we pass in the source image,
98:17 - and this kernel size will not be a tuple of
three by three, but instead, just an integer
98:24 - to three. And the reason for this is because
open CV automatically assumes that this kernel
98:30 - size will be a three by three, just based
off this integer. And we can print this out.
98:36 - Let's call this median, blue, and pass in
median. And let's compare it with that. So
98:45 - I set that to seven. And comparing it with
Gaussian Blur, and averaging blur, you tend
98:56 - to look at this. And you can make up some
differences between the two images. So it's
99:02 - like as if this was your painting, and it
was still drawing. And you take something
99:06 - and smudge over the image and you get something
like this. Now generally, medium blurring
99:13 - is not meant for high Colonel sizes like seven
or even five in some cases, and it's more
99:19 - effective in reducing some of the noise in
the image. So let's, let's change this all
99:24 - to three by three. Let's copy that, change
that to three by three. And we can change
99:34 - that to three. And now let's have a comparison
between the three. This is your Gaussian below.
99:42 - This is your average in blue, this is your
median love. So compared with these two, you
99:47 - can see that there is kind of less blurring
when Gaussian when you can sort of make out
99:58 - the differences between the two Very subtle,
but there are a couple of differences between
100:03 - the two. Finally, the last method we're going
to discuss is bilateral blurring caused by
100:09 - natural lateral. Now bilateral bearing is
the most effective, and sometimes used in
100:21 - a lot of advanced computer vision projects,
essentially because of how it blurs. Now traditional
100:28 - blurring methods basically blur the image
without looking at whether you're, whether
100:32 - you're reducing edges in the image or not.
bilateral blurring applies blurring but retains
100:38 - the edges in the image. So you have a blurred
image, but you get to retain the edges as
100:43 - well. So let's call this bilateral and multilateral
and set this equal to CV dot bilateral filter.
100:52 - And we pass in the image, we give it a diameter
of the pixel neighborhood. Now notice this
100:58 - isn't a kernel size, but in fact, a diameter.
So let's set this to five for now, give it
101:05 - a sigma color, which is basically the color
sigma sigma color, a larger value for this
101:11 - color sigma means that there are more colors
in the neighborhood, that will be considered
101:16 - when the blue is computed. So let's set this
to 15. For now. And sigma space is basically
101:22 - your space sigma. larger values of this space,
sigma means that pixels further out from the
101:30 - central pixel will influence the blurring
calculation. So let's set this to 50. So let's
101:35 - take a look at that sigma spacing. So for
example, in bilateral filtering, if this is
101:42 - the value for this central pixel, or the true
center is being computed, by giving a larger
101:48 - values for the Sigma space, you essentially
are indicating that whether you want pixels
101:53 - from this far away, or maybe this far away,
or even this far away from influencing this
102:00 - particular calculation. So if you give like
a really huge numbers, then probably a pixel
102:06 - in this region might influence the computation
of this pixel value. So let's set this to
102:13 - 15. For now, and let's display this image.
So call the cv.on show is called as bilateral
102:21 - and pass on bilateral. Let's save that and
run. And this is your bilateral image. So
102:30 - let's compare with all the previous ones that
we had. Compared with this. Much better compared
102:38 - with averaging way much better. Let's compare
with median. The edges are slightly, it's
102:47 - slightly blurred. If you compare with the
original image, they kind of look the same
102:53 - thing. Okay, it kind of looks like there's
no blur applied. So maybe let's increase this
102:59 - diameter to I know 10. And not much was done,
the edges are still there, it kind of looks
103:08 - like the original image itself. So let's try
into one of the other parameters. Let's add
103:15 - this to 3435. Let's set this dude 25. We're
only playing around with these with these
103:24 - values. And now you can basically make our
generic that this is starting to look a lot
103:29 - like median blow. We need even larger values.
It's starting to show you that this is more
103:35 - looking like a smudged painting version of
this image, right, there's a lot of blur applied
103:41 - here, but the council looking smudged. So
definitely keep that in mind when you are
103:47 - trying to apply blurring the image, especially
with the bilateral and median lowering, because
103:53 - higher values of this basic mouth or bilateral
or the kernel size for medium glowing, and
103:58 - you tend to end up with a washed up smudged
version of this image. So definitely keep
104:05 - that in mind. But that kind of summarizes
whatever we we've done in this video, we discussed
104:11 - averaging, Gaussian, median and bilateral
blurring. So in the next video, we'll be talking
104:17 - about bitwise operators in open CV. So again,
like always, if you have any questions, leave
104:23 - them in the comments below. Otherwise, I'll
see you guys in the next video. Hey everyone,
104:30 - and welcome back to another video. In this
video we're gonna be talking about bitwise
104:35 - operators in urban CV. Now, there are four
basic bitwise operators and or XOR and not.
104:44 - If you've ever taken an introductory CS course,
you will probably find these terms familiar
104:49 - bitwise operators, and they are in fact used
a lot in image processing, especially when
104:56 - we're working with masks like we'll do in
the next video. So at a very high level bitwise
105:01 - operators operate in a binary manner. So a
pixel is turned off if it has a value of zero,
105:08 - and is turned on if it has a value of one.
So let's actually go ahead and try to import
105:14 - NumPy as NP. And what I'm going to do is I'm
going to create a blank variable and set this
105:21 - equal to NP dot zeros of size 400 by 400.
And we can give it a datatype of you I empty
105:33 - it is what I'm going to do is I'm going to
use this blank variable as a basis to draw
105:38 - a rectangle and draw a circle. So I'm going
to say return angle is equal to CV dot rectangle,
105:48 - we can say blink dot copy. And we can pass
in the starting point. So let's give it a
105:55 - margin of around 30 pixels on either side.
So we're going to start from 30, comma 30.
106:01 - And we can go all the way across to 370370.
And we can give it a color. Since this is
106:10 - not a color image, but rather binary image,
we can just give it one parameter, so 255.
106:16 - White, and give it a thickness of negative
one, because we want to fill this image. And
106:22 - then I'm going to create another circle variable
and set this equal to CV dot circle, we're
106:28 - going to say blank, don't copy, we are going
to give it a center. So the center will be
106:33 - the absolute center, so 200 by 200. And let's
give it a radius of give a radius of 200.
106:41 - And give it a color up to five, five, and
let's fill in the circle. So negative one.
106:46 - So let's display this image and see what we've
seen or we're working with. So we'll call
106:51 - this rectangle and passing the rectangle.
And we're going to do the same thing with
106:58 - the circle, it's called a circle. And pass
in the circle, save that and run Python bitwise
107:07 - r p y. So we have two images that we're going
to work with this image of rectangle, and
107:14 - this image of a circle. So let's start off
with the first basic bitwise operator, and
107:21 - that is bitwise. And so before we actually
discuss what bitwise ad really is, let me
107:27 - show you what it does. So essentially, what
I'm going to do is I want to say bitwise is
107:32 - go and is equal to CV dot bitwise. And, and
basically what I have to do is pass in two
107:41 - source images that are these two images, rectangle,
and circle. Now we can display this image,
107:48 - let's call this beautiful lines, and let's
pass in bitcoins and save, run. And essentially,
107:57 - you get back this image. So essentially, what
bitwise AND did was it took these two images,
108:06 - placed them on top of each other, and basically
returned the intersection. Right, and you
108:12 - can make out when you take this image, put
it over this image, you have some triangles
108:18 - that are common to both of these images. And
so those are set to black zero, while the
108:23 - common regions are returned. So the next one
is basically bitwise. Or now bitwise, or real
108:34 - simply returns both the intersecting as well
as the non intersecting regions. So let's
108:40 - try this bitwise OR is equal to CB dot bitwise
AND scope or you pass in rectangle, we pass
108:46 - in circle. Now we can print that, let's call
this bitwise OR pass in bitwise. Oops, that
108:55 - was or save that and run and bitwise OR, okay,
there's a bitwise OR, by mistake. It's a bitwise
109:09 - OR basically return this funky looking this
funky looking shape. Essentially what it did
109:14 - is it took these two images, put them over
each other from the common regions and also
109:19 - found regions that are not common to both
of these images and basically superimpose
109:24 - them. So, basically, you can just put them
together and find the resulting shape and
109:29 - this is what you get, but this image over
this and you get this moving on. The next
109:35 - one is bitwise XOR, which basically is good
for returning the non intersecting regions.
109:45 - So this found the the intersecting oops, the
inter setting regions this found the sky Brought
110:00 - back, the no one intersecting in interest
selecting regions, and xR only finds the non
110:13 - intersecting regions. So let's do that I say
bitwise call this XOR is equal to CV dot bitwise
110:21 - underscore xR, we pass in the rectangle, passing
the rectangle when we pass in the circle,
110:30 - we can display this CV and I'm sure close
bitwise XOR. And we can pass in bitwise XOR.
110:40 - Save that and run. And here we have the non
intersecting regions of these two images when
110:48 - you put them over each other. Pretty cool.
And just to recap, this bitwise AND AGAIN,
110:57 - returns the intersection regions bitwise,
or returns the knowledge second regions as
111:02 - well as the intersecting regions bitwise XOR,
returns the knowledge second regions. So essentially,
111:09 - if you take this bitwise XOR, and subtract
it from bitwise, or you get bitwise end. And
111:16 - conversely, if you subtract bitwise, and from
the device, or you get bitwise XOR. Just so
111:22 - essentially, that's a good way of visualizing
what exactly happens with these bitcoins operators.
111:29 - And finally, the last method we can discuss
is bitwise. Not essentially, it doesn't return
111:38 - anything. What it does is it inverts the binary
color. So let's do that. So let's call this
111:46 - bitwise. Not is equal to CV dot bitwise. underscore
not. And this only takes in one source image.
111:53 - So let's set this to the rectangle put out.
And we can display this. Let's call this rec
112:00 - tangle not, we can pass in bitwise not see
that. And basically what it did is if you
112:11 - look at this image, it found all the white
regions, all the white pixels in the image
112:17 - and inverted them to black and all the black
images it inverted to white, essentially,
112:22 - it converted the white to black and from the
ads from black to white. So we can try that
112:26 - with the circle. Let's call this circle, we
can pass in the circle here. Save and Run
112:34 - and the resultant the resulting circle, not
that you get is this. This is white hole.
112:41 - This is a black hole for physicists out there.
Okay, so that's pretty much it. For this video,
112:48 - I just wanted to introduce you all to the
idea of bitwise operations and how it works.
112:54 - In the next video, we'll be actually talking
about how to use these bitwise operations
112:58 - in a concept called masking. So if you have
any questions, leave them in the comments
113:03 - below. Otherwise, I'll see you guys in the
next video. Hey, everyone, and welcome back.
113:11 - In this video, we're going to be talking about
masking in open CV. Now in the previous video,
113:16 - we discussed bitwise operations. And using
those bitwise operations, we can essentially
113:22 - perform masking in open CV masking essentially
allows us to focus on certain parts of an
113:30 - image that we'd like to focus on. So for example,
if you have an image of people in it, and
113:36 - if you're interested in focusing on the faces
of those people, you could essentially apply
113:41 - masking and essentially mask over the people's
faces and remove all the unwanted parts of
113:47 - the image. So that's basically our high level
intuition behind this. So let's actually see
113:52 - how this works in open CV. So I basically
read in a file and display that image. The
114:00 - other thing I'm going to do is I'm going to
import NumPy NumPy as NP, what I'm going to
114:06 - do is I'm going to say blank is equal to NP
dot zeros of size of size image dot shape
114:13 - with the first two values. Now this is extremely
important, the dimensions of the mask have
114:18 - to be the same size as that of the image.
If it isn't, it's on good work. And we can
114:23 - give it a data type of UI eight, you can see
it if you want to display this, we can display
114:32 - this. It's just going to be a black image,
schools blank image and pawson blank. Essentially,
114:40 - what I'm going to do is I'm going to draw
a circle over this blank image and call that
114:46 - my mask. So I'm going to say mask is equal
to CV dot circle. We're going to draw the
114:51 - blank image on the blank image, we can give
it a center of this image so let's say image
114:58 - dot shape of Have one divided by two divided
by two, and image down shape of two image
115:06 - a shape of zero divided by divided by two.
And we can give it a radius of, I don't know,
115:12 - I'd say 100 pixels, give it a color of 255,
give it a thickness of negative one. And we
115:19 - can visualize a mask as mask and passing mask.
So let's run that. Python masking dot p y.
115:31 - And this is essentially our mask. There's
the blank image we're working with. And this
115:36 - is the image that we want to mask over. So
let's actually create a masked image, we're
115:42 - going to say masked image is equal to CV dot
bitwise. underscore and this source image.
115:52 - So IMG, IMG, and we specify the parameter
mask is equal to mask, which is this circle
116:02 - image over here. And we can display this image,
call this masked image. And we can pass in
116:09 - masked, save that and run. And this is essentially
your masked image, you took this image, you
116:17 - took this image, you put this image over and
found the intersecting region. Okay, by optionally
116:25 - passing the mask is equal to mask. That's
exactly what we're doing. Cool. That's right.
116:31 - And, you know, play around with this, let's
maybe move this by a couple of pixels around,
116:38 - let's say 45. Save and Run moves down to zero,
okay, this has to be 45 plus 45, save up and
116:53 - running. And we get the image of the cat,
we can draw, we can draw a circle, or we can
116:58 - draw a rectangle instead. What's bottom blank,
skip that. Let's give you that in draw, give
117:09 - it a static endpoint of let's copy this and
add a couple of pixels or maybe 100 pixels
117:19 - this way, in 100 pixels. This way, we can
get rid of this, we don't need that and say
117:25 - that, right? This is this, this is the square.
And this is essentially the masked image.
117:31 - So let's actually try this with. So let's
actually try this with a different image.
117:38 - So we have got an image. Let's try it with
maybe these cats too. Let's go back to cats
117:46 - to save that run. And this is the mask that
we get by putting these two on each other.
117:55 - And essentially, you can play around with
these as you feel fit. You can maybe try different
118:01 - shapes, weird shapes. And the way you can
do get these weird shapes, essentially creating
118:07 - a circle or rectangle and applying bid wise
and you get this weird shape. And then you
118:12 - can use that weird shape as your mask. So
let's just try that. Let's let's try that.
118:18 - Oh, we're going to say let's, let's call this
circle and blanked out copy copy and create
118:30 - a rectangle. Let's just grab it from this
re read Where are we from bitvise Let's grab
118:46 - this rectangle copy that piece over time the
copy 3030 Okay, blank, same shape. So let's
119:00 - create this weird weird shape is equal to
CV dot bitcoins on the scope end of this circle
119:09 - this rectangle and we don't need to specify
anything else. um what's one of visualizes
119:17 - let's close this out try to see see it on
on show call this the weird shape passing
119:26 - the weird shape and wrong. masking undefined
was mask westmar Mosque Okay. Good. This is
119:38 - the weird shape that we get. We're not really
going for a half moon But hey, whatever. Let's
119:45 - close this out. Use this weird shape is mask.
So use weird shape as a mask and let's see
119:55 - the final mask image and this is essentially
your weird weird shape, masked image. Let's
120:04 - call this a weird shape mask image, weird
shaped mask damage. This little halfmoon here.
120:12 - And essentially you can, you can do pretty
much anything you want with this, you can
120:17 - experiment with various shapes and sizes and
stuff like that. But just know that the size
120:23 - of your mask has to be at the same dimensions
as that of your image. If you want to see
120:29 - why not maybe subtract 100 pixels possible,
but let's support it, though. So that's maybe
120:38 - like subtract tubal on it. I don't know whether
that'll work. But guess what? Okay, so let's
120:47 - just say, image on shape of while I'm okay,
let's just give it a different size. What
120:55 - are we? Why are we even using image, let's
go this size of 300 by 300. Definitely not
121:01 - the size of this. And we get this assertion
failed m time, blah, blah, blah, maskhadov,
121:08 - same size, in function, whatever. So essentially,
these need to be at the same size, otherwise,
121:13 - it's going to fail and throw you an error.
So that's it for this video, we talked about
121:18 - masking, again, nothing to do different. We've
essentially used the concept of bitcoins and
121:23 - from the previous video, and you will see
that when we move on to computing histograms
121:28 - in the next video, where masking really comes
into play, and how masking really affects
121:33 - your histograms. So if you have any questions
again, leave them in the comments below. Otherwise,
121:40 - I'll see you in the next video. Hey, everyone,
and welcome back to another video. In this
121:48 - video, we're going to be talking about computing
histograms in open CV. Now histograms basically
121:54 - allow you to visualize the distribution of
pixel intensities in an image. So whether
122:00 - it's a color image, or whether it's a grayscale
image, you can visualize these pixel intensity
122:06 - distributions with the help of a histogram,
which is kind of like a graph or a plot that
122:12 - will give you a high level intuition of the
pixel distribution in the image. So we can
122:17 - compute a histogram for grayscale images and
compute a histogram for RGB images. So we're
122:24 - gonna start off with computing histograms
for grayscale images. And so let's just convert
122:28 - this image to grayscale is activity don't
CVD color, pass the image and give it a color
122:34 - code of of color underscore BGR. To gray,
it means read this image with gray and passing
122:44 - Great. Now to actually compute the grayscale
histogram. What we need to do is essentially
122:54 - call this gray underscore hist and set this
equal to CV dot calc hist. This method will
123:02 - essentially compute the histogram for the
the image that we pass into. Now this images
123:09 - is a list, so we need to pass in a list of
images. Now since we're only interested in
123:15 - computing a histogram for one image, let's
just pass in the the grayscale image, there
123:21 - thing we have to pass in is the number of
channels which basically specify the index
123:27 - of the channel we want to compute a histogram
for that since we are computing the histogram
123:31 - for a grayscale image, let's wrap this as
a list and pass in zero. The next thing we
123:38 - have to do is provide a mask do we want to
compute a histogram for a specific portion
123:43 - of an image, we will get to this later. But
for now just have this to num. His size is
123:50 - basically the number of bins that we want
to use for computing the histogram. Essentially,
123:56 - when we plot a histogram, I'll talk about
this concept of bins. But essentially, for
124:01 - now, just set this to 256 wrapped as a list.
And that's wrapped out as list. And the next
124:07 - thing I want to do is specify the range of
the range of all possible pixel values. Now
124:12 - for our case, this will be 02256. And that's
it. So to prop this image, let's actually
124:20 - use matplotlib. So import map plot matplotlib.pi
plot as PLT, and then we can instantiate of
124:29 - PLT dot figure, a PLC figure. Let's give it
a tidy, let's call this gray kale histogram.
124:42 - We can essentially give it a label across
the x axis and we're going to call this bins.
124:52 - Let's give this a y label and set this equal
to the number of pixels. The number Have pixels.
125:03 - And that's why label. And finally, we can
plot by saying PLT dot plot the, the grayscale
125:13 - histogram. And Valley, we can essentially
give it a limit across the x axis. So PLT
125:20 - dot x Lim have a list of 02256. And finally,
we can display this image. So PLT dot show,
125:30 - save that and run Python histogram, dot p
y. And this is the distribution of pixels
125:37 - in this image. As you can see, the number
of bins across the x axis basically represent
125:43 - the the intervals of pixel intensities. So
as you can see that there is a peak at this
125:52 - region, this means that this is close to 5060
ish. So this means that in this image, there
126:00 - are close to 4000 pixels that have an intensity
of 60. And as you can see that there's a lot
126:08 - of, there's a lot of peeking in this region,
so between probably 40 to 70, there is a peak
126:15 - of pixel intensities of close to 3000 pixel
intensities in this image. So let's try this
126:20 - with a different image. Let's try this with
a cants. I'm just going to save that and run.
126:31 - And there is a peaking of pixel values in
between 202 25. And this makes sense because
126:40 - most of the image is white. So given that
reason, you can probably deduce that there
126:46 - will be a peak into words white or 255. Five.
So this is essentially computing the grayscale
126:52 - histogram for the entire image, what we can
do is we can essentially create a mask, and
127:00 - then compute the histogram only on that particular
mask. So let's do that. Let's go back to masking.
127:07 - Let's grab this, grab this. Let's go right
up there. I set this to image dot shape of
127:21 - the first two values the sizes of the same.
Let's essentially draw a mask, which will
127:30 - be CV dot circle of all blank. And we can
get the center of image into a shape of one
127:38 - by by divided by two, image doing shape of
zero divided by two over two, give it a radius
127:45 - of 100 pixels, give it a color of 245 give
it a thickness of negative one, we can display
127:50 - a mask let's call this let's call as mask
policy mask. And here's where things get interesting.
127:58 - We can get the grayscale histogram for this
mask. And the way we do that is by setting
128:04 - this mask parameter to mask two instead of
none. We set this to mask and let's see what
128:11 - that does to our histogram MPs and undefined
great. And I couldn't make this kind of made
128:27 - a mistake here. Oh, that's right. This is
the Masters not exactly the Masters is circle.
128:36 - This is a this will be a circle circle. And
essentially we need to mask out the image
128:46 - so we so the way we do that is by creating
a mask and setting this equal to CV dot Bitcoins.
128:52 - bitwise unscored, and we can pass in the grayscale
image the grayscale image, and we can pass
128:59 - in the mask which is equal to circle. Now
we can use that as the mask. So let's display
129:08 - that x Sorry, I made a mistake, but hopefully
things should be fine right now. So this is
129:18 - the mask and this is the histogram computed
for this particular mask. As you can see that
129:25 - there is a peaking of pixel intensity values
in this region. And there are smaller pickings
129:32 - in in these regions down below. Let's try
this with another image. Let's pass in the
129:39 - cats cats to the cats though jpg. This is
our mask and this is the there is a peaking
129:49 - in this image towards 50. Okay, so that was
it for computing grayscale histograms. Let's
129:58 - move on to true To compute a color histogram,
that is to compute a histogram for a color
130:05 - image to an RGB image. So let's call this
color histogram. And the way we do that is,
130:13 - instead of converting this image to grayscale,
let's comment all of this out. We will use
130:23 - a mask later. That's come in all of this out.
There is mask will be for IMG, IMG. And yeah,
130:36 - that's pretty much it. So let's start with
the color histogram. The way we do that is
130:41 - let's define a tuple of colors, and set this
equal to b, then tuple of G, a tuple element
130:51 - of R. And what I'm going to do next is I'm
going to say for our common call in enumerate
131:00 - of colors. What I'm going to do is I'm going
to say hist. So I'm going to plot the histogram
131:07 - by saying CV dot calc hist, we're going to
compute it over the image itself, the channels
131:14 - will be I mean, this eye over here, we're
going to provide a mask of none for now. Give
131:23 - it a his size of 256 and give it a ranges
of 02256. And then let's do a PLT dot plot
131:38 - hist and give it a color equal to call. And
only we can do a PLT dot x Lim of 02202256.
131:50 - And for this purpose, we can essentially grab
this, copy that uncomment this out. And we
132:03 - can do a PLT dot show. So this should work.
We're missing something Oh, no, don't think
132:11 - of him. We're not, we're not computing this
histogram for a mask, or we live there next.
132:17 - But let's save that run. Oh, cool. And let's
close enough, I made a mistake, this is a
132:26 - color histogram shouldn't make much of a difference.
So this is the color histogram that we get
132:34 - for the original image not for a mask. But
in fact, this image. So as you can see that
132:40 - this color image basically computed the plot
for blue channel, the red channel and the
132:47 - green channel as well. So using this, you
can basically make out that there is a peaking
132:52 - of blue pixels that have a pixel intensities
of 30. There's a peaking of red, probably
132:58 - around 50, peaking of green, probably around
8075 to 80. Cool and using this, you can basically
133:07 - make up the distribution of pixel intensities
of all three color channels. So let's try
133:13 - and apply a mask by setting this equal to
mask. Let's see whether we have everything
133:17 - in order. It's a bit more than mass mass,
mass mass mass masks. Masks are not the same
133:23 - size, okay, I finally got the error. So basically,
the mass needs to be a binary format. So instead
133:32 - of passing in this mask, this will actually
be the masked marks image, Regan passes me
133:38 - fat mask, and we can change the circle to
mask. Now this should work without any arrows.
133:52 - And we can change that to masked. Yeah, that's
around that. And now we get the color histogram
133:59 - for this particular mask, I made a mistake
because I use this as my mask to compute the
134:06 - histogram for one channel. The problem was
this masked image was actually a three channels
134:12 - and I attempted to use this s3 channeled mask
to calculate the histogram per channel, which
134:19 - isn't allowed in open CV. So that was my mistake.
What kind of use the wrong variable names
134:26 - so confused, but essentially, this is it,
you're computing the histogram for a particular
134:31 - section of this image. And this is what you
get there is a high peaking of red in this
134:37 - area, high peaking of blue in this era, and
high peaking of greens I'm over here. So essentially,
134:46 - that's it for this video. histograms actually
allow you to analyze the distribution of pixel
134:51 - intensities, whether for a grayscale image
or for a colored image. Now these are really
134:56 - helpful in a lot of advanced computer vision
projects. When you actually trying to analyze
135:01 - the image that you get, and maybe try to equalize
the image so that there's no peeking of pixel
135:08 - values here and there. In the next video,
we'll be talking about how to thresh hold
135:12 - an image and the different types of thresholding.
As always, if you have any questions, leave
135:17 - them in the comments below. Otherwise, I'll
see you guys in the next video. Hey, everyone,
135:24 - and welcome back to another video. In this
video, we're going to be talking about thresholding
135:29 - in open CV. Now, thresholding is a binary
realisation of an image. In general, we want
135:36 - to take an image and convert it to a binary
image that is an image where pixels are either
135:42 - zero or black, or 255, or white. Now, a very
simple example of thresholding would be to
135:49 - take an image and take some particular value
that we're going to call the thresholding
135:54 - value. And compare each pixel of the image
to this threshold of value. If that pixel
135:59 - intensity is less than the threshold value,
we set that pixel intensity to zero. And,
136:06 - and if it is above this threshold value, we
set it to 255, or white. So in this sense,
136:13 - we can essentially create a binary image just
from a regular standalone image. So in this
136:20 - video, we're actually going to talk about
two different types of thresholding, simple
136:24 - thresholding and adaptive thresholding. So
let's start off with simple thresholding.
136:32 - So in essence, what I want to do is, before
I talk about simple thresholding, is I want
136:37 - to convert this BGR image to grayscale. So
I'm going to say gray is equal to CV dot CVT
136:44 - color, we pass in the image, we pass in the
color code, which is vgr. To correct, we can
136:54 - display this image called this gray, we can
pass in great. Cool. So let's start off with
137:02 - the simple thresholding. So essentially to
to apply this this idea of simple thresholding,
137:09 - we essentially use the CV dot threshold function.
Now this function returns a threshold, and
137:15 - Thresh, which is equal to CV dot threshold.
And this in essence takes in the grayscale
137:24 - image, the grayscale image has to be passed
in to this thresholding function, then what
137:29 - we do is we pass in a threshold value. So
let's set this to 150 for now, and we have
137:35 - to specify something called a maximum value.
So if that pixel value is greater than is
137:41 - greater than 150, what do you want to set
it to, in this case, we want to binarize the
137:45 - image. So we set it to 245. And finally, we
can specify a thresholding type. Now this
137:53 - thresholding type is essentially CV dot thrush
underscore binary. And what this does is basically
138:02 - it looks at the image compares each pixel
value to this threshold value. And if it is
138:08 - above this value, it sets it to 255. Otherwise,
it infers that if it falls below, it sets
138:14 - it to zero. So essentially returns two things
trash, which is the thresholded image or the
138:21 - binarized image and threshold, which is essentially
the same value that you passed 150, the same
138:26 - threshold value you pass in, will be returned
to this threshold value. So let's actually
138:33 - display this image. So let's say cv.rm show,
we'll call this threshold. We'll call this
138:44 - simple thresh hold dead, and we can pass in
thrash. So let's save that and run Python
138:51 - thrash. Da p y in this is a thresholded image
that you get. Again, this is nothing too different
138:57 - from when we discussed thresholding in the
in one of the previous videos, but this is
139:01 - essentially what you get. So let's play around
with these threshold values. Let's set this
139:07 - to 100. And let's see what that does. And
as a result, both parts of the image have
139:13 - become white. So and of course, if you give
it a higher value, less parts of the image
139:19 - will be white. So let's set this to 225. And
very few pixels in this thresholded image
139:29 - actually have a pixel intensity of greater
than 225. So what we can do after this is
139:37 - essentially create an inverse thresholded
image. So what we could do is we could essentially
139:43 - copy this and instead of saying Thresh, I'm
going to say thrush underscore inverse, and
139:51 - I'm going to leave everything else the same.
Let's set this to 150. And the same thing
139:56 - here, and instead of passing in the type of
thresholding, I'm going to say CV dot Thresh
140:00 - underscore binary under scope inverse. And
let's call this thresholded inverse. And we
140:07 - can pass in inverse. So let's save that and
run. And this is essentially the inverse of
140:15 - this image, instead of setting pixel intensities
that are greater than 150 to 255, it sets
140:21 - whatever values that are less than 150, to
255. So that's essentially what you get. Right,
140:30 - all the black parts of this image will change
to white, and all the white parts of the image
140:36 - will change to black. Cool. So that's a simple
threshold. Let's move on now to adaptive threshold
140:45 - data thresholds. Now, as you can imagine,
we got different images, when we provided
140:55 - different threshold values. Now, kind of one
of the downsides to this is that we have to
141:01 - manually specify a specific threshold value.
Now, some cases this might work, in more advanced
141:08 - cases, this will not work. So one of the things
we could do is we could essentially let the
141:12 - computer find the optimal threshold value
by itself. And using that value that refines
141:18 - it binary rises over the image. So that's
an essence the entire crux of adaptive thresholding.
141:25 - So let's set up a variable called adaptive
on its growth Thresh. And set this equal to
141:31 - CV dot adaptive threshold. And inside I want
to pass in a source image. So let's set this
141:39 - to gray, I'm going to pass in a maximum value,
which is 255. Now notice there is no threshold
141:46 - value. adaption method basically tells machine
which method to use when computing the optimal
141:53 - threshold value. So for now, we're just going
to set this to the mean of some neighborhood
141:58 - of pixels. So let's set this to CV dot adaptive
on the scope Thresh. And score mean underscore
142:06 - C. Next, we'll set up a threshold type. This
is CV dot Thresh. underscore binary, which
142:14 - again, I think do different from this from
the first example. And two other parameters
142:21 - that I want to specify is the block size,
which is essentially the neighborhood size
142:25 - of the kernel size, which open CV needs to
use to essentially compute mean to find the
142:31 - optimal threshold value. So for now, let's
set this to 11. And finally, the last method
142:37 - we have to specify is the c value. Now this
c value is essentially an integer that is
142:43 - subtracted from the mean, allowing us to essentially
fine tune our threshold. So again, don't worry
142:51 - too much about this, you can set this to zero.
But for now, let's set this to three. And
142:57 - finally, once that's done, we can go ahead
and try to display this image. So let's call
143:02 - this adaptive thresholding. And we can pass
in adaptive cash. So let's save that and run.
143:13 - And this is essentially your adaptive thresholding
method. So essentially, what we've done is
143:18 - we've defined a kernel size or window that
is drawn of this image. In our case, this
143:23 - is 11 by 11. And so what open CV does is it
essentially computes a mean over those neighborhood
143:30 - pixels, and finds the optimal threshold value
for that specific part. And then it slides
143:35 - over to the right, and it slides, it does
the same thing. And it's lines down and does
143:40 - the same thing so that it essentially slides
over every part of the image. So that's how
143:46 - adaptive thresholding works. If you wanted
to fine tune this, we could change this to
143:50 - a threshold, just go binary and scope inverse,
you're just to see what's really going on
143:56 - under the hood. Cool. So all the white parts
of the image will change the black and all
144:00 - black parts of the image have changed white.
So let's play around with these values. Let's
144:05 - set this to probably 13 and see what that
does. Okay, definitely some difference from
144:13 - the previous hyper parameter. So let's try
it. Let's go with let's set this to 11. And
144:20 - let's set this to maybe one. Okay, definitely
more white. Let's set this to maybe five in
144:31 - a row that you can play around with these
values, right, the more you subtract from
144:36 - the mean, the more accurate it is, right,
you can basically make out the edges now in
144:41 - this basket. So let's maybe increase that
to nine. And you get less white spots in the
144:51 - image. But essentially, now you can make the
features better. Cool. So that was essentially
144:56 - adaptive thresholding, adaptive thresholding
that essentially can Did the optimal threshold
145:01 - value on the basis of the mean? Now we don't
have to stick with the mean, we can go with
145:06 - something else. So instead of mean, let's
set this to Gaussian. So let's save that and
145:13 - see what that does. And this is the thresholded
image using the Gaussian method. So the only
145:20 - difference that Gaussian applied was essentially
add a weight to each pixel value, and computed
145:25 - the mean across those pixels. So that's why
we were able to get a better image than when
145:30 - we use the mean. But essentially, the adaptive
thresholding mean works. In some cases, the
145:36 - Gaussian works in other cases, there's no
real one size fits all. So really play around
145:40 - with these values, see what you get. But that's
essentially all we have to discuss. For this
145:45 - video, we talked about two different types
of thresholding, simple thresholding and adaptive
145:51 - thresholding. In simple thresholding, we have
to manually specify a threshold value. And
145:56 - in adaptive thresholding, open CV does that
for us using a specific block size, or current
146:02 - size and other computing the threshold of
value on the basis of the mean, or on the
146:06 - basis of the Gaussian distribution. So in
the next video, the last video in the advanced
146:11 - section of this goes, we're going to be discussing
how to compute gradients and edges in an image.
146:18 - So if you have any questions, leave them in
the comments below. I'll be sure to check
146:22 - them out. Otherwise, I'll see you guys in
the next video. Thanks for watching, everyone,
146:29 - and welcome back to another video. In this
video, we're going to be talking about gradients
146:33 - and edge detection in urban CV. Now, you could
think of gradients as these edge like regions
146:39 - that are present in an image. Now, they're
not the same thing gradients and edges are
146:44 - completely different things from a mathematical
point of view. But you can pretty much get
146:49 - away with thinking of gradients as edges from
a programming perspective only. So essentially,
146:57 - in the previous videos, we've discussed the
canny edge detector, which is essentially
147:01 - kind of an advanced edge detection algorithm.
That is essentially a multi step process.
147:07 - But in this video, we're going to be talking
about two other ways to compute edges in an
147:11 - image. And that is the lat placing and the
Sobel method. So let's start off with the
147:17 - left place here. So the first thing I want
to do is I want to convert this image to grayscale,
147:24 - recalling the CVT. DVD to color color method,
we pass in the image, and we say CV color
147:32 - on describe BGR to grip, we can display this
image is called as gray. And we can pass in
147:39 - every pass. Great. So let's start with the
Laplacian. So we're going to define a variable
147:47 - called lap and set this equal to CV dot lap
lesion. And what this essentially will do
147:54 - is it will take in a source image, which is
great now, and it will take in something called
148:00 - a D depth or data depth. Now for now when
we set this to CV dot 64, F is for long with
148:07 - whatever I do next, I'm going to say lap is
equal to NP dot u 98. And instead I'm going
148:12 - to pass an NP dot absolute. And we can pass
in lap. And since I'm using NumPy, I can actually
148:21 - go ahead and import NumPy as NP and when I
go to display this image coil CV dot I'm sure
148:33 - method is called this lamp lesion. And we
can pass on lap lap Save and run a call this
148:42 - Python good radians dot p y invalid syntax
CV dot Okay, it's cv.cv on score 64 F. Say
148:56 - that. And this is essentially the law placing
edges in the image kind of looks like an image
149:03 - that is drawn over a chalkboard and then smudge
just a bit. But anyway, this is the lab laser
149:09 - method. Let's try this with another image.
Let's try this with this park called Boston.
149:18 - Let's call this the park. Save that in right.
And this essentially looks like a pencil shading
149:27 - off this image. It's all the edges that exists
in the image, or at least most of the edges
149:32 - in the image are essentially drawn over with
the pencil and then lightly submerged. So
149:37 - that's essentially the left lacing edges you
could say. So again, don't worry too much
149:46 - about why we converted this to in the UI and
then we computed the absolute value. But essentially
149:52 - the Laplacian method computes the gradients
of this image the grayscale image. Generally
149:58 - this involves a lot of mathematics but Essentially,
when you transition from black to white and
150:02 - white to black, that's considered a positive
and a negative slope. Now, images itself cannot
150:09 - have negative pixel values. So what we do
is we essentially compute the absolute value
150:15 - of that image. So all the pixel values of
the image are converted to the absolute values.
150:20 - And then we convert that to a UI 28 to an
image specific datatype. So that's basically
150:27 - the crux of what's going on right over here.
So let's move on to the next one. And that
150:33 - is the subtle gradient magnitude representation.
So essentially, the way this does is that
150:40 - Sobel computes the gradients in two directions,
the x and y. So we're gonna say sobble x,
150:47 - which is the gradients that are computed along
the x axis, and Seth is equal to CV dot Sobel.
150:55 - And we can pass in the image, let's add this
to the grayscale image, we pass in a data
150:59 - depth, which is cv.cv on school 64 F. And
we can give it an x direction. So let's set
151:08 - this to one and the y direction, we can set
that to zero. And let's copy this 
151:18 - and call it soble. Why, and instead of one,
zero, we can save zero comma one. And we can
151:26 - visualize this let's print. Let's call this
symbol x, and we can pass in sub x. And we
151:34 - can say it's either long show Sabo y and set
this to Sabo y. Call that and these are essentially
151:46 - the gradients that are computed, this is over
the y axis. So you can see a lot of y horizontal
151:53 - specific gradients and the sub x was computed
across the y axis. So you can see y axis specific
152:01 - gradients. Now we can essentially get the
combined Sobel image by essentially combining
152:08 - these two Sobel x and Sobel why, and the way
we do that is we're gonna say combined on
152:15 - combined underscore sobald and set this equal
to CV dot bitwise. on school or, and we can
152:24 - pass in Sabo x and symbol y. And we can display
this image, so let's call CV dot I'm show
152:34 - we get to combined Sobel and we can pass in
the combined symbol. Let's run that. And this
152:42 - is essentially the combined sobble that you
get. It isn't, let's go back here. So it essentially
152:50 - took these two apply and CV dot bitwise OR,
and essentially got this image. So if you
152:57 - want to compare this with lat race in two
completely different algorithms, so the results
153:02 - you get will be completely different. Okay,
so let's compare both of these left patient
153:09 - and the Sobel with the canny edge detector.
So let's go down here. Let's say Kenny is
153:15 - equal to CV, don't, Kenny. And we can pass
in the image. So let's possible a grayscale
153:22 - image. Let's give it to threshold values of
150 and 175. And we're done. Let's display
153:30 - this image. Let's call this Kenny, we can
pass in Kenny. So let's save that. And let's
153:37 - see what that gives us. So let's compare that
with you. So that's essentially it. This is
153:46 - the last place in gradient representation,
which essentially returns kind of this pencil
153:51 - shading version of the image of the edges
in the image, combined several computes the
153:57 - gradients in the X in the y direction. And
we can combine these two with bitwise OR,
154:03 - and Kenny is basically a more advanced algorithm
that actually uses Sobel in one of its stages.
154:10 - Like I mentioned, Kenny is a multi stage process,
and one of its stages is using the symbol
154:17 - method to compute the gradients of the image.
So essentially, you see that the canny edge
154:24 - detector is a more cleaner version of the
edges that can be found in the image. So that's
154:29 - why in most cases, you're going to see the
Kenny used. But in more advanced cases, you're
154:34 - probably going to see a Sobel use a lot. Not
necessarily lap racing. But so definitely.
154:42 - So that's pretty much it for this video. And
in fact, this video concludes the advanced
154:47 - section of this course. Moving on to the next
section, we will be discussing face detection
154:53 - and face recognition in urban see, we're actually
going to touch on using hard cascades To perform
155:01 - some face detection, and face recognition,
we actually have two parts. Face Recognition
155:07 - with open CV is built in face recognizer.
And the second part will be actually building
155:12 - our own deep learning model to essentially
recognize some faces in an image. Again, like
155:19 - always, if you have any questions, leave them
in the comments below. Otherwise, I'll see
155:23 - you guys in the next section. Hey, everyone,
and welcome back to another video. We are
155:31 - now with the last part of this Python and
open CV coasts, where we are going to talk
155:35 - about face detection and face recognition
in open CV. So what we're going to be doing
155:41 - in this video is actually discussing how to
detect faces in urban CV using something called
155:47 - a har cascade. In the next video, we will
talk about how to recognize faces using open
155:54 - CV is built in face recognizer. And after
that, we will be implementing our own deep
155:59 - learning model to recognize during the simpson
counters, we're going to create that from
156:04 - scratch and use open CV for all the pre processing
and displaying of images and stuff like that.
156:11 - So let's get into this video. Now, face detection
is different from face recognition. Face Detection
156:17 - merely detects the presence of a face in an
image, while face recognition involves identifying
156:24 - whose face it is. Now, we'll talk more about
this later on in this course. But essentially,
156:31 - face detection is performed using classifiers.
A classifier is essentially an algorithm that
156:38 - decides whether a given image is positive
or negative, whether a face is present or
156:45 - not. Now classify needs to be trained on 1000s
and 10s, of 1000s of images with and without
156:51 - faces. But fortunately for us, open CV already
comes with a lot of pre trained classifiers
156:58 - that we can use in any program. So essentially,
the two main classifiers that exist today
157:05 - are har cascades, and mo advanced classifiers
core local binary patterns, we're not going
157:12 - to talk about local binary patterns at all
in this course. But essentially the most advanced
157:17 - how cascade classifiers, they're not as prone
to noise in an image as compared to the hard
157:23 - cascades. So I'm currently at the open CVS
GitHub page where they store their whole cascade,
157:32 - there are cascade classifiers. And as you
can see, there are plenty of hard cascades
157:38 - that open CV makes available to the general
public. You have a hard cascade for an eye,
157:44 - fragile cat face, from face default, full
body, your left eye, a Russian license plate,
157:53 - Russian plate number, I think that's the same
thing. How cascade to detect smile, Hawk cascade
158:00 - for detection of the upper body, and things
like that. So feel free to use whatever you
158:08 - want. But in this video, we're going to be
performing face detection. And for this, we're
158:12 - going to use the har cascade underscore frontal
face underscore default dot XML. So when you
158:18 - go ahead and open that, you're going to get
about 33,000 lines of XML code. So all of
158:25 - this. So what do you have to do is essentially,
go to this role button, and you'll get all
158:33 - this raw XML code, all you have to do is click
Ctrl A, or Command D if you're on a Mac, and
158:39 - click Ctrl C, or Command C, and then go to
your VS code or your editor and create a new
158:46 - file. And we're going to call this har unscrew
face dot XML. And inside this, I want to paste
158:53 - in those 33,000 lines of XML code. Go ahead
and save that and our classifier is ready.
159:00 - So we can go ahead and close this out. So
we're going to be using this Hawk cascade
159:07 - classifier to essentially detect faces that
are present in an image. So in this file called
159:14 - face detect, face underscore detected py,
I inputted open CV, I basically read in an
159:20 - image of Lady a person, that is this image
over here. And we can go real quick and display
159:28 - this. So let's run Python face to face on
disco with detect dot p y, and we get an image
159:36 - in a new window. Cool. So let's actually implement
our code. The first thing I want to do is
159:42 - convert this image to grayscale. Now face
detection does not involve skin tone or the
159:48 - colors that are present in the image. These
hard cascades essentially look at an object
159:54 - in an image and using the edges tries to determine
whether it's a face or not. So We really don't
160:00 - need color in our image. And we can go ahead
and convert that to grayscale, TV dot CVT
160:06 - color, passing the image in CV dot color on
BGR. To gray. And we can display this call
160:14 - this gray of color is gray person, we can
pass in our name. Let's save them and run.
160:24 - And we have to pass in the gray. Okay, we
have a blu ray person over here. So let's
160:34 - move on to essentially reading in this har
underscore face dot XML file. So the way we
160:40 - do that is by essentially create a har cascade
variable. So let's set this to her underscore
160:46 - cascade. And we're going to set this equal
to CV dot cascade classifier, in inside, what
160:54 - I essentially want to do is, is parsing the
path to this har to this XML file. That is
161:02 - as simple as saying har en disco face dot
XML. So this cascade classifier class will
161:11 - essentially read in those 33,000 lines of
XML code and store that in a variable called
161:17 - har underscore cascade. So now that we've
read in all har cascade file, let's actually
161:23 - try to detect the face in this image over
here. So what I'm going to do is essentially,
161:28 - say faces on school rect is equal to har underscore
cascade dot detect multi scale, and instead,
161:37 - we're going to pass in the image that we want
to detect based on. So this is great, we're
161:43 - going to pass in a scale factor. Now let's
set this to 1.1. Give it a variable called
161:49 - minimum neighbors, which essentially is a
parameter that specifies the number of neighbors
161:54 - rectangle should have to be called a face.
So let's set this to three for nap. So that's
162:02 - it. That's all we have to do. And essentially,
what this does is this detect multiscale,
162:08 - an instance of the cascade classifier class
will essentially take this image, use these
162:13 - variables called scale factor and minimum
labels to essentially detect a face and return
162:19 - essentially the rectangular coordinates of
that face as a list to faces on the score
162:25 - rec. That's exactly why we are giving it faces
on scope rect rect, to rectangle. So you can
162:33 - essentially print the number of faces that
were found in this image by essentially printing
162:37 - the length of these faces on the score rect
variable. So let's do that. Let's print the
162:45 - number, number of faces found is equal to,
we can pass in the length of faces on school
162:55 - rect. So let's save that and run. And as you
can see that the number of faces that were
163:00 - found one, and that's true, because there's
only one person in this image code. Now utilizing
163:09 - the fact that this faces on school rec is
essentially the rectangular coordinates for
163:14 - the faces that are present in the image, what
we can do is we can essentially looping over
163:19 - this list and essentially grab the coordinates
of those images and draw a rectangle over
163:25 - the detected faces. So let's do that. So the
way we do that is by saying for x comma y
163:33 - comma w comma H, H in faces underscore rect,
what we're going to do is we're going to draw
163:41 - a rectangle CV to a rectangle over the original
image. So IMG give the point one, this point
163:51 - one is essentially x comma y. And point two
is essentially x plus w comma y plus H. Let's
164:01 - give it a color. Let's set this to green.
So zero comma 255 comma zero, give it a thickness
164:08 - of two. And that's it. And we can print this
or we can display this image. So let's set
164:16 - this to detected basis. And we can pass in
OMG. And if you look at this image, you can
164:28 - essentially see the rectangle that was drawn
over this image. So this in essence, is the
164:35 - face that open CV is hard cascades found in
this image. So let's try this with another
164:42 - image. So what I have here are a couple of
people, a couple of other people then image
164:47 - of five people, so we're going to use that
image and try to see how many faces OBG these
164:53 - hard cascades could detect in this image.
So let's set this to group two. We can change
165:00 - that to a group of five people. Save that
close, right people save and run. And I want
165:11 - to point real quick that the number of faces
that we found, were actually seven. Now we
165:16 - know that there are five people in this image.
So let's actually see what open CV thought
165:21 - was face. So we can go real quick. So actually
detected all the faces in this image, all
165:28 - the five people, but it also detected two
other guests a stomach and part of a neck.
165:34 - Now this is to be expected because her cascades
are really sensitive to noise in an image.
165:40 - So if you have something that pretty much
looks like a face, like a neck looks like
165:46 - a face, it has the same structure as the typical
face would have. I don't know why her stomach
165:53 - was recognized as face. But again, this is
to be expected. So one way we can try to minimize
166:02 - the sensitivity to noise is essentially modifying
the scale factor in minimum neighbors. So
166:09 - let's increase the minimum neighbors to maybe
six or seven. Save that in run. In as you
166:17 - can see, now six faces were found. So I guess
by increasing the minimum neighbors parameter,
166:23 - we essentially stopped open open CV from detecting
her stomach as face. So let's try this with
166:32 - another more complex image, a couple of people
in group one. So if I change that to group
166:39 - one, save rock. Now, as you can see that the
number of faces we've never found was six.
166:47 - And we know that this is not six. So let's
actually change this minimum minimum neighbors
166:53 - just a bit. Let's change this first to three
and see how many faces we'll found. Now we
167:00 - got 14. Okay, some people at the back want
chosen because either the faces are not perfectly
167:07 - perpendicular to the camera, or they're wearing
some accessories on the face, for example,
167:13 - eyeglasses. This dude's wearing a hat, this
dude ran on cap, and stuff like that. So let's
167:19 - actually change this to one. And let's see
what that gets us to one. So, Ron, and now
167:27 - we got 19 faces that were found in this image.
So it's about looping through these values
167:34 - by changing these values. by tweaking these
values, you can essentially get a more robust
167:39 - result. But of course by by minimizing these
values, you're essentially making open CV
167:44 - small cascades more prone to noise. That's
the trade off you need to consider. Now, again,
167:50 - hard cascades are not the most effective in
detecting faces, they're popular, but they're
167:56 - not the most advanced, they are probably not
what you would use if you were to build more
168:02 - advanced computer vision projects. I think
for that, dealings face recognizer is more
168:09 - effective and less sensitive to noise than
open CV is our cascades. It stands for your
168:14 - use case hard cascades are most more popular.
They're easy to use, and they require minimal
168:21 - setup. And if you wanted to extend this to
videos, you could all you have to do is essentially
168:28 - detect hot cascades on each individual frame
of a video. Now I'm skipping that because
168:35 - it's pretty self explanatory. So that's pretty
much it. For this video, we discussed how
168:41 - to detect faces in open CV using open CV as
har cascades. In the next video, we will actually
168:48 - talk about how to recognize faces in open
CV using open CV is built in face recognizer.
168:56 - So like always, if you have any questions,
comments, concerns, whatever, leave them in
169:00 - the comments below. Otherwise, I'll see you
in the next video. Hey everyone, and welcome
169:08 - back to another video. In this video, we will
learn how to build a face recognition model
169:13 - in open CV using open CV is built in face
recognizer. Now, on the previous video, we
169:18 - dealt with detecting faces in open CV using
hard cascades. This video will actually cover
169:25 - how to recognize faces in an image. So what
I have you have five folders or five different
169:33 - people. Inside each folder, I have about 20
images of that particular person. So Jerry
169:39 - has 21 images. Anson has 17 Mindy kailyn has
22 Ben Affleck has 14 and so. So what I'm
169:53 - essentially going to do is we're going to
use open CV is built in face recognizer. And
169:58 - we're going to train that right now. So on
all of these images in these five folders,
170:04 - now this is sort of like building a mini sized,
deep learning model, except that we're not
170:09 - going to build any model from scratch, we're
going to use open TVs built in face recognizer,
170:15 - or we're going to do is we're actually going
to pass in these close 90 images. And we're
170:20 - going to train that recognizer on these 90
images. So let's create a new file. And we're
170:27 - going to call this faces ns, go train dog
p y, we're going to input always, we're going
170:34 - to input CV to our CV, and we're going to
import NumPy as NP. So the first thing I want
170:42 - to do is essentially create a list of all
the people in the image. So this is essentially
170:48 - the names of the folders of these particular
people, what you could do is you can manually
170:53 - type those in, or you could essentially create
an empty list. Let's call this P. and we can
171:00 - loop over every folder in this folder, and
let's set this to an Austrian. And we can
171:13 - say P dot append, I, or we can print P. Let's
save that and run Python beaters on skirt
171:24 - on skirt trained on p y. And we get the same
list that we got over here. So that's one
171:31 - way of doing it. And what I'm going to do
next is I'm essentially going to create a
171:37 - variable called dir, and set this equal to
this base folder, that is this folder which
171:43 - has, which contains these five folders of
these people. Cool. So with that done, what
171:52 - we can do is we can essentially create a function
called def create unscrewed train, that will
171:59 - essentially loop over every folder in this
base folder. And inside that folder, it's
172:06 - going to loop over every image and essentially
grab the face in that image and essentially
172:12 - add that to our training set. So our training
set will consist of two lives. The first one
172:18 - is called features, which are essentially
the image arrays of faces. So let's set this
172:24 - to an empty list. And the second list will
be our corresponding labels. So for every
172:29 - face in this features list, what is its corresponding
label, whose face does it belong to, like
172:36 - one image could belong to Ben Affleck, the
second image could belong to elton john, and
172:41 - so on. So let's create a function. So we're
going to say we're going to loop over every
172:46 - person in this people list, we're going to
grab the path for this person, so for every
172:52 - folder in this base folder, going through
each folder and grabbing the path to that
172:58 - folder. So that's essentially as simple as
saying, Oh s dot path dot join the join. And
173:05 - we can, we can join the der with person. And
what I'm going to do is I'm gonna create a
173:13 - labels label variable, and set this equal
to people don't index of person. And now the
173:25 - way inside each folder, we're going to loop
over every image in that folder. So we're
173:32 - going to say for image to image in our stock
list there. In path, we are going to grab
173:41 - the image Park. So we're going to say image,
underscore path is equal to OS dot path dot
173:48 - join. We're going to say join, we're going
to join the PATH variable to the image. Now
173:57 - that we have the path to an image, we're going
to read in that image from this path. So we're
174:03 - going to create a variable called IMG underscore
rain is equal to CV dot m read image on the
174:10 - scope path. We're going to convert this image
to grayscale I think CVT color, pause and
174:17 - IMG. On scope right here we can pass in t
v dot c, CV dot color on the screw BGR to
174:26 - grip. Cool and now now with that done we can
essentially trying to detect the faces in
174:34 - this image. So let's go back to face underscore
detect and grab the whole cascade classifier
174:42 - variable here. Let's paste that there. And
we can create a set of faces on school rect
174:52 - and set this equal to har underscore cascade
dot detect multi scale this will take in the
174:59 - gray image scale factor of 1.1 and add a minimum
neighbors of four. And we can loop over every
175:08 - every face in this face rect. So for for x
comma y comma w comma each in faces rect,
175:16 - we are going to grab the bases region of interest,
and set this equal to and basically crop out
175:24 - the face in the image. So we're going to say
gray, y two y plus h, and x 2x plus W. And
175:36 - now that we have a faces a face region of
interest, we can append that to the features
175:42 - list. And we can append the corresponding
label to the labels list. So we're going to
175:47 - do features dot append, we're going to pass
in faces on scope, or y. And we can do a labels
175:55 - dot append label. This label variable is essentially
the index of this list. Now the idea behind
176:07 - converting a label to numerical values is
essentially reducing the strain that your
176:12 - computer will have, by creating some sort
of mapping between a string and the numerical
176:19 - label. Now the mapping of we are going to
do is essentially the index of that particular
176:24 - list. So let's say that I grab the first image,
which is an image of Ben Affleck, the label
176:32 - for that would be zero, because Ben Affleck
is at the zeroeth index of this people list.
176:39 - Similarly, elton john, an image of elton john
would have a label of one because it is at
176:44 - the second position or the first index in
this people's lists. So that's essentially
176:50 - the idea behind this. Now, with that done,
we can essentially trying to run this and
176:57 - see whether we got any errors or not. And
we can bring the length of the features. So
177:03 - let's say length, length of the features list,
is equal to the length of features. And we
177:17 - can do the same thing. This was copy this
length of the labels list, set this to length
177:27 - of labels. So that shouldn't give us any error.
So let's run that. And we get the length of
177:36 - the features 100 and length of labels 100.
So essentially, what we have 100 faces, and
177:42 - 100 corresponding labels to this faces. So
we don't need this anymore. What we can do
177:49 - is we can essentially use this features and
labels list now that it's appended to train
177:55 - our recognizer on it. So the way we do that
is we instantiate our face recognizer, call
178:03 - this, as the instance of the cv.face.lb p
h face recognizer underscore create class.
178:13 - And this will essentially instantiate the
face right now. Now we can actually train
178:19 - the recognizer on, on the features list, and
the labels and the labels list. So the way
178:32 - we do that is by saying face underscore recognizer
dot train, and we can pass in the features
178:39 - list, and we can pass in the labels list.
And before we actually do that, I do want
178:45 - to convert this features and labels list to
NumPy arrays. So we're going to do so we're
178:52 - going to say, features is equal to NP dot
array of features. And we can say of labels
179:01 - is equal to NP dot array of labels, and save
that and run. OK, data object. So let's add
179:10 - this to the typed object. Horse in detail
type is equal to object. And we can actually
179:23 - print when this is done, so let's say craning
don't. And we can actually go ahead and save
179:31 - this features and labels list. And we're going
to say NP dot save, we're going to call this
179:39 - features.np y, and we can pass in features.
And we can do NP dots, MP dot save labels
179:47 - dot nPy. And we can pass in the labels. So
let's save that and run cool. So essentially,
179:56 - now the face recognizer is trained and we
You can now use this. But the problem here
180:02 - is that if we plan to use this face recognizer
in another file, we'll have to separately
180:09 - and manually repeat this process, this whole
process of adding those images to a list and
180:15 - getting the corresponding labels, and then
converting that to NumPy rays, and then training
180:20 - all over again, what we can do and what open
CV allows us to do is essentially save this
180:28 - trained model so that we can use it in another
file in another directory in another part
180:33 - of the world just by using that particular
YAML source file. So we're going to repeat
180:39 - this process again. But the only change that
I'm going to do is I'm going to say face recognizer
180:45 - dot save. And we're going to give the path
to a YAML source file. So we're going to save
180:52 - face unscrewed trend, dot yamo. So let's repeat
this process, again, trainings down. And now
181:02 - you'll notice that you have face on scope
trained a yamo file in this directory, as
181:08 - well as faces, as well as features that nPy
annal labels dot nPy. So let's actually use
181:16 - this train model to recognize faces in an
image. So let's close this out. And create
181:25 - a new file. And we're going to call this face
on school rec hig nition dot p y. Very simply,
181:33 - we're going to import NumPy as MP and CV to
sc V, we don't need us anymore, because we're
181:42 - not looping over directories, we can essentially
create our har underscore cascade file. So
181:48 - let's do that. Let's go up here, grab this,
we can load our features and label rate using
182:00 - by saying features is equal to NP note load
features.np y. And we can say labels is equal
182:11 - to NP dot load with called as labels.np y.
And we can essentially now read in this face
182:20 - on the scope train that yamo file. So let's
go over here. Let's grab this line. And let's
182:30 - say face recognizer dot read. And we're going
to give it the path to this YAML source file.
182:36 - So face unscrewed face on screw trained dot
yamo. So that's pretty much all we need. Now
182:44 - we need to get the mapping. So let's grab
this list as well. And so that's pretty much
182:54 - all we have to do. So let's create a variable
image that set this to save it out in read,
183:01 - give it a path. Let's create eight. Let's
grab one from this validation and one from
183:12 - this validation. I have one of Ben Affleck.
So let's try this with grab that piece on.
183:22 - And Graham, maybe this image from but I have
a piece out there. And that's a JPG file.
183:33 - And we can convert that image to grayscale
tv.tv t color positive image CV no color,
183:40 - I'm just going to BG BGR to Great. So let's
just play this image. See, Cole is the person
183:48 - on identified person that's patient on the
board. So what we're going to do is we're
183:54 - going to first detect the face in the image.
So the way we do that is by saying faces on
184:02 - underscore rect is equal to r on this go cascade
dot detect multiscale we pass in the gray
184:10 - image, we pass in the scale factor, which
is 1.1 give it a minimum neighbors of foe
184:17 - and we can loop over every face in his faces
on score rect Sue, Sue for x comma y comma
184:25 - w comma H in faces basis on score rect. We
can grab the region of interest to what we're
184:35 - interested in finding your Why two one plus
H and x 2x plus H. And now we can predict
184:49 - using this face recognizer to we get the label
and a confidence value. And we say face recognizer
184:58 - dot predict And we predict on this faces on
scope ROI, let's print. Let's call this label
185:08 - is equal to label with confidence, off color
confidence. And since we're using numerical
185:22 - values, we can probably we can probably say
people off label. Okay. And we can essentially
185:32 - what we can do is we can put some text on
this image just to show us what's really going
185:38 - on, we can put this on the image, we can create
a string, variable of people of label. So
185:48 - the person involved in that image, given an
origin, let's say 10, let's say 20 by 20.
185:59 - Give it a font face of CV dot font, unscrew
Hershey on school complex. Give it a font
186:09 - scale of one point of 1.0 here to color of
zero, comma 255 comma zero and give it a thickness
186:17 - of two. And we can draw a rectangle over the
image over the face. This is we draw this
186:26 - over the image, we give it x to y and x plus
delta t comma y plus H. We give it a color
186:36 - of zero comma two five comma zero, and we
can give it a thickness of two. So with that
186:45 - done, we can find this display this image
called as the detected bass. And we can pass
186:52 - the image. And finally we can do a CV Delta
Wait, key zero. So let's save and see what
187:01 - we get Python. Python face on school record,
Nish. nation dot p y. Cannot be alone in love
187:12 - pickles equals false. Gosh, where's that?
We probably don't need this anymore. So let's
187:20 - come up with that out. there if you wanted
to use these again, you could essentially
187:27 - use MP dot load. Since the data types are
objects, you can basically say allow pickle
187:33 - is equal to true. That's essentially but we're
not going to use it. So let's comment that
187:37 - out. Save. And okay, we get Ben Affleck with
a confidence of 60%. So that's pretty good.
187:52 - 60% is good, given the fact that we only train
this recognizer on 100 images. So let's try
188:00 - this with another image of Ben Affleck, maybe
this image, copy that go right across here.
188:11 - And this again, is Ben Affleck with the confidence
of 94% pretty good. Let's go back. Let's go
188:23 - maybe to an odd person. Let's go to Madonna.
For to grab this. It's a pain rally. But let's
188:34 - change this to Madonna. And let's grab this
person, I'm not sure whether it will detect
188:44 - a face because of the head. But let's face
that anyway. Now this is where you'll find
188:53 - that obon TV's face recognizer built on face
recognizer is not the best. It currently detects
189:00 - it currently detects that this person in the
image is actually Jerry seinfield. And that
189:05 - will be the confidence of 110%. Maybe there's
an error somewhere. I'm not sure why that
189:10 - went to 111. But pretty sure there's an error
somewhere. But essentially, this is where
189:18 - the discrepancies lie. It's not the best so
it's not going to give you accurate results.
189:24 - So let's try this with another image. Let's
go about to maybe share image, copy that piece
189:33 - of paper. Okay, this is Madonna with the confidence
of 96.8% Okay, let's move on to elton john
189:46 - Watson had problems with elton john. Given
the fact that he looked pretty similar to
189:53 - Ben Affleck for some reason. Copy that chain
got to elton john just called john and print
190:03 - that. Okay, elton john with the confidence
of 67% pretty good. Okay, so not bad. This
190:14 - is more accurate than what I predicted. before
filming this video, I did a couple of trial
190:18 - runs, and I got very good results. For example,
elton john was continually detected as Jerry
190:26 - seinfield or Ben Affleck. Madonna was detected
as Ben Affleck, Ben Affleck was detected as
190:33 - Mindy kaylin. Minnie kailyn was detected as
elton john, and a whole bunch of weird results.
190:41 - So I guess that we did something right. I
must have done something wrong in the trial
190:46 - runs. But hey, we get good results. And that's
pretty good. Now, I'm not sure why that gave
190:52 - a confidence of 111%. Maybe there's an error
somewhere with the training sent. But I guess
191:00 - for the most part, you can ignore that. Given
the fact that we get pretty good results.
191:09 - So that's pretty much it. For this video,
we discussed face recognition. In open CV,
191:16 - we essentially build a features list and a
labels list and we train a recognizer on those
191:21 - two lists. And we saved a model as a YAML
source file. In another file, we essentially
191:27 - read in that saved model saved YAML source
file. And we essentially make predictions
191:33 - on an image. And so in the next video, which
will actually be the last video In this course,
191:40 - we will discuss how to build a deep learning
model to detect and classify between 10 Simson
191:47 - characters. So if you have any questions,
comments, concerns, whatever, leave them in
191:52 - the comments below. Otherwise, I'll see you
in the next video. Hey, everyone, and welcome
192:00 - to the last video in this Python and urban
TV cuts. Previously, we've seen how to detect
192:05 - and recognize faces Pioli in open CV, and
the results we got were varied. Now, there
192:11 - are a couple of reasons for that. One is the
fact that we only had 100 images to train
192:16 - the recognizer on. Now, this is a significantly
small number, especially when you're training
192:22 - recognizes and building models. Ideally, you'd
want to have at least a couple of 1000 images
192:27 - per class. The second reason lies in the fact
that we want using a deep learning model.
192:33 - Now as you go deeper into especially computer
vision, you will see that there are very few
192:37 - things that can actually beat a deep learning
model. So that's what we're going to be doing
192:41 - in this video. Building a deep computer vision
model to classify between the sensing characters
192:47 - now generate open CV GS for pre processing
the data that is performing some sort of image
192:53 - normalization, mean subtraction, and things
like that. But in this video, we're going
192:57 - to be building a very simple model. So we're
not going to be using any of those techniques.
193:02 - In fact, we'll only be using the open CV library
to read an image and resize them to a particular
193:08 - size before feeding it into the network. Now,
don't worry if you've never used a built a
193:14 - deep learning model before. But this video
will be using tensor flows implementation
193:18 - of Kara's now I want to keep this video real
simple, just so you have an idea of what really
193:23 - goes on in more advanced computer vision projects.
And carers actually comes with a lot of boilerplate
193:29 - code. So if you've never built a deep learning
model before, don't worry, Cara's will handle
193:34 - that for you. So kind of one of the prerequisites
to building a deep learning model is actually
193:39 - having a GPU. Now GPU is basically a graphical
processing unit that will help speed up the
193:46 - training process of a network. But if you
don't have one, again, don't worry, because
193:51 - we'll be using candle, a platform, which actually
offers free GPUs for us to use. So real simple,
193:58 - before we get started, we need a couple of
packages installed. So if you haven't already
194:03 - installed Sierra at the beginning of this
course, go ahead and do a pip install Sierra.
194:09 - The next package you require is conero. And
this is a package that I built specifically
194:15 - for deep learning models built with Kerris.
And this will actually appear surprisingly
194:19 - useful to you, if you're planning to go deeper
into building deep computer vision models.
194:24 - Now, installing this package on your system
will only make sense if you already have a
194:28 - GPU on your machine. If you don't, then you
can basically skip this part. So we can do
194:35 - a pip install conero. And can our actually
installs TensorFlow by default, so just keep
194:45 - that in mind. So with all the installations
out of the way, let's actually move on to
194:50 - the data that we're going to be using. So
the data set that we're going to be using
194:54 - is the Simpsons character data set that's
available on kaggle. So the So the actual
195:01 - data that we're interested in lies in this
instance on score data set folder. This basically
195:06 - consists of a number of folders with several
images inside each subfolder. So Maggie Simpson
195:13 - has about 12 128 images. Homer Simpson has
about 2200 images. Abraham has about 913 images.
195:24 - So essentially, what we're going to do is
we are going to use these images and feed
195:30 - them into our model to essentially classify
between these characters. So first thing we
195:36 - want to do is go to kaggle.com slash notebooks,
go ahead and create a new notebook. And under
195:44 - Advanced Settings, make sure that the GPU
is selected, since we're going to be using
195:48 - the GPU off of that click Create. And we should
get a notebook. So we're going to rename this
195:57 - to Simpsons. And one thing I want to do is
enable the internet since we're going to be
196:03 - installing a couple of packages over the internet.
So do use the Simpsons character data set
196:09 - in our notebook, you need to go head to add
data search for Simpsons. And the first one
196:16 - by Alec city, I should pop up, go ahead and
click Add. And we can now use this data set
196:23 - inside a notebook. So the first thing I want
to do is we're going to pip install, seer.
196:30 - And now, now the reason why I'm doing this
yet again. Now the reason why I'm doing this,
196:37 - again, is because candle does not come pre
installed with Sierra and conero. Now I did
196:42 - tell her to install it on your machine. And
the reason for that is because y'all can work
196:47 - with it and experiment with. So once that's
done, go ahead to a new cell. And let's import
196:54 - all the packages that we're going to need.
So we're going to input o s, we're going to
196:58 - input seer, we're going to input conero. We're
going to import NumPy. As NP we're going to
197:07 - input CV to add CV, and we're going to input
GC for garbage collection. Then next what
197:13 - we want to do is in basically when building
deep computer vision models, your model expects
197:20 - all your data or your image data to be of
the same size. So since we're working with
197:25 - image data, this size is the image size. So
all the data or the images in our data set
197:32 - will actually have to be resized to a particular
science before we can actually feed that into
197:38 - the network. Now with a lot of experiments,
I found that an image size of 80 by 80 works
197:44 - well, especially for this Simpsons data set.
Okay, the next variable we need is the channels.
197:53 - So how many channels do we want in our image.
And since we do not require color in our image,
197:59 - we're going to set this to one basically grayscale.
To run back. What we need next is we're gonna
198:05 - say car on the scope path is equal to the
base path where all the data where all the
198:13 - actual data lines, and that is in this Simpsons
on a school dataset, this is the base folder
198:19 - for where all our images are stored in. So
we're going to copy this file path. And we're
198:24 - going to paste that in that. Cool. So essentially,
what we're going to be doing now is, we're
198:32 - essentially going to grab the top 10 characters,
which have the most number of images for that
198:37 - class. And the way we're going to do that
is we are going to go through every folder
198:42 - inside the Simpsons underscore data set, get
the number of images that are stored in that
198:46 - data set, store all of that information inside
a dictionary, so that dictionary in descending
198:52 - order, and then grab the first 10 elements,
first n elements in the dictionary, hope that
199:01 - made sense. So what we're going to do is we're
going to say create an empty dictionary. We're
199:08 - going to say for character in our stop list,
der called car path, we are going to say car
199:16 - underscore dict of car is equal to length
of s dot list dir of Oh s dot path dot join.
199:25 - We're going to join the car on a scope pump
with car. So essentially, all that we're doing
199:31 - is we're going through every folder or grabbing
the name of the folder, and we're getting
199:35 - the number of images in that folder. And we're
storing all that information inside the dictionary
199:41 - called car underscore dict. Once that's done,
we can actually sort this dictionary in descending
199:47 - order. Sending order and the way we do that
is with a car unscored dict is equal to car
199:58 - dot SOT unscored dict of car underscore dict.
And we said descending equals to true. And
200:07 - finally, we can print the dictionary that
we get. So this is the dictionary that we
200:15 - have. As you can see, Homer Simpson has the
most number of images at close to 2300. And
200:20 - we go all the way down to Lionel, who has
only three images in the data. So what we're
200:26 - going to do is now that we have this dictionary,
what we're going to do is we are going to
200:30 - grab the names of the first 10 elements in
this dictionary, and store that in a list
200:35 - of characters list. So we're gonna say characters.
So we're gonna say characters is equal to
200:43 - is equal to an empty list. And we're going
to say, for i in car underscore dict. We're
200:50 - going to say characters, dot append, and we're
going to append the name. So we say I have
200:58 - zero. And we say, if count is greater than
or equal to 10, we can break it, we need to
201:05 - specify a count of zero, and increment that
counts. Okay, once that's done, let's print
201:16 - what our characters looks like. So we've essentially
just grabbed the names of the characters.
201:24 - So with that done, we can actually go ahead
and create the training data. And to create
201:32 - a training data is as simple as saying train
is equal to seer dot pre process. From there,
201:41 - we pass in the car on scope, puff, the characters,
the number of channels, the image size, image
201:54 - size, as we say, is shuffle equals true. So
essentially, what this will do is it will
202:03 - go through every folder inside car on the
scope path, which is Simpsons underscore data
202:08 - set. And we'll look at every element inside
characters. So essentially, it is going to
202:13 - look for Homer Simpson, inside the Simpsons
underscore data set, it will find Homer Simpson,
202:19 - whereas Homer Simpson, it even finds Homer
Simpson is going to go through inside that
202:25 - folder, and grab all the images inside that
folder, and essentially add them to our training
202:31 - set. Now, as you may recall, in the previous
video, a training set was essentially a list.
202:38 - Each element in that list was another list
of the imagery and the corresponding label.
202:45 - Now the label that we had was basically the
index of that particular string in the characters
202:50 - list. So that's essentially the same type
of mapping that we're going to use. So Homer
202:55 - Simpson is going to have a label of zero,
Ned will have label of one, Liza will have
203:01 - label of three, and so on. So once that's
done, go ahead and run this. Now, basically,
203:08 - to basically the progress is displayed at
the terminal. If you don't want anything outputted
203:15 - to the terminal, you can basically just set
set the verbosity to zero. But I'm going to
203:21 - leave things just as it is, since there are
a lot of images inside this data set. This
203:26 - may take a while depending on how powerful
your machine is. So that's only took about
203:32 - a minute or so to pre process our data. So
essentially, let's try to so let's essentially
203:39 - try to see how many images there are in this
training set. We do that by saying the length
203:44 - of trip. And we have 13,811 images inside
this training set. So let's actually try to
203:52 - visualize the images that are present in this
dataset. So we're going to import matplotlib.pi
203:59 - plot as PLT, we're going to do a PLT dot bigger.
And we're going to give it and we're going
204:05 - to give it a big size of 30 by 30. Let's do
a plt.im show, we can pass in first. The first
204:14 - element in this training sets are zero and
then zero. And we can give it a color map
204:21 - off gray. And we can display this image. Now
the reason why I'm not using open CV to display
204:32 - this image is because for some reason, open
CV does not display properly in Jupyter Notebook.
204:38 - So that's why we're using matplotlib. So this
is basically the image that we get somebody
204:44 - legible, but to a machine. This is a valid
image. Okay, the next thing we want to do
204:53 - is we want to separate the training set into
the features and labels. Right now. The train
205:00 - That basically is a list of 13,811 lists inside
it. Inside each of that sub lists are two
205:08 - elements, the actual array and the labels
itself. So we're going to separate the feature
205:14 - set, or the arrays and the labels into separate
lists. And the way we do that is by saying
205:20 - feature set and labels is equal to car dot
zip on school train, we are going to separate
205:29 - the training set and give it an image size
of image size. And that's n equals two. So
205:42 - basically, what this is going to do is going
to separate the training set into the feature
205:46 - set and labels and also reshape this feature
set into a four dimensional tensor, so that
205:52 - it can be fed into the model with no restrictions
whatsoever. So go ahead and run that. And
205:58 - once that's done, let's actually try to normalize
the feature sets. So essentially, we are going
206:06 - to normalize the data to be in the range of
to be the range of zero comma one. And the
206:16 - reason for this is because if you normalize
the data, the network will be able to learn
206:20 - the features much faster than, you know, not
normalizing the data. So we're gonna say feature
206:26 - set is equal to square dot normalize, and
when to pass in, peaches set. Now we don't
206:36 - have to normalize the labels. But we do need
to one hot encode them that is convert them
206:41 - from numerical integers to binary class vectors.
And the way we do that is by saying from TensorFlow,
206:48 - del Kara's dot EDU tools input to underscore
categorical. And we can say labels is equal
206:57 - to two categorical, and we get possible labels,
and the number of categories, which is basically
207:04 - the length of this characters list. Cool.
So once that's done, so once that's done,
207:10 - we can actually move ahead and try to create
our training and validation data. Now, don't
207:15 - worry too much if you don't know what these
are. But basically, the model is going to
207:20 - train on the training data and test itself
on the validation data. And we're going to
207:24 - say x underscore train x underscore Val and
y underscore train and y underscore Val is
207:32 - equal to sere dog train, Val split. And we're
going to split the feature set and the labels
207:45 - using a particular validation ratio, which
we're going to set as point two. So that's
207:53 - basically what we're doing, we're splitting
the feature set and labels into into training
207:57 - sets and validation sets with using a particular
validation ratio to 20% of this data will
208:03 - go to the validation set, and 80% will go
to the training set. Okay. Now, just to save
208:09 - on some memory, we can actually remove and
delete some of the variables and we're not
208:13 - going to be using. So we do that by saying
Dell crane, Dale feature sets, do labels,
208:23 - and we can collect this by saying GC dot collect.
Cool. Now moving on, we need to create an
208:33 - image data generator. Now this is basically
an image generator that will essentially synthesize
208:41 - new images from already existing images to
help introduce some randomness to our network
208:47 - and make it perform better. So we're gonna
say data, Gen is equal to can narrow down
208:54 - generators, dot image, data generator. And
this basically instantiates, a very simple
209:03 - image generator from the caros using the Kara's
library. And once it's done, let's create
209:09 - a training generator. By setting this equal
to data Jim, don't float. And we can pass
209:17 - in extra rain and wind rain and give it a
batch size equal to batch size. So let's actually
209:29 - create some variables here. That's set my
batch size to 32. And maybe let's train the
209:38 - network for 20 bucks. So once that's done,
that's wrong bet. So with that done, we can
209:48 - actually proceed to building our model. So
let's call this creating the model. And before
209:56 - making this video, I actually tried and tested
out a couple of models found that one actually
210:02 - provided me with highest level of accuracy.
So that's the same model, the same model architecture
210:07 - that we're going to be using. So we're gonna
say model is equal to conero dot models dot
210:15 - create Simpsons model, we're going to pass
in an image size, which is equal to the image
210:23 - size, we're going to say set the number of
channels equal to the number of channels,
210:30 - we're going to say, we're going to set the
output dimensions to the to 10, which is basically
210:39 - the length of our characters, then we can,
then we can specify a loss, which is equal
210:50 - to binary binary cross entropy. There we get
set a decay of E of e to the negative sixth
211:04 - power, we can set a learning rate equal to
point 001. We can set Oh momentum of point
211:20 - nine, and we can set Nesterov to true. So
this will essentially create the model using
211:30 - the same architecture I built and will actually
compile the model so that we can use it. So
211:36 - go ahead and run this. And we can go ahead
and try to print the summary of this model.
211:46 - And so essentially, what we have is a functional
model, since we're using Kerris as functional
211:51 - API. And this essentially has a bunch of layers,
and about 17 million parameters to drain out.
212:01 - So another thing that I want to do is create
something called a callbacks list. Now this
212:05 - callbacks list will contain something called
a learning rate shedule that will essentially
212:11 - sheduled the learning rate at specific intervals
so that our network can essentially train
212:17 - better. So we're going to say call callbacks
list is equal to learning rate shedule. And
212:29 - we're going to pass in conero.lr on SCO LR
underscore schedule. And since we're using
212:36 - learning where shedule Let's go and input.
So from TensorFlow, Delve, Cara's no callbacks
212:44 - input learning rate schedule. And that should
about do it. So let's actually go ahead and
212:53 - train the model. So we're gonna say training
is equal to model dot fit, we're gonna pass
212:58 - in the train gin, we're going to say, steps
per epoch is equal to the length of X on school
213:10 - train divided by divided by the batch size.
We're going to say epochs, is equal to epochs.
213:21 - We're going to give the validation data validation
data equal to a tuple of x underscore Val,
213:29 - and y underscore Val. And we're going to say
validation steps. Easy Steps is equal to the
213:39 - length of y on school Val, divided by divided
by the batch. batch size. And finally, we
213:51 - can say callbacks, is equal to callbacks,
callbacks on school missed steps per epoch,
214:02 - that steps for epoch. And that should begin
training. And once that is done, we end up
214:09 - with a baseline accuracy of close to 70%.
So here comes the exciting part, we're now
214:15 - going to use open CV to test how good our
model is. So what we're going to do is we're
214:20 - going to use open CV to read in an image at
a particular file path. And we're going to
214:25 - pass that to our network and see what the
model spits out. So let's go ahead and go
214:32 - to this Simpson test set. So let's go ahead
and try to search for all the way down here.
214:43 - Let's look at our characters. Let's just print
that out just to see what characters we trained
214:49 - on. Okay, let's look for Bart Simpson. Probably
bit irritating, but since data sent Okay,
215:01 - we got an image of Bart Simpson. So click
this and random path, got a test path, set
215:11 - this equal to our string. And what we're gonna
do is we're gonna say mg is equal to CV dot
215:18 - m read test on secure path. And, and just
to display this image, we can use PLT dot
215:27 - m show, we can pass the image, pass in the
image and give it a column map of gray. And
215:36 - we can do a PLT dot show. Okay, PLT show.
And okay, so this is an image of Bart Simpson.
215:53 - So what we're going to do is we are going
to create a function called prepare, which
216:00 - will basically prepare our image to be of
the same size and shapes and dimensions as
216:07 - the images we use to prepare the model in.
So this will take in a new image. And what
216:13 - this will do is we'll, we'll convert this
image to grayscale so we're gonna say injury
216:17 - is equal to CV dot, CVT color, and we're gonna
pass in the injury and we're gonna say CV
216:24 - dot color on scrub BGR. To gray, we can resize
it to our image size. So we're going to say
216:30 - mg is equal to CV dot resize, we're going
to resize the image to be image underscore
216:38 - with size, I'm going to reshape this image.
So injury is equal to stare dot reshape, reshape
216:46 - of image. We want to reshape the image to
be of image size with channels equal to one.
216:57 - And we can return image. So let's run that.
And let's go down here. And let's say predictions
217:04 - is equal to model dot predict and prepare
image. And we can visualize this predictions.
217:16 - So let's print predictions. And essentially,
this is what we get. So to print the actual
217:25 - class, what we can do is we can print their
characters, BB NP dot arg Max, and we can
217:34 - say predictions of zero. You're not trying
to visualize this image so we can do a PLT
217:42 - dot m show. Let's pass in the image. And PLT
dot show. Let's grab this and move this down.
217:57 - ver. That's right. Yeah. Okay, so this is
our image. And right now our model thinks
218:05 - that buttons in is in fact, Lisa Simpson.
Let's go. Lisa Simpson. Okay. Let's try another
218:18 - image. Let's try probably this image is Bart
Simpson 28. Let's go up they and maybe change
218:29 - that to two, eight. run that. This is Bart
Simpson. Let's run this. And let's and again,
218:42 - we got Lisa Simpson. So let's try with a different
image. Yeah, we do. We did Charles Montgomery
218:53 - to copy this. All the way down there. We got
Charles predict, and we get van Hughton. Okay,
219:15 - definitely not the best model that we could
have asked for. But hey, this is a model.
219:22 - Right now this base discounting has a baseline
accuracy of 70%. Although I would have liked
219:28 - it to go to at least 85%. In my test, it had
gone close to 90 92%. I'm not sure exactly
219:36 - why this went to 70%. But again, this is to
be expected into building deep computer vision
219:41 - models is a bit of an art. And it takes time
to figure out what's the best model for your
219:46 - project. So that's it for this Python and
open c because this goes to is basically kind
219:52 - of a general introduction to open CV and what
it can do. And of course, we've only just
219:58 - scraped the surface and really this A whole
new world of computer vision now fair. Now,
220:03 - while we obviously can't cover every single
thing that open CV can do, I've tried my best
220:07 - to teach you what's relevant today in computer
vision. And really one of its most interesting
220:12 - parts, building deep learning models, which
is in fact, where the future is self driving
220:17 - vehicles, medical diagnosis, and tons of other
things that computer vision is changing the
220:22 - world. And so all the code and material that
was discussed throughout this course is available
220:27 - on my GitHub page. And the link to this page
will be in the description below. And just
220:32 - before we close, I do want to mention that
although I did recommend you installed Sierra
220:37 - in the beginning, we barely use it throughout
the coasts. Now, it's probably not going to
220:42 - make sense to you right now. But if you plan
to go deeper into computer vision into building
220:48 - computer vision models, Sierra lasher proved
to be a powerful package for you. It has a
220:53 - lot of helper functions to do just about anything.
Now I'm constantly updating this repository.
220:59 - And if you want to contribute to these efforts,
definitely do that you can set a pull request
221:03 - with your changes. And if it's helpful, it
will be merged into the official code base,
221:08 - and you'll be added as a contributor. If you
want to building deep learning models with
221:13 - Kara's then conero will be useful to you.
But again, for the most part, it's usually
221:17 - software that you'll be using. So anyway,
with that said, I think I'll close up this
221:23 - course, if this goes helped you in any way
and God you're more interested in computer
221:28 - vision, then definitely like this video, subscribe
to my channel, as I'll be putting up useful
221:32 - videos on Python computer vision and deep
learning. So I guess that's it. I hope you
221:38 - enjoyed this post and I'll see you in another
video.

Cleaned transcript:

Everyone and welcome to this Python and open CV course. In this course, we'll be talking about everything you need to know. To get started with open CV in Python, we're going to start off with the very basics that is reading images and video, manipulating those media files with image transformations, and how to draw shapes and put text on those files. Then we're going to move on to the most advanced parts of open CV that is switching between color spaces bitwise operators, masking, histograms, edge detection and thresholding. And finally, to sum things up, we'll be talking about face detection and face recognition in open CV, so how to detect and find faces in an image and how to recognize them using inbuilt methods. In the last video, we'll be building a deep computer vision model to classify between the characters in The Simpsons based off some images. All material discussed will be available on my GitHub page, and all relevant links will be put up in the description below. If that sounds exciting, don't forget to head over and subscribe to my channel. And I'll see you in the course. Hey, everybody, and welcome to this Python and urban TV coast. Over the next couple of videos, we're going to be talking about using the open CV library to perform all sorts of image and video related processing and manipulations. Now I won't be delving into what open CV is really is. But just be brief. It is a computer vision library that is available in Python, c++ and Java. A computer vision is an application of deep learning that primarily focuses on deriving insights from media files, that is images and video. Now, I'm going to assume that you already have Python installed on your system. And a good way to check this is by going to terminal and typing Python dash dash version. Now make sure you're running a version of Python of at least 3.7 above whatever we do in this post wonderly work in some older versions of Python, and especially Python two, so just make sure that you have the latest version installed, go ahead to python.org and download the latest version from bet. Now assuming that you've done this, we can proceed to installing the packages that we require in this course. The first one is open C. So go ahead and do a pip install Open CV dash contrib dash Python. Now sometimes you may find people telling you to install just open CV dash Python. Well, this open team dash Python is basically the main package the main module of open CV, open CV dash contract dash Python includes everything in the main module, as well as a contribution modules provided by the community. So this is something I recommend you install as it includes all of open CV functionality. You may also notice that urgency, we tried to install the NumPy package. Now NumPy is kind of a scientific computing package in Python, that's extensively used in matrix an array manipulations, transformations, reshaping and things like that. Now, we'll be using NumPy in some of the videos in this course. But don't worry if you've never used them before. It's simple and relatively easy to get started with. Now the next package, I'd like you to install a sphere. So go ahead and do pip install seer. Now, slight disclaimer, this is a package that I built to basically help you to speed up your workflow. Sierra is basically a set of utility functions that will prove super useful to you in your computer vision journey. It has a ton of super useful helper functions that will help speed up your workflow. Now, although we're not going to be using this for a good part of this course, in fact, we'll only begin to use this in our last video of this course when we're building a deep computer vision model. I recommend you install it now so that you don't have to worry about the installation process later on. If you're interested in contributing to this package, or just simply want to explore the codebase I'll leave a link to this GitHub page in the description below. Okay, that's it for this video. In the next video, we'll be talking about how to read images and video in open CV. So I'll see you guys in the next video. Hey everybody, and welcome back to another video. In this video, we're going to be talking about how to read images and video in open CV. So I have a bunch of images in this photos folder, and a couple of videos in this videos folder. In the first half of this video, we'll be talking about how to read in images in open CV, and towards the end we'll be actually talking about how to read in videos. So let's start off by creating a new file and call this reader dot p y. And the first thing we have to do is actually input CV two as CV. So the way we read in images in open CV is by making use of the cv.im read method. Now this method basically takes in a path to An image and returns that image as a matrix of pixels. Specifically, we're going to be trying to read this image of a cat here. So we're going to say photos slash cat dot jpg. And we're going to capture this image in a variable called IMG. Now you can also provide absolute paths. But since this photos folder is inside my current working directory, I'm going to reference those images relatively. Now once we've read in our image, we can actually display this image by using the cv.rm show method. Now this method basically displays the image as a new window. So the two parameters we need to pass into this method is actually the name of the window, in this case is going to be kept and the actual matrix of pixels to display, which in this case is IMG. And before we actually move ahead, I do want to add an additional line a CV dot wait key zero. Now the CV or wiki zero is basically a keyboard binding function, it waits for a specific delay, or time in milliseconds for a key to be pressed. So if you pass in zero, it basically waits for an infinite amount of time for a keyboard key to be pressed. I didn't worry too much about this, it's not really that important for this course. But we will be discussing some parts of it towards the end of this video. So let's actually save this and run by saying Python, read dot p y, and the image is displayed in a new window. Cool. Now this was a small image, this was an image of size 640 by 427. Now we're going to try and read in this image of the same cat, but a much larger version, a 2400 by 1600 image. So we're gonna say Cat on a school large dot jpg. Let's save that and run. And as you can see, this image goes way off screen. The reason for this is because the dimensions of this image were far greater than the dimensions of the monitor that I'm currently working on. Now currently, open CV does not have an inbuilt way of dealing with images that are far greater than your computer screen. There are ways to mitigate this issue. And we'll be discussing them in the next video when we talk about resizing and rescaling frames and images. But for now, just know that if you have images, if you have large images, it's possibly going to go off screen. So that's it for reading images, we can then move on to reading videos in open CV. So that's called reading videos. So what we're going to do is we're actually going to read in this video of a dog, and the way we read in videos is by actually creating a capture variable and setting this equal to CV dot video capture. Now this method either takes an integer arguments like 0123, etc, or a path to a video file. Now you would provide an integer argument like 012, and three, if you are using your webcam or a camera that is connected to your computer. In most cases, your webcam would be referenced by using the integer zero. But if you have multiple cameras connected to your computer, you could reference them by using the appropriate argument. For example, zero would reference your webcam, one would reference the first camera that is connected to your computer to would reference the second camera and so on. But in this video, we'll be actually looking at how to read an already existing videos from a file path. Now specifically, we'll be reading this dog, this video for dog here. And the way we do that is by providing the path so videos, slash dog dot mp4. Now, here's where reading videos is kind of like different from reading images. In the case of reading and videos, we actually use a one loop and read the video frame by frame. So we're going to say while true. And the first thing we want to do inside this loop is say is true. And frame is equal to capture dot read. Now this capture dot read basically reads in this video frame by frame, it returns the frame and a Boolean that says whether the frame was successfully read in or not. Do you display this video we can actually display an individual frame. So we do this by saying TV on show and we call this video and we pass in the frame and finally for some way to stop the Do from playing indefinitely is by saying if CV don't wait, Ki 20 and 0x ff is equal to equal to Ord of D. There we want to break out of this while loop. And once that's done, we can actually release the capture pointer. And we can destroy all windows. And we can get rid of this. So basically just to recap, the capture variable is an instance of this video capture clause. Inside of while loop, we grab the video frame by frame. By utilizing the captured read method, we display each frame of the video by using the CV dot m show method. And finally, for some way to break out of this while loop, we say if See, we don't wait ki 20 if and 0x f f is equal to or D, which basically says that if the letter D is pressed, then break out of this loop and stop displaying the video. And finally, we release the capture device and we destroy all the windows since we don't need them anymore. So let's save that and run. And we get a video displayed in a window like this. But once it's done, you will notice that the video suddenly stops and you get this error. More specifically a negative 215 assertion failed error. Now if you ever get an error like this negative 215 assertion failed. This would mean in almost all cases is that open CV could not find a media file at that particular location that you specified. Now, the reason why it happened in the video is because the video ran out of frames, open CV could not find any more frames after the last frame in this video. So it unexpectedly broke out of the while loop by itself by raising a CV to error. And now you're gonna get the same error. If we comment this out, we uncomment this out. And we specify a wrong path to this image. So I see me Oh wait, wait key, zero, save that and run and we get the exact same error. This basically again says that open CV could not find the image or the video frame at a particular location basically, it could not be ready. That's what it's saying. So that's pretty much it. For this video, we talked about how to read any images in open CV and how to read in videos using the video capture class. In the next video, we'll be talking about how to rescale and resize images and video frames in open CV. So see you then. Hey, everyone, and welcome back. In this video, we're going to be talking about how to resize and rescale images and video frames in open CV. Now, we usually resize and rescale video files and images to prevent computational strain. Large media files tend to store a lot of information in it and displaying it takes up a lot of processing needs that your computer needs to assign. So by resizing and rescaling, we're actually trying to get rid of some of that information. rescaling video implies modifying its height and width to a particular height and width. Generally, it's always best practice to downscale or change the width and height of your video files to a smaller value than the original dimensions. The reason for this is because while most cameras your webcam included, do not support going higher than its maximum capability. So for example, if a camera shoots in 720 P, chances are it's not going to be able to shoot in 1080 P or higher. So to rescale a video frame or an image, we can create a function called def rescale frame. And we can pass in the frame to be resized and scale the value which by default we're going to set as point seven five. So what I'm going to do next is I'm going to say with is equal to frame dot shape of one of one times scale. And I'm going to copy this and do the same thing for the height. Now remember frame no shape of one is basically the width of your frame or your image and frame note shape of zero is basically the height of the image. Now since width and height are integers, I can actually convert these floating point values to an integer by converting it to an iron T. And what we're going to be doing is we're going to create a variable called dimensions, and set this equal to a table of width, comma height. And we can actually return CV don't resize the frame, the dimensions, and we can pass in it interpolations of CV dot into on the school area. Now we'll be talking about CV dot resize in an upcoming video. But for now, just note that it resizes the frame to a particular dimension. So that's all a function does, it takes in the frame, and it scales that frame by a particular scalar value, which by default is point seven, five. So let's actually try to see this in action. Let's go back to this readout p y, and grab this code. And we can paste there, we don't need us for now. uncomment these out. Now what I'm going to do is after I've read in the frame, I'm going to create a new frame call frame on this go resized, and set this equal to rescale frame of frame. And let's leave the scale value is point seven, five. And we can actually display this video resized by passing a frame on the scope resized. Resize. So let's save that and run Python rescale del p why that was an error. Okay, we don't need this. Let's close that out, Save and Run. And this was our original video. And this is actually a resize the video with the video resize by point seven 570 5%. We can modify this by changing the scale of value to to maybe point two, so we rescaling to 20%. And we get an even smaller video in a new window. So let's close that out. Now you can also apply this on images. So let's uncomment that out, change that to cat dot jpg. And we can do receive our show. Image and pawson the resized image. And we can create a resize image by calling rescale frame and we could pass in the IMG. So let's see that in Rome. And this is a small videos we're not concerned with that. This is actually the big image the large image. And this is the recent version of this image. So let's close that out. Now there is another way of rescaling or resizing video frames specifically. And that's actually using the capture dot set method. Now this is specifically for videos, and will work for images. So let's go ahead and try to do that. Let's call this depth change rez. So we're changing we're changing the resolution of the image of video. And we can pass in a width and a height. And what we're going to do is we're going to say capture, don't set three comma with and we're going to do the same thing with capture dot set four comma height. Now three info basically stands for the properties of this capture class. So three references the width and full references the height. You can also expand this to maybe change the brightness in the image. And I think you can reference that by setting this to 10. But for now we're going to be interested in the width and the height. Now, I do want to point out this, this method will work for images, videos, and live video. Basically, for everything you can use this rescale frame method. But the changes function only works for live video. That is video you read in from an external camera or your webcam for instance. So video that is going on currently, this is not going to work on standalone video files, video files that already exist. It just doesn't work. So if you're trying to change the resolution of live video, then go with this function if you're trying to change the resolution of an old already existing video, then go with this function. So that's pretty much it for this video that we talked about how to resize and rescale video frames and images in open CV. In the next video, we'll be talking about how to draw shapes, and write text on an image. So that's everything. I'll see you guys in the next video. Hey, everyone, and welcome back to another video. In this video, we're going to be talking about how to draw and write on images. So go ahead and create a new file and call this draw dot p y. We're going to input CV two and CV, we're going to input the NumPy package that open CV had installed previously. And we're going to input that as MP, we will read in an image by saying OMG is equal to cv.rm, read person photos, photos slash cat dot jpg, we can display that image in a new window. And we can do receive out of weight key zero. Now there are two ways we can draw on images by actually drawing on standalone images like this image of a cat to or we can create a dummy image or a blank image to work with. And the way in which we can create a blank image is by saying blank is equal to NP dot zeros of shape 500 by 500. And give it a data type of ui 98. You ID eight is basically an image the datatype of an image. So if you want to try and see this image, see what this image looks like. We can say blank, and we can pass in like save that and run Python drawed or p y. And this is basically the blank image that you can draw on. So we're going to be using that in instead of drawing this cat image. But feel free to use this cat image if you'd like. So the first thing we're going to do is try to paint is trying to paint the image a certain color. And the way we do this is by saying blank and reference all the pixels and set this equal to zero comma 255 comma zero. So by painting the entire image green, and we can display this image by saying green in passing the blank image, save that and run. Can I broadcast Yeah, okay, you need to give it a shape of three, basically, we are giving the shape of height, width, and the number of color channels. So just keep that in mind save up. And this is the green image that we get cool, we can even change this and try to change this to red zero comma 255. Save that. And we get a red image over here. Now you can also call a certain portion of the image by basically giving it a range of pixels. So we can say 200 to 300. And then from 300 to 400. Save that and run and you got a Red Square in this image. The next thing we're going to do is we're going to draw a rectangle. And the way we do this is by using the CV don't rectangle method. This method takes in an image to draw the rectangle over, which in this case is blank. And it takes in point 1.2, color, thickness and a line type if you'd like. So the point one will specifically be zero comma zero, which is the origin. And we can go all the way across to 250 comma 250. Let's give it a color of zero comma 255 comma zero, which is green, give it a thickness of let's say two, which is basically saying the thickness of the borders. And once that's done, we can display this image by saying let's call this rectangle in passing and passing the blank image. We can comment this out since we don't need this anymore. And we get a green rectangle that goes all the way from the origin to 250 comma 250. You can play around with it if you like so we can go from 250 to maybe 500. And it goes all the way across the image. So you basically divide the image in half. Now there is a way of filling in this image a certain color. And the way we do this is instead of saying thickness is equal to two, we say thickness is equal to CV dot field. That basically fills in the rectangle to get this green rectangle. Now Alternatively, you can also specify this as negative one, negative one. And we get the same result, what we can also do is, instead of giving it fixed values like 250, and 500, what we could do is we could say, IMG done shape of zero, of one divided by divided by two, and image dome shape of zero, divided by divided by two. Let's save that and run. image is not in fact, God, this is blank, this is blank, save that and run. And we get a nice little rectangle, or square, if you will, in this image, what it basically did is it scaled the rectangle from instead of being these, this entire square, this rectangle basically has dimensions, half of that of the original image. So moving on, let's try and draw a circle. Draw circle. This is also fairly straightforward, we do a CV dot circle. And we pass in the blank image. And we give it a center, which basically the coordinates of the center for now let's set this to the midpoint of this image by saying 250 comma 250. Alternatively, you could also get this let's give it a radius of 40 pixels, give it a color of zero comma, zero comma 255, which is red BGR. And give it a thickness of let's say three. We can display this image, say, circle is equal to blank. And we get a nice little circle over here, that has its center at 250 km 250, and radius of 40 pixels. Again, you can also fill in this image by giving a thickness of negative one. Here, we get a nice little dot here in the middle. Cool. Now there's something else that I forgot. And that is how to draw a line a standalone line on the image. That again, is fairly straightforward, say draw a line, we use a cv.in line method. And this takes in the image to draw the line on and two points, that's just copy these points, basically everything. And this basically draws a point from zero comma zero to half these image dimensions. So that's 252 50. And then it draws a line of color zero comma 255, comma zero. Let's set this to full white 2255, d 5255. And it's green thickness you can specify as three. And we didn't display this image. See you don't on show colas line, rule the line, blank image, and we get a line that goes all across from zero comma, zero comma zero to 250, comma 250. Let's try and play around with this. And let's draw a line from 100 to maybe 250. And then it goes all the way to 300 to 400, save that. And you've got a line that goes from 100 100 to 300, comma 400. Cool. And finally, the last thing that we will discuss in this video is how to write text on an image that that's right text on an image. Now, the way we do this is very straightforward. We see we do a CV dot put text. And this will put text on the blank image. We specify what we want to put on. So let's say hello. We can give it an origin, which is basically where do we want to draw the image from? Let's set this to 225 and 225. And we can also specify font face. Now open CV comes with inbuilt fonts. And we will be using the CV dot font unschool Hershey ns go. We'll be using the triple x, you have complex you have duplex you have plain. You have script simplex and a lot of inbuilt phones. But for now, let's use a triplex. Let's give this a font scale, which is basically how much do you want to scale the font by, let's set this to 1.0. We don't want to scale a font, let's give it a color of zero comma 255, comma zero, and give it a thickness of two. Commit that out. And we can display this image. So you don't I'm show let's call this text and pass in the blank image. And we get some text that is placed on the image. You play around with it and say, Hello, my name is Jason. Save and Run. And it goes off screen. I when we're dealing with large images, but we can there's no way of actually handling this except for maybe changing the margins here a bit, too, we can do that by saying let's say it's zero comma two to five. And it sounds from zero and says Hello, my name is yes. So that's it. For this video, we talked about how to draw shapes, how to draw a lines, rectangles, circles and how to write text on an image. Now in the next video, we'll be talking about basic functions in open CV, that you're most likely going to come across whatever project in computer vision you end up doing. So if that's it, I'll see you guys in the next video. Hey, everyone, and welcome back to another video. In this video, we're going to be talking about the most basic functions in open CV that you're going to come across in whatever computer vision project you end up building. So let's start off with the first function. And that is converting an image to grayscale. So we've written an image, and we've displayed that image in a new window. And currently, this is a BGR image, a three channel blue, green and red image. Now there are ways in open CV to essentially convert those BGR images to grayscale so that you only see the intensity distribution of pixels rather than the color itself. So the way we do that is by saying gray is equal to CV dot CBT color, we pass in the image that we want to convert from, which is IMG, and we specify a color code. Now this kind of code is CV dealt kind of unskilled BGR. To great, since we're converting a BGR image to a grayscale image. And we can go ahead and display this image by saying CV don't show passing gray and pass in the gray image. Save that and run your Python basic.pi. And this was the original image. And this is the grayscale image. Let's try this with another image. Slide with no this is the image of a park in Boston save and maybe change that to Boston. And this is the BGR image in open CV, and this is its corresponding grayscale image. So nothing too fancy. We've just converted from a BGR image to a grayscale image. The next function we're going to discuss is how to blur an image. Now blurring an image essentially removes some of the noise that exists in an image. For example, in an image, there may be some extra elements that were there because of bad lighting when the image was taken, or maybe some issues with the camera sensor and so on. And some of the ways we can actually reduce this noise is by applying a slight blur. There are way too many blurring techniques which we will get into in the advanced part of this goes. But for now we're just going to use the Gaussian Blur. So what we're going to do is we're going to create a blurred image. I think blur is equal to CV dot Gaussian Blur. And this image will take an associate image which is the IMG it will take in a kernel size, which is actually a two by two tuple which is basically the window size that open CV uses to compute the blown the image. We'll get into this in the advanced part of the scope so don't worry too much about this, just know that this kernel size has to be an odd number. So So let's start a real simple and keep the kernel size to three by three. And another thing that we have to specify is CV dot border on school default. So go ahead and try to display this image, the same blur, and pawson blue. Now, you will be able to notice some of the differences in this image. And that is because of the blur that is applied on it. Right this people in the background are pretty clear on this image. And over here, they're slightly blurred. To increase a blind his image, we can essentially increase the kernel size from three by three to seven by seven, save that and run. And this is the image that is way more blurred than the previous image. So that's it. The next function we're going to discuss is how to create an edge cascade, which is basically trying to find the edges that are present in the image. Now again, there are many edge cascades that are available. But for this video, we're going to be using the canny edge detector, which is pretty famous in the computer vision world. Essentially, it's a multi step process that involves a lot of blurring and then involves a lot of grading computations and stuff like that. So we're gonna say, Kenny, Kenny is equal to CV dot Kenny, we pass in the image, we pass in to threshold values, which for now I'm going to say 125 and 175. Let's go ahead and try to display this image, get the Kenny images. And we can pass in county. Save that and run. And these were the edges that were found in this image. As you can see that it hardly any edges found in the sky. But a lot of features in the trees and the buildings. And quite a few, you know features and edges in the grass and stuff. We can reduce some of these edges by essentially blurring the image. And the way we do that is instead of passing the IMG, we pass in the blur. See that run. And as you can see that there were far less edges that were found in the image. And this is a way you can basically reduce the amount of edges that were found by a lot by applying a lot of blur, or get rid of some of the edges by applying a slight blur. Now the next function we're going to discuss is how to dilate an image using a specific structuring element. Now the structuring element that we are going to use is actually these edges, the canny edges that were found, so we're gonna say dominating the image. And the way we do that is by saying dilated is equal to CV dot dilate. And this will take in the structuring element, which is basically the canny edges. And we'll take a kernel size, which we'll specify as three by three for now. And it will also take n iterations of one. Now, dilation can be applied using several iterations of the time, but for now, we're just going to stick with one. So go ahead and try to display this image by saying CV dot m shope. Call this dilated. And we can pass in David. Save that and run. And if these were, if these were edges, these are the dilated edges, we can maybe increase the kernel size to maybe seven by seven and tried to see what that does hold on. And nothing much was done. Not much difference was that let's try to increase the number of iterations to maybe three. And it's definitely way thicker. But you're gonna see subtle differences with the amount of features and edges that you find. Now there is a way of eroding this dilated image to get back this structuring element. Now, it's not going to be perfect, but it will work in some cases. So we're gonna say, call this roading and we call this eroded is equal to CV don't erode, it will take in the dilated image, pass and dilated, it will take a kernel size of let's start off with three by three and given n iterations of one just for now. And we didn't display this image show coolest clothes eroded, eroded and if this was your structuring element, and this was your dilate image, this is basically the result you get from eroding this image. Now, it isn't the same as a structural element. But you can just about to make the features that. But you can see that between this and this, there is a subtle change in the edges and the thickness of these edges, we can maybe try to match these values, so that we attempt so that there is an attempt to get back this edge cascade. And yes, we got the edges back there, as you can see that you compare these two, they look pretty much the same. And the edges are the same. So essentially, if you follow the same steps, you can, in most cases, get back the same edge cascade. And probably the last function that we're going to discuss is how to resize and crop an image. So we're going to start with resize. So we come to resizing video frames and images in the previous video in one of the previous videos. But we're just going to touch on the CBO resize function just a bit. So we're going to say resized, resized equal to CV dot resize, this will take an image to be resized, and it will take in a destination size, which let's set this to 500 by 500. And so this essentially takes in this image of the park, and resize that image to 500 by 500, ignoring the aspect ratio. So we display this image by saying saved out I'm sure resized and resized. Save that and run. And let's go back to this image. If this is the original image, this is the image that was resized to 500 by 500. Now by default, there is an interpolation that occurs in the background, and that is CV dot into on the scope area. Now this interpolation method is useful if you are shrinking the image to dimensions that are smaller than that of the original dimensions. But in some cases, if you are trying to enlarge the image and scale the image to a much larger dimensions, you will probably use the inter underscore linear or the inter on scope cubic. Now cubic is the slowest among them all. But the resulting image that you get is of a much higher quality than the inter on scope area or the inter underscore linear. So let's touch on cropping. And that's basically by utilizing the fact that images are arrays. And we can employ something called Array Slicing, we can select a portion of the image on the basis of your pixel values. So we can say cropped is equal to the image. And we can select a region from 50 to 200. And from 200 to 400. And we can display this image Cole is cropped, possibly cropped. And this is a cropped image of let's go back here of this original image, you try to superimpose them, it's probably going to be you. Yeah, it's basically this portion. So that's pretty much it. For this video, we talked about the most basic functions in open CV, we talked about converting an image to grayscale by applying some blur by creating an edge cascade by dilating the image by eroding that dilated image by resizing an image and trying to crop an image using Array Slicing. In the next video, we're going to be talking about image transformations in open CV, that's translation, rotation, resizing, flipping and cropping, so if you have any questions, leave them in the comments below. Otherwise, I'll see you guys in the next video. Hey, everyone, and welcome back to this Python and open CV course. In this section, we're going to cover basic image transformations. Now these are common techniques that you would likely apply to images, including translation, rotation, resizing, clipping and cropping. So let's start off with translation. Translation is basically shifting an image along the x and y axis. So using translation, you can shift an image up, down, left, right, or with any combination of the above. So so to translate an image, we can create a translating function, we're gonna call this def translate This translation function will take in an image to translate and take an x and y, x and y basically stands for the number of pixels, you want to shift along the x axis and the y axis respectively. So do translate an image, we need to create a translation matrix. So we're going to call this transmit is equal to NP dot float 32. And this will take in a list with two lists inside of it. And the first list we're going to say, one comma zero comma x, and zero comma one comma y. And since we're using NumPy, we can import NumPy, import NumPy as NP. And once we've created our translation matrix, we can essentially get the dimensions of the image saying dimensions, which is a tuple of image don't shave off one, which is the width an image dot shape of zero, which is the height. And we can return CV dot warp a fine. This will take in the image matrix to trans MIT animal taking the dimensions. And with that data, we can essentially translate our image. And before we do that, I do want to mention that if you have negative values for x, you're essentially translating the image to the left, negative negative y values implies shifting up positive x values implies shifting to the right. And as you guessed, positive y values shifted down. So let's create our first translated image. We're setting this equal to translate, we're going to pass in the image, the image and we're going to shift the image right by 100 pixels, and down by 100 pixels. That's to receive it on on the show, translated and translate tip. Save that and run Python krones formations dot p y. And this is your translated image, it was shifted down by 100 pixels and shifted to the right by 100 pixels. So let's change that. Let's shift the image left by 100 pixels and down by 100 pixels. So we pass in negative values for x and it moved to the left. Feel free to play around with these values as you see fit. Just know that negative x shifts to the left, negative y shoves it up, x shifted to the right and positive y values shifted down. Moving on, let's talk about rotation. rotation is exactly what it sounds like rotating an image by some angle. Open CV allows you to specify any point any rotation point that you'd like to rotate the image around. Usually if the center but but with open CV, you could specify any arbitrary point it could be any corner, it could be 10 pixels to the right 40 pixels down, and you can shift the image around that point. So to draw to rotate the image, we can create a rotating function, let's call this dev rotate. This will take an image angle to rotate around and a rotation point which we're going to say which we're going to set is not so we're going to grab the height and width of the image by pressing by setting this equal to IMG dot shape of the first two values. Basically, if the rotation point is none, we are going to assume that we want to rotate around the center. So we're going to say rot point is equal to width divided by two divided by two in height divided by divided by two. And we can essentially create the rotation matrix like we did with the translation matrix. By setting this equal to rot met is equal to CV dot get rotation matrix 2d. We're going to pass in the center the rotation point and angle to rotate around which is angle and a scale value. Now we're not interested in scaling the image when we've rotated so we can set this to 1.0. value we can set a dimensions variable equal to the width and the height and we can return the rotated image which is a CV don't warp a fine image rot met the destination size which is dimensions. And that's it. That's all we need for this function. So we can create a rotated image by setting this equal to rotate, and we can rotate the original image by 45 degrees. So let's display this image, call this rotated, and pass and rotated. Save that in rock. And this is your rotated image. As you can see, it was rotated counterclockwise by 45 degrees. If somehow you wanted to rotate this image clockwise, just specify negative values for this angle, and it will rotate the image around rotated clockwise. Now you can also rotate a rotated image that is take this image and rotated by 45 degrees further. So let's call this rotated, rotated rotated is equal to rotate or rotate tid. And we can rotate this image by another 45 degrees. So we're rotating it clockwise. And we can see the.on show called is rotated, rotated. And we can pause and rotated, rotated, whatever, rotate it. And this is your rotate rotated image. Now the reason why these black lines were included is because if there's no image in it, if there's no part of the image in it, it's going to be black by default. So when you took this image and rotated it by 45 degrees, you essentially rotated the image, but introduce these black triangles. Now if you tried to rotate this image further by some angle, you are also trying to rotate these black triangles along with it. So that's why you get these kind of a skewed image. So there's additional triangles are included over here. But save yourself the trouble and basically add up these angles and you will get the final angle. So we can change that to 90 and retake the original image by negative 90. And this is essentially the image that we were trying to go for, take this image rotated 45 degrees clockwise and rotate this 45 degrees image by further 45 degrees, save yourself the trouble and add those two angle values. So so far, we've covered two image transformations, translation and rotation. Now we're going to explore how to resize an image. Now this is nothing too different from what we've discussed previously. But let's touch on adjust a bit resizing. And we can create a resized variable and set this equal to CV don't resize, we can pass in the image to resize and the destination signs of maybe 500 by 500. And by default the interpolation is CV dot inter underscore area. You can maybe change this to into underscore linear or inter underscore cubic. Definitely a matter of preference depending on whether you're enlarging or shrinking the image. If you're shrinking the image, you will probably go for into underscore area or stick with default. If you're enlarging the image, you could probably use the inter underscore linear or the dansko cubic cubic is slower, but the resulting image is better with over high quality. Again, I think it's you different from what we discussed before. So we can display this image. I can resize and passing and resized. Save that run and we've got a resized image. Next up we have flipping how to flip an image. So we don't need to define a function for this, we just need to create a variable and set this equal to CV dot flip. This will take in an image and a flipped code. Now this flip code could either be 01 or negative one. Zero basically implies flipping the image of vertically that is over the x axis one specifies that you want to flip the image horizontally or over the y axis and negative one basically implies flipping the image both vertically as well as horizontally. So let's start off with zero claiming it vertically. I'm show call this flip in Parson boop, Save and Run. And this is the image that was clipped vertically. Let's try out a horizontal clip how we get a horizontal Flip, surely see whether it was a horizontal flip, we can bring these two images together. And if they looked like mirror images, then it was flipped horizontally. This is a kind of a symmetric image. So it's not that obvious, but bring them together and you can maybe find out the difference. We could also try to flip the image vertically and horizontally by specifying negative one as a flip code. And the image was flipped both vertically, as well as horizontally mirror images, but reverse mirror images. And the last method is cropping now being discussed cropping again, I'm just going to touch on it, we can create a variable called corrupt and set this equal to IMG and perform some Array Slicing. So 200 to 403 100 to 400. Save that and run. We didn't display the search. Even though I'm show it's cool as cropped, past and cropped, Save and Run. And this is the cropped image we try to bring this together can be brought together, cutting gram holders. Okay. So that's pretty much it. For this video, we talked about translating an image, rotating that image, resizing an image, flipping an image and cropping those images, we are basically just covering the basics, basic image transformations. There are of course, way mo transformation that you could possibly do with open CV. But just to keep this go simple and beginner friendly, I'm only covering the basic transformations. So that's it for this video. In the next video, we're going to be talking about how to identify countries in an image. So if you have any questions, leave them in the comments below. Otherwise, I'll see you guys in the next video. Hey everyone, and welcome back to another video. In this video, we're going to be talking about how to identify contours in open CV. Now contours are basically the boundaries of objects, the line or curve that joins the continuous points along the boundary of an object. Now from a mathematical point of view, they're not the same as edges. For the most part, you can get away with thinking of contours as edges. But from a mathematical point of view, contours and edges are two different things. contours are useful tools when you get into shape analysis and object detection and recognition. So in this video, I sort of want to introduce you to the idea of contours and how to identify them in open CV. So the first thing I've done is I've read in a file, an image file, and I've displayed that image using the cv.rm show method. Then next thing I want to do is convert this image to grayscale by saying gray is equal to CV dot CVT color IMG CV dot color on this go BGR to great, and we can display this. So just know that we're on the same footing. I'm going to run this Python, Cantu's down p y. And we get a gray image over here. Now after this, I want to essentially grab the edges of the image using the canny edge detector. So I'm going to say Kenny is equal to CV Kenny, we're going to pass in the IMG and we're going to give it to threshold values. So 125 and 175. And we can display this image calling this Kenny edges passing Kenny. I save that and run it I didn't save it, save it in ROM and these are the edges that were there in the image. Now, the way we find the contours of this image is by using the find contours method. Now this method basically returns two things, contours and higher keys. And essentially this is equal to CV dot find Cantu's. This takes in the edges. So Kenny, it takes in a mod in which to find the contents now this is either CV dot retter on a scope tree, if you want all the hierarchical contours, or the rhetoric external if you want only the external countries, or, or retter list if you want all the cartoons in the image. The next method we pass in is actually the cone to approximation method for now we're going to set this to CV dot chain, unscrew approx ns go numb. So let's, let's just have a top down look at what this function does. So essentially, the CBO fund contours method looks at the structuring element or the edges of a found in the image and returns to values, the contours, which is essentially a Python list of all the coordinates of the contours that were found in the image. And hierarchies, which is really out of the scope of this course. But essentially, it refers to the hierarchical representation of contours. So for example, if you have a rectangle, and inside the rectangle, if you have a square, and inside of that square, you have a circle. So this hierarchy is essentially the representation that open CV uses to find these courtrooms. This even retinal list essentially is a mod in which this fine contries method returns and finds the cuantos. Read a list essentially returns all the quantities that find in the image. We also have Reto external that we discussed radix download retrieves only the external conduits to all the ones on the outside, it returns those revenue underscore tree returns all the hierarchical contours, all the contours that are in a hierarchical system that is returned by record underscore tree. For now, I'm just going to set this to will list to return all the contours in the image. The next one we have is the contour approximation method. This is basically how we want to approximate the contour. So chain approx none does nothing, it just returns all of the contracts. Some people prefer to use red chain approx symbol, which essentially compresses all the quantities that are returned in the simple ones that make most sense. So for example, if you have a line in an image, if you use chain approx none, you are essentially going to get all the contours all the coordinates of the points of that line, chain approx simple essentially takes all of those points of that line, compresses it into the two end points only. Because that makes the most sense, a line is defined by only two end points, we don't want all the points in between. That, in a nutshell is what this entire function is doing. So since cartoons is a list, we can essentially find the number of cartoons that were found by finding the length of this list. So we can print print length of this list. And we can say fair, we can say we can say these many contused. Found. Okay, so let's say that and Ron. And we found 2794 quantos in the image. And this is huge. This is a lot of code who's ever found in the image. So let's do a couple of things. Let's try to change this chain approx symbol to chain approx none, and see what that does. See how that affects our length. Now there isn't any difference between those two, because I'm guessing that there were no points to compress and sin there are a lot of edges and points in this image. So there wasn't a lot of compression. So let's change the back to symbol. And actually, what we want to do is I want to blow this image before I find the edges. So let's do this. Let's do a blue is equal to CV dot Gaussian Blur can pass in the gray image. And we can give the kernel size of let's let's do a lot of blur. So five by five. And maybe we can give it by the default of CV dot border on disko default. And we can if you want to, and we can display this image, call this blur and pass an error we can find the edges on this blurred image. So let's close below. And as you can see this significant reduction in the number of Quorn twos that were found just by blurring the image. So it went all the way from 2794 to 380. That's closest seven times just by blurring the image with the kernel size of five by five. Okay, now there is another way of finding the corner shoes is that it's stead of using this canny edge detector, we can use another function in open CV, and that is threshold. So I'm just going to comment this out. And down here, what I'm going to do is I'm going to say, ret Thresh is equal to CV don't threshold, this will take in the gray image, and we've taken a threshold value of 125 and a maximum value of 255. I don't worry too much about thresholding. For now, just know that threshold essentially looks at an image and tries to binarize that image. So if a particular pixel is below 125, if the density of that pixel is below 125, it's going to be set to zero or blank. If it is above 125, it is set to white or two by five. That's all it does. And in the find quantities method, we can essentially pass in the thrush value. So let's save that. Let's close this out and try to run that. Type. Okay. threshold missing. Okay, I think I forgot one part, where to specify a threshold and type. So this is CV dot Thresh. On this go, binary, binary raising the image basically. Okay, let's run that. And there were 839 contours that were found, we can visualize that let's print ad to display this Thresh. Image, passing Thresh. Same that run. And this was the thresholded image you're using 125. close this out, using 125 as our threshold value, and 255 as a maximum value, we got this thresholded image. And when we tried to find the current use on this image, we got 839 concepts. Now don't worry too much about this thresholding business, we'll discuss this in the advanced section of this goes more in depth just know that thresholding attempts to binarize an image, take an image and convert it into binary form that is either zero or black, or white, or to Vi five. Now what's cool in open CV is that you can actually visualize the contours that were found on the image by essentially drawing over the image. So what do we do real quick is actually input NumPy NumPy as NP and after this, I'm going to create a blank variable and set this equal to NP dot zeros of image dot shape of the first two values, and maybe give it a data type of I know you are 28 we can display this image because blank pawsome blank, just to visualize and have a blank image to work with. Let's save that and go to a blank image. This is of the same dimensions as our original accounts image. So what I'm going to do is I'm going to draw these contours on that blank image so that we know what kind of contours that open CV found. So the way we do that is by using the CV dot draw contours method, it takes in an image to draw over fill blank, it takes in the contours, which has to be a list, which in this case is just the quantities list. It takes an account to index which are basically how many countries do you want in the image. Since we want all of them since we want to draw all of them, we can specify a negative one, give it a color, let's add this to BGR. So let's set this to red zero comma zero comma 255. And we can give it a thickness of maybe two. And we can display the blank image. So let's call this contused join. And we can pass in blank. Save that and run. Okay, there was an error I think this has to be shaped. Okay, so these were the cartoons that would draw on the image. If you take a look at the threshold value thresholded image, it's not the same thing. What I believe it attempted to do is instead it found the edges of this image all the edges of this image and attempted to draw it out on this blank image. Let's set this so let's set the thickness to maybe one so that we have a crisper view Okay, so these were the quantities that were drawn in the image. And in fact, if you try to visualize it with Kenny, let's actually visualize that with Kenny uncomment. That out, run. blows on the point undefined. Okay, that has to be an image. Okay, let's look at Kenny, let's look at this. Okay, it's not the same thing. And that makes sense, because our firing coaches method and use Kenny, as the basis of detecting and finding the controls. But we can do that. Let's not use a thresholding method. And instead, let's use Kenny. So we can pass in Kenny here. Save that and run. And, okay, that pretty much the same thing, right? It's basically a mirror image of these two, like I said, you can get away with thinking of contours as edges. They're not the same thing. But, but you can think of them as edges. Because from a programming point of view, they kind of like the edges of the image. Right? The other boundaries, they are curves that join the points along the boundary, those are basically edges. So let's try to blow that image. Let's uncomment that out. Let's see what that does. I don't think that had any effect because we didn't pass in blood. Okay, 380 countries have found and mirror images of each other. So generally, what I recommend is that you use scanning method first, and then try to find the corn who's using that, rather than try to threshold the image and then find the contours on that. Because like we will discuss in the advanced section, this type of thresholding. The simple thresholding has its disadvantages. Maybe because we're passing in a simple, just one value, dread binarize the image using this threshold value, right? It's not the most ideal, but in some cases, in most cases, it is most favored kind of thresholding because it's the simplest, and it does the job pretty well. So that's pretty much it. For this video, we talked about how to identify quantities in open CV. But in two methods first trying to find the edge cascades of the image using the canny edge detector, and try to find the quantities using that and also trying to binarize that image using the CV dot threshold and finding the contours on that. So if you have any questions, leave them in the comments below. I'll be sure to check them out. Otherwise, as always, I'll see you guys in the next video. Hey, everyone, and welcome back to another video. We are now at the advanced section of this course, where we are going to discuss the advanced concepts in open CV. So what we're going to be doing in this video is actually discussing how to switch between color spaces in urgency. Our color spaces, basically a space of colors, a system of representing an array of pixel colors. RGB is a kind of space grayscale is color space. We also have other color spaces like HSV, lamb, and many more. So let's start off with trying to convert this image to grayscale. So we're going to convert from a BGR image which is open CV is default way of reading and images. And we're going to convert that to grayscale. So the way we do that is by saying gray is equal to CV dot CBT color. We pass in the image and we specify a color code, which is CV dot color, underscore BGR to to grip since we're converting from a BGR image format to grayscale format, and we can display this image I st gray and passing in grip. Let's save that and run Python spaces dot p y. We had a problem as a comma, Save and Run. And this is the grayscale version of this BGR image. Cool pretty cool. grayscale images basically show you the distribution of pixel intensities at particular locations of your image. So let's start off with trying to convert this image to an HSV format. So from Jeff from vgr to HSV. HSV is also called hue saturation value and is kind of based on how humans think and conceive of color. So the way we conduct that is by saying HSV is equal to CV dot CBT color, we pass in the IMG variable. And we specify a color code, which is CV dot color, undergo BGR to HSV. And we can display the syringe called as HSV and pass in HSV. Let's save that. And this is the HSE version of this BGR image. As you can see that there was a lot of green in this era and the skies are reddish. Now we also have another kind of color space. And that is called the LA be color space. So we're going to convert from BGR to L A, B. This is sometimes represented as L times A times B, but but v free to use whatever you want. So lb is equal to CV dot CVT color, we pass the MG and the color on the scope of BGR. to AB see that I'm sure colas lamb pass and lamb is wrong that and this is the LGB version of this BGR image. This kind of looks like a washed down version of this BGR image. But hey, that's the lamb format is more tuned to how humans perceive color. Now when I started off with this goes, I mentioned that open CV reads in images in a BGR format that has blue, green and red. And that's not the current system that we use to represent colors outside of open CV. Outside of open CV, we use the RGB format, which is kind of like the inverse of the BGR format. Now if you try to display this IMG image in a Python library that's not open CV, you're probably going to see an inversion of colors. And we can do there real quick. Let's try to input mat plot lib dot pie plot as PLT. And we can can, we can basically uncomment commented that out. And we can try and display this image variable. So we're gonna say PLT dot, I am show pass in the image. And we could say a peak, or we could say PLT dot show, maybe let's comment this out, save that and run. And this is the image you get. Now, if you compare with the image that open CV read, this is completely different, these two are completely different images. And the reason for this is because this image is a BGR image and open CV displays BGR images. But now if you tried to take this BGR image and try to display it in matplotlib, for instance, matplotlib has no idea that this image is a BGR image and displays that image as if it were an RGB image. So that's why you see an inversion of color. So where there's red over here, you see a blue, where there's blue over here you see a red, and there are ways to convert this from BGR to RGB. And that is by using open CV itself. So let's comment that out. And let's uncomment this all out. And right over here, let's say BGR to RGB. And what we're going to say is our RGB is equal to CV dot CVT color, we can pass in the BGR image oopsie, we can pass in the br image. And what we're going to do is specify a color code, which you see without color on the scope BGR to RGB. And we can try to display this image in in open CV and see what that displays RGB. And we can also display this in matplotlib. So I've passed in the RGB. And we can do PLT dot show, save that and go here it is you Python spaces dot p y. What I'm most interested in is this. And this. Now again, you see an inversion of colors, but this time in open CV because now you provided open CV and RGB image. And it assumed it was a BGR image. And that's why there's an inversion of colors. But we pass in the RGB image to matplotlib and matplotlib is default is RGB. So that's why I displayed the proper image. So just keep this in mind when you're working with multiple libraries, including open CV and matplotlib for instance, because do keep in mind the inversion of colors that tends to take place between these two libraries. So now another thing that I want to do is we've essentially converted the BGR to grayscale, we've essentially converted BGR, HSV BGR to RGB BGR to RGB, what we can do is we can do the inverse of that, we can convert a grayscale image to BGR, we can convert an HSV to BGR, we can convert an LNB to BGR, and RGB to be GL, and so on. But here's one of the downsides. You cannot convert grayscale image to HSV directly. If you wanted to do that, what do you have to do is convert the grayscale to BGR. And then from video to HSV. So we're gonna do that real quick. So we're gonna say HSV, two BGR. Okay, so the first thing we do is HSV, underscore vgr. Basically, converting from HSV to BGR is equal to CV dot CVT color, this will take in the HSV image. And the color code will be color on Cisco HSV, two BGR. And we can display this image, let's call this HSV, two BGR and pass in HD on the scope BGR. On screw VR, save that and run. Okay, we're not interested in this. So let's close this out. But essentially, this is the HSV, two BGR image. If this was the HV image, we converted this image to BGR. And we can try this with lamb. So let's call this lamb to lamb, and of course, lamb. And let's copy this and paste that. We can get rid of Mapplethorpe's it's been addressed in an email. So go out and run. Okay, that was a mistake. We said HSV, lamb to L baby to BGR. That was my mistake. Cool. So if this was the lamb version, this is the lamb to BGR version back from BGR to lamb and from lamb to BGR. So that's pretty much it. For this video, we discussed how to convert, we discussed how to convert between color spaces from BGR to grayscale, HSV, LGB, and RGB. And if you want to convert from grayscale to nav, for instance, note that there is no direct method, what you could do is convert that grayscale to BGR. And then from BGR to and maybe that's possible. By directly. I don't think there was a way to do that, if open CV could come up with the feature like that, it would be good, but it's not gonna hurt you to write extra lines of code, at least two or three lines of code extra, moderately hard. In the next video, we will be talking about how to split and merge color channels in open CV. If you have any questions, leave them in the comments below. Otherwise, I'll see you guys in the next video. Everyone and welcome back to another video. In this video, we're going to be talking about how to split and merge color channels in open CV. Now, a color image basically consists of multiple channels, red, green, and blue. All the images you see around you all the BGR or the RGB images are basically these three color channels merged together. Now open CV allows you to split an image into its respective color channels. So you can take a BGR image and split it into blue, green and red components. So that's what we're going to be doing in this video, we're going to be taking this image of the park that we had seen in previous videos, and we're going to split that into its three color channels. So the way we do that is by saying b comma g comma r, which stands for the respective color channels, and set this equal to CV dot split split of the image. So the CV dot split basically split the image into blue, green and red. And we can display this image by saying CV dot I'm sure, let's call this blue and pass in blue. And let's do the same for green image and pass in G and we can do the same for the red part two are and we can actually visualize the shape the shapes of these images. So let's first print the image node shape, and then print the bead on shape. And then print the genome shape and then print the our dot shape. Basically, we're printing the shapes and dimensions of the image and the blue, green and red and we're also displaying these images. So let's run Python split merge dot p Why. And these are the images that you get back. This is the blues, the blue image, this is the green image. And this is the red image. Now these are depicted and displayed as grayscale images that show the distribution of pixel intensities. regions where it's lighter showed that there is a far more concentration of those pixel values and regions where it's darker, represented a little or even no pixels in that region. So take a look at the blue pick the blue channel first. And if you can, if you compared with the original image, you will see that the sky is kind of almost white, this basically shows you that there is a high concentration of blue in the sky, and not so much in the the trees or the grass, let's take a look at the green. And there is a fairly even distribution of pixel intensities between the between the grass, the trees, and some parts of the sky. And take a look at the red color channel. And you can see that parts of the trees that are red are whiter and the grass in the sky are not that white in this red image. So this means that there is not much red color in those regions. Now coming back, let's take a look at the shapes of the image. Now this stands for the original image, the BGR image, the additional elements in the tuple here represents the number of color channels, three represents three color channels blue, green, and red. Now if we proceeded to display the shapes of BG and our components, we don't see a three in the tuple. That's because the shape of that component is one. It's not mentioned here, but it is one. That's why when you try to display this image using see even if I'm show it displays it as a grayscale image, because grayscale images have a shape of one. Now, let's try and merge these color channels together. So the way we do that is by seeing the merge image, merged images equal to CV dot merge. And what we do is we pass in a list of blue of blue comma g comma r, I'd save that in let's display that things either on show call this them call this the merged image. And we can pass in merged. So let's save that and run. And we get back the merged image by basically merging the three individual color channels red, green, and blue. Now there is a way an additional way of looking at the actual color there is in that channel. So instead of showing you grayscale images, it shows you the actual color involved. So for the blue image, you get the blue color channel for the red channel, you get the red color for that channel. And the way we do that is we actually have to reconstruct the image. The shapes of these images are basically grayscale images. But what we can do is we can actually create a blank image, a blank image using NumPy. And essentially, what we're going to do is we're going to say blank is equal to NP dot zeroes. And we're going to set this to the shape of the image, but only the first two values. And we can give it a data type of you iemt, eight, eight, which basically are for images. And to print the blue color channel, what we're going to do is we're going to say, down here, we're going to say blue is equal to CV dot image, we're going to pass in the list of b comma, blank comma blink. And we're going to do the same thing for green and set is equal to CV dot merge of blank comma g comma blank. And we're going to do the same thing for red by setting this equal to CV dot merge of blank comma blink, comma, comma red. Basically, what I've done is this blank image basically consists of the height and the width, not necessarily number of color channels in the image. So by essentially merging the blue image in its respective compartment, so blue, green and red, we are setting the green and the red components to black and only displaying the blue channel. And we're doing the same thing for the green by setting the blue and the red components to black. And the same thing for red by setting the blue and the green components to black. And we can display this by saying blue, green, and red. Let's save that and run and now you actually get the color in its respective color channels. Take a look at this, you now be able to visualize the distribution much better. Here you can see lineup later portions represent a high distribution. Lighter portions here represent the high distribution of red and higher and wider regions represent a high distribution of green. So essentially, if you take these three images of these color towns and merging them together, you essentially get back the merged image. That's the merged image. So that's pretty much it. For this video, we discuss how to split an image into three respective color channels, how to reconstruct the image to display the actual color involved in that channel, and how to merge those color channels back into its original image. In the next video, we'll be talking about how to smooth and blur an image using various blurring techniques. If you have any questions, leave them in the comments below. Otherwise, I'll see you guys in the next video. Hey, everyone, and welcome back to another video. In this video, we're gonna address the concepts of smoothing and blurring in urban CV. Now, before I mentioned that we generally smooth and image when it tends to have a lot of noise, and noise that's caused from camera sensors are basically problems in lighting when the image was taken. And we can essentially smooth out the image or reduce some of the noise by applying some blurring method. Now Previously, we discussed the Gaussian Blur method, which is kind of one of the most popular methods in blurring. But generally, you're going to see that Gaussian Blur won't really suit some of your purposes. And that's why there are many blurring techniques that we have. And that's what we're going to address in this video. Now, before we actually do that, I do want to address a couple of concepts. Well, let's actually go to an image and discuss what exactly goes on when you try to apply blur. So essentially, the first thing that we need to define is something called a kernel or window. And that is essentially this window that you draw over an image that has two lines here. Let's draw another line. So this is essentially a window that you draw over a specific portion of an image. And something happens on the pixels in this window. Let's change it to blue. Yeah. So essentially, this window has a size, this size is called a kernel size. Now kernel size is basically the number of rows and the number of columns. So over here, we have three columns and three rows. So the kernel size for this is three by three. Now, essentially, what happens here is that we have multiple methods to apply some blue. So essentially, blur is applied to the middle pixel as a result of the pixels around it, also called the surrounding pixels. Let's change that to a different color. So something happens here as a result of the pixels around the surrounding pixels. So with that in mind, let's go back and discuss the first method of blurring which is averaging. So essentially, averaging is we define a kernel window over a specific portion of an image, this window will essentially compute the pixel intensity of the middle pixel of the true center as the average of the surrounding pixel intensities. So if this was to green, suppose if this pixel intensity was one, this was maybe two, this is 345678, you get the point. Essentially, the new pixel intensity for this region will be the average of all the surrounding pixel intensity. So that's summing up one plus two plus three plus four plus five plus six plus seven plus eight, and dividing that by eight, which is essentially the number of surrounding pixels. And we essentially use that result as the pixel intensity for the middle value, or the true center. And this process happens throughout the image. So this window basically slides to the right. And once that's done, it slides down, and computed basically for all the pixels in the image. So let's try to apply and see what this does. So what we're going to do is we're going to say average, is equal to CV don't blur. The CV or blow method is a method in which we can apply averaging blur. So we define the source image which is IMG, we give it a kernel size of let's say three by three. And that's it. We can display this image called as average, average blur. Save that and run Python smoothing dot p y net Gosh, we have to pass an average, save that and run. And this is basically the average blow that's applied. So what the algorithm did in the background was essentially define a candle window of a specified size three by three. And it computed the center value for a pixel using the average of all the surrounding pixel intensities. And the result of that is we get a blurred image. So the higher kernel size we specified, the more blur there is going to be in the image. So let's increase that to seven by seven and see what that does. And we get an image with way more blur. So let's move on to the next method, which is the Gaussian Blur. So Gaussian basically does the same thing as averaging, except that instead of computing the average of all of this running pixel intensity, each running pixel is given a particular weight. And essentially, the average of the products of those weights gives you the value for the true center. Now using this method, you tend to get less blurring than compared to the averaging method. But the Gaussian Blur is more natural as compared to averaging. So let's print that out. Let's call this Yes. And set this equal to CV dot Gaussian Blur. And this will take in the source image, so IMG kernel size of seven by seven, just to compare with the averaging. And another parameter that we need to specify is sigma x, or basically the standard deviation in the x direction, which for now, just going to set as zero. And we can put that out, call this Gaussian Blur and pass in gaps, save that and run. If you can bear with this, you see that both of them use the same code size, but this is less blurred as compared to the average method. And the reason for this is because a certain weight value was added when computing the blur. Okay, so let's move on to the next method. And that is median blur. So let's go back to our image. And medium blurring is basically the same thing as averaging, except that instead of finding the average of the surrounding pixels, it finds the median of the surrounding pixels. Generally, medium blurring tends to be more effective in reducing noise in an image as compared to averaging and even Gaussian Blur. And it's pretty good at removing some salt and pepper noise that may exist in the image. In general, people tend to use this image in advanced computer vision projects that tend to depend on the reduction of substantial amount of noise. So let's go back here. And the way we apply the blur is by saying, let's call this median and set the Z and set this equal to CV dot median, blue, we pass in the source image, and this kernel size will not be a tuple of three by three, but instead, just an integer to three. And the reason for this is because open CV automatically assumes that this kernel size will be a three by three, just based off this integer. And we can print this out. Let's call this median, blue, and pass in median. And let's compare it with that. So I set that to seven. And comparing it with Gaussian Blur, and averaging blur, you tend to look at this. And you can make up some differences between the two images. So it's like as if this was your painting, and it was still drawing. And you take something and smudge over the image and you get something like this. Now generally, medium blurring is not meant for high Colonel sizes like seven or even five in some cases, and it's more effective in reducing some of the noise in the image. So let's, let's change this all to three by three. Let's copy that, change that to three by three. And we can change that to three. And now let's have a comparison between the three. This is your Gaussian below. This is your average in blue, this is your median love. So compared with these two, you can see that there is kind of less blurring when Gaussian when you can sort of make out the differences between the two Very subtle, but there are a couple of differences between the two. Finally, the last method we're going to discuss is bilateral blurring caused by natural lateral. Now bilateral bearing is the most effective, and sometimes used in a lot of advanced computer vision projects, essentially because of how it blurs. Now traditional blurring methods basically blur the image without looking at whether you're, whether you're reducing edges in the image or not. bilateral blurring applies blurring but retains the edges in the image. So you have a blurred image, but you get to retain the edges as well. So let's call this bilateral and multilateral and set this equal to CV dot bilateral filter. And we pass in the image, we give it a diameter of the pixel neighborhood. Now notice this isn't a kernel size, but in fact, a diameter. So let's set this to five for now, give it a sigma color, which is basically the color sigma sigma color, a larger value for this color sigma means that there are more colors in the neighborhood, that will be considered when the blue is computed. So let's set this to 15. For now. And sigma space is basically your space sigma. larger values of this space, sigma means that pixels further out from the central pixel will influence the blurring calculation. So let's set this to 50. So let's take a look at that sigma spacing. So for example, in bilateral filtering, if this is the value for this central pixel, or the true center is being computed, by giving a larger values for the Sigma space, you essentially are indicating that whether you want pixels from this far away, or maybe this far away, or even this far away from influencing this particular calculation. So if you give like a really huge numbers, then probably a pixel in this region might influence the computation of this pixel value. So let's set this to 15. For now, and let's display this image. So call the cv.on show is called as bilateral and pass on bilateral. Let's save that and run. And this is your bilateral image. So let's compare with all the previous ones that we had. Compared with this. Much better compared with averaging way much better. Let's compare with median. The edges are slightly, it's slightly blurred. If you compare with the original image, they kind of look the same thing. Okay, it kind of looks like there's no blur applied. So maybe let's increase this diameter to I know 10. And not much was done, the edges are still there, it kind of looks like the original image itself. So let's try into one of the other parameters. Let's add this to 3435. Let's set this dude 25. We're only playing around with these with these values. And now you can basically make our generic that this is starting to look a lot like median blow. We need even larger values. It's starting to show you that this is more looking like a smudged painting version of this image, right, there's a lot of blur applied here, but the council looking smudged. So definitely keep that in mind when you are trying to apply blurring the image, especially with the bilateral and median lowering, because higher values of this basic mouth or bilateral or the kernel size for medium glowing, and you tend to end up with a washed up smudged version of this image. So definitely keep that in mind. But that kind of summarizes whatever we we've done in this video, we discussed averaging, Gaussian, median and bilateral blurring. So in the next video, we'll be talking about bitwise operators in open CV. So again, like always, if you have any questions, leave them in the comments below. Otherwise, I'll see you guys in the next video. Hey everyone, and welcome back to another video. In this video we're gonna be talking about bitwise operators in urban CV. Now, there are four basic bitwise operators and or XOR and not. If you've ever taken an introductory CS course, you will probably find these terms familiar bitwise operators, and they are in fact used a lot in image processing, especially when we're working with masks like we'll do in the next video. So at a very high level bitwise operators operate in a binary manner. So a pixel is turned off if it has a value of zero, and is turned on if it has a value of one. So let's actually go ahead and try to import NumPy as NP. And what I'm going to do is I'm going to create a blank variable and set this equal to NP dot zeros of size 400 by 400. And we can give it a datatype of you I empty it is what I'm going to do is I'm going to use this blank variable as a basis to draw a rectangle and draw a circle. So I'm going to say return angle is equal to CV dot rectangle, we can say blink dot copy. And we can pass in the starting point. So let's give it a margin of around 30 pixels on either side. So we're going to start from 30, comma 30. And we can go all the way across to 370370. And we can give it a color. Since this is not a color image, but rather binary image, we can just give it one parameter, so 255. White, and give it a thickness of negative one, because we want to fill this image. And then I'm going to create another circle variable and set this equal to CV dot circle, we're going to say blank, don't copy, we are going to give it a center. So the center will be the absolute center, so 200 by 200. And let's give it a radius of give a radius of 200. And give it a color up to five, five, and let's fill in the circle. So negative one. So let's display this image and see what we've seen or we're working with. So we'll call this rectangle and passing the rectangle. And we're going to do the same thing with the circle, it's called a circle. And pass in the circle, save that and run Python bitwise r p y. So we have two images that we're going to work with this image of rectangle, and this image of a circle. So let's start off with the first basic bitwise operator, and that is bitwise. And so before we actually discuss what bitwise ad really is, let me show you what it does. So essentially, what I'm going to do is I want to say bitwise is go and is equal to CV dot bitwise. And, and basically what I have to do is pass in two source images that are these two images, rectangle, and circle. Now we can display this image, let's call this beautiful lines, and let's pass in bitcoins and save, run. And essentially, you get back this image. So essentially, what bitwise AND did was it took these two images, placed them on top of each other, and basically returned the intersection. Right, and you can make out when you take this image, put it over this image, you have some triangles that are common to both of these images. And so those are set to black zero, while the common regions are returned. So the next one is basically bitwise. Or now bitwise, or real simply returns both the intersecting as well as the non intersecting regions. So let's try this bitwise OR is equal to CB dot bitwise AND scope or you pass in rectangle, we pass in circle. Now we can print that, let's call this bitwise OR pass in bitwise. Oops, that was or save that and run and bitwise OR, okay, there's a bitwise OR, by mistake. It's a bitwise OR basically return this funky looking this funky looking shape. Essentially what it did is it took these two images, put them over each other from the common regions and also found regions that are not common to both of these images and basically superimpose them. So, basically, you can just put them together and find the resulting shape and this is what you get, but this image over this and you get this moving on. The next one is bitwise XOR, which basically is good for returning the non intersecting regions. So this found the the intersecting oops, the inter setting regions this found the sky Brought back, the no one intersecting in interest selecting regions, and xR only finds the non intersecting regions. So let's do that I say bitwise call this XOR is equal to CV dot bitwise underscore xR, we pass in the rectangle, passing the rectangle when we pass in the circle, we can display this CV and I'm sure close bitwise XOR. And we can pass in bitwise XOR. Save that and run. And here we have the non intersecting regions of these two images when you put them over each other. Pretty cool. And just to recap, this bitwise AND AGAIN, returns the intersection regions bitwise, or returns the knowledge second regions as well as the intersecting regions bitwise XOR, returns the knowledge second regions. So essentially, if you take this bitwise XOR, and subtract it from bitwise, or you get bitwise end. And conversely, if you subtract bitwise, and from the device, or you get bitwise XOR. Just so essentially, that's a good way of visualizing what exactly happens with these bitcoins operators. And finally, the last method we can discuss is bitwise. Not essentially, it doesn't return anything. What it does is it inverts the binary color. So let's do that. So let's call this bitwise. Not is equal to CV dot bitwise. underscore not. And this only takes in one source image. So let's set this to the rectangle put out. And we can display this. Let's call this rec tangle not, we can pass in bitwise not see that. And basically what it did is if you look at this image, it found all the white regions, all the white pixels in the image and inverted them to black and all the black images it inverted to white, essentially, it converted the white to black and from the ads from black to white. So we can try that with the circle. Let's call this circle, we can pass in the circle here. Save and Run and the resultant the resulting circle, not that you get is this. This is white hole. This is a black hole for physicists out there. Okay, so that's pretty much it. For this video, I just wanted to introduce you all to the idea of bitwise operations and how it works. In the next video, we'll be actually talking about how to use these bitwise operations in a concept called masking. So if you have any questions, leave them in the comments below. Otherwise, I'll see you guys in the next video. Hey, everyone, and welcome back. In this video, we're going to be talking about masking in open CV. Now in the previous video, we discussed bitwise operations. And using those bitwise operations, we can essentially perform masking in open CV masking essentially allows us to focus on certain parts of an image that we'd like to focus on. So for example, if you have an image of people in it, and if you're interested in focusing on the faces of those people, you could essentially apply masking and essentially mask over the people's faces and remove all the unwanted parts of the image. So that's basically our high level intuition behind this. So let's actually see how this works in open CV. So I basically read in a file and display that image. The other thing I'm going to do is I'm going to import NumPy NumPy as NP, what I'm going to do is I'm going to say blank is equal to NP dot zeros of size of size image dot shape with the first two values. Now this is extremely important, the dimensions of the mask have to be the same size as that of the image. If it isn't, it's on good work. And we can give it a data type of UI eight, you can see it if you want to display this, we can display this. It's just going to be a black image, schools blank image and pawson blank. Essentially, what I'm going to do is I'm going to draw a circle over this blank image and call that my mask. So I'm going to say mask is equal to CV dot circle. We're going to draw the blank image on the blank image, we can give it a center of this image so let's say image dot shape of Have one divided by two divided by two, and image down shape of two image a shape of zero divided by divided by two. And we can give it a radius of, I don't know, I'd say 100 pixels, give it a color of 255, give it a thickness of negative one. And we can visualize a mask as mask and passing mask. So let's run that. Python masking dot p y. And this is essentially our mask. There's the blank image we're working with. And this is the image that we want to mask over. So let's actually create a masked image, we're going to say masked image is equal to CV dot bitwise. underscore and this source image. So IMG, IMG, and we specify the parameter mask is equal to mask, which is this circle image over here. And we can display this image, call this masked image. And we can pass in masked, save that and run. And this is essentially your masked image, you took this image, you took this image, you put this image over and found the intersecting region. Okay, by optionally passing the mask is equal to mask. That's exactly what we're doing. Cool. That's right. And, you know, play around with this, let's maybe move this by a couple of pixels around, let's say 45. Save and Run moves down to zero, okay, this has to be 45 plus 45, save up and running. And we get the image of the cat, we can draw, we can draw a circle, or we can draw a rectangle instead. What's bottom blank, skip that. Let's give you that in draw, give it a static endpoint of let's copy this and add a couple of pixels or maybe 100 pixels this way, in 100 pixels. This way, we can get rid of this, we don't need that and say that, right? This is this, this is the square. And this is essentially the masked image. So let's actually try this with. So let's actually try this with a different image. So we have got an image. Let's try it with maybe these cats too. Let's go back to cats to save that run. And this is the mask that we get by putting these two on each other. And essentially, you can play around with these as you feel fit. You can maybe try different shapes, weird shapes. And the way you can do get these weird shapes, essentially creating a circle or rectangle and applying bid wise and you get this weird shape. And then you can use that weird shape as your mask. So let's just try that. Let's let's try that. Oh, we're going to say let's, let's call this circle and blanked out copy copy and create a rectangle. Let's just grab it from this re read Where are we from bitvise Let's grab this rectangle copy that piece over time the copy 3030 Okay, blank, same shape. So let's create this weird weird shape is equal to CV dot bitcoins on the scope end of this circle this rectangle and we don't need to specify anything else. um what's one of visualizes let's close this out try to see see it on on show call this the weird shape passing the weird shape and wrong. masking undefined was mask westmar Mosque Okay. Good. This is the weird shape that we get. We're not really going for a half moon But hey, whatever. Let's close this out. Use this weird shape is mask. So use weird shape as a mask and let's see the final mask image and this is essentially your weird weird shape, masked image. Let's call this a weird shape mask image, weird shaped mask damage. This little halfmoon here. And essentially you can, you can do pretty much anything you want with this, you can experiment with various shapes and sizes and stuff like that. But just know that the size of your mask has to be at the same dimensions as that of your image. If you want to see why not maybe subtract 100 pixels possible, but let's support it, though. So that's maybe like subtract tubal on it. I don't know whether that'll work. But guess what? Okay, so let's just say, image on shape of while I'm okay, let's just give it a different size. What are we? Why are we even using image, let's go this size of 300 by 300. Definitely not the size of this. And we get this assertion failed m time, blah, blah, blah, maskhadov, same size, in function, whatever. So essentially, these need to be at the same size, otherwise, it's going to fail and throw you an error. So that's it for this video, we talked about masking, again, nothing to do different. We've essentially used the concept of bitcoins and from the previous video, and you will see that when we move on to computing histograms in the next video, where masking really comes into play, and how masking really affects your histograms. So if you have any questions again, leave them in the comments below. Otherwise, I'll see you in the next video. Hey, everyone, and welcome back to another video. In this video, we're going to be talking about computing histograms in open CV. Now histograms basically allow you to visualize the distribution of pixel intensities in an image. So whether it's a color image, or whether it's a grayscale image, you can visualize these pixel intensity distributions with the help of a histogram, which is kind of like a graph or a plot that will give you a high level intuition of the pixel distribution in the image. So we can compute a histogram for grayscale images and compute a histogram for RGB images. So we're gonna start off with computing histograms for grayscale images. And so let's just convert this image to grayscale is activity don't CVD color, pass the image and give it a color code of of color underscore BGR. To gray, it means read this image with gray and passing Great. Now to actually compute the grayscale histogram. What we need to do is essentially call this gray underscore hist and set this equal to CV dot calc hist. This method will essentially compute the histogram for the the image that we pass into. Now this images is a list, so we need to pass in a list of images. Now since we're only interested in computing a histogram for one image, let's just pass in the the grayscale image, there thing we have to pass in is the number of channels which basically specify the index of the channel we want to compute a histogram for that since we are computing the histogram for a grayscale image, let's wrap this as a list and pass in zero. The next thing we have to do is provide a mask do we want to compute a histogram for a specific portion of an image, we will get to this later. But for now just have this to num. His size is basically the number of bins that we want to use for computing the histogram. Essentially, when we plot a histogram, I'll talk about this concept of bins. But essentially, for now, just set this to 256 wrapped as a list. And that's wrapped out as list. And the next thing I want to do is specify the range of the range of all possible pixel values. Now for our case, this will be 02256. And that's it. So to prop this image, let's actually use matplotlib. So import map plot matplotlib.pi plot as PLT, and then we can instantiate of PLT dot figure, a PLC figure. Let's give it a tidy, let's call this gray kale histogram. We can essentially give it a label across the x axis and we're going to call this bins. Let's give this a y label and set this equal to the number of pixels. The number Have pixels. And that's why label. And finally, we can plot by saying PLT dot plot the, the grayscale histogram. And Valley, we can essentially give it a limit across the x axis. So PLT dot x Lim have a list of 02256. And finally, we can display this image. So PLT dot show, save that and run Python histogram, dot p y. And this is the distribution of pixels in this image. As you can see, the number of bins across the x axis basically represent the the intervals of pixel intensities. So as you can see that there is a peak at this region, this means that this is close to 5060 ish. So this means that in this image, there are close to 4000 pixels that have an intensity of 60. And as you can see that there's a lot of, there's a lot of peeking in this region, so between probably 40 to 70, there is a peak of pixel intensities of close to 3000 pixel intensities in this image. So let's try this with a different image. Let's try this with a cants. I'm just going to save that and run. And there is a peaking of pixel values in between 202 25. And this makes sense because most of the image is white. So given that reason, you can probably deduce that there will be a peak into words white or 255. Five. So this is essentially computing the grayscale histogram for the entire image, what we can do is we can essentially create a mask, and then compute the histogram only on that particular mask. So let's do that. Let's go back to masking. Let's grab this, grab this. Let's go right up there. I set this to image dot shape of the first two values the sizes of the same. Let's essentially draw a mask, which will be CV dot circle of all blank. And we can get the center of image into a shape of one by by divided by two, image doing shape of zero divided by two over two, give it a radius of 100 pixels, give it a color of 245 give it a thickness of negative one, we can display a mask let's call this let's call as mask policy mask. And here's where things get interesting. We can get the grayscale histogram for this mask. And the way we do that is by setting this mask parameter to mask two instead of none. We set this to mask and let's see what that does to our histogram MPs and undefined great. And I couldn't make this kind of made a mistake here. Oh, that's right. This is the Masters not exactly the Masters is circle. This is a this will be a circle circle. And essentially we need to mask out the image so we so the way we do that is by creating a mask and setting this equal to CV dot Bitcoins. bitwise unscored, and we can pass in the grayscale image the grayscale image, and we can pass in the mask which is equal to circle. Now we can use that as the mask. So let's display that x Sorry, I made a mistake, but hopefully things should be fine right now. So this is the mask and this is the histogram computed for this particular mask. As you can see that there is a peaking of pixel intensity values in this region. And there are smaller pickings in in these regions down below. Let's try this with another image. Let's pass in the cats cats to the cats though jpg. This is our mask and this is the there is a peaking in this image towards 50. Okay, so that was it for computing grayscale histograms. Let's move on to true To compute a color histogram, that is to compute a histogram for a color image to an RGB image. So let's call this color histogram. And the way we do that is, instead of converting this image to grayscale, let's comment all of this out. We will use a mask later. That's come in all of this out. There is mask will be for IMG, IMG. And yeah, that's pretty much it. So let's start with the color histogram. The way we do that is let's define a tuple of colors, and set this equal to b, then tuple of G, a tuple element of R. And what I'm going to do next is I'm going to say for our common call in enumerate of colors. What I'm going to do is I'm going to say hist. So I'm going to plot the histogram by saying CV dot calc hist, we're going to compute it over the image itself, the channels will be I mean, this eye over here, we're going to provide a mask of none for now. Give it a his size of 256 and give it a ranges of 02256. And then let's do a PLT dot plot hist and give it a color equal to call. And only we can do a PLT dot x Lim of 02202256. And for this purpose, we can essentially grab this, copy that uncomment this out. And we can do a PLT dot show. So this should work. We're missing something Oh, no, don't think of him. We're not, we're not computing this histogram for a mask, or we live there next. But let's save that run. Oh, cool. And let's close enough, I made a mistake, this is a color histogram shouldn't make much of a difference. So this is the color histogram that we get for the original image not for a mask. But in fact, this image. So as you can see that this color image basically computed the plot for blue channel, the red channel and the green channel as well. So using this, you can basically make out that there is a peaking of blue pixels that have a pixel intensities of 30. There's a peaking of red, probably around 50, peaking of green, probably around 8075 to 80. Cool and using this, you can basically make up the distribution of pixel intensities of all three color channels. So let's try and apply a mask by setting this equal to mask. Let's see whether we have everything in order. It's a bit more than mass mass, mass mass mass masks. Masks are not the same size, okay, I finally got the error. So basically, the mass needs to be a binary format. So instead of passing in this mask, this will actually be the masked marks image, Regan passes me fat mask, and we can change the circle to mask. Now this should work without any arrows. And we can change that to masked. Yeah, that's around that. And now we get the color histogram for this particular mask, I made a mistake because I use this as my mask to compute the histogram for one channel. The problem was this masked image was actually a three channels and I attempted to use this s3 channeled mask to calculate the histogram per channel, which isn't allowed in open CV. So that was my mistake. What kind of use the wrong variable names so confused, but essentially, this is it, you're computing the histogram for a particular section of this image. And this is what you get there is a high peaking of red in this area, high peaking of blue in this era, and high peaking of greens I'm over here. So essentially, that's it for this video. histograms actually allow you to analyze the distribution of pixel intensities, whether for a grayscale image or for a colored image. Now these are really helpful in a lot of advanced computer vision projects. When you actually trying to analyze the image that you get, and maybe try to equalize the image so that there's no peeking of pixel values here and there. In the next video, we'll be talking about how to thresh hold an image and the different types of thresholding. As always, if you have any questions, leave them in the comments below. Otherwise, I'll see you guys in the next video. Hey, everyone, and welcome back to another video. In this video, we're going to be talking about thresholding in open CV. Now, thresholding is a binary realisation of an image. In general, we want to take an image and convert it to a binary image that is an image where pixels are either zero or black, or 255, or white. Now, a very simple example of thresholding would be to take an image and take some particular value that we're going to call the thresholding value. And compare each pixel of the image to this threshold of value. If that pixel intensity is less than the threshold value, we set that pixel intensity to zero. And, and if it is above this threshold value, we set it to 255, or white. So in this sense, we can essentially create a binary image just from a regular standalone image. So in this video, we're actually going to talk about two different types of thresholding, simple thresholding and adaptive thresholding. So let's start off with simple thresholding. So in essence, what I want to do is, before I talk about simple thresholding, is I want to convert this BGR image to grayscale. So I'm going to say gray is equal to CV dot CVT color, we pass in the image, we pass in the color code, which is vgr. To correct, we can display this image called this gray, we can pass in great. Cool. So let's start off with the simple thresholding. So essentially to to apply this this idea of simple thresholding, we essentially use the CV dot threshold function. Now this function returns a threshold, and Thresh, which is equal to CV dot threshold. And this in essence takes in the grayscale image, the grayscale image has to be passed in to this thresholding function, then what we do is we pass in a threshold value. So let's set this to 150 for now, and we have to specify something called a maximum value. So if that pixel value is greater than is greater than 150, what do you want to set it to, in this case, we want to binarize the image. So we set it to 245. And finally, we can specify a thresholding type. Now this thresholding type is essentially CV dot thrush underscore binary. And what this does is basically it looks at the image compares each pixel value to this threshold value. And if it is above this value, it sets it to 255. Otherwise, it infers that if it falls below, it sets it to zero. So essentially returns two things trash, which is the thresholded image or the binarized image and threshold, which is essentially the same value that you passed 150, the same threshold value you pass in, will be returned to this threshold value. So let's actually display this image. So let's say cv.rm show, we'll call this threshold. We'll call this simple thresh hold dead, and we can pass in thrash. So let's save that and run Python thrash. Da p y in this is a thresholded image that you get. Again, this is nothing too different from when we discussed thresholding in the in one of the previous videos, but this is essentially what you get. So let's play around with these threshold values. Let's set this to 100. And let's see what that does. And as a result, both parts of the image have become white. So and of course, if you give it a higher value, less parts of the image will be white. So let's set this to 225. And very few pixels in this thresholded image actually have a pixel intensity of greater than 225. So what we can do after this is essentially create an inverse thresholded image. So what we could do is we could essentially copy this and instead of saying Thresh, I'm going to say thrush underscore inverse, and I'm going to leave everything else the same. Let's set this to 150. And the same thing here, and instead of passing in the type of thresholding, I'm going to say CV dot Thresh underscore binary under scope inverse. And let's call this thresholded inverse. And we can pass in inverse. So let's save that and run. And this is essentially the inverse of this image, instead of setting pixel intensities that are greater than 150 to 255, it sets whatever values that are less than 150, to 255. So that's essentially what you get. Right, all the black parts of this image will change to white, and all the white parts of the image will change to black. Cool. So that's a simple threshold. Let's move on now to adaptive threshold data thresholds. Now, as you can imagine, we got different images, when we provided different threshold values. Now, kind of one of the downsides to this is that we have to manually specify a specific threshold value. Now, some cases this might work, in more advanced cases, this will not work. So one of the things we could do is we could essentially let the computer find the optimal threshold value by itself. And using that value that refines it binary rises over the image. So that's an essence the entire crux of adaptive thresholding. So let's set up a variable called adaptive on its growth Thresh. And set this equal to CV dot adaptive threshold. And inside I want to pass in a source image. So let's set this to gray, I'm going to pass in a maximum value, which is 255. Now notice there is no threshold value. adaption method basically tells machine which method to use when computing the optimal threshold value. So for now, we're just going to set this to the mean of some neighborhood of pixels. So let's set this to CV dot adaptive on the scope Thresh. And score mean underscore C. Next, we'll set up a threshold type. This is CV dot Thresh. underscore binary, which again, I think do different from this from the first example. And two other parameters that I want to specify is the block size, which is essentially the neighborhood size of the kernel size, which open CV needs to use to essentially compute mean to find the optimal threshold value. So for now, let's set this to 11. And finally, the last method we have to specify is the c value. Now this c value is essentially an integer that is subtracted from the mean, allowing us to essentially fine tune our threshold. So again, don't worry too much about this, you can set this to zero. But for now, let's set this to three. And finally, once that's done, we can go ahead and try to display this image. So let's call this adaptive thresholding. And we can pass in adaptive cash. So let's save that and run. And this is essentially your adaptive thresholding method. So essentially, what we've done is we've defined a kernel size or window that is drawn of this image. In our case, this is 11 by 11. And so what open CV does is it essentially computes a mean over those neighborhood pixels, and finds the optimal threshold value for that specific part. And then it slides over to the right, and it slides, it does the same thing. And it's lines down and does the same thing so that it essentially slides over every part of the image. So that's how adaptive thresholding works. If you wanted to fine tune this, we could change this to a threshold, just go binary and scope inverse, you're just to see what's really going on under the hood. Cool. So all the white parts of the image will change the black and all black parts of the image have changed white. So let's play around with these values. Let's set this to probably 13 and see what that does. Okay, definitely some difference from the previous hyper parameter. So let's try it. Let's go with let's set this to 11. And let's set this to maybe one. Okay, definitely more white. Let's set this to maybe five in a row that you can play around with these values, right, the more you subtract from the mean, the more accurate it is, right, you can basically make out the edges now in this basket. So let's maybe increase that to nine. And you get less white spots in the image. But essentially, now you can make the features better. Cool. So that was essentially adaptive thresholding, adaptive thresholding that essentially can Did the optimal threshold value on the basis of the mean? Now we don't have to stick with the mean, we can go with something else. So instead of mean, let's set this to Gaussian. So let's save that and see what that does. And this is the thresholded image using the Gaussian method. So the only difference that Gaussian applied was essentially add a weight to each pixel value, and computed the mean across those pixels. So that's why we were able to get a better image than when we use the mean. But essentially, the adaptive thresholding mean works. In some cases, the Gaussian works in other cases, there's no real one size fits all. So really play around with these values, see what you get. But that's essentially all we have to discuss. For this video, we talked about two different types of thresholding, simple thresholding and adaptive thresholding. In simple thresholding, we have to manually specify a threshold value. And in adaptive thresholding, open CV does that for us using a specific block size, or current size and other computing the threshold of value on the basis of the mean, or on the basis of the Gaussian distribution. So in the next video, the last video in the advanced section of this goes, we're going to be discussing how to compute gradients and edges in an image. So if you have any questions, leave them in the comments below. I'll be sure to check them out. Otherwise, I'll see you guys in the next video. Thanks for watching, everyone, and welcome back to another video. In this video, we're going to be talking about gradients and edge detection in urban CV. Now, you could think of gradients as these edge like regions that are present in an image. Now, they're not the same thing gradients and edges are completely different things from a mathematical point of view. But you can pretty much get away with thinking of gradients as edges from a programming perspective only. So essentially, in the previous videos, we've discussed the canny edge detector, which is essentially kind of an advanced edge detection algorithm. That is essentially a multi step process. But in this video, we're going to be talking about two other ways to compute edges in an image. And that is the lat placing and the Sobel method. So let's start off with the left place here. So the first thing I want to do is I want to convert this image to grayscale, recalling the CVT. DVD to color color method, we pass in the image, and we say CV color on describe BGR to grip, we can display this image is called as gray. And we can pass in every pass. Great. So let's start with the Laplacian. So we're going to define a variable called lap and set this equal to CV dot lap lesion. And what this essentially will do is it will take in a source image, which is great now, and it will take in something called a D depth or data depth. Now for now when we set this to CV dot 64, F is for long with whatever I do next, I'm going to say lap is equal to NP dot u 98. And instead I'm going to pass an NP dot absolute. And we can pass in lap. And since I'm using NumPy, I can actually go ahead and import NumPy as NP and when I go to display this image coil CV dot I'm sure method is called this lamp lesion. And we can pass on lap lap Save and run a call this Python good radians dot p y invalid syntax CV dot Okay, it's cv.cv on score 64 F. Say that. And this is essentially the law placing edges in the image kind of looks like an image that is drawn over a chalkboard and then smudge just a bit. But anyway, this is the lab laser method. Let's try this with another image. Let's try this with this park called Boston. Let's call this the park. Save that in right. And this essentially looks like a pencil shading off this image. It's all the edges that exists in the image, or at least most of the edges in the image are essentially drawn over with the pencil and then lightly submerged. So that's essentially the left lacing edges you could say. So again, don't worry too much about why we converted this to in the UI and then we computed the absolute value. But essentially the Laplacian method computes the gradients of this image the grayscale image. Generally this involves a lot of mathematics but Essentially, when you transition from black to white and white to black, that's considered a positive and a negative slope. Now, images itself cannot have negative pixel values. So what we do is we essentially compute the absolute value of that image. So all the pixel values of the image are converted to the absolute values. And then we convert that to a UI 28 to an image specific datatype. So that's basically the crux of what's going on right over here. So let's move on to the next one. And that is the subtle gradient magnitude representation. So essentially, the way this does is that Sobel computes the gradients in two directions, the x and y. So we're gonna say sobble x, which is the gradients that are computed along the x axis, and Seth is equal to CV dot Sobel. And we can pass in the image, let's add this to the grayscale image, we pass in a data depth, which is cv.cv on school 64 F. And we can give it an x direction. So let's set this to one and the y direction, we can set that to zero. And let's copy this and call it soble. Why, and instead of one, zero, we can save zero comma one. And we can visualize this let's print. Let's call this symbol x, and we can pass in sub x. And we can say it's either long show Sabo y and set this to Sabo y. Call that and these are essentially the gradients that are computed, this is over the y axis. So you can see a lot of y horizontal specific gradients and the sub x was computed across the y axis. So you can see y axis specific gradients. Now we can essentially get the combined Sobel image by essentially combining these two Sobel x and Sobel why, and the way we do that is we're gonna say combined on combined underscore sobald and set this equal to CV dot bitwise. on school or, and we can pass in Sabo x and symbol y. And we can display this image, so let's call CV dot I'm show we get to combined Sobel and we can pass in the combined symbol. Let's run that. And this is essentially the combined sobble that you get. It isn't, let's go back here. So it essentially took these two apply and CV dot bitwise OR, and essentially got this image. So if you want to compare this with lat race in two completely different algorithms, so the results you get will be completely different. Okay, so let's compare both of these left patient and the Sobel with the canny edge detector. So let's go down here. Let's say Kenny is equal to CV, don't, Kenny. And we can pass in the image. So let's possible a grayscale image. Let's give it to threshold values of 150 and 175. And we're done. Let's display this image. Let's call this Kenny, we can pass in Kenny. So let's save that. And let's see what that gives us. So let's compare that with you. So that's essentially it. This is the last place in gradient representation, which essentially returns kind of this pencil shading version of the image of the edges in the image, combined several computes the gradients in the X in the y direction. And we can combine these two with bitwise OR, and Kenny is basically a more advanced algorithm that actually uses Sobel in one of its stages. Like I mentioned, Kenny is a multi stage process, and one of its stages is using the symbol method to compute the gradients of the image. So essentially, you see that the canny edge detector is a more cleaner version of the edges that can be found in the image. So that's why in most cases, you're going to see the Kenny used. But in more advanced cases, you're probably going to see a Sobel use a lot. Not necessarily lap racing. But so definitely. So that's pretty much it for this video. And in fact, this video concludes the advanced section of this course. Moving on to the next section, we will be discussing face detection and face recognition in urban see, we're actually going to touch on using hard cascades To perform some face detection, and face recognition, we actually have two parts. Face Recognition with open CV is built in face recognizer. And the second part will be actually building our own deep learning model to essentially recognize some faces in an image. Again, like always, if you have any questions, leave them in the comments below. Otherwise, I'll see you guys in the next section. Hey, everyone, and welcome back to another video. We are now with the last part of this Python and open CV coasts, where we are going to talk about face detection and face recognition in open CV. So what we're going to be doing in this video is actually discussing how to detect faces in urban CV using something called a har cascade. In the next video, we will talk about how to recognize faces using open CV is built in face recognizer. And after that, we will be implementing our own deep learning model to recognize during the simpson counters, we're going to create that from scratch and use open CV for all the pre processing and displaying of images and stuff like that. So let's get into this video. Now, face detection is different from face recognition. Face Detection merely detects the presence of a face in an image, while face recognition involves identifying whose face it is. Now, we'll talk more about this later on in this course. But essentially, face detection is performed using classifiers. A classifier is essentially an algorithm that decides whether a given image is positive or negative, whether a face is present or not. Now classify needs to be trained on 1000s and 10s, of 1000s of images with and without faces. But fortunately for us, open CV already comes with a lot of pre trained classifiers that we can use in any program. So essentially, the two main classifiers that exist today are har cascades, and mo advanced classifiers core local binary patterns, we're not going to talk about local binary patterns at all in this course. But essentially the most advanced how cascade classifiers, they're not as prone to noise in an image as compared to the hard cascades. So I'm currently at the open CVS GitHub page where they store their whole cascade, there are cascade classifiers. And as you can see, there are plenty of hard cascades that open CV makes available to the general public. You have a hard cascade for an eye, fragile cat face, from face default, full body, your left eye, a Russian license plate, Russian plate number, I think that's the same thing. How cascade to detect smile, Hawk cascade for detection of the upper body, and things like that. So feel free to use whatever you want. But in this video, we're going to be performing face detection. And for this, we're going to use the har cascade underscore frontal face underscore default dot XML. So when you go ahead and open that, you're going to get about 33,000 lines of XML code. So all of this. So what do you have to do is essentially, go to this role button, and you'll get all this raw XML code, all you have to do is click Ctrl A, or Command D if you're on a Mac, and click Ctrl C, or Command C, and then go to your VS code or your editor and create a new file. And we're going to call this har unscrew face dot XML. And inside this, I want to paste in those 33,000 lines of XML code. Go ahead and save that and our classifier is ready. So we can go ahead and close this out. So we're going to be using this Hawk cascade classifier to essentially detect faces that are present in an image. So in this file called face detect, face underscore detected py, I inputted open CV, I basically read in an image of Lady a person, that is this image over here. And we can go real quick and display this. So let's run Python face to face on disco with detect dot p y, and we get an image in a new window. Cool. So let's actually implement our code. The first thing I want to do is convert this image to grayscale. Now face detection does not involve skin tone or the colors that are present in the image. These hard cascades essentially look at an object in an image and using the edges tries to determine whether it's a face or not. So We really don't need color in our image. And we can go ahead and convert that to grayscale, TV dot CVT color, passing the image in CV dot color on BGR. To gray. And we can display this call this gray of color is gray person, we can pass in our name. Let's save them and run. And we have to pass in the gray. Okay, we have a blu ray person over here. So let's move on to essentially reading in this har underscore face dot XML file. So the way we do that is by essentially create a har cascade variable. So let's set this to her underscore cascade. And we're going to set this equal to CV dot cascade classifier, in inside, what I essentially want to do is, is parsing the path to this har to this XML file. That is as simple as saying har en disco face dot XML. So this cascade classifier class will essentially read in those 33,000 lines of XML code and store that in a variable called har underscore cascade. So now that we've read in all har cascade file, let's actually try to detect the face in this image over here. So what I'm going to do is essentially, say faces on school rect is equal to har underscore cascade dot detect multi scale, and instead, we're going to pass in the image that we want to detect based on. So this is great, we're going to pass in a scale factor. Now let's set this to 1.1. Give it a variable called minimum neighbors, which essentially is a parameter that specifies the number of neighbors rectangle should have to be called a face. So let's set this to three for nap. So that's it. That's all we have to do. And essentially, what this does is this detect multiscale, an instance of the cascade classifier class will essentially take this image, use these variables called scale factor and minimum labels to essentially detect a face and return essentially the rectangular coordinates of that face as a list to faces on the score rec. That's exactly why we are giving it faces on scope rect rect, to rectangle. So you can essentially print the number of faces that were found in this image by essentially printing the length of these faces on the score rect variable. So let's do that. Let's print the number, number of faces found is equal to, we can pass in the length of faces on school rect. So let's save that and run. And as you can see that the number of faces that were found one, and that's true, because there's only one person in this image code. Now utilizing the fact that this faces on school rec is essentially the rectangular coordinates for the faces that are present in the image, what we can do is we can essentially looping over this list and essentially grab the coordinates of those images and draw a rectangle over the detected faces. So let's do that. So the way we do that is by saying for x comma y comma w comma H, H in faces underscore rect, what we're going to do is we're going to draw a rectangle CV to a rectangle over the original image. So IMG give the point one, this point one is essentially x comma y. And point two is essentially x plus w comma y plus H. Let's give it a color. Let's set this to green. So zero comma 255 comma zero, give it a thickness of two. And that's it. And we can print this or we can display this image. So let's set this to detected basis. And we can pass in OMG. And if you look at this image, you can essentially see the rectangle that was drawn over this image. So this in essence, is the face that open CV is hard cascades found in this image. So let's try this with another image. So what I have here are a couple of people, a couple of other people then image of five people, so we're going to use that image and try to see how many faces OBG these hard cascades could detect in this image. So let's set this to group two. We can change that to a group of five people. Save that close, right people save and run. And I want to point real quick that the number of faces that we found, were actually seven. Now we know that there are five people in this image. So let's actually see what open CV thought was face. So we can go real quick. So actually detected all the faces in this image, all the five people, but it also detected two other guests a stomach and part of a neck. Now this is to be expected because her cascades are really sensitive to noise in an image. So if you have something that pretty much looks like a face, like a neck looks like a face, it has the same structure as the typical face would have. I don't know why her stomach was recognized as face. But again, this is to be expected. So one way we can try to minimize the sensitivity to noise is essentially modifying the scale factor in minimum neighbors. So let's increase the minimum neighbors to maybe six or seven. Save that in run. In as you can see, now six faces were found. So I guess by increasing the minimum neighbors parameter, we essentially stopped open open CV from detecting her stomach as face. So let's try this with another more complex image, a couple of people in group one. So if I change that to group one, save rock. Now, as you can see that the number of faces we've never found was six. And we know that this is not six. So let's actually change this minimum minimum neighbors just a bit. Let's change this first to three and see how many faces we'll found. Now we got 14. Okay, some people at the back want chosen because either the faces are not perfectly perpendicular to the camera, or they're wearing some accessories on the face, for example, eyeglasses. This dude's wearing a hat, this dude ran on cap, and stuff like that. So let's actually change this to one. And let's see what that gets us to one. So, Ron, and now we got 19 faces that were found in this image. So it's about looping through these values by changing these values. by tweaking these values, you can essentially get a more robust result. But of course by by minimizing these values, you're essentially making open CV small cascades more prone to noise. That's the trade off you need to consider. Now, again, hard cascades are not the most effective in detecting faces, they're popular, but they're not the most advanced, they are probably not what you would use if you were to build more advanced computer vision projects. I think for that, dealings face recognizer is more effective and less sensitive to noise than open CV is our cascades. It stands for your use case hard cascades are most more popular. They're easy to use, and they require minimal setup. And if you wanted to extend this to videos, you could all you have to do is essentially detect hot cascades on each individual frame of a video. Now I'm skipping that because it's pretty self explanatory. So that's pretty much it. For this video, we discussed how to detect faces in open CV using open CV as har cascades. In the next video, we will actually talk about how to recognize faces in open CV using open CV is built in face recognizer. So like always, if you have any questions, comments, concerns, whatever, leave them in the comments below. Otherwise, I'll see you in the next video. Hey everyone, and welcome back to another video. In this video, we will learn how to build a face recognition model in open CV using open CV is built in face recognizer. Now, on the previous video, we dealt with detecting faces in open CV using hard cascades. This video will actually cover how to recognize faces in an image. So what I have you have five folders or five different people. Inside each folder, I have about 20 images of that particular person. So Jerry has 21 images. Anson has 17 Mindy kailyn has 22 Ben Affleck has 14 and so. So what I'm essentially going to do is we're going to use open CV is built in face recognizer. And we're going to train that right now. So on all of these images in these five folders, now this is sort of like building a mini sized, deep learning model, except that we're not going to build any model from scratch, we're going to use open TVs built in face recognizer, or we're going to do is we're actually going to pass in these close 90 images. And we're going to train that recognizer on these 90 images. So let's create a new file. And we're going to call this faces ns, go train dog p y, we're going to input always, we're going to input CV to our CV, and we're going to import NumPy as NP. So the first thing I want to do is essentially create a list of all the people in the image. So this is essentially the names of the folders of these particular people, what you could do is you can manually type those in, or you could essentially create an empty list. Let's call this P. and we can loop over every folder in this folder, and let's set this to an Austrian. And we can say P dot append, I, or we can print P. Let's save that and run Python beaters on skirt on skirt trained on p y. And we get the same list that we got over here. So that's one way of doing it. And what I'm going to do next is I'm essentially going to create a variable called dir, and set this equal to this base folder, that is this folder which has, which contains these five folders of these people. Cool. So with that done, what we can do is we can essentially create a function called def create unscrewed train, that will essentially loop over every folder in this base folder. And inside that folder, it's going to loop over every image and essentially grab the face in that image and essentially add that to our training set. So our training set will consist of two lives. The first one is called features, which are essentially the image arrays of faces. So let's set this to an empty list. And the second list will be our corresponding labels. So for every face in this features list, what is its corresponding label, whose face does it belong to, like one image could belong to Ben Affleck, the second image could belong to elton john, and so on. So let's create a function. So we're going to say we're going to loop over every person in this people list, we're going to grab the path for this person, so for every folder in this base folder, going through each folder and grabbing the path to that folder. So that's essentially as simple as saying, Oh s dot path dot join the join. And we can, we can join the der with person. And what I'm going to do is I'm gonna create a labels label variable, and set this equal to people don't index of person. And now the way inside each folder, we're going to loop over every image in that folder. So we're going to say for image to image in our stock list there. In path, we are going to grab the image Park. So we're going to say image, underscore path is equal to OS dot path dot join. We're going to say join, we're going to join the PATH variable to the image. Now that we have the path to an image, we're going to read in that image from this path. So we're going to create a variable called IMG underscore rain is equal to CV dot m read image on the scope path. We're going to convert this image to grayscale I think CVT color, pause and IMG. On scope right here we can pass in t v dot c, CV dot color on the screw BGR to grip. Cool and now now with that done we can essentially trying to detect the faces in this image. So let's go back to face underscore detect and grab the whole cascade classifier variable here. Let's paste that there. And we can create a set of faces on school rect and set this equal to har underscore cascade dot detect multi scale this will take in the gray image scale factor of 1.1 and add a minimum neighbors of four. And we can loop over every every face in this face rect. So for for x comma y comma w comma each in faces rect, we are going to grab the bases region of interest, and set this equal to and basically crop out the face in the image. So we're going to say gray, y two y plus h, and x 2x plus W. And now that we have a faces a face region of interest, we can append that to the features list. And we can append the corresponding label to the labels list. So we're going to do features dot append, we're going to pass in faces on scope, or y. And we can do a labels dot append label. This label variable is essentially the index of this list. Now the idea behind converting a label to numerical values is essentially reducing the strain that your computer will have, by creating some sort of mapping between a string and the numerical label. Now the mapping of we are going to do is essentially the index of that particular list. So let's say that I grab the first image, which is an image of Ben Affleck, the label for that would be zero, because Ben Affleck is at the zeroeth index of this people list. Similarly, elton john, an image of elton john would have a label of one because it is at the second position or the first index in this people's lists. So that's essentially the idea behind this. Now, with that done, we can essentially trying to run this and see whether we got any errors or not. And we can bring the length of the features. So let's say length, length of the features list, is equal to the length of features. And we can do the same thing. This was copy this length of the labels list, set this to length of labels. So that shouldn't give us any error. So let's run that. And we get the length of the features 100 and length of labels 100. So essentially, what we have 100 faces, and 100 corresponding labels to this faces. So we don't need this anymore. What we can do is we can essentially use this features and labels list now that it's appended to train our recognizer on it. So the way we do that is we instantiate our face recognizer, call this, as the instance of the cv.face.lb p h face recognizer underscore create class. And this will essentially instantiate the face right now. Now we can actually train the recognizer on, on the features list, and the labels and the labels list. So the way we do that is by saying face underscore recognizer dot train, and we can pass in the features list, and we can pass in the labels list. And before we actually do that, I do want to convert this features and labels list to NumPy arrays. So we're going to do so we're going to say, features is equal to NP dot array of features. And we can say of labels is equal to NP dot array of labels, and save that and run. OK, data object. So let's add this to the typed object. Horse in detail type is equal to object. And we can actually print when this is done, so let's say craning don't. And we can actually go ahead and save this features and labels list. And we're going to say NP dot save, we're going to call this features.np y, and we can pass in features. And we can do NP dots, MP dot save labels dot nPy. And we can pass in the labels. So let's save that and run cool. So essentially, now the face recognizer is trained and we You can now use this. But the problem here is that if we plan to use this face recognizer in another file, we'll have to separately and manually repeat this process, this whole process of adding those images to a list and getting the corresponding labels, and then converting that to NumPy rays, and then training all over again, what we can do and what open CV allows us to do is essentially save this trained model so that we can use it in another file in another directory in another part of the world just by using that particular YAML source file. So we're going to repeat this process again. But the only change that I'm going to do is I'm going to say face recognizer dot save. And we're going to give the path to a YAML source file. So we're going to save face unscrewed trend, dot yamo. So let's repeat this process, again, trainings down. And now you'll notice that you have face on scope trained a yamo file in this directory, as well as faces, as well as features that nPy annal labels dot nPy. So let's actually use this train model to recognize faces in an image. So let's close this out. And create a new file. And we're going to call this face on school rec hig nition dot p y. Very simply, we're going to import NumPy as MP and CV to sc V, we don't need us anymore, because we're not looping over directories, we can essentially create our har underscore cascade file. So let's do that. Let's go up here, grab this, we can load our features and label rate using by saying features is equal to NP note load features.np y. And we can say labels is equal to NP dot load with called as labels.np y. And we can essentially now read in this face on the scope train that yamo file. So let's go over here. Let's grab this line. And let's say face recognizer dot read. And we're going to give it the path to this YAML source file. So face unscrewed face on screw trained dot yamo. So that's pretty much all we need. Now we need to get the mapping. So let's grab this list as well. And so that's pretty much all we have to do. So let's create a variable image that set this to save it out in read, give it a path. Let's create eight. Let's grab one from this validation and one from this validation. I have one of Ben Affleck. So let's try this with grab that piece on. And Graham, maybe this image from but I have a piece out there. And that's a JPG file. And we can convert that image to grayscale tv.tv t color positive image CV no color, I'm just going to BG BGR to Great. So let's just play this image. See, Cole is the person on identified person that's patient on the board. So what we're going to do is we're going to first detect the face in the image. So the way we do that is by saying faces on underscore rect is equal to r on this go cascade dot detect multiscale we pass in the gray image, we pass in the scale factor, which is 1.1 give it a minimum neighbors of foe and we can loop over every face in his faces on score rect Sue, Sue for x comma y comma w comma H in faces basis on score rect. We can grab the region of interest to what we're interested in finding your Why two one plus H and x 2x plus H. And now we can predict using this face recognizer to we get the label and a confidence value. And we say face recognizer dot predict And we predict on this faces on scope ROI, let's print. Let's call this label is equal to label with confidence, off color confidence. And since we're using numerical values, we can probably we can probably say people off label. Okay. And we can essentially what we can do is we can put some text on this image just to show us what's really going on, we can put this on the image, we can create a string, variable of people of label. So the person involved in that image, given an origin, let's say 10, let's say 20 by 20. Give it a font face of CV dot font, unscrew Hershey on school complex. Give it a font scale of one point of 1.0 here to color of zero, comma 255 comma zero and give it a thickness of two. And we can draw a rectangle over the image over the face. This is we draw this over the image, we give it x to y and x plus delta t comma y plus H. We give it a color of zero comma two five comma zero, and we can give it a thickness of two. So with that done, we can find this display this image called as the detected bass. And we can pass the image. And finally we can do a CV Delta Wait, key zero. So let's save and see what we get Python. Python face on school record, Nish. nation dot p y. Cannot be alone in love pickles equals false. Gosh, where's that? We probably don't need this anymore. So let's come up with that out. there if you wanted to use these again, you could essentially use MP dot load. Since the data types are objects, you can basically say allow pickle is equal to true. That's essentially but we're not going to use it. So let's comment that out. Save. And okay, we get Ben Affleck with a confidence of 60%. So that's pretty good. 60% is good, given the fact that we only train this recognizer on 100 images. So let's try this with another image of Ben Affleck, maybe this image, copy that go right across here. And this again, is Ben Affleck with the confidence of 94% pretty good. Let's go back. Let's go maybe to an odd person. Let's go to Madonna. For to grab this. It's a pain rally. But let's change this to Madonna. And let's grab this person, I'm not sure whether it will detect a face because of the head. But let's face that anyway. Now this is where you'll find that obon TV's face recognizer built on face recognizer is not the best. It currently detects it currently detects that this person in the image is actually Jerry seinfield. And that will be the confidence of 110%. Maybe there's an error somewhere. I'm not sure why that went to 111. But pretty sure there's an error somewhere. But essentially, this is where the discrepancies lie. It's not the best so it's not going to give you accurate results. So let's try this with another image. Let's go about to maybe share image, copy that piece of paper. Okay, this is Madonna with the confidence of 96.8% Okay, let's move on to elton john Watson had problems with elton john. Given the fact that he looked pretty similar to Ben Affleck for some reason. Copy that chain got to elton john just called john and print that. Okay, elton john with the confidence of 67% pretty good. Okay, so not bad. This is more accurate than what I predicted. before filming this video, I did a couple of trial runs, and I got very good results. For example, elton john was continually detected as Jerry seinfield or Ben Affleck. Madonna was detected as Ben Affleck, Ben Affleck was detected as Mindy kaylin. Minnie kailyn was detected as elton john, and a whole bunch of weird results. So I guess that we did something right. I must have done something wrong in the trial runs. But hey, we get good results. And that's pretty good. Now, I'm not sure why that gave a confidence of 111%. Maybe there's an error somewhere with the training sent. But I guess for the most part, you can ignore that. Given the fact that we get pretty good results. So that's pretty much it. For this video, we discussed face recognition. In open CV, we essentially build a features list and a labels list and we train a recognizer on those two lists. And we saved a model as a YAML source file. In another file, we essentially read in that saved model saved YAML source file. And we essentially make predictions on an image. And so in the next video, which will actually be the last video In this course, we will discuss how to build a deep learning model to detect and classify between 10 Simson characters. So if you have any questions, comments, concerns, whatever, leave them in the comments below. Otherwise, I'll see you in the next video. Hey, everyone, and welcome to the last video in this Python and urban TV cuts. Previously, we've seen how to detect and recognize faces Pioli in open CV, and the results we got were varied. Now, there are a couple of reasons for that. One is the fact that we only had 100 images to train the recognizer on. Now, this is a significantly small number, especially when you're training recognizes and building models. Ideally, you'd want to have at least a couple of 1000 images per class. The second reason lies in the fact that we want using a deep learning model. Now as you go deeper into especially computer vision, you will see that there are very few things that can actually beat a deep learning model. So that's what we're going to be doing in this video. Building a deep computer vision model to classify between the sensing characters now generate open CV GS for pre processing the data that is performing some sort of image normalization, mean subtraction, and things like that. But in this video, we're going to be building a very simple model. So we're not going to be using any of those techniques. In fact, we'll only be using the open CV library to read an image and resize them to a particular size before feeding it into the network. Now, don't worry if you've never used a built a deep learning model before. But this video will be using tensor flows implementation of Kara's now I want to keep this video real simple, just so you have an idea of what really goes on in more advanced computer vision projects. And carers actually comes with a lot of boilerplate code. So if you've never built a deep learning model before, don't worry, Cara's will handle that for you. So kind of one of the prerequisites to building a deep learning model is actually having a GPU. Now GPU is basically a graphical processing unit that will help speed up the training process of a network. But if you don't have one, again, don't worry, because we'll be using candle, a platform, which actually offers free GPUs for us to use. So real simple, before we get started, we need a couple of packages installed. So if you haven't already installed Sierra at the beginning of this course, go ahead and do a pip install Sierra. The next package you require is conero. And this is a package that I built specifically for deep learning models built with Kerris. And this will actually appear surprisingly useful to you, if you're planning to go deeper into building deep computer vision models. Now, installing this package on your system will only make sense if you already have a GPU on your machine. If you don't, then you can basically skip this part. So we can do a pip install conero. And can our actually installs TensorFlow by default, so just keep that in mind. So with all the installations out of the way, let's actually move on to the data that we're going to be using. So the data set that we're going to be using is the Simpsons character data set that's available on kaggle. So the So the actual data that we're interested in lies in this instance on score data set folder. This basically consists of a number of folders with several images inside each subfolder. So Maggie Simpson has about 12 128 images. Homer Simpson has about 2200 images. Abraham has about 913 images. So essentially, what we're going to do is we are going to use these images and feed them into our model to essentially classify between these characters. So first thing we want to do is go to kaggle.com slash notebooks, go ahead and create a new notebook. And under Advanced Settings, make sure that the GPU is selected, since we're going to be using the GPU off of that click Create. And we should get a notebook. So we're going to rename this to Simpsons. And one thing I want to do is enable the internet since we're going to be installing a couple of packages over the internet. So do use the Simpsons character data set in our notebook, you need to go head to add data search for Simpsons. And the first one by Alec city, I should pop up, go ahead and click Add. And we can now use this data set inside a notebook. So the first thing I want to do is we're going to pip install, seer. And now, now the reason why I'm doing this yet again. Now the reason why I'm doing this, again, is because candle does not come pre installed with Sierra and conero. Now I did tell her to install it on your machine. And the reason for that is because y'all can work with it and experiment with. So once that's done, go ahead to a new cell. And let's import all the packages that we're going to need. So we're going to input o s, we're going to input seer, we're going to input conero. We're going to import NumPy. As NP we're going to input CV to add CV, and we're going to input GC for garbage collection. Then next what we want to do is in basically when building deep computer vision models, your model expects all your data or your image data to be of the same size. So since we're working with image data, this size is the image size. So all the data or the images in our data set will actually have to be resized to a particular science before we can actually feed that into the network. Now with a lot of experiments, I found that an image size of 80 by 80 works well, especially for this Simpsons data set. Okay, the next variable we need is the channels. So how many channels do we want in our image. And since we do not require color in our image, we're going to set this to one basically grayscale. To run back. What we need next is we're gonna say car on the scope path is equal to the base path where all the data where all the actual data lines, and that is in this Simpsons on a school dataset, this is the base folder for where all our images are stored in. So we're going to copy this file path. And we're going to paste that in that. Cool. So essentially, what we're going to be doing now is, we're essentially going to grab the top 10 characters, which have the most number of images for that class. And the way we're going to do that is we are going to go through every folder inside the Simpsons underscore data set, get the number of images that are stored in that data set, store all of that information inside a dictionary, so that dictionary in descending order, and then grab the first 10 elements, first n elements in the dictionary, hope that made sense. So what we're going to do is we're going to say create an empty dictionary. We're going to say for character in our stop list, der called car path, we are going to say car underscore dict of car is equal to length of s dot list dir of Oh s dot path dot join. We're going to join the car on a scope pump with car. So essentially, all that we're doing is we're going through every folder or grabbing the name of the folder, and we're getting the number of images in that folder. And we're storing all that information inside the dictionary called car underscore dict. Once that's done, we can actually sort this dictionary in descending order. Sending order and the way we do that is with a car unscored dict is equal to car dot SOT unscored dict of car underscore dict. And we said descending equals to true. And finally, we can print the dictionary that we get. So this is the dictionary that we have. As you can see, Homer Simpson has the most number of images at close to 2300. And we go all the way down to Lionel, who has only three images in the data. So what we're going to do is now that we have this dictionary, what we're going to do is we are going to grab the names of the first 10 elements in this dictionary, and store that in a list of characters list. So we're gonna say characters. So we're gonna say characters is equal to is equal to an empty list. And we're going to say, for i in car underscore dict. We're going to say characters, dot append, and we're going to append the name. So we say I have zero. And we say, if count is greater than or equal to 10, we can break it, we need to specify a count of zero, and increment that counts. Okay, once that's done, let's print what our characters looks like. So we've essentially just grabbed the names of the characters. So with that done, we can actually go ahead and create the training data. And to create a training data is as simple as saying train is equal to seer dot pre process. From there, we pass in the car on scope, puff, the characters, the number of channels, the image size, image size, as we say, is shuffle equals true. So essentially, what this will do is it will go through every folder inside car on the scope path, which is Simpsons underscore data set. And we'll look at every element inside characters. So essentially, it is going to look for Homer Simpson, inside the Simpsons underscore data set, it will find Homer Simpson, whereas Homer Simpson, it even finds Homer Simpson is going to go through inside that folder, and grab all the images inside that folder, and essentially add them to our training set. Now, as you may recall, in the previous video, a training set was essentially a list. Each element in that list was another list of the imagery and the corresponding label. Now the label that we had was basically the index of that particular string in the characters list. So that's essentially the same type of mapping that we're going to use. So Homer Simpson is going to have a label of zero, Ned will have label of one, Liza will have label of three, and so on. So once that's done, go ahead and run this. Now, basically, to basically the progress is displayed at the terminal. If you don't want anything outputted to the terminal, you can basically just set set the verbosity to zero. But I'm going to leave things just as it is, since there are a lot of images inside this data set. This may take a while depending on how powerful your machine is. So that's only took about a minute or so to pre process our data. So essentially, let's try to so let's essentially try to see how many images there are in this training set. We do that by saying the length of trip. And we have 13,811 images inside this training set. So let's actually try to visualize the images that are present in this dataset. So we're going to import matplotlib.pi plot as PLT, we're going to do a PLT dot bigger. And we're going to give it and we're going to give it a big size of 30 by 30. Let's do a plt.im show, we can pass in first. The first element in this training sets are zero and then zero. And we can give it a color map off gray. And we can display this image. Now the reason why I'm not using open CV to display this image is because for some reason, open CV does not display properly in Jupyter Notebook. So that's why we're using matplotlib. So this is basically the image that we get somebody legible, but to a machine. This is a valid image. Okay, the next thing we want to do is we want to separate the training set into the features and labels. Right now. The train That basically is a list of 13,811 lists inside it. Inside each of that sub lists are two elements, the actual array and the labels itself. So we're going to separate the feature set, or the arrays and the labels into separate lists. And the way we do that is by saying feature set and labels is equal to car dot zip on school train, we are going to separate the training set and give it an image size of image size. And that's n equals two. So basically, what this is going to do is going to separate the training set into the feature set and labels and also reshape this feature set into a four dimensional tensor, so that it can be fed into the model with no restrictions whatsoever. So go ahead and run that. And once that's done, let's actually try to normalize the feature sets. So essentially, we are going to normalize the data to be in the range of to be the range of zero comma one. And the reason for this is because if you normalize the data, the network will be able to learn the features much faster than, you know, not normalizing the data. So we're gonna say feature set is equal to square dot normalize, and when to pass in, peaches set. Now we don't have to normalize the labels. But we do need to one hot encode them that is convert them from numerical integers to binary class vectors. And the way we do that is by saying from TensorFlow, del Kara's dot EDU tools input to underscore categorical. And we can say labels is equal to two categorical, and we get possible labels, and the number of categories, which is basically the length of this characters list. Cool. So once that's done, so once that's done, we can actually move ahead and try to create our training and validation data. Now, don't worry too much if you don't know what these are. But basically, the model is going to train on the training data and test itself on the validation data. And we're going to say x underscore train x underscore Val and y underscore train and y underscore Val is equal to sere dog train, Val split. And we're going to split the feature set and the labels using a particular validation ratio, which we're going to set as point two. So that's basically what we're doing, we're splitting the feature set and labels into into training sets and validation sets with using a particular validation ratio to 20% of this data will go to the validation set, and 80% will go to the training set. Okay. Now, just to save on some memory, we can actually remove and delete some of the variables and we're not going to be using. So we do that by saying Dell crane, Dale feature sets, do labels, and we can collect this by saying GC dot collect. Cool. Now moving on, we need to create an image data generator. Now this is basically an image generator that will essentially synthesize new images from already existing images to help introduce some randomness to our network and make it perform better. So we're gonna say data, Gen is equal to can narrow down generators, dot image, data generator. And this basically instantiates, a very simple image generator from the caros using the Kara's library. And once it's done, let's create a training generator. By setting this equal to data Jim, don't float. And we can pass in extra rain and wind rain and give it a batch size equal to batch size. So let's actually create some variables here. That's set my batch size to 32. And maybe let's train the network for 20 bucks. So once that's done, that's wrong bet. So with that done, we can actually proceed to building our model. So let's call this creating the model. And before making this video, I actually tried and tested out a couple of models found that one actually provided me with highest level of accuracy. So that's the same model, the same model architecture that we're going to be using. So we're gonna say model is equal to conero dot models dot create Simpsons model, we're going to pass in an image size, which is equal to the image size, we're going to say set the number of channels equal to the number of channels, we're going to say, we're going to set the output dimensions to the to 10, which is basically the length of our characters, then we can, then we can specify a loss, which is equal to binary binary cross entropy. There we get set a decay of E of e to the negative sixth power, we can set a learning rate equal to point 001. We can set Oh momentum of point nine, and we can set Nesterov to true. So this will essentially create the model using the same architecture I built and will actually compile the model so that we can use it. So go ahead and run this. And we can go ahead and try to print the summary of this model. And so essentially, what we have is a functional model, since we're using Kerris as functional API. And this essentially has a bunch of layers, and about 17 million parameters to drain out. So another thing that I want to do is create something called a callbacks list. Now this callbacks list will contain something called a learning rate shedule that will essentially sheduled the learning rate at specific intervals so that our network can essentially train better. So we're going to say call callbacks list is equal to learning rate shedule. And we're going to pass in conero.lr on SCO LR underscore schedule. And since we're using learning where shedule Let's go and input. So from TensorFlow, Delve, Cara's no callbacks input learning rate schedule. And that should about do it. So let's actually go ahead and train the model. So we're gonna say training is equal to model dot fit, we're gonna pass in the train gin, we're going to say, steps per epoch is equal to the length of X on school train divided by divided by the batch size. We're going to say epochs, is equal to epochs. We're going to give the validation data validation data equal to a tuple of x underscore Val, and y underscore Val. And we're going to say validation steps. Easy Steps is equal to the length of y on school Val, divided by divided by the batch. batch size. And finally, we can say callbacks, is equal to callbacks, callbacks on school missed steps per epoch, that steps for epoch. And that should begin training. And once that is done, we end up with a baseline accuracy of close to 70%. So here comes the exciting part, we're now going to use open CV to test how good our model is. So what we're going to do is we're going to use open CV to read in an image at a particular file path. And we're going to pass that to our network and see what the model spits out. So let's go ahead and go to this Simpson test set. So let's go ahead and try to search for all the way down here. Let's look at our characters. Let's just print that out just to see what characters we trained on. Okay, let's look for Bart Simpson. Probably bit irritating, but since data sent Okay, we got an image of Bart Simpson. So click this and random path, got a test path, set this equal to our string. And what we're gonna do is we're gonna say mg is equal to CV dot m read test on secure path. And, and just to display this image, we can use PLT dot m show, we can pass the image, pass in the image and give it a column map of gray. And we can do a PLT dot show. Okay, PLT show. And okay, so this is an image of Bart Simpson. So what we're going to do is we are going to create a function called prepare, which will basically prepare our image to be of the same size and shapes and dimensions as the images we use to prepare the model in. So this will take in a new image. And what this will do is we'll, we'll convert this image to grayscale so we're gonna say injury is equal to CV dot, CVT color, and we're gonna pass in the injury and we're gonna say CV dot color on scrub BGR. To gray, we can resize it to our image size. So we're going to say mg is equal to CV dot resize, we're going to resize the image to be image underscore with size, I'm going to reshape this image. So injury is equal to stare dot reshape, reshape of image. We want to reshape the image to be of image size with channels equal to one. And we can return image. So let's run that. And let's go down here. And let's say predictions is equal to model dot predict and prepare image. And we can visualize this predictions. So let's print predictions. And essentially, this is what we get. So to print the actual class, what we can do is we can print their characters, BB NP dot arg Max, and we can say predictions of zero. You're not trying to visualize this image so we can do a PLT dot m show. Let's pass in the image. And PLT dot show. Let's grab this and move this down. ver. That's right. Yeah. Okay, so this is our image. And right now our model thinks that buttons in is in fact, Lisa Simpson. Let's go. Lisa Simpson. Okay. Let's try another image. Let's try probably this image is Bart Simpson 28. Let's go up they and maybe change that to two, eight. run that. This is Bart Simpson. Let's run this. And let's and again, we got Lisa Simpson. So let's try with a different image. Yeah, we do. We did Charles Montgomery to copy this. All the way down there. We got Charles predict, and we get van Hughton. Okay, definitely not the best model that we could have asked for. But hey, this is a model. Right now this base discounting has a baseline accuracy of 70%. Although I would have liked it to go to at least 85%. In my test, it had gone close to 90 92%. I'm not sure exactly why this went to 70%. But again, this is to be expected into building deep computer vision models is a bit of an art. And it takes time to figure out what's the best model for your project. So that's it for this Python and open c because this goes to is basically kind of a general introduction to open CV and what it can do. And of course, we've only just scraped the surface and really this A whole new world of computer vision now fair. Now, while we obviously can't cover every single thing that open CV can do, I've tried my best to teach you what's relevant today in computer vision. And really one of its most interesting parts, building deep learning models, which is in fact, where the future is self driving vehicles, medical diagnosis, and tons of other things that computer vision is changing the world. And so all the code and material that was discussed throughout this course is available on my GitHub page. And the link to this page will be in the description below. And just before we close, I do want to mention that although I did recommend you installed Sierra in the beginning, we barely use it throughout the coasts. Now, it's probably not going to make sense to you right now. But if you plan to go deeper into computer vision into building computer vision models, Sierra lasher proved to be a powerful package for you. It has a lot of helper functions to do just about anything. Now I'm constantly updating this repository. And if you want to contribute to these efforts, definitely do that you can set a pull request with your changes. And if it's helpful, it will be merged into the official code base, and you'll be added as a contributor. If you want to building deep learning models with Kara's then conero will be useful to you. But again, for the most part, it's usually software that you'll be using. So anyway, with that said, I think I'll close up this course, if this goes helped you in any way and God you're more interested in computer vision, then definitely like this video, subscribe to my channel, as I'll be putting up useful videos on Python computer vision and deep learning. So I guess that's it. I hope you enjoyed this post and I'll see you in another video.
