With timestamps:

00:00 - Welcome to this course on deep learning for computer vision with TensorFlow. In this course,
00:05 - you'll master deep learning concepts and their applications in computer vision tasks,
00:11 - such as image classification, object detection, and image generation. You'll learn about tensors,
00:16 - variables, and neural networks, including convolutional neural networks through
00:21 - practical projects like predicting car prices and diagnosing malaria. You'll also learn advanced
00:27 - techniques for model performance, data augmentation, and deployment, as well as learn about modern
00:34 - convolutional neural networks, transfer learning, and transformers in vision. This course comes from
00:40 - NeuralLearn, which offers a variety of machine learning courses. Hi, everyone, and welcome to
00:46 - this course on deep learning for computer vision by neurallearn.ai. In this course, we shall make
00:52 - use of tools like Hug and Face, TensorFlow, Onyx, and 1DB to build and deploy different computer
01:01 - vision solutions. Applications of computer vision are everywhere today, going from Tesla self-driving
01:08 - cars to livestock farmers who can now automatically count the number of animals they have, to mobile
01:14 - facial recognition apps, and even in the hospitals where the gastroenterologists could make use of
01:21 - computer vision for much faster diagnostics. Behind those different computer vision solutions
01:29 - is deep learning models like the convolutional neural networks and the vision transformer.
01:35 - Throughout this course, we shall explain in detail how the confidence of the vision transformers
01:42 - work. Given that these are all deep learning-based models, let's look at a high level how deep
01:50 - learning works. Let's suppose we want to build a system where we get an input like this one,
01:57 - and we are able to say that this is a damaged car, or get an input like this one, and we say that
02:04 - this car is still intact. Then in that case, we could build and train a deep learning model,
02:10 - which takes in this image, for example, and then for its output here, we'll be able to say that
02:17 - this car is damaged. Or in the case where this model takes in this other image,
02:24 - then we'll be able to say that this car is intact. Nonetheless, for this deep learning model to be
02:32 - intelligent enough to make these kinds of decisions, we need to train it. And the way this
02:38 - training is carried out is we have thousands, hundreds of thousands, or even millions of
02:45 - damaged cars like this. And so our output level is obviously damaged. On the other hand, we have
02:53 - thousands of cars which are still intact, and the output level is intact. And so now we have what
02:59 - we'll call our data set, that is this input and your corresponding levels. And then in this deep
03:07 - learning model right here, we stack several neural network layers. So we'll have this, we have some
03:16 - other layer, some other layer, right up to the output. Now it should be noted that this neural
03:24 - network layers here are essentially mathematical functions. If you do not have a background in
03:30 - mathematics, you shouldn't be worried as in this course, we shall focus on implementing all this
03:36 - practically. So that said, as we're saying, this we have here, all these layers, these neural network
03:43 - layers we have here are essentially mathematical functions. And if we have a linear layer,
03:49 - then this means that we could take an input x, multiply this by a weight which we'll call w,
03:57 - and then add some bias which we'll call b. And then this gives us the output which is that
04:06 - output of a given layer. So here for example, let's suppose this input is x, we have x here,
04:11 - we'll take x, multiply it by the weight, and then add up the bias to have the output at this level
04:19 - y. Let's call this x1, then this is y1, and then this y1 will be used as input to this next layer,
04:26 - and then we'll have y2, right up to say yn. And so during the training process, given that we know
04:35 - the input and the output, our aim will be to obtain the values of this w and this b which ensure that
04:47 - when we pass in images like this or this, the model is able to know whether it's a damaged
04:54 - or an intact car. Now we see that at every layer we have our weights and biases which are all going
05:03 - to be updated during the training process, making use obviously of our training data, that is these
05:09 - inputs and their corresponding levels, such that when shown an image which this model has never seen,
05:16 - our deep learning model is able to take the right decision. So in essence, these kinds of models are
05:24 - called deep learning models because we have several layers stacked between the inputs and the outputs.
05:32 - And the number of layers we stack represent the depth of our model. On the other hand, deep
05:39 - learning algorithms fall under a category of artificial intelligence algorithms known as
05:47 - machine learning, where the model learns from the training data. A prerequisite to this course
05:55 - is knowledge of Python programming. So you could head over to the Neural Learn YouTube channel
06:01 - and check out our free essential Python programming playlist which will help you master
06:07 - the basics of Python programming. Let's now take a look at what we'll be learning in this course.
06:14 - The way we've designed this course is such that we'll start from the very basics.
06:21 - So we'll suppose that you have no prior experience in deep learning and so we'll start with basic
06:27 - topics in TensorFlow like the tensors and the variables. And from here, we'll dive into solving
06:35 - our first practical problem which is that of call price prediction. Now it should be noted that
06:42 - although this isn't necessarily a computer vision problem, this will permit us to learn how to
06:49 - prepare our data with TensorFlow, build simple models like the linear regression model,
06:55 - train these models, evaluate them, and test out this model such that by the time we move into our
07:06 - next project which happens to be now a computer vision based project which is that of
07:12 - Malaria diagnosis, we already have what it takes to start building more complex models like the
07:19 - convolutional neural network. But before building them, we'll first of all understand how and why
07:25 - they work. By the end of the section, we should have learned how to build a simple solution
07:31 - for our Malaria diagnosis problem and so now we're ready to dive into building more advanced models
07:39 - with TensorFlow. After looking at these more advanced models, we'll then dive into evaluating
07:46 - classification models. So we'll look at different metrics like the precision, recall, accuracy,
07:53 - and then we'll learn how to come up with the confusion metrics and the ROC plots.
08:01 - Once we're done with this, we'll dive into model performance. Here we shall look at TensorFlow
08:08 - callbacks, learning rate schedulers, model checkpointing, and then how to solve the problems
08:14 - of overfeeding and underfeeding. That said, one main way in which we could mitigate the problem
08:22 - of overfeeding is by using data augmentation and so we have the section reserved for data augmentation
08:29 - using TensorFlow and albumentations. Now once we're done with data augmentation, we'll look at
08:37 - more advanced concepts in TensorFlow like custom losses and metrics, the eager and graph modes,
08:44 - then custom training loops. So we'll learn how to train our model without necessarily
08:52 - using the fit method. We'll look at TensorBoard integration where we'll learn how to carry out
08:59 - data logging, viewing model graphs, hyperparameter tuning, then profiling and visualizations.
09:07 - From here, we'll get into machine learning operations with weights and biases where we shall
09:14 - look at how to carry out experiment tracking, hyperparameter tuning, data set, and model
09:19 - versioning with 1 dB. And now we shall move to our next project which is that of human emotions
09:27 - detection. Again here, we shall prepare our data set, build our model, carry out data augmentation,
09:34 - and then we shall look at TensorFlow records. Now once we're done with this section, we shall
09:41 - go ahead to look at modern convolutional neural networks like the AlexNets, VGGNets, ResNets,
09:47 - MobileNets, and the EfficientNet. With all this in place, we'll learn about transfer learning.
09:55 - We should help us now train our models much more efficiently using already pre-trained convolutional
10:04 - neural networks. Then we'll look at how the convolutional neural networks take decisions
10:10 - by visualizing intermediate layers. And so up to this point so far, we've been looking at the
10:17 - convolutional neural networks and now we'll be set to dive into the vision transformers.
10:23 - We'll understand how to work and even get to fine tune our own vision transformer using the
10:31 - hugging face transformers library. And so now that we have a working solution, the next logical step
10:37 - will be to deploy this such that anyone around the world could make use of our model which we've
10:45 - just built. And so we'll convert our trained model to the onyx format, we'll quantize this, build out
10:51 - a simple API, and then go ahead to deploy this API to the cloud. Now that we've learned how to
10:58 - deploy our computer vision models to the cloud, we could dive into other computer vision problems
11:05 - like object detection. And in this section, we'll look at the basics of object detection
11:11 - and also build and train our own object detection YOLO model from scratch with TensorFlow.
11:18 - Then finally, we'll dive into the domain of image generation where we'll look at
11:23 - the variational auto encoders and the generative adversarial neural networks which we shall use for
11:30 - digit generation and face generation. In this course, the coding platform we'll use to build
11:38 - and train our models will be Google collab. We shall make use of Google's free GPUs to train
11:46 - our models. We've just explained that in order to train a model like this one, we make use of
11:54 - a training data set which has its inputs and its corresponding outputs. These inputs and outputs
12:03 - are multi-dimensional arrays which are commonly known as tensors.
12:08 - In this section, we shall start with tensor basics. Then we'll move on to casting and tensor
12:19 - flow. We'll look at initialization, indexing, broadcasting, algebraic operations, matrix
12:29 - operations, commonly used functions in machine learning. We'll look at the different types of
12:36 - tensors like the ranked tensors, sparse tensors, and even the string tensors. In the context of
12:44 - deep learning, tensors can be defined as multi-dimensional arrays. An array itself is an
12:53 - ordered arrangement of numbers. It's important we take note of these keywords as the data we shall
13:03 - be dealing with like for example this image right here can be represented using these numbers which
13:12 - have been arranged clearly in an ordered manner and can be represented in multiple dimensions.
13:19 - In the specific case, we have this array which is represented in one, two dimensions. So this is a
13:28 - two z or two dimensional array or what we generally call a matrix. Now we'll explore
13:38 - different types of arrays based on their dimensionality. So here for example we have what we'll
13:46 - call a zero dimensional array which is simply because this array or this tensor contains
13:55 - a single element. So let's say we have 21. This is a zero dimensional array. Let's say we will have
14:02 - one. This is zero dimensional so on and so forth. So essentially once we have a single element then
14:07 - it's a zero d array and then now for our next example we have this 1d tensor right here
14:17 - which in fact is a combination of several zero d tensors. So if you look at this you see that
14:26 - this is a zero d tensor. This is another zero d tensor. This is another zero d tensor. So this
14:32 - vector is made of three of these kinds of elements. Now we could have other examples like this.
14:38 - Let's say we have five, eight and then three. Let's change the length. So let's say we have
14:49 - one of length five for example. We could have ten, two, eleven, four and seven. So this is
14:58 - a 1d tensor we have right here. So so far we've looked at the zero d tensor. We've looked at the
15:06 - 1d tensor and now we could dive into the 2d tensor which essentially is made of a combination
15:15 - of several 1d tensors. You could see that right here. This is a 1d tensor here. This is another
15:23 - 1d tensor. This is another 1d tensor and finally here we have a 1d tensor. So when you bring
15:31 - together this 1d tensors you form this 2d tensor. For a three-dimensional tensor you might have
15:39 - guessed this right. You would simply combine several two-dimensional tensors. So right here
15:46 - we have this 2d tensor with this other 2d with this other 2d and this other 2d forming a 2vd
15:58 - tensor. We could visualize this differently here. So we take this this this and this. Now you see
16:07 - we have one two and the third dimension. Now that we understand this we're going to take a look at
16:17 - the concept of tensor shapes. Starting with this given that it's zero dimensional then there is
16:24 - no shape. With this one here it's one dimensional and it's made of three elements and so we could
16:32 - say this is of shape three. So we have three and that's it. Now for this other one it's made of
16:42 - one two three and four 1d tensors and each and every one of this is made of three elements.
16:56 - So this here for example is made of three elements. This made of three, this three, this three and
17:02 - given that there are four of this made of three elements its shape is four by three. So that's it.
17:11 - That's how we obtain the shape for this. Now for the 3d tensor we have here it's made of one two
17:19 - three four 2d tensors. So this here we have four two or two d tensors and each and every one of
17:29 - this is made of two 1d tensors where each and every one is made of three elements. So what we'll
17:37 - see here is this is two by three this year because we have one two is two by three this is two by
17:49 - three and this year is two by three and now given that we have four of this different two by three
17:58 - um two d tensors then we will see the shape of this 3d tensor is four by two by three. So similar
18:08 - to this we have four by three this is four by two by three. Now notice that the number of elements
18:16 - we have here tells us or gives us information about the number of dimensions our tensor is.
18:23 - So here because we have a 1d tensor or one dimensional tensor we have just one here
18:28 - because it's 2d or two dimensional we have two and here because it's 3d we have three.
18:36 - Also in matrix notation we would say that this year or this matrix right here is made of one
18:43 - two three and four rows. So number of rows we have here is four and then here we have one
18:57 - two three columns number of columns is three. In the case of this 3d tensor we have right here
19:08 - we have one two rows and one two three columns for each and every one of this 2d tensor we have
19:19 - right here. Now we have some basic knowledge on tensors let's go ahead and create them with tensor
19:29 - flow. So first things first we'll import tensor flow as tf and then you should note that because
19:38 - we're using colab we do not need to install tensor flow before making use of it all we need to do
19:46 - here is just import and we're good to go. Now first thing we're going to do is we're going to
19:52 - have our tensor which we'll call tensor 0d and the way we're going to create this tensor is by
19:59 - calling on the constant method. So we have tensor flow's constant method and then we specify for
20:05 - example that we want this to be let's say four so that's it that's how we create a zero-dimensional
20:13 - tensor. You could see here from the documentation that this constant method takes in a value
20:19 - takes in a data type takes in a shape and a name but for now we pass in only this value right here
20:28 - and then we could go ahead and print this so let's have tensor 0d run that you could see right here
20:37 - we've had this tensor which has value four it has no shape and it's of type int 32.
20:46 - With this we could go ahead and build out our tensor 1d, tensor 1d we have as usual our constant
20:55 - method but this time around because it's 1d we're going to have a list. We're going to take this
21:01 - list we had here 2 0 negative 3 and then putting this now right here we have 2 0 and negative 3.
21:09 - Now let's print this out we have tensor 1d and there we go you see we have the value which is
21:19 - this 1d or this list we could see shape right here and then the data type now let's modify
21:27 - this and say we have 8 and let's say 90 let's run this now and check out its shape you see here
21:33 - this is our new list or this our new tensor and then you see now it's five because we have five
21:39 - elements in here but again this is still an integer so let's add this here changes to a float
21:46 - and then see what we have see here now this is a float so it's still our inputs by this time
21:53 - around floats same shape but the data type changes from int to float let's get back to integer and
22:00 - then we move now to the 2d tensor so we've looked at 0d 1d to create a 2d tensor it's going to be
22:08 - quite as simple as we've done with the 0d and 1d so here we have 2d and then we have as usual our
22:17 - constant and then let's open up the square brackets so we have the square brackets right here
22:25 - open up and then we'll have this 2d tensor we have seen already so let's have this right here
22:34 - we have 1 2 0 we put a comma to move to the next row or to the next line we have 3 5 minus 1
22:45 - then we have 1 5 6 and then we have 2 3 8 okay so as you could see here we have
22:53 - this 1d tensors which we stuck and which will form our 2d tensor now as usual we have our
23:02 - constant method which takes in this value and you should also take note of the fact that we have
23:07 - this outside of this stacked 1d tensors now we have this let's go ahead and print out tensor 2d
23:18 - we have tensor 2d and there we go as you could see we have our 2d tensor that's it we have the
23:26 - shape 4 by 3 which is what's expected and then the data type is in 32 from here we move on to the
23:34 - 3d tensor which is essentially made of this 2d tensors which have been stacked together so let's
23:42 - go ahead and see how to create this with tensor flow we'll start by putting together those 2d
23:47 - tensors we had 1 2 0 3 5 minus 1 so that's it 3 5 minus 1 and then we have all these remaining
24:00 - 2d tensors so let's get here we have tensor 3 tensor 3d it's a constant we have our square
24:13 - bracket here then we simply copy this copy this and then paste this in here now let's
24:23 - print this out and see what we get so here we have tensor 3d as you can see we get an error
24:33 - and this comes from the fact that we omitted commas now if you look at this you'll see that
24:38 - uh between one 1d and another 1d to form this 2d there is a comma and so this means that between
24:46 - this 2d and this other 2d there should be a comma so let's have this comma here let's have this
24:54 - comma let's have this comma and this comma okay so let's run that again and see what we get
25:00 - as you could see we have this um our 3d output of shape 4 by 2 by 3 and that's exactly what
25:08 - we expect to get data type int 32 now before we move on we could we should note that we could
25:14 - also do let's let's have this here we could also do um tensor 3d shape to obtain this tensor shape
25:25 - so you see that we have that um right there we could replace this zero we could have one
25:33 - see that's zero that's one that's two um there we go let's let's have one that's one um that's it
25:44 - okay and also we could also have here let's say three we could do ending and this gives us a
25:51 - value of three showing us that our tensor is a 3d tensor at this point we've looked at zero d 1d
25:59 - 2d and even the 3d tensor let's go ahead and check out a 4d tensor but to construct a 4d tensor
26:08 - we need several 3d tensors so let's have this here we have this 3d well we saw this already
26:16 - this is this 3d and then we move to this other one right here or this two others we we have this
26:24 - other 3d and this other 3d and then now if we stack this three 3ds up what would obtain
26:32 - would be a 4d tensor see we have this one we add this other and we have this other so now we have
26:40 - here a four dimensional tensor which is made of three three dimensional tensors now you should
26:48 - know that this could be two this could be four this could be whatever number so if we consider
26:53 - only this two then we're talking about two three dimensional tensors but uh what's so important to
27:00 - note here is the fact that once you stack several 3d tensors you create a 4d tensor now at this
27:08 - point you could take it as an exercise to create your own 4d tensor so you could pause the video
27:14 - but um we are going to go ahead and show you how to create this first things first we're going to
27:22 - take up our 3d here we have this 3d tensor let's just copy this there we go we paste this out here
27:31 - let's repeat that operation copy and paste again but now let's modify some values so let's say we
27:39 - have 13 26 let's just randomly modify this um 23 so it's now this this other 3d is different from
27:48 - the other one say 30 okay now let's copy this again and then paste out here so we have the three
27:57 - 3d tensors um let's say this is one oh let's just add zeros we have here um 23 um two
28:12 - four and six okay so now we have our three 3d tensors we have this one we have this other one
28:22 - and then we have this other one now to create our 4d tensor as usual we call on our constant
28:31 - method so we have 4d um we have the constant and as usual we have our square brackets which will
28:40 - open up and then now we just simply take this copy all this to the end cut this from here
28:48 - and then paste this out here okay so we see we have three of these um 3d tensors now remember
28:55 - last time when you were separating two 3d tensors or between the three d tensors we need to have a
29:02 - comma so here we're gonna have this comma and then here we're gonna have this comma and here
29:10 - we have a comma okay so that's it so let's run this oh let's print this out and then see what
29:16 - we get so here we have print um tensor 4d there we go you can see from here we have our
29:28 - 4d tensor you see it's cheap three by four by two by three and this is because our 4d tensor
29:36 - is made of one stick this off it's made of one two three three d tensors so that's why we have
29:45 - three and then for each and every one of this we have four that's one two three four two d tensors
29:57 - so here we have four and for every two d tensor we have one two that's two rows and one two three
30:09 - that's three columns and that's now exactly what we have here so we still have the same data type
30:16 - we have uh values which we could have here and that's it if we get back here we'll see that
30:23 - we have different data types which we could use so instead of um does this default int uh we could
30:30 - have um tf float say 32 and you'll notice that the outputs as we have seen already
30:40 - should have this decimal since we're now dealing with floats now the float we have in here is of
30:49 - type um float 32 or specifically the float 32 meaning that the position value here is 32 now
30:56 - if you reduce this if you reduce this you'll see that you actually have the same output
31:04 - but the difference is that less memories are located for storing this tensor as compared to
31:11 - maybe even let's say the 64 that's float 64 and so in certain contexts where we have the memory
31:18 - constraint we would want to use the lower precision tensors in the documentation you could have um
31:27 - all the different data types um which are supported in tensorflow so here we have quantized
31:33 - data types and talking about quantization we're going to treat this in subsequent sections and
31:40 - so don't bother for now if you don't really master this um here we have the brain floating
31:50 - point here we have the boolean here we have complex and it's 128 and 64 bit versions here we
31:59 - have the double which in fact means the double precision floating point now this is double
32:05 - because the float 32 is a single precision so here we have single the float 16 is half
32:13 - prediction or precision and then the double is double precision here we have the ints
32:20 - under different versions or the different uh positions we have the quantized ints
32:25 - um we have resource we have string we have unsigned int and then we have variant getting
32:36 - back to the code if for example here we have this float let's just add a decimal so we suppose that
32:42 - this is a float and then now we do int let's say int 64 we run that you'll see that would obtain
32:50 - an error so let's have this here what do we see we told that we cannot convert this um tensor to
32:58 - or to eager tensor of the data type int 64 but now if we use the cast method if we do tensorflow
33:06 - cast um and then here all right let's take this back let's let's have our tensorflow or our tensor
33:15 - 1d and then let's take this off or let's yeah let's just have this float um 64 there we go or
33:24 - let's say float 32 and then let's say we define some casted tensor casted tensor 1d and then
33:33 - this casted tensor 1d is simply going to be the casted version of this or 1d tensor so here we
33:39 - have tensor 1d and then we specify the data type and now we say we want this to be in 16 for example
33:48 - so here we're going to print out the tensor 1d i'm also going to print out the casted tensor 1d
33:56 - we get in this error we should have had tf dot so let's have this here there we go and then
34:05 - let's look at what we have as output as you can see thanks to the casting operation we are able to
34:11 - get from this float to this integer tensor and so this tells us that instead of coming right here
34:21 - and saying for example int 16 or let's say int 32 it's preferable to make use of the cast method
34:27 - that said let's say for example instead of having this that's instead of casting this into
34:36 - an integer we want to cast this into a boolean so let's run this you see here that we get all
34:43 - true except for this one year which is false and that's simply because this is a zero so it looks
34:49 - like the what a casting method does is for all the values which are different from zero
34:55 - they're true but values equal zero equal false we could also go ahead and create our own boolean
35:01 - so let's say we have tensor tensor bool we call this tensor bool and then we have all this lists
35:11 - made of this tensor so we could have true true false true true false okay so let's print out
35:18 - tensor bool and see what we get as you can see here we have our output tensor of shape three
35:26 - and here is values you see the data type bool right here now another data type which you could
35:32 - look at is um the string so let's let's say we want to have tensor string um there we go let's
35:40 - say we want to have um halo world halo world um let's print this out tensor string there we go
35:51 - we have our tensor we could also put this in the list because right now this is a zero dimensional
35:56 - tensor so let's put this in the list and um hello world hi and we'll close that and now we have this
36:07 - 1d tensor of shape two which is a string from here we would also look at how to create or how
36:14 - to convert a non-py array into a tensor so let's call this um np array first of all we'll start by
36:23 - importing non-py as np we could just do that right here import non-py as np there we go and then we
36:31 - have um np array and let's say we have np array and then let's take in this here let's say one
36:41 - two three or one two four there we go print this out we have np array right here now we could make
36:49 - use of this tensor flow convert to tensor method so let's say we have your converted converted
36:58 - tensor and we have um convert to tensor which essentially takes in a non-py array now let's
37:09 - have this and then print out the converted um tensor with that getting back to the documentation
37:18 - we will look at a couple of other um methods like the i method here we have this i method right here
37:27 - as usual we have the well described method in the documentation which comes with this um
37:36 - short phrase explaining how it works it comes with different arguments and also even some
37:45 - examples so getting back here we told that this i method permits us construct an identity matrix
37:54 - or batch of matrices so now we have this definition we could simply copy this out
38:00 - and then get back to the code right here paste this out and let's say we call this um i tensor
38:08 - i tensor and for now we have this number of columns which is known the batch shape known
38:16 - data type float 32 name known now let's say we want to have three rows so we have number of rows
38:24 - which is equal three now in this case uh we'll print this out so we see the kind of output we
38:31 - get here we have i tensor and there we go as you could see we have this identity matrix
38:39 - where all the values of the matrix are zero except for those of the leading diagonal let's um show
38:47 - clearly this leading diagonal right here here we have our matrix which is this and then we have
38:54 - this element of the leading diagonal which is equal one now you should note that you could
38:59 - you could say for example um let's say three times this and you should have all the elements
39:07 - of the leading diagonal to be equal to three and the others zero so in this case we now have
39:13 - someone like this so you see all this three while the rest zero now let's get back let's take this
39:19 - off here we have our i tensor and then we see that we could specify the number of columns now here
39:26 - or previously where this was set to known we had a square matrix that is when you define the
39:32 - number of rows to be three the number of columns automatically takes up the value of three so we
39:37 - have a three by three matrix and obviously the type here is float 32 could modify this and say
39:43 - for example 16 from that let's try out bold after this there we go we have float 16 let's try out
39:53 - bold you see everywhere is false false false false false false except for the leading diagonal
40:01 - which has true now getting back let's say here we have five and then now number of columns let's
40:08 - set this to three you see right here this is the output we get now it's it acts like we have
40:16 - a five by five matrix so we suppose we had a five by five matrix where this would be zero
40:21 - zero this would have been zero zero this would have been zero zero this would have been one
40:27 - if this was five by five zero and then here zero one so this is what we would have had if we do not
40:35 - if we let this to to be known that is number of rows equal number of columns but now that we've
40:40 - set number of columns to be three this part has been cut off and so this is what we're left with
40:46 - now with that said let's go ahead and take this back to known and then let's set a bad shape
40:54 - so let's say this is three around that as you could see because we set this bad shape to be three
41:04 - here we have this output which is three by five by five well let's let's change this to two
41:11 - so you could see that this actually is responsible for deciding on what number of batches we have
41:18 - right here so here we have two by five by five so essentially saying that you want to have this bad
41:24 - shape to be two like this means you want two five by five matrices which are identity matrices
41:34 - that is which have their values or zeros except for those of the leading diagonal which are
41:42 - equal one and so we see how we could create this 3d matrix or this 3d tensor from the i
41:52 - method now we could go ahead and say for example one foyer let's run this and see what we get we
41:59 - should get two by four two by four by five by five here we have two by four by five by five
42:06 - and that's it for the i method we move on to the next method the field method
42:13 - here we have a method as defined here in the documentation which creates a tensor
42:19 - filled with a scalar value so here we have this field method which we're going to copy
42:23 - but before testing out the code you could see here from this example that with this field method
42:29 - we have this tensor which takes all which has shape two by three and which has value nine on
42:38 - all different positions so let's get back here paste this out let's call this field tensor
42:47 - and let's say this dimensions let's say we have three by four and then let's still want to have
42:54 - the value um let's say five okay so let's print this out and see what we get field tensor
43:04 - and there we go that's the output we get now let's have this one for example run that
43:11 - see we're able to create this 3d tensor where all the values all the elements in each position
43:18 - takes the value five from here we'll go on to the ones method and it should be noted that this
43:23 - ones method is quite similar to the field method in the sense that this creates a tensor just like
43:29 - with a field method but the only difference is that here all the elements are set to one so you
43:35 - notice that here in this definition we do not have this value argument right here we had a value which
43:42 - we passed but with the ones there is no value because the value by default is one so let's
43:47 - have this here and then paste out right here see we have this ones so we'll call this ones tensor
43:56 - um there we go we specify a shape let's say five by three and then we print that out so here we
44:04 - have ones tensor and there we go we have this output matrix which um is made of five rows one
44:14 - two three four five and three columns one two three where all the elements of this tensor take
44:23 - up the value one now you could obviously do this to have or to obtain a 3d tensor so that's the
44:31 - output we get you see it's five by three by two from here we'll look at the ones like
44:39 - and what this one does is it creates a tensor of all ones that has the same shape as the input
44:49 - so what this means is if you have an input like this one let's say we have
44:56 - twelve let's not make that one twelve one three and then five um seven two this actually a
45:05 - two by three matrix let's change this so you see that the this is two so we have the shape
45:12 - the shape is um two by three so its shape is two by three now if this is the input
45:20 - here if this is the input into our ones like method what we'll get as output will be
45:27 - um a matrix or tensor with this same shape that is we shape two by three so our output is going
45:34 - to be having a shape two by three meaning that we're going to have um two rows and then one two
45:42 - three columns but the output or rather the values we're going to insert here will be all ones so
45:50 - now we're going to have one one one one one one so hence the name ones like so this like is actually
46:03 - for the shape of this so you imitate in the input with respect to its shape now getting back here
46:12 - you see we had this field tensor which is a one by three by four tensor so let's um let's
46:18 - have this right here let's say we want to have uh ones like tensor there we go tf ones like
46:28 - and then we have field tensor field tensor there we go okay so let's print out those ones like
46:38 - tensor now before printing let's um try to obtain the output so here we know that this is field
46:45 - tensor the shape is one by three by four so it means that we should have an output of shape one
46:50 - by three by four where all these values are ones so let's run that and and make sure that that's
46:56 - the output we have there we go c is one by three by four and we have all ones from here we move to
47:06 - the zeros method so it's quite similar to the ones here uh just like with the ones we have
47:12 - um all elements set to a given value with the ones all the elements will set to one
47:17 - with the zeros all elements are set to zero so that's essentially it now let's just modify the
47:24 - code a little right here so this was once tensor um let's change the shape this three by two
47:31 - and then instead of ones here let's say zeros here we have zeros and um let's call this well
47:39 - let's say this is let's just copy this out and put out separately so here we have this
47:46 - you paste that out here we have zeros and here we have zeros fine and here we have zeros okay
47:59 - so let's run that and there we go we should have this here so you see three by two three rows two
48:06 - columns that's fine and all values are zeros if you were once then you have all values once
48:12 - and for the ones or the zeros like is similar to the ones like so you could take that as a simple
48:19 - exercise we then move forward we have this shape method right here where we thought that this
48:26 - returns a tensor take note of that it returns a tensor containing the shape of the input tensor
48:33 - so um as we had seen before we could obtain the shape of a tensor by simply um making use
48:42 - of that tensor's name dot shape now if we want to have an output like in our case now we want to
48:50 - have this output um tensor which contains the shape of our tensor then we could make use of
49:00 - this shape method from tensorflow so you see here we have the same four by two by three
49:06 - but this is now in this list um and then we have its shape and we also have its data type obviously
49:13 - it's made of integer so we shouldn't expect to have a float data type right here
49:19 - another method which we could make use of is a rank method which simply returns the rank of a
49:24 - tensor so it takes an input and then returns its rank you have a simple example right here
49:30 - where we have this 3d tensor t and then when you call on the rank method you see that we obtain
49:37 - the output which is three so let's um put that out here let's just paste it out here and test
49:44 - that quickly there we go as you could see because the output um is a zero-dimensional tensor the
49:53 - shape it actually has no shape but you could see its value right here that's three now if you take
49:59 - out this and take out this and then take out this other part here there we go we run this again we
50:10 - should have a rank of two this time around that's fine it's here rank is two we now move to the size
50:19 - here the size method which returns the size of a tensor so we put an input um in the size method
50:26 - and it gives you the size of the tensor we have as usual in the example here which we could copy
50:32 - but before running that we could read this um note here what we told this returns a 0d tensor
50:39 - representing the number of elements in the input of type output type this is the output type here
50:46 - we by default is int 32 so let's get back here and paste this out and check out the size
50:55 - as you could see here we have this size of 12 and that's because we have 12 different
51:01 - positions or 12 different elements in our tensor now let's take this off again take this off
51:10 - take this off there we go let's run that we should have six so that's it you see we have six
51:18 - and here we could also specify this d type um let's say float 32 we get an error here
51:29 - uh got an unexpected keyword argument d type now getting back here you see this is actually out
51:35 - type not d type so let's get back here and then we have um out type so we have out type there we go
51:46 - run that and as you could see right now we have a float instead of an int the next step
51:56 - we'll take is creating our own random tensors and that is essentially creating tensors which
52:03 - take up random values now in the case of tensorflow random normal our output values are going to be
52:12 - from or are going to be drawn from a normal distribution and we are going to explain at
52:18 - least at a high level what this really means for now let's just copy this out and get back to the
52:26 - code and run this and see what we get so right here let's say we have random tensor there we go
52:35 - we need to specify the shape so we'll say let's take let's make um a matrix or 2d tensor so let's
52:42 - say it's three by two now you see we have this mean value standard deviation values which have
52:48 - been given right here by default and we're going to look um at what this actually means shortly
52:55 - the data type is float 32 cdc is known name noon now let's run this or let's print this out and
53:01 - see what we get so here we have random tensor and there we go as you can see we have the set of
53:08 - values which are all negatives and if you notice they are very close to zero so here we have
53:16 - practically negative one about negative two negative zero point three very small number
53:24 - and negative zero point five negative zero point eight seven so this number is very close to seven
53:30 - let's run this again so you see that we're able to randomly generate a matrix with this shape
53:37 - with random values it takes up different numbers this time around we have a mixture
53:42 - of positive and negative numbers but again these numbers are very close to zero
53:49 - now what if we modify this mean so we modify this mean run this again let's see what we obtain
53:56 - you see that we have uh these numbers now which are instead close to a hundred and so what this
54:04 - tells us is that we could make use of this mean and the standard deviation to decide on the kinds
54:13 - of random values we want to have here now to better understand what is going on let's consider
54:22 - this figure from this probability playground by Adam Kerningam from the university of Buffalo
54:29 - as you had noticed in the code when we did tensorflow random normal and the mean when we
54:38 - specified the mean yeah the mean is this mu you have here when we specify the mean to be equal
54:44 - zero most of the values we had were surrounding zero and when we set this mean to a hundred when
54:54 - we set it to a hundred most of the values were surrounding a hundred now this curve you have here
55:02 - actually explains why so you have this curve right here uh which is bell shaped and the idea here is
55:10 - that it permits us randomly pick values around the mean that's around zero so that's why you'd
55:20 - notice that at zero we have the highest probability score here our probability score is f of x so um
55:31 - for values around our values surrounding zero that's this value surrounding zero
55:37 - so there is much higher probability or chance of them being picked as compared to values far away
55:49 - from zero so let's pick out these two values let's say we have um let's say we want to pick
55:56 - out the two values 0.5 let's say 0.5 and we want to pick out negative five you'll see from here
56:05 - that the probability of of us having negative five is particularly zero that's almost zero
56:11 - but the probability of us having 0.5 as you could see from here if you're taking this middle
56:16 - the probability of us having 0.5 is about 0.35 now if we change this value and take a hundred
56:27 - as we didn't with the code let's take a hundred we have that um hundred well it looks like they
56:37 - fixed this to six so we cannot go above this we could play around this like this you go from
56:43 - negative six to six okay let's say the mean is six so we have a mean of six one thing you can
56:48 - notice is that now this um 0.5 as you could see 0.5 let's take this off um 0.5 no longer has a
56:59 - probability of about 0.35 of being picked 0.5 has a probability now of about zero of being picked
57:06 - while negative five also still has a probability of about zero being picked but um on the other
57:12 - hand as you could see here let's take this off you see that values like five have much higher
57:17 - probability have been picked seven eight six they now have much higher probabilities have been
57:22 - picked now this means that if this was a hundred or if we're able to get to a hundred then uh
57:29 - will be values like 97 uh 98 99 100 101 102 and this explains why when we run this code with this
57:41 - mean we had values surrounding a hundred and that's simply because those values surrounding the mean
57:47 - have um higher probabilities of being picked now let's take this back to zero and then talk about
57:55 - the standard deviation so we have the zero there we go see get back to zero and now let's talk
58:01 - about a standard deviation but before um get into any um explanations let's modify this let's say
58:08 - we take 10 here okay it looks like the max is fixed at 2.5 okay so let's let's say we have that
58:14 - 2.5 one thing you can notice is we still have this our mean at zero we still have our mean at zero
58:22 - does it make sense because we haven't changed this but our bell curved um shape now appears wider
58:32 - so an increase in sigma sigma here is what we call the standard deviation so it's what we
58:41 - called std dev in the code so we had a mean and the standard deviation so sigma standard deviation
58:49 - the sigma square is the variance so sigma square is the variance anyway uh what we're trying to
58:56 - say here is an increase in sigma will make this um curve wider and a decrease let's decrease sigma
59:05 - you see decrease in sigma makes the curve thinner and narrower now what this implies is that if we
59:14 - have let's say let's get back to this if we have sigma square as 2.5 there are much higher chances
59:21 - now that we could um randomly select negative five as compared to before so if this is negative
59:29 - five here you see there's a slightly higher probability of negative five and peaked as
59:35 - compared to when we reduce this you see when we reduce this um you see negative five here
59:42 - is practically zero but when we increase see negative five is at least this time around having
59:49 - some negligible value and so this is essentially the row that the standard deviation plays
59:56 - now if we reduce this again we find that even 0.5 because this is 0.5 around here even 0.5
60:03 - let's take this off even 0.5 which used to have a probability of about 0.35 of occurring um now
60:12 - has almost no chance of being picked let's increase it back and set it back to zero okay
60:20 - and so that's it at this point we could modify this we could take this to six and then take
60:27 - this to 2.5 and you see that we could take values now between zero see go from zero to about
60:34 - um 11 with that said we'll then look at the uniform distribution or how to generate random
60:43 - numbers or random values drawn from a uniform distribution so again we'll just copy this out
60:49 - and then get back to the code we'll paste this here there we go we have the shape let's pick
60:57 - um this uh one okay and then minimum value zero and now we have maximum value so you see that
61:05 - unlike here where we have the minimum the the mean and the standard deviation now we have instead of
61:11 - a minimum value and a maximum value so let's let's let's leave the leave it at this and then let's
61:18 - say we have random tensor random tensor okay so let's print this out let's print out our random
61:27 - tensor there we go we have random tensor let's increase this so we have many more values five
61:36 - from that okay you see we have these values now let's let's say we change this max value and say
61:43 - it's um eight eighty or let's say eight okay let's say eight let's run that and what do you
61:51 - notice now is you have much larger values well values between zero and eight but um before
61:59 - the values we're getting where values ranging between zero and one now this tells us that
62:05 - most probably this maximum value by default is one so here uh you see it's default so one you
62:12 - could always make use of the documentation whenever you're in doubt so let's get back here
62:17 - you see we'll take this to 100 and then see what we get you see we have this values now between zero
62:26 - and 100 now the question you may be asking yourself is what then is the difference between
62:31 - this uniform and this um normal distribution so let's get back again here um in our probability
62:38 - playground and then let's pick out our uniform so there as you could see here many other
62:45 - properties distributions but we're going to focus on the normal distribution and the uniform
62:50 - distribution so we get into the uniform distribution and here's what we get so we have
62:57 - a and b now this is like our mean vowel which we saw in the code and this is like our max vowel
63:05 - so when we say mean vowel negative one max vowel one obviously our values will fall in this range
63:12 - now by default this is zero and this is one on tensorflow so um that's why you saw the values
63:19 - ranging between zero and one now as the term goes it's actually uniform so this means that
63:26 - all this um values here have equal chances or probabilities of being picked and so unlike the
63:38 - normal distribution where we would have something like this something like this now we have this
63:46 - square instead or this rectangle and so simply here all we're doing is picking the range of
63:55 - values which we want to have or which we want to be outputted so we could get here increase this
64:02 - um where you cannot you cannot the mean cannot pass the cannot be greater than the max so that
64:07 - makes sense let's reduce this reduce that see from negative four to four see we take all the
64:12 - values well that's it see there we go we could go from negative one to two that's it now one
64:19 - great thing is also the fact that we could modify this and have ints so let's run this and see what
64:26 - we get you see we have only integers now now let's say we had norm let's run that you see we have
64:34 - an arrow most specify a max vowel in the case where the data type is integer so this actually
64:42 - um reading out here in the documentation so you can see here now that you need to have this max
64:48 - vowel so let's change that to let's say 1000 so you get values ranging between zero and 1000 so
64:56 - we have five values here from the shape we could change this let's say five by five and then all
65:03 - this values range between zero and 1000 another argument which we have been spoken of so far is
65:11 - this seed argument and here we're told that this seed argument when used in combination with
65:20 - tensor flow random set seed we'll be able to create a reproducible sequence of tensors across
65:28 - multiple calls and so in cases where we want to produce reproducible experiments we want to set
65:37 - we want to set this global seed value and also the seed argument value in this uniform or let's say
65:48 - normal function so let's go ahead and copy this out we get back to the code there we go let's just
65:56 - run this right here let's take off some parts and print this out so we print this out
66:03 - then we modify the shape and the max values so we have exactly the same value for the seed year
66:10 - which is 10 and then here we set this global seed to five let's run this you see it outputs 4 3 1
66:17 - 4 3 2 1 1 1 and 1 3 3 now let's create another cell um let's copy this there we go paste this
66:27 - out here and you see we have the same global seed um and then we have the seed of 10 now let's run
66:34 - this again and as you could see we have exactly the same output as this one see 4 3 1 4 3 1
66:43 - 4 3 2 4 3 2 1 1 1 1 1 and 1 3 3 now obviously modifying this and taking for example one gives
66:54 - us something different but if you take 10 again you see you should have exact same output now you
67:01 - also see that here when we take this off if we take this off and we run this cell you see we have
67:07 - different outputs but when we get back here let's let's run this again see we have another set of
67:14 - different outputs but when we take this run that you see we have exact same output we had already
67:20 - and now if you want to have more details about certain seeds you could always check out this
67:25 - um the documentation now we're going to move on to tensor indexing so let's take this example we
67:33 - have this tensor to be indexed and we declare this we have the state of the constant and this
67:39 - 1d tensor we pin this out this is what we get now supposing we want to get this first four elements
67:48 - to get this four four elements we need to consider these are the indexes so here we have the 0th
67:55 - index then here we have the first index here the second here the third and the fourth so we want
68:03 - the first four elements all we need to do is take out this first four elements by doing this so we
68:10 - have indexed and then we have the square brackets we start with the minimum index and then we have
68:17 - minimum index that's zero that's the 0th index which is this we go one two three four now here
68:27 - we specify four there we go and that's what we get we have this first four elements right here
68:37 - now you might have noticed that the fourth index is actually this year but then since we started
68:43 - from zero our third index happens to be the fourth value so if we're taking this from zero
68:50 - we have one two three four so y'all first four values now note that if you want to grow from the
68:58 - zeroth or from the first let's say we want to go from this index here six to 66 if you want to
69:05 - go from six to 66 what you will have to do is this we have that we take out zero we go start
69:15 - from the first zero one so we have this one right here the minimum and then want to go up to 66 then
69:22 - we'll have to go one two three four five now if you put this five right here you'll notice that
69:30 - it's not going to give you exactly what you want so let's run this and then we get that so see that
69:35 - we have here six two four six we actually have this instead of all this so how do we do to get
69:42 - all this in order to go from one index to say a maximum index what you need to do is to add this
69:50 - plus one so you want to go from the first index to the fifth index right here that's one two three
69:56 - four five want to go up to this index i need to include i need to put this plus one it's similar
70:01 - to with a tf.range read scene previously where in order to put out an tensor from let's say
70:10 - a range two to five what we needed to do was specify this from two to six so that it would be
70:15 - like two three four five so yeah what we did was add two five plus one so it's a similar kind of
70:23 - pattern that we're following right here so to go from this up to six to six
70:29 - we need to add plus one so let's run this now run that and there we go is we have exactly
70:35 - what we expected so let's take this off now and we have six so with this we see how we're able
70:41 - to slice out some parts of this array of this tensor from here we could also include steps so
70:50 - if we put out say two steps of two you will notice that we go from the first that's the
70:57 - first index this is the zero index so we'll go zero we start with this one we skip by two so
71:05 - we skip this element and then we go straight to four so notice how we go four and then from this
71:11 - four we skip this element and then we go to six to six so that's it so we could include the steps
71:17 - right here now by default we have a step of one so when we do this you see we have exactly what
71:22 - we do what we get when we don't put the steps at all so that's it we've seen that in general we
71:30 - have a minimum value right here and then we have a max value or rather a max index so yeah yeah
71:40 - we have so that's it in general we have a min index up to the max index plus one so generally
71:49 - this what we get now in the case where we don't specify this min index that's if we have this
71:55 - for example let's suppose we have zero this we don't specify any min index and then we go up to
72:00 - four then this is considered to be zero so you notice that this tool come up with the same answer
72:06 - we have exactly the same answer right here when you specify the minimum value say three and you
72:13 - don't specify the maximum we're just gonna go right up to the end so let's run this we'll see
72:18 - that we'll get zero one two three so we're gonna start from this and then go right up to the end
72:23 - so we'll have four six six six seven so that's it we have exactly as expected four six six six
72:30 - seven because we're starting from three and we're going right up to the end if we want to go from
72:35 - this first off from this minimum index right up to the last but one value that is want to go from
72:42 - this four right up to 66 what we could do is have our maximum value to be negative one so doing this
72:50 - you'll see that we have four six six six so we're going from this index this minimum index to the
72:55 - last but one value now let's take this to two see we have four six take this to three we just have
73:04 - four take this to four this should be an error from this point we'll see how indexing is done
73:11 - with tensors of dimension greater than two so we will start with a two-dimensional tensor which
73:18 - we had declared previously now for indexing to be done we'll start with a comma right here so we
73:25 - have this comma and then if we want to get the first the first three rows so suppose we want to
73:35 - get this first three rows want to get this first three rows and then in this first three rows
73:41 - we want to get just the first two columns so we want to get this colon and this colon in fact
73:48 - fact what we want to get is this one two three five one five this happens to be
73:52 - the first three rows and the first two columns so in order to do this we note
73:58 - as we said here we have this comma and then to this left side we have the rows
74:04 - and then to the right side we have the columns so what we do here is since we
74:10 - want to get the first three rows we're going to go zero one two so what we're
74:14 - gonna have here zero right up to three so we're gonna get zero one two obviously
74:21 - two plus one three so we have this so it's kind of like similar to what we've
74:25 - seen already here but with the difference that all this let's say all
74:29 - this gets to this left side and then for the columns are right here for this yeah
74:35 - for the columns want to get the first two columns so what we're gonna have
74:40 - here is zero two so we'll have that there we go and then let's run it so
74:48 - running that we say we have one two three five one five now let's suppose we
74:52 - want to get this first three rows like this and then want to get all the
74:57 - columns so basically here what we can do is this so just like you're getting all
75:01 - the elements we just had to do this that is with this column yeah we just have to
75:07 - put this column right here and then we're gonna go so let's run that and we
75:13 - should have one two zero three five negative one one five six so we've got
75:17 - in the first three rows and then all the columns now if you want to get a
75:24 - particular row so suppose I want to get just the third row that's want to get
75:29 - only this row right here we're gonna specify its index this is zero one two
75:34 - so here we have two so for the rows recall we have the left side
75:39 - representing the rows and the right side representing the columns so yeah what
75:42 - we're saying is we're getting the second index or this row the second index or
75:47 - the third row which is which happens to be this and therefore the columns to
75:52 - get in each and every column so let's run this we should have one five six now
75:58 - if we want to specify or if we want to get just some of these columns that is
76:02 - if we want to get maybe the this only this column right here we're gonna have
76:07 - two and then we specify the zeroth column so run that and we should have one
76:14 - now if we want to go ahead and take say one right up to the end you see we have
76:19 - five six because here we'll specify want to get a second row that's zero one two
76:26 - second row and then for the columns we're going from one this one because
76:32 - this is zero one two one right up to the end so that's it just like with the rows
76:39 - we could also specify or we could also get just a particular column so suppose
76:44 - we want to get this zeroth column right here which made of one three one two so
76:50 - to have that done we are taking all the rows so we're gonna have all the rows
76:56 - and then we will pick out just the zeroth column so let's run this and we
77:02 - have one three one two which is this column right here now let's take this
77:08 - from here and we'll see we have an error because we haven't specified how this
77:15 - rows are gonna be managed so here again let's say we want to do one two three so
77:23 - right here we have three one three one because we're making for the columns
77:28 - we're taking the zeroth color so we're taking all this and then for the rows
77:31 - we're going for one two three so zero one this our first this one two and then
77:38 - three meaning that we're stopping at two because stopping at three minus one so
77:44 - we're going we're taking this and then this and then since we're taking the
77:47 - zeroth column we just have three and one so that's how we get this so let's come
77:52 - back we have this and then if we want to pick out the first the column the first
77:57 - index we have two five five three now this year means we're picking up all the
78:03 - index indices so if I'm picking up all the indices what we could do is specify
78:09 - just this three dots three dots simply means picking up everything so we're
78:14 - picking up all the indexes or we're picking up all the rows and then we'd
78:19 - specify in the first column so running this should give exactly the same
78:23 - upwards we could see right here we now move to the 3d tensor so we have this
78:29 - 3d tensor right here let's take this off now we kind of like this kind of like
78:35 - very similar to what we've seen already with the 2d tensors but with the
78:39 - difference that we have the two commas now we're asking why we have this two
78:43 - commas recall that with three dimensions we have the first dimension the second
78:48 - dimension and the third dimension so first thing let's put this this way
78:54 - let's have this we have this each of this is a two by three shaped tensor so
79:04 - here we have two by three that's what's not a shape actually and then yeah we
79:12 - have one we have one two three four elements so we have four elements so
79:19 - this happens to be the shape of our 3d tensor right here now if I want to pick
79:25 - out just this first elements right here all I need to do is specify a zero and
79:30 - then I could take all this all this simply means that I've picked out this
79:36 - first index and then taking all the rows and all the columns so let's run
79:42 - this and you should get just this right here so we see we are 1 2 0 3 5 negative
79:47 - 1 so that's fine now if in this index that if after selecting this index I
79:52 - want to say get only this first row what I'll do is I'm gonna take 0 and then
80:00 - right here I'm gonna pick everything so I'll run it and I'll have 1 2 0 I have
80:06 - only this right here now if I want to get this last column what I'm gonna do
80:11 - is I'm gonna specify I want to get all the rows and then for the column I want
80:15 - to get the last column so there we go I have the last column 0 negative 1
80:20 - because I'm turning from the end actually here now another way we could
80:25 - do this is by saying okay I have 0 1 2 so I'll take the second index run that
80:30 - and I should have the same answer so that's it now if I want to go from 0 to
80:36 - 2 so I'm gonna pick 0 to 2 means I'm picking 0 and 1 so I'm picking this 2
80:40 - actually picking this 2 around that see I have 0 0 negative 1 2 all right I have
80:46 - 0 negative 1 0 2 the reason why I have this is because right here I have the 0
80:51 - negative 1 and then yeah I have 0 2 so that's it the 0 negative 1 is because I
80:57 - have picked I've picked this 2 indices so 0 negative 1 is the last right here
81:03 - the last index or the last column pick out the last column and therefore this
81:08 - I also pick out its own last column so that's how this works we just have this
81:12 - commerce we separates this now another way I could do this is I could take all
81:16 - so doing all I should have exactly the same response so instead of having that
81:21 - I could just put out this three dots right here then here I could pick out
81:27 - also yeah I'm supposing I'm taking all this since we're picking up all this
81:31 - indices that's we're picking a 1 2 3 4 all this four elements we're picking up
81:37 - for each of these elements all the rows picking this all the rows and then for
81:44 - each of these we also pick up just the second column so if we yell picking up
81:50 - all those rows and just a second columns that will be left with 0 for this and
81:56 - negative 1 then for this we left with 0 2 this 0 0 there's 932 we now move on to
82:06 - tensorflow.math which we could see clearly here
82:12 - we now move on to tensorflow.math which we could see clearly here we'll be able
82:18 - to use all these math functions which are made available with tensorflow so as
82:27 - you could see we go right up to this zeter function so starting from the
82:32 - apps function this apps here clicking on it will have tf.math.apps and note
82:39 - that for each and every function you'll have the function definition and you
82:44 - will have an explanation so like you have this kind of example where the
82:48 - function has been applied and you'll be able to understand exactly how those
82:54 - functions are used so in case you want to understand how the A10 to function
82:59 - works just click on that you have this we click on A sign actually let's all
83:04 - let's just work with A sign in case we don't work with the A sign you would have
83:07 - this you have this example and then you get this explanation on all these
83:14 - arguments which get into this function like the X you have the name and then
83:19 - what it actually returns so you don't need to always figure out by yourself
83:24 - how this functions work you just basically need to make good use of this
83:29 - documentation now that said let's look at a couple of functions we have the
83:34 - apps function it's actually the absolute value so it gives absolute value of a
83:38 - tensor now to get an absolute value we simply pass in the tensor so like in
83:45 - this case where we define this X so X apps we have the X which is this tensor
83:52 - right here and then we could get an absolute value very easily so we have
83:56 - that we run it and you see we have the absolute value now what's an absolute
84:01 - value up to the values are defined as such if the number or the input is a
84:06 - negative so if the input is a negative it sends it to positive so it turns it
84:11 - into positive if the input is a positive it turns it into a positive so it
84:18 - remains the same or basically it remains the same so that said we could say for
84:22 - example TF that apps of say TF that constant 0.2 if you run that you'll see
84:31 - you still have the same now if we tend this to negative 0.2 you would have still
84:37 - 0.2 so it takes it from a negative number to a positive number or
84:41 - basically we can say that the absolute value of X is negative X when X is less
84:48 - than 0 and it's equals X when X is greater than 0 so if we have a positive
84:53 - number it remains the same and if we have a negative number you attach a
84:57 - negative sign and obviously a negative with a negative would turn this to a
85:01 - positive so that's why when you have this negative 0.2 here you have
85:05 - negative of negative 0.2 it turns into 0.2 so that's it for the absolute value
85:11 - function you could also check out when we have the complex number that is if a
85:17 - tensor if we have a complex tensor you could check out this absolute value
85:22 - function right here the inputs of this absolute value function can also be
85:27 - complex numbers now if we have a complex number say a plus bg then its absolute
85:36 - value is computed as a squared plus b squared all of that square root so if we
85:42 - have negative two point two five plus four point seven five j as see it looks
85:48 - exactly same as a plus bj right here then we'll have the absolute value or
85:53 - get absolute value by doing negative two point two five square plus or rather the
85:59 - square root of negative two point two five square plus four point seven five
86:03 - square that's that let's take this off let's just have one so if we have just
86:09 - this one right here okay now we run this xops xops complex complex and then yeah
86:21 - we have xops complex to complex okay so let's run this and you get this output
86:30 - five point two five now let's do a squared as let's take this if we have
86:36 - the square root we'll be using the square root method so we have to have the
86:39 - square root of negative two point two five square notice how the this pops up
86:46 - right here with the definition nice with the documentation of the square root
86:51 - method so we have a square root text in this input and then name so basically
86:56 - we just find the square root of all the elements we have in our tensor so that's
87:01 - it we have negative two point two five plus four point seven five square so four
87:06 - point seven five square so that's it we completed square root and we see that
87:12 - we do not have the same answer that's because we didn't put this in a bracket
87:17 - so there we go we run this again and we see we have exactly the same answer so
87:22 - we now take on the addition so yeah we look at this addition function this can
87:28 - be seen as element wise addition so supposing we have x1 and then x2 tf.constant
87:39 - we have c7 6 to 6 7 and 11 the type a specified a d type tf.int 32 we
87:54 - could simply do tf.add x1 and x2 so let's run this we'll get our output
88:03 - it's basically the addition of all this element so it's an element wise addition
88:07 - now you could check out other methods like multiply so you could have multiply
88:12 - multiply we run that c5 times 735 and so on and so forth you could check out
88:18 - subtract so we could have you subtract and you have your answers let's check
88:24 - out divide divide there we go run that there we go five divided by seven to
88:31 - rewrite by six and so on and so forth then we also have other interesting
88:36 - methods like this divide no none so in the case where you divide into values
88:41 - and you expect it to have a man like when you divide him by zero expected to
88:45 - have a man that's not a number this method takes care of that exception for
88:51 - you so let's come in here and instead of zero run that again you see we have this
88:57 - now let's of this infinite infinity so let's now say no man run that we have
89:08 - this error because this is actually a math so you should have that we have
89:14 - here that tensors of type interior two are not allowed so let's take for
89:19 - example a float so let's change this we have float float there we go run that
89:26 - again and it's fine so you see that when you have this like if you have it a
89:32 - problem where you just want to have this NAND or this output where it's meant to
89:37 - have infinity give you a zero then you could use this method right here now
89:43 - note that it's not every time that while we're doing this element wise operations
89:49 - that we have the two tensors being of the same shape so that said we could
89:56 - modify this so let's take this to be just seven right here let's take back
90:02 - the add so we have add and around this again you will notice that we were able
90:08 - to do this addition but with a difference from the others where the two
90:14 - tensors had the same shape in that yeah we have this seven which adds up to each
90:20 - and every value we have right here so seven plus five twelve seven plus three
90:25 - ten and so on and so forth now this is what we call broadcasting in broadcasting
90:32 - we have this smaller tensor which is stretched out match the shape of this
90:38 - bigger tensor so that the patient could be carried out so in fact what's going
90:43 - on here is you have this X2 right here which has been stretched out so we can
90:50 - see X2 stretched stretched and then what we actually have is this so finally when
90:57 - we carry out the operation we have this seven let's make sure we have this
91:01 - values right seven and then we have this last seven so basically this is what we
91:06 - have now let's in this out let's copy this and then we have X2 stretched
91:11 - that's that's it now we run it and we see that we have exactly the same output
91:17 - so if I why you put this tensor flow does broadcasting and this tensor right
91:24 - here is being stretched out like this and then the operation is carried out so
91:31 - we could test this out with multiplication let's modify this supposing
91:36 - we have this right here we have that say five forty five so on and so forth okay
91:46 - so you have X2 and then we have X1 let us put out let's do run with the
91:51 - addition let's take off the shape run that see give the same answer we go to
91:58 - multiplication multiply multiply there we go run it again we see we have this
92:06 - element wise multiplication now another slightly more complicated way of looking
92:12 - at this is we have a shape or when we have a tensor where one of the indices
92:20 - has a length of one so if we take this off let's suppose it would take let's
92:26 - take this what I want off so if you take this one off right here you see we have
92:29 - a shape of 1 by 6 now let's define X2 differently let's take off the X2
92:36 - stretch from here so let's define X2 differently we have in this case instead
92:41 - let's define a 3 by 1 tensor so would have this we have 5 we have 3 so there
92:52 - we go run that let's take this one off let's print out the shapes actually
93:00 - let's print out X1 that shape print out X2 that shape there we go we have this
93:11 - is X1 1 by 6 X2 3 by 1 and then the output of the element wise multiplication
93:19 - is this matrix we have here so now let's explain how we get this matrix yeah if
93:25 - we write this tensor right here would have this it has just one row so we have
93:31 - one row and then six columns as you can see here and then this other where we
93:35 - have three rows and one column just as one column three rows one two three now
93:42 - that said when we want to carry out our operation here we're taking the
93:47 - incarceration the fact that we have three rows right here so what goes on is
93:53 - this one row is stretched out so it's stretched out so that we now have three
94:01 - rows to match up with this three rows we have in this X2 so that said we have now
94:08 - five three and finally six right here so there we go we now have this new tensor
94:19 - that's it and then for this you should guess it right we are gonna stretch this
94:26 - out such that the number of columns match with the number of columns of this
94:31 - so basically we're just gonna rewrite this six times there we go three right
94:39 - here we're reading this six times or stretch this out and now we could carry
94:44 - out our multiplication operation so let's take for example this element we
94:50 - have this six times five right here we should have 30 so that's why we have 30
94:56 - here and if we take 4 times 5 20 that's why you have 20 here if you take this 4
95:03 - by 3 that's why we have 12 right here so we have that as a rule of thumb both
95:11 - should have one dimension of length 1 as you could see here and then the other
95:19 - dimensions are stretched so as to match one another our next method will be the
95:26 - TF the maximum method so there we go we have TF the maximum which gives us the
95:33 - max of two tensors or better set the element wise maximum of two tensors now
95:43 - this should be the same with a minimum which returns the element wise mean of
95:47 - two tensors so that said if we have this two tensors X and Y you see that we're
95:53 - gonna have here negative five so negative five zero negative two the
95:57 - mean is negative two negative two zero zero means zero zero three means zero so
96:02 - that's it as you could see here this suppose the broadcast semantics that the
96:08 - broadcasting we've just seen so that's it for the maximum and minimum we also
96:13 - have the act max and act mean which we're gonna see shortly so let's go we
96:20 - have the act max okay that's it now what I max is a slightly different syntax
96:27 - yeah we have an input and then we specify the axis so let's copy this B
96:33 - right here let's copy this tensor is this out there we go okay we have this B
96:44 - oh we have this egg egg arc X arc max for example so we have this tensor let's
96:51 - rewrite this we have this tensor here and then we let's print out X arc max
97:00 - that shape bring out a shape okay now we want to get the arc max but before
97:08 - looking at the arc max of this tensor let's look at a simpler tensor so let's
97:13 - take something simpler let's have this let's say we have yeah let's close this
97:21 - okay so we have this let's print out X max we have TF TM the mat that arc max
97:32 - this and then we have that we have it okay so let's run that we see that we
97:39 - have a value of 2 now why do we have this value of 2 why do we like with this
97:45 - matrix given to us have this so here now what if we modify some of these values
97:52 - say let's take this to be 200 let's take this to be 120 stay 130 and 0 3 run it
98:00 - again oh we have this run it again we see that now this value changes to 0 why
98:06 - does it change to 0 now if you notice previously like let's copy this again
98:12 - this is on here let's take now this or let's take this to that okay you notice
98:19 - that with this the position of the maximum value is the zeroth index now if
98:27 - we have here this is zeroth index this is our first index this is our second
98:36 - index your third and then your fourth so where is the maximum value here you
98:44 - guessed it right is 200 where does it fall zeroth index and so the output is
98:49 - zero let's take some example where does the maximum value fall it falls here
98:54 - whereas the index 0 1 2 3 and that's why we have this so actually the arc max is
99:05 - different from a max and it says that yeah we're actually looking for the
99:09 - index or the position of the maximum value so that's it if one I have the
99:17 - mean we just have to change this to mean so we could have mean run that you will
99:22 - see we should get this year so we have 0 1 2 3 that's it there's a minimum value
99:27 - this position is the third position now if it changes to mean 2 we have that we
99:32 - should have this last position 0 1 2 3 4 so that's it now when we did it with
99:39 - multi-dimensional tensors like this one right here we can specify and exist so
99:48 - let's do this we have we print out TF the math the arc max of X marks or X arc
99:56 - max and that will specify the zeroth axis now we'll look at the output and
100:03 - we explain why we will have the output so here we have 2 2 0 2 2 now let's
100:09 - understand why we have this output given to us notice that right here we have
100:15 - fixed the axis to zero and so that said we are actually fixing the rows so we
100:23 - have a row we have this three rows or let's take this we have this one row we
100:28 - have this row and then we have this row and then we're doing comparisons on the
100:33 - each element or the corresponding elements on each row so it turns out now
100:43 - to be the column so yeah we compare this two that's on this row we compare this
100:48 - two with this three with this 14 and then yeah we compare this yeah we
100:54 - compare this yeah we compare this let's take that off
101:02 - yeah we compare this 3 1 5 yeah we compare 6 8 27 so that's it so when you
101:12 - fix the axis to zero it simply means comparison is done on the other axis so
101:20 - in this case that other axis is a column so we're doing comparisons on each and
101:25 - every column now what is the maximum value here the maximum value is 14
101:30 - where does it fall in this in the tensor if we have this tensor to 3 14 we see
101:38 - clearly its position if the axis we have the zero position for its position second
101:44 - position so we see that it falls on the second position that's why here you have
101:49 - this too now if we look at this maximum is this falls in the second position
101:53 - that's why you have to we'll continue to this our maximum year is 30 falls on
101:58 - the 0th position if you want to look at it clearly we have 30 extract that we
102:04 - have 30 16 and 23 our maximum false year the 0th index 0 1 2 so that's why you
102:15 - have 0 and then for the next you have 5 and then yeah we have 27 that's why we
102:20 - have this 2 2 0 2 2 now let's take this off and then change this to mean so
102:28 - change that to mean so we have a different angle we have here 0 our
102:33 - minimum is this next is 1 our minimum is this next our minimum is this one
102:38 - minimum is this one next we have this okay so that's it now let's modify this
102:44 - axis so we change axis now we will be working on each row so we'll be doing
102:50 - comparisons on this row comparisons on this and then comparison this so if
102:55 - understood what we've seen previously when axis was equals 0 and we had axis
103:00 - equals 0 then you should be able to pause at this point and then try to solve
103:04 - that for when axis equals 1 so that said let's explain how this works we're gonna
103:11 - have comparisons with this so the maximum years or the minimum since we'll
103:16 - change this to mean our minimum year will be 2 year our minimum is 1 your
103:22 - minimum is 5 so that said we're gonna have 0 since axis position is 0 or we
103:29 - are the 0 index and then yeah we add 0 1 2 3 3 and then yeah we add 0 1 2 3 so
103:39 - we're gonna have 0 3 3 so let's take that off run it and we have 0 3 3 as
103:45 - expected you have other functions right here we have the equal function which
103:54 - could be used to compare tensors so we could compare to tensors and in the case
104:00 - where each and every element is the same we're gonna have this output boolean
104:06 - tensor so you see d type bool and then here we have true true the reason why we
104:12 - have true true year is because this element 2 is the same as this and here 4
104:16 - it's almost 4 now we see again broadcasting year where we have 2 4
104:21 - compared with 2 2 now if you have a tool to broadcast it to match the shape so
104:26 - this 2 turns into 2 2 now the first 2 compares with this 2 it is true and the
104:32 - next 2 compares with false with 4 is not the same so it turns into false so
104:36 - that's how we look at this we have the documentation here we could look at
104:41 - other methods we have this power method right here we have yeah we have the power
104:51 - method yeah we basically to get the power or to raise all these elements
104:56 - we're given power let's copy this so we better understand that this is our right
105:02 - here scroll down okay so we have this X we have the Y and then let's understand
105:08 - this power now basically what you understand here is let's reduce this to
105:12 - 0 and then here 1 okay so as we're saying stick this of force we should have
105:18 - 81 there so while we have it you guys are taking 2 to the power of 3 2 to the power of 0
105:24 - 3 to the power of 1 3 to the power of 4 so if we run this this off run this you see
105:30 - clearly we have 2 to the power of 3 is 8 2 to the power of 0 is 1 3 to the power of 1 3 3 to the power of 4
105:37 - 81 so that's how it works now we could have TF the power sorry power and then
105:43 - TF that constant of C2 and then TF that constant constant of say 3 so we have
105:54 - that run it there we go we have 8 on next we will be just reduces we have here
106:00 - the reduce all reduce any reduce Euclidean reduce max mean prod STD some
106:07 - variance let's start with a reduced sum which is kind of like the most popular
106:11 - of all this so we have the reduced sum what goes in here is it competes it
106:18 - takes in the input tensor exists keep that keep dimension set to false by
106:23 - default name known so it computes the sum of elements across dimensions of a
106:27 - tensor so again here we're specifying the axis and we're going to sum through
106:32 - that all the elements in that axis so let's look at some examples this is our
106:39 - right here so we have this input tensor let's copy let's say we have let's work
106:46 - with one of the tensors we had declared previously so let's have this tensor to
106:50 - the right here we have tensor 2d we have it here okay tensor 2d there we go now
107:00 - we want to print this out print of this we have that and then in here we have
107:07 - our tensor 2d tensor 2d there we go we specify the axis let's let it let it
107:16 - let's first run this and see what it gives us its output so we told we have
107:21 - an error oh we didn't close this run that again okay so we have this year
107:28 - first of all this is this 2d you created in the hour let's run this okay so now
107:35 - we have this value of 35 how do we have this value of 35 when you don't specify
107:39 - the axis what goes on yours we just add up all elements so we reduce our
107:46 - elements I will reduce the tensor and then the reduction is done by summing
107:51 - all the elements which make up this tensor 1 plus 2 plus 0 is 3 3 6 11 10
107:58 - 11 16 plus 6 22 24 27 and 35 so that's how we get all this already that's how
108:08 - we get this by summing up all this so this will reduce sum that said we could
108:14 - do a reduce max so we just max there we go we have 8 so what goes on here is
108:22 - we're reducing all these values into just its maximum value so we see with
108:28 - the sum we adding up everything with a max we're just looking at a maximum
108:32 - value so if it changes to let's say we take this to 100 you see you get now a
108:37 - hundred now if you look at reduce mean you run that it should have zero so
108:42 - that's it let's take negative here run it okay we have negative too so that's
108:48 - it now let's go back to our reduce sum and then let's specify the axis let's
108:54 - say the axis is zero let's print back the shape of this tensor to the tensor
109:02 - to the that shape let's run that so we get a shape okay it's 4 by 3 now let's
109:08 - okay we specify the axis zero now we have 7 11 4 or 140 now why do we have
109:15 - this specifying this axis zero means we fixing the axis our way like taking a
109:23 - fixed position on this axis zero and then we paint around with the other axis
109:28 - which is the axis one and which happens with a column because we have the rows
109:34 - axis zero and then we have the columns axis one so what goes on here is we now
109:41 - doing this we comparing already we summing up we reducing this into one we
109:47 - reduce this and then we reduce this so one plus three plus one plus five plus
109:55 - two seven so that's how we have this negative two plus all of that eleven if
109:59 - you add all of this you have 140 and so that's how we get this by when we fix
110:04 - this axis to zero now let's fix the axis to one you'll see that we'll fix the
110:10 - columns and we paint around with the rows so yeah we compare this all right I
110:14 - will sum this up summing this up we have negative one this should be a hundred
110:19 - and eight this should be 12 and so on and so forth so let's run it and we see
110:23 - that we have negative one hundred eight twelve thirteen and that's it so that's
110:29 - how we look at this now we could go back to max reduce max that's it we could go
110:36 - to reduce mean see zero thirty six four fourteen now the mean is like the
110:42 - average value so let's take this axis oh let's let's do that zero run it and we
110:48 - have that now what's the main of this when comparing this we have one plus
110:53 - three four plus one five plus two seven seven divided by four is one point seven
111:02 - five but since we didn't with integers we just have one so let's change this d
111:07 - type let's say we want to have a d type of TF the float 16 so we run that and
111:14 - there we go we have one point seven five so that's it we just calculate in the
111:17 - moon of each on each of these columns because we've fixed we've said to fix
111:26 - the rows and then we paint around with each and every column now we could
111:32 - change this to standard deviation standard deviation we have that zero
111:36 - point eight to nine two point eight six forty one point three eight so that said
111:42 - we now look at the keep deems now take a note look at this shape right here we
111:47 - just have this one dimension tensor here now when we do this to won't send this
111:53 - to true we run that we see that distance or two dimensional tensor with the same
111:59 - elements but we've added this extra axis of extra dimension right here and you
112:06 - could actually get to see this explanation from this so that's it we've
112:11 - looked at the reduces or you reduce max mean STD pro variance none of that now
112:18 - we continue our journey through our mat functions we have this sigmoid which is
112:25 - one of the very popular method the sigmoid we're gonna see this is
112:31 - basically this formula is this guy this function so y equals it takes in X and
112:36 - then what it does it takes the exponential of negative X 1 plus
112:40 - exponential and then 1 divided by 1 plus exponential so basically this is this
112:45 - function and then when we run the sigma we just take each and every element and
112:51 - then pass into this function our next method will be this top k mat function
112:56 - with a top key what we're doing here is we're taking as input our tensor and
113:03 - then we're taking the top or the top say if K equals 2 we're taking the top two
113:10 - values just like in the class of a hundred students and then you want to
113:13 - get the top 10 students oh it's kind of like the same function so that's it we
113:20 - have that let's see how that works in the notebook so yeah we have top K
113:28 - right at TF the mat that top K and let's just take in this tensor 2d right here
113:35 - so we have the stands are 2d we run that you get this output note that this
113:41 - output comes out or with two values so we have first of all this first part of
113:49 - the couple and this second part of the couple the second part has to do with it
113:53 - indices that is the positions of this top K values so by default as we saw the
114:00 - K is equals 1 so by default we have hopes by default we have K equals 1 so
114:07 - that's what we have by default so run that we should have the same answer so
114:11 - that said let's come slide this up okay so what we're seeing here is we going
114:18 - through each in every row so we're going to each in every row we look what's the
114:24 - top value right here is one what's its position zero so we have one year
114:28 - position zero so our value has a hundred position two top value is six
114:32 - position two top value is eight position two so that's how we get these values
114:36 - now let's say we want to take top two top two we see we have the top two values
114:42 - one and zero we have their positions 105 positions six and five that's it
114:47 - eight and three as you can see that's it and by default this is sorted so you
114:54 - don't need to bother about sorting now you could change this to false in case
114:58 - you don't want to do or you don't want to have it necessarily sorted
115:03 - let's now go ahead and look at linear algebra of patients so here we have TF
115:11 - dot linoc good right here we have TF dot linoc and we have all those different
115:17 - linear algebra of patients which come with tensor flow let's look at the
115:22 - matrix multiplication giving yours matmul so TF dot linoc dot matmul what does it do
115:29 - multiplies a matrix a by matrix B producing a times B now note that this
115:37 - is different from the multiply the TF dot mat the multiplier we had seen
115:41 - previously as the TF dot mat the multiplier was an element wise
115:45 - multiplication whereas here we're working with a matrix multiplication so
115:51 - right here we pisses out we have TF dot linoc dot matmul and then let's go ahead and
115:57 - define this two matrices X1 TF dot constant and then X2 so we have X2 just
116:06 - we copy this and then we have that now we have our X1 X2 and then we want to
116:14 - find a matrix multiplication of the two matrices so it will piss here X1 and
116:18 - then we have X2 for now we have all this default values so that's fine now let's
116:26 - run this and then see what we get now what do we have we have an arrow which
116:31 - is normal we've been told that there is a mismatch now why is there this
116:36 - mismatch if we check out the shapes like you know the shapes of X1 and X2 X1 X2
116:44 - shape you would see that both have shapes 2 by 3 2 by 3 now if you want to
116:51 - do matrix multiplication then this isn't possible since in order for matrix
116:56 - multiplication to be valid the total number of columns we have here must
117:02 - match the number of rows we have here so since there's number of columns which is
117:08 - equals 3 zero from this number of rows there is not it's not possible for us to
117:13 - carry out the matrix multiplication operation so that said let's modify this
117:18 - we have let's now have a 3 by 3 the shape matrix so we would have this for
117:25 - that and then we run it see that works so this works now because this 3 that
117:34 - the number of columns here equals number of rows right here now let's even change
117:39 - this that's let's increase the number of columns if increases number of columns
117:43 - we have this oh we're increasing the number of rows by doing this so let's
117:49 - go back okay let's increase the number of columns to do that we need to do this
117:55 - to let's take two let's take zero so let's run that we see it's still valid
118:01 - because yeah we still have this equal this that is the number of columns here
118:05 - equals number of rows here so since it matches we could still work that out now
118:11 - once you take off so if we take this off you see running this now we still have
118:16 - the mismatch because here we have this to be driven for one let's take one step
118:21 - back we run that and that's what we have now also note that the output of this
118:27 - matrix multiplication has a shape which is dependent on inputs so if you notice
118:35 - the output has a shape 2 by 4 and this 2 here is going from this number of rows
118:41 - and then the foyer is going from this number of columns and that said if now
118:47 - we instead have a 1 by 3 that is if we have 1 by 3 times 3 by 4 our output will
118:53 - be 1 by 4 so let's run this you see you have 1 by 3 3 by 4 our output is 1 by 4
118:59 - and here's what we get now if you know the voice with this linear algebra
119:06 - terminologies you could check on our linear algebra course available on our
119:11 - platform neural learn AI so you could check on this yeah you will have this
119:16 - course on all this section on matrix algebra you will have matrix
119:21 - multiplication right here and you get to understand all this in depth so you
119:25 - could check on this preview you see how yeah we explained step-by-step with some
119:30 - class exercises how all this works another way you could compute the
119:36 - matrix multiplication of the x1 and x2 is by having this so we could say TF or
119:43 - rather we could simply have x1 right here at x2 so this is matrix
119:51 - multiplication while this is element wise multiplication so we have that add
119:56 - run it we should have exactly the same answer we got here so there we go we
120:02 - have exactly this same answer another very common matrix operation is that of
120:10 - the matrix constables so yeah we'll look at how to do the transpose of the
120:15 - matrix just you just put this dot capital T you run that another way you
120:21 - could compute the matrix multiplication of the x1 and x2 is by simply doing x1
120:28 - at x2 so we run this you see we have exactly the same response we got from
120:34 - here now we move to another linear algebra operation or matrix operation
120:42 - which is that of the matrix transpose so let's compare the matrix transpose we
120:49 - just have TF the transpose TF the transpose and then we pass in our matrix
120:56 - so we have passed in x1 we run that you will notice that the rows becomes the
121:02 - columns and and you'll notice how rows become columns and columns become rows
121:10 - so let's go back to our x1 let's print out x1 side here x1 there we go we have
121:20 - this x1 right here we see how the shape is 1 by 3 and that of the transpose is
121:29 - 3 by 1 it still contains exactly the same numbers but now this matrix that's
121:37 - the initial x1 has just one row you could see one row whereas here we have
121:43 - three rows so this becomes this this two becomes this this zero becomes this now
121:53 - let's take this now with x2 so we run x2 or rather let's let's bring out x2
121:59 - first so we have x2 right here now notice how this one two zero that's this
122:06 - first row 1 2 0 2 becomes our first column this second row becomes our
122:12 - second column and this third row 5 4 4 5 6 0 becomes our third column so we have
122:21 - a year 4 5 6 0 so this is basically the transpose operation now how is that
122:27 - related to this matrix multiplication you may have noticed here we have
122:32 - transpose a false and transpose B false so in the case you were trying to
122:35 - multiply x1 by say x2 transpose you just have to put B true recall that x2 is B
122:44 - so because initially we had air I am B so if we multiply this or rather change
122:52 - this to true now you be multiplying x1 by x2 transpose let's run this and we
123:00 - should have an error this is normal why because we're multiplying x1 by x2
123:06 - transpose now what is extra transpose let us let us say we have TF dot
123:11 - transpose transpose x2 so there's a transpose and then we get a ship so we
123:18 - run that and we see that we have 1 by 3 times 4 by 3 we see that this 3 is not
123:24 - equal to 4 so the number of columns here is not equal to number of rows right here
123:28 - and so we cannot do the matrix multiplication so that set that's not
123:37 - valid so let's modify this let's say we take let's create x3 so we could have x3
123:44 - which has four columns so we could have x3 with four columns let's change this x3
123:51 - and then yeah let's say we have this just two okay so we have that we have
123:59 - that we'll print out x3 so we print out x3 shape x3 that shape there we go we
124:08 - run that we have x1 x2 x3 x4 now notice how so it is x2 so this is x2
124:17 - transpose now because this is three four three four tens of four three so it's
124:21 - normal so we have x3 what we're trying to find here is x3 at x3 matrix multiplied
124:30 - by x2 transpose so matrix multiplied by the transpose of x2 that's it so let's
124:37 - in this out in the out see if that's valid so we have in this error TF TF
124:48 - the transpose so there we go we see that we are having an output right here which
124:55 - is two by three that's because this far is the same as this four and then our
125:00 - up but will take number of rows two and then this number of columns here to
125:06 - read so that's it so it kind of like takes the this output this outer axis
125:13 - that we have here so that said we have in our transpose of x2 and then we
125:21 - multiply x3 by 8 now how do we have this exact same response without necessarily
125:28 - having to say for example TF the transpose x2 all we need to do is just
125:33 - specify that our x2 is going to be transposed and here we have x3 so we
125:40 - have x3 and then x2 so it's just like we multiply x3 by x2 but we're saying that
125:45 - our x2 is going to be transposed so that's it we run that and we should have
125:49 - exactly the same response so that's it we have exactly the same response is
125:53 - doing this is the same as saying x3 times or actually matrix multiplied by x2
126:00 - transpose so we'll send transpose B is true now if you send this to true this
126:07 - should let's check out what is gonna be shown we have an x3 x3 shape is 2 4 so
126:13 - if we have the transpose of x3 is gonna give us 4 2 times the transpose of x2 x2
126:21 - is actually let's check actually x is actually 3 4 the transpose is 4 3 so
126:27 - we have 4 2 by 4 3 it's not gonna work so when this they shouldn't work we're
126:32 - having 4 2 by 4 3 now if we change this because we have an x2 transpose let's
126:42 - say x2 transpose is 4 2 and then x3 transpose is 4 3 so for this to match we
126:53 - must modify this so if we change x3 to a 3 by 2 matrix let's have that rather
127:02 - yes actually x3 so this x3 and this x2 so let's modify x2 so that we have a 3
127:11 - by 2 matrix and if we modify x2 to be a 3 by 2 matrix then x2 transpose will be a
127:17 - 2 by 3 matrix and if we have 2 right here this matrix multiplication will
127:22 - match so let's go ahead and change this into a 3 by 2 matrix 3 color 3 rows and
127:27 - then 2 so let's take this off take this off take this off that's fine okay so x2
127:36 - now is 3 by 2 and x2 transpose is 2 by 3 so they should match let's comment this
127:42 - out that's fine and run it so yeah we have in an error okay the error is
127:48 - coming from the fact that yeah we need to put a transpose so initially have TF
127:53 - the transpose because now we're doing x3 transpose times x2 transpose so we're
127:59 - gonna have this TF the transpose here that's fine so let's run it now and
128:06 - everything should work fine see we have exactly the same response because we
128:10 - actually do an x3 transpose by x2 transpose this shapes here now match now
128:17 - the way we look at this next arguments here does the adjoints is exactly the
128:23 - same as for the transpose and I join it I joined in a matrix is another
128:29 - operation which will not get into the details but just note that specifying
128:36 - this or saying that I join of a is true for example is the same as just saying
128:41 - we're taking the adjoint of x3 and multiplying by x2 if we say if this is
128:48 - false and then here is true so if we have this then what we're saying is with
128:53 - multiplying x3 by the adjoint of x2 so it's the same as or similar to the way
128:59 - we treat the transpose arguments right here let's now look at how to multiply
129:04 - matrices with greater than two dimensions so let's go ahead and take
129:10 - this tensor 3d we had defined previously we have tensor 3d right here we just put
129:18 - it out down right here we have tensor 3d we've now defined this to three
129:26 - dimension tenses x1 and then x2 so there we go we run this and this is what we
129:36 - get so we have this two three dimension tensors and we want to do matrix
129:43 - multiplication so we could simply have TF the mat model or TF the lean log in
129:49 - your algebra the mat model we have that and then we specify x1 and specify x2
129:56 - running this should produce an error this is normal again because there is no
130:01 - matching the shapes now how do we look at this now the way we look at this is
130:06 - quite straightforward what we do is when having these kinds of matrix
130:14 - multiplication where matrix multiplication is done in batches what
130:19 - we do is look at each and every batch and look at its corresponding batch in
130:25 - the other matrix and then do the matrix multiplication so what we have here is
130:29 - we're multiplying one this matrix here this 2d matrix by this matrix now if you
130:38 - have to multiply this matrix here by this matrix and then later on this
130:43 - matrix by this matrix and then this matrix by this matrix now you must make
130:50 - sure that there is a matching in the shapes now this matrix right here is 2
130:55 - by 3 and this is 2 by 3 we've seen already that 2 by 3 multiplied with 2
131:00 - by 3 wouldn't work because the number of columns here is 3 and number of rows
131:04 - here is 2 which don't match so what we need it do now is we could modify this
131:11 - tensor such that each of this matrices right here have a shape which will match
131:18 - with this so that we could do the matrix multiplication so let's change this now
131:22 - into 2 by 2 matrices so instead of this take this off take that off 2 by 2
131:30 - comma and finally here we have this comma and that's it so now we have X1
131:38 - which is 2 by 2 and then X2 which is 2 by 3 2 by 2 2 by 3 will output as 2 by
131:46 - 2 and then 2 by 3 will give us an output which is 2 by 3 so it's going to be 2 by
131:55 - 2v output now let's take that off and then run this while we run that we see
132:02 - this output let's take this off let's take X1 X2 off so we take that off
132:09 - run it again so we could see clearly our output now notice how what goes on here
132:14 - is we have this matrix multiply with this this with this so on and so forth
132:22 - now if you want to check this let's go ahead and recreate this right here so
132:28 - what we're gonna do now is we're gonna just take let's take for some this middle
132:33 - values here off take this off take this off there we go we have this okay so
132:43 - that's fine so now we have this okay we have this to we have this matrix does
132:51 - this to the tensor and then let's take this middle value because we chose the
132:55 - middle so let's take this take this off off and then yeah we take this off okay
133:04 - so we have X1 and X2 and then we're trying to print this out run this we
133:09 - have 10 20 20 11 8 6 so taking the middle values even look at this carefully you
133:15 - see we have exactly the same response so this confirms the fact that while we're
133:20 - doing what we call batch multiplications where a batch is considered to be one of
133:27 - this indexes in the zeroth axis so we have this zeroth axis right here we have
133:37 - three indexes so each and every one of those indexes is considered to be a
133:41 - batch so when we're doing this kind of batch multiplications we just take each
133:45 - and every one and multiply with its corresponding value in the other metrics
133:50 - from here we explain why we are how we use this sparse or this be sparse on
133:56 - this a's bars arguments that we have right here now the way this works is
134:01 - sometimes we have matrices which are full of zeros so we may have a matrix
134:06 - like this let's modify this and then here we have zero here we have a zero
134:14 - yeah we have zero and then yeah we could have a zero zero and that's it so we
134:21 - could have matrices like this which are made of mostly zeros and so what happens
134:27 - is tensorflow has a way of optimizing competitions involving tensors which are
134:34 - mostly made of zeros this type of tensors are known as sparse tensors so
134:39 - when you specify that tensor a particular tensor is sparse tensorflow
134:45 - takes that into consideration when carrying out a computation and this
134:50 - helped us carry out computations even faster since tensorflow now knows that
134:56 - this matrix or the particular matrix or particular tensor is made of mostly
135:02 - zeros so that's it we could check on other methods like the adjoint method
135:08 - which we spoke of previously this is adjoint method right here I mentioned to
135:13 - get the adjoint of a matrix very easily let's now look at the band part method
135:19 - and this band part method we are actually rewriting the tensor but setting
135:26 - some values to zero based on certain conditions so here we have an input
135:31 - we have a num lower and a num upper we have the name as usual so right here we
135:39 - have an input which could be of key dimensions as is defined right here and
135:46 - then we have these conditions so we have this indicator function which is say in
135:55 - band and with those conditions and then we take this in band and multiply by the
136:01 - input matrix right here to finally get the output so let's look at this example
136:07 - right here we see that we have this input we have this which is passed into
136:13 - the band part method and then we have this output you'll notice that this
136:18 - output looks similar to the input what a difference that at some positions you
136:22 - have zeros so this negative 3 is 10 to 0 negative 2 10 to 0 and this negative 2
136:29 - you're 10 to 0 so that's how we look at it before looking at the special cases
136:37 - let's take an example so we understand exactly how this works note that here
136:43 - this indicator function is defined such that for each and every element of this
136:50 - new matrix in band because we're creating a new matrix in band
136:55 - such that each and every element is defined such that if this num lower
137:03 - num lower is passed in here so if for example like this num lower if the
137:07 - num lower is less than 0 or n minus n n minus n happens to be the position of
137:14 - each element in the matrix on the tensor so if n minus n is less than num lower
137:20 - less than 1 and if num upper is less than 0 or n minus m is less than or
137:28 - equals num upper then this in band will be such that when multiplied with this
137:35 - corresponding element in the input then that input remains the same but if this
137:43 - condition is not verified then that input turns to a 0 here we have this
137:49 - tensor this 2d tensor which was defined we have that condition which was given
137:57 - to us in the documentation and then what we do is we're gonna define this two
138:02 - matrices one is m minus n and the other one is n minus m notice how this tool
138:10 - have exactly the same shape as the input so what's going on right here what
138:17 - happens here is we have m first of all you have to understand that m is for the
138:22 - rows and then n for the columns so what goes on here is we have at this position
138:29 - we have the zeroth row zeroth column so m minus n that's 0 minus 0 is equal to 0
138:39 - at this position we are at the zeroth row and the first we are not allowed to
138:44 - 0 minus 1 times so this position 0 row minus second column 0 minus n and we
138:51 - have our position so we have this row 0 negative 0 let's call it 0 1 2 0 0 1 row
138:57 - here finally 9 second row alright we add all first row in this column so we have
139:04 - one row and here we add first just row second column 1 minus 2 negative 1 and
139:13 - then we continue with this and we'll get all these values so this is how we get m
139:17 - minus n we're simply taking the index of the row index minus the column index
139:25 - now we look at n minus m for n minus m we're looking at the column index minus
139:32 - the row index so yeah we are the zeroth row zeroth column 0 minus 0 0 here we
139:40 - add the first column and the zeroth row so 1 minus 0 is 1 here we add the second
139:48 - column and the zeroth row so 2 minus 0 is 2 so that's how we get this n minus m
139:55 - matrix right here now once we've got an m minus n matrix and n minus m matrix we
140:03 - can now see how to get the output very easily so we have this output for now
140:08 - we replace it by this axis and we are gonna get for each and every element its
140:16 - exact value after going through this band part method right here since we
140:23 - have as input our lower to be 0 and our upper to be 0 we could take this
140:28 - condition off because our lower will never be less than 0 so we could take
140:33 - that off so we could have this out there we go and now we'll be focusing on n
140:39 - minus n so at this position n minus n which is 0 is actually less than or equal
140:46 - is actually equal the lower so our lower is 0 and our upper is 0 so we have n
140:52 - minus n here is it it's actually less than or equals our lower so we have that
140:59 - to be true and then n minus m which is 0 is actually less than or equals our
141:06 - upper since our upper is 0 so that's true so since all this is true we have
141:11 - these value at this position that's the zeroth row zeroth column position that's
141:20 - when m equals 0 this is where m equals 0 and n equals 0 so at this position where
141:25 - m equals 0 and n equals 0 this is gonna be maintained so this one is gonna be
141:32 - maintained so that's how it gets output maintained now we move to this next one
141:37 - year so in this next we are having m to be equals 0 and then n moves to 1 while
141:43 - the first column we come again and check here m minus n we have a negative 1
141:49 - negative 1 is it less than or equals our lower yes that's true it's less than or
141:53 - equals 0 and then n minus m n is 1 m is 0 that is 1 is 1 less than or equals
142:02 - upper no that's not true 1 is not less than or equals 0 so this condition is
142:08 - not fulfilled since this is not true so both must be true so since this is not
142:13 - true this value this negative 2 is 10 to a 0 so here we have a 0 now we'll move
142:20 - to the next we do the same here we have a 0 now what choice we're always gonna
142:26 - have a 0 because if the 0 is maintained we have a 0 if we turn to a 0 we still
142:31 - have a 0 so no need chicken on this now we'll go to the next we have to read for
142:36 - this we are at m equals that's m equals 1 and then n equals 2 no n equals yeah we
142:46 - at m equals 1 and then n equals 0 sorry so we have n equals 0 now we have n
142:53 - equals 0 we could compute this m minus n 1 minus 0 is 1 but 1 is not less than
143:00 - 0 or it's not less than or equals 0 so this is not true so this 3 turns to a 0
143:05 - so yeah we will have a 0 that's how we get the 0 right here now we go to this
143:09 - 5 we have m equals 1 and n equals 1 in this case m minus n is equal to 0 which
143:16 - is less than or equals 0 so that's this is true this here is true and then n
143:21 - minus m is 0 this is also true so we maintain this value so this turns to a
143:27 - 5 when we get this we have m equals 1 in this case and then n equals 2 so we
143:36 - have 1 right here and then we have 2 okay n minus n we have 1 minus 2
143:42 - negative 2 negative 2 is actually less than 0 so this is valid and then yeah we
143:47 - have n minus m 2 minus 1 1 1 is not less than 0 so this is not valid that's
143:54 - sad since this is not valid this 100 turns to a 0 so that's it you could just
144:00 - repeat this and what you should have will be something like this so yeah you
144:04 - should have 0 and then yeah you should maintain this value and yeah you have
144:10 - 8 here you have 3 and then here you have 2 so that's how you get this oh yeah you
144:17 - should have 0 actually this is 0 here 0 now everywhere if you notice everywhere
144:22 - m is different from n you will never you will never have a situation where m is
144:29 - different from n and then m is less than equal to 0 and at the same time n minus
144:35 - m is less than equal to 0 so that's why all the other values of 0 except for
144:40 - this diagonal values so you see here 1 5 6 I maintained now let's test this so
144:48 - with this command that and then we run this so there we go as a response we get
144:55 - this comes to confirm what was said here where the give this useful special
145:03 - cases and we told that whenever we have this 0 0 that's got an input with 0 0
145:10 - the output is going to be a diagonal matrix so diagonal tensor so right here
145:15 - we understand why we should have that diagonal now if you repeat the same
145:19 - process you should be able to see that 0 negative 1 we give upper triangular and
145:23 - then negative 1 0 would give the lower triangular now let's go ahead and see
145:28 - what this upper and lower triangular tensors mean so yeah let's take this to
145:35 - negative 1 we run that and what do we notice we notice that this matrix of the
145:41 - input that stands out to D does it here is actually maintained so 1 3 1 2 see
145:47 - everything's maintained except for the upper part now if you have a matrix like
145:53 - this and that you have this diagonal and that all this this is what we call the
146:00 - lower triangular part of the matrix and this is the upper triangular part of the
146:05 - matrix so as you could see here this upper triangular part of this matrix is
146:12 - all zeros and then if we take this to 0 so we have 0 negative 1 if you have 0
146:20 - negative 1 and you run that you'll notice now that is instead the lower
146:24 - triangular part so here we have this lower triangular part this lower
146:30 - triangular part right here which is not made of all zeros so that said in case
146:35 - you want to get this lower triangular you just have to specify 0 negative 1
146:39 - upper triangular you have negative 1 0 and then when I want to have a diagonal
146:45 - matrix you just need to specify 0 on both sides that's it for this band part
146:51 - method we look at other methods we have a color scheme decomposition so here we
146:56 - have a way of decomposing matrices we wouldn't get into that here we have the
147:02 - cross product this is completed pairwise cross product cross product of A and B
147:07 - so here you could complete a cross product of two input tensors here you
147:12 - could get a determinant of a tensor so we have an input you could get this
147:16 - determinant we have many other interesting methods here we have the
147:21 - inverse linac dot inf we have your tf dot linac dot inf tensor 2d we run this we
147:29 - should have an error so that's normal so this is normal because to obtain the
147:34 - inverse of a matrix that matrix must be a square matrix that is a number of rows
147:41 - must be equal the number of columns so that said let's recopy this and then put
147:48 - it here we have we really find this answer 2d take this off that's it and
147:55 - then we now run this again we told you I cannot find device for node and we
148:02 - given a set of data types which registered for the operation matrix
148:08 - inverse so this means most probably our data type isn't registered in this
148:15 - matrix inverse operation now we could search on stack overflow we click on
148:21 - this what we get here I think your value should be float as error says I have
148:28 - encountered the same problem while finding the determinant of a matrix and
148:31 - change the detail to float 32 and the polynomial solved so here we have a great
148:36 - way of taking for this kind of errors but nonetheless we understood already
148:41 - that this should most probably come from the data type we're working with so
148:46 - let's change this to float 32 as it was set by the stack overflow user and we
148:51 - have now this other error which corresponds with the error we expected so
148:55 - here we are getting this error and we told that let's take this off we told
149:02 - that oh for doing from three now this is normal because as we said the number of
149:09 - rows must be equal the number of columns for us to find or calculate the
149:15 - inverse of that matrix or the tensor so that said what we're gonna do here is if
149:21 - we take this off and create a 3 by 3 matrix what we have now will be the
149:27 - right answer so we see we found an inverse of this matrix and this inverse
149:31 - so let's let's put this in a variable as a tensor to the inverse so this
149:39 - inverse is such that when you multiply so let's run this when you multiply
149:44 - tensor or when you do a matrix multiplication not actually not an
149:49 - element multiplication element wise multiplication when you do a matrix
149:53 - multiplication of this tool you should obtain the identity matrix so here you
149:58 - have tensor 2d inverse around that and you see that you have the identity
150:04 - matrix so here you have 1 0 0 this is 0 very small number 1 0 this is 0 0 1 so
150:12 - here we have the identity matrix taking even in the documentation you see that
150:17 - we're given a list of accepted data types and the float 16 which we were
150:23 - using previously was in a month so you have to ensure the input data types are
150:29 - in this list we have this matrix transpose here similar to the T of the
150:36 - transpose we have seen already so that's our transpose we'll see the matrix small
150:41 - that's it we have the trace we have the singular value decomposition of a
150:46 - matrix so here you could obtain the matrices singular value decomposition this
150:53 - returns three outputs s u and v where s is a tensor of singular values u tends
151:02 - of left singular vectors v tends of right singular vectors so the SVD is a
151:08 - way of breaking up a matrix in a way that less important information contained
151:14 - in a matrix is eliminated so that said we could check this out right here we
151:22 - have SVD taking the tensor so we could simply just pass in SVD and then get the
151:28 - output so here we have SVD of tensor to the SVD of tensor to D there we go as we
151:42 - did not defined TF the Linux the SVD we run that and then we get this output so
151:49 - we have this three outputs now let's define this s u v equals that and then
151:56 - let's print out s so print out s first to see the singular values there we go
152:01 - we have the singular values we print out u we have tensor of left singular
152:08 - vectors and then we print out v to give us our tensor of right singular vectors
152:14 - so this is what we get right here so feel free to always look at this
152:19 - documentation and the more you use these methods the less time you would even
152:26 - need to always come back to documentation so just make sure you keep
152:32 - practicing so you get to master all those methods we now go ahead and look
152:38 - at a tf.ensom method in terms of flow you could have it as tf.ensom you
152:45 - could look at this documentation right here with some examples before diving
152:50 - deep into understanding how this operator works it's important to know
152:53 - that this operator uses or takes in arrays of all sorts of dimensions that
153:00 - is one the arrays to the arrays up to n the arrays so we'll start straight up
153:07 - with this example with two dimensional arrays I will see how the handsome
153:13 - operator can be used to replace the usual matrix multiplication by matrix
153:19 - here will simply meaning a 2d array and this example will suppose that we have
153:25 - this array A and then this other array B A is of dimension of shape 3 by 4 B of
153:33 - shape 4 by 5 in other for the matrix multiplication to be valid we have to
153:39 - ensure that the number of columns of A have to be exactly the same as the
153:44 - number of rows of B in that case we have to ensure that in
153:48 - generality this J has to be the same as this right here and that's the case for
153:54 - this example and so the matrix multiplication of A and B is valid and
153:58 - what plan this with this would give us C but also note that the shape of C
154:06 - depends on that of A and B as you could see here if we kind of merge these two
154:12 - shapes and then take off this same shapes does this column for the first
154:19 - matrix and then this row for the second matrix would have 3 by 5 so look at this
154:25 - outer values right here this 3 and this 5 and that's what gives us the shape of
154:29 - the output matrix C in general if you have ij jk then we should have an output
154:35 - ik so that's how we have this right here it's very important for you to take note
154:42 - of the shapes whenever you're working with an handsome operator okay that's
154:47 - understood let's now dive into the code we started by importing on pi and B
154:52 - we have that and then we have this two matrices A and B which we've defined
154:59 - already so let's command this for now we have the matrix A which is on the slides
155:05 - and we have the matrix B as you could see printing out this would give us A
155:09 - that shape now we'll print out B shape so we have A that shape B that shape gives us
155:15 - three four and then four five so three by four matrix and then a four by five
155:20 - matrix right here as B and that's it now let's look for the or let's find the
155:27 - matrix multiplication of a and B know that and non pi just suffice to have m
155:32 - beta mod so this matrix multiplication would as an A and then B so there we go
155:38 - we run that now we'll find this is the answer we get which is correct this is
155:43 - our C so we've got an RC now how can we replace this matrix multiplication with
155:50 - the any some operation in order to do that just take note of this syntax right
155:57 - here notice how we have first of all this screen right here so we have the
156:05 - string and what do we put in the string first we have this IG and then next we
156:11 - will follow this IG after the idea we have a comma and then we have the G key
156:16 - and then we have this arrow to I K now this can be read as a interacting with
156:23 - B to produce seed and the way we chosen this I J J K such that it matches up
156:31 - with the shapes of A and B and with a kind of operation we are dealing with
156:35 - given that with a matrix multiplication we have a so supposing it's I J and then
156:42 - if we have a B then we must ensure that the J we had here has to be the same as
156:48 - a J we have here so this is just like the row so either row J the column and
156:53 - yet the row and then you're the color so we have to make sure that the number of
156:58 - columns equal number of rows that's why you have the J and J right here and then
157:03 - since the output is such that it takes the the shape of the output is I K
157:10 - that's what we specify here so we've taken a B and then transform into a C
157:16 - now whatever you have before this transformation has to be passed
157:20 - obviously so that's why we have this I J so the comma here does that we have
157:26 - this I J match now with a and then this J K matches our would be so we've kind
157:33 - of like separated the two different array is to be passed and then we have
157:38 - this I see which is output we get that's why when we print this out now we're
157:42 - gonna have an output so this I K is output we see clearly that this gives
157:47 - the same answer as just doing the matrix multiplication well at this point you
157:54 - will be down like why do I need to do all this when I could just simply put
157:59 - in the matrix multiplication or NP dot mall a B and then I have my answer well
158:05 - that's a very common and normal question and if you follow to the end you'll see
158:10 - that in some examples are in many applications you find that working with
158:16 - the ASAM operator is gonna be easy than working with the usual non-pi functions
158:24 - or methods you used to working with so let's go ahead we've had that I will
158:29 - now understand the ASAM syntax let's go ahead and look at many other examples
158:36 - for next example will be using the ASAM operator to do an element wise
158:43 - multiplication let's take this example right here we have a that's the symmetrics
158:48 - a we have the symmetrics B obviously for LNY element wise multiplication we have
158:54 - to ensure that the two matrices A and B have the same shape so as you can see
159:01 - from this we know a shape and B shape we have 3 by 4 and then 3 by 4 let's
159:09 - comment this for now so we have this tool right here 3 by 4 3 by 4 let's go
159:18 - ahead the computer element wise multiplication the Hadamard multiplication
159:24 - we have that there we go we see that we have 2 times 2 gives us 4, 6 times 9, 54
159:30 - and so on and so forth so just simply each element on this position multiplying
159:35 - with this corresponding or the corresponding element on the other
159:39 - matrix like if you just pick up this random we have 4 times 5 20 so that's
159:44 - the output here now in this case you see that if we have this matrix A right here
159:49 - of shape ij then B has to be of shape ij and then the output has to be of shape
159:54 - ij and so that's why you would see let's recommend this part right here you'll
160:00 - see that we have this input ij or the first input ij which is a the second
160:07 - input ij and then the output ij so the ASAM operator automatically sees this
160:13 - as a Hadamard operation so here we're gonna have the right output on that oh
160:20 - let's take this off and that's it so there we go we see that we have the same
160:27 - output as with the usual Hadamard operation or the usual element wise
160:32 - multiplication for next we go on to matrix transpose so at this point I'll
160:39 - urge you to like take a pause try to come up with a corresponding matrix or
160:46 - the corresponding ASAM operation for the matrix transpose before continuing with
160:50 - a video now hopefully you got it right you have this matrix A right here which
160:57 - is to be transpose to see we have 2 2 1 6 negative 2 5 and so on and so forth so
161:02 - basically we have a matrix which is of shape ij and has an output ji in this
161:08 - case would have say print the ASAM transpose of A is equals the prints of
161:19 - mp.ansam we have that mp.ansam of ij which is being transformed to ji and
161:29 - then we pass in the matrix A so let's take this off right here and then run
161:35 - that there we go we have exactly the same answer so yeah we pass in A we have
161:41 - this ij no need for any commerce that's if you have to multiply let's say we
161:45 - have a b c g so on and so forth then you have say ij jk k l l m yeah we have four
161:56 - right here and does it so now we have just one so we have that let's take this
162:01 - off again that's fine so hopefully you had this correct we now move on to
162:08 - working with three dimensional arrays in machine learning is very common to how
162:13 - to deal with this type of arrays let's suppose we have this array A which is a
162:19 - 3d array of shape 3 2 by 3 by 4 now let's see the way we can read this we
162:25 - have this old array A and then we have this two boxes in this two arrays this
162:33 - one and then this one so that's where we have this two dimension right here and
162:39 - now for each of these that's for each of these arrays we have 2d array in it
162:47 - that's for the first one that's for this first box right here we have this 2d
162:52 - array which is of shape can be that's 1 2 3 by 4 so that's why we have 2 and
163:00 - then by 3 by 4 so all those a three-dimensional array of shape 2 by
163:08 - 3 by 4 we could generalize this as B by I by J and then for B we have a 2 by 4
163:16 - by 5 dimensional array now note that we have here that those B's could be taken
163:25 - as a batch size and this is because in general or many times in machine
163:31 - learning computations are done on batches of data so in this case we could
163:36 - have this as a single batch that's this box right here as a single batch and
163:42 - then we have this other one as another batch giving us two batches so we have
163:46 - the batch size equal to and then we may want to do matrix multiplication or
163:52 - batch matrix multiplication where want to multiply this matrix right here of
163:57 - this 2d array with this one and then get this corresponding output and then
164:03 - multiply this one with this one and then get this other corresponding output and
164:08 - we may not want to say use a for loop to say for each of this we multiply this
164:14 - down as they used to get output and so on and so forth we may just want to use a
164:19 - single operation which understands that this multiplication is done for every
164:26 - position in the batch also note that in this kinds of computations the batch
164:32 - dimension remains exactly the same we have here 2 2 2 while 3 4 multiply with
164:40 - 4 5 gives 3 5 so we still have this exact same value of B which is maintained
164:48 - everywhere now that's that let's go straight away into the coding we have
164:53 - this 3d array a right here and of this to 2d arrays the sort of one this one
165:02 - and then this other 2d array we could come in this part and then now we have
165:10 - to print out a that shape and then B that shape there we go Oh print that's
165:20 - fine so 2 by 3 by 4 2 by 4 by 5 now note that the matrix multiplication can
165:27 - permit us do this without any problems that is we could get the we could do the
165:33 - batch multiplications with the non-pise matrix multiplication as non-pise
165:39 - matrix multiplication understands that when data is placed in batches like this
165:44 - all we need to do is to ensure that each element in the batch multiplies the
165:49 - corresponding element in the batch in the other array now if we want to use
165:55 - this we could also use the N sum operator now the way we use the N sum
165:59 - operator here is by again specifying the shapes so we have B I J for A and then
166:05 - we have B J K for B and then for C we still have B I K notice how we have I J
166:12 - J K and I K while B remains the same so with this around this and we'll see that
166:20 - we have exactly the same answer for the two outputs that's when we will use the
166:25 - matrix multiplication and when we use the N sum operator and still up to this
166:29 - point we see that we could always we may not necessarily use the N sum operator
166:35 - now what if we want to sum up all the elements in a given array so yeah we'll
166:41 - look at another different way of applying or making use of this N sum
166:45 - operator so yeah we have this matrix or rather we have this 3d array a right
166:51 - here and then we want to sum up all the values in the array yeah we have the
166:57 - sum gives us a value of 72 and then if one of you is the N sum operator what
167:02 - we'll be doing is we have the shapes that's B I J put an arrow and then when
167:09 - you don't have any output while you're simply doing is you're summing up all
167:13 - the different possible values so that's what this signifies right here so it's
167:20 - just like saying we're summing up all the possible values then we put in or
167:24 - we make sure we have the right shape of a put right here notice I will do this
167:28 - you see we have an arrow so we'll correct that we have G on that and that's fine
167:35 - we could also sum up all elements in a given dimension or given row given
167:42 - column say we're working with a matrix here we're having this in sum let's just
167:50 - this okay so yeah we'll see that we have this matrix a shape IJ and now when we
168:03 - specify only one of this axis right here what we're doing is we're summing up all
168:11 - the elements in each column because the J is like the column so there's a row
168:16 - and then there's a column so we're summing up all the elements in each
168:19 - column and that's how we have this we see we have two for the first column we
168:23 - have two plus two plus one five six plus minus two plus five nine five plus two
168:29 - plus four eleven two plus three zero five so that's how we have that so the
168:34 - way we understand this is we just summing up all the elements in each
168:38 - column and then if we change this to I will summing up all the elements in each
168:42 - row around that you see that for this row we sum this sum this now we sum that
168:48 - now moving to a more practical example and this attention is all you need paper
168:54 - you will have yeah this formula right here where we have the query multiplied
169:01 - with the transpose of a key if this key has a shape of by size by sequence
169:11 - length of the key by a model size and the query shaped by size by sequence
169:17 - length of the query by a model size then we could define this non-py array query
169:24 - of shape 32 by 64 by 512 and key 32 by 128 by 512 and then what we will do now
169:35 - is define the query by transpose operation where we would have MP the
169:45 - in sum of this is considered to be say be the by size by cure by M and then
169:55 - this is considered to be say B by K by M so yeah we would have B cure M comma B
170:02 - K M which outputs B now at this point we have to be very careful we have cure M
170:10 - times K M transpose so this becomes B cure M times B MK and now when we
170:19 - multiply this cure M and MK what you have is cure K so yeah we have B cure K
170:25 - let's take it away again we have B cure M and B K M by dimension doesn't matter
170:32 - take that off so we have this now and then what we transpose is we have MK and
170:37 - when we multiply this we left with cure K so that's how we have that and then
170:42 - here we have as input cure and then key so let's run this and there we go we
170:49 - have this output which is of shape with verified shape which is of shape cure
170:55 - key we have your cure which is 64 by key which is 128 so we have 32 by 64 by 128
171:04 - output right here our next and final example inspired from the reformer paper
171:10 - in this paper the others explain how the data could be breaking up into chunks
171:16 - and then the attempts of each other again you don't need to understand what's
171:21 - gone on that paper but it's just for you to get an idea of where this problem
171:27 - were to present comes from now supposing we have this matrix A of initial shape
171:33 - 2 by 4 by 8 all to keep them simple let's say 1 by 4 by 8 so we have a batch
171:39 - size of 1 we have a sequence length of 4 and a model size of 8 here you could
171:45 - see this one that's the only single batch we have here then we have four
171:51 - rows and then eight columns B2 has a shape 1 by 4 by 4 so we have four rows
172:00 - and then four columns and the paper we just saw they further break up this data
172:07 - into buckets so and so right here we have four different buckets now we see
172:13 - that this eight columns are broken up into separate four by two matrices here
172:20 - we have the four by two matrices we have four rows two columns four rows two
172:24 - columns and so on so forth now the same is done for B with a difference that
172:30 - would break this far up that would break the four columns up until four by one
172:34 - matrices where we have four columns and just all right at four rows and just one
172:39 - column in the paper there is a task which involves taking the transpose of
172:45 - this B and multiplying by a and in doing that we are in fact multiplying the
172:51 - corresponding batches and the corresponding buckets so you see that
172:57 - again here we have some sort of two fixed dimensions and then we have this
173:04 - dimension or this matrix here which is actually gonna be involved in the
173:11 - multiplication process and so if we have a defined as B C I J or the shape B C I
173:18 - J and B B C I K multiplying the transpose of B or getting the transpose
173:25 - of B would give us B C C B and C fixed and then I K turns to K I and A is still
173:34 - B C I J now we have K I times I J giving us K J so we have B C K J so if we have
173:41 - this 1 4 4 2 by 1 4 4 1 we have 1 4 1 2 and now with a Einstein operator if we
173:49 - have the a given as this here we have B C I J and this B C I K so let's just put
173:58 - in common so we have this B C I J B C I K we had seen already on the slides on
174:04 - that now we're trying to find B transpose by a so what we're gonna have
174:09 - here is nonpiler and sum of B C I K this year we have B transpose times a so we
174:19 - have B C I K comma B C I J which is gonna output B C now at this point we
174:27 - recall that we're gonna have the I K which is gonna be interchange to give us
174:32 - K I K I times I J gives us K J so when I have K J just as we had in the slides
174:37 - and then we pass in B and then pass in a so run that and then look at the ship
174:43 - that's it so we see that we're able to compute this very easily with an
174:48 - answer operator as compared to the usual map mall where we have to ensure that
174:55 - the matrix B is transposed correctly so yeah we're gonna have MP that transpose
175:01 - of B and then we have to ensure that we have zero one so zero one fixed and then
175:09 - we have three two because this is why we do the transposing and then we now
175:16 - passing a from this so we have that and then there we go we still have the same
175:23 - shape we could take this off you see that we have the same answer for both
175:31 - methods but this method looks clearly is cleaner than this one as if we have to
175:40 - go to say 5 or 5d array we'll just let's say for example we have six year and
175:48 - then we have six all we need to do here is to say for example D so put that D
175:52 - that way D and that's fine so let's run this again see the ship and that's fine
176:00 - but with this we have to say 0 1 2 and then we would have to ensure that yeah
176:07 - we have four and then yeah we have three for that to work and so this is an
176:13 - example showing how the answer operator comes to make our work clearer and
176:19 - easier to understand
176:24 - let's now look at the expand deems method with this method we get to add an
176:30 - extra axis to an input tensor and that extra axis has a length of one if we
176:38 - suppose we have this tensor for example tensor 3d then let's print out the
176:45 - shape 4 by 2 by 3 now what we could do is we could add an extra axis such that
176:52 - this tensor 3d now takes a shape of 1 by 4 by 2 by 3 that's that what we do is
177:01 - tf.expand deems and then we pass in tensor 3d tensor 3d that's it and we
177:11 - specify the axis so axis specified to be 0 we're gonna have this output shape so
177:18 - let's run that and then put out a shape here so that's it now let's take a
177:23 - simple example suppose we have X equals tf.constant and then we have this right
177:30 - here two three four five for example we could print out X the shape and then we
177:38 - could print out tf.expand deems of X and then specify the axis to be zero you
177:46 - will see that we're gonna have this shape 4 and then we have this output
177:50 - let's put this shape right here we have this output 1 by 4 notice how this
177:57 - tensor leaves from a 1d tensor into a 2d tensor still we have the same elements
178:04 - but we've added one extra axis right here this extra axis would have been
178:11 - added manually by doing this so if we had done this would have had that extra
178:15 - axis we run that you see we have one by four and now doing the expand deems we
178:20 - have one by one by four so now we'll have from 2d to 3d and then if we keep
178:27 - doing this you see how we'll leave again from 3d now to 4d so we have one by one
178:34 - by one by four so that's how we look at the expand deems method if we want to do
178:40 - this expansion or this addition of this extra dimension in a different axis we
178:45 - could specify that axis let's say the first axis axis equals one notice how
178:51 - we'll leave from 4 to 3 now to 4 1 to 3 so we kind of like fit in this year
178:59 - let's take this off kind of like fit in this year so when you say axis equals 0
179:05 - is fitted right here x equals 1 is fitted here as this is called so be
179:10 - fitted here as equal to 3 fitted here as as equals 4 invalid so that's it
179:16 - let's take as equals 4 for example you see it's not valid you have 3 so now
179:23 - you have 4 by 2 by 3 by 1 from here we look at another method which is like the
179:29 - opposite of this expand deems method here we have the squeeze method with a
179:35 - squeeze method what you have is you taking this input specify the axis just
179:40 - like similar to the expand deems method but here we instead remove the
179:47 - dimension of size 1 from the shape of the tensor so let's get back to this we
179:55 - had around this and we had let's take this of X squeezed let's define X
180:01 - squeezed sorry X expanded X expanded is this that's from that we have X expanded
180:09 - 1 by 1 by 1 by 4 now let's do X squeeze X squeezed is equals TF the squeeze and
180:21 - then we pass in X expanded so firstly we're gonna specify the axis to be
180:27 - equals 0 and then we print out our X please so we have X squeezed right here
180:31 - X squeezed print it out and we should have this 1 by 1 by 4 so 1 axis has been
180:40 - taken off let's do this several times so here we have 4i range see let's do this
180:48 - twice copy this pieces out here there we go
180:53 - X squeezed X squeezed so we start by squeezing the first time and then we
180:58 - continue squeezing let's print out X squeeze here so here we have X squeezed
181:03 - that's it we run that and we should get this so there we go we've left from this
181:10 - 4d tensor to this 1d tensor coming back to this other example where we had the
181:18 - shape that is we have expanded such that this extra axis was added in the
181:24 - last position so yeah what we're gonna do is do a TF the squeeze with X which
181:32 - is an expanded and then specify the axis so now we're gonna specify the third
181:38 - axis because we want to squeeze out this third or last axis right here now
181:44 - before squeezing out that last axis let's take let's suppose we're trying to
181:47 - squeeze out this zero axis you see we're gonna squeeze out this because we
181:52 - expect the dimension or the length of this to be equals 1 before we could
182:00 - squeeze it so that said if we specify now 3 you see that way it's fine so now
182:06 - we have 4 by 2 by 3 record that this was X exp that shape was actually 4 by 2 by
182:15 - 3 by 1 so squeezing out now wait this works now because we specify the right
182:20 - axis which is of length 1 so that said we've looked at a squeeze and the
182:25 - expandings methods which have to do with modifying or reshaping tensors let's
182:32 - look at the reshape method so here we have our reshape method as usual we have
182:37 - the definition reshape the tensor and then we have some examples so right here
182:43 - we'll see how we're modifying or reshaping this 2 by 2 tensor into a 1d
182:50 - tensor before looking at that example let's take a look at how we could use
182:55 - the TF the reshape TF the reshape to modify the shape of this X expanded that
183:03 - is this X which produces the shape such that we have an output of 4 by 2 by 3 so
183:08 - there we go we have this we pass an X expand it and then we specify the shape
183:14 - we want to get us output or the new shape of our tensor so yeah we specify
183:19 - 4 by 2 by 3 we don't want to get a tensor so let's put that shape even
183:25 - here let's take this shape we run that and this is what we get we see that we
183:31 - have exactly the same output shape right here now let's change this remove this
183:37 - and then we compare these two tensors and we see we have exactly the same
183:42 - outputs so this tells us that what we do we did with a squeezing could be
183:48 - possibly done with a reshaping now let's take another example supposing we have
183:52 - this tensor X reshape we could actually reshape this as we could have TF the
184:00 - reshape and then pass in X reshape such that we modified our changes into 1d
184:06 - array so or 1d dancer so if you put this to save 6 we run that we should have an
184:14 - error so invalid syntax well this is different from the error we expected okay
184:22 - so this is the exact error we get to input to reshape the tensor with eight
184:26 - values but the requested shape has six so that said the fact that you could
184:31 - reshape doesn't mean you just be able to modify any tensor and then change
184:36 - that to just any type of shape you have to make sure that when we shaping the
184:42 - number of values that you are getting the initial tensor actually fit in this
184:48 - new shape of this new form you understand so to take so you have eight
184:53 - values and you don't expect to reshape this into six values so you have to put
184:57 - this eight so running that now you see how we've reshaped this and we have
185:03 - three five six six three five six six and then four six negative one two so
185:08 - it's kind of like we'll flatten out this matrix now let's do another taking a
185:14 - look at another reshaping we could change this to four so here we have four
185:17 - by two now we're changing all these two by four these already two by four so we
185:21 - could put it four by two four by two we run that so yeah we have two by four now
185:27 - we could change it to four by two notice that this actually isn't exactly the
185:33 - same as the transpose of this matrix so we're shaping it from two four to four
185:38 - two doesn't find get the transpose of the initial tensor extra shape we could
185:45 - change this again let's say we have four to four to one we run that see that that
185:51 - works just fine and this is because with this we expect eight values and here we
185:57 - have this eight values now it may happen that you will be dealing with tensors
186:02 - wearing you don't exactly know whether this is a four by two tensor you want to
186:09 - get as the output of this reshaping so you can just put a negative one and
186:13 - tensor for automatically knows that this is two so if you run this you would get
186:19 - this output right here let's bring this out let's have this bring it out so we
186:26 - print this out copy this print it out we have to run it and we see that we have
186:33 - exactly the same output now if you just put in negative one yeah you would
186:39 - expect to get the same output as this so putting this you see it automatically
186:44 - understands that the value should have here should be eight we now look at the
186:50 - concatenate function that's T of the concat which permits us concatenate
186:55 - tensors along one dimension so right here we have this values as input we
187:01 - have a specified axis and then we have the name so at the bottom yeah you can
187:07 - have this value you could be a list of tensors or just a single tensor now
187:11 - let's look at this example so let's just copy this example from here so we're
187:15 - going to copy this example we paste this out right here we have T1 T2 and then
187:21 - let's look at the output I'll get the output of this before getting to the
187:26 - output we're gonna have print T this is actually a list so let's say print
187:31 - tf.constant T1.shape and then we print tf.constant T2.shape so we have
187:44 - that so we've been opening out this to T1 and T2 and then we're concatenating
187:50 - them let's run this we have the shape 2 by 3 2 by 3 and we have this
187:56 - concatenation which is 4 by 3 now what do we notice we notice that this first
188:02 - part here this first two rows first two rows actually T1 and the next last two
188:08 - rows T2 now the reason why we have this is because we'll specify this axis this
188:15 - axis right here to be equals 0 now saying that axis equals 0 simply means we're
188:21 - doing concatenation across the rows doing row concatenation means we're taking this
188:29 - we're taking this first tensor taking this other tensor and then we are
188:37 - completing the rows of this T1 right here so it's just like we were taking
188:42 - let's copy this so this concatenation operation is just like this we have in
188:47 - this year completing like this and there we go so this is exactly what we're
188:53 - doing we actually just completing this and then having this so that's why yeah
188:58 - when you look at this you see it's exactly the same as what we have here so
189:02 - that's how the concatenation operation is done to go back we have T2 T1 now we
189:10 - could modify this and take to one now if you specify the axis to be equals one it
189:16 - means we're doing concatenation across the rows or rather across the columns so
189:20 - that said instead of adding extra rows as we just did previously when the axis
189:26 - was zero now we add in extra columns so we come here and we add extra columns so
189:32 - here we have seven we have eight we have nine and then here we have ten we have
189:39 - eleven and then we have twelve so that's what we're gonna get let's go back take
189:46 - that back that's fine and then let's run this and see what we get we see that we
189:50 - have one two three seven eight nine so we add the extra columns and then ten
189:56 - eleven twelve so with the axis equals one that's when we were specifying that
190:01 - we're working on the columns we add in extra columns when axis equals zero
190:05 - that's what the rows we add in extra rows now what if we are having 3d
190:11 - tensors or let's say we're having a 3d list right here let's take this and that
190:16 - and then this you look at you'll know the shape will get from here so this is
190:23 - going to print out the shapes and then let's specify this is zero so we run
190:27 - that and then we have one two three so shape one by two by three one by two by
190:34 - three and then our output here is two by two by three so in fact we're doing the
190:42 - concatenation on the zero axis which is this axis right here so our output is
190:49 - one plus one now we'll go back slightly so let's get back to what we had
190:56 - previously there's a point we didn't mention so let's run this again we have
191:00 - this now what we forgot to mention was that the output that is when we specify
191:05 - axis equals one our output will be we go to the axis one and then we add this to
191:10 - up so up will be two so we fix the other axis and then we add this to up so we
191:15 - have two by six now when is zero we're gonna fix this other axis and then we're
191:20 - gonna add this one up to make four by three so let's run this I'm gonna get
191:24 - four by three let's go forward we will run this again and this is what we have
191:32 - so as we're saying simply specify axis zero we fix the remaining axis and then
191:38 - we add this up so here we have two by two by three now what if we specify the
191:45 - axis to be equals one if our axis equals one our output should be fixing we fix
191:51 - the one and then we add this to up one by four by three so same way
191:56 - concatenating on the first axis here means we'll be stacking up the rows and
192:03 - we'll get this kind of output so let's run this there we go we have one by four
192:08 - by three and then we have this year which is this and then this other one
192:13 - which is the t2 if we had to do this for axis 2 we would have this inspected see
192:21 - we just stack this up on the columns added extra columns and then here we
192:27 - added other extra columns right here to obtain this new tensor we have now
192:32 - another very commonly used method in terms of flow is the stack method as TF
192:39 - the stack TF the stack and then yeah we just go straight away and then you see
192:45 - the kind of output it produces so yeah we have specific we specify axis equals
192:50 - zero it's kind of like similar syntax with the concatenate so we specify as
192:54 - equal to zero we run that and then we should have this so there we go we have
193:00 - instead of like with the concatenation where we just join this two tensors
193:08 - across the rows here when we specify axis equals zero it simply means we are
193:14 - creating a new axis so if both tensors are all 2 by 3 2 by 3 like this we
193:22 - create a new axis and now and then you axis depending on the number of tensors
193:28 - we have so if we had for example if we have to add C1 again let's add this C1
193:33 - depending on the number of tensors we have we are going to have this shape or rather
193:39 - we're gonna have this axis having a length corresponding to the number of
193:44 - tensors we have so like here we have this three so that's why here we have
193:47 - three we're taking this back to two we're gonna have two so basically we
193:52 - actually stacking up this tensors and you should be able to clearly see the
193:56 - difference between the stack and the concatenate and then using this new T1
194:01 - T2 for the stack we have that when we have the stack axis equals zero
194:08 - there's an output get shape 2 by 4 by 3 so from here we could understand that we
194:16 - have T1 4 3 T2 4 3 and then we create this extra axis at the zeroth position
194:28 - so we create this extra axis right here so 4 3 4 3 now we add this extra axis and
194:35 - we have now 2 4 3 where basically this T1 and T2 stay intact as you could see
194:43 - here we have this is T1 and then this is T2 all the values are intact forming
194:51 - this big tensor of shape 2 by 4 by 3 so we leave from this two 2d tensors to
194:59 - this 3d tensor by just stacking up this two tensors now when we change the axis
195:07 - or when we working with axis equal one the stacking up is done such that we
195:14 - having this 4 3 4 3 but the outputs we get is such that this additional axis
195:26 - is at the position 1 so unlike here where this additional axis at the position 2
195:31 - now this additional axis at the position 1 so just like we have in let's say we
195:36 - have in 4 and then we have in 3 so if axis equals or 0 we add it at this
195:44 - position if axis equals 1 we added at this position if axis equals 2 we added
195:50 - at this position we also understand that if we have 3 of this so if we have let's
195:57 - say T1 and then yeah we take T1 let's run this too if we run this too this is
196:05 - what we now obtain we have here for when axis equals 0 we have 3 by 4 by 3 so
196:11 - let's see why we have this we have in this tool we have in 4 by 3 stack with
196:18 - 4 by 3 and then stacked again with 4 by 3 so we have this 3 tensors been
196:25 - stacked here T1 T2 and then T1 again so here we're gonna add an extra axis but
196:33 - then since we have three of this we have we just extra axis 3 length 3 so we have
196:39 - 3 by 4 by 3 that's why we have this right here and therefore when axis equals
196:43 - 1 we add an extra axis so we add the extra axis here we have 4 by 3 we add
196:50 - the extra axis in the middle so here we have 3 so that's why we have we have 4
196:56 - by 3 by 3 let's check this out we have 4 by 3 by 3 when the axis equals 1 the
197:03 - stacking is done at this position one year and so what you have here is this
197:10 - 1 2 3 stacked with 7 8 9 and again stacked with 1 2 3 this is because we
197:17 - have T1 T2 T1 so if I take this off if I run this I have 1 2 3 stacked with 7
197:25 - 8 9 so I have this 1 2 3 here stacked with 7 8 9 and then to obtain the next I
197:31 - have 4 5 6 stacked with 10 11 12 I have this 4 5 6 stacked with 10 11 12 and
197:38 - then the next is 5 6 2 stacked with 0 0 2 5 6 2 stacked with 0 0 2 1 2 1 stacked
197:45 - with negative 1 5 2 that's it 1 2 1 stacked with negative 1 5 2 so let's go
197:52 - back to when we had C1 right here C1 we run that and that's it so we understand
198:00 - exactly why we have this responses here we understand exactly why we have this
198:08 - up right here to get this for example we have 5 6 2 stacked with 0 0 2 and
198:16 - re stacked with 5 6 2 since we have T1 T2 T1 now at this point you could pause
198:21 - the video and try to find the output when we have axis equal to when axis
198:29 - equal to we are meant to have this so let's just recopy this is that when
198:35 - axis equals to we have 4 3 4 3 4 3 that's T1 T2 T1 and then 4 3 and then
198:41 - we add this axis at the end so here we have still 4 3 3 now to make it clearer
198:48 - let's just take T1 T2 so if we have T1 T2 yeah we just have this too so in this
198:56 - case we just have 2 now we have 2 we're gonna have 4 3 2 so 4 3 2 and that's it
199:05 - so we have 4 3 2 unlike here where we have 4 2 3 and here where we have 2 4 3
199:11 - that's if we constrain that we're doing we just T1 and T2 so that's it
199:18 - and that's fine so as you could see our up now before by 3 by 2 and the stacking
199:26 - will be done on the columns so let's look at what we'll get let's have this
199:33 - run right here let's run this then we compare with our values here we see we
199:40 - have 4 3 2 and then this output can be seen as you taking this row that's this
199:49 - first row stacking up with this one and then transposing it so it's kind of like
199:55 - similar to what we had here 1 2 3 7 8 9 but yeah we now transpose it then yeah
200:01 - we have 4 5 6 10 11 12 transpose to obtain this we transpose this to obtain
200:07 - 1 negative 1 2 5 1 2 we transpose this other you look at it that way you can
200:14 - see directly that you have 1 7 there we go next 2 8 there we go next 3
200:25 - 9 here is it and then we move to the next where we have 4 10 so we have 4 10
200:35 - 5 11 6 12 and so on and so forth so that's how we get this output right here
200:42 - with the communication we'll see how the stack method can be written as the
200:48 - concatenation here that's I'm reading as a combination of the concatenation
200:54 - method and the expand deems method so yeah we have this stack of t1 t2 on 0
201:02 - axis which produce this and then following the exact same method we've
201:07 - been given in the documentation we have this concatenation and then we run it so
201:13 - we run this and we see that we have exactly the same answer the reason why
201:18 - we have exactly the same answer here is simple you see that we have this expand
201:23 - deems so if we have let's suppose that we have this two tensor or we have this
201:28 - least made of t1 and t2 which is a which are the tensors one stack so here we
201:33 - have 4 by 3 and then we have 4 by 3 obviously when we do expand deems of
201:40 - each and every one of this that's basically what we're doing here for all
201:44 - this when we do this expand deems 0 axis will be left with one so either this
201:49 - extra axis we're left with this now once we do this we now concatenate on the
201:54 - 0 axis and we have the output 2 for 3 that's because we just basically add
202:04 - this to up so 1 plus 1 gives us 2 as we saw with concatenation so here we have
202:08 - 2 for 3 and this coincides with just doing a stacking of this two tensors t1
202:16 - t2 then we get to the method path as TF dot path right here we have the
202:22 - definition and then we have this example so we just look at this example here we
202:29 - have this tensor T and then we define this patterns which we've seen up here
202:35 - so we have to pass in the tensor and the patterns copy with the mode and its
202:41 - constant values okay by default this is 0 and here is constant but we must put in
202:46 - the tensor and this patterns now let's take a look at that as we said we have
202:51 - the tensor we have this patterns which itself is another tensor and the way it
202:56 - works is we are gonna add this tensor right here with a constant value by
203:03 - default this constant value is 0 so we have gonna we're gonna do the pattern
203:07 - with a value of 0 that's why if you notice you have this 1 2 3 here is it
203:12 - here 1 2 3 4 5 6 this is it and then we do this pattern with all values
203:19 - surrounding it being zeros let's copy this and check this out in a notebook
203:26 - we run this here okay so that's what we get now let's modify this and say we
203:33 - want to have constant values to be equal say three you run that and what do we
203:41 - notice we notice that we still have 1 2 3 4 5 6 but all the remaining values
203:50 - surrounding it actually all to me now we are having this output here but how is
203:56 - output generated now if you look at this carefully let's take this let's send this
204:01 - back to a 0 if you look at this carefully the way this tensor is defined
204:07 - that this pattern stands as defined as such that here we have the number of
204:13 - rows above the initial tensor that's if we have this initial tensor here we have
204:21 - a set of number of rows above this initial tensor which we are going to
204:25 - pad with this value 0 so yeah we have one row above one lower below so that's
204:30 - why you notice here let's run this see we have this that's why you notice here
204:35 - we have this one row above one row below and then we have two columns to the left
204:43 - that's this two columns and then two columns to the right so that's how we
204:47 - generate this this all padded tensor right here now let's modify this let's
204:53 - say we want to have three to the right one I have three to the right you see
204:57 - here we have now three to the right still two to the left one row above and
205:02 - one row below so that's it now let's change this let's say we want to have
205:06 - five rows below you see we have one two three four five six and then all this
205:12 - five rows below now we could change this method to reflect our symmetric as we've
205:21 - seen here another way of doing tensor indexing is by using this gather method
205:28 - which comes as tf.gather yeah as usual we have the definition and some examples
205:35 - to permit us understand exactly how this works so that said let's look at this
205:42 - first example where we see how this tf.gather does a similar thing as the
205:49 - usual indexing which we've seen already so right here we have this tensor which
205:55 - we see here and then we have the tensor that's params params three simply means
206:02 - we're selecting the value or the element at the third position third index so if
206:09 - we count here we have zero one two three this is our third index and the
206:15 - output should be p3 so right here you see we have p3 now if we want to do this
206:21 - same thing with the gather method we just have to pass in our params and then
206:27 - specify the index so you see a specified three and then we have this
206:31 - output now let's this is out here unlike with the indexing where we had for
206:38 - example suppose we want to do params or want to take in these values what we're
206:44 - gonna do is just pass in say zero because yeah this is actually zero one
206:50 - so one two three so if we want to take all these numbers we'll have one two
206:56 - three and then we add a foot so here we have one two three plus one to give us
207:03 - those values so you're gonna have p1 p2 p3 that's it p1 always set it from zero
207:08 - so stuff on one so we have p1 p2 p3 which gives us this as expected I want
207:16 - to redo this gather we have this we pass in the params and then we specify this
207:24 - indices so right here we have one two and three we run that and that's fine
207:32 - we have p1 p2 p3 we could also do TF the range one up to four so we have that
207:42 - that's fine we run that there we go we have exactly the same response so let's
207:50 - get one step back we have this so let's pair on with a syntax now supposing we
207:56 - do not want to get maybe this three consecutive elements or any key
208:02 - consecutive elements supposing we want to get this zero element want to get the
208:08 - last element I want to get this element right here so this is 0 1 2 3 4 5 I want
208:16 - to get 0 that's 0 1 2 3 so I want to get 0 I want to get this last and I want to
208:21 - get it second to the last. So that said I'll just have to put your zero. I want
208:27 - to get the last element so negative one and so that one negative three so I run
208:33 - that I expect to have those elements. You see here we have an error so this
208:40 - gather method doesn't understand the syntax so we just have we just have to
208:44 - use the usual syntax yeah we have zero one two three four five so we'll put in
208:50 - here five and then here we have three we run that again there we go we have p0
208:57 - p5 and p2v you see that this permits us to do this kind of even more complex
209:05 - tensor or slicing. Let's take another example we have supposing we have this
209:11 - tensor right here params and then we want to have tf.gather we pass in
209:19 - params and then we have three one for example now note that this params is to
209:26 - is rather is a four by two V tensor so it's of shape four by two V so we want
209:33 - to get want to gather this three one so look at this again let's start from
209:39 - something simple so let's start from the zero you see that doing this will get a
209:45 - zero one two so yeah we're seeing that we want to get just the first or rather
209:53 - just the zero element and it happens to be this one right here now this is this
209:59 - other this other params here we have the shape we had a shape of params
210:08 - that shape we had a shape of six and then here we have in a different shape
210:16 - so this is 1d and this other params here is 2d so the way we approach the index
210:26 - in here is slightly different now let's look at this when we do this gather we
210:32 - have the index zero selected and the default axis is equal to zero even here
210:40 - our default axis was equal to zero that is with this one right here the default
210:45 - axis here is equal to zero if you do this you should have exactly the same
210:50 - response oh yeah we should run this around that so you see that we have
210:57 - exactly the same response now let's get back to this our axis is zero means that
211:03 - we are dealing with this axis right here which happens to be the columns or rather
211:10 - which happens to be the rows so putting out zero here simply means we working
211:17 - with the zeroth row so we're working with this and that's why the output is as
211:22 - such now if we do this as if we do zero for example let's say three and we run
211:30 - this what we get is zero one two and thirty thirty one thirty two so we get
211:37 - in this and we get in this this is because we're dealing with a zero row
211:44 - and the third row so zero one two three you have this and we have this if instead
211:53 - we want to do this selections based on the columns then we will have to deal
211:58 - with the first axis so x is equal to one running this this is what we obtained
212:04 - yeah we're saying we're dealing with a zeroth column so we have zero ten twenty
212:09 - thirty so that's it right here and then the third column but yeah we don't have
212:15 - any third column so that's why it just by default just places all those zeros
212:20 - right here now let's change it and put for example the second column we have
212:25 - two twelve twenty two thirty two two twelve twenty two thirty two we could
212:29 - change this to zero we run that and we have two twelve twenty two thirty two
212:36 - zero ten twenty thirty now let's modify this let's modify our params so let's
212:44 - have this yeah this and we run it again before running that let's let's go back
212:51 - to zero so let's suppose the axis is zero our ship is one by four by three
212:55 - now our axis is zero so we are on this zero axis right here and if you
213:03 - considering the zero axis here we have only this so basically have only this
213:10 - now we say we want a second so one second element of this 3d tensor but
213:20 - there is no second element since we only have just this one so what happens is we
213:25 - have all the zeros right here now the next is we say we want a zeroth element
213:30 - since this element actually exists we see what we get now is exactly this so
213:36 - it's basically giving us back this year now we turn this to one what do we what
213:45 - are we supposed to get since here oh we are on this axis we focus in on this
213:52 - rose focus on this rose we pick out the second row so zero one two we pick out
213:59 - the second row and the zeroth row so let's run this and we expect to get 20
214:05 - 21 22 and 0 1 2 so let's run that there we go 20 21 22 0 1 2 which is what we
214:13 - expected to get now let's double this so let's have this supposing we have this
214:21 - step of 3d tensor let's modify say this one let's find that 0 0 2 and that's it
214:37 - so let's run this now and what do we get we see that since we are on this we've
214:43 - picked out the first axis let's let's even go back to 0 let's run this yeah
214:49 - we see again that we still have the 0 0 because there is no second element but
214:55 - if we had one right here if you had one yeah we don't have the zeros anymore we
215:00 - have this year here is it and then we have the zeroth and here is it yeah now
215:07 - let's put this back to two and then send this axis to one we run that and here's
215:14 - what we get so as we are saying we are working with this axis this one fixed
215:21 - we're working with this axis we just function with a row so for this element
215:26 - right here we check out the rows what is the second row this is the second row
215:31 - and this one so that's what we have here and then for the next element where the
215:38 - second row we have 0 5 55 and 3 1 21 so that's how we obtain this now we have
215:46 - this other method gather nd so it's slightly different from the gather what
215:52 - it does is it gathers slices from params into a tensor which is specified by the
215:58 - indices so this indices is gonna influence the output shape so you have
216:06 - to say whereas in tf.gather that when working with a gather method indices
216:11 - defines the slices into the first dimension of params and the tf.gather
216:16 - nd indices define slices into the first n dimensions of params that said let's
216:23 - look at some examples so right here we have some examples here we have this
216:28 - indices the params gather nd takes in the params and the indices note that
216:35 - unlike with a gather yeah we don't have this axis argument so the gather
216:42 - nd doesn't take in the axis argument and we just basically have this params and
216:48 - then the indices so let's see how this works we have a param this tensor right
216:54 - here or this list which subsequently become a tensor after getting to this
216:59 - method and then as it was said in the documentation we're gonna have a tensor
217:05 - which is going to get the shape of the indices right here but the way is gonna
217:11 - get this is such that we're gonna have this zero element of params and the zero
217:18 - element of params happens to be this here's a zero element of params and then
217:22 - you're gonna have the first element of params so this happens to be this so
217:27 - let's run this and we see what that gives us we have here a b c d so that's
217:33 - fine we have exactly the same as params now if we take this off so let's run
217:39 - that we would have a b we have this one by two shaped output yeah we have this
217:47 - a b right here notice how since we've picked out the zeroth element we have
217:54 - this this is actually the zeroth element so we take this and then we replace right
217:59 - here and we get this output let's modify this to take e and then f so here we
218:10 - have e f that's it and let's do two for example we run this and we should get e
218:18 - f so that's it we have e f now let's do two with one so two one what do we have
218:25 - we have f now why do we have f we have f here because we're picking up the second
218:31 - row that's 0 1 2 this row and the first column so that's why we have just this
218:39 - f right here this is different from the gather where when we do this that's when
218:45 - we have like this index in this is specified to one and then we let's run
218:51 - this and then we do tf.gather params indices here yeah we should have this
219:03 - year ef that's two and then cd1 so we should note that with those gather nd
219:13 - our output is our output ship is defined by this indices right here so what we
219:21 - have is two one we pick this up and then there we go here's what we have
219:26 - whereas with a gather first of all we have an axis which is zero so we're
219:34 - working on the zeroth axis that is we're working with all the rows and then we
219:42 - can out the second row and zero all right on the first row so that's why we
219:47 - have ef cd stick this off we take this other example here now we have this
219:54 - indices so obviously we know that our output is going to take the shape so we
220:02 - expect to have this kind of output but then what values are we going to put in
220:07 - this output so that's what we're going to look now we have this year zero one
220:14 - since there's a 3d shape it's a 3d tensor we have zero when we say zero
220:21 - we're picking up this first element here the zero element and when we say one
220:26 - we're picking up this element right here now zero one means we're picking up this
220:32 - element and then one means we're picking up this because first of all when you
220:37 - pick up this element you have two elements in this element so we have this
220:42 - year and we have this year so when we pick up this element that's the zero
220:50 - element right here we now have to pick up this first element or the element of
220:54 - the first position so here we have the zero position and then we have the first
220:59 - position so that's how we have c0 d0 now for this other one right here so we
221:05 - already know that we would have an output which will look like this so we'll
221:10 - have c0 d0 they're actually strings so understand that yeah we have one zero
221:20 - now one year means we after this two elements we have out of this one on this
221:27 - element right here we're picking up this one so we pick this one year and then
221:33 - after picking this one we're gonna pick out the zeroth element so we have in two
221:38 - elements this zeroth and this first element we pick out this one so we have
221:43 - a1 b1 so right here we should have a1 and b1 now let's run this and we confirm
221:51 - that we have exactly the values we expected now if after picking that's if
221:57 - after picking for example here if we pick this zeroth element and then we
222:02 - pick the first element so we've picked the zeroth and then we've picked this
222:06 - first element right here now what if we want to pick as if we pick this first
222:11 - element in this element we have two elements that's c0 and d0 what if we want
222:17 - to pick this zeroth element so the zeroth element we should have c0 and now
222:23 - yeah we'll pick this first element with the zeroth that's a1 b1 what if we pick
222:29 - the first element that's b1 so this one right here we run that which we have c0
222:35 - and b1 so that's it let's take this example if we have this indices given to
222:43 - us and then we have the primes and we're trying to gather gather and the
222:48 - we're trying to get it up right here now what would have us output as usual would
222:54 - be of similar shape with the indices now we have 0 1 so notice how we have in
223:02 - here a three dimensional indices so we should have something like this 3d there
223:11 - we go we have this and then we have this okay so we have this 3d indices now we
223:23 - pick out 0 1 picking out 0 1 simply means we're picking out the zeroth
223:28 - element and then after picking out the zeroth element we pick out this first
223:34 - position here so we have c0 d0 and then from here so we first of all start by
223:41 - picking out c0 d0 c0 d0 and then we have then we have the next 1 0 1 0 we're
223:57 - picking out this first element and we're picking out a1 b1 so we have this a1 b1
224:04 - that's it we download this first part so we close that so we have this here we
224:12 - have this 2d tensor right here which is like this one and then we move on so for
224:22 - the next we have 0 0 1 1 now 0 0 is simply this so we have the zero and then
224:28 - we have this 0 so we have a let's just copy this copy this we have a right here
224:39 - we have a1 b1 a1 b1 and then 1 1 that's picking out this one and then this one
224:50 - so we have c1 d1 c1 d1 that's it okay so we're done with that we could now take
224:59 - this off so that's fine let's now go ahead and run this and compare answers
225:05 - here we have I suspected this 2 by 2 by 2 that's because we have this two
225:12 - elements and then for each of these elements we have 2 by 2 tensor so that's
225:19 - normal for the shape then here we have c0 d0 a1 b1 a0 b0 c1 d1 we actually
225:28 - having something different here here we have a0 b0 and here we have a1 b1
225:33 - let's understand why we should have a0 b0 right here we get back to this at
225:39 - this position we have this 0 0 now 0 0 we have the zeroed element and after
225:48 - picking up the zeroed element we have this zeroed position 0 a0 b0 so that
225:54 - should be a0 b0 so that's normal we made an error here we have had a0 b0 we now
226:01 - look at this gather nd method while taking into consideration this argument
226:08 - that's the batch dims argument right here by default the batch dims equals 0
226:13 - so by default we have this output right here where to obtain this 0 1 when you
226:21 - say 0 1 you're simply saying you're taking the zeroed element and then with
226:26 - the zeroed element you're taking the first index so you have this c0 d0 and
226:31 - then 1 0 means you're taking this first position and 0 so first position and
226:39 - this first position you have this zero position so a1 b1 and that's how we get
226:44 - this now let's turn this by dimension to 1 you see that we have a different
226:52 - output now let's understand how this output is gotten you'll have to note
226:58 - that this batch dims here is kind of like making this method to be batch
227:05 - aware so that said once we have this indices passed in that's this indices
227:12 - here we understand that this corresponds to a given batch that's this
227:19 - batch and this corresponds to this other batch so that said oh we haven't 0 1 and
227:27 - then we have already picked this batch so when we select the bad things equal
227:31 - one yeah we've already picked this zero batch and so doing zero year means we're
227:38 - picking this zero element here and then doing a one means after picking the
227:44 - zero element we're picking this one year and position one so that's why we have a
227:49 - b0 now for the next as I said is batch aware so this matches with this second
227:57 - element here with this element at position 1 now 1 here is 1 and 0 here is
228:06 - 0 c1 so that's how we obtain this right here unlike with a format where we had
228:13 - batch dims equals 0 let's look at this other example we have this indices we
228:20 - have the params now when the batch dims equals 0 saying that you want to have 0
228:28 - 1 is just like picking this so we pick this and then we have 1 which is c0 d0
228:34 - so that's why we have this c0 d0 1 0 we're picking this we're picking this
228:40 - element at position 1 and then 0 is a1 b1 so c0 d0 a1 b1 and then a0 b0 c1
228:50 - d1 we've seen this already now when you say batch dims equal 1 while you're
228:55 - saying here is all the indexing you will do here will match with this zeroed
229:03 - element so this zeroed element here will match with this so it's not batch aware
229:08 - and then this first element will match with this that said 0 1 year means
229:16 - first of all we've already selected this so 0 means we're picking out this
229:22 - zeroed element and then 1 means we're picking out this one so we're gonna have
229:27 - b0 so we have b0 1 0 we've already picked this one so we have c we have this
229:34 - 1 0 with this and this so we have b0 c0 now for the next we have is batch aware
229:43 - so we've already picked this and then 0 is this 0 0 is only picking only this
229:52 - one so we have a1 now for the next 1 1 means we're picking this one and then
229:56 - we're picking this d1 so we have b0 c0 a1 d1 now let's run this and we hope
230:05 - to get an exact answer so here we have c0 b0 c0 as expected and then a1 d1
230:17 - you could continue exploring other methods we'll now move on to racked
230:22 - tensors so we have your tf. racked yes overview we try to understand first of
230:32 - all what racked tensors are about to understand the racked tensors let's take
230:38 - this example we have this tensor 2d good print out this shape tensor 2d shape we
230:47 - have that which is 4 by 3 now let's copy this and then paste it out here and
230:56 - modify this one so we'll modify this so we have this 3 this like this 2 3 and
231:05 - that's it now let's print out tensor 2d let's print out tensor 2d there we go
231:15 - print out the shape what we get here is an expected error can't convert
231:22 - non rectangular Python sequence to a tensor so as you could see here
231:29 - tensors are meant to be rectangular that is if we decide to have four rows like
231:37 - in this case we have four rows that's this this this and this then each of
231:44 - this four rows must have the same number of columns so here we have three columns
231:49 - this one one column there's three columns there's two columns hands this
231:55 - arrow is thrown here but then it happens that sometimes we may be working with
232:03 - data which comes in this form that is we may have data where we could have for
232:10 - example this rose or we have this row or having three columns and this other row
232:16 - having one column this row other row even having say five columns go we have
232:22 - this kind of data and the way tensor flow deals with this is by using racked
232:30 - tensors now let's see how to create a rack tensor to create a rack tensor is
232:35 - quite easy we have tensor let's say tensor racked is equals TF dot racked
232:42 - dot constant so instead of just TF dot constant we have TF dot racked dot
232:47 - constant and there we go so we could also have this is like a simple list so
232:52 - could have this as a list let's put it this way we could have a list and then
232:59 - just pass this in here so we have tensor 2d now let's go ahead and print out a
233:07 - shape we have tensor racked that's it let's run that we have an invalid syntax
233:14 - here okay it's because of this so let's run this again and there we go we see we
233:21 - have for by noon now for because we have four rows that's fixed but then we don't
233:29 - have an exact number of columns so we just have unknown here we don't know
233:35 - exactly the number of columns we're dealing with but nonetheless we have
233:40 - racked tensor which we've just created so we see here we have this TF the
233:46 - racked tensor unlike with this let's print out tensor 2d if you print out
233:55 - tensor 2d you see you have TF dot constant now your this actually a list
234:02 - so let's say TF dot constant TF dot constant and then we have that oh we
234:13 - have an error here let's just come let's use this errors because it's not
234:20 - rectangular so that's normal so okay there we go we see we have TF dot tensor
234:25 - here and then here we have let's run this again here we have TF dot racked
234:31 - tensor so this is an example of a racked tensor wherein we have known rectangular
234:39 - data like in this case here apart from this constant method which we just saw
234:45 - we also have methods like the Boolean mask now with a Boolean mask we pass in
234:51 - our tensor or our list and then we pass in the mask let's check out on some
234:59 - examples so we see here we have this 1 2 3 4 5 6 7 8 9 we're passing the mask
235:08 - true false true false false false and true false false notice how wherever we
235:15 - have the false this data or this element in the data is taken off so this 2 is
235:21 - taken off and we're left with 1 3 so that's why we have 1 3 right here
235:26 - because we have true true and then here we have all false so this is an empty
235:30 - list here we have true false false so you have with just seven so that's it
235:35 - and from this we found this racked tensor right here now we could also do
235:40 - this for rows so instead of just specifying for each and every element we
235:45 - just say true false true and then this understands automatically that this
235:50 - means our first row is going to be left out so it's all right our first row is
235:57 - going to be taken so we have 1 2 3 maintained and then this next is false
236:03 - so it's going to be taken off and then this next is true you could check on
236:08 - this other methods we are going to look at the tf.ragged tensor this ragged
236:14 - tensor class contains several methods as you could see here let's check on this
236:19 - from the row lens method right here to see how we could create ragged tensors
236:25 - it takes a potentially ragged tensor the row lens is specified a name and then
236:32 - validate so let's see how this works we have this input right here and then we
236:40 - have this row lens specified so we run this what do we get we have 3141 empty
236:48 - list 5 9 2 6 empty list now how has this gotten you see that with this row lens
236:54 - we're saying okay we are going to start from this first position or from the
236:58 - 0th position and then the first four or the next four we have will form a row so
237:06 - we have 3141 from in this row and then from here we say the next we're gonna
237:14 - have is gonna be an empty list or the next row we're gonna have is gonna be
237:20 - empty so that's why we have this and then from here we continue from our
237:24 - position where we go to the next three so we've gone to the next four at this
237:28 - point we created this empty and then now we're going to the next three 5 9 2
237:34 - that's how we have this and then from here we have the next one six and then
237:39 - we have an empty so if we take this empty off of take that zero obviously you
237:44 - don't have the empty list anymore so that's it that's how we can create this
237:50 - rack tensors from the row lens from that we now look at the from row limits
237:57 - method here again we have this input values we have the row limits let's see
238:02 - how that works here since we're given the row limits we have this first value
238:07 - four so we come and put ourselves at this position right here we simply go
238:13 - one two three four elements we fix ourselves about this position we create
238:17 - this first element of our rack tensor there we go and then for the next four
238:22 - we are still at that same position so we move we still add that same position
238:28 - since we are at the same position and that we've already used up this first
238:32 - four positions we have this empty list again we move to the seventh position so
238:39 - we go let's go this way we move to the seventh position here we are at this
238:44 - point we pick this up so here we have 5 9 2 and then from the seventh position
238:52 - we move to the eighth position so we go to this position right here we pick this
238:57 - up we have the six again we have this eight position we still at the same
239:02 - position since we've already taken off all of this we just have an empty list
239:07 - so that's how this works let's now take one last you could always come back to
239:13 - the documentation to explore all those other methods with the from row splits
239:19 - method what we do is we actually checking out in this row split we've been
239:24 - giving here each element and the next element so we have 0 to 4 see right here
239:32 - 0 to 4 we pick out this 0 to 4 and yeah we constrain that this position this
239:41 - position here is 0 this position here is 1 this position here is 2 this position
239:48 - here is 3 and then this position here is 4 so when we say 0 to 4 we actually
239:53 - taken from this 0 1 2 3 4 so it does about this position and we get this
239:59 - first four elements that's how we get 3 1 4 1 and then the next we still at 4 so
240:06 - we still at this position so we're getting this 3 1 4 1 so we just have an
240:09 - empty list here now we move to we have 4 to 7 so we go again from here this is
240:17 - 4 this is 5 this is 6 this is 7 so we have all this 5 9 2 and then we have 7
240:25 - to 8 we have the 7 and then here we have 8 so we get 6 and then 8 to 8 since we
240:31 - are the same position we have this empty list right here we could also convert
240:37 - tensors directly into sparse tensors using the from tensor method so here we
240:43 - have this tensor defined this is rectangular and then we could automatically
240:48 - convert this into a sparse or rather into a racked tensor this from tensor
240:54 - method takes in this lens parameter where with this lens given to us we're
241:00 - able to take only certain parts of our racked tensor so yeah since we have one
241:07 - it means for the first row or for this row that we have here we're gonna take
241:13 - only this one or the first element the next we're gonna take no element so we
241:17 - have an empty list the next we're going to take all three elements so we have
241:21 - six zero zero so that's it for the racked tensor you can always explore all
241:27 - the sort of methods the documentation it should be noted that many times we will
241:33 - have to deal with data which contains many zeros
241:39 - it should be noted that many times we will have to deal with data which
241:45 - contains many zeros and a more efficient way of treating and storing these kinds
241:52 - of data is by using sparse tensors so we get to TF dot sparse right here
241:59 - chicken sparse tensor there we go we see how to create the sparse tensors
242:06 - right here we specify the indices we're gonna understand this soon so let's say
242:11 - we have 1 1 it's actually 2d so yeah we have 1 1 yeah we could have say 3 4
242:18 - there we go and then we specify the values this is gonna take let's say we
242:24 - add 11 and then 56 specify the shape let's say it's 5 by 6 there we go we
242:33 - have indices values and dense shape so yeah we've defined this tensor sparse
242:41 - run it and then let's bring this out so yeah we have tensor sparse there we go
242:48 - we see that we have exactly the same values we put out here but then let's
242:53 - understand how this relates to a usual tensor or how we could map this to a
242:59 - tensor now to do this let's do TF dot sparse dot to dense so note that we are
243:06 - not doing TF dot sparse dot sparse tensor we're just doing TF dot sparse dot to
243:11 - dense directly so we take this and we pass in tensor
243:14 - sparse we run that and what do we get we have this 5 by 6 tensor right here which
243:22 - happens to be what we had defined here so we had defined the shape to be 5 by
243:26 - 6 we said that at the position 1 1 that's at this this is the zero row the
243:34 - first row so we have this position first row first columns 1 1 we want to fit in
243:40 - this value 11 and we also said that at a position 34 want to fit in the value 56
243:47 - so we go 1 2 3 that's sorry 0 1 2 3 and then 0 1 2 3 4 so this is a position 3 4
243:59 - right here and then put in this value 56 so that's how we're able to map this
244:04 - back to a usual tensor then you'll notice that all the remaining values are
244:12 - zeros so this parse tensors permit us to work with these kinds of tensors more
244:20 - efficiently
244:23 - this last tensor type we'll be looking at is the string tensors so yeah we have
244:31 - TF dot strings and then we have this different methods which we could use
244:36 - before looking at this methods let's see how to create a simple string tensor so
244:44 - right here we have tensor string call it tensor string equals TF dot constant so
244:51 - basically we just have it a list made of string elements so hello I
245:01 - a string so that's it let's bring this out we have tensor string we run that
245:11 - and there we go we have this string tensor right here which hasn't defined
245:18 - the type string and then we have the different elements which make up our
245:23 - string you have as we have seen previously all this string methods right
245:29 - here let's look at this joint method with this joint method we are able to
245:35 - combine several elements of this list together and with a specific separator
245:43 - by default our separators is empty string right here so that said let's do
245:49 - this let's say we want to have all those elements joined together so we just have
245:56 - TF dot strings dot join and then we pass in tensor string so we pass in tensor
246:04 - string our separator our separator is equals let's say for now let's keep it
246:11 - at its default value we run that and what do we get we have hello I am a
246:15 - string now let's suppose that we want to have a separator which is the space so
246:22 - let's have this separator you see hello now there's some space between now let's
246:28 - modify the separator we put a plus and we run that and there we go hello I am a
246:36 - string with this separator right we have all the methods like the length the
246:42 - length basically tells us how long our string is so here in the case where we
246:49 - have this you see of hello tensor flow and you have this Unicode string you
246:56 - see how we're able to get that this length is five one two three four five
247:00 - this is 10 and then this is four we have the lower method which converts all
247:05 - uppercase characters into your respective lowercase replacements we have
247:09 - the n grams method and all these other methods which you could explore so that's
247:17 - it for tensors we now move on to variables to better understand tensor
247:22 - variables let's come back to this model which we had defined at the beginning of
247:27 - this course this values a1 a2 b1 b2 which are given initial values get
247:35 - updated as we train our model and so we need to use variables which can be
247:42 - updated as we do model training that said instead of using the TF dot
247:49 - constant we are gonna have X var X var which is equals TF dot variable so this
247:58 - how define this and a variable must always be initialized so yeah we pass an
248:02 - X and then we print out X var and there we go we have this TF the variable its
248:10 - name shape data type and its content we could include this so let's say we want
248:17 - to have var 1 let's specify it as name so we have that and now we've specified a
248:25 - variables name var 1 here's how we create a tensorful variable notice also
248:31 - that we have this trainable argument right here which says whether during the
248:37 - training we update that variable or not and so if this is false then during
248:44 - training this variable wouldn't be updated if it's true then this variable
248:48 - can be updated during training and then we have this other arguments which you
248:53 - could explore now well this TF the variable has other methods so we have
248:59 - the assign method for example and we'll see an example of how this is used we
249:04 - have this variable defined so we pass in this number 1 and then we assign its
249:11 - value to 2 so now when you print out this V what you're gonna get is this new
249:19 - value for our variable now you see we have the assign add this other method
249:24 - right here with the assign add we're simply doing addition on the elements
249:29 - now variable so let's check on assign sub this assign subtract yeah we're just
249:36 - doing subtraction so yeah we have xvar and we could do some subtraction so we
249:43 - say xvar dot assign sub of this three four so it takes in this three four and
249:51 - then what goes on here is we're gonna have one that's the actual value of xvar
249:57 - no xvar is actually this so okay as one so xvar we have this one minus this
250:03 - three and then this two minus is four so let's run this what do we get we have
250:08 - xvar negative two negative two if we modify this to six we have negative five
250:14 - negative eight reason being that the first time we did the update we went to
250:19 - negative two we had negative two negative two now we're doing the
250:23 - subtraction so we have a negative two minus three negative five and then
250:28 - negative two minus six negative eight so now we have negative five negative
250:32 - eight let's go ahead and add eight so let's just do this we have add we run
250:38 - that we should have zero zero so that's fine we have zero zero because negative
250:42 - five negative eight plus five eight would give us zeros so as usual you could
250:48 - check up on all this other methods in the documentation then another important
250:53 - part which we need to mention is the fact that you could decide on what
250:57 - device you want your variable to be so generally we have CPUs GPUs CPUs let's
251:08 - check on this runtime let's change runtime type you see that
251:10 - personally we're using a GPU so if I say none then that's the CPU TPU and then
251:17 - let's do that so yeah we could have with TF device and then we specify that
251:24 - device in this case we have a GPU so we have that GPU with this we could define
251:31 - our X bar so you define your X bar in this scope and you're actually saying
251:36 - that you want your X bar so X bar that's variable one or X bar to be in this GPU
251:44 - now if you wanted to be in a CPU you just simply specify your CPU and that's
251:49 - it now you could also let's print this out first so we have X bar let's run
251:56 - that and then you could check out a device which is found so check out a
252:00 - device see it's not CPU if we modify this to GPU you run that it's not GPU you
252:12 - should also note that this could be done with just simple tensor so we could have
252:17 - X tensor tf.constant and then 0.2 there we go print out X tensor the device so
252:28 - let's run that and we see we have this now we could let's do this copy that
252:35 - there we go we have now let's put your CPU so we have the CPU and then let's
252:43 - take this off there we go we run that and we compare so here we have a GPU and
252:48 - then we have a CPU for our tensor that said it's possible to initialize so you
252:55 - could initialize this right here like this let's say we want to have 1 2 3 4
253:03 - so we want to have 1 3 4 that's our X let's say this is X 1 and then yeah we
253:09 - want to have X 2 which is tf.constant and then we want to have here 1 so let's say
253:17 - we have this year let's put this list so we want to have this X 1 and X 2 which is in a
253:24 - CPU initialize in the CPU and then we want to carry out the computations in
253:29 - the GPU so that these computations could run faster so right here what we're
253:34 - gonna do is we're gonna have X 3 which is equals X 1 plus X 2 so as simple as
253:42 - that now obviously broadcasting will happen here and then we're gonna have 1
253:46 - 1 1 which when we add with all this it gives us 2 4 and then 5 so that's it
253:53 - let's now print out our X 1 let's take this off let's print out X 1 print out
254:01 - let's print out X 1 and where it's found so X 1 the device copy that take this
254:08 - off that's it we have now X 1 X 2 X 2 X 2 X 3 here X 3 so let's run this and see
254:20 - what we get we see we have this CPU CPU and then your GPU now we have 2 4 5 so
254:29 - output which is expected so that's it with just looked at tensor for variables
254:35 - and this marks the end of this part on tensors and variables don't forget to
254:41 - like and subscribe so you never miss amazing content like this and in case you
254:46 - want to gain some solid foundation on linear algebra you could check out our
254:50 - cards see you next time
254:54 - what's up everyone and welcome to the session where we build a linear
255:00 - regression model and a deep neural network to predict the current price of
255:05 - a second-hand car based on the number of years that cars been used its number of
255:11 - kilometers traveled its rating condition economy the present state of the economy
255:16 - top speed horsepower and torque and so we're gonna build models which when
255:23 - given this inputs permit us predict this current price as we could see in this
255:29 - results right here where we have in blue the models predictions and in orange the
255:36 - actual price so before moving on it's important to note that we are gonna
255:42 - follow this machine learning development lifecycle where we start with defining
255:47 - the task we'll look at the data source we're gonna prepare the data build
255:52 - machine learning models which permit us learn from this data to the right arrow
255:58 - functions for this learning process and then get into the training and
256:04 - optimization from here we're gonna measure the performance of the model on
256:08 - this data we're gonna validate and test our model and then finally we are going
256:15 - to take up some corrective measures to improve the performance of this model
256:20 - our task here is to predict the price of the used car using some input features
256:25 - in this case we've selected just one feature that is say the horsepower so
256:30 - supposing we have this and then we have the price in thousand dollars and then
256:36 - we want to make use of this input data right here that is this horsepower and
256:42 - the corresponding prices to train a model such that when given the
256:48 - horsepower we'll be able to predict the corresponding price of that used car or
256:54 - when given this 150 we'll be able to predict the price of that used car based
257:00 - on a model which has been trained on this data right here plotting out this
257:06 - data we could have this in the x-axis and then the price on the y-axis as you
257:11 - could see on this plot based on a given horsepower we are able to predict that
257:19 - the price of the second-hand car now if we draw points like this to draw lines
257:25 - like this from those points you see that we have a continuous range of real
257:30 - valued outputs right here and since our outputs here can take up continuous
257:38 - values we term our task as a regression task now let's modify this problem so
257:45 - that you better understand regression problem here we have this house power and
257:51 - then if we modify this such that we are going to insert predict whether the car
257:57 - is going to have the car is going to be noted as expensive or not so instead of
258:02 - predicting the price we may want to predict whether that car is expensive or
258:09 - not so let's say for all cars below say eight let's say 8.5 they are cheap and
258:18 - then so we'll call that cheap see and then greater than 8.5 we have the
258:24 - expensive cars so yeah we have going equals to here we have cheap here is
258:30 - expensive and so from here we may have a different kind of task that is one in
258:38 - which we want to say if based on some input or some inputs because you may
258:45 - have many inputs right here based on the inputs the car falls under one of these
258:53 - categories right here so you could see that this kind of problem is one in
259:00 - which our outputs are discrete see you have just two options and then with this
259:06 - problem where we're trying to predict the price we don't just have some two or
259:11 - three or four options which want to pick from we have an infinite number of
259:16 - options since the prices can go from let's say a thousand dollars we suppose
259:23 - that we'll fix a minimum to a thousand dollars and then the maximum to a
259:27 - hundred thousand dollars so we could fix this to a hundred K from K from one
259:34 - thousand to a hundred thousand but the values fall under this range which is
259:40 - actually an infinite number of possibilities unlike here where we have
259:44 - a finite number of possibilities
259:49 - we now move on to the data and before diving into that let's look at a big
259:54 - picture so here we are having a model which is gonna fit in this inputs and
260:00 - the outputs such that later on we could have this input fed and then we obtain
260:08 - the outputs notice a difference the change in direction of the arrows so
260:12 - initially we have this inputs so we could have this inputs right here let's
260:17 - take this off we could have this inputs that is this X and then this Y so here
260:23 - we go we have this inputs and then after the model trains we will now pass in
260:29 - just this and then we get this one automatically so let's get straight away
260:36 - into how we're gonna prepare this data such that it could be passed into this
260:43 - model right here you already have the second-hand cars data set made available
260:49 - on the cargo platform by Mayang Patel you already have a description of our
260:56 - data set so we have these different features which will be using to predict
261:01 - the price of the second-hand car so that said you could see here you could get
261:08 - more details so basically here's our data set as you could see and then by
261:15 - clicking on this so you could start and then you could get to this compact view
261:21 - column view here you see that we have the ID which is we're not gonna use
261:28 - this because this is actually not part of our data set but just an ID then we
261:35 - have this unrolled old feature right here though isn't actually well explained
261:41 - because we don't clearly see exactly what it signifies nonetheless we have
261:47 - let's take this off we have this mean and the standard deviation so as we had
261:54 - seen the previous video we could have this and most of our values around the
262:02 - mean that is most values we get in this data set for the unrolled old feature
262:10 - lie between the mean this is new the mean and signal standard deviation lie
262:15 - between the mean minus the standard deviation and the mean plus the standard
262:20 - deviation so in this range we have most of the values so this simply means most
262:25 - of our values lie between 602 K as we could see there plus or rather 602 K
262:32 - minus 558.4 K so that should be about 543.6 so if you subtract this 58.4 we
262:44 - have 543.6 so this means our values fall in this range now most of our values
262:56 - fall in the range 543.6 and 602.2 plus 58.4 so that's 660.4 so most of our
263:10 - values fall in this range right here so that's what we mean by having this mean
263:15 - and the standard deviation so we're gonna have this same right here now we
263:19 - have other features let's take this off we have other features as you could see we
263:24 - have unrolled old and unrolled now which wasn't very well explained we
263:29 - wouldn't use this and then we have the number of years that's obviously number
263:34 - of years of usage of the car you see we have missing number of missing values we
263:40 - have known number of mismatches known and valid a thousand so this kind of
263:45 - like cleaned data we have the mean that's 4.56 years and the standard deviation
263:52 - 1.72 years so most of our cars in this data set have been used for most of them
264:02 - will fall under the range for the 4.56 minus 1.72 and 4.56 plus 1.72 years so
264:10 - number of kilometers covered there we go we have the rating that's the car
264:17 - rating at the moment you want to buy it the condition and moment you want to buy
264:21 - it you see there's a mean right here the current state of the economy the top
264:28 - speed of the car the car's horsepower and that's it so your other different
264:34 - features we have and which we will use in predicting a car's current price so
264:43 - that said we could download this for free so we have this downloaded we now
264:47 - get into our collab notebook where we're gonna do the preparation of this data so
264:52 - here we have this three inputs we have tensor flow for our models we have
264:56 - pandas for reading and processing data we have C bond this is going to be for
265:07 - visualization so that's it now you don't need to have any prior knowledge on
265:13 - this tool as we're gonna explain every single step of our data preparation and
265:18 - visualization processes we've now uploaded our data set right here the
265:23 - strain dot CSV file where you download it from the cargo platform you could see
265:27 - it right here so double clicking on this gives us this file we have here now
265:35 - click on a hundred yeah we have 10 per page okay let's say I want to get a
265:40 - hundred per page so that's it we have this different data points right here
265:45 - now that said as you could see we have the ID on road old on road now the years
265:52 - number of kilometers radiant conditions economy top speed HP torque and then the
265:59 - current price so that's it that's for each and every data point and now let's
266:05 - do this so let's break this up so we could do this we could break this up so
266:10 - if we break this up like this you see we have this first section our inputs and
266:19 - we have this other sections right here all this this other section actually we
266:26 - have this this section our outputs so here is now X and here we have the Y
266:34 - yeah we suppose that we have totally n data points that is if we have if you
266:41 - count all these data points and we have n of them then we could have this shape
266:46 - all could represent this inputs in a tensor of shape and by yeah we're not
266:53 - taking this we're not gonna take this column we're not gonna take less we're
266:57 - not gonna take this column we're not gonna use this column because we don't
267:00 - understand it very well we're not gonna use this we're gonna use this so we have
267:04 - one two three four five six seven and eight so we have n by eight that's it
267:16 - that's our input X that's a shape of our input X and here we have n by one so
267:25 - this is a shape of our output tensor if you're able to put this data we should
267:33 - be given the CSV file in the form of a tensor with the shapes then you could
267:38 - pass this now into a model like this let's call this M a model has this in
267:46 - and then train this model such that when you come now with new data the model is
267:53 - able to predict the current price now to read CSV files we are gonna make use of
268:00 - this pandas rivalry right here is here we will define I was imported this as
268:04 - PD so that's why we have in this year so this particularly pandas and then we
268:08 - have read CSV so this method is used in reading CSV files like the train the
268:16 - CSV file we have here and then we specify the separator let's open up this
268:21 - file with Excel there we go yes what we get is very similar to what we've seen
268:28 - already closes up and now let's open this up with a notepad so okay that's
268:36 - it so you'll notice now that this is our same data but the way is put out is
268:43 - quite different as you could see all our column heads are separated by this
268:48 - commerce so we separate each column by a comma you can see that clearly from this
268:54 - and then when you go to the next row we also have the separation by commerce
269:00 - let's do this let's put this link so we see this one see linked right here we
269:07 - have this number right here linked to this we have this next number linked to
269:14 - this and so on and so forth so that's it and that's why we are having comma
269:22 - separated values format so this is the meaning of the CSV format commerce
269:28 - appeared values so we see each and every value is separated by a comma that's each
269:32 - and every value which makes up this row is separated by a comma and that's how
269:38 - CSV's are formed and you should now understand why in this code when we read
269:45 - the CSV was specified at our values are separated by commerce now in a case
269:52 - where you have data and then you have something like this so we may have this
270:00 - semi-colons everywhere so let's say we have the semi-colons we're gonna save
270:05 - it as a different file so we could have the semi-colons there we go you know all
270:11 - the separators common columns are separated with the semi-colons we save
270:17 - this and we've now uploaded it right here so that's what we have see we have
270:22 - this right here now let's go ahead and read this so we've created this data and
270:29 - we could say data.head you see data.head we run that and see we get the first
270:36 - five rows of our data and straight away you could already do stuff like this
270:42 - data.shape you see we already get the shape of our data we have a thousand by
270:47 - twelve now let's get it as semi so let's just copy this so we'll copy that and
270:54 - paste it out here and then let's run it with supposing our values are separated by
271:00 - commerce so let's run that and this is what we get you see we have this head
271:05 - which is not actually well formatted because by default this formatting is
271:10 - meant for comma separated values now what if we modify this we run that again
271:16 - and you notice that there is a difference actually now so let's do this
271:20 - side by side from that let's take this back to comma so we have that and you
271:27 - notice that there's a difference now here as well formatted whereas here is
271:31 - not very well formatted so that said if you want to work with these kinds of
271:36 - data you could always make use of the pandas library let's print out data
271:41 - that's shaped here we run that and then yeah we print out let's say data.shape so
271:48 - let's have that see yeah we have one by one which is not exactly what we expect
271:55 - to get we've now been able to read our data and then put this data in some sort
272:04 - of data structure let's now visualize how each and every feature right here is
272:11 - related with each other and to do this we're gonna use a seaborn library right
272:15 - here so seaborn has been imported as SNS so we have SNS.pairplot which we
272:21 - have here SNS.pairplot we pass in our data and then we have all these
272:28 - features with the output you can check out the seaborn.pairplot documentation on
272:33 - the seaborn website so right here you have the different parameters you have
272:38 - the data you have the hue which has to do with coloring and then we have this
272:44 - data kind which we use and the data kind is a kind of plot for the diagonal sub
272:49 - plots which we're gonna see shortly so yes we see we've selected a KDE we run
272:54 - this the KDE actually stands for canal density estimate after running this this
273:01 - is what we get we see this KDE plots in the diagonal now let's look at each and
273:09 - every one of this so let's take off this IDs because we're going to be using this
273:13 - ID unrode old unrode now that's because we don't understand exactly what the
273:20 - signify so we will run this again here is what we have now scroll up a bit and now
273:26 - we'll say we have the number of years and this plot here shows us how the
273:32 - number of years is related to the number of years you see how this related to a
273:39 - number of years in the diagonal we have how each feature is related to itself so
273:46 - let's reduce this okay so yeah you could see how this okay so you see out here we
273:56 - have this years related to years here we have kilometers related to kilometers
274:01 - radius to radians and so on and so forth that so that was diagonal that was a KDE
274:05 - plot now here we have how the years is related to the kilometers so we see how
274:15 - okay let's take it from this so we have here this kilometers and then how the
274:22 - years later to the kilometers here we have years related to the radians and
274:30 - you repeat the same process to get all those relationships between these
274:34 - different features but one very interesting point to note here is we
274:39 - could already see the relationship between these different features and the
274:44 - current price so let's take for example yeah we have current price so this is a
274:49 - plot showing how the years number of years is related to the current price and
274:54 - this is based on the data we have we see clearly from here that for each number
275:02 - of years spent the current price could go from very low to very high now if we
275:10 - move to the kilometers you see a certain pattern you notice that as we increase
275:16 - in the number of kilometers covered by the car the overall current price of the
275:21 - car drops so this is one very interesting feature to note we see a
275:27 - similar pattern to this years with the rating the condition the economy the top
275:33 - speed HP and tort so here is how each feature is related to the current price
275:42 - at this point we convert our data into a tensor so we have tensor data equals
275:49 - t of a constant and we pick in this data so we do this and we run that and there
275:56 - we go we have our tensor data so that's it let's print out this without a shape
276:02 - so we have that there we go we see we have our tensor of type float 64
276:07 - obviously you could do some casting so we could say tensor data equals TF the
276:14 - cast of tensor data and then we specify we want to have an eat say all right I
276:22 - want to have a float say 16 so let's run that again a problem which arises now is
276:29 - we have some values which are converted into infinity the reason why we have
276:36 - this is because some of these values are too large to be stored as foot 16 data
276:45 - types so we're gonna put this 32 you see we run that and everything is intact
276:51 - another important step to take is to randomly shuffle our data so to avoid
276:58 - any bias based on the way in which the data was gathered with it is random
277:04 - shuffling such that this other right here is no longer respected so let's do
277:12 - that what we need to do we have our tensor data is now equals TF the random
277:19 - that shuffle so we have that and we pass in tensor data that's it let's print out
277:26 - tensor data let's say we print out the first five elements so let's let's
277:32 - print that out here is what we get but then let's copy this so you could see
277:38 - the difference before and after the shuffling so let's say we have this
277:44 - let's take this off let's have that and then we have our tensor data let's run
277:51 - this again we print out tensor data you see we have this and then we print out
277:57 - this shuffled tensor data you see that here this ID order was respected 1 2 3
278:05 - and 2 1 2 4 whereas here we have 551 567 229 557 402 so we've actually
278:18 - shuffled our data and then let's take this off now so we've shuffled our data
278:24 - and then we're ready to break this data up such that we have both the inputs
278:34 - that's X and outputs Y or the output Y but continuing let's add this little text
278:42 - here so we have this text we add a text and we say we're doing data preparation
278:50 - that's it put it in bold and we're fine so oh we have that data preparation now
278:58 - right here as we're saying we need to get this X and the Y let's start by
279:04 - getting the X so here we have X tensor data that's it and then we pick out all
279:12 - the rows so obviously we're interested in getting all the rows interested in
279:18 - getting all thousand or 1000 rows but then we're not interested in some
279:24 - columns we're not interested in this column this column this column and this
279:29 - column so we're interested in just this or rather this so that said we have here
279:35 - 0 1 2 3 so we're gonna take from 3 right up to this position right here so that's
279:45 - it so we do that we just have here we pick out everything and then in this
279:52 - next we have 3 right up to negative 1 so let's print out a shape before
280:00 - printing out a shape let's look at this and get that shape before printing out so
280:07 - normally we should have a thousand by 12 but since we're picking out just the
280:12 - inputs now we have a thousand by 1 2 3 4 5 6 7 8 so we have now thousand by 8 so
280:24 - running this should give us just that we run that and there we go we have a
280:28 - thousand by 8 if you bring this out let's print out the first 5 we have that
280:35 - there we go we have just this now notice how here this first 3 values have
280:43 - been taken off and we left with this right up to this value so that's it and
280:50 - then the current price was been taken off we repeat the same process to get
280:55 - the output so here we have Y that's it okay but now we get in all the rows
281:03 - that's true but we get in just that last column so that's it yeah we know why and
281:12 - that's fine so that's what we get for Y not notice that here we have the shape
281:19 - which is actually just 1D so what we could do is after getting this we could
281:25 - expand themes so we have TF that expand themes to add that extra dimension so
281:33 - we have to have the expand themes and then we put that we specify the axis
281:38 - here we have negative 1 so we run that again and what do we get this what we
281:44 - get we have 5 by 1 just like here we have shape 5 by 8 now we have 5 by 1
281:50 - all like previously where we had just this 1D shape let's take this 5 here so
282:05 - previously we had just this 1D shape now we've added this extra dimension so that
282:10 - it matches that of the inputs we have here from this point another very common
282:17 - transformation which we could do on our data to enable our model train faster
282:25 - is by normalizing this data in fact we actually normalizing the inputs that is
282:34 - we take for every input we subtract the mean and we divide by the standard
282:42 - deviation in this example here the mean of this eight values is around 138 so
282:52 - that means if one normalizes data we're gonna have 109 minus 138 divided by the
283:04 - standard deviation let's say the standard deviation is 150 for example so
283:10 - this means this point is going to be converted into negative 0.193 if we now
283:23 - take a value like 206 we're gonna have 206 minus 138 divided by 150 which will
283:36 - now give us around 0.45 so if you notice our input features have been rescaled
283:47 - before passing into the model and to carry out those features killing TensorFlow
283:54 - has this normalization layer right here which is part of the tf.keras.layers
284:01 - now we haven't spoken much about tf.keras because we have not gotten
284:06 - into the modeling but for now just note that in this tf.keras.layers we have
284:13 - this normalization class which can be defined by specifying an axis the moon
284:21 - and the variance that's that we have from tensorflow.keras.layers we are
284:29 - going to import normalization so that's it that's from that and then we get to
284:37 - use this normalization layer before looking at our data let's see how we
284:44 - could use this normalization layer we define a normalizer then we have
284:48 - normalization we pass in nothing inside there by default the axis is negative 1
284:54 - we'll look at that shortly and then here we are gonna have the X to be normalized
285:01 - so we have X to be normalized which is this tensorflow tensor right here let's
285:10 - say we want to have 3 4 5 6 7 there we go and then we do normalizer and we
285:19 - pass in X to be normalized so that's it we have as this and we see we have
285:25 - exactly the same output that's because yeah we haven't specified the mean and
285:30 - the variance so we just have this the input pass as an output now let's go
285:37 - ahead and specify the mean so if we say we want to have a mean of let's say
285:41 - let's if we look at this let's say 5 and then the variance of C4 we run
285:49 - that and there we go what do we notice we notice that this inputs are being
285:57 - rescaled into this new inputs now let's add another wheel right here so we want
286:06 - to have four say five six seven let's keep it simple so we have this okay
286:13 - that's eight so we have this next row that's it and what do we do we run this
286:21 - let's correct that there we go we run that and we see we have this normalized
286:27 - properly though it's worth noting that seems by default the axis is equals
286:32 - negative 1 so by default we have this axis equal negative 1 it means this
286:38 - normalization here is done with respect to the columns so if you could recall
286:44 - from our previous section we have our shape 2 by 5 and picking the axis
286:50 - negative 1 means we're picking this axis here and this actually corresponds to
286:56 - the columns that's 1 2 3 4 5 so we have all five columns and then for each
287:03 - column we are doing the normalization so practically what we're doing here we're
287:08 - doing 3 minus 5 divided by the variance we should speak we normalize that 4
287:13 - minus 5 divided by the variance 4 we have the value and so on and so forth
287:18 - we've made a slight error in this definition is actually X minus the mean
287:24 - divided by the standard deviation so it's not a variance not divided by the
287:30 - variance but if a standard deviation now we also know that the standard deviation
287:34 - squared is equals the variance so basically when we have this variance to
287:44 - obtain the standard deviation we simply find the square root of our variance so
287:50 - in this case our variance is 4 so standard deviation is 2 if we replace here by 2 our
287:57 - mean is 5 that's it and now let's pick X let's take X for example to be this to
288:07 - be right here so if we have 3 see when we run 3 you should have negative 1 and if you
288:13 - pick 4 you should have negative 0.5 so that's it so that's how we get this now
288:19 - let's take another example but in this example we don't specify the mean and
288:25 - the variance now you should know that it's not an
288:28 - every situation where you could get the mean and the variance upfront so there
288:34 - are some cases or in fact in most cases you just have the data so you wouldn't
288:39 - always go and calculate this mean and variance upfront so what TensorFlow
288:46 - allows us to do is to obtain this mean and variance automatically so the TensorFlow
288:55 - permits us to adapt to the data we're given so if we're given this data for
289:00 - example what we're gonna do is get the mean and variance for this column that's
289:07 - this column right here middle 34 and then get the mean and variance for this
289:11 - column for this column this column this column and be able to normalize our data
289:17 - so just like here where we suppose the mean and variance for each and every
289:22 - column was 5 4 yeah we're gonna get the mean and variance for each and every
289:29 - column got an automatically so let's look at that we don't need to put a
289:34 - mean and variance now so we just let that go could take this off too since by
289:40 - default the axis is already negative 1 so we have that and then what we do is we
289:44 - do normalizer that adapt so we're gonna adapt to this our data we adapt to the
289:51 - normalized data and then we pass the normalizer pass X normalizing this to
289:58 - obtain this now so this is what we obtain when we adapt automatically so
290:02 - our data now let's understand what goes on we have 3 and 4 here the mean is 3.5
290:09 - so that's clear so we have X minus 3.5 and divided by the standard deviation if
290:20 - we have two values 3 and 4 it's clear the mean is gonna be 3.5 that's gonna be
290:28 - in the middle and then if we have a mean of 3.5 so we have something like this our
290:34 - standard deviation should be 3.5 minus 0.5 gives us 3 so here we have our mean
290:43 - minus the standard deviation to give our least possible value so here we have
290:49 - standard deviation of 0.5 such that 3.5 minus the standard deviation gives us
290:55 - 3 and still 0.5 such that 3.5 plus 0.5 gives us 4 so that's it so coming back
291:05 - here if we take this 0.5 and we put in the value 3 we run that you see we
291:14 - should let's correct this you see we have negative 1 so that's how we have
291:19 - negative 1 here and when we put 4 you see we have 1 now if we move to this
291:25 - next you have 4 and 5 and we maintain the same mean and standard deviation you
291:30 - see we wouldn't get those answers right so let's take this with 5 you see here
291:36 - we have 3 which is not what we get here see when we have 5 5 is transformed to
291:40 - 1 and this is made possible because we get the mean and the variance for each
291:46 - and every column so that said when it's 4 and 5 we have a mean of 4.5
291:54 - standard deviation 0.5 so if we change this now 4.5 0.5 put this to 4 you see
292:01 - we have negative 1 again and then 5 oh we have 1 so if we modify this now let's
292:08 - take this to let's say 10 let's run that what do you see we have negative 1 1 why
292:16 - do we have this is because the mean of this is 7 so yeah we're gonna have a
292:22 - mean of 7 we're gonna have a standard deviation of 3 when we have 10 we run
292:29 - that we see we have 1 still and then with 4 we have negative 1 still we could
292:37 - go ahead and add now one extra row so let's say 32 1 that and this let's run
292:47 - this again and we see that we have our normalized data right here and which has
292:54 - been done automatically unlike where we needed to specify the mean for each and
292:59 - every column here our normalizer adapts to our input data so how is all this
293:06 - related to what we've been doing here all we need to do is we'll be given X so
293:13 - we have X recording in normalization on the Y obviously we're doing this on the
293:18 - inputs so we are having our inputs X we've gotten already so we could print
293:24 - out the shape let's take this off you know the shape of X X shape see we have
293:31 - oh correct that we have a thousand by eight so we're gonna normalize for each
293:38 - of these eight columns and instead of struggling to get a mean and variance
293:43 - for each of those columns tensor for premise also adapts to this our data set
293:49 - so that said we have that given to us we could just we copy this oops we have
293:55 - that is it out we have our normalization normalized this is actually X so we
294:02 - don't need to specify this anymore so here we just adapt to X adapt to X and
294:09 - that's it let's bring out this X so we see kind of the kind of outputs we get
294:14 - let's say we want to get the first five elements again so that's it so what do
294:20 - we see we have that and then let's bring out X first five elements our first five
294:27 - rows there we go so yeah we see we have this five which is converted 0.25 we
294:32 - have this 4 to negative 0.2 3 2 and so on and so forth so this is with an
294:38 - automatically and we are now ready to clean our model using this normalized
294:45 - beater
294:47 - we've gotten to the point where we're gonna trade the machine learning model
294:51 - in our case our model is simply this straight line right here of the form of
294:58 - equation Y equals MX plus C as you could see here we have this inputs X so there
295:07 - we go inputs X outputs Y and we have our weights M and C which happens to be this
295:15 - constants that said we have an X which gets in here it gets multiplied by M so
295:25 - we have a multiplication by M and then we obtain M X so now we have M X and
295:33 - then it gets added to C so here we have multiplication and here we have addition
295:41 - and this gives us Y which actually equals MX plus C so let's say it gives
295:46 - us MX plus C let's take this off we have MX and then we put all this into a box
295:56 - which is what we call our model now notice here let's have this we have our
296:01 - model we have X get into a model and we have our output right here and then what
296:09 - we're trying to do is to get the most appropriate values for M and C such that
296:15 - we have an output which best represents our data set so that said our model could
296:26 - be represented by this line so we could have a model like this we could have
296:30 - this we could have this you see we have so many different possibilities and so
296:36 - when we talking about a model essentially what we're having is a
296:42 - function which tries to be representative of our data set now this
296:50 - means if let's take this off this means if we have a model like this clearly
296:57 - this isn't a very good model this is a very poor model since it doesn't
297:02 - actually represent it doesn't embody the data set we have in here and so a model
297:11 - looking like this will be a better one for now we'll just stick to the model
297:18 - creation but we should note that this model has to be chosen such that it
297:25 - represents best our data set so we're gonna look at this in the error
297:30 - management and training and optimization sections and before we get
297:35 - there that is before we see how to get the optimal values for M and C we are
297:41 - going to create this model using TensorFlow the good news is TensorFlow
297:49 - makes it very easy for us to create deep learning models so here we can define
297:54 - our model TensorFlow.keras.sequential so we have that and then we pass in
298:03 - our normalizer and we have a dense layer so here we have a dense layer which
298:09 - takes in an output of one now this dense layer is actually a Keras layer so
298:14 - let's get in here so from TensorFlow Keras layers we imported normalization
298:21 - previously now we import the dense layer so we have dense we run that again and
298:26 - right here we have a model model summary there we go we could visualize this
298:35 - model we've just created our first ever model with TensorFlow so let's break all
298:41 - this down now we have this sequential API which we use here in creating this
298:48 - model this is just one out of two different ways and ways models are
298:53 - generally created with TensorFlow we have the sequential API we have the
298:57 - functional API and then we have the subclassing method so that said for now
299:05 - we'll work with a sequential API we could look at the documentation for the
299:09 - sequential API in the TensorFlow.keras model so right here we have this
299:15 - sequential click on that and there we go so basically what this takes in is just
299:22 - layers and here we have some examples on how this is used so that said you could
299:29 - see we define a model and then we add layers to this model so basically we're
299:34 - just stacking up layers to this model now this means instead of having this
299:39 - syntax right here we could instead have this so we could take this off we take
299:46 - this off we have that and then we have model dot add tf.keras our
299:52 - legacy normalizer so we add the normalizer and then model dot add the
299:58 - dense layer so we have our dense layer output one and that's it let's run this
300:04 - and we get exactly the same output we had right here now let's take this off
300:11 - get back to our sequential API so as the word goes is the way we build models
300:19 - when layers all form a sequence that is if you're building deep learning models
300:28 - where the way they are constructed is such that we have the input we have the
300:33 - model we have the output and then all the layers which make up this model
300:41 - simply just stacked up one layer after another so we could have this layers
300:49 - which are from the model we'll look at more complex layers later but for now
300:54 - let's suppose we have this kind of model made of different layers layer one
300:58 - layer two up to layer n where we just simply stack up this layers in this way
301:05 - then working with a sequential API is a good option now let's take this off
301:13 - let's take this and then make use of the exact model we're actually currently
301:21 - dealing with right here we have this model and we have this other model so we
301:27 - have all rather we have this layer and we have this other layer right here we
301:32 - have the normalization layer which we've seen already we understand that our
301:37 - inputs need to be normalized before being passed into our dense layer right
301:46 - here so here we actually have this dense layer but then this should be the first
301:50 - time or maybe it's the first time you get in of this dense layer so we try to
301:55 - explain what goes on in this dense layer but you should note that this model is
302:03 - simply made of the normalization layer and the dense layer so without the
302:08 - normalization layer let's look at how the dense layer works with a dense layer
302:13 - as you could see here let's suppose we have an input so we have the horsepower
302:17 - input getting in here we have our input then in this dense layer we take this
302:24 - input the horsepower which we call X multiply by M and then add a value C
302:33 - this M happens to be the weight and then the C is what we call the bias so that
302:39 - said we have this and then here we have M X plus C so here's our output which is
302:49 - actually equals Y predicted so let's call this Y predicted and that's it so
302:57 - what do we do now well we have many variables once we have many variables we
303:03 - still have our same dense layer but there's a difference so here we have
303:08 - this variable let's suppose we have eight variables so we have one two three
303:14 - there we go we have this eight variables now all these get into our dense layer
303:22 - so we pass this into our dense layer then what goes on in this dense layer is
303:27 - that for each of these inputs we have this M1 M2 M3 M4 M5 up to M8 and we
303:39 - have this M1 times X so we have M1 X similar to what we had here here we just
303:47 - had one M since we had one input but now we have M1 times X that's the input here
303:53 - let's say X1 plus up to M8 X8 then we add that bias so we add this bias C
304:07 - right here and then we have plus C so yes our dense layer right here this is
304:13 - exactly what goes on in our dense layer and then we have obviously our output Y
304:18 - predicted so that's it we now understand exactly how this dense layer works now
304:25 - you'll notice that we have one two three four five six seven eight weights plus
304:31 - this bias making us nine different parameters and that's why right here you
304:39 - actually have this nine params given to you you have this non-trainable params 17
304:45 - this comes from the normalization layer now it's non-trainable simply because
304:51 - we've already fitted this normalization layer to our data or we've adapted this
304:58 - normalization layer to our data so we don't need to modify the parameters mu
305:03 - and variance anymore we don't need to modify the mean and the variance anymore
305:08 - actually so that's it we understand exactly what's going on and why we have
305:13 - nine trainable parameters here so from here we should note that the way we
305:19 - constructed dense layer is quite simple all you need to do is to see how many
305:24 - outputs do I need to have now in our case we just want to output a current
305:30 - price and since we want to output this current price which is just one value our
305:35 - number of outputs here equal one so that's why we pick this number of outputs
305:39 - to be equal one now if we needed to predict say two current prices or maybe
305:44 - you needed to predict the current price now and the current price in the year say
305:48 - 2030 then we would have output 2 so that's it we just have one output that's
305:54 - why we do that from here we have this model the summary which you can call
305:59 - very easily and see this kind of interesting summaries of our model so
306:04 - that's it we now look at how to plot this model out that's quite easy we have
306:11 - tensorflow.carazadutils.plot model and now we pass in our model we
306:17 - specify to file want to plot this model out and then generate a file so we just
306:25 - say model.png show shapes show shapes there we go true so we want to show
306:33 - shapes we run that we see clearly here we have an input layer we have our
306:39 - normalization and our dense layer notice how the inputs and the output has been
306:46 - specified so this known here is actually the back dimension and since we could
306:53 - treat data of any batch size we just have this as known now what's our batch
307:01 - size you see our full data set we had right here generally we are not going to
307:08 - load all this data set at once into our model as we may be limited by memory
307:15 - requirements so that said what we pass into our model is actually passing
307:21 - batches so we could work in batches of two so we could just pass out just two
307:26 - values or two of these data points into our model we could pass just this two
307:32 - and so on and so forth if our batch size is say eight we just pass all this into
307:39 - our data into our model at once and then move to the next batch and so on and so
307:44 - forth but now note that generally you will not want to have a very large batch
307:50 - size as this will lead to a problem known as over treating which will treat
307:56 - subsequently in this course but for now just note that working with batch sizes of 32
308:04 - and below is actually a good idea so let's come back to our model which we
308:10 - had here we now understand why we have this nodes here this actually our batch
308:16 - dimension now we could also in addition to this normalizer or before the
308:22 - normalizer would define an input layer so let's let's just declare this here
308:28 - let's just add this here let's import this we have input layer there we go so
308:34 - we have our input layer which we're gonna add from this Keras layers if you
308:39 - notice here we just had this node so our inputs are not specified the input
308:44 - shape is a specified output shape not specified so what if we come and
308:48 - specify this here we have input layer input layer sorry we have this and then
308:56 - we have the input shape which we could specify so yeah we have a shape of the
309:05 - batch by 8 so we have a batch dimension so if it's 32 we have 32 by 8 so this
309:11 - is what's getting to our model at once and not maybe the whole thousand samples
309:16 - we have so instead of sending all this we send just a batch of 32 now when
309:23 - creating this input layer since obviously upfront we don't know the
309:27 - exact batch size we'll take this off and just have that so we just have this
309:32 - shape we pass that we have this error let's add a comma and we also have to
309:39 - know that this is a list we're passing in so we have this first element of the
309:43 - list this next element on this other element and I would do this or we use
309:47 - the model add option which we saw already so we run this let's have this
309:55 - here and there we go so we see we have exactly the same kind of response as
310:01 - previously and then let's run this again so we see this difference now we notice
310:06 - that we don't have known again here now we know exactly the shape of our inputs
310:11 - so that said we could now get straight away to the next section which is that
310:16 - of error sanctioning
310:20 - so at this point with how to build a model as you could see y equals mx plus
310:27 - c and we'll now see how to know how well or how representative this model is of
310:38 - our data set so to check this out you'll notice that for each and every output we
310:46 - have so supposing we have this points right here we are going to compare the
310:52 - actual output with what the model gives us so our model tells us that at this
311:01 - point so we add this X this is X axis at this points right here we should have
311:10 - what our model tells us is we should have this output this so this is our
311:17 - selling price are cut into the model but then what we actually have is this so
311:24 - what comes from our data set or the actual selling price is at this point
311:29 - if we look at this other points right here you see that our model does quite
311:34 - well as our model or the actual price is this and if we extrapolate we see our
311:41 - model tells us our price is this so the actual price and the models price is
311:47 - quite similar now if you take another point like this you see this performs
311:51 - poorly here our model tells us we should be around this selling price and the
311:58 - actual selling price is around this so this tells us that if we want to choose
312:04 - the best possible values for m and c then we have to choose them such that we
312:11 - minimize these differences so these differences we have here that's this is
312:16 - what the model gives so we have to try to minimize these differences by
312:22 - minimizing the differences we actually sanction in this error so with sanction
312:27 - in the model every time it makes these kinds of errors so with these kinds of
312:31 - large errors we have here we're trying to sanction the model and then when we
312:37 - kind of like work perfectly like this our model receives less sanction or in
312:46 - the case where it's actually perfect our model is if no sanction that said this
312:51 - sanction is actually quite simple like oh what we'll do is when you have an
312:57 - error or simply let's say we have an output so we have this error but the way
313:04 - we calculate this error was we have an output and then we have the already we
313:09 - have the actual output and we have the models predicted output so we have the
313:13 - actual output here let's call it YA and then we have Y spread so this is what
313:19 - our model produced right here now what we could do is we subtract this two and
313:25 - then we square this and you see clearly that if the Y actual on the Y predicted
313:32 - both the same then in that case we will have zero because YA Y spread is the
313:38 - same so YA minus Y spread is zero and if we have zero it means we haven't to give
313:45 - that model a zero sanction for that particular prediction like in this case
313:50 - right here now if there's a very great difference between this two then we are
313:56 - subtracting it and also squaring so you see clearly that if you have maybe two
314:02 - and four so if our actual is four and what the model produces two then two
314:09 - minus four would have already four minus two anyway actually because we are going
314:14 - to square this so we have four minus two we're gonna give us two and if we just
314:20 - Y spread minus YA we would have two but now we're squaring this so we have two
314:24 - squares so we now have four so we're squaring this error so any time we make
314:29 - an error we amplify that error now if we want to get an overall error we could
314:35 - use what we call the mean square error function with a mean square error
314:39 - function what we're basically doing is this but we're repeating this for each
314:43 - and every point and then we're finding the average of all those errors and as
314:50 - usual tensorflow already has this built in so all we need to do is just make use
314:57 - of this function which has already been built here we have our loss function
315:02 - you notice that it's under the tensorflow.keras, tf.keras let's roll up here we have
315:09 - tf.keras we've seen the sequential now we're looking at the losses so here we
315:13 - have this losses we could also check in this layers here and you'll see that we
315:18 - have our dense layer so we start to understand how this tf.keras section
315:28 - here is structured so here we have our dense layer which we've seen already the
315:32 - number of output units we're going to look at all this subsequently for now
315:37 - let's close this and get back to our mean square error so here we have losses
315:44 - mean squared error that's it there we go we have here some examples so we can
315:53 - look at this y true that's the y actual under y predicted by our model now
315:58 - doing the mean square error here we have 0 minus 1 that's 1 1 square is 1 1 minus 1
316:07 - 0 we have 1 plus 0 plus 1 obviously 0 minus 1 is 1 is negative 1 negative 1
316:15 - square is 1 so for now we have 1 plus 1 because 0 minus 0 is 0 so when you add
316:21 - up all this we have 1 plus 1 that gives us 2 now how many different elements we
316:26 - have we have 4 elements so 2 divided by 4 it gives us 0.5 so we've summed up all
316:33 - these square errors and then we divide by the total number of elements we have
316:37 - which in this case is 4 so that's exactly how we obtained a 0.5 now we
316:43 - have that we just simply come right here and then we have from tensor flow that
316:50 - keras the losses we're going to import our mean squared error so that's it
316:57 - from that that's fine and we could make use of this mean square error now the
317:04 - way we make use of this is actually when we compile in our model so we have
317:09 - model compile and we define our loss so our loss is a mean square error which
317:16 - we just defined mean square error that's it and what this does is as we
317:23 - compile in our model we can take into consideration the fact that our error
317:28 - sanctioning function is going to be the mean squared error now for regression
317:34 - tasks apart from the mean square error we also have the mean absolute error and
317:41 - here every time we do the subtraction of the y true that's why actual and the y
317:47 - predicted instead of squaring this what we do is we calculate the absolute value
317:52 - so it's quite similar to what we've seen already with a mean square error just
317:57 - the only difference here is we have the absolute of the subtraction of the
318:03 - difference between the actual and the predicted value of y to better
318:08 - understand when to use the mean square error or the mean average error or rather
318:15 - the mean absolute error let's take this example you see with this example we
318:20 - have the horsepower and the current price most times the horsepower is
318:28 - positively correlated to the current price that is if we increase the
318:33 - horsepower generally overall we have an increase in the current price and for
318:39 - reduce the horsepower we have an overall reduce in the current price now we may
318:45 - have this point here which is actually what we call an outlier we see that we
318:51 - have a very high horsepower but the price is very low and in the case where
319:01 - we're using the mean squared error we'll be having y minus y is y a or let's say
319:09 - y yeah y actual and then y pred so we have y actual minus y pred and then
319:15 - we're gonna square all this you see that this is just one or is kind of like a
319:21 - minority in our data set and when modifying the weights of our model we
319:31 - don't want these to weigh too much or to have so much priority in the way we
319:38 - choose the values for M and C and so that said using a loss function like
319:46 - this which actually squares this errors is not a good idea because since this
319:54 - error here is going to be very large so if you continue with this this let's
320:02 - suppose around this so our model others are this X our model pick something like
320:06 - this will be around this and then the actual beyond this so we have this large
320:11 - error squaring this large error again gives us a very large value and so this
320:16 - will have very much to say when we're picking out this values for M and C as
320:24 - basically we're modifying these values based on this error so based on the error
320:30 - we're trying to modify the values such that we pick the values which minimize
320:35 - this error actually and as we've said in the case of the mean square error for
320:43 - these kinds of outliers we making them too important when we should focus more
320:50 - on these other data points now with a mean average error or with a mean
320:58 - absolute error we have YA minus YP and we computed absolute value now here
321:06 - clearly we understand that our overall loss or the loss we get with this kinds
321:13 - of outliers is reduced compared to this since here we just simply finding the
321:19 - absolute value unlike here where we're squaring and so in working with data
321:24 - sets where we have outliers like this is preferable to use the mean absolute
321:30 - error now there is a loss function known as a Yuba loss which actually permits us
321:38 - make use of this mean square error and the mean absolute error in a more
321:43 - intelligent manner that is when we have an outlier we are going to use the mean
321:49 - absolute error but when we have a normal data point we use the mean square error
321:55 - by normal data point we mean a point where the Y true minus the Y prepped
322:02 - which here is this X isn't or is less than a given threshold and by an outlier
322:10 - we're talking about points where the Y2 minus Y prepped is greater than a
322:14 - certain threshold so here we have the definition for the Yuba loss we see how
322:18 - this X X actually Y2 minus Y prepped so when coming the Yuba loss if we have
322:23 - this condition we use this variance of the mean square error where we have 0.5
322:30 - times this Y2 minus Y prepped square and then when is greater than D so here
322:36 - we're dealing with outliers as these are threshold when we dealing with outliers
322:40 - we have 0.5 times D squared that's times this different squared plus this D times
322:48 - the absolute value of Y true minus Y prepped minus D so there's a formula the
322:55 - Yuba loss formula again we don't need to write all this from scratch since we
323:01 - have this with tensor flow so that said all we need to define here is our delta
323:07 - which is that threshold that defines whether a data point is an outlier or
323:14 - not so here again we have our losses we have the Yuba loss and then we have the
323:19 - mean average or rather the mean absolute error this actually Yuba simply so we
323:28 - run that there we go so yeah we yeah we're gonna compile our model let's
323:34 - take this we have the mean square we have the mean absolute and we could
323:43 - also have the Yuba so if you run this you see your model compiles you could
323:47 - just simply specify Yuba but we need to pass in our Delta so we're running this
323:55 - we use the default Delta which was given to us if we want to specify our own
323:59 - Delta we just specify that way so we could take 0.2 and stuff like that so
324:04 - yeah we have all the default data is actually is one so that's it let's take
324:09 - this off we're gonna be using the mean the mean absolute error now we do a lot
324:20 - of experiments so after we're gonna train our model we're gonna measure its
324:25 - performance and so if you measure the performance and you see that it isn't
324:29 - good enough you could come and modify the error function using so you could
324:36 - change the mean absolute error we about to use to mean square error or maybe to
324:41 - the Yuba loss and from there you try to see which loss functions permits you get
324:49 - the best possible performance for your model
324:55 - from here we'll move on to training optimization recall our model was linear
325:01 - function which we were trying to eat such that this year this M and this X
325:09 - are rather this M and the C right here peaked so that this errors here are
325:19 - minimized so just looking at this we could have another line drawn here we
325:24 - have this line we could have this other line so we could have the line and so
325:30 - many on infinite possibility of an infinite number of possible lines we
325:36 - could generate and so in order to get this M and C what we use the method
325:46 - commonly used today is stochastic gradient descent now let's understand how
325:53 - this works we'll just use this or write this out in this one formula we have a
326:00 - weight which has to be updated we call that initially our weights random so we
326:08 - randomly initialize our weights so this means here that if we randomly
326:13 - initialize M and C and then let's suppose we we have M to be 0 so we have
326:20 - M zero and then let's say C is one then in that case we would have something
326:26 - like this we would have a line like this and it's clear that this line isn't very
326:32 - representative of this data we have here and so what we could do now is to update
326:39 - this M and C such that the take up values which permit us adapt to this
326:48 - data set which will be given and the way that's done as we've said already is
326:53 - with the SGD algorithm and this is how it goes so we have a weight so we have a
327:00 - weight previous so we have the previous weight or in the case where we are the
327:05 - first step we have the initialized weights minus a learning rate so we have
327:13 - a learning rate the rate at which we're learning this data which we've been
327:21 - given times the derivative now in case you have no background and calculus you
327:28 - shouldn't have any worries as TensorFlow would take care of this so yeah we have
327:34 - learning rate times this derivative of the loss function with respect to that
327:42 - weight so yeah we have the weight previous so that said if initially we
327:49 - have zero and then here we have C let's say C as we said already is two now what
327:55 - happens is for each and every weight we're gonna take this so we want to get
328:00 - the new values for the weights or for M so we have M it's gonna be equals its
328:05 - initial value is zero so the previous value is zero minus now let's pick a
328:10 - learning rate generally learning rates are picked in the other 0.001 0.1 0.01
328:21 - 0.0001 and you could continue to 1 times 10 to the say negative 6 so yeah we're
328:34 - supposing we've picked 0.1 and now we have 0.1 so that's our learning rate
328:40 - times the derivative of the loss now what's the loss we call we have already
328:50 - seen three loss functions in our error management section so here we have in
328:55 - 0.1 times the rate at which the loss changes with respect to that particular
329:03 - weight so we have 0.1 times this we respect to M and the same is done for C
329:15 - so we're gonna do 2 minus our same learning rate times DL on DC we've now
329:27 - updated M and C and then now if we compute the error we would have Y
329:34 - actual minus this new values of M and C are going to be used here so we're gonna
329:40 - have our new M times X plus our new C and we compute a new error so we'll have
329:47 - a new and so from here we expect that as we keep on training so as we keep on
329:56 - training we want our loss to keep dropping up to say a value of zero
330:03 - since we want to have zero loss though in this case we see clearly that it's
330:08 - not possible for us to use just a straight line and obtain a loss of zero
330:17 - since we cannot pass through each and every point with a straight line so if
330:23 - we're able to build this kind of model then would have a loss of zero since for
330:28 - each and every point the actual output is the same as our predicted output and
330:34 - so we'll obviously have zero loss but with a straight line that's not possible
330:41 - so as a recap we see that we have our inputs and we have our outputs we have M
330:48 - and C that's our model parameters we pass our inputs using our initialized M
330:56 - and C we get outputs we compute a loss that's we compute the difference between
331:01 - what we actually supposed to get and what our model predicts get this
331:07 - difference we could use a square or just absolute value function we get this loss
331:15 - and based on this loss we modify these values and then we repeat the same
331:22 - process to our training converges our training converges simply means we've
331:30 - attained a point where our loss doesn't increase anymore so if we have training
331:37 - like this we could get to this point to see at this certain point we keep on
331:43 - training that is we keep on repeating the gradient descent step which we've
331:48 - seen already where we updates by doing W equal W minus a learning rate times DL
331:58 - over DW so we repeat the step which we've seen already and if our loss
332:06 - doesn't change much or doesn't even change at all then our model has
332:11 - converged and we could stop training at that point as usual TensorFlow does all
332:17 - the hard work for us and so here we have model of feeds we pass in our X that's
332:22 - our inputs XY we specify the number of epochs let's say 100 verbose equals 1
332:30 - now we understand what this X and Y means that basically our data set now
332:35 - the epochs or the number of epochs here is specified the number of times we are
332:41 - going to update our weights so the number of times we are going to go to the
332:46 - gradient descent step and for verbose which it has to do with the outputs from
332:55 - our training step so yeah we've run we've compiled let's correct this we
333:01 - have okay so we have that we compile that's fine and we run this you see
333:08 - because our variables equal one we are able to get these kinds of outputs so
333:13 - here we're going to see in real time the values of our loss let's stop this so
333:20 - we stop that and then we send variables to zero you will notice that as the
333:26 - training goes on we don't get to see those loss values anymore so let's take
333:33 - this back stop this you see variables equal one now we're able to see a lot so
333:40 - as we go from one epoch to another we are able to get the mean absolute errors
333:48 - that the total mean absolute error which is an absolute value of our model
333:56 - predictions or the absolute value of the difference between our model predictions
334:00 - and the actual current prices when we get into this tier the Kerasone model
334:08 - documentation and we go to compile you see we could have this right here we
334:14 - have the definition of compile we see how by default our optimizer is RMS prop
334:19 - now note that this optimizer or those different optimizers are essentially
334:26 - variants of the stochastic gradient descent algorithm so yeah we get into
334:32 - optimizer as you could see we have added Delta out of grad item and you see you
334:37 - have SGD so this is the SGD we've seen already right here so we can have this
334:44 - SGD will specify the learning rate this point we understand what learning means
334:49 - we have the momentum we could increase this parameter so that we could speed up
334:54 - training and we could also specify whether we're having a nester of type
334:59 - momentum right here so when we see nester of true then we're having a nester
335:04 - of type momentum so that's it of all these optimizers the most commonly used
335:11 - is the Adam optimizer so you'll see that many sand is in practitioners generally
335:19 - use this optimizer by default the learning rate is 0.001 beta 1 0.9 beta 2
335:26 - 0.999 epsilon or 1 times 10 to the negative 7 and the AMS grab time is set
335:35 - to false to better understand the learning rate let's take this example so
335:40 - here we have loss versus a weight like the M or the C and now we have the
335:46 - derivative of the loss with respect to the weight so let's consider that this
335:49 - derivative is positive so let's take a point like this we have the weight here
335:55 - is this particular weight let's say W I would have picked out this point and we
336:00 - have this derivative of the loss with respect to the weight or the partial
336:05 - derivative actually and then we have let's say we have the slope so the slope
336:09 - is positive to update this weight from W I to a new weight we apply this formula
336:18 - so we have W I minus a learning rate times this positive slope now if this
336:25 - learning rate is too small let's say 10 to the power of negative 10 so we have a
336:30 - very small learning rate it's clear that we are not going to have a great
336:35 - difference between the W we obtain after this update since this is going to be
336:43 - multiplied by this positive value and W minus a very small number will give us
336:50 - value very close to W so there's not going to be a very great change after
336:56 - this update we're gonna have a very small change actually smaller than this
336:59 - now if L R does a learning rate is too large let's say we take a learning rate
337:05 - of 10 to the power of 3 in that case we have W minus a thousand times this positive
337:12 - value then the change is going to be too brutal and instead of say going slowly
337:19 - towards this point we are going to actually skip that point and get to this
337:27 - other point right here and if we keep doing this we actually go away from this
337:33 - point so we're picking out points which are far away from this point now why is
337:37 - this point important this point is important because at this point as we
337:41 - could see here the loss is minimal so recall that as an aim of this error
337:49 - sanctioning section where we're trying to minimize the loss that said many times
337:54 - by default or from many experiments starting out with 0.001 is going to be a
338:01 - good idea but note that you could always change this so we could feel free to
338:06 - change this you could take something bigger
338:08 - recall that if you take a bigger value your training is going to be faster but
338:12 - risk divergent that is risk not leaving you towards that minimal loss whereas if
338:19 - you take it too small then you would converge but your training is going to
338:24 - take too much time so this value is kind of a great one that is very commonly
338:30 - used now particularly for this item optimizer we have the parameters beta 1
338:34 - and beta 2 now your beta 1 max and beta 2 max are all 1 so taking a value like 0.9
338:41 - and 0.99 here means we're taking higher values of beta 1 and beta 2 you want to
338:46 - speed up training you want to increase the values for beta 1 and beta 2 by
338:50 - default this value is set to 0.9 0.99 now when doing or carrying out
338:57 - computations respect to the item optimizer we're trying to avoid dividing
339:01 - by zeros so we have this epsilon parameter here which is by default 1
339:07 - times 10 to the negative 7 now if one of the AMS grant variants of the item
339:12 - optimizer you could set this to true so that's it we now have from tensor flow
339:20 - that harass the optimizers we're gonna import Adam so we've imported that that's
339:29 - fine now we're gonna make use of that so actually in this compile we have
339:35 - optimizer equals Adam and that's it so we'll define our optimizer and we'll
339:41 - define the loss so let's run this again we actually had an arrow so let's take
339:47 - this Adam run that that's fine and there we go now we've been getting these
339:54 - values for loss what if we start this in a variable so we could have history
340:00 - equal model of feet so this time around after doing the training we'll be able
340:05 - to recall all this loss values we got during training there we go we have
340:12 - history the history we run that and here we got all our lost values let's climb
340:20 - up and you see here we have this dictionary we have lost and then we have
340:24 - this list of all the lost values we now import matplotlib so we now paste this
340:31 - out here we specify this plot we have history the history which we've seen
340:37 - already I was specified a loss because this actually a dictionary will pick the
340:41 - loss we specify the title y-level x-level the legend and then we show this lot so
340:50 - this is what we get right here we see how we drop our loss value is dropping
340:56 - but the way that which is dropping is actually quite small so what if we speed
341:02 - up the training by modifying the learning rate so right here we have
341:06 - learning rate equal 0.1 or let's just pick a learning rate of 1 and run that
341:16 - we run this and we see that the loss actually drops faster this time around
341:24 - from here we move on to performance measurement we actually measuring how
341:30 - well this model performs and one common function used in regression problems
341:38 - like this is the root mean square error you should note that is not in every
341:44 - case where you would have the performance measurement function similar
341:50 - to that of the loss as this two actually quite different concepts for the
341:55 - performance measurement are making use of the performance measurement we are
342:00 - able to see whether these two models so supposing we have two models model one
342:05 - and model two have the same performance or do have different performance and
342:15 - which of them outperforms the other so if we run performance measurement we'll
342:21 - be able to see whether model one out performs model two or model one under
342:28 - performs model two that said we could include this metric in here so we have
342:35 - the optimizer loss and the metric so we have this root mean square error metric
342:40 - which has been added and which has already been imported so imported this
342:43 - year note that this is a metric not loss so we have this metrics with important
342:49 - means square error and if that's fine we could go ahead compile and start our
342:56 - training again now you'll notice that both the loss and the root mean square
343:02 - errors are being printed out so this what we could see now after the training
343:08 - will be done we'll be able to plot this to out so here we have the loss to see
343:13 - we run the loss and we have the root mean square error so this is what we are
343:18 - putting to get this exact string right here you could do history dot history so
343:27 - here we have the history and you would see that okay here you have the loss so
343:34 - we have the first key loss then we have this list of all losses and then we have
343:41 - the next root mean square error and then we have this list made of all the
343:45 - root mean square error values so that's what we that's how we get that we've
343:50 - plotted it out and this is what we have another method which comes with tensor
343:58 - flow models is the evaluate method so here we could have model dot evaluate
344:03 - and we simply pass an X and Y so we evaluate our model this way so you see
344:09 - we've evaluated our model and we have the models loss and the root mean
344:16 - squared error
344:19 - from here we move to validation and testing to better understand validation
344:25 - and testing let's take a simple example imagine you get into class that is you
344:32 - get a class on the first day and you are being taught some course materials so
344:37 - you've been taught this course materials throughout the whole term or say the
344:41 - whole semester and then at the end of the semester the teacher who is feeling
344:49 - lazy tells each and every student to feel free to produce their own exam and
344:56 - after producing this exam they should sit for this exam produced by themselves
345:02 - so they could feel free to come up with any kind of question sit for this exam
345:08 - and after sitting for this exam the Mac describes by themselves it's clear that
345:13 - most students will have max greater than say 18 and 20 and this is simply because
345:20 - this exam has been set by the students themselves now it may happen that a
345:27 - student has followed through the course work and mastered everything and then set
345:32 - real questions or tough questions and got this Mac of 18 same as other
345:38 - students may not have terribly gone through this course work and produce
345:45 - very easy questions which will get them a Mac of greater than 18 without any
345:53 - much effort or without mastering the course work and so that's why the
345:58 - strategy is very dangerous as we need that external validation from our
346:09 - teacher and so far what we've been doing here is kind of like the first method
346:16 - where each and every student produces the exams writes sits for the exams and
346:22 - then marks the exams by themselves and that's simply because here we're using
346:27 - our full data set training on this full data set and evaluating this performance
346:36 - without taking into consideration data that a model has never seen so the idea
346:44 - is to be able to create a model that when it sees new data is able to come up
346:50 - with a reasonable current price which is as close as possible to the actual
346:56 - current price and so when dealing with machine learning models it's important
347:03 - to say break data set into two parts so if you have like the example we're
347:11 - working on 1000 examples you could break this up such that you have eight
347:18 - other examples which you train your model on and then you could test that
347:23 - model on another 200 examples so here we have never other model has never seen
347:32 - this part of your data set right here and this is one very important use of
347:38 - the shuffling because with the shuffling we are sure that there is no bias in the
347:43 - way the data set is is constituted as now if we just break this up we have one
347:52 - part for testing another part for training which has been randomly built
347:57 - and so as we're saying if a model for example has a performance of say
348:06 - root mean square value equals let's take say five on the training data so on
348:17 - this training data and then on the test data it has a root mean square value of
348:23 - 50,000 then it's clear that this is a very poor performing model as it does
348:32 - well on only on data it has seen but on data it hasn't yet seen it doesn't
348:38 - perform well and so recall machine learning is all about empowering the
348:44 - machine to do stuff humans and them to do and obviously humans use some
348:51 - intelligence and doing all those tasks so if you want to build a model you have
348:56 - to ensure that it performs well on data it has never seen and so we always have
349:03 - to split our data set so firstly we shuffle our data set very important we
349:09 - split this data set before proceeding with modeling and training now sometimes
349:16 - we don't want to wait until a model are strained before testing it out and
349:22 - discovering that it performs very poorly on data is never seen so what we want to
349:27 - do is while doing the training we want to be able to see how it performs on
349:33 - data it has not seen and that's why we have a validation set so that's why I
349:40 - need a validation set we'll see now we need a testing set as you can see here
349:44 - and then now we need a validation set and that said yeah we could reduce our
349:50 - training 600 and then we increase this so we have 200 so we have our data set
349:57 - of a thousand data points which is broken up now into our training
350:01 - validation and testing now note that if you have a data set of say let's
350:08 - increase this of this how many this is a hundred if you have a hundred million
350:14 - data points then taken for example like previously we chose this to this is
350:21 - actually 60% 20% 20% but if we have a hundred million data points we could
350:26 - always use 90% year year we could use 5% and year we could use 5% reason being
350:33 - that since the total data set is very large getting 5% of a very large number
350:41 - is already quite a huge number of samples so that is good enough for us to
350:47 - build our validation and testing sets we get back to our data set preparation
350:53 - let's include a cell here we specify our training and validation our training
350:59 - validation and test ratios so we're gonna use 80% of our total data for
351:04 - training or 10% for validation and 10% for testing the data set size we have a
351:12 - year the length of X is a thousand and we run this we added a code cell
351:21 - actually right here at this then we have this X train Y train made of elements in
351:28 - the top 80% position as here we have data set size so if you have 1000 times
351:34 - this ratio would give us 800 and we have the first 800 elements same with this
351:41 - we print out the X train shape and we could do same for Y train so that's it
351:47 - you have this shape right here so that's it we have booking this up thousand now
351:53 - we have a hundred let's do same so let's add this and do same now for the validation
351:58 - so instead of training we have valve and down here we have valve so we have valve
352:05 - and now we want the next 10% so what we're gonna do here is we simply copy
352:11 - this that's it here we have the validation ratio alright we have we're
352:19 - going from the train ratio actually so we have the train ratio up to the train
352:25 - ratio plus the validation ratio because we're going from 0.8 now to 0.9
352:32 - that said we would have this plus the validation ratio so that's fine next we
352:43 - let's just copy this so we've copied this there we go take this off we get
352:51 - the next values so that's it we've got in this next values oh we have X right
352:58 - here we have X valve X valve and here X valve so we run that let's close this
353:09 - right here so we have this we run it that's fine okay so now we have the next
353:15 - in our next hundred and that we now get the last hundred so since we have no
353:24 - other section we just all we need to do is just specify this specify this to
353:32 - validation actually but right up to the end so we have that we run it and there
353:38 - we go we have the data set which has been split into the training validation
353:43 - and testing we have X train Y train X from Y valve yeah we have X test this
353:49 - should be X test so let's change this X test X test X test okay so we run that
353:57 - and that's fine now one important point to notice you have to avoid information
354:05 - leaking from the training into the validation and the testing so this means
354:11 - even when doing normalization and you're trying to adapt your normalizer to your
354:16 - data you have to not use the validation and testing you just have to use the
354:22 - training so here we're gonna use only the training set to adapt our normalizer
354:28 - to our data from here we have to modify the way we do our training so here we
354:36 - just have to specify that we have X train we have Y train and then our
354:41 - validation data is equal we have this X valve and Y valve so that's it so here
354:54 - we specify X valve Y valve as your validation data and then you have X train Y
354:58 - train as your training data now note that when doing this so with a
355:05 - validation data you actually specify X valve Y valve but there is another
355:11 - argument which is the validation split where you just specify the fraction of
355:18 - the training data to be used as validation data so again here as usual
355:23 - you could go through this documentation you see you have shuffling which you
355:27 - could do but we should have done already you have all these other arguments which
355:31 - you could check out in this documentation getting back to the code
355:36 - we run this we run our training now training is done you could notice how we
355:43 - have this extra outputs here the validation would mean squared error and
355:50 - the validation loss so you could see you notice this right here let's get
355:55 - straight away and to put in the spots again let's do history let's add this
356:01 - so here history history here we see we have the validation loss go up we have
356:12 - the room in square error we should we should get the validation room in square
356:16 - error so let's scroll down there we go we have the validation would mean square
356:21 - error just right here so now we have the training and validation which has been
356:29 - outputted during training process we call the use of the validation is for
356:34 - us to be able to see during training how well our model performs on data it is
356:40 - never ever seen before so let's go ahead and do some plotting here we have our
356:47 - loss we have this right here we specify the validation loss there we go let's
356:59 - run that okay so we see our validation and training loss let's this year we
357:04 - have our loss so we specify the legend okay there we go we have our training
357:09 - data and we have our validation data we see that our validation data our model
357:15 - does better on our validation data actually as we have lower loss values
357:21 - for the validation data we could repeat the same process with the root mean
357:26 - square error so here we have that there we go we now put in your validation so
357:34 - here we have validation and then we have validation we run that and it's kind of
357:40 - like similar here the validation does better or the model does better on our
357:45 - validation set from this point let's take this off we get back to model
357:50 - evaluate so let's evaluate our model by just specifying this wouldn't help so
357:58 - you could actually put this X valve Y valve so we're not evaluating anymore on
358:04 - our training data but not a validation data and we could also now evaluate on
358:09 - our test data recall our validation was used during training we saw how it
358:16 - performed during training now we're gonna evaluate our model on data it has
358:20 - never seen so we evaluate on our test data and this is what we get you see we
358:25 - have this loss and this root mean square error value at this point we'll train our
358:32 - data or we've trained our model on our data and we can get to that long
358:40 - awaited part that is testing our model so here we are not just evaluating our
358:47 - model or doing some validation but we are passing in data and then we are
358:52 - allowing our model to predict the car price for us so that said let's get
358:59 - straight away to that we have X test which we know already well which we
359:03 - built already there is it extend our shape there we go
359:10 - let's pick out a value from this X test let's just say X test 0 so what we do is
359:16 - model that predicts so now we've trained our model we will now do model
359:20 - predict how we pass in X test so here we don't need to pass Y test we just
359:26 - pass X test so here we pass X test and our model should predict the car price
359:35 - for us now passing X test we have all those predictions so because we have a
359:41 - hundred you see shape there we go we have a hundred different elements so for
359:47 - each and every data point of for each and every input in our test and set we
359:53 - have all these outputs let's just pick out a few let's say we want just the
359:57 - first so there's it there is one which is normal because the input shape here
360:03 - has to be of type batch size by 8 but yeah we send in this shape 8 so we
360:12 - should do expand games in case you don't understand this well let me repeat again
360:17 - or let me just do this test zero that shape that's a shape you see that's the
360:25 - zero shape now X test the shape that's the shape now this model as we defined
360:32 - previously you know input right here you know input takes in batch size by 8 so
360:38 - here we have to ensure that if we're passing in any data it has to be of the
360:44 - sheep so now we have put in this and what we know to do is to expand them so
360:51 - that will leave from the shape 8 to the shape 1 8 so that it takes this form and
360:59 - when is like this it means our batch size is 1 so that said let's do expand
361:04 - teams we've seen this in the previous section we have expand teams there we
361:08 - go we add this axis equals zero that's fine and we run that so that's it
361:19 - everything works fine now let's take this off we run that again and we have
361:24 - our response so this tells us that for our first point in our data set or first
361:30 - input our car price prediction is this now oh this is very interesting to view
361:39 - but what if we compare this with what the model or what the data already
361:47 - presented to us because we it's true we level this as test but we know the
361:53 - actual values so we know the actual car price and what if we compare it with
361:58 - this predicted car price so that said let's look at why test 0 and check out
362:06 - this value so here we have a hundred and thirty thousand dollars and here we're
362:11 - predicting eleven thousand nine hundred and forty dollars so clearly our model
362:16 - is performing very poorly so visually understand this form model performance
362:22 - we are gonna plot a bar chart showing the predicted and true values shown
362:31 - side-by-side let's take this to say 20 and there we go as you could see right
362:39 - here scroll okay that's fine as you could see we have this blue which is for
362:46 - predicted and then the orange for the actual values of y so we have the
362:54 - predicted and the actual prices which is shown to us now the world is out is
363:01 - quite simple we have we define we have the figure size we have this bar so
363:08 - yeah we put now some bar charts and we specifying the position of each element
363:14 - respect to the bar chart now there's in here if you plot this out so we're not
363:20 - we also have a non pie to get this so let's run that and you see that this is
363:26 - a list comparison a value from 0 to 99 so this permits us get the position for
363:34 - each and every element so we have a hundred different positions and then the
363:40 - width obviously this Claire let's reduce the width so you understand that you see
363:46 - that visually let's take this let's increase it to one so we've increased the
363:49 - width you see this becomes very large now let's take that back let's see 0.1
363:57 - you have this you see comes very team and that's it so we could play around
364:03 - with a width now once we fix a position say we'll fix this position we now move
364:10 - with steps further so that's what we do and we have that passed in here we have
364:18 - the level X level Y level and then we put that out so that's how we come up
364:23 - with this plot we shows clearly that our model is performing very poorly
364:31 - this takes us to our next section on corrective measures this one model of
364:38 - performance actually has a name it's called under PT so when you're training
364:45 - loss looks like this and your validation loss normally validation loss
364:50 - should be above the training loss but in some cases like as we just saw it was
364:56 - below the training loss so we have a validation so let's say we have our
365:01 - training and then we have our validation losses which are like this and even after
365:07 - training for a long period of time or for so many epochs we are not able to go
365:12 - below a certain threshold this is known as under feeding and the idea here is to
365:20 - bring this loss values like this so we want to modify our model so that we
365:28 - could have better loss values that's we could reduce our loss values as much as
365:33 - we can that said in order to do this it suffices to make our model more complex
365:40 - for now the model we're having is like this so here we're having this simple
365:46 - regression model where we have our inputs right here and one m2 m2 m8 our
365:54 - weights and then our bias C we add this up and then we have our output so what
366:00 - if we don't we take this off and then we add up more neurons right here so instead
366:09 - of just having one we're gonna stack up more neurons but they are doing
366:13 - basically the same thing so kind of like repeating the same process here so we're
366:18 - gonna have this link to this this one link to this the same way is linked to
366:23 - this one we had previously this link to this and so on and so forth and then
366:28 - we'll do the same process for this link then from here so here we have this
366:35 - dense layer from here again we could add more dense layers so this is what we
366:43 - call a hidden layer here we have a hidden layer we add more so here we
366:48 - could have one and you see here all these are all linked up this way and it's
366:53 - actually the same operation mx plus C so basically we're taking this weight
366:59 - times this plus bias and then we get in this take this times its weight and we
367:06 - get this plus bias we have this and then we continue this same operation so we
367:12 - have this stacked again we have first hidden layer in layer one in layer two
367:16 - and we could even go to another one again so let's add this to here let's
367:22 - add this other hidden layer let's do this there we go we've created this
367:28 - turret hidden layer so here we have our inputs and we have three hidden layers
367:34 - and obviously since we just producing a single price from there we could have this so we now have this our output layer here so input output and then hidden layers
367:49 - here we have this so we have this dense layer and we have also this dense layer
367:56 - let's try to write out or sketch out some tensorflow code here so basically we're
368:02 - gonna have dense so we have a dense layer right here and what does it output that is this first dense layer has key outputs basically comma and then we move to the next dense layer so if I have an at least we have this dense layer right here
368:19 - the outputs unlike the previous one we have just one output now we have this dense layer with two outputs we stack the next dense layer so you see the way its easy to carry out all these operations with tensorflow
368:33 - we stack the next layer with four so we have four outputs then the next we have two outputs and then finally we have one output it's very important to ensure that
368:48 - we have this one output because that is matching up with our data we have a hint where we have one output and we put this in bracket and then have this sequential and all that
369:05 - so that's it we have seen how we could do the real returns of flow and there is one point we need to mention that's the activation functions
369:15 - activation functions since we put non-linear functions which add even more complexity to the model you saw how previously we had these inputs and then we had this one output and we linked them up like this
369:31 - now we've made this model more complicated we've added more hidden layers so that we could learn more complex information stored in our data set
369:42 - now another thing we could do is add in the activation functions as we have said already and make this model even more complex
369:53 - common activation functions are the sigmoid activation function, the tensh activation function, the relu activation function rectified linear unit and the leaky relu
370:07 - so we could have this relu that is when x is greater than 0 we maintain x that is x remains the same or the output equal the input but when x is less than 0 the output becomes 0
370:22 - now here when x is greater than 0 the output is the same when x is less than 0 the output is negative of certain alpha times x
370:34 - hence the term leaky as this alpha is generally a very small number so if we have 0.1 we could have negative 0.1x which is kind of like giving us very small outputs which are kind of close to 0
370:53 - now we have the sigmoid activation function 1 divided by 1 plus e to the negative x see looks like this the tensh e to the x minus e to the negative x divided by e to the x plus e to the negative x
371:07 - for now we are going to use this relu activation function and you will see subsequently that all these activation functions could be gotten from tensor flow parast activations
371:23 - so note that if we are to apply for example the sigmoid activation function right here what we will do is just after summing up or just after multiplying the weights by the inputs and getting the outputs
371:37 - once we have this we are going to do sigmoid of this so we do sigmoid so actually for each and every neuron right here we have the sigmoid of this so here we have sigmoid of our computations
371:55 - or the output of the computations here we have sigmoid here we have sigmoid and in the case in this layer we have the sigmoid then for each and every neuron we have sigmoid if here we have for example relu then for each and every neuron we have relu activation
372:09 - we now have all that is needed to make our model perform better and stop under feeding and so we come right up here we have a dense layer now let's say we have 32 so our op will be 32 neurons then from this we have 32 again
372:27 - and then we stack up another one and finally we have this layer right here recall that we've added up this other dense layers but at the output we need to have just one neuron so it's important to ensure that our output here is one
372:47 - we now specify the activation so we have activation equal relu and here we do the same for our output here we are not going to specify any activation because we don't want to interfere in the way our model comes up with its outputs
373:07 - that said we could run this and to make our model even more complicated we could increase this value so we could take this to 128
373:18 - let's run that again you see that we have many more parameters we are training on again we are still having the 17 non-trainable parameters
373:27 - so when we had this when we had this you see we had 17 and 9 trainable now that we have this you see the number of trainable parameters increases while the non-trainable parameters remain on the same spot
373:44 - we have the 17 non-trainable parameters which is logical since our normalizer remains the same now with that said we've done that we could put our model so we see this plot right here
373:57 - that's a clear plot to see how we live from the input to this dense layer see here known by 8 known by 128 this next dense layer this other dense layer and finally this dense layer from there we compile our model let's take this so here we have 0.1
374:17 - let's run this and we are ready to fit our model so let's run that we now notice how this loss right here is a smaller order of magnitude as compared to what we had previously
374:35 - notice how this drops from 100,000 about 144,000 and goes down to like 30,000 so that's it for this training we now go ahead and view our plots
374:52 - so there we go we see we have totally different plot now we live from this and then we actually drop to about 30,000
375:02 - also notice how this time around we have a validation loss which is higher than that of the training and this is normal because obviously the model was trained on the training data so it would turn to perform better than data it wasn't trained on
375:23 - now if a model performs very very well on the training data and doesn't perform well on the validation then we have a problem which is known as over feeding
375:38 - and we are going to trade this in subsequent sections so for now we just continue we cut out the root mean squared error notice recall how we used to have values of this order 234,000 but now we are around 30,000
375:55 - okay let's run this what do we have okay so we have something very similar to the mean average error that's our loss that's fine we evaluate our model so that's it we have this loss for our model on our test data
376:12 - so here is our loss on the test data here is our root mean squared error from there that's fine okay we do this white bread and here is what we now obtain
376:27 - we notice how this model performs way better than that previous smaller model
376:35 - and as you could see here for this particular test data point you see our what our model predicts is exactly the actual current price
376:45 - our model isn't perfect but it's doing quite well at this point we've taken up some creative measures and our model now performs better
376:56 - what if we look at how to load our model even faster and more efficiently this can be done thanks to the TensorFlow data API
377:08 - so right here you have tf.data you can go to the overview and then for now we check on this data set right here
377:17 - this class is made of many methods so you have those different methods here well now we'll start with the fromTensorSlices method
377:28 - so from here we're going to adapt our code to use TensorFlow's data API in order for us to gain from all the advantages that come with it
377:38 - now note that when you're working with a data set of say a thousand elements you wouldn't see or you wouldn't clearly notice the advantage of using this data API
377:50 - but as your data set gets larger it becomes very important to master how to use this API
377:58 - now that set we're going to redefine our XTrain so yeah we're going to say train data set train data set and we're going to have tf.data.data set
378:09 - .fromTensorSlices so that's it now we're going to have this tuple which we create and then we pass in XTrain and YTrain so that's it
378:24 - now we've gotten this the next thing we're going to do is we're going to shuffle our data set now we've already done shuffling
378:34 - but in the case where shuffling wasn't done previously you could do the shuffling very easily now so here we have this.shuffle and we specify a buffer size
378:46 - buffer size let's say 8 and let's see exactly what this buffer size actually means
378:54 - so here we have a seed and you have reshuffle each after each iteration now let's look at how this buffer size or what it means exactly
379:03 - so here we have been told if you did a set contains 10,000 elements by buffer sizes set to a thousand then shuffle will initially select a random element
379:12 - from only this first 1000 elements so it's just like saying I'm picking this first 1000 elements and I want to carry out my shuffling
379:24 - only from this first 1000 elements such that when I'm picking a random value obviously when we're doing shuffling we're picking out random values
379:33 - in a random order so when I'm picking up my random value I'm going to pick it up from this first 1000 values then as I said here
379:43 - this once this element is selected its space in this buffer is replaced by the next 1000 first elements so if we have a data set like this
379:55 - with one, two, three, four, five, six elements and then we want to shuffle with a buffer size of three in this case we'll initially select
380:07 - or pick up a random value from this here from this buffer from this first three elements and then if suppose we pick this random value right here
380:17 - what we'll do let's take this let's annotate this one, two, three, four, five and six so if we've picked up two we have picked up two
380:30 - now we're going to have our next buffer to be like this we're going to now have one, two, three and four
380:40 - so this next element is going to be added to our buffer and then we'll have this five, six, five, six right here
380:48 - and once we're done with this so once we pick up another random number let's say we pick up four, we pick up four here
380:54 - our next buffer will be one, three, five so we have one, three, the next five
381:00 - one, three, five and we repeat this and so on and so forth so that's how we work with this
381:10 - that said yeah we could also pick reshuffle each iteration so after each epoch we're going to reshuffle again
381:21 - so that's it we've had our trained data set shuffled and then we have again trained data set bashed
381:30 - so we could just add this right here actually we could say we've shuffled then we batch this by size 32
381:37 - and then from here we could do prefetching so let's do some prefetching prefetch
381:44 - we're taking a documentation to see what it takes this argument to see again here we have a buffer size now
381:51 - we are told that this allows later elements to be prepared why the current element has been pre-processed or been processed
381:59 - and so if we have a data set like this so let's suppose that we have this three elements element one, elements two and elements three
382:09 - so currently we are actually training on this data or on this batch here we're actually training on this
382:16 - and generally when we train on this we have to train after training we have to load this data process it in case we need to process it
382:26 - and then so let's let's suppose that this is processing time so for our key this is training time
382:35 - after we process then after we train after we process this here so after we process that and then we train
382:47 - initially we even process this so initially we will have this so we process this one
382:52 - we train we process we train we process with train now what if instead of going to these steps
383:00 - we load, so yeah, we start by loading, we load.
383:05 - And then while we're training,
383:08 - we actually loading up our data.
383:11 - So we could be loading up our data while we're training.
383:14 - So this could be done.
383:16 - And then after training,
383:17 - we have this loaded up data already.
383:20 - So we just continue with the training.
383:22 - And then from here, again, while we're training here,
383:26 - we load it up.
383:28 - So loading up, suppose we load this up like this.
383:32 - And then from here, from this point, we train.
383:36 - So recall we have one, two, three train steps
383:39 - or train blocks.
383:41 - So at this point, we add our last block, so that's it.
383:45 - So from here, we see that we take less time
383:49 - to load and train our data as compared to this first method.
383:53 - So working with prefetching is very important.
383:57 - Now, this prefetching takes up a buffer size,
384:00 - which can be auto-tuned.
384:01 - So here you have tf.data.auto-tuned.
384:04 - In case you don't want to specify the buffer size,
384:07 - you could just simply allow tensorflow
384:11 - dynamically tune this buffer size for you.
384:14 - Now that's set, yeah, we have this.
384:16 - So we have prefetch and then tf.data.auto-tuned, that's fine.
384:22 - So that's our data, let's take this off.
384:27 - So we now have train data or train data set.
384:30 - Let's add this and let's run this actually.
384:34 - And then for
384:37 - T in train data set, we print out C.
384:44 - Now with, okay, we'll batch that.
384:46 - So let's print out C and then let's print out
384:48 - just the first element actually.
384:50 - Let's say for x, y, so we could print out x and y separately.
384:54 - So we have x, y, there's a first, that's it.
384:58 - You see, we're going to have this, like this, let's go up.
385:02 - Okay, so yeah, we see we have this right here.
385:07 - What's its shape?
385:09 - So we have 32 by eight, which is normal batch size by eight.
385:13 - And then we scroll down, scroll down.
385:18 - Yeah, we have 32 by one, which is normal.
385:20 - So that's very normal.
385:22 - We've bashed our data and we're ready to train this data.
385:26 - So let's just recopy this and do the same
385:28 - for the validation right here.
385:30 - We have a validation,
385:34 - val data set, and we have val data set.
385:40 - Don't forget to change this.
385:41 - So you have val, that's it.
385:45 - That's fine.
385:47 - We run this, our val data set.
385:49 - And now we go to the test.
385:54 - So right here, you could also do the same for the test.
385:57 - We have that, test, test, there we go, test, test.
386:06 - So that's what we need to do to convert this
386:09 - so that we could use it as the,
386:11 - or we could work with a TensorFlow data API.
386:14 - We don't do any modifications in the model.
386:16 - All we need to do here is come and specify,
386:19 - train data set.
386:21 - And then for validation, we have our val data set.
386:27 - So here we have val data set.
386:29 - So that's fine.
386:30 - You see training has started and everything seems okay.
386:34 - So we'll now go ahead and do this right here.
386:42 - So that's what we get.
386:43 - So it still continues dropping.
386:45 - Actually we could, from the previous training,
386:50 - we would have continued training.
386:51 - So we could have increased the number of epochs.
386:54 - That's fine.
386:56 - Let's look at this as well.
387:00 - And we could evaluate our model.
387:04 - Now note that this actually isn't very different
387:09 - from what we had previously as you may feel like
387:11 - this is producing a different kind of plot.
387:15 - But the model is in its current state already had a loss
387:20 - or was around, had a loss of about 30,000.
387:25 - So if we have to recompile our model,
387:30 - so right here, if we have to run this,
387:33 - recompile our model and feet,
387:36 - you get to see that it isn't much different
387:39 - from what we had previously.
387:41 - So this doesn't actually come to better our model
387:45 - or model performance actually,
387:47 - it comes to speed up the training.
387:49 - So you wouldn't expect to have better loss values
387:53 - with a tf.data, but instead you could attain
387:58 - this better performance faster.
388:01 - So let's run this again.
388:03 - And you get to see that it's kind of like similar
388:05 - to what we had already.
388:06 - So that's fine.
388:08 - We run this as well.
388:11 - You could always increase the number of epochs.
388:14 - You'll see that here, that's what we'll get.
388:17 - And so you could predict with this, fine.
388:22 - And now we could go ahead and view this on our bar chart.
388:27 - So we look at this similar to what we had already.
388:30 - So that's fine.
388:32 - We now master the basics of working with a data API.
388:37 - Though in subsequent sections,
388:38 - we'll look at even more interesting ways
388:40 - of working with this API.
388:43 - That's it.
388:43 - Hope you enjoyed this.
388:45 - Thank you for following up to this point.
388:47 - Don't forget to like, subscribe, and share.
388:50 - See you next time.
388:54 - Hello everyone, and welcome to this session.
388:57 - According to the World Health Organization,
389:00 - the estimated number of malaria deaths stood
389:02 - at 409,000 in the year 2019.
389:07 - In this section, we are going to build
389:09 - a machine learning model based
389:12 - on convolutional neural networks
389:14 - to diagnose malaria in a person
389:17 - based on cell information gotten from a microscope.
389:22 - In this section, we'll start with loading this data.
389:26 - After loading our data, we are going to visualize this data,
389:31 - process this data, build a model suited for this data,
389:36 - then train this model,
389:38 - and finally, evaluate and test our model.
389:43 - As usual, we'll start by defining the task,
389:47 - which in this case entails correctly classifying
389:51 - whether an input cell contains the malaria parasite or not.
389:57 - Then we'll go ahead to prepare our data.
390:00 - This data is going to be made available to us
390:03 - from the TensorFlow data sets.
390:05 - Then we'll build a model.
390:08 - The model or the particular model we'll be working with
390:11 - in this section will be the convolutional neural network.
390:16 - Then from here, we'll define the error function.
390:19 - We'll go ahead and train our convolutional neural network.
390:23 - We'll check out on some performance measurement metrics
390:27 - like accuracy, F1's core, precision, recall,
390:31 - and many others.
390:33 - Then we'll do validation and testing.
390:36 - And finally, we'll take as many corrective measures
390:39 - as we can.
390:41 - In essence, what we want to do is to build a model like this,
390:45 - which takes as input this segmented cell
390:49 - from a team blood smear,
390:50 - and say whether this segmented cell right here
390:55 - is parasitized or unparsitized.
390:59 - We're supposing you have no medical background,
391:02 - so we'll briefly look at how malaria diagnosis is related
391:07 - to those segmented blood cells.
391:09 - To start, we get infected by malaria
391:12 - once we are beaten by a mosquito.
391:14 - These mosquito bites usually lead to the passing
391:19 - of Plasmodium pacifarum parasite into our blood system.
391:23 - And so to diagnose whether a particular person
391:27 - has got the malaria or not,
391:30 - it's important to get that person's blood.
391:33 - Here you see how the medical practitioner
391:35 - has to select the finger to puncture,
391:37 - usually the third or fourth finger.
391:39 - Then to obtain this blood,
391:41 - you puncture the side of the ball of the finger.
391:44 - In the case the blood doesn't well up,
391:46 - you would have to generally squeeze the finger
391:49 - so you can obtain the blood.
391:50 - Then always grab the slide by its edge.
391:53 - So to control the size of the blood,
391:55 - drop on the slide, touch the finger to the slide from below.
391:59 - From that cdc.gov website,
392:02 - we'll get to this microbe notes website
392:05 - where we'll get more colored images.
392:08 - Now we've obtained the patient's blood or the person's blood
392:12 - and there are two possibilities.
392:14 - One is getting a thin smear like this
392:17 - and one is getting a thick smear.
392:19 - In our case, our data set is gotten
392:22 - from a thin smear like this.
392:24 - So here we have this thin smear,
392:26 - which when passed on a microscope,
392:29 - produces images like this.
392:31 - So here we have thin blood film
392:34 - and we have thick blood film.
392:36 - From this point, we now segment the cells.
392:40 - The cells are now segmented
392:41 - and that's how we obtain an image like this
392:45 - or a segmented cell image like this,
392:49 - which now could be used by a model
392:52 - to predict whether that patient
392:55 - has got the malaria parasite or not.
392:58 - It should also be noted that we're dealing
393:00 - with a classification problem
393:02 - since our output can only take two discrete values.
393:06 - This type of classification problem
393:09 - is known as binary classification.
393:12 - Since throughout the section,
393:13 - we'll be dealing with image data,
393:16 - it's important we understand how image data is represented.
393:20 - So here we have this image of this bird
393:23 - and then when you zoom in, this is what you get.
393:30 - If you zoom in again, you should have this.
393:32 - So this tells us that this image right here
393:36 - is formed by combining all these little boxes
393:41 - we have right here.
393:43 - And these boxes are known as pixels.
393:47 - That said, here we could localize this pixel.
393:50 - We have this pixel, this one, this one,
393:53 - and so on and so forth.
393:54 - So basically, this image is made of all these little boxes
393:59 - which we call pixels.
394:02 - If now we take this image from our dataset,
394:04 - you could see that, notice from here,
394:07 - we have 86 by 82 PX, 86 by 82 pixel image,
394:14 - meaning that we have 86.
394:16 - Notice how here we go up to 86.
394:19 - So at this point, we have 86 and the width.
394:22 - So we have 86 of these boxes.
394:25 - If you have to go from this point
394:27 - to this other point right here,
394:30 - we would go through 86 of these pixels.
394:33 - And then if you have to go from this point
394:35 - to this one right here, you have 82.
394:38 - Now check out what is displayed at this position.
394:42 - So as we move, you should be able to see what's displayed.
394:45 - You see that at this point, for example,
394:48 - we had position 85, 34.
394:53 - So we've gone 85 steps horizontally
394:56 - and then 34 steps vertically.
394:59 - We're considering our origin to be at this top left corner
395:02 - right here.
395:03 - So we have 85 steps and then 34 steps in this direction.
395:10 - And then you should be able to localize
395:12 - these pixels right here.
395:14 - So you could notice how all those pixels now,
395:18 - when combined, will be able to form this image.
395:21 - To reduce this, you see that it becomes less evident
395:24 - to notice those pixels.
395:25 - So that's it.
395:29 - It should also be noted that each of those pixels
395:32 - contain values ranging from zero to 255.
395:39 - So here we have values ranging between zero and 255.
395:43 - And for each and every pixel,
395:46 - we have three different components,
395:48 - the red, the green, and the blue.
395:52 - So if we break this image here
395:54 - into these three different components,
395:56 - we would have something like this.
395:57 - Now note that these values are actually normalized.
396:01 - So basically what they've done here
396:03 - is they've taken all these values
396:05 - and then they've divided by 255.
396:08 - So if you want to get the un-normalized values,
396:10 - you should be able to take this
396:12 - and multiply by 255 to obtain the original values.
396:17 - That said, notice how at this point here,
396:19 - this position, we have this.
396:22 - At this position, we have this and this.
396:26 - And so we could represent image data
396:29 - in terms of the height, the width,
396:34 - and the number of channels,
396:36 - which is in this case equal to three.
396:39 - So we have height,
396:41 - that is the shape of our image tensor,
396:43 - height by width by three.
396:47 - And then another common format is the grayscale format.
396:51 - With the grayscale format,
396:52 - we, this number of channels equal one.
396:55 - We have just one channel,
396:57 - and it could be represented as a 2D tensor.
397:01 - Another interesting point to notice,
397:04 - though we've said for each position or for each pixel,
397:08 - we have a given value per each per component.
397:12 - We have to note that all these values
397:16 - fall between black and white.
397:21 - That is for black, if you want to get a black value,
397:25 - that the pixel value will be zero.
397:27 - And then for white, the pixel value will be 255.
397:33 - Obviously, when we normalize these values,
397:35 - here we're going from zero up to one.
397:39 - So that's it.
397:43 - In this data section,
397:44 - we'll make use of a dataset contained
397:47 - in the TensorFlow datasets module right here.
397:51 - So here, you just have to pick your problem
397:54 - and you will have a whole lot of datasets available to you.
397:59 - In our case, we're dealing with image classification.
398:02 - So we select this,
398:03 - we scroll down and we should get malaria.
398:07 - So here we have malaria.
398:09 - So we double click on that and here we go.
398:12 - We have our malaria dataset.
398:14 - Now you could visualize your dataset
398:16 - and you could get some information
398:18 - of all description about your dataset.
398:20 - The malaria dataset contains a total of 27,558 cell images
398:25 - with equal instances of parasitized and uninfected cells
398:29 - from the teen blood smears like images of segmented cells.
398:34 - So here we are going to explore data.
398:39 - You could come right here, draw the border.
398:41 - So that's why you have this border and then group by level.
398:45 - Notice how we have the parasitized level
398:48 - and the uninfected level
398:50 - and both have equal number of images.
398:53 - So in fact, what the scientists have done
398:56 - to get at this dataset is
398:58 - they have gotten this segmented blood cells
399:02 - from tested malaria patients
399:05 - and this cells right here from tested non-malaria patients.
399:10 - Let's click on this.
399:12 - You see, we have our cells.
399:14 - There we go.
399:16 - You could always visualize all the cells here
399:19 - and then let's get more information
399:21 - about the image content metadata aspect ratios.
399:25 - You see, there we go.
399:26 - We have the format, this is PNG, mega pixel resolution,
399:32 - mode, RGB mode.
399:35 - We're going to look at this shortly.
399:37 - Pixel height, 112, pixel width, 91.
399:42 - Also note how not all the cells
399:45 - have exactly the same pixel height and pixel width.
399:48 - So they all have different pixel heights and pixel width.
399:53 - We have other information about a dataset
399:55 - like the homepage, source code, versions, download size,
399:59 - dataset size, autocached, splits.
400:03 - In this case, we just have a train split.
400:04 - So it's left on us to split the dataset
400:08 - into train, validation and testing sets.
400:12 - We have the features here.
400:14 - We see we have an input image
400:15 - and we have the output level.
400:18 - Note how the number of classes here equal two.
400:20 - Now we could look at some examples.
400:22 - And finally, we have the authors.
400:26 - In order to load this data so we could make use of it,
400:29 - we're going to make use of tensorflow.dataset.load method
400:33 - right here.
400:34 - You see that this method takes several arguments
400:37 - and this here have default values,
400:39 - but we need to specify the string.
400:41 - So we need to specify the name,
400:43 - which in this case is malaria.
400:45 - This also has some outputs.
400:48 - So if you get this or if you succeed to load
400:52 - the dataset, then you should be able to get this output.
400:55 - Notice now how this integrates
400:58 - with a tensorflow data API,
401:00 - which we talked of in the previous section
401:02 - and which is an efficient way
401:05 - of dealing with tensorflow datasets.
401:07 - So here, all you need to do is to load the data
401:11 - and you should have your tensorflow dataset
401:14 - with the dataset information.
401:17 - Let's now dive into the code.
401:19 - We start by importing tensorflow, numpy, map.lib,
401:23 - and we shall import tensorflow datasets as TFDS.
401:30 - So we import out tensorflow datasets, that's fine.
401:33 - And we could go straight away into loading this dataset.
401:36 - Here we have tensorflow.load
401:38 - and we specify the name malaria.
401:41 - So once you specify this, you should obtain your dataset.
401:45 - Recall we have our dataset, let's say dataset,
401:49 - and we have dataset info.
401:52 - So that's it.
401:55 - Let's run this.
401:58 - You see, we downloading this dataset.
402:01 - After loading the dataset, we obtain an error.
402:04 - So let's go ahead and check in the documentation.
402:07 - By default, this with info information right here is false.
402:11 - So trying to get this dataset information right here,
402:16 - we'll try an error.
402:18 - Let's now add this with info, right here with info.
402:23 - We set that to true and we run again.
402:27 - So you see that now all is well, we have our dataset.
402:30 - We have our dataset information.
402:32 - Wish we could check out right here.
402:34 - So we have dataset and then we have dataset info.
402:39 - That's it.
402:40 - Clearly we see prefetched dataset.
402:46 - We have the image and we have the level.
402:49 - We have for data dataset,
402:54 - we are going to print out our data.
402:58 - And then after this, we just take a break.
403:01 - We could also do for data in dataset.take.
403:04 - So just to take one element of our dataset
403:06 - instead of doing this, let's take this off.
403:09 - So let's run that now.
403:12 - We have an error, we told the object has no attribute take
403:15 - and that's because let's run the dataset again.
403:20 - We run that and you see,
403:22 - we actually have this dictionary made up the train,
403:25 - which is our dataset and then we have the types.
403:28 - So let's specify we want to get just the train right here.
403:33 - We have dataset train and we will run that again.
403:37 - So now we have our data.
403:39 - You see, we have this 103 by 103 by three.
403:42 - That's our image.
403:44 - And then if we scroll down, you see we have the level.
403:49 - So we see it's one.
403:51 - Let's scroll back and let's say we take two values.
403:54 - So let's take even four values and see.
403:57 - So we have those four values.
403:59 - You see, let's scroll down.
404:01 - You see here we have one.
404:04 - Scrolling, we have this other one, zero,
404:08 - different levels, different images.
404:11 - And here we have one and here we have one.
404:16 - So that's it.
404:17 - We see how we obtain the images
404:19 - and the corresponding levels.
404:22 - We now pass in more arguments like the shuffle files
404:27 - as supervised and the split.
404:31 - Here we have more elaborate explanation
404:33 - of how we can work with a split.
404:35 - You see, we want to split our dataset
404:37 - and so say train and test.
404:39 - You could do this, you could simply pass this
404:42 - and all you could specify the percentage
404:45 - of the training set on the whole dataset
404:48 - and could specify the percentage of the test set.
404:52 - So there we go.
404:53 - We simply add as supervised.
404:57 - There we go.
404:58 - True shuffle files.
405:02 - There we go.
405:03 - True.
405:05 - And then finally we specify the split.
405:08 - So we have our split.
405:10 - That's it.
405:11 - We are going to pass this out.
405:12 - We have the train test.
405:14 - So we could just simply do train test.
405:15 - Let's take this off.
405:17 - So for now we have the split, train test
405:20 - and let's run this.
405:24 - We obtain this error telling us
405:25 - that there's an unknown split test
405:28 - and that the splits we have here
405:32 - should be part of this list train.
405:34 - Now, actually in other datasets available
405:37 - in TensorFlow datasets,
405:38 - we already have the train test split.
405:41 - And in those cases, we could make use
405:43 - of this kind of split.
405:45 - Now we only have the train.
405:47 - So here it's needless even putting the split
405:50 - or you could just, let's just take this off.
405:52 - Let's take this off now.
405:54 - And you'll see this should work fine.
405:56 - So that's it.
405:57 - So we have our training set.
405:59 - We'll split this separately.
406:02 - And since we've shuffled the files,
406:04 - now if we run this,
406:05 - we'll notice how the shape is going to change
406:07 - because our files are now shuffled.
406:10 - So let's go up, scroll this up.
406:14 - There we go.
406:16 - And you see, we have different files.
406:18 - Whereas when we don't shuffle
406:19 - or when we set the shuffling to false,
406:24 - you will always have the same image
406:25 - at the same position.
406:27 - So that's it.
406:28 - Info specified and that's fine.
406:31 - In order for us to create this year,
406:35 - that's to create the train validation and test sets.
406:39 - We are going to make use of the take method
406:42 - and the skip method.
406:45 - So let's take this example.
406:47 - We have this little dataset, we'll create it.
406:49 - And we want to print out the dataset
406:52 - after skipping seven values.
406:54 - So here, you see, if we skip seven values, we have this.
406:59 - Now, if we take six values,
407:01 - you see we have those first, rather your take.
407:06 - So let's put your take.
407:07 - We have those first values.
407:10 - So let's do this.
407:11 - At every level, we print out dataset.s non-py iterator.
407:18 - So we print out a dataset.
407:20 - We have dataset take, we take the first values.
407:23 - Let's define this percentage.
407:26 - All the ratio, we have the train ratio
407:30 - is equals say 0.8.
407:32 - Val ratio equals 0.2, no 0.1.
407:37 - And the test ratio equals 0.1.
407:41 - There we go.
407:42 - We have the train ratio, validation ratio,
407:44 - and test ratio.
407:46 - Now we are going to pick this up.
407:48 - So we have the length of our dataset.
407:51 - We could, let's comment this part
407:53 - and then print out this length.
407:56 - There we go.
407:57 - We have length of dataset.
408:01 - And as you could see, we have 10 elements.
408:05 - So when we run this, you have this element here
408:09 - and our length equals 10.
408:11 - Now that's set.
408:12 - To obtain the train dataset,
408:14 - all we need to do is simply take the first 80%
408:19 - of our dataset.
408:21 - So here we're going to have this, let's say train dataset.
408:26 - And then we take 0.8 times the length of,
408:31 - let's define this.
408:32 - Let's say we have length, all dataset size.
408:37 - Dataset size.
408:40 - There we go.
408:41 - So we have defined our dataset size.
408:42 - We have that, dataset size.
408:46 - That's fine.
408:49 - Now that we have this, we print out the string dataset.
408:53 - So you could see here how we are not able to convert this.
408:57 - So let's, cannot convert 8 to eager tensor of type int64.
409:06 - Right here, we're supposed to pass in an int.
409:08 - So let's have that.
409:11 - There we go.
409:13 - Okay, so we see that we have the first eight elements
409:17 - right here.
409:17 - We have this, this, this, eight.
409:20 - Now we have gotten this 80%.
409:22 - Let's take this, let's say we take this to 60.
409:26 - So let's reduce that.
409:27 - And you have this 60% right here.
409:32 - Let's modify this here.
409:35 - So let's now have the train ratio.
409:37 - So here we have train ratio.
409:39 - We run that.
409:40 - And there we go.
409:41 - We have the top 60% and if we get back to 80,
409:44 - we have top 80% or rather the first 80%.
409:49 - Now we have the first 60%.
409:51 - So we have this first six elements right here.
409:53 - And we're left with this other four elements.
409:57 - Now let's go ahead and see how we could get this
409:59 - out of four elements.
410:00 - To obtain this, all we need to do is to use this keep.
410:03 - So we're going to have that and define our validation.
410:07 - So we have your vowel.
410:09 - Now, instead of taking the first six elements,
410:12 - we're going to skip this first six elements.
410:14 - So we could start getting this other elements right here.
410:17 - So here we have skip.
410:19 - And then we specify that we're skipping
410:23 - this first six elements.
410:24 - So here we're going to have six.
410:27 - So that's it.
410:28 - And we print this out.
410:29 - Let's have this vowel right here.
410:32 - So that's it.
410:32 - We have our validation.
410:33 - You see, we get this first six elements.
410:36 - But recall that our validation is in fact this two elements
410:40 - right here.
410:41 - So what we're going to do again is after skipping
410:44 - and getting this last elements,
410:48 - which essentially is made of the validation and training set,
410:52 - we are now going to take out,
410:55 - take the first two elements
410:57 - which correspond to the validation set.
411:00 - So right here we have vowel data set is now equal.
411:05 - Notice how instead of data set as previously,
411:08 - now we're dealing with a vowel data set.
411:09 - So we have vowel data set dot take.
411:13 - So here we have a vowel data set of take.
411:16 - What are we taking?
411:17 - We're taking this first two elements,
411:20 - which could be obtained by having this int of vowel.
411:25 - So where we have vowel ratio times our data set size.
411:33 - So times our data set size,
411:35 - we have the vowel data set.
411:36 - And now let's run this.
411:39 - There we go.
411:40 - You see, we have now six, seven.
411:41 - Now in order for us to get this next,
411:44 - all we need to do is to skip.
411:47 - So we've gotten this as validation data set.
411:50 - Let's call this vowel test, actually.
411:53 - Let's say this is our vowel test
411:55 - and here we have our vowel test.
411:57 - So we get the vowel test, which is this.
412:00 - So up in the validation, we take this first two
412:02 - and then to obtain the test, we skip this first two.
412:06 - So right here, we are gonna copy this again
412:10 - and then paste this out.
412:11 - We have the test, which takes the vowel test
412:15 - and then takes the vowel test.
412:17 - But the difference is, yeah, we have the skip.
412:19 - So we run this and there we go.
412:24 - It's not exactly what we expect.
412:25 - We should change this.
412:26 - Yeah, we have test.
412:28 - So that's it.
412:29 - Now we've gotten all the, we've gotten the train set.
412:31 - We've gotten the validation set
412:33 - and we've gotten the test set.
412:35 - We could play around with these values.
412:37 - Let's suppose we have no validation.
412:39 - So we have that, you see that we have this empty list
412:42 - and then all this is used for the test.
412:45 - Now we could create a function from there.
412:48 - We'll call this function the splits function.
412:51 - So we have our splits function.
412:54 - What it takes in is the ratios.
412:56 - So we have the train, the train ratio,
413:00 - the validation ratio.
413:03 - We have the test ratio.
413:05 - That's it.
413:07 - And what does it do?
413:08 - Simply does exactly what we've just done right here.
413:11 - So let's copy this out.
413:12 - Let's cut down and then have it here.
413:15 - Yeah, we're taking the data set too.
413:17 - So we have our data set and that's it.
413:20 - Now we need to define our data set again.
413:22 - So we just have this data set and let's take this off.
413:28 - So we cut that off.
413:30 - Let's put this here.
413:31 - And we have our data set size from the data set.
413:33 - We define a train data set, no quinting again.
413:37 - We have the validation.
413:39 - We have that, take this off.
413:41 - We have the test and then we go ahead and return.
413:43 - So we return our train data set,
413:48 - our validation data set and our test data set.
413:53 - So that's it.
413:54 - So we've returned all this and let's send this up.
413:59 - So we have this method right here we've built.
414:03 - And then from this, we have our data set.
414:08 - We're going to pass this in here.
414:09 - So let's say we want to have vow.
414:13 - So we have vow data set.
414:14 - All right, a train data set.
414:18 - We have vow data set.
414:21 - We have test data set.
414:24 - And this equals split of data set
414:27 - and train, train ratio,
414:32 - vowel ratio and test ratio.
414:36 - So that's it.
414:37 - So we pass all those in
414:39 - and then now we could from here print out.
414:44 - So here we could print out our train data set,
414:49 - for example.
414:50 - So we have this train data set.
414:53 - We could print out our vowel data set.
414:55 - And then we could print out our test data set.
414:58 - So we get this right.
415:00 - That's it.
415:01 - We run it and we told split is not defined.
415:06 - This actually splits.
415:07 - So let's have that S run again.
415:10 - And that's fine.
415:11 - There we go.
415:12 - We have our three data sets.
415:14 - So we have our train, we have our validation
415:16 - and then we have our test data set.
415:19 - In here, we could always have this.
415:22 - So we could see a list as NumPy iterator.
415:27 - So yeah, we have this.
415:30 - There's a list.
415:31 - So we have that and that's it.
415:33 - So there we go.
415:34 - We have the train.
415:35 - We could do the same for the validation and the testing.
415:38 - This is where we now obtain.
415:40 - We can now test out the splitting with our own data set.
415:44 - So we'll comment this part and go with a data set.
415:49 - We will load this, here we go.
415:53 - We've loaded that.
415:55 - And then we shall pass this data set now in here.
416:00 - So we should have this.
416:03 - Let's rerun this, run this and that's fine.
416:06 - So now we have this error list object has no attribute take.
416:10 - So better understand this error.
416:13 - Let's run the cell right here.
416:15 - You see this data set is actually made up of a list.
416:18 - This list is made up of the data set and the types.
416:23 - So when we doing this, we actually taking the data,
416:27 - we're taking all this list.take.
416:30 - So what we do is we should pick out just the data set.
416:33 - And to do that, all we need to do is specify here
416:37 - that we are picking out just the data set.
416:39 - So that said, we have picked out a data set.
416:41 - We could now run this.
416:43 - Before running this, let's make sure we take out just
416:45 - an element because running that full data set
416:48 - is going to be very time consuming.
416:50 - So let's take this one and pick this out right here.
416:55 - So that's fine.
416:56 - Now we will run this and wait a little.
416:59 - Now, as you could see, you have this right here.
417:03 - That's the image and the label given to us.
417:08 - Notice how we have this empty list because we've set here
417:12 - that we want our validation ratio to be equal to zero.
417:14 - So since we have this at zero,
417:16 - we have the empty list for the validation.
417:19 - So our validation is actually empty.
417:21 - Now you could modify this, let's take 0.1, 0.1
417:26 - and right here is 0.8, we run that.
417:29 - And here's the response we get.
417:31 - So that's fine.
417:33 - We have all the, we have the validation training test set,
417:38 - which are known empty lists.
417:40 - We'll now get into visualizing our data set,
417:47 - visualizing some elements in our data set.
417:49 - So we have for I image.
417:53 - So here we're picking out the image and the level
417:55 - and image rates.
417:57 - And we have the train data set.
418:00 - You could as well pick out the validation
418:03 - or the testing data set.
418:04 - So we have that.
418:05 - Let's take say 16 elements.
418:07 - And then for all this, we'll come up with subplots.
418:13 - And here we pick out four by four
418:15 - because we are having to plot out 16 different images.
418:22 - At this point, we do plots.imshow of our image.
418:28 - So that's our image we're plotting out
418:30 - and then we'll run this.
418:33 - Here's what we get.
418:34 - This is actually what we expect.
418:36 - And now what we could do is by the side of every image,
418:40 - we could put out a level.
418:42 - For each and every image, we'll put out a title.
418:44 - We have the title and what we'll do is we have data set info
418:49 - because you're interested in getting
418:50 - the corresponding level name.
418:52 - So we have say a level zero, want to get his name
418:56 - and we could have a level one,
418:58 - want to get his particular name.
419:00 - Here we have this and we have the features.
419:04 - To understand this better,
419:05 - you could check out this data set info right here.
419:08 - You notice that we have this features
419:10 - and then we'll pick out the level.
419:14 - That said, we are just gonna go straight away into this.
419:16 - We have level and then we convert this into a string.
419:21 - So we have the string and then we pass on the level.
419:24 - Now we pass it on this level we've got right here
419:26 - for the corresponding image.
419:29 - And then once we have this right, we run it
419:32 - and we take this off.
419:35 - So that should be fine now.
419:36 - We have this images and the corresponding levels.
419:41 - You can see how this is pasteurized.
419:44 - This, for example, is uninfected,
419:46 - but this isn't very clear.
419:48 - So let's take off this, put that axis.
419:53 - We set this to off, run that.
419:57 - And that looks better, but we could make this better.
420:01 - If you wanna know which of these is the level zero
420:05 - and which is the level one, let's take this from here.
420:08 - And then we print out to zero.
420:11 - You see that this level zero,
420:12 - it's given to us as pasteurized and one is uninfected.
420:18 - So that's it.
420:22 - At this point, we're gonna dive into data processing.
420:25 - Our data processing unit from now
420:27 - will be made of two parts.
420:29 - The first part will be the resizing part.
420:31 - So if we have an input image of say 102 by 102
420:40 - as width and height, we could have this.
420:43 - We're gonna transform this into an image
420:47 - of a fixed width and fixed height.
420:50 - Now that said, all our images,
420:53 - irrespective of their width and heights,
420:56 - will now have just one width and one height.
421:00 - In this case, we'll consider an image size of 224.
421:04 - So our width and height will be converted to 224.
421:08 - And subsequently, we'll see why
421:10 - we are picking this image size right here.
421:16 - After this resizing, our next part will be on normalization.
421:22 - So we'll have an input image which will normalize
421:25 - such that all the data falls in a given range.
421:31 - Here we have the standardization process
421:34 - and we have the normalization process.
421:38 - In this case, we are gonna use a normalization process
421:42 - and we're gonna explain why.
421:44 - In the previous example on the car price prediction,
421:47 - we actually work with standardization.
421:50 - In standardization, each value is subtracted
421:54 - from the mean and then divided by the standard deviation.
421:58 - That's actually each value on each and every column
422:03 - of X right here is being standardized
422:08 - based on its mean and the standard deviation.
422:11 - As you'll notice, these values, for example,
422:16 - are normally distributed.
422:18 - That is, we have a mean value on average value
422:21 - and we have a standard deviation or range of values
422:25 - where most of times our values will fall in.
422:29 - So it's gonna look bell shaped like this
422:31 - as we've seen before.
422:33 - We have the mean and then we have a certain
422:36 - standard deviation right here.
422:39 - And that's why if you look at X2,
422:41 - you wouldn't have say 5,600, 7,100
422:45 - and then here, for example, come and have a value
422:48 - like say 12, this isn't very typical since
422:53 - most of the times these values fall under a given range
422:56 - and there's a certain average value
422:59 - for which most other values just fall around.
423:04 - So that's why it's typical to have these kinds of values.
423:08 - Let's take this off.
423:10 - For the next, you'll notice how these values fall
423:13 - under a given kind of range and with this too.
423:17 - Now, this is for when we deal with standardization
423:20 - and that's why previously we used this actually.
423:24 - Now, in the case of image data,
423:26 - the choice of whether to standardize or to normalize
423:30 - will depend on the kind of data we're dealing with.
423:33 - So if we have images where most of its pixels
423:38 - revolve around a particular mean value,
423:42 - then we'll wanna standardize
423:44 - and if this image is made of pixels
423:48 - where their values are mostly different from one another,
423:52 - then we would want to normalize.
423:56 - Now, in our case, as we continue in this section,
424:00 - we are gonna go with normalization,
424:02 - that is we will have X minus X mean, which is zero,
424:06 - divided by X max, which is 255,
424:11 - minus X mean, which is zero.
424:13 - So simply do X divided by 255.
424:17 - So we'll normalize the inputs
424:19 - before passing them into our model.
424:22 - Now, it should be noted that for some other datasets like,
424:25 - for example, ImageNet or ImageNet,
424:29 - we have ImageNet dataset,
424:31 - or the RDE20K image segmentation dataset,
424:36 - they have their known mean and standard deviation values.
424:43 - Nonetheless, for whatever problem you have to deal with,
424:47 - you may work with standardization or normalization,
424:52 - experiment for yourself and see which one works better.
424:57 - Now that said, since we're dealing with a TensorFlow data API,
425:01 - we are gonna use this map method
425:03 - to help us in this pre-processing.
425:07 - So there we go, we have trained dataset.
425:09 - We'll start first with the resizing.
425:11 - Trained dataset equals trained dataset.map,
425:15 - and then we'll call this resizing method,
425:17 - which we shall define.
425:19 - Let's add this and put this up here.
425:22 - So we have our resizing method.
425:24 - This method takes in the image and the level.
425:29 - So we have our image and the level.
425:32 - And the level right here.
425:35 - But note that we're doing processing on only the image.
425:38 - So we're gonna pass this in,
425:39 - and then we're gonna resize this using the resize method,
425:44 - which comes with TensorFlow images right here.
425:47 - So we have TensorFlow image,
425:49 - and then we have this resize method.
425:52 - Here you could see the arguments which are passed.
425:54 - For now we'll focus on the image and the size.
425:58 - We will also note that you could change the method
426:00 - using resizing.
426:02 - But for now we use this default values given to us.
426:07 - So that's it.
426:08 - We are gonna return the image.
426:12 - So let's have our TensorFlow image dot resize,
426:18 - and it takes in our image.
426:20 - And then we have the image size.
426:24 - So we'll have in size by in size.
426:27 - Note that we are gonna define the in size here.
426:30 - So we'll have in size equal to 124.
426:35 - So we have this resizing.
426:37 - Our image is returned to this,
426:39 - 224 by 24, 24 of a shaped image.
426:43 - Now we have resized this.
426:45 - We just put out a level.
426:47 - So here again, we just basically taking the image,
426:51 - resizing it, and then the level remains the same.
426:55 - So we could run this,
426:56 - and then we have our trained data set,
426:58 - which now has been resized.
427:00 - That's it.
427:01 - We could say for data in train data set,
427:07 - we're gonna print out, oh, let's take one value.
427:09 - So yeah, we take just one value,
427:12 - and we have our, let's say image,
427:15 - actually image and the level.
427:20 - We print out, we print out the image and the level.
427:25 - Let's run that, and there we go.
427:27 - You'll notice that the shape of this image
427:29 - is gonna be 224 by 224,
427:32 - irrespective of whichever, or which image we pick,
427:36 - and then we have its level, which remains unchanged.
427:40 - Notice how this is a plot 32.
427:43 - Unlike previously where we had an unsigned int.
427:48 - Now you could always do casting right here
427:50 - to modify this depending on what you're working on.
427:55 - So that said, we're done with the resizing.
427:58 - We could now look at reskilling.
428:01 - So let's put this as resizing and reskilling.
428:06 - So we have resizing, resize, let's just say resize, reskill.
428:11 - So our resizing reskill function is such that
428:13 - after resizing, we reskill by dividing by 255.
428:19 - So we divide all our values by 255,
428:22 - our level remains the same.
428:23 - We run this, we run that, here is resize, reskill.
428:29 - There we go, we run that, and then we have this.
428:33 - So that's fine.
428:34 - Just as we did before, that was in the previous section,
428:37 - we're gonna shuffle our data,
428:39 - we're gonna put this in batches,
428:40 - and then we'll do prefetching.
428:43 - So all this was explained in the previous section.
428:54 - We are now ready to build our model,
428:56 - and up to now neural networks
428:59 - have been performing quite well.
429:01 - We had seen previously that if we have three neurons,
429:05 - that's one, two, three in the input,
429:08 - and in the output we have three neurons too,
429:13 - then there'll be nine different connections here,
429:17 - and hence nine different weights plus the bias.
429:21 - But for this example, let's consider only the weights.
429:24 - So we have nine different parameters
429:26 - for three inputs, three outputs.
429:30 - And if here we have five, considering only the weights,
429:33 - we would have three times five,
429:35 - that's 15 different parameters.
429:39 - Now, if we're dealing with an image like this one,
429:42 - so let's consider this image,
429:44 - which is 224 by 224 by three image,
429:52 - three for the three different red, green, and blue channels.
429:58 - So we have this input image now.
430:00 - So unlike previous examples,
430:03 - where we would have features,
430:06 - or specifically in this case input features,
430:09 - we do not contain as many elements as this.
430:13 - We now have this case where if we want to count
430:16 - the number of features or the number of pixels,
430:19 - we would have 150,528 different input features
430:26 - to take into consideration.
430:28 - Hence, instead of having a total number of three right here,
430:33 - we are going to have 150,528 different values.
430:40 - And if we want to do the same competition
430:42 - which got us this number of parameters,
430:44 - we would have 150,000 times three,
430:48 - that's approximately 450,000 different parameters.
430:54 - Now, what if we modify this number of neurons right here
430:58 - and take say 1,000 neurons in this output right here?
431:03 - We would see that we'll move from 150,000
431:06 - or rather from 450,000 to 150 million different parameters
431:14 - where each and every parameter
431:16 - has to be trained and optimized.
431:19 - This becomes clear that deep neural networks like this
431:23 - are better still than layers of fully connected layers
431:29 - aren't scalable.
431:32 - Since when we increase the number of features,
431:35 - the total number of parameters also increase considerably.
431:38 - Hence, we need to build a type of layer
431:41 - unlike this one where each neuron isn't connected
431:47 - to all the previous neurons.
431:49 - And this layer happens to be the convolutional layer.
431:55 - In order to better visualize this,
431:57 - we'll use this demo platform from Wrightson University.
432:01 - So here we'll put in a figure, let's say four,
432:05 - and then we'll see exactly how this confnet work.
432:09 - So we have this and then we have this input right here.
432:14 - So we get this input, we have some weights
432:18 - and then this output features.
432:20 - Note how to obtain this particular pixel right here,
432:26 - only a few of these inputs are used.
432:32 - And so yeah, we call this the receptive field.
432:36 - As you could see, we have the receptive field right here.
432:40 - And if we take this other example,
432:42 - you'll see it's on receptive field.
432:44 - You see if to get this value,
432:45 - only these four values you see below, actually this one,
432:50 - this, this and this, as you could see here, let's scroll back.
432:54 - Okay, so only those four values have a role to play
432:59 - when it comes to giving us the corresponding value
433:03 - we have here.
433:04 - And so unlike with a dense layer,
433:06 - where to obtain this value,
433:08 - we needed to link this to each and every previous neuron.
433:12 - Now only just some neurons in this neurons receptive field
433:19 - play a role in getting this value.
433:22 - Another great tool for better understanding
433:24 - the convolutional layer is this CNN explainer.
433:29 - Actually CNN stands for convolutional neural networks.
433:32 - This is created by J. Wang, Robert, Omar,
433:36 - Park, Das, Frank, Kang and Polo.
433:40 - We are really grateful for this tool.
433:42 - Before getting back to the explainer,
433:44 - let's take this example, right?
433:46 - Here we have a four by four image.
433:48 - So we have 16 different pixels.
433:51 - If we flatten out those pixels,
433:52 - that's if we put those pixels out like this,
433:55 - such that we have those inputs.
433:57 - And then in the output, we have four different neurons,
434:01 - then here would have 16 by four connections
434:05 - that will have 64 different parameters,
434:08 - excluding the bias, but with the conv layer,
434:13 - we could leave from this four by four to just a two by two.
434:20 - So we could leave from this four by four to this two by two
434:25 - with just nine parameters.
434:28 - So we would have what we call a kernel right here or filter.
434:34 - So we have this filter, which is three by three,
434:37 - which actually corresponds to our weights,
434:39 - which we've seen already.
434:41 - This kernel here, kernel size three,
434:45 - will produce this output of two by two,
434:49 - which when we flatten out can give us this output right here.
434:54 - And so instead of working with 64 parameters,
434:56 - we're working with nine parameters.
434:59 - If we want to replicate that same example
435:01 - in the CNN explainer, here's what we get.
435:04 - We have this input right here,
435:06 - which produces this output.
435:08 - Now, notice how we specify that a kernel size equal to three.
435:13 - And because a kernel size equal to three,
435:15 - we are able to get this output.
435:17 - But how is this output gotten?
435:19 - You would see that at this top left corner,
435:23 - we are going to feed our kernel,
435:25 - which is of size three by three.
435:27 - So we put in our kernel right here,
435:29 - and then we take each and every value of our kernel
435:34 - and multiply it with a corresponding value in the input.
435:38 - To obtain the first value,
435:40 - you see how we pass this kernel on the input.
435:43 - So at this top left position, we pass this kernel.
435:46 - Notice how we have a three by three kernel,
435:50 - which is passed on this input.
435:51 - And we have the output right here.
435:53 - Then to get the next position, to get this next output,
435:57 - you see how this kernel is passed
436:00 - on this next part of the input.
436:04 - And the way we get this next part
436:06 - is by simply sliding through the image.
436:09 - You see how we're left from this,
436:10 - we slide that through the image and we got this,
436:13 - we got this next output.
436:14 - And since we've gotten to the end,
436:16 - we move to the next position, which is this,
436:19 - we get this next value right here.
436:24 - And then from here, we slide again to this end,
436:26 - and we finally get this value.
436:28 - So that's how we get all this outputs
436:31 - from the inputs and the kernel or the filter.
436:36 - We can now go ahead and increase this input size.
436:38 - Let's take, for example, seven.
436:40 - You see, we have a input size of seven,
436:43 - seven by seven input image.
436:45 - And then we have this output five by five.
436:48 - The reason why we have this five by five
436:50 - is because we have this kernel size of three.
436:52 - If we get to increase the kernel size of five,
436:54 - you see our output reduces.
436:56 - You see, when the kernel size gets to six,
436:58 - our output reduces.
437:00 - Let's take this back to three.
437:02 - So you take back to three,
437:04 - and this input of seven by seven
437:06 - gives an output of five by five.
437:08 - We see how when we get to this, we have that output,
437:11 - move, slide, slide, slide, slide,
437:15 - move to this next here, slide, slide, slide,
437:18 - and so on and so forth, right up to the end.
437:22 - So from here, we see how the size of the receptive field
437:26 - of each of these outputs equal to three,
437:29 - which is actually our kernel size.
437:32 - The next thing to notice is reducing the kernel size
437:37 - permits us extract more features from the inputs.
437:42 - You would see that since we have this input seven by seven,
437:46 - with kernel size of three, we have the output five by five.
437:49 - So we've extracted much more features from this input
437:52 - as compared to when we push a kernel size of six.
437:55 - Here we extract less features from this inputs.
438:00 - Now, though using a smaller kernel size
438:04 - permits us extract much more complex information
438:09 - or complex features from the inputs,
438:12 - working with larger kernels
438:14 - permit us extract larger input features.
438:19 - One logical question which may come to your mind is,
438:22 - how do I get the size of this output feature map right here?
438:29 - To get this, we'll use this formula
438:32 - where this output width is equals the input width
438:37 - minus the filter size plus one.
438:40 - If we take this example, we have an input width of seven
438:44 - minus a filter size of three plus one, this gives us five.
438:50 - So that's how we obtain this right here.
438:52 - Now, if we take, say, kernel size,
438:56 - let's take the kernel size to be four, for example.
439:00 - In that case, we have seven minus the kernel size four,
439:08 - which is three plus one, which gives us an output of four.
439:11 - And that's how we obtain this output right here.
439:14 - Now, in a case where you're designing a convolutional layer
439:18 - and you want to get a particular feature map size,
439:22 - say, for example, in this case,
439:24 - if you want to get an output size of three,
439:29 - then all you need to do is specify this kernel size
439:33 - to be equal to five and you should get this output.
439:36 - Now, in some cases,
439:38 - you may want to have a particular feature map shape
439:42 - that using just the kernel size,
439:46 - you wouldn't be able to get that.
439:48 - And so to match up with a particular output shape
439:52 - would include a pattern.
439:55 - Let's look at this pattern.
439:56 - You see, we have a seven by seven.
439:58 - And when we take this to one, you see that we go from
440:02 - seven by seven to now nine by nine.
440:05 - Notice how it's written here.
440:06 - After pattern, we have nine by nine.
440:09 - So we'll leave from this seven by seven.
440:11 - Let's take this in this box right here, this internal box.
440:14 - And then the pattern is also nine by nine.
440:17 - Notice how after doing the pattern,
440:19 - and if we pair around with our kernel size,
440:22 - we go from one, one to eight, eight.
440:25 - So we go from one, one to eight, eight.
440:28 - But with zero pattern and the input size of seven,
440:34 - you see, we go two, three, four, and six.
440:37 - So we cannot go up to eight, eight.
440:39 - We're gonna have an output of eight, eight
440:41 - when the pattern is zero.
440:43 - Whereas when we increase the pattern by one here,
440:47 - increase the pattern by one, that's one.
440:50 - You see that we could go from,
440:52 - we could get an output of say, let's put a count to three.
440:58 - We could get an output of seven, of eight,
441:04 - you see, up to one.
441:06 - So that's it.
441:07 - Now, or we understand how this pattern works.
441:11 - Basically, we just have our input
441:14 - and then we add this surrounding elements.
441:19 - Right here, we just add our input image.
441:23 - Now, the pattern generally we use is the zero pattern,
441:27 - though there are other pattern methods,
441:30 - but the zero pattern is one of the most common
441:33 - since it's easy to use
441:34 - and it's computationally less expensive to work with.
441:38 - So that said, here's our pattern.
441:40 - And then we also have another advantage
441:42 - of working with a pattern,
441:44 - which is that of ensuring that the corner pixels
441:49 - have an influence on our output features
441:52 - which are generated.
441:53 - Now let's take this back to zero pattern.
441:56 - We have zero pattern
441:57 - and we have this input image right here.
442:00 - If we're doing an image in which most of the information
442:04 - or most of the relevant information is centered,
442:07 - then there is actually no issue
442:09 - since this photo right here will go through
442:14 - each and every pixel we have
442:17 - where we have this person and the image.
442:19 - Now, if we modify this image such that we have a person's,
442:24 - let's suppose we have a person's face here
442:26 - just at the corner of the image.
442:28 - You'll see that unlike with this image
442:31 - where this person was centered
442:33 - and that our kennel was able,
442:36 - our filter was able to pass through each and every part
442:40 - of this image.
442:41 - With this one, we have a different scenario.
442:46 - We've just modified this
442:47 - so it looks similar to what we had here.
442:49 - Now, what we could do is
442:54 - if we monitor the number of times
442:56 - this filter goes through the person's head,
442:59 - we would see that we'll have here one time
443:03 - because our filter passes here once.
443:06 - The next time after the sliding, we'll have this.
443:10 - So second time, the next time this other sliding
443:13 - will lead to this four times.
443:16 - So at least it passes through the head
443:17 - and then here we have five times.
443:20 - Now we have one, two, three, four times actually.
443:23 - But in this case, since we are on the borders,
443:26 - you'll see we have this one time
443:30 - and that's practically all.
443:31 - So we see that in this example or in this case
443:35 - where we are on the borders,
443:36 - this influences the outputs
443:40 - in a smaller way or exerts less influence
443:45 - on the values we get in this feature maps
443:48 - which are generated.
443:50 - So in an example where this,
443:52 - let's suppose an example where all this wasn't there
443:54 - and that where our information lies most is this,
443:58 - you will find that it would have been better to at least
444:03 - pass through this head region
444:05 - just as we did with this year
444:08 - where we pass through the head region four times
444:10 - and we're able to extract very useful information
444:13 - from this image because our filter
444:16 - goes through this many more times.
444:19 - Now to remedy this situation, we have the pattern.
444:22 - You see that when we increase the pattern,
444:23 - so let's take this to one, we've increased the pattern,
444:26 - let's increase the pattern to,
444:28 - oh, okay, yeah, let's fix to just one.
444:30 - Anyway, we just increase the pattern to one
444:33 - and then we'll retake that example.
444:36 - Now we're taking the example,
444:38 - we'll see that if we maintain our filter size of two,
444:41 - this filter here will at least touch the head
444:45 - although slightly and then the second one does this twice.
444:50 - So in this case, our filter touches the head twice
444:54 - as compared to previously where it touches the head
444:56 - only once and hence this useful information
445:01 - has more influence on the output features
445:05 - which are being generated right here
445:07 - and which is very important as practically
445:10 - that's what we're trying to do.
445:11 - We're trying to extract information from this input
445:14 - and pass to the output.
445:17 - Another hyper parameter which we could look at is a stride.
445:21 - Now note that for now we've looked at this one
445:24 - this and this.
445:25 - So here's what we call the hyper parameters.
445:29 - We have the stride and we'll understand
445:31 - surely how this works.
445:33 - For now we'll deal with a stride of one.
445:35 - Why one?
445:36 - Simply because you just slide in through one step
445:39 - we're going to the next.
445:40 - So you just slide one, one, one and so on and so forth.
445:44 - Now if we move this to two,
445:46 - we start from this position right here, fix that.
445:50 - You start from this position.
445:51 - There's a problem.
445:55 - You get back to one and back to two.
445:58 - You notice that as we go to one this turns blue
446:01 - and then to two it turns red.
446:03 - The reason why it turns red is because
446:05 - there is no valid output or there is no whole number
446:10 - which with a stride of two can produce an output.
446:14 - So in fact what we're going to do
446:16 - is we modify this input right here.
446:18 - So modify this input, this works now.
446:20 - So it's possible for us to leave from this input of six
446:22 - to this output, but with a stride of two.
446:25 - So let's now understand how strides work.
446:27 - We start with this.
446:28 - Notice now how we're going to skip two steps
446:31 - instead of just one step.
446:32 - So as we go, you see we skip two steps.
446:34 - Notice two, two and that's it.
446:39 - So we move to the next.
446:40 - And now even moving downward, you see,
446:42 - it's not like with a stride one
446:43 - where we just had one step, like with a stride one
446:46 - we just did this one step, but with a stride two
446:50 - we're going two steps below.
446:52 - So that's it.
446:53 - Now that said, increasing the size of our stride
446:58 - actually reduces the size of our output
447:03 - and hence reduces the amount of information
447:06 - we extract from the inputs.
447:09 - And so in general, we get better results
447:11 - by working with smaller kernels and smaller stride values
447:15 - since we're able to extract more information
447:18 - from our inputs.
447:20 - Generally the kernel size is used to read
447:23 - the kernel size of three is generally used in practice
447:26 - and a stride of one.
447:28 - So we may decide to use pattern or not.
447:33 - And the new formula when we include the pattern
447:36 - and the stride is given as such.
447:39 - So we have the output equal to input size
447:43 - minus the field of size plus two times the pattern
447:47 - divided by the stride plus one.
447:52 - In this case, if we have an input size of six,
447:55 - so here we have six, we have six minus field of size three
448:00 - plus two times the pattern, also our pattern one
448:04 - so it's plus two, divide that by the stride,
448:08 - stride is one and plus one.
448:11 - There we go, we have an answer of three plus two,
448:14 - which is five and then plus one, which is six.
448:17 - So that's how we obtain this output size right.
448:21 - Also note that one good thing when working with a library
448:24 - like TensorFlow is when you don't know the exact pattern
448:29 - to use such that you have particular output size,
448:34 - you could specify the pattern to be valid.
448:36 - Once you specify the pattern to be valid,
448:38 - TensorFlow automatically calculates the pattern for you
448:42 - such that the output you want matches up.
448:46 - Up to this point, we've been supposing that our input image
448:50 - is two dimensional, that is we have a height and a width.
448:55 - Now what if we use the kinds of images we have in real life,
448:59 - that is 3D images where we have the red channel,
449:05 - the green channel and the blue channel.
449:09 - So if we have this RGB image right here, RGB image,
449:14 - we'll see how we get the output.
449:17 - Now the way this is done is quite straightforward.
449:20 - So what we have is in this case, we include this pattern.
449:23 - So you see the zero pattern, we've included this pattern
449:26 - and then we have our kennel right here.
449:30 - Now notice how we have this kennel
449:32 - and then it goes through this first part
449:36 - or this top left corner.
449:38 - And what we do is we multiply each value right here.
449:42 - We have negative one times zero
449:44 - plus negative one times zero plus negative one times zero
449:48 - and so on and so forth up to plus negative one times zero
449:51 - right here, you have negative one times four.
449:53 - So basically what we're doing here
449:55 - is kind of like a dot product.
449:57 - So we're taking all these elements,
449:59 - multiplying by the corresponding elements
450:00 - and then adding all this up.
450:03 - Once all this added up, we should have 41.
450:06 - So you could take this as a simple exercise
450:07 - and then you should be able to get 41.
450:10 - And then we have this other kennel right here
450:12 - where we repeat the same process.
450:14 - We have this and then we have this kennel, we have this.
450:19 - So here what we do is we have, we take this,
450:22 - we obtain this value, take this obtain this value,
450:25 - this obtain this value and then we add all this up
450:28 - to get this answer 41.
450:30 - Now note that we also have the bias
450:32 - which we've already included.
450:33 - So for now we have this 41.
450:35 - Now we move to the next step.
450:37 - So we slide through next step.
450:39 - Yeah, the stride is equal one actually.
450:42 - From here we do the same process, negative one times zero,
450:45 - this negative one times zero, add it up and all of that.
450:49 - So all that added up, all this added up,
450:51 - all this added up, we have 12.
450:53 - We repeat the same process right up to the end.
450:56 - So that's how we obtain our output right here.
450:59 - You can also check out on this other more visually appealing
451:02 - example in the CNN Explorer website.
451:05 - So here we double click on this
451:07 - and then we see exactly what's going on.
451:12 - We have here, you see how we're forming this outputs
451:16 - by sliding through the kennels over the inputs.
451:20 - So here I click on this, you see a slide through this
451:24 - and you see how all these values are multiplied
451:26 - and then added together to form the output.
451:30 - So that's it.
451:32 - From this point, we move to this explained visually project
451:36 - by Setoza, filtering is of course part of image processing.
451:43 - And since we're dealing with image day to year,
451:45 - it's important for us to better understand how this works.
451:49 - So right here, we'll choose a kernel or filter,
451:53 - or let's pick this sharpen filter right here
451:56 - or this sharpen kennel.
451:58 - Notice how once we pick a kernel,
451:59 - those values change,
452:01 - that's the values of the filters actually change.
452:04 - Let's take this outline, you notice the change.
452:06 - Now let's keep this outline
452:09 - and then we'll notice that every time we,
452:13 - let's hover over this input image,
452:15 - we have the values right here, that's to the right.
452:20 - So for now we have this and then let's notice
452:23 - that here we have this negative one, that's negative one,
452:26 - times at this position, notice at this position,
452:29 - we have the value 255.
452:33 - So 255 times negative one plus 255 times negative one
452:36 - plus unknown value, unknown year,
452:39 - because we are the borders, so there's no value.
452:42 - And then we have 249 times negative one,
452:44 - 255 times eight, 233 times negative one,
452:48 - 255 times negative one,
452:50 - and then all this sums up to give us a given value.
452:54 - Since we have this unknown values,
452:55 - year is unknown, so we have no value there.
452:57 - But if we change this position
452:59 - and then we fit around the I region,
453:01 - you see we have a value of negative 533.
453:04 - So we notice that no matter the image we put in year,
453:09 - we'll always get an output
453:12 - where the outlines are being highlighted.
453:17 - So if you check out this image,
453:20 - which we've just imported of Elon Musk,
453:22 - you see that the output is this image right here,
453:26 - which highlights the edges.
453:29 - And so as it's explained here,
453:32 - the highlight large differences in pixel values,
453:35 - which generally occur at the edges.
453:37 - So around those regions, you see these large differences
453:40 - are being outlined as compared to this zone year
453:43 - where there's no difference.
453:44 - And since there is no difference,
453:45 - we just have this black region.
453:50 - So here again, you see that we have the filter, that's it.
453:54 - This is the exact same value we have right here.
453:56 - We have the filter and then this example shows us
454:01 - how this output is gotten.
454:05 - Now the major difference between what we're doing right here
454:09 - and the convolutional neural networks is that with this,
454:12 - we know this can of values.
454:15 - So we know that we have this metrics negative one,
454:18 - negative one, negative one, negative one, eight,
454:20 - negative one, negative one, negative one, negative one,
454:23 - which is an edge detector.
454:24 - So we know this and then we know the obvious output.
454:29 - But with the convolutional neural network
454:31 - or with convolution layer to be specific,
454:35 - what we do is initially we just initialize these values
454:40 - and we let the model doing training
454:44 - to learn these values automatically.
454:47 - So these values are learned by the model
454:50 - during training automatically.
454:52 - One of the very first convolutional neural networks
454:55 - or convnets was built by Yann LeCun in 1989.
455:02 - And right here we have the structure
455:05 - of this convnet known as the LeNet.
455:08 - This LeNet takes in an image.
455:10 - Here we have a 28 by 28 by one image.
455:15 - Here we just have one channel, black and white image.
455:17 - And then we pass this to a convolutional layer,
455:20 - which we've just seen after passing through
455:23 - convolutional layer, we have the sigmoid.
455:26 - But also note that here we have a five by five kernel.
455:30 - We have a plus two pattern.
455:32 - So if it's zero pattern, we'll add zeros around our input
455:38 - and then add another zero or another group of zeros
455:42 - around our features.
455:43 - Then from here, we have this output 28 by 28 by six.
455:48 - So shortly we'll see exactly how this output is gotten.
455:52 - So we've seen we have the sigmoid activation.
455:55 - From here we have the pooling layer.
455:57 - Now this is a sub-sampling layer
455:59 - and we'll understand how it works.
456:01 - So we have two by two average pooling,
456:03 - kernel and with a stride of two.
456:07 - From this we have another convolutional layer
456:10 - with five by five kernel.
456:12 - Here is no pattern and we have this output.
456:15 - Activation, pooling, flattening, with flattening,
456:18 - all the features have been modified.
456:21 - So we'll leave from this 3D tensor to a 1D tensor.
456:27 - And then we have this dense fully connected layer
456:32 - followed by sigmoid followed by another dense
456:34 - fully connected layer followed by sigmoid.
456:36 - And finally, we have this dense layer
456:39 - with output 10 neurons.
456:42 - And the exact reason why we have this output
456:46 - of 10 classes or 10 neurons since we have 10 classes
456:52 - is because we were predicting whether an input
456:56 - is either a one.
456:57 - So those inputs are images of handwritten digits.
457:03 - So we want to predict whether the handwriting digit
457:06 - is a one or a two or three up to nine.
457:10 - And so here we have this and then we also have a zero
457:14 - so that makes 10.
457:16 - So zero to nine gives us 10 possibilities
457:19 - and that's why we have 10 different classes right here.
457:22 - Now for the AlexNet, it was built to correctly classify
457:28 - whether an input image belongs to one of a thousand classes
457:33 - in the imageNet data set.
457:36 - So here we have this AlexNet with a different architecture
457:40 - as we could see right here.
457:43 - And now we'll go ahead and understand
457:46 - how all those outputs are gotten.
457:50 - And so we'll be rebuilding the Lynette architecture
457:54 - but this time around considering an input 64 by 64 by three.
457:59 - So right here we have the RGB channels.
458:03 - We have R, G and B.
458:08 - If we pass this input now through this convolutional layer
458:14 - we are going to have this output.
458:16 - And how do we get this output?
458:18 - To get this output, we have to take into consideration
458:21 - this parameters which have been given to us
458:23 - for a convolutional layer.
458:25 - That said, the photo size is equal to five.
458:28 - So we have here photo size of five,
458:31 - there is no pattern, stride equal one
458:35 - and then the number of filters equals six.
458:39 - We'll calculate the number of parameters shortly.
458:42 - Anyway, these are the four most important parameters.
458:46 - So here we have this and this is what we call our filter.
458:51 - We have this filter.
458:52 - You see that we have the dot products
458:56 - which are completed to get the outputs as usual.
458:59 - And so we take this, like this R, we have this.
459:04 - Then we follow through with this
459:06 - and then we follow with this.
459:08 - So we take this and then complete the dot products
459:13 - and with the other two.
459:15 - And then from this, we add all this up
459:18 - to obtain each and every value
459:20 - for this very first feature map right here.
459:26 - So to obtain this feature map, we are using this filter.
459:30 - Now, this shows us clearly that when we specify
459:35 - that the number of filters equals six,
459:37 - it doesn't mean we actually have just six of this
459:41 - as you may feel like number of filters equals six
459:46 - means we have six filters stack like this.
459:49 - Here we have five and then six.
459:52 - So we feel like we have just six stack like this
459:55 - which is not actually the case.
459:57 - What happens is we have three, there's these three channels
460:03 - and then each of our filters also have the three channels
460:07 - as you could see.
460:08 - And so what we call a filter here is this three
460:12 - or rather is this five by five by two B
460:17 - kernel right here.
460:19 - Now we've done a competition and we've had for this,
460:21 - we repeat the same process.
460:24 - So we take this for the R, we repeat this, we repeat this
460:29 - and then we obtain this next.
460:31 - We do all this the same way up to this position
460:34 - and then we have this right here.
460:36 - And that's why when we have six filters,
460:38 - we always gonna have six channels.
460:43 - Then if you do five by five by three and all that times six.
460:47 - So if you have five by five times three, that's 75.
460:50 - 75 times six should give you a total of 450.
460:59 - And since for each of those filters, we add the bias.
461:03 - That is we have an extra bias for each of these filters.
461:07 - We have one, two, three, four, five, six biases.
461:10 - So adding this plus six, we have 456 parameters to be trained.
461:15 - So here we have 450 weights and six biases.
461:23 - That said, we understood why we have the six channels.
461:26 - Now, how do we obtain the 60 by 60?
461:29 - The way we obtain the 60 by 60 is by applying this formula,
461:32 - which we've seen already.
461:33 - So here we just have the output equal 64 photosize five.
461:40 - So we have minus five, add in no pattern.
461:43 - So pattern is zero, stride one.
461:45 - So we have 64 minus five and then plus one.
461:49 - This gives us 64 minus four, which is equal to 60.
461:53 - And that's how we have this output right here.
461:58 - From here, we move to the subsampling pooling layer.
462:02 - For the pooling layer, we have these two parameters.
462:06 - That is we have the number of filters
462:08 - and then already we have the filter size,
462:10 - not the number of filters, the filter size.
462:13 - And then we have the number of strides.
462:16 - To obtain the dimension for the pooling,
462:18 - the formula is slightly different, obviously,
462:21 - because here we don't have the pattern.
462:23 - So we have X minus F divided by S plus one.
462:27 - And then we'll leave from this
462:29 - to this feature map right here.
462:32 - Notice how we still maintain the number of channels,
462:34 - but our input feature map has been subsampled.
462:39 - For the particular case of max pooling,
462:43 - if you want to understand how this works,
462:45 - let's say we are at this position.
462:48 - Let's take this position.
462:49 - Let's take a position where we have some values.
462:51 - Notice how as we pick this value,
462:53 - we see the max is 0.02.
462:55 - So you should look at this.
462:57 - So we see here the max is 0.08 and so on and so forth.
463:03 - So basically what we're doing is
463:04 - we're just simply sliding through the whole image.
463:07 - And then for every, since our kennel size is equal to two,
463:11 - we have a two by two kennel size here.
463:14 - So for every four values,
463:18 - we are gonna pick out one to represent them.
463:21 - And in this case, this one is the max.
463:24 - In some cases, we will instead take the average of this.
463:28 - That is known as average pooling.
463:30 - But for this max pooling,
463:31 - which is the most commonly used,
463:35 - we take the max of all these different values.
463:39 - That said, to obtain this, we have X.
463:41 - That is in this case, we have X equals 60.
463:44 - So let's take this off.
463:46 - X equals 60 minus F.
463:52 - F equals two divided by the stride.
463:55 - Our stride is two and then plus one.
463:59 - So 60 minus two, 58 divided by two,
464:03 - 29 plus one gives us 30.
464:07 - That's how we get this.
464:08 - We still maintain a number of channels.
464:11 - Here's another example showing even more clearly
464:14 - how this max pooling works or pooling in general works.
464:18 - So if we have this input,
464:19 - so we're picking out just one of this,
464:21 - suppose we're picking out just one of these channels
464:23 - and we have this,
464:25 - we are gonna have our kennel two by two, that's it.
464:28 - And then what we get is output is zero
464:31 - since the max of zero, zero, zero is zero.
464:34 - We do, we have a stride of two.
464:36 - So notice how we've shifted by two positions, stride two.
464:39 - And then the max here is two.
464:41 - We move again, stride two, max now is two.
464:44 - We move, you see max is one, move, max is zero.
464:48 - And then you go to the next, max three
464:50 - and so on and so forth.
464:51 - So that's how we obtain this new feature map right here.
464:55 - Notice how we have this is five, 10, five, 10
465:00 - and then here we have five.
465:03 - So we leave from 10 by 10 to five by five.
465:07 - After this subsampling layer, we have the activation.
465:10 - We've already looked at activation in the previous section.
465:13 - We've seen the sigmoids and we've seen the relu,
465:16 - we've seen the tangs and we've seen the leaky relu.
465:19 - So that said, we understand that
465:21 - and now we move to the next convolutional layer
465:24 - or filter size equal to five, add in no pattern,
465:27 - stride one, number of filters 16,
465:29 - number of parameters 2,416.
465:32 - So you could take this as an exercise
465:34 - to be able to show that the total number of parameters
465:38 - we have is 2,416.
465:41 - That said, we're going to have this output right here.
465:44 - We have the relu added and then we have this output.
465:46 - So we have the output 26 by 26 by 16.
465:51 - Recall number of filters dictates
465:53 - the number of channels right here
465:55 - and not the filter sizes, the number of filters.
465:58 - And then this 26 by 26 is gotten
466:00 - by using the same formula we've seen already.
466:03 - From here, we have another subsampling.
466:07 - We've understood that already
466:08 - and then we flatten all this out.
466:10 - So after doing the subsampling,
466:13 - we have this 13 by 13 by 16.
466:15 - When you multiply all this, it should give you 2,704.
466:20 - And this is what we call the flatten layer.
466:22 - So we pass this to a flatten layer to obtain this.
466:25 - Now this takes each and every value we have here
466:28 - in our feature map and then just simply places it
466:32 - in this one dimensional output right here.
466:37 - Then from here, we have the dense layer,
466:39 - which we're used to working with already.
466:41 - And then we have a thousand neurons in the output.
466:45 - And finally, we have 200 neurons.
466:48 - In our case, we should have two
466:50 - since we're actually predicting whether it's a parasite
466:55 - or it's a parasite, so we should just have two.
466:58 - Anyways, we understand all this now
467:01 - and we should be ready to dive into the code.
467:04 - Before moving on to the code,
467:06 - let's get to see what the feature maps
467:09 - of a trained convolutional neural network looks like.
467:13 - As you could see right here from this example
467:16 - from the Stanford website,
467:19 - we have this car which is passed
467:20 - into a convolutional neural network
467:23 - and then its output, we have it predicting a car.
467:28 - As you can notice,
467:30 - this first layers actually serve as filters
467:36 - for low level features like the edges.
467:40 - So you'll notice that this input or this feature maps
467:45 - from the first layers produce visually interpretable outputs.
467:53 - Now you will notice also that as we go farther or deeper,
467:58 - we have this outputs which are less visually interpretable.
468:03 - And the focus more on high level features
468:07 - like the car parts,
468:09 - which per meters correctly classified
468:12 - that this image contains a car.
468:16 - And so the first layers are for feature extraction
468:20 - and then the last layers are for classification.
468:26 - Both subsections actually need each other
468:29 - as if you do feature extraction
468:32 - and you don't have layers which are able
468:35 - to permit us correctly, classify this image,
468:38 - then we will not achieve our goal.
468:43 - And in the same sense,
468:45 - if we have a good classifier,
468:48 - but we get this kind of inputs directly,
468:54 - where we've not extracted useful features from them yet,
468:58 - then our classifier is not gonna perform well.
469:01 - You could also check this ConvNet-Amnes demo
469:05 - by Andres Caparte,
469:07 - where he trains a model on the Amnes dataset.
469:10 - Here we have for model, the input, conv, pull,
469:13 - conv, pull, and softmax.
469:14 - So here we have this on Amnes dataset.
469:17 - And then we have in this passed in, as you could see,
469:20 - notice how in this first layers,
469:23 - the feature maps actually contain much more visual content
469:29 - as compared to this final layers right here.
469:33 - See, if you look at this final layers,
469:36 - you see that we have this feature maps,
469:39 - which instead contain content,
469:42 - which permits us see whether a particular input
469:47 - belongs to a particular class.
469:49 - If you want to build convolutional layers with TensorFlow,
469:53 - you could make use of this TensorFlow Keras model right here.
469:58 - So that said, we're just gonna come to the layers,
470:01 - see other Keras, and then we pick layers,
470:04 - and then we'll find the conv2D layer.
470:07 - So here is the layer we're searching for,
470:09 - double click on that, and then here we go.
470:13 - We have the arguments, that's what we pass in this layer.
470:18 - The filters right here corresponds
470:20 - to the number of filters.
470:22 - So let's take this off.
470:23 - Here is the number of filters, we've seen this already,
470:25 - and F, and then the kernel size
470:28 - corresponds to the filter size.
470:32 - From there, we move to the stripes,
470:33 - which take a tuple, let's take this off and check this out.
470:39 - So here we have this tuple actually,
470:42 - and then what's important to note here is the fact that
470:46 - if you want to do a striding of say one two,
470:50 - that is one two, then in the height dimension,
470:57 - your striding is gonna be equal one,
471:00 - and then the width dimension is trying to be equal two.
471:03 - So if we have this feature map right here,
471:06 - and we pass a kernel, we are gonna be sliding
471:12 - and skipping two steps in this horizontal direction,
471:16 - while when we come in, when we're going downwards,
471:19 - we are gonna be skipping only one step.
471:22 - So that's what this means.
471:23 - Now in the case where we are having exactly the same
471:26 - number of steps sliding horizontally and vertically,
471:30 - then the striding, like in this case,
471:32 - can just be given as equal one.
471:36 - From there we have the pattern,
471:37 - by default the pattern is valid.
471:39 - Here we tell that for the pattern,
471:43 - if the pattern is valid,
471:45 - it means there's actually no pattern,
471:47 - so we have zero pattern.
471:48 - And then when the pattern is same,
471:50 - the result of the pattern are the,
471:52 - we've padded with zeros to ensure that
471:56 - the input dimension equal the output dimension.
471:59 - Also note that this is only possible
472:02 - when the stride is equal one.
472:05 - So that said, oh, if we have a feature map like this,
472:09 - let's say we have 60 by 60.
472:13 - Then we wanna pass this to a conv layer
472:16 - to get an output feature map.
472:19 - And if we want this output feature map
472:21 - to be the same dimension as the input that is 60 by 60,
472:25 - then all we need to do is specify
472:27 - that the pattern is same.
472:29 - So once we have this,
472:31 - the output will be the same as that of the input.
472:34 - From here we have the data format.
472:37 - Now note that there are generally two kinds of data formats.
472:40 - That is we have height.
472:43 - So for an image, we have the height, width,
472:46 - and then the channel.
472:48 - So if we have a 224 by 224 by two V image,
472:59 - then we actually making use of this data format.
473:03 - Now we could also use this format
473:05 - where we start with a channel.
473:06 - So we have the channel by height and by width.
473:13 - And so here we have three by 224 by 224.
473:17 - Note that by default we have the channel last.
473:20 - So by default we have this first convention last right here.
473:23 - But if you wanna take this convention,
473:27 - all you need to do is to specify
473:28 - that you're working with the channels first.
473:33 - Then from here we have the dilation rate.
473:35 - So we'll look at this GitHub repository by Vido Mulung
473:41 - where he uses animations to explain how convolutions work.
473:45 - So you could scroll down
473:48 - and you would find the dilated convolution animation,
473:51 - which when we click on, we have this.
473:54 - So there we go.
473:55 - We have this example in which
473:57 - the dilation rate is equal to.
474:01 - So the way this dilation works is we have this kernel
474:06 - which initially had no spaces between its values.
474:10 - So we had something like this.
474:12 - We had this three by three kernel.
474:15 - And now what we do is we have this spaces
474:18 - or this holds between these different values.
474:23 - And so now what we get is
474:25 - some sort of five by five kernel right here.
474:29 - If we're working with a dilation rate r equals three,
474:33 - then instead of a single whole year,
474:36 - we are gonna fit in two other holes.
474:39 - So then I have this, we just have one, two,
474:42 - and then we fit this.
474:44 - Then we have one, two, and then we fit this.
474:48 - So that's how we build this out.
474:50 - We have this and then that.
474:53 - Now to get the shape, we have one, two, three, four,
474:57 - five, six, seven.
474:58 - So we have a seven by seven filter now.
475:03 - It should be noted that the dilated convolutions
475:06 - are used in problems where we wanna keep increasing
475:10 - the receptive field as we keep going deeper in the network
475:16 - while maintaining the number of parameters.
475:19 - The next argument is the groups.
475:22 - And according to the documentation,
475:24 - this is a positive integer specifying the number of groups
475:27 - in which the input is split along the channel axis.
475:31 - This means that in this case, for example,
475:33 - we could break this up into three different groups.
475:36 - So we have one, two, and then three groups.
475:41 - Then each group has its own group filters.
475:45 - And then the output is a concatenation
475:47 - of all the group results across the channel axis.
475:51 - From here, we have the activation we've seen already.
475:56 - And then we'll see whether we're gonna use the bias or not.
476:00 - We have this kernel that is the weight initialization.
476:04 - And then we have the bias initialization.
476:06 - We have this regularizes, which we'll see subsequently.
476:11 - And then we have the kernel and bias constraints.
476:19 - In order to create our convolutional neural network,
476:23 - we're just gonna make use of this sequential API
476:26 - which we had built previously.
476:28 - And then we'll define the input layer.
476:31 - So here we have this input shape 224.
476:34 - Let's, we had defined this as in size actually.
476:37 - So we have in size, that's it, by in size by three.
476:44 - We could also define a number of channels,
476:45 - but I just let it to be equal to three.
476:48 - Then from here, we have no normalizer.
476:51 - And then we have this conv2d layer right here.
476:55 - So we have this arguments which are default.
476:58 - So we'll then bother to check on this.
477:01 - We'll take this off, take this off, and that's fine.
477:06 - So we'll have this, there we go.
477:09 - We have our conv2d, our conv2d which we copy.
477:14 - Or rather we cut that off
477:16 - and then we put this after the input layer right here.
477:20 - So there we go, we have our input layer.
477:22 - We want six filters and then with a kernel size of five.
477:28 - So that's it, our stride.
477:30 - We could take this to be equal to one.
477:32 - There we go, padding is valid.
477:36 - So that's what we expect.
477:37 - And then just here we have our activation.
477:41 - So we have activation, we could say sigmoid.
477:44 - We have the sigmoid activation.
477:47 - Since we're replicating the ReLU
477:50 - or rather the LeNet architecture.
477:52 - So there we go, we have the sigmoid
477:55 - and that's our conv2d.
477:57 - From the conv2d, we have a max pooling layer.
478:00 - Let's take this off, let's take the dense layers off.
478:03 - From this conv2d, we have the max pooling layer.
478:05 - Now what we could do is
478:07 - we could just simply import all this.
478:09 - So let's go ahead and say,
478:12 - import the conv2d, max pool2d and the dense layer.
478:18 - Coming back to our documentation,
478:20 - we could check out on the max pool2d.
478:23 - So there we go, we have max pool2d.
478:27 - We'll specify the pool size, which is equal to
478:29 - and then the number of strides we call two.
478:32 - Padding is valid and data format,
478:35 - we're not going to specify that.
478:36 - So we have our max pool2d
478:38 - and then the pooling size with the strides.
478:41 - Copy that.
478:43 - We actually don't know the stride of two.
478:44 - So we replace that, we have a stride of two.
478:47 - The next step is this one right here,
478:50 - similar to what we had seen previously,
478:52 - with a difference that now we have 16 number of filters.
478:56 - And so we could simply copy this
478:58 - and then paste this out right here.
479:01 - We have conv2d, max pool2d, now conv2d, max pool2d.
479:05 - And then here we have 16 number of filters.
479:08 - The padding is valid, activation sigmoid.
479:11 - Then this next max pool2d is still the same
479:14 - as what we had previously.
479:15 - And so we move on to the flatten layer.
479:19 - We add this here, we have flatten, there we go.
479:23 - And we run that.
479:25 - So while that's running, we just simply come right here
479:29 - and then add that flatten layer.
479:31 - So we have this flatten layer,
479:33 - which is in charge of flattening this
479:36 - or converting this into this 1D output.
479:40 - From this flatten, we now have a dense layer,
479:43 - which is what we've seen already.
479:45 - So we just have to put this dense layer here.
479:48 - Let's take this, this, and there we go.
479:51 - So we have our dense layer
479:53 - and then the activation is sigmoid.
479:56 - So we're respecting the activation using the Lynette paper.
480:01 - Then we have this here, we'll take a thousand.
480:04 - And then we have that.
480:07 - We add another dense layer, add this other dense layer.
480:10 - Now we'll make sure that as we create this final dense layer
480:16 - right here, it has an output of two neurons
480:20 - since we are dealing with a binary classification problem.
480:26 - So that's fine.
480:27 - Take this here and then run our model.
480:31 - We have it at input layers now defined.
480:33 - So let's simply add that here.
480:35 - We have the input layer.
480:39 - So run that and then check that out again.
480:43 - There we go.
480:44 - We have this input layer, which we run, which we add
480:48 - and then we run this now, everything is fine.
480:50 - So that's it.
480:51 - We have 45 million parameters
480:55 - and no non-trinnable parameter.
480:59 - Notice how this dense layer here is responsible
481:02 - for a huge percentage of our parameters.
481:06 - So we could reduce this.
481:08 - Let's take this to a hundred and then this to like 10.
481:11 - Let's run that again.
481:13 - And there we go.
481:14 - We have a smaller model this time around.
481:17 - A point to note is this number of parameters
481:19 - which we pre-calculated here.
481:21 - Here we have 456 and here we have 2460.
481:26 - I will see how we get exactly this number of values here.
481:30 - For the dense layer, it's quite obvious
481:32 - as we just have a hundred times 10 plus the 10 biases
481:36 - giving us a hundred times 10 is 1000 plus 10
481:40 - giving us a thousand and 10.
481:42 - And then here we have 10 times two 20,
481:45 - 20 plus two bias is giving us 22.
481:47 - It should also be noted that there was a slight error here
481:50 - as we don't have like with this, we have five by five by three
481:54 - but here is five by five by six
481:57 - since we have the input number of channels equals six.
482:01 - So this means this is five by five by six
482:05 - and here is five by five by three.
482:07 - All this five by five by three
482:09 - and all this five by five by six.
482:11 - And if you take five by five by six
482:13 - and multiply it 16 times, you should have 2416.
482:18 - It should be noted that here we're trying to replicate
482:21 - the lunette architecture and this is in no way
482:25 - the state of the art kinds of models we use today.
482:29 - So in the section on the corrective measures
482:31 - we are going to use even better models.
482:35 - Now let's just work with this.
482:41 - We're now building the model.
482:43 - Let's now move on to the error sanctioning section.
482:47 - For error sanctioning binary classification problems
482:51 - we generally use the binary cross entropy loss.
482:56 - Let's look at those formula of the binary cross entropy loss
483:00 - from those ML cheat sheet website.
483:03 - Just as most error sanctioning functions
483:07 - with a binary cross entropy,
483:09 - we're trying to penalize the model
483:11 - when the actual prediction is different
483:13 - from the predicted value.
483:17 - And so in this case of a binary classification problem,
483:21 - if our actual prediction is meant to be a one
483:24 - and our model predicts a zero,
483:26 - putting this in here would have one log zero.
483:33 - So we have one log zero.
483:36 - And then here we have one minus one, one minus one is zero.
483:40 - So we have here zero, one minus one,
483:43 - log of one since P is zero.
483:47 - So we have one minus one log of one.
483:50 - If we plot out the curve for the log
483:54 - we would have something like this, we have this plot right here.
483:58 - Here is one.
484:00 - And then we'll notice that as we are approaching zero,
484:04 - that's X.
484:05 - And then here we have Y equals log of X.
484:09 - And as we are approaching zero,
484:11 - that's as X is tending to a zero,
484:13 - we would have this log which is going to us negative infinity.
484:18 - And so the log of zero is a very large number.
484:23 - Now with this case, we have zero, so this is taken off.
484:26 - And so this means when the actual prediction is one
484:29 - and the predicted value is zero,
484:31 - our final output becomes a large number.
484:34 - Now let's modify this.
484:36 - Let's say we have zero here and then here we have one.
484:41 - In this case, we would have a similar scenario
484:43 - because here we're going to have for the Y, Y is zero.
484:47 - So you have zero, zero times whatever we would have here
484:50 - will be zero.
484:51 - We would have zero here and then here we would have one
484:55 - because one minus Y, let's erase this one minus Y.
485:01 - Take this off.
485:02 - One minus Y in this case will give us one
485:08 - since we'll have zero here.
485:09 - And then we'll have log of one minus P, P is one.
485:13 - So we have log of zero.
485:14 - And again, we have a large value because log of zero
485:18 - is taking us to us negative infinity.
485:20 - And then we have one times that big number,
485:23 - giving us a big number.
485:25 - And so our model is sanctioned
485:27 - because it hasn't correctly predicted the expected output.
485:34 - And then supposing the actual prediction is one
485:36 - and then our model actually predicts one.
485:39 - In that case, we would have year one.
485:42 - So we have one, log of one, and then we have one minus one.
485:47 - So here we have one minus one, and then log of one
485:50 - minus one, that's log of zero.
485:53 - But here we have one minus one, which is zero.
485:55 - So zero times this will cancel out.
485:58 - And then what will be left with will be this.
486:03 - And as you could see here, when X equals one,
486:06 - log of X equals zero.
486:09 - So log one is zero.
486:10 - And then we have a final answer of zero.
486:14 - So our final output here is zero,
486:16 - telling us that the model has done its job correctly.
486:20 - If you do the same for when actually zero
486:22 - and predicted zero,
486:22 - you'll see that you always have this year zero.
486:26 - Now note that this zero, log zero
486:29 - is actually a standard limit,
486:31 - but we wouldn't get into that.
486:32 - And you could check on our calculus
486:35 - cause to better understand that.
486:37 - For now, just note that this will give you zero.
486:39 - And then yeah, we have also zero.
486:42 - So our final output is a zero.
486:44 - And our model now makes use
486:47 - of the binary cross entropy loss to update its weights.
486:52 - If you have a zero,
486:53 - and then let's say we have a 0.8 right here,
486:56 - we compute this BCE as a binary cross entropy loss,
486:59 - Y true, Y red.
487:05 - There we go.
487:06 - You see, we have this value of 1.6.
487:09 - Now let's modify this.
487:13 - So let's have say 0.02.
487:16 - I run that, you see we leave from 1.6 to 0.02.
487:21 - And then if we take say 0.2,
487:24 - you see we have this.
487:26 - Now you could always stack up outputs
487:29 - as we had in the beginning.
487:30 - So let's have back this outputs.
487:32 - And then we have BCE, Y true, Y red,
487:38 - and run that again.
487:40 - You see, we have this value here for our loss.
487:44 - And then if we take this to one,
487:46 - you see now we have your zero one,
487:48 - you see the loss is increased.
487:51 - Another argument we could pass in here
487:53 - is a from logits argument.
487:55 - You could check on the other arguments in documentation.
487:58 - For the from logits argument, by default,
488:01 - it's actually false.
488:03 - But then this default value of false
488:06 - is supposing that the output of our model,
488:09 - the Y red will always produce values in the range zero one.
488:14 - Now, the way our model has been constructed
488:17 - ensures that all our values will be produced in that range
488:20 - because we have the sigmoid right here.
488:22 - And if you could recall, we had the sigmoid,
488:26 - which was like this.
488:27 - So with our sigmoid, we have this function
488:31 - which looked like this.
488:33 - And that as we increased the value of X,
488:38 - the output was going towards one.
488:40 - So as we increase the value of X,
488:42 - output goes towards one.
488:43 - As X becomes very small or take a very large negative number,
488:48 - X goes towards zero.
488:49 - So in fact, it's gonna always ensure
488:52 - that the output lies between zero and one.
488:54 - That's why it's very important to have the sigmoid here.
488:57 - Now, in the case where this,
488:59 - we don't have the sigmoid,
489:01 - that is our output doesn't necessarily lie
489:03 - between zero and one.
489:05 - What we're gonna do is say that
489:07 - we're gonna use from logits equal true.
489:11 - So specifying from logits equal true
489:13 - simply means you are trying to say,
489:16 - you are not sure that your output or your model output
489:20 - will always fall under range zero, one.
489:24 - Now that said, if you run this,
489:27 - you see we have a totally different response.
489:31 - So you have to be very careful when working with this.
489:34 - If your values range between zero and one,
489:36 - make sure you use a default from logits equal false.
489:40 - From here, let's go ahead and compile our model.
489:43 - We have our optimizer, we have the loss,
489:46 - our losses, the binary cross entropy loss.
489:50 - So here we have binary, binary cross entropy.
489:56 - There we go.
489:57 - And then the metrics for now, let's take this off.
490:01 - So we just come in this part for now.
490:03 - We're not gonna take that into consideration.
490:05 - So that's it, we've compiled our model.
490:08 - So we could run this, compile our model,
490:11 - item is not defined.
490:12 - Let's go ahead and define all this.
490:17 - And I'm now defined, let's take this metrics off.
490:20 - And then we have the binary cross entropy,
490:23 - binary cross entropy, there we go, we run that.
490:27 - We're running this now, everything works fine.
490:30 - And so we go ahead and train our model
490:33 - in a similar way we had done previously, right?
490:37 - Here we have our train data and our validation data.
490:41 - Let's also reduce this learning rate, let's run that.
490:44 - And then we start with the training.
490:47 - We have those arrow which reads logits
490:50 - and labels must have the same shape.
490:53 - This shape is given versus this one.
490:57 - Now we'll try to understand together
491:00 - why we're having this error.
491:03 - So let's go right up to the model creation.
491:08 - Now, when creating the dataset or when processing
491:13 - this dataset, we had these kinds of inputs and outputs.
491:23 - For the inputs, we had 224 by 224 by three
491:26 - and for the output, we had just one.
491:31 - And this was because our output could take either a zero
491:35 - or a one.
491:37 - But the way we'll define our model is slightly different.
491:41 - With this model definition,
491:43 - we actually have a shape year of two.
491:48 - So let's take this, we actually have a shape of two,
491:52 - meaning that we could have two outputs.
491:56 - Whereas the delay our dataset was constructed
491:59 - was such that we could only have one output.
492:02 - When our output is zero, we suppose that it's parasitized
492:06 - and when our output is one, it is uninfected.
492:10 - And so because of this, we are going to modify our code
492:14 - such that this output year is one.
492:17 - Let's run that again and there we go.
492:20 - So we've modified our model, we recompile our model
492:24 - and then we get to train our model.
492:27 - We still have the same error, but this comes from the fact
492:31 - that we changed this name from model to lunet model
492:35 - so that when working with other models,
492:37 - we could always find ourselves.
492:41 - So we have lunet model and then here is lunet model.
492:45 - Lunet model.
492:46 - So that's it, we run that again and that should be fine.
492:49 - So we've started out with our training process.
492:55 - You see, we're training, but this looks very slow.
492:58 - Let's check on the runtime.
493:00 - So let's click on change runtime type,
493:02 - hardware accelerator and we see known.
493:04 - So we actually using a CPU year
493:07 - of which we should be using a GPU.
493:10 - So that's it, let's check on this
493:12 - and then let's select GPU.
493:14 - So that's fine now and we save.
493:16 - We will run that again.
493:22 - We told this model is not defined
493:24 - and this is because every time it changes runtime,
493:27 - you have to restart all this.
493:29 - So let's rerun this and then get to the training.
493:34 - The training process is now much faster
493:37 - than what we had previously as you could notice.
493:40 - And this is because we're now using a GPU.
493:44 - After training, we have this error.
493:46 - Now the fact that the model trains
493:50 - and then gets right up to the end of the epoch
493:53 - before trying this error should give us a hint
493:57 - that most probably this error is coming
493:59 - from the validation set because for the training,
494:02 - everything went well.
494:03 - And then at the end where some validation has to be computed
494:08 - give us a validation loss, we have an error.
494:10 - This means that that validation data set
494:13 - has a problem.
494:15 - So here we have validation train data set,
494:18 - valid data set.
494:19 - And then what we notice is that
494:22 - these two are slightly different.
494:24 - So here we've not done the resizing.
494:27 - We've not yet done the pre-processing of the validation set
494:31 - as we did with the train data set.
494:34 - So let's get back and check on that.
494:38 - So right here, we have this.
494:40 - We have this, let's add this code
494:44 - and repeat this for the validation and the test set.
494:49 - So that's what we get.
494:51 - And here is it.
494:52 - Let's ensure that we had no pre-processing before this.
494:55 - So here we have this train.
494:59 - So we have to repeat the same for the validation.
495:01 - Everything we do with the train,
495:02 - we do the same with the validation.
495:05 - And coming back up here, we have, okay, everything is okay.
495:09 - So yeah, we need to make sure that this train
495:12 - and validation have the same pre-processing.
495:16 - We have the train and then the validation.
495:19 - So we have validation and your validation.
495:24 - We repeat the same two for the test.
495:26 - So we have test data set, equal test data set,
495:32 - the map, resize and rescale.
495:37 - That's fine.
495:38 - So we run that and then we go ahead
495:41 - and check now on this next one.
495:44 - Here we have the batch size defined,
495:46 - the train shuffling and batching and pre-fetching.
495:51 - So yeah, let's take this off since it's already defined.
495:53 - We have the validation.
495:56 - Here is validation.
495:59 - There we go, validation and that should be fine.
496:02 - So we have the validation which has been processed.
496:07 - So we've done this, we run this
496:08 - and then we have the test.
496:11 - We do the same for the test.
496:15 - But when we check this out,
496:16 - we see that we are trying to shuffle this test
496:19 - which is actually a very useless operation
496:21 - since we do not want to shuffle our test set.
496:25 - And then we do not also want to do this batching.
496:27 - So this tool already useless
496:30 - and then the pre-fetching useless.
496:31 - So practically we don't need to do this for the test set.
496:34 - So that's fine.
496:35 - And then we have train and then validation.
496:38 - Train, validation data set, we run that and here we go.
496:42 - Now we have this known and known here
496:44 - because we have completed this batching
496:48 - the first time for the training
496:50 - and then we've redone this again.
496:52 - So it's like a batch on a batch.
496:54 - That's why we have this.
496:55 - So we have to get back and make sure
496:57 - that we work in from this.
496:59 - So we have to, we run this to make sure we have
497:04 - this train data set calculated again.
497:09 - So now we have our train data set
497:11 - which has been completed from the original data set
497:14 - and everything should be fine.
497:15 - We've done that and we'll visualize,
497:20 - resize and rescale.
497:22 - Yeah, this could be done too for the testing.
497:25 - So that's why we allow this
497:26 - because we need to resize and rescale for our test set.
497:31 - So we have that, fine.
497:33 - And then here we go.
497:35 - We run this right here, run this validation and train.
497:41 - You see, everything is now okay.
497:43 - So we have that, that's fine.
497:45 - Our model, we have a model.
497:47 - It's needless actually,
497:49 - reinitializing our model
497:50 - since we've already started with training.
497:52 - So we just keep it from here.
497:54 - But for demonstration purposes,
497:56 - let's just rerun this again.
497:58 - So we've been training for a while
498:01 - and we're getting this very poor results.
498:03 - We notice how the loss isn't changing at all
498:07 - for the validation.
498:08 - It's kind of a similar situation.
498:09 - We're having these changes within five, four, four, five
498:13 - and that's it.
498:14 - Another thing we could do to make this debugging faster
498:19 - is we'll for now take off the validation.
498:22 - So let's take this off for now
498:25 - and then let's stop the stringing.
498:28 - We now make some changes to our model right here
498:32 - where we, instead of having this activation,
498:34 - we have relu.
498:36 - And then here we have this relu.
498:39 - Right here we have relu.
498:42 - There we go.
498:43 - We have relu and this is a sigmoid.
498:47 - We would also want to reduce the size
498:50 - of this receptive field.
498:51 - So we take the current size to three
498:53 - and your current size three.
498:55 - So that's it.
498:56 - Let's rerun this and see what we get.
498:59 - There we go.
499:00 - We run that and now we don't have the validation.
499:03 - So the training should go faster.
499:06 - We see again here that nothing has really changed
499:08 - with the training.
499:09 - So let's interrupt this and get back to our model.
499:13 - There we go.
499:14 - We have our model.
499:15 - We're now going to include batch normalization.
499:19 - In batch normalization, values of the same,
499:23 - all values belonging to the same batch are standardized.
499:28 - So we have X becoming X minus the mu, mu,
499:32 - divided by the standard deviation.
499:36 - So that's it.
499:37 - Let's add this batch normalization layer right here.
499:40 - Batch normalization.
499:42 - There we go.
499:43 - We have the batch normalization.
499:45 - After this conf 2D again,
499:47 - we have batch normalization.
499:51 - And then with the dense,
499:54 - we have batch normalization.
499:59 - We'll do the same for this.
500:01 - Let's not forget a commerce.
500:02 - So we should have this here and have this here.
500:06 - Let's copy this and then paste it out here.
500:10 - We have batch normalization.
500:12 - So that's fine.
500:13 - We have included batch normalization.
500:15 - We will run the model, batch normalization not defined.
500:21 - Let's have this here.
500:23 - We have batch normalization.
500:25 - There we go.
500:26 - So we take this and then we will run our model.
500:30 - We're running this.
500:32 - That should be fine.
500:34 - Our model is now recompiled.
500:36 - So we recompile our model and then we feed the model.
500:41 - What do we notice?
500:42 - We have this loss which now drops normally.
500:46 - And so we'll see how important it is
500:48 - to work with a batch normalization layer.
500:51 - Apart from this,
500:53 - the batch normalization layer serves as a regularizer.
500:56 - I will see that subsequently.
500:59 - Given that our model is now trained properly,
501:01 - we could halt this training process
501:04 - and then include performance measurement.
501:07 - So here we have the metrics and we have accuracy.
501:11 - So our performance metric here is accuracy
501:15 - and we run this.
501:17 - So when training we'll be able to see how the loss
501:21 - and the accuracy will evolve.
501:23 - There we go.
501:24 - We recompile the model and we go back to training.
501:29 - A model's accuracy is equal the total number of times
501:34 - that model predicted an output correctly
501:39 - divided by the total number of predictions.
501:43 - This means if we have a model right here,
501:46 - let's say we have model A right here,
501:49 - and then we have another model say model B,
501:53 - and that we allow these two models to carry out
501:57 - say 1000 predictions.
501:58 - So we have totally 1000 predictions.
502:02 - Now, if model A does 800 correct predictions,
502:07 - like correct predictions,
502:08 - then its accuracy is 800 divided by 1000.
502:13 - Now you could put this as a fraction or in percentage.
502:18 - So here we have 80% accuracy.
502:21 - Now, if we have model B,
502:24 - which does 980 correct predictions,
502:27 - so we have this accuracy of 98% for B.
502:32 - And in this case, we'll see that model B outperforms model A.
502:38 - It should be noted that the accuracy as a performance metric
502:43 - isn't always the best choice of a performance metric
502:48 - when it comes to classification problems
502:50 - as others like the precision, the recall, the F1 score,
502:55 - and many others exist.
502:57 - For now, we'll use the accuracy,
502:59 - and later on we will look at the other metrics
503:02 - which we could use when we're dealing
503:04 - with classification problems.
503:06 - After training through 20 eBooks,
503:09 - we have this resource here.
503:11 - Now let's plot this out.
503:12 - So let's plot our loss and then plot the accuracy.
503:16 - We have these two plots, there we go.
503:19 - We see that the training and validation losses
503:22 - both keep dropping,
503:25 - and then the accuracy tool keeps increasing,
503:28 - though the training accuracy is slightly greater
503:31 - than that of the validation accuracy.
503:36 - Our next move will be to evaluate our model.
503:39 - So let's go ahead, we have learnnetmodel.evaluate.
503:44 - We evaluate this model, and here's what we get.
503:47 - We receive this error telling us
503:49 - that there is an incompatibility
503:52 - when they expect that shape
503:54 - and the shape of this test dataset right here.
503:58 - So recall that when building the test dataset
504:02 - that was on this position,
504:04 - when building the test dataset,
504:05 - we didn't include this batching.
504:08 - So let's just do that straight away.
504:11 - We have, let's add a code cell and that's it.
504:15 - So here we have test dataset.
504:19 - Now before doing that, let's print this out first.
504:21 - Let's print out the test dataset
504:23 - and you see the train dataset.
504:28 - So we have that.
504:29 - You'll notice that with the train,
504:30 - we have this batch dimension,
504:32 - whereas here we don't have that.
504:34 - So to include that, we have test dataset
504:36 - across test dataset,
504:38 - and then we include a batch of one
504:41 - since we just tested on single elements.
504:45 - So we have that, we run it, and there we go.
504:49 - So here, if we run test dataset this time around,
504:52 - see we should have this batch dimension.
504:55 - Now let's go ahead and evaluate our model.
504:57 - There we go.
504:58 - On data, this model has never seen.
505:02 - It has 94.16% accuracy and a loss of 0.2.
505:09 - This sounds interesting.
505:10 - And note that we could continue with this training.
505:14 - So you could train for more epochs as compared to this.
505:18 - And many scientists have made this remark
505:22 - where sometimes the forget to stop the training
505:26 - and then the comeback and notice that they've gotten
505:29 - an even better performing model
505:32 - because they allow that model to train for longer time.
505:35 - After evaluating our model,
505:37 - let's look at how to do model predictions.
505:41 - Now the whole idea of model predictions
505:44 - makes sense since we have trained our model
505:47 - inputs and outputs,
505:49 - and then now we want to pass in an input
505:53 - and let our model automatically come out
505:57 - or come up with the output.
505:59 - That is to say whether the image contains
506:03 - a parasitized cell or an uninfected cell.
506:08 - That said, all we need to do right here
506:10 - is our model.predict.
506:12 - So we have this predict method right here,
506:15 - and then we pass in our data.
506:18 - This case we have test data set,
506:20 - and then we take one value, that's it.
506:24 - Then we pick this up.
506:26 - Now we run that.
506:28 - Model not defined, we have the net and run that.
506:35 - So that's it.
506:35 - We're told that this is an uninfected cell.
506:39 - Now we'll define this method parasite or not,
506:42 - which is defined such that if we have an input X,
506:47 - then that if that X is less than 0.5,
506:50 - consider that we have a parasitized cell,
506:54 - and if it's greater than or equal to 0.5,
506:56 - then it's an uninfected cell.
506:58 - Recall that the way the data was created
507:00 - was such that parasitized was zero,
507:02 - and then uninfected was one.
507:06 - So we are having a threshold value of 0.5.
507:11 - This threshold value is defined now
507:13 - such that every value less than it
507:15 - is considered parasitized and everybody greater than it
507:18 - is considered uninfected.
507:20 - So that's it.
507:22 - If you now replace here with parasite or not,
507:28 - parasite or not, and we run this again,
507:32 - we are told that this is uninfected.
507:35 - We are going to do a test on nine different elements.
507:39 - So right here, we take nine of this,
507:42 - and then we do the subplots.
507:44 - First, we do the initial, and we specify this
507:47 - because we don't want to batch dimension.
507:51 - And then we have the title.
507:53 - On the title, we have the actual output,
507:56 - and we have the model's predicted output.
507:59 - Now that said, we run this.
508:02 - Here's what we get.
508:03 - We see that for this year, we have UU.
508:05 - So the actual uninfected is both uninfected.
508:09 - UU, UU, here we have UP,
508:11 - meaning that the actual is uninfected
508:13 - but it predicts parasitized.
508:16 - Here we have PP, correct, UU, PP, UU, and PP.
508:21 - Thank you for getting to this point.
508:23 - In our next section, we'll look at corrective measures.
508:26 - So we'll look at how to slow and save our model,
508:29 - how to build other types of models using different APIs,
508:34 - how to use different kinds of metrics,
508:37 - visualizing what our model sees,
508:39 - using callbacks, data augmentation,
508:42 - dropout regularization,
508:44 - early stopping, batch normalization,
508:46 - instance normalization,
508:48 - layer normalization, weight initialization,
508:51 - learning rate scheduling, custom losses and metrics,
508:54 - and sampling methods, custom training,
508:57 - tensor bot and hyperparameter tuning,
509:00 - weights and biases logs, weights and biases artifacts,
509:03 - and finally weights and biases sweeps.
509:06 - That said, see you next time.
509:19 - In our previous section,
509:20 - we've built a deep learning model based
509:22 - on convolutional neural networks
509:24 - to help detect the presence of malaria in blood cells.
509:29 - Nonetheless, in the real world,
509:31 - we are not always going to be using our models
509:35 - on a collab notebook like this.
509:38 - Hence, we need to be able to save this model
509:40 - so it could be used externally.
509:43 - In this section, we'll learn how to save and load a model
509:47 - and also do the same process with Google Drive.
509:52 - That is, we'll be able to save our model in our Google Drive
509:56 - and then later on when we want to use this model,
509:59 - we'll just load it from our Google Drive.
510:01 - That said, don't forget to subscribe
510:03 - and hit that notification button
510:05 - so you don't miss amazing content like this.
510:08 - We've built this very performance model
510:11 - though we could improve on it.
510:14 - But then once we close this,
510:17 - we do not save this model's current state.
510:21 - And so if we have to come next time,
510:24 - the model will have randomly initialized weights
510:29 - which will be different from the weights we've got now
510:31 - after training on this data set.
510:35 - Another issue is in case we want to use this model
510:39 - in another scenario or in another environment
510:43 - like say on a browser or on a mobile phone,
510:49 - we'll need to find a way to export this model from here.
510:53 - And so TensorFlow allows us to save our model.
510:57 - Now we'll have to differentiate between a model's
511:01 - configuration and a model's weights.
511:05 - So a model like this, let's suppose we have a model
511:10 - which is defined as such.
511:11 - We have the input, which we pass into a count layer
511:15 - then we have batch normalization,
511:17 - we have pooling for subsampling and then we flatten
511:21 - and after flattening, we pass through a dense layer
511:23 - and we have our output.
511:24 - So suppose we have this small model.
511:28 - Now, all the parameters for the creation of this model
511:34 - are known as the model's configuration.
511:37 - In the model's configuration,
511:39 - we may have it that the model for example,
511:42 - like in this case here,
511:45 - the model starts with a count layer with six filters,
511:49 - kind of size three, batch norm and all this.
511:52 - So these are our model's configurations,
511:56 - but this model's configurations are different
511:59 - from the model's weights.
512:01 - The model's weights are those filters we have,
512:05 - for example, in the case of the conf 2D.
512:08 - So we have the model weights
512:10 - and the model's configuration.
512:15 - And upon summarizing the model,
512:17 - we see clearly here that we have this conf 2D
512:21 - and then we have this number of parameters.
512:24 - And so whenever we want to save a model,
512:28 - we have to take into consideration
512:30 - this configuration and the weights
512:33 - because for this same configuration,
512:36 - we could have different weights.
512:38 - And so there are actually two main options.
512:42 - The first option is to save the full model.
512:46 - That is to save the model configuration
512:49 - and the model weights.
512:51 - Another option will be to save only the model weights.
512:56 - So we could save only the model weights.
512:59 - Now, this option is used when, for example,
513:03 - we don't want to, or we don't even know
513:06 - this model configuration upfront.
513:09 - So we have used this year,
513:12 - we've defined the model's configuration,
513:14 - we've trained it, we've got a new weights
513:16 - and this is the current model state.
513:19 - But if we take this to another environment
513:21 - where we don't have this configuration,
513:26 - then if we've saved this model's configuration and weights,
513:30 - all we need to do is just to load this configuration
513:35 - and weights which have been packaged as the full model.
513:39 - Now, in another case where we are able
513:42 - to get the configuration and all we need is just the weights
513:46 - then we'll just save the weights and then reload this weights
513:51 - since we already have the configuration.
513:54 - Either ways, we'll always need the configuration
513:57 - and the weights.
513:59 - Nonetheless, it's important to note that
514:01 - the most important part of this is actually the weights
514:04 - since working with a randomized or randomly initialized
514:08 - weights after we've trained our model isn't very useful.
514:13 - And sometimes we may take many days to train this model.
514:17 - So imagine you've trained your model for like 10 days
514:21 - and then you wanna reuse that model
514:23 - and the weights have been randomly initialized.
514:26 - You find that those 10 days have been wasted
514:29 - both time-wise and monetary-wise.
514:31 - So you have to ensure that you save your weights properly
514:35 - such that you could reuse them.
514:37 - And then the great thing with TensorFlow is
514:40 - you could also continue training from the state.
514:43 - So this means that at this point where we've gotten
514:47 - this model's performance year where we have 94%,
514:52 - we could keep training from year so that we could get
514:55 - even to say 99%.
514:59 - So you have to ensure that your saving is done properly.
515:04 - Now let's get into that.
515:05 - But before getting to that, one last point.
515:07 - Also note that with the first method here,
515:12 - with this first method,
515:13 - apart from this model configurations,
515:16 - we also have information like the metrics.
515:20 - So the metrics you use like the accuracy,
515:23 - the loss you use, the optimizer.
515:26 - So the optimizer information you use and all that.
515:29 - So this kind of hyper parameter information
515:32 - has been saved here.
515:33 - So next time all you need to do is just to load your model
515:37 - and then make use of it.
515:39 - Whereas here all you're saving is just the weights.
515:43 - That's it.
515:44 - Let's save our model.
515:45 - This case we have lenetmodel.save.
515:49 - Actually, lenetmodel.save and we give it a name.
515:52 - So we say lenet save model, for example, that's it.
515:57 - Now we have this lenet save model.
515:59 - Oh, we run that.
516:01 - So, and we check this out here.
516:03 - So we check out these files.
516:05 - And what do you see?
516:06 - You have this lenet save model folder.
516:09 - In this folder, you have the assets,
516:11 - which in this case is empty.
516:13 - You have the variables,
516:16 - which actually contains the weights.
516:18 - So you could download this from here.
516:20 - You could download this and then upload it next time.
516:23 - So from here, you see click on download.
516:25 - That's fine.
516:26 - We have the variables, which contains the weights.
516:29 - That's it.
516:30 - And then we have this saved model that put about file here,
516:35 - file here, which actually contains our configuration.
516:39 - We've had our configuration saved and our weights saved.
516:44 - Now let's load this.
516:46 - So we've saved this and now we can now load it.
516:50 - Note that you could always download this.
516:52 - So you could download the weights right here,
516:54 - download this.
516:55 - So let's click on download, download all this.
516:58 - And then next time, all you need to do is just to load it.
517:03 - Now let's go ahead and load this.
517:06 - The loading is quite simple.
517:08 - Here we will define a new model,
517:10 - lenet load that will have loaded model
517:14 - equals tf.keras.models.load model.
517:20 - There we go.
517:20 - And now we specify this exact same name.
517:23 - We have lenet saved model.
517:27 - Now what we'll do is we're going to do a lenet loaded model
517:32 - model and then summary.
517:35 - So that's it.
517:36 - We're going to run that and we're getting this error here,
517:41 - which is unusual.
517:43 - Changing this name actually makes this work.
517:46 - So lenet and then here.
517:49 - So we have this lenet and let's run that again.
517:54 - We save that and then we load this
517:58 - and we have our model right here.
518:00 - So this means if you have to come back to this notebook,
518:05 - all you need is to load this model,
518:08 - which has been saved right here in this lenet folder.
518:13 - And so just like with this,
518:14 - we'll replace this lenet model by lenet loaded model.
518:19 - So let's load this.
518:21 - Let's use this loaded model and do some predictions.
518:25 - There we go.
518:25 - Yes, we'll get UU, UPP, PP, PP.
518:29 - Here we have one error, UU, UU and PP.
518:33 - So it's kind of similar to what we have with the original
518:37 - model.
518:39 - From here, we could also evaluate this model.
518:41 - We have the net loaded model.
518:43 - We evaluate that and let's see what we get.
518:47 - Recall previously we had 94.16%.
518:50 - So now we expect to have something around that value.
518:55 - There we go.
518:56 - Exactly the same output as previously.
519:00 - Now we are going to look at how to load and save
519:04 - with the HDF5 format.
519:07 - Now this HDF5 format is a lightweight version
519:12 - of this TensorFlow model saving method.
519:15 - Here there's only this slight difference.
519:19 - All we need to do is to say,
519:22 - include this file extension.
519:23 - So we have your HDF5 and then we save that.
519:28 - Now you check this out.
519:30 - You should have the HDF5 appearance.
519:33 - So here we have the net HDF5 and then you could see
519:36 - its width by 53 megabytes.
519:40 - Now let's load this model.
519:42 - To load it, what we have here is the same code
519:45 - we had previously.
519:47 - And then here we specify HDF5.
519:50 - So there we go.
519:51 - We run that and we have exactly the same summary.
519:55 - So that's it.
519:57 - Now we're yet to work with custom layers
520:00 - but you have to note that in the case
520:02 - where you built custom layers,
520:04 - then those configurations aren't stored
520:07 - when you're dealing with this HDF5 format.
520:10 - And so that's why generally it's preferable
520:14 - for you to use this first formatting which we presented.
520:19 - That said, we're done with this first method
520:21 - where we save the configurations and the weights.
520:24 - Now let's look at this next method
520:25 - where we save only the weights.
520:27 - So in this case, for example,
520:30 - where we're having this notebook,
520:32 - what we could do is simply just save the weights
520:34 - given that we already have the model's configuration
520:37 - defined in here.
520:38 - So let's get straight away into looking at
520:40 - the save weights method which comes with TensorFlow.
520:43 - So we'll take that off.
520:45 - And then right here we have the net model
520:48 - which we defined already.
520:49 - And then we save this weights as the net weight.
520:53 - So here we've saved this weights.
520:55 - Let's put it in a folder.
520:56 - So we have the weights folder.
520:59 - We run that again.
521:00 - So we could see clearly our weights.
521:03 - Click on that and there we go.
521:05 - We have our weights.
521:06 - And this weights, we have the checkpointing.
521:09 - We have the weights.
521:10 - Now notice how this weights here does in this variables.
521:14 - Click on this variables.
521:16 - Okay, so notice how there is some similarity
521:20 - between what we had here and this.
521:22 - Notice how this is the same as this.
521:25 - And then this index here is the same as this here.
521:28 - Because we said that these variables contain the weights.
521:30 - And then we have the checkpoints.
521:33 - Subsequently, we're gonna look at checkpoints
521:34 - in with TensorFlow.
521:35 - So for now, just know that this is how we save the weights.
521:39 - And then upon defining your model,
521:43 - so you've defined your model,
521:45 - you can now load just this weights
521:48 - and not the whole model.
521:49 - But loading the weights,
521:51 - you're saying that you don't want the optimization
521:55 - or the optimizer configuration.
521:57 - You don't want the metrics
521:59 - and you don't want the last configurations.
522:02 - So that said, let's look at how to load this weights.
522:05 - Now here's all we need to load the weights.
522:07 - We have the learned model, the load weights,
522:09 - and then we load this weights.
522:11 - Let's do this so you see clearly that
522:13 - this loading actually works.
522:15 - So the first thing we'll do is
522:17 - we are gonna re-initialize our model.
522:19 - So we'll rerun this.
522:21 - So we rerun this.
522:23 - We'll compile our model.
522:24 - And then we run this evaluation right here.
522:28 - So we evaluate the model.
522:30 - And so you see that when the models
522:31 - weights are run on the initialized,
522:33 - we have very poor results.
522:36 - So there we go.
522:37 - After random initialization, we have this.
522:39 - Now what we'll do is we'll take the net model
522:44 - and then we load the weights.
522:47 - So we're gonna load the weights
522:49 - and then pass in our weights slash the net weights.
522:56 - That's fine.
522:57 - Let's run this again
522:58 - and then get our new models performance.
523:03 - There we go.
523:04 - Let's see that our model now gets back to the 94.16%
523:10 - accuracy we had initially after doing the training.
523:14 - At this point, we've been able to load
523:16 - and save our model right here on Google Collab.
523:21 - But as we know, at the end of the session,
523:24 - or after closing my notebook,
523:26 - all this information will be lost.
523:28 - So let's see how to save this information on Google Drive.
523:32 - We'll start by imparting this drive.
523:36 - So we'll have from google.drive, from google.collab,
523:41 - we're gonna impart the drive, let's run that.
523:44 - And then the next thing we'll do is to mount this drive.
523:48 - So we have drive.mount,
523:50 - and then we specify the location.
523:53 - So we have your drive and running that,
523:58 - you will be asked to put in an authorization code
524:01 - right here to get this authorization code.
524:03 - So if I just click on this right,
524:05 - this link given to us here, so we click on that link.
524:09 - And then once this pops up, we have this,
524:11 - you select your account.
524:14 - Once you select that account, you now go to connection.
524:18 - So you've connected and then you copy your code.
524:21 - So your code is copied.
524:22 - Now you put this in here and then you press enter.
524:26 - Once that's fine,
524:28 - you see we have here mounted at content slash drive.
524:33 - So mounted in this location, you could see clearly from here.
524:37 - And this tells us we are in this directory content
524:41 - and in this directory content,
524:42 - we've created this other directory drive.
524:46 - Now we click this open and then from this,
524:50 - I can get access to my own Google Drive.
524:54 - If now I want to copy this Lynette folder
524:58 - into my Google Drive so that next time
525:00 - I could just load it from my Google Drive,
525:03 - I'll make use of this CP command right here.
525:08 - So what we'll have is CP, some option,
525:13 - and then we have the source and the destination.
525:16 - So here we are going to specify,
525:18 - or we are going to use this R, so recursive.
525:22 - So we're going to use this to copy directories recursively.
525:25 - And that said, we run the command.
525:29 - We have this here and then we'll specify
525:33 - this folder's directory here.
525:35 - So we have content and then Lynette.
525:38 - So we copy in this Lynette
525:40 - and then to what destination to my drive.
525:43 - So we specify my drive.
525:46 - We have my drive and then in here I have Lynette.
525:51 - I'll let you say Lynette collab, so that's it.
525:55 - From here, I'm going to run this, my drive,
525:59 - and then I will search for Lynette collab.
526:01 - So that's what I have now.
526:02 - I have this Lynette collab right here.
526:05 - And then our next step will be to copy from the my drive
526:10 - to the Google collab such that next time in case
526:15 - where we have not, for example, saved this year,
526:18 - we'll be able to just quickly get that information
526:21 - from the drive onto the Google collab.
526:24 - Also note that this is really used through a data sets.
526:28 - So what we could have here is a data set in our collab
526:32 - and then we could transfer that data set to our drive
526:35 - and vice versa.
526:37 - Now let's do the same thing.
526:39 - So here we are going to copy this back, but this time around
526:43 - we are going to copy this into Lynette collab.
526:47 - So we're going to create a new photo here, Lynette collab
526:49 - and then take this information.
526:51 - So this time around we're copying from our drive
526:54 - into Lynette collab.
526:56 - So that's it.
526:57 - And then Lynette collab, we run that.
527:00 - And let's click on this, click again.
527:03 - And guess what we see?
527:04 - We have our Lynette collab right here.
527:07 - Thank you very much for filling up to this point
527:09 - and see you next time.
527:11 - Hello everyone and welcome to this new section
527:18 - in which we'll look at different ways of creating models
527:22 - other than the sequential API which we've seen so far.
527:26 - In this section, we'll look at the functional API.
527:29 - We'll look at building collable models.
527:33 - We'll look at building model via sub-classing.
527:36 - We'll also look at building our own custom layers.
527:41 - Previously in those cars, we said that there are three ways
527:44 - in which models are built in TensorFlow.
527:47 - That is the sequential API using the functional API
527:53 - and then finally model sub-classing.
527:57 - As of this point, we have been using the sequential API
528:02 - as you could see right here.
528:04 - Now you may ask yourself, why do we need to use
528:08 - a different method in creating TensorFlow models
528:12 - when so far we've achieved close to 99% train accuracy
528:17 - and around 95% test accuracy.
528:21 - Now, as you may have noticed, so far all the models
528:25 - we've been building have taken up this kind of structure
528:29 - where we have an input, we have the first layer,
528:32 - the next layer, which has been stacked
528:35 - in this sequential manner
528:38 - right up to this very last layer here
528:41 - and then we have the output.
528:43 - So the question we could ask ourselves is,
528:46 - what if we have a model which takes in say two inputs
528:50 - and has three outputs?
528:54 - These kinds of models are very popular in deep learning
528:59 - and we shall look at them subsequently.
529:02 - But before getting there, you could just imagine
529:05 - a problem where instead of classifying whether
529:08 - we have a non-parasitic or a parasitic cell,
529:13 - we wanna know the exact position
529:18 - of that parasitic cell or in general that cell in the image.
529:24 - You would find that you would have one output
529:27 - which classifies whether it's a parasitic or not.
529:31 - So we have this first output, parasitic or uninfected
529:36 - and then this other output which gives us the position
529:39 - of the cell or the exact position of the cell in the image.
529:45 - So here we see already how we could get two outputs
529:49 - from this, let's take this third output out.
529:52 - So here we see we could have, let's even take this one.
529:55 - So here we have this one output, two outputs model
530:00 - and with a sequential API, we can't really do this.
530:04 - So that's why working with a functional API
530:07 - is very important.
530:10 - The next point is we'll be able to create more complex models
530:16 - with the functional API.
530:18 - So there is this model known as the ResNet
530:21 - which is very popular in deep learning computer
530:25 - or deep learning for computer vision.
530:27 - Now, a ResNet like structure will look like this.
530:30 - We have this model, this layer of outputs
530:34 - I've been passing to this next layer
530:36 - and then we have the outputs of this
530:42 - which are gonna be concatenated with this outputs
530:46 - and then after this concatenation,
530:49 - we are gonna pass this to the next layer right here.
530:52 - So if we wanna add this layer, we could have a layer here
530:56 - and have that.
530:56 - So as we're saying, we take this output
530:59 - and then concatenate it with this next output
531:02 - before passing to this next layer.
531:04 - And so those kinds of structures or those kinds of models
531:07 - could not be built with the sequential API
531:11 - and hence the need for the functional API.
531:14 - And then the last reason why we are gonna be using
531:18 - the functional API is the fact that we could use
531:22 - shared layers.
531:24 - With shared layers, we could have a layer
531:27 - or a particular layer in our model
531:31 - which has already a predefined way of encoding information.
531:35 - So when we pass information, let's say we have this input,
531:38 - let's say input one, when we pass this input one,
531:41 - this layer right here or this encoder produces an output
531:48 - which is gonna be different from when we pass in
531:51 - another input I2, but the way it produces these outputs
531:58 - is in a very thoughtful manner.
532:00 - So we could have I1, I2, I3, which all share this layer
532:08 - and then we have other layers of the model which follow on.
532:12 - That said, we'll look at how to create the functional API.
532:16 - So here we have the sequential and then just below
532:19 - we are gonna create this functional API.
532:23 - Before starting all the creation,
532:25 - we are gonna impart some classes.
532:27 - So start by imparting the input class right here
532:31 - which is a layer, we import input
532:34 - and then we have from tensorflow Keras layer,
532:40 - rather models, we're gonna import model.
532:45 - So we import the model right here
532:48 - and we import the input.
532:50 - We run this, that should be fine.
532:54 - We now have this func input
532:56 - since we're using the functional API,
532:58 - this way of calling that, we have the func input
533:01 - and then we have input which we just called
533:03 - and this takes in the shape.
533:05 - So here we are gonna copy this exact shape
533:08 - we use in the sequential API.
533:10 - Have the shape right here, there we go.
533:13 - We copy that shape and then we reuse it here
533:17 - and create in this input layer.
533:20 - So here we have that and then we have your shape.
533:23 - So we've had the shape, those points,
533:26 - you could start stacking up all these different layers
533:30 - we had stacked up in the sequential
533:32 - or with this when we're using the sequential API.
533:36 - We started with this, yeah, this com2D
533:39 - right up to this dense layer.
533:40 - So there we go, we is gonna make use of this.
533:43 - So we copy that and then we are gonna paste this out
533:47 - right here.
533:48 - Now, first things first, we have an output.
533:52 - So first things we have this layer
533:54 - that we have this com2D which we've defined
533:58 - and then we pass in the output from this input layer.
534:03 - So here we have this func input, we copy that
534:06 - and then we pass this into this comf layer right here.
534:12 - Now, once it passes into this comf layer,
534:14 - we have an output and that output is this x
534:18 - and then you should guess that right,
534:20 - we pass this x into this back norm layer.
534:24 - So here we have x as you could see
534:27 - and then we have an output of x, there we go.
534:30 - From here, we pass in the x into the max pull 2D layer.
534:37 - So we have this, we cut that
534:40 - and then we have this x right here.
534:42 - So we'll just repeat this same process right up to the end
534:46 - and there we go.
534:47 - You see that we haven't done much changes
534:51 - as compared with the sequential API.
534:54 - So that's it, we pass in this input right here,
534:58 - we have x, we pass it in, we have this, we pass in
535:02 - and right up to this end.
535:04 - Now, once we get to the end,
535:06 - we are now going to create the lunet model from this.
535:09 - So we have lunet model equal model, which will import it
535:14 - and then we have the func input.
535:17 - Now, yeah, let's say we have func output.
535:21 - So we pass this last
535:23 - and then our last output is func output.
535:26 - So we have the input and then we have the output.
535:30 - So there we go.
535:31 - We can now give it a name.
535:32 - We have name lunet model.
535:36 - If you look up, we have,
535:39 - let's take a look at this right here.
535:41 - So this would be the input image.
535:43 - Here we have our input image
535:45 - and then we've created a model lunet model.
535:49 - And then from here you could simply do lunet model summary.
535:54 - Now you'll notice that we should have exactly the same
536:00 - summary as we had with a sequential API.
536:03 - So let's run that and see what we get.
536:06 - Yeah, we have how many parameters?
536:09 - We have 4,668,297 parameters.
536:16 - There we go.
536:17 - You see, we have exactly the same number of parameters,
536:19 - the same number of trainable and non-trainable parameters.
536:23 - So basically what we've done here is we've created
536:26 - this model created with a sequential API.
536:30 - Now we've gotten this.
536:31 - We'll see that we have to change
536:34 - absolutely nothing from our code.
536:37 - So yeah, we was going to compile our model
536:41 - without changing anything.
536:43 - We have the same lunet model.
536:45 - Now we could also change this, let's say lunet func.
536:47 - So you see clearly that we are actually using
536:50 - this functional model right here.
536:53 - So we have this func, there we go, func and that's it.
536:58 - We could run that.
537:00 - And then we recompile right here.
537:04 - So we are not changing any parts of this.
537:06 - We recompile that and then we train the model.
537:11 - We are getting this arrow because of the way we named
537:15 - this model right here.
537:16 - So let's have this lunet model.
537:19 - That's fine.
537:22 - We recompile and then we run.
537:26 - So that's it.
537:27 - We train our model and here is what we get as results.
537:33 - Now coming back to our model, we'll see that we have
537:36 - this feature extraction unit right here.
537:40 - So this conf layers are responsible for extracting
537:44 - useful features from the images.
537:46 - And then this last layers are responsible for correctly
537:51 - classifying whether the image is parasitic or not.
537:56 - That said, we could build a model known as feature extractor.
538:01 - And so here we're at this.
538:04 - We have our model feature extractor,
538:07 - which is going to be like similar in construction
538:11 - as what we've done so far.
538:13 - So we just have that copied and then we have this.
538:17 - But the difference is we are not going to include
538:19 - this other, this final layers right here.
538:23 - We're only at this point.
538:25 - And then we'll have as output this year.
538:30 - So this is our functional,
538:32 - here we'll call this extractor.
538:34 - Let's just say we have this as output.
538:36 - So we have this output and there we go.
538:40 - So here we have our functional input
538:42 - and then we have this output.
538:46 - And then here is the feature extractor.
538:48 - So we have our feature extractor model right here.
538:53 - We could do this feature extractor.
539:00 - And then we summarize this.
539:03 - So let's run this and see what we get.
539:05 - So that's it.
539:06 - We have our input and then we have this output right here.
539:11 - At this point, instead of writing all this here,
539:16 - we're just going to call or let's take from this point.
539:19 - So we have our feature.
539:22 - Let's look at the name.
539:23 - We gave it, we gave it the name feature extractor model.
539:26 - So here we have feature extractor model.
539:31 - So here's our feature extractor model.
539:33 - So we take all this off.
539:35 - And then in here we pass in our input.
539:40 - So notice how we are making this model look like a function.
539:45 - So TensorFlow models are callable,
539:49 - just like the layers.
539:52 - And as you could see here,
539:53 - this feature extractor model could be seen as a layer,
539:58 - just like the dense layer, the batch norm layer
540:01 - and all other layers.
540:03 - So we've gotten this X from this input,
540:05 - which has been passed in our model.
540:08 - And then from here, you see,
540:09 - we pass this X into this flatten and we have the rest.
540:13 - So that's it.
540:14 - Let's now rerun this again.
540:16 - So you could see what we get as output.
540:19 - And as you could see,
540:20 - we get exactly what we expected.
540:23 - We have the same number of parameters
540:25 - and there is this difference here
540:27 - where we have this feature extractor.
540:30 - So unlike before where we had the conf nets,
540:34 - like the conf 2D batch norm max pooling
540:38 - and the same, like let's go up here.
540:41 - There's actually a feature extractor.
540:43 - So unlike before where we had this and then this,
540:46 - now it has been replaced with this feature extractor,
540:50 - like right here.
540:51 - That said, we've just built this model
540:55 - using the functional API.
540:56 - And in subsequent sections,
540:58 - we'll build even more complex models
541:01 - using this functional API models,
541:03 - where we're going to use shared layers.
541:06 - We're going to have multiple inputs, multiple outputs
541:09 - and models where we're going to have
541:11 - even more complicated model configurations.
541:14 - It's important to note that you could mix up
541:17 - the functional API model creation style
541:20 - with that of the sequential API.
541:23 - So you're, instead of creating this,
541:26 - so instead of having this of feature extractor
541:29 - created like this,
541:30 - we are going to create it using the sequential API.
541:34 - Let's add that.
541:35 - And then we copy out this from your,
541:38 - copy out this full model with a sequential API,
541:42 - this is out and we take all of the feature extraction part.
541:47 - Here, you see, we take this off
541:49 - and then we're left only with this feature extraction part.
541:52 - Now let's call this feature extractor.
541:57 - So feature extractor, sequential model.
542:02 - There we go, this is out right here and we're fine.
542:07 - So we have our feature extractor model.
542:10 - We run that, that's okay.
542:12 - Let's take this off and then we'll just make sure
542:17 - we put exactly the same here.
542:21 - So there we go, we paste it out and we rerun this.
542:25 - You see, we should be able to get exactly the same output.
542:28 - See, we have exactly the same output and here,
542:31 - instead of our feature extractor model,
542:33 - we have your sequential layer.
542:36 - So that's it.
542:36 - This shows us that we could mix up
542:40 - these different ways of creating models.
542:44 - From this point, we'll look at the model subclassant.
542:48 - So right here we have our model subclassant.
542:54 - There we go.
542:55 - It's important to note that model subclassant permits us
542:59 - to create recursively composable layers and models.
543:04 - Now, what does that mean?
543:08 - This means I could create a layer
543:11 - where its attributes are other layers
543:15 - and this layer tracks the weights
543:20 - and biases of the sub layers.
543:24 - Before taking an example, let's make this import.
543:28 - So we're going to import layer from layers.
543:31 - We have tensorflow.keras.layers.
543:35 - We're going to import layer.
543:38 - We run that and then we move on
543:40 - to create our model using the model subclassant.
543:50 - Now that's set, we have this feature,
543:53 - our feature extractor.
543:56 - So we have feature extractor right here
543:59 - and then this inherits from layer.
544:03 - So inherits from tensorflow layer
544:06 - and then we have an init method
544:11 - and followed by a call method.
544:15 - So that's it.
544:16 - Now let's use the right syntax.
544:18 - Here's a class, here's a method, init method.
544:23 - There we go.
544:25 - You could always check on our free cars on Python programming
544:30 - in case you're not versed with all the syntax.
544:33 - So that's it.
544:34 - We've had that.
544:35 - And now just like the way we did
544:38 - when we were creating this feature extractor,
544:40 - let's go back to our feature extractor
544:43 - with a functional API actually.
544:45 - Here you'll see, let's copy this out
544:49 - and then get back to our model subclassant.
544:52 - So that's it.
544:53 - Here, let's just put this down here.
544:58 - And then we have the super feature extractor.
545:05 - There we go.
545:06 - That init, so that's it.
545:10 - Now that we have defined this,
545:12 - we could now go ahead and use this layers
545:16 - as the attribute for this feature extractor layer.
545:21 - So there we go.
545:22 - We have this here.
545:24 - We have our conf, the self.self.conf one,
545:30 - which is this conf2d right here.
545:33 - So we take all this off and just place it here.
545:37 - And then from here we have the batch batch one,
545:43 - which is this batch norm layer right here.
545:46 - Take this and then place it right here.
545:49 - We have the max pull2d, take that off.
545:53 - We have self.pull one, there we go.
545:59 - So that's it.
546:00 - And then we just repeat this process.
546:03 - So we could have the self.conf one,
546:06 - conf two rather, so conf two.
546:09 - And then we just take up this parameters.
546:13 - Let's take this from here and place it right here.
546:19 - So here instead of having this, we will just get this.
546:22 - And then the batch norm export2d remain the same.
546:25 - So that's it for our init method.
546:27 - We now go ahead to build our call method.
546:31 - We have our call method here, which takes this input x.
546:35 - And then here, what it does is it permits us
546:40 - call each and every layer defined here in this init method.
546:48 - So here we have x equals self.
546:52 - So the value of x is going to change.
546:53 - We're going to pass it through self.conf one
546:56 - and self.conf one, we're not taking x.
546:59 - This looks similar to the functional API.
547:02 - We have x equals self.batch one, there we go.
547:07 - Batch one and then we have x equals self.pool one,
547:16 - x equals self.conf two, x and then self.batch two.
547:29 - And finally we have self.pool two, that's it.
547:34 - That's it.
547:36 - So now from here, we just return x.
547:38 - Also note that we could pass in a parameter,
547:42 - an argument like the training,
547:44 - which can tell us whether to use a given layer
547:48 - or not during the training process.
547:52 - Nonetheless, for now, all this is going to be
547:54 - used during training.
547:55 - So we have that.
547:56 - Now let's take all this off.
548:00 - We run that.
548:01 - There we go.
548:02 - That's run correctly.
548:05 - Now we should be able to build our model.
548:07 - So here we have this Lumenet model,
548:10 - which took the feature extractor model from here.
548:13 - Let's just copy this.
548:15 - And then, but before copying that,
548:17 - we have to ensure that we create this here.
548:21 - So we have feature subclassed.
548:25 - So we have our feature subclassed,
548:26 - which is this feature extractor viewed right here.
548:31 - So yeah, we have feature extractor built.
548:36 - So that's it.
548:37 - Now you could always pass this parameters,
548:40 - like the number of filters, the kernel size via this.
548:44 - So we could pass this here.
548:46 - You could specify filters and the kernel size.
548:50 - So let's just do that.
548:51 - So let's say filters takes kernel size,
548:55 - stripes and say padding and activation.
549:02 - So if we could pass all this here,
549:04 - such that when we get to this point,
549:06 - we just, we don't need to specify all this.
549:09 - So we'll simply take this off.
549:13 - We don't need to specify all this anymore.
549:16 - Now we have activation, all that's specified.
549:20 - Okay, we could also include the pool size.
549:23 - Let's include the pool size and all the strides here.
549:29 - Okay, so let you say we're gonna have two times the strides
549:32 - because we always specify the strides to be one.
549:35 - So let's take this off.
549:37 - Yeah, we're gonna have two times the strides
549:40 - and we've defined, here we have the pool size.
549:43 - So let's take this off and let's take this off rather.
549:50 - Let's get back.
549:51 - So we take this off and take this off.
549:55 - So that's it.
549:56 - We've defined all that.
549:57 - And now we are ready to pass all these values.
550:01 - So just simply copy all those values.
550:04 - And then in here, we specify the number of filters.
550:08 - So yeah, let's take eight, the kernel size.
550:11 - Let's take three, the number of strides.
550:14 - We have one, the padding.
550:16 - Here is valid, valid activation.
550:20 - ReLU pool size is two.
550:25 - So that is it.
550:26 - So we've defined all this.
550:28 - Now at this point, let's ensure that we have this times two.
550:32 - So let's run this now again.
550:34 - And then we have our feature extractor
550:39 - that we're getting this error.
550:41 - Let's try to understand why and how to solve this error.
550:46 - So there we go.
550:47 - Scrolling, we have this.
550:49 - Now, the reason why we have this error
550:51 - is because of the order
550:53 - in which this come to the Texas arguments.
550:58 - So if you look at this, trying to get this to come up.
551:03 - Anyway, we just look at the documentation.
551:05 - You see, we have filters, kernel size, strides,
551:08 - padding, data format.
551:10 - So you see, we have the data format,
551:12 - the duration rate groups before the activation.
551:16 - So it's important that we specify
551:19 - that this is filters equal filters.
551:22 - Then you will specify kernel size equal kernel size.
551:26 - If not, it's gonna take this to be the data format.
551:29 - So that's it, strides, padding equal padding,
551:34 - and activation equal activation.
551:38 - So this should be fine now.
551:39 - We'll run that again.
551:42 - We have this error,
551:44 - but this time around is for the second conf layer.
551:46 - So let's just redo what we had done here
551:50 - and take this off.
551:52 - Oh, notice filter size times two.
551:54 - So here we have filters times two.
551:58 - We try to do the same for the max pool 2D.
552:01 - So here we have pool size equal pool size.
552:08 - That's it, strides, that's fine.
552:11 - And then yeah, we have pool size equal pool size.
552:16 - Okay, so now everything should work fine.
552:19 - We run that and there we go.
552:22 - Everything works fine.
552:24 - So now we've created our layer, our feature subclass layer.
552:29 - We will now be able to use it in this model right here.
552:34 - So let's copy that and then get back to this.
552:39 - Oh, this is out here.
552:41 - And now here we have our feature subclassed.
552:44 - Let's take all this off subclassed.
552:48 - And then we could comfortably run this
552:51 - and we get in this error.
552:54 - Now let's get back to check.
552:56 - We see batch two, there's an error level of batch two.
553:00 - Now we see here, we have this two,
553:02 - there should be two and there should be two.
553:05 - So that's fine.
553:06 - Let's run that again and everything works well.
553:10 - So you see, it gives us exact same output
553:13 - we expect to get with this feature extractor right here.
553:17 - Then one last thing we could do is
553:20 - instead of doing this way, let's insert some code.
553:23 - Instead of doing this way, we are going to create a model
553:27 - using this model subclassing method.
553:31 - So yeah, we just copy this out.
553:35 - So we copy that out.
553:37 - And then in here, instead of having layers,
553:41 - so yeah, we're not having a layer, we're having a model.
553:44 - So instead of having layer, we now have model
553:47 - and then we're going to define a feature extractor.
553:53 - So our feature extractor now is going to be
553:57 - the feature extractor we've just defined here.
554:00 - So let's go up and there we go.
554:03 - So we're going to get this feature extractor right here.
554:07 - And then what we'll do is we put it in here.
554:11 - So let's take all this off.
554:14 - We have all this off.
554:16 - That's now our feature extractor.
554:17 - So we've got in this feature extractor.
554:21 - That's it.
554:22 - And then once we get X,
554:23 - we're going to pass this to our feature extractor.
554:27 - There we go.
554:29 - Let's take all this off and we'll find.
554:33 - Now we're done with the feature extraction.
554:35 - We could get the other parts which make up the model
554:38 - like this pattern, the dance and the back norm.
554:43 - So let's take this here.
554:45 - There we go.
554:47 - We are in this model and let's just copy it.
554:50 - Let's paste it out here.
554:52 - So here we're going to have this.
554:55 - Also note that we're going to have feature,
554:57 - or let's say, LearnNet.
555:00 - This is our LearnNet model.
555:03 - Modify this here.
555:04 - We have the LearnNet model.
555:07 - LearnNet model.
555:08 - So that's fine.
555:09 - Now we've gotten this.
555:11 - Everything is understood.
555:12 - We now check out this self.flatten, equal flatten.
555:19 - And then next we have self.dance1 equal,
555:24 - let's copy this out from here simply.
555:27 - So we have this dance right here,
555:30 - which is our dance one.
555:32 - There we go.
555:36 - We have our dance.
555:38 - And then we have self.batch1,
555:43 - which is our batch normalization.
555:47 - There we go.
555:49 - Copy this paste.
555:52 - Move to dance2 and batch2.
555:55 - So here we have dance2, we have batch2.
555:58 - This is 10 actually.
556:00 - So we have that.
556:01 - And then finally we have this dance layer.
556:04 - So let's take this off.
556:05 - We have the dance layer.
556:07 - Output one, activation sigmoid.
556:12 - So that's it.
556:13 - Oh, we call this dance3.
556:15 - So that's fine.
556:16 - Everything seems okay.
556:18 - Let's take this off now.
556:20 - And then there we go.
556:22 - We get into our call method.
556:24 - So yeah, we get into this call method.
556:26 - And this call method will basically
556:29 - call all these different layers.
556:31 - So after the feature extraction,
556:33 - notice how we've created this class.
556:36 - And this class makes use of this feature extractor,
556:41 - which was also created using the same model
556:43 - subclassing method.
556:45 - So there we go.
556:46 - We're using this here and we're actually using it here.
556:50 - So that's it.
556:52 - Now we just make this calls.
556:54 - So we have x equal self.flatten.
556:59 - And then we pass x.
557:01 - x equal self.dance1 as in x.
557:09 - x
557:14 - x equals self.batch1 as in x.
557:20 - And then finally we have this.
557:22 - So there we go.
557:23 - We now return x just as we did previously,
557:26 - and we have our model.
557:27 - So here we have our lunette model.
557:33 - Lunette subclass model.
557:37 - And we have this lunette here, lunette model.
557:43 - So let's take this off and there we go.
557:46 - We've just built our model,
557:48 - which when we try to find a summary,
557:51 - so we try to do lunette subclass.summary.
557:57 - What do we obtain?
558:00 - See, this model has not yet been built.
558:04 - Do the model first by calling build
558:06 - or by calling the model on a batch of data.
558:08 - So we're going to call this model on a batch of data.
558:11 - Right here, we're going to have lunette subclass.
558:14 - And then we have tf.zeros, tf.zeros,
558:19 - and one, two, two, four, two, two, four,
558:24 - and three.
558:25 - So let's run this and see what we get.
558:28 - That's fine.
558:29 - So yeah, we have our summary
558:31 - and then we are ready to compile this model.
558:35 - Yeah, we have lunette, lunette subclass.
558:40 - So we compile lunette subclass.
558:42 - And then we're going to fit lunette subclass,
558:45 - lunette subclass.
558:48 - So we feed that and everything should work fine.
558:51 - Let's take this to just five ebooks.
558:55 - You can see that we're getting similar results
558:57 - when we compare this with what we have
558:59 - with a functional API and the sequential API.
559:02 - So we now move on to creating custom layers.
559:08 - If you could recall from the previous sections,
559:11 - the way the dense layer is built
559:15 - is such that if we have this as our dense layer
559:19 - and then we have this input right here,
559:22 - let's call this I, or let's say it's X,
559:25 - we have this input X,
559:26 - then we have a certain M times X plus C.
559:34 - So this M is actually the weights.
559:36 - So the weights times X plus the bias,
559:41 - let's call this B and then this is now equal our output.
559:50 - So here we have an output of Y, Y equals MX plus C.
559:53 - So the symptom means if we want to recreate the dense layer,
559:59 - then we have to take this into consideration
560:02 - or the definition of the layer from scratch
560:06 - into consideration.
560:07 - That said, we could define a neural learn dense.
560:12 - So it's like our custom dense, neural and dense.
560:16 - It's gonna inherit from layer
560:21 - and then right here, this class.
560:23 - So we have this class inherits from layer.
560:27 - Then from here, we have our init method, init.
560:32 - init and then we have the super
560:37 - with pass and neural learn dense.
560:41 - There we go, we have that, that init.
560:46 - So that's it.
560:48 - Now from here, if you had noticed,
560:50 - whenever we're creating a dense layer,
560:52 - like let's call up here,
560:54 - whenever we're creating a dense layer,
560:56 - we generally had to specify at least
561:00 - because this is by default.
561:02 - This is not by default, but.
561:05 - So here we need to pass in this year
561:08 - to specify the number of output units.
561:12 - Now that said, we have to take that into consideration
561:14 - when building our neural and dense layer.
561:18 - So here we have output units.
561:21 - There we go, we have our output units.
561:24 - And then here we'll define self dot output units
561:29 - to be equal output units.
561:33 - So that's it.
561:34 - Now from this point, we're gonna build this layer.
561:38 - In order to build this layer,
561:39 - we have to take into consideration
561:41 - this definition right here.
561:45 - But this definition put out the way this isn't very clear.
561:49 - Now let's make this, or let's break this up.
561:52 - So you suppose now we have this input
561:54 - of shape batch size by let's say number of features.
562:00 - So suppose we have this input.
562:03 - Now what happens here is this input
562:07 - is gonna be multiplied by this weights.
562:11 - So it's gonna multiply by this weight,
562:12 - which happens to be a matrix.
562:15 - Now we take this and multiply by that matrix.
562:19 - And for the multiplication to be valid,
562:21 - we need to ensure that the number of columns we have here
562:25 - matches with the number of rows of this matrix.
562:28 - But then what are the dimensions of this matrix?
562:32 - We have to note that this matrix has to be defined
562:36 - such that we have a shape of F.
562:43 - This F here must match by the number of output units.
562:48 - So if you want a number of output units, for example,
562:51 - to be one, then here you should have one.
562:54 - And that's why when defining the dense layer,
562:57 - we don't need to specify this as this value
563:03 - is gotten automatically from the number of columns
563:07 - in the inputs since if we don't take this,
563:10 - we are gonna have an error.
563:13 - So TensorFlow test takes this automatically piece in here
563:17 - and then collects the input you pass in the dense.
563:21 - So when you specify, when you say you have this dense
563:25 - like this, and then we say, for example, one
563:28 - and maybe some activation, then you have that.
563:31 - So let's say we have some activation here.
563:33 - Now, once this one gets here,
563:36 - this weights matrix is now defined
563:39 - such that the input you pass into this dense
563:42 - is gonna affect the number of rows we have.
563:46 - But then what you pass in as argument here
563:49 - is gonna tell us or give us a number of columns
563:52 - we're gonna have for this weight matrix right here.
563:55 - So that said, we have F by one.
563:59 - And then when we multiply this, we're gonna have B by one.
564:04 - So we see, we now understand how we get this output.
564:07 - Then we have plus B by one.
564:12 - So that's it.
564:12 - Where this one comes from the bias.
564:17 - Now, once we add this up, we have an output of B by one.
564:21 - And that's our Y.
564:23 - That's the shape of our Y.
564:25 - If that's understood, we'll move on to building.
564:28 - So here we have our build method.
564:31 - We have self and for now let's keep it that way.
564:35 - So there we go, we have this build method.
564:37 - And then we're gonna define our weights.
564:40 - So we have self.weights.
564:42 - Let's specify weights, we call that.
564:45 - And then here we're gonna have the self.addWeights method.
564:51 - So this actually comes with this layer class right here.
564:56 - So we're able to call this because we are inheriting
564:59 - from the layer class.
565:00 - So here we have self.addWeight.
565:02 - And now we specify the shape.
565:04 - So guess what?
565:05 - We are gonna have our number of rows.
565:10 - So n rows, which is gonna come from the inputs.
565:13 - And then we're gonna have our number of columns,
565:16 - which is gonna come from this output units we specify.
565:20 - We should pass in when calling the neural and dense layer.
565:25 - So here we're gonna have self.outputUnits.
565:29 - So we get a number of output units.
565:32 - Now, how do we obtain this number of input units?
565:35 - We're gonna look at that shortly.
565:36 - For now we have the weights.
565:38 - And then we have that.
565:40 - And then let's go to bias.
565:42 - So we have self.biasis.
565:45 - And self.this is also a weight.
565:48 - Add weights.
565:51 - And then we specify just the number of output units
565:56 - since it's one dimensional.
565:57 - So we have output units.
566:00 - There we go.
566:01 - And so at this point we'll define our weights
566:07 - and our bias matrix.
566:09 - This actually weights.
566:11 - Just similar.
566:12 - So that's it.
566:13 - Now let's go into call.
566:15 - So we have our call method.
566:17 - We should actually get a job done.
566:20 - And then we have our input.
566:22 - So let's call this, yeah, input.
566:26 - Let's add an S.
566:28 - Or let's say input data.
566:29 - Or input features.
566:32 - So there we go.
566:33 - So we have that.
566:34 - And then what we're gonna do here is
566:36 - we're gonna return simply the matrix modification
566:41 - as we've seen already of the weights.
566:44 - So we have self.weights.
566:46 - And this input features.
566:51 - It's actually the input features times the weights.
566:55 - Let's get back to this.
566:56 - Here if we take the input, yeah, and it takes input
567:00 - times the weights.
567:01 - Because if we have the weights times the input
567:03 - we will have F1 times the F.
567:07 - We call our weights this, our weight shape
567:11 - and then this our input shape.
567:13 - More than this, you see that this is always the same.
567:16 - So it's not gonna be, you're gonna try an error.
567:20 - And so that's set.
567:21 - What we're gonna do is we're gonna just simply have
567:24 - input features right here.
567:28 - So that's it.
567:29 - And then we add up the bias.
567:31 - So here we have self. biases.
567:36 - Which is from this one right here.
567:39 - Now the way we're gonna get this number of rows
567:41 - is gonna be easy.
567:43 - Yeah, all we need to do is to specify here
567:45 - that we have the input feature, features, shape.
567:51 - We have our input features shape.
567:53 - Which is gonna come automatically from this.
567:56 - And then to get the number of rows,
567:59 - all we need to do here is have input, features, shape.
568:05 - And then we get that last dimension.
568:09 - So let's get back to this.
568:11 - We call, here we have an E by F plus E, F.
568:17 - And so here we need the weights,
568:20 - it needs to be F by the output.
568:24 - So to get this F, we just need to take the input
568:28 - and then get this last element right here.
568:31 - And there is the number of columns we have here.
568:36 - So that's set.
568:37 - We will just have that input features.
568:40 - Specify this, take this index.
568:43 - And that will be good.
568:44 - So that's how we get, we obtain this value automatically.
568:48 - The number of rows we've been looking for.
568:51 - So now we've gotten this, everything seems fine.
568:54 - The next thing to do is to specify that it's trainable.
568:58 - So we have to specify that these weights are trainable
569:00 - because in some cases we may want
569:02 - that the weights shouldn't be trainable.
569:04 - So let's have the trainable equal true.
569:08 - Now there's no self here, there is an argument.
569:11 - Like it's one of the arguments which I've been passing
569:14 - to this add weight method right here.
569:17 - So we would have that.
569:18 - And then there again, we have this trainable equal true.
569:24 - Now apart from that,
569:25 - we could randomly initialize our weights in biases.
569:30 - So here we have random, normal,
569:35 - random normal initialization, it's fine.
569:40 - Then it is same here.
569:41 - So we have our initializer equals random normal.
569:50 - That's good.
569:51 - We now run this.
569:53 - There's our neural learn dense layer.
569:57 - Then after running this, let's get to integrate this.
570:01 - Let's make it quite simple.
570:03 - We just use our sequential API.
570:06 - So let's get back to this sequential API
570:08 - we have built initially, copy that.
570:11 - And then we are going to integrate this new dense layer,
570:15 - this neural learn custom dense layer.
570:18 - So there we go, we have that.
570:20 - And then instead of dense layer,
570:22 - here we have neural learn custom dense layer.
570:25 - So you see that you can be able to create your own layers
570:30 - with TensorFlow, that's it, neural learn.
570:34 - So neural and dense, neural and dense.
570:37 - Now you see, this should try an error
570:38 - because we don't take into consideration the activation.
570:42 - And so what we could do is,
570:44 - we could get back right here and say,
570:48 - if the activation is equal relu,
570:54 - if activation is equal relu, we'll return this.
570:58 - And then else, let's say elif activation
571:03 - is equal sigmoid, return that, let's get back.
571:13 - We have this.
571:14 - So we're going to return this, let's copy this
571:18 - and then paste out right here.
571:20 - And then we have els, we will do the same.
571:23 - So now let's get into this
571:24 - and see the modifications we're going to make.
571:27 - There we go.
571:28 - So here we have our relu.
571:30 - In case it's relu, what we want to have here is
571:34 - tf.nn.relu of that.
571:39 - So we're going to pass this in to our relu.
571:41 - And then if the activation is sigmoid,
571:44 - we should have tf.mat.sigmoid.
571:49 - So that's it.
571:50 - There we go, we have that.
571:52 - And then this one we just maintain.
571:54 - So we have modified our code
571:57 - such that we now integrate the activations.
572:00 - There we go.
572:02 - That seems fine.
572:03 - So let's run this and then take this error.
572:10 - So here we have this double equals,
572:14 - find some syntax.
572:16 - And then we come down here,
572:18 - return seems fine.
572:21 - Let's take this again.
572:23 - Here we're supposed to have,
572:24 - we're supposed to specify activation.
572:26 - So we're supposed to have activation.
572:29 - And then salve.activation for activation.
572:35 - And then here we have salve.activation,
572:39 - salve.activation.
572:41 - We run that again, it's fine.
572:44 - And then now what we do is we pass,
572:46 - we just simply run this.
572:48 - So run that and we get in this error right here.
572:55 - To solve this problem,
572:56 - what we're gonna do is we're gonna include the shape here.
573:00 - So we're gonna have shape equal that
573:02 - and then shape equal this.
573:05 - Let's rerun that and we should have no error again.
573:09 - Oh yeah, we have in cancer attributes weights,
573:12 - likely because it conflicts with an existing read
573:15 - only property of the object.
573:18 - So right here, instead of using weights,
573:20 - we're just gonna say W.
573:22 - So we have the W and then,
573:25 - right here, we should have W.
573:28 - Now, since we don't want to repeat this over,
573:30 - what would you say we have a pre output.
573:35 - So we have our pre output, which is this here.
573:41 - And is that out?
573:43 - And then here we have W.
573:46 - Right here, we have B.
573:48 - That's it.
573:49 - And now we have, if this,
573:51 - then we pass the pre output.
573:55 - And we do the same right here.
573:58 - So we have just a pre output before the activation.
574:04 - Now, yeah, we just have pre output actually.
574:06 - So we just have pre output to take that out.
574:09 - Now running this should be fine.
574:11 - So let's run that and see what we get.
574:13 - We run this, we run that, and there we go.
574:16 - We have our model, the same exact model
574:19 - we have been building right from the start.
574:21 - But this time around, we're having,
574:22 - we're using a custom dense layer,
574:25 - which is our neural and dense layer.
574:28 - Now let's go ahead and compile this model
574:31 - and then train it.
574:34 - Yeah, we've actually maintained this year.
574:38 - So we have to change this name.
574:41 - Here's Lynette model.
574:43 - Let's say Lynette custom.
574:46 - Yeah, custom model.
574:50 - Yeah.
574:51 - Okay, we copy that.
574:54 - We have all the net custom model.
574:57 - Let's have it here.
575:00 - There we go.
575:01 - We run it.
575:03 - That's fine.
575:05 - Right here, we have Lynette custom.
575:07 - We run that.
575:09 - Yeah, we have Lynette custom.
575:11 - We run that.
575:12 - And then let's check out.
575:15 - As you can see with the train,
575:16 - there's a slight difference in the last values.
575:20 - And accuracy we're getting for this first epoch.
575:24 - And most probably, this is coming from
575:26 - this random normal initialization we've chosen here
575:31 - as with the standard dense layer,
575:35 - which you could see here.
575:36 - This kernel initialization,
575:39 - or the weight initialization is using this blurrow uniform.
575:44 - And the bias initialization is the zeros.
575:47 - So here we have all zeros for the biases.
575:50 - And then here we use the blurrow uniform method.
575:53 - So yeah, as you could see,
575:54 - this is in performing as well as what we had previously.
575:57 - But scrolling up, there is this error.
576:00 - So yeah, it was meant to be sigmoid.
576:02 - So let's stop this.
576:05 - Let's interrupt this training.
576:07 - And then get back and run this.
576:11 - So we will run this and compile
576:15 - and then start with the training.
576:17 - Again, this time around, it looks better.
576:20 - It looks more like what we should expect.
576:23 - So even though we're not using
576:24 - the blurrow initialization method,
576:27 - this training process looks quite similar
576:30 - to what we would have had in the case
576:32 - of the blurrow initialization method,
576:34 - which is used with the standard dense layer.
576:37 - And here is what we get after training for over five epochs.
576:41 - Thank you for getting up to this point
576:43 - and see you next time.
576:45 - Hello, everyone.
576:49 - And welcome to this new section
576:52 - in which we will look at other methods
576:54 - of evaluating our model
576:56 - other than the binary accuracy,
576:58 - which will be seen so far.
577:00 - So in this section,
577:01 - we'll look at how to compare the true positives,
577:04 - false positives, true negatives, false negatives,
577:07 - the precision, the recall, the area under the curve,
577:11 - how to come up with confusion metrics like this,
577:14 - and finally, how to plot out an ROC curve like this one,
577:19 - which permits us select the threshold more efficiently.
577:23 - Don't forget to subscribe
577:25 - and hit that notification button
577:26 - so you never miss amazing content like this.
577:30 - Let's now look at other ways of evaluating our model
577:34 - other than the accuracy, which we've seen so far.
577:38 - To better understand why working with other accuracy
577:43 - isn't always a great idea,
577:45 - we have to take into consideration the fact that
577:48 - our model on a test set has 94% accuracy.
577:55 - Now, this means that we have
577:59 - six out of 100
578:03 - predictions which are actually false.
578:07 - Now, what if I get to the hospital
578:10 - and I'm told that I don't have malaria
578:14 - when in fact I actually have this disease?
578:18 - So that said, the model predicts uninfected
578:22 - and I actually have the parasite in my bloodstream.
578:27 - Now, this particular situation becomes very dangerous
578:31 - because the patient gets by home thinking
578:34 - he or she doesn't need any treatment,
578:36 - whereas that patient actually has this parasite.
578:41 - You see that even with a 94% accuracy,
578:45 - we wouldn't be able to save ourselves
578:47 - from such chaotic model predictions.
578:51 - Now, in another example, you have a situation
578:55 - where the actual is, let's put it out here.
578:58 - So in another example, we can have a situation
579:00 - where the actual is unparasitized.
579:03 - So actually you do not have this parasite,
579:07 - but the model predicts that you have the parasite.
579:11 - Now, in this case, although we have a wrong prediction,
579:17 - we have actually a less chaotic situation
579:20 - as compared to this previous case here,
579:23 - since at least actually you are uninfected.
579:28 - If we consider negative, that's this negative
579:33 - to be uninfected and positive
579:39 - to contain the parasite, that's P.
579:44 - So here we consider we have positive
579:46 - and then we have negative.
579:48 - So here we have negative, uninfected
579:50 - and positive parasitized.
579:53 - Now, if we let this, you will find that this first situation
579:56 - where actually we have the parasite
580:00 - and the model predicts unparasitized
580:02 - is known as a false negative.
580:08 - So here we have this false negative.
580:13 - And this is because the model predicts negative
580:16 - when it isn't actually negative.
580:19 - So since we have this wrong prediction for negative,
580:24 - we call it a false negative.
580:27 - And in this case where we have the model predicting
580:31 - parasitized, that is positive, when it's actually negative,
580:35 - we call this a false positive.
580:38 - So here we have FP and here we have FN.
580:44 - Now, there are two other scenarios
580:47 - that is with TN and TP.
580:51 - For the TN, we have the true negatives
580:55 - and the TP, the true positives.
580:57 - For the TN, we have the model predicting negative
581:02 - when actually we are negative.
581:04 - That is, we have the model saying this is uninfected
581:08 - when actually it's uninfected.
581:10 - So that's a true negative.
581:11 - And then for the true positive,
581:13 - the model predicts a positive that's parasitized
581:17 - when actually it's parasitized.
581:19 - So that's a true positive.
581:21 - Now, hopefully you've understood the concepts
581:23 - of true negatives, true positives, false negatives,
581:27 - and false positives.
581:30 - We can then summarize all this information
581:33 - in this matrix known as the confusion matrix.
581:37 - This confusion matrix, we have the true negatives,
581:39 - the number of true negatives here,
581:41 - the number of true positives,
581:43 - the number of false negatives,
581:45 - and the number of false positives.
581:48 - This means that if we have a test set of say,
581:52 - 2,750 different data points,
581:57 - and then we run this or we evaluate this with our model,
582:01 - we'll be able to get this number of true negatives,
582:05 - get this number of false negatives,
582:06 - number of true positives, number of false positives,
582:10 - and hence better evaluate this model.
582:14 - So if we take this example where we've evaluated our model
582:18 - on the test set, and then this model A produces
582:21 - this confusion matrix, and this model B produces
582:25 - other confusion matrix, where here we see we have
582:27 - for the true negatives and true positives,
582:29 - we have 1,000, 1,000, that's 2,000 correctly
582:33 - predicted data points, and then here we have 1,000,
582:37 - 1,000, 2,000 correctly predicted data points.
582:40 - And then for this model A, we have 700 false positives.
582:45 - And 50 false negatives.
582:51 - Whereas for model B, we have 50 false positives
582:55 - and 700 false negatives.
582:58 - Recall we had defined negative to be uninfected
583:01 - and positive to be parasitized.
583:04 - And hence, if we had to choose a model between A and B,
583:09 - we'll try to choose that model which minimizes
583:13 - the number of false negatives.
583:15 - So we are not saying that we shouldn't minimize
583:20 - the number of false positives because we have to try
583:23 - to minimize all the false predictions.
583:27 - But then, since with the false negatives,
583:30 - we are telling a sick person that he or she isn't sick,
583:36 - this at least is worse than telling a healthy person
583:41 - that he or she is sick.
583:43 - And so we'll try to prioritize the number
583:46 - of false negatives and based on this prioritization,
583:50 - we are gonna prefer model A since we have the smaller
583:55 - number of false negatives.
583:57 - And so here, we'll choose model A over model B.
584:02 - Now, as a quick note, you may decide to say
584:05 - no negative is for parasitized and positive is for uninfected.
584:10 - It isn't a must that this must be tied together like this,
584:16 - but for clarity purposes, it's better to look at it this way
584:20 - since saying you are tested negative means you're uninfected
584:23 - and tested positive means you're parasitized.
584:28 - Now, you should also note that depending on the kind
584:31 - of problem you want to solve, in some cases,
584:34 - you will want to prioritize minimizing the number
584:37 - of false positive over the number of false negatives.
584:41 - So this actually depends on the problem you're trying
584:44 - to solve.
584:46 - But in this case, we are prioritizing the number
584:50 - of false negatives.
584:51 - Now, based on what we've seen so far,
584:54 - we're gonna introduce several new performance metrics.
584:59 - And others take up this different formulas
585:02 - we could see right here.
585:04 - We have the precision, which is the number of true positives
585:08 - divided by the number of true positives,
585:10 - plus the number of false positives.
585:11 - Recall, true positives divided by a number
585:15 - of true positives plus a number of false negatives.
585:17 - The accuracy, the number of two negatives,
585:20 - plus number of true positives divided by number
585:23 - of true negatives plus true positives
585:25 - plus false negatives, plus false positives.
585:27 - So we'll stop in this first three for now.
585:32 - Now, what do you notice?
585:33 - You'll notice that in this position and recall,
585:36 - we have this true positive, true positive,
585:39 - and here true positive, true positive.
585:42 - What differentiates them is the fact that
585:44 - in the position we have false positive in the denominator,
585:47 - and in the recall we have false negative in the denominator.
585:51 - This means that if the number of false negatives is high,
585:56 - that is we have, let's say we have a constant,
586:00 - a constant K divided by a high value.
586:05 - So constant divided by a high value.
586:08 - So constant divided by high.
586:10 - Here we're going to have a low output.
586:12 - And so if we want to have a low recall,
586:16 - then we need to have a high number of false negatives.
586:20 - And if we want to have a low position,
586:23 - then we need to have a high number of false positives.
586:28 - Now in our case, we're trying to minimize
586:30 - the number of false negatives.
586:34 - And since we're trying to minimize
586:35 - the number of false negatives,
586:36 - it means that we're trying to maximize the recall.
586:41 - Since minimizing this denominator,
586:43 - we're until maximizing this overall TP on TP plus FN.
586:50 - And so here we're trying to prioritize
586:53 - the recall over the position.
586:56 - Now, if you look at the accuracy,
586:58 - you'll notice that we have TN plus TP
587:02 - and TN plus TP right here.
587:04 - TN plus TP, TN plus TP, and we have FN plus FP.
587:08 - If you are keen enough, you should see that
587:11 - this accuracy doesn't give any parity
587:16 - for whether the false negatives are the false positives.
587:20 - It treats these two as the same.
587:23 - But as we've seen previously in the real world,
587:26 - in solving real world problems,
587:28 - many times we'll have to prioritize.
587:31 - Hence, the accuracy may not always
587:34 - be the best metrics for our problem.
587:37 - In our case, we find that using the recall
587:43 - is even better than using the accuracy.
587:46 - As with the recall, we get to reduce or,
587:51 - with the recall, we get to see
587:53 - whether our model does well at minimizing
587:59 - the number of false negatives.
588:03 - Now, we also have this F1 score,
588:05 - two times the precision times recall,
588:07 - this precision is recall,
588:08 - divided by the precision plus recall,
588:10 - the specificity, the number of true negatives
588:14 - divided by number of true negatives
588:15 - plus number of false positives.
588:18 - And then we also have this ROC plot right here,
588:21 - ROC stands for receiver operating characteristics.
588:25 - Here we have the true positive rate
588:27 - and the false positive rate.
588:29 - The true positive rate is the number of true positives
588:33 - divided by number of true positives
588:34 - plus number of false negatives,
588:36 - which happens to be the recall.
588:39 - And then the false positive rate
588:41 - is the number of false positives
588:42 - divided by number of false positives
588:44 - plus the number of true negatives,
588:47 - which if you look carefully, you'll find that is equal.
588:50 - One minus the specificity,
588:53 - which has been defined right here.
588:55 - Before getting to understand this ROC plot,
588:59 - which we've put out here,
589:01 - let's recall this two models,
589:05 - which we had described previously,
589:07 - that's model A and model B,
589:09 - where model A had a smaller number of false negatives
589:15 - as compared to model B.
589:17 - Now, if we pick out just this model A
589:22 - and then we are interested in reducing this,
589:26 - or let's say we pick our model B.
589:28 - Suppose we pick our model B,
589:29 - I will interested in reducing this number
589:33 - of false negatives right here.
589:36 - Then one solution could be that of modifying the threshold.
589:42 - So if we have a threshold of 0.5,
589:45 - meaning that above, like we have the 0.5,
589:49 - below 0.5 we consider negative,
589:52 - above 0.5 we consider positive,
589:54 - that is pasteurized and then below uninfected.
589:58 - Then what I could do here is reduce the threshold.
590:03 - So if I take the threshold to say value of 0.2,
590:07 - let's take this here.
590:08 - So I've reduced the threshold now to 0.2.
590:11 - You see that for most of the predictions,
590:16 - our model is going to say that this
590:18 - is a parasitized output
590:22 - since now the threshold has been reduced.
590:25 - This means that if we have a model prediction of 0.3,
590:29 - which initially would have been uninfected,
590:33 - now this model sees this as parasitized.
590:38 - And so this makes it now more difficult
590:42 - for our model to have false negatives
590:46 - since our model now has this tendency
590:50 - of predicting that a given input image is parasitized.
590:57 - That said, we now need to look for a way
591:02 - that we could automate this process.
591:05 - That is we want to be able
591:08 - to choose this threshold correctly or rightly.
591:11 - Because if let's say we take a threshold of say 0.001,
591:18 - it means that anytime our model predicts less than 0.01,
591:22 - it's uninfected and then greater than 0.01 is parasitized,
591:29 - then this will be very dangerous
591:31 - for the overall model performance
591:34 - as now most times would have the model predicting
591:38 - that the input is parasitized.
591:43 - And so our aim here is to pick this threshold
591:47 - such that this number of true positives
591:52 - and true negatives we've had right here don't get reduced.
591:58 - Now, the way we could look at this
592:01 - is now by using this ROC plot.
592:04 - With this ROC plot, what you actually have here
592:07 - is the different true positive rates
592:10 - and false positive rates you'd have at a given threshold.
592:14 - So this means that a point picked here
592:17 - is just a given threshold.
592:20 - Now let's suppose that the threshold 0.5 is about here.
592:24 - So let's pick this.
592:25 - Let's suppose that this is 0.5.
592:28 - We could pick another threshold.
592:30 - Let's say this one is say 0.2 and so on and so forth.
592:36 - Let's say this one is 0.1.
592:40 - Now we could have another model
592:42 - with this different ROC plot,
592:44 - another one with this kind of ROC plot,
592:47 - but note that overall our aim is to ensure
592:50 - that this false positive rates is minimized
592:54 - and the true positive rate is maximized.
592:57 - So if we have an ROC plot, which is like this,
593:00 - so let's redraw this.
593:03 - If we have this kind of ROC plot,
593:05 - that is one that goes up straight like this
593:08 - and then comes this way, we have this right here.
593:13 - So if we have this kind of ROC plot,
593:15 - then we will be able to pick out
593:19 - this threshold right here, 0.0, say X.
593:24 - We'll be able to pick this threshold
593:26 - because at this threshold, for this threshold,
593:30 - at this point, the true positive rate
593:33 - is at its highest value, that is of one.
593:35 - And then the false positive rate
593:37 - is at its lowest possible value, that is of zero.
593:40 - So here we have one and then zero.
593:43 - So here we go, we have this
593:44 - and then we pick out the threshold.
593:47 - Now this value of X could be five, could be four, whatever.
593:51 - So we have a zero point, whatever value
593:54 - will lead us to this.
593:57 - Nonetheless, many times we wouldn't have
593:59 - this kind of plots.
594:00 - So we will do it plus, which will look like this,
594:05 - this and so on and so forth.
594:08 - Now, once given a plot like this one,
594:10 - let's suppose we have a plot like this one.
594:13 - The aim here is to ask yourself the right question.
594:16 - If I want to make sure that my recall
594:20 - is always maximized, which is our case,
594:23 - we will try to ensure that we pick out
594:26 - these points around this.
594:28 - So we'll try to pick out these points
594:31 - towards the top right here.
594:32 - Let's take some of this off.
594:35 - So as we're saying, if we want to maximize our recall,
594:40 - is that normal that will pick out threshold values,
594:43 - which will take us around this region
594:46 - because it's around this region
594:47 - that our recall is maximized.
594:49 - Let's do this so you could see that clearer.
594:53 - Okay, now the problem with picking a point around this
594:58 - is that when you pick a point around this,
595:00 - the false positive rates is increased.
595:03 - So you need to find that balance
595:06 - between this false positive rate and true positive rate.
595:09 - So it will be much logical to pick a point
595:12 - around this right here.
595:14 - So we could pick around this region instead
595:18 - of this previous region right here.
595:21 - Now you could see that in this region,
595:22 - or let's say we pick this point.
595:24 - If you pick this point,
595:25 - your false positive rate now is smaller
595:28 - while your true positive rate is,
595:31 - or your recall is maximized,
595:33 - though it isn't the best possible recall we could have.
595:37 - But trying to focus on getting that recall of one
595:41 - will lead you into trouble since getting a recall of one
595:44 - in this case will increase our false positive rate.
595:48 - And then if we're dealing with a problem
595:50 - where we're trying to maximize the precision,
595:55 - then in that case,
595:56 - we want to ensure that this false positive rate is minimized.
596:00 - And so in this kind of problems,
596:02 - we'll want to pick a point around this.
596:04 - So you see, we want to pick this kind of value
596:07 - since at least our false positive rate is minimized.
596:11 - But then if you want to go and pick a point around this year,
596:15 - you would have a false positive rate of zero,
596:18 - but doing this will get you this into trouble
596:22 - because here you're going to have a true positive rate,
596:25 - which is very small.
596:27 - And so you need to get that balance.
596:30 - Now, if you're having a problem where it doesn't really matter
596:33 - that is you aren't trying to prioritize
596:35 - the precision or the recall
596:37 - and working with the accuracy is just fine,
596:40 - then you could pick out this point right here.
596:43 - So this is when you are having,
596:46 - or you don't have to prioritize on any,
596:48 - here is when you're trying to prioritize on the recall
596:50 - and here is when you're trying to prioritize
596:52 - on the precision.
596:54 - But as is one great thing is with this tool,
596:57 - that is the ROC plot,
596:58 - we are able to pick out this point
597:01 - and then automatically get that threshold
597:04 - we need to work with.
597:07 - And so when doing predictions, we will not,
597:09 - or we may not use 0.5,
597:11 - but we're going to use a certain threshold,
597:14 - which will suit this objectives we've set initially.
597:18 - Now we'll now move to the area under the curve.
597:22 - For the area under the curve,
597:23 - we generally use this when we comparing two models.
597:28 - Here we have this model, let's call it model A.
597:31 - It's not actually this model A, it's a different model A.
597:35 - Let's call this model alpha.
597:37 - And then we have this other model,
597:39 - which we shall call model beta.
597:42 - So we have model alpha and model beta.
597:45 - It's clear that model beta is better
597:48 - because it gives us better options.
597:52 - Now you'll see that if I find myself here,
597:57 - I get it better through positive rate,
598:00 - false positive rate balance
598:02 - as compared to when I find myself at this position.
598:05 - And so if we are comparing these two models,
598:08 - we could make use of the area under the curve
598:11 - by calculating this area covered here.
598:13 - So let's bound this and then for alpha,
598:17 - we have this area under the curve,
598:20 - popularly known as AUC, A small U and then C,
598:26 - which will give us this.
598:28 - And then for beta, we're going to have this area
598:31 - under the curve now, which covers this area
598:34 - plus this extra area right here.
598:38 - And so in general, if we have two models
598:41 - and then we want to compare them,
598:43 - then we could use this area under the curve.
598:46 - Since it shows us how much freedom we have
598:49 - in playing around with the thresholds.
598:52 - We now get back to the code
598:53 - and see how we're going to implement this new metrics
598:57 - we've just talked about.
598:58 - So right here, we have metrics
599:00 - and then we have the false positives.
599:03 - We have the false negatives.
599:05 - We have the true positives.
599:08 - We have the true negatives.
599:11 - We have the precision.
599:13 - We have the recall.
599:15 - We have the AUC, AUC.
599:18 - And then since we're done with a binary classification
599:20 - problem here, we have binary accuracy.
599:23 - We run that, that's fine.
599:25 - And then right here, instead of having this metrics,
599:28 - we'll define this metrics list,
599:31 - which will contain those different metrics,
599:34 - which we've just talked of.
599:35 - So we have the true positive, false positive,
599:38 - right up to the AUC.
599:39 - Let's run this, compile, and then feed our model.
599:44 - You'll see that as we train,
599:45 - we have the true positives, false positives,
599:47 - true negatives, false negatives, accuracy, precision,
599:51 - recall, and AUC scores, which have been given to us.
599:55 - As you could see, the other results we obtained,
599:58 - we're going to go ahead to evaluate our model.
600:02 - So let's get to the model evaluation.
600:05 - We run this, test data,
600:08 - and then we have our model evaluated.
600:12 - There we go, our model has been evaluated.
600:14 - As you could see, we have this last 0.35,
600:19 - number of true positives, 1,323,
600:24 - number of false positives, number of true negatives,
600:27 - false negatives, accuracy, precision, recall, and AUC.
600:32 - So that's what we have for this model.
600:39 - For now, we have been showing you this binary accuracy,
600:43 - false positives, up to AUC.
600:46 - What if we plot out the confusion metrics
600:49 - and also the ROC plot?
600:52 - So we'll impart scikit-learn and seaborne.
600:58 - Right here, we have scikit-learn
601:01 - and from scikit-learn.metrics,
601:06 - we're going to impart confusion metrics.
601:09 - And then for seaborne, we need to import seaborne
601:14 - and as SNS, so that's it.
601:17 - So here we are importing scikit-learn and seaborne,
601:22 - which we'll both use in plotting out
601:27 - this confusion metrics.
601:28 - So let's run this and then we'll visualize
601:32 - our confusion metrics.
601:34 - So we'll have to get the level,
601:37 - that's the true values of the outputs.
601:41 - And then we'll also get the predicted values.
601:44 - So yeah, we want to get the levels and the predicted values.
601:48 - Now let's start with the levels.
601:50 - So yeah, we're going to have levels.
601:52 - We have this list, there we go.
601:56 - And then for x, y,
602:00 - or let's say for x, y in the test data set,
602:07 - we're going to take this as numpy iterator.
602:12 - So for x, y in this, we have the x and the y,
602:16 - we have that.
602:18 - And then we append every output to these levels.
602:23 - So yeah, we have levels.append y and we run that.
602:29 - So that's fine.
602:30 - We can now print out levels.
602:32 - What do we get?
602:34 - There we go.
602:35 - You see, we have this levels.
602:36 - Now let's convert it into a simpler form
602:40 - where we just have only this values in here
602:44 - and not this arrays, which we have here.
602:47 - So in order to get that, we'll have levels
602:51 - will be equal levels or rather will be equal array.
602:55 - And in here, we're going to create this list.
602:57 - So in this list here,
603:00 - for every element of this levels list.
603:04 - So for here, for i in levels,
603:11 - we're going to do i the zeroth index.
603:16 - So we're going to take always this elements right here.
603:21 - So that said, we have that and now we have levels.
603:24 - Let's print out the levels after doing this transformation.
603:28 - There we go.
603:29 - Here's our levels now.
603:30 - So we have that fine, take that off.
603:35 - We have this error actually, because we tried,
603:38 - we've converted these levels already
603:40 - and then we're trying to reconvert them.
603:41 - So let's comment this right here.
603:45 - We comment that and run this and we have all levels.
603:48 - Okay, now we have this levels,
603:50 - we could move ahead and get the predicted values.
603:54 - So to get this predicted values,
603:56 - we have our lunette model, lunette model.
604:01 - There we go, dot predict.
604:04 - What we're going to pass in this predict method
604:06 - is our input.
604:08 - So we have the input and then right here,
604:10 - we add this input list and then we do same
604:13 - as we did with the levels.
604:15 - So we have input, dot append X, which is this input here.
604:20 - Now we run this and then let's add a cell.
604:24 - So we add this code cell
604:26 - and then we're going to pre-process our input.
604:28 - So from this, we're just going to print out the input
604:31 - to see what it gives us or what kind of shape
604:33 - we're having right here.
604:35 - Let's put non-py array while that's loading, shape.
604:40 - Then we run this other cell right here.
604:43 - Here's what we get as shape.
604:47 - So we have to take this off, this one off here.
604:51 - And to do that, you see we would have the input.
604:55 - We'll select this first index.
604:57 - So we take that and then for this next, we take only one.
605:02 - So we're going to chop this off and then we have the rest.
605:06 - So that's it.
605:07 - Now we run this again
605:09 - and we should have put a shape here.
605:13 - So we run that again and there we go.
605:17 - So this is what we expect.
605:19 - Now we've gotten the levels
605:22 - and then we are now ready to get the predictions.
605:27 - So here we have this input, take this
605:31 - and then we pass it in here.
605:32 - We pass our input and we get our predictions.
605:35 - Let's print out the predicted here.
605:38 - Oh yeah, let's print out a predicted and see what we get.
605:41 - So what we get right here, we could print out a shape
605:45 - to better understand what this is.
605:47 - And as you could see, we have this.
605:50 - So how do we take this over?
605:52 - How do we make it look like the levels?
605:55 - We could simply take this off.
605:58 - So we have, we select the first
606:01 - and then for this one, we take it off.
606:04 - So we run that again and there we go.
606:07 - So now we could do this, take out the shape
606:11 - and then it should look like the levels we've had here
606:16 - and that's it.
606:17 - So now we've gotten this.
606:20 - Our next step is to use our confusion metrics
606:23 - from scikit and metrics and use our confusion metrics.
606:28 - Now let's comment this sections first.
606:31 - So here we've passed in the levels,
606:33 - we've passed in the predicted
606:34 - and then we're specifying the threshold.
606:36 - So what we're saying here is all values greater
606:40 - than the threshold considered uninfected
606:42 - and all values less than or equal to threshold
606:45 - is gonna be considered as contained in the parasite.
606:49 - And then we're gonna have this
606:50 - in this confusion metrics here.
606:52 - So we run that and then as you could see,
606:54 - we have our confusion metrics,
606:56 - which shows us the number of true positives, true negatives,
607:00 - false positives and false negatives.
607:03 - Getting back to our model evaluation,
607:06 - we'll see exactly which of those values
607:09 - are the true positives, false positives,
607:11 - true negatives or false negatives.
607:13 - So let's just copy this out here.
607:16 - We take this up and we shall see that below.
607:21 - So here we have this confusion metrics,
607:24 - which we've just seen.
607:25 - Here is it right here, confusion metrics right here.
607:30 - And then let's place this out here.
607:32 - So we have the number of true positives.
607:34 - This one matches.
607:37 - And then number of true negatives, one, two, nine, eight.
607:41 - Yeah, this is closer.
607:42 - So what Psychelearn gives us is quite similar
607:45 - to what we had with TensorFlow,
607:48 - but they're not exactly the same.
607:50 - So this tells us that this is a number of true negatives.
607:52 - This is the number of,
607:54 - no, this is a number of true negatives right here.
607:57 - So here we have true negatives.
607:58 - Here we have true positives.
608:01 - Then here we have false positives.
608:03 - And then here we have false negatives.
608:07 - We'll then see how modifying the threshold
608:10 - will either reduce the false negatives
608:13 - or reduce the false positives.
608:18 - So let's have say 0.25 right here.
608:21 - We run that again and there we go.
608:25 - Let's print this.
608:26 - So we see that we now have a number of false negatives
608:29 - reduced when we reduce the threshold.
608:33 - We now go back and then take this to 0.75.
608:36 - We run that again.
608:38 - You see that this is now reduced.
608:40 - So here we have an increased number of false negatives
608:44 - and a reduced number of false positives
608:46 - when the threshold is increased to 0.75.
608:50 - So that's it.
608:51 - Let's now print out or rather let's now plot out
608:54 - this more elegantly looking figure with the CBUN library.
609:00 - So here we go.
609:01 - We run that and that's it.
609:03 - So here we have same confusion metrics
609:06 - but more elegantly plotted.
609:11 - Let's take back to 0.5 here and have that.
609:16 - Okay, so that's it.
609:18 - Now you notice that when we're trying to say
609:21 - reduce the more false negatives
609:22 - or the more false positives,
609:24 - what we're doing is we're just picking up some values here
609:28 - like we could pick up 0.2
609:30 - and then we see its effect
609:31 - and we could do the same for 0.25.
609:34 - You see how this takes us to 58 and this to 130.
609:38 - We could continue doing this until we get our best results.
609:43 - But then this isn't very efficient
609:46 - since we're just trying out different values.
609:48 - Now the way we could do this more efficiently
609:51 - is by working with ROC plots
609:53 - where we'll be able to choose a threshold
609:56 - in a more efficient manner using the plots.
609:59 - And so we'll be able to reduce the number of false negatives
610:02 - or number of false positives
610:04 - without having to try out all these threshold values manually.
610:13 - The very first thing we'll start with
610:14 - will be to import ROC curve.
610:16 - So we have ROC curve
610:18 - and this is part of scikit-learn metrics.
610:21 - So we'll run this again
610:24 - and then just right here we'll make use of our method.
610:27 - We're going to output a number of false positives
610:31 - through positives and then thresholds
610:33 - which we'll use in coming up with the ROC plot.
610:36 - So we have that, we have ROC curve, ROC curve
610:41 - and then this takes in the levels and then the predicted.
610:46 - So we have this predicted value.
610:48 - Now if you print this length out, the length of FB,
610:52 - length of TP and length of thresholds,
610:57 - so we have exactly the same number.
611:00 - Thresholds here, we should have exactly the same number.
611:04 - Here we have 330.
611:07 - Okay, so now we have this.
611:10 - Now note that the reason why we need this
611:12 - is because when coming up with our ROC plot,
611:16 - like say we have this ROC plot,
611:18 - what I have for each and every point,
611:21 - the corresponding TP, FP and then the threshold
611:27 - which will lead to that TP, FP pair.
611:33 - So that said, we are going to make use of this data now
611:37 - and then plot out the ROC curve.
611:40 - So let's get straight into that.
611:41 - We have our plot and then we do plotting.
611:45 - We pass in the false positives through positives
611:48 - just like XY.
611:49 - So here we have X and then here is Y.
611:53 - Let's get back, that's fine.
611:54 - And then from here we have the levels.
611:57 - So we have X level, which is our false positive.
612:03 - And then the Y level, our true positive rate.
612:09 - So we've seen this already.
612:10 - Now we have this, we could include the grid.
612:14 - So we have this grid and then we show.
612:19 - So that's it.
612:21 - We run that and here is what we get.
612:24 - So this is our ROC plot right here
612:27 - based on this FP and TP we got from here.
612:31 - Now, how do we include the thresholds?
612:33 - In order to include the thresholds,
612:36 - we are going to make use of my plot leaps test method.
612:39 - So we have plot the text and then here we're going to have
612:44 - the TP or rather the FP TP.
612:49 - And then what if the actual text will be put in here
612:53 - will be the thresholds.
612:54 - So we'll be passing in the thresholds.
612:56 - But now note that we actually have to do this
613:00 - for each and every point, which isn't possible
613:03 - since there will be too many texts put out here
613:06 - and it's going to be chucked up.
613:07 - So what we could do is we could skip some values.
613:11 - So we'll say for I in range, we're going to start from zero
613:16 - right up to the length.
613:18 - That's actually going to be 330, length of thresholds.
613:21 - And then we're going to be skipping some values.
613:23 - So skip and then we'll define the skip.
613:26 - So let's start with a skip of 20.
613:27 - So initially we're going to skip 20 values.
613:31 - Then once we skip this value, let's pass this in here.
613:35 - Once we skip this value, we pick in a given I,
613:39 - pick in that given I, same here.
613:41 - And then we do the same with the thresholds.
613:44 - So we get the corresponding false positive rate,
613:47 - corresponding true positive rate
613:48 - and then the corresponding threshold.
613:50 - Now that's done, we could run this.
613:53 - So here's what we get.
613:56 - We see this plot.
613:59 - Now we could see the ROC plot
614:02 - with the different thresholds.
614:06 - I think we could let it like this.
614:09 - It's fine.
614:09 - Let's create a size.
614:13 - And then we try to focus just on this portion,
614:17 - which actually matters the most
614:18 - because we wouldn't want to get into these regions
614:22 - because in these regions our false positive rate
614:25 - is going to be too high.
614:26 - And these regions below this
614:28 - would have a very small true positive rate.
614:31 - So generally we'll try to focus on this zone right here.
614:35 - Now, depending on the problem you are trying to solve,
614:38 - if your false positive rate is what matters the most,
614:43 - that is if you cannot afford to have
614:45 - a high false positive rate,
614:47 - then you would tend to pick values.
614:49 - Let's say, let's break it out like this.
614:51 - So here is like the meat point.
614:53 - We have some sort of meat here.
614:55 - Let's draw this line.
614:59 - Okay.
614:59 - So we have some sort of meat point here.
615:01 - So this is like 0.5, 0.46, 0.62 and all of that.
615:06 - So we're breaking it up like this.
615:08 - And then if you want to ensure that
615:10 - you try to minimize as much as possible
615:12 - your false positive rate
615:14 - without reducing your true positive rates too much,
615:17 - then you tend to take values around this.
615:19 - But if you want to make sure a true positive rate
615:25 - remains quite high,
615:26 - even at the detriment of the false positive rate,
615:28 - then you would tend to pick out values in this zone.
615:32 - So you have these two zones to pick your threshold
615:35 - from this is the main zone.
615:38 - And then you have this zone right here,
615:41 - this other zone and this other zone.
615:45 - So you have zone one and zone two
615:48 - from which you have to pick from.
615:50 - One other quick note is that if you have a problem
615:54 - like the one we're trying to solve
615:56 - where parasite is zero
616:00 - and then uninfected is one.
616:03 - So this is how the data set was created.
616:05 - And based on this we build our model.
616:08 - Then this means that this will be considered
616:10 - as negative samples
616:13 - while this will be considered as positive samples.
616:16 - Whereas in the real world,
616:17 - we'll tend to look at uninfected as negative
616:21 - and parasitic as positive.
616:23 - So you have to be very careful with these terms
616:25 - and know exactly how your data and models have been built.
616:30 - And that's why in our case
616:32 - where we're trying to avoid situations
616:34 - where our model predicts a fake uninfected output,
616:38 - that is a patient who actually has a parasite
616:40 - but the model predicts that's uninfected.
616:43 - There's actually a fake uninfected.
616:45 - This is a fake or false positive in our case.
616:50 - So we'll tend to minimize this number of false positives.
616:56 - Now, if your model was built such that parasitic is one
617:01 - and then uninfected is zeros,
617:05 - then it's clear that you try to instead minimize
617:08 - the number of false negatives
617:11 - since uninfected is considered as negative.
617:16 - That said, coming back to our problem,
617:18 - since our data set was constructed in this way,
617:21 - we try to minimize the number of false positives
617:24 - at our cost.
617:25 - But while doing this,
617:27 - we have to ensure that the true positive rate
617:29 - remains at the reasonable position.
617:33 - And so we could pick out a threshold of like 0.6265
617:38 - given right here.
617:40 - Getting back to this, let's take 0.6265, we run that.
617:46 - We have here a number of false positives to be 87,
617:49 - which is gonna be smaller than
617:52 - when we're having a threshold of say 0.5.
617:56 - Run that again.
617:57 - You see, 87 is gonna be smaller than this 99,
618:01 - the value of 99 we're getting now.
618:04 - And so that's it for this section on metrics.
618:06 - Thank you for following up up to this point
618:08 - and see you next time.
618:16 - What's up everyone and welcome to this new section
618:20 - in which we'll build callbacks with TensorFlow.
618:23 - In this section, we'll look at how to build a callback
618:27 - from scratch by inheriting from the callback class.
618:32 - And then we'll build other callbacks made available
618:35 - by TensorFlow like the CSV logger,
618:38 - early stopping, learning rate scheduler,
618:41 - model checkpointing,
618:42 - and finally reduce learning rate on plateau callback.
618:46 - Don't forget to subscribe and hit that notification button
618:49 - so you never miss amazing content like this.
618:53 - Callbacks are methods we call during training,
618:56 - evaluating our prediction.
618:59 - This callbacks can permit us extract useful information
619:03 - from those processes we just listed
619:05 - or even carry out changes on those processes.
619:08 - Here in this TensorFlow documentation,
619:11 - we have TF Keras and then we have the callbacks.
619:14 - So this is what we have for the documentation.
619:17 - We could go through each and every one of this,
619:20 - but for the sake of this course,
619:23 - we are gonna look at the key ones.
619:25 - So right here, we have this callback,
619:28 - but before getting to this,
619:30 - we are gonna look at the history.
619:31 - The reason why we're looking at history first is actually
619:34 - because we've used it already.
619:36 - So we use this callback without really knowing
619:38 - that we're using callbacks.
619:41 - So yeah, we told this is a callback
619:43 - that records events into a history object.
619:47 - So remember the times we were having this year.
619:50 - So after training, that's when we're training,
619:55 - we start some information in this history.
619:58 - And then after training,
619:59 - we're able to come up with plots like this
620:02 - because we had this information
620:04 - start in this history right here.
620:07 - So that said, as you could see in this example,
620:11 - which is kind of like similar to what we've been seeing so far,
620:14 - we have this history and it collects all values
620:17 - we've been starting during training.
620:20 - And then we could print out this params, history.params.
620:24 - We get the params, and then we could also get the keys.
620:29 - So that's how this works.
620:30 - We've seen this already.
620:32 - And then we could look at this callback class right here.
620:37 - Now, the reason why this is like the most important
620:40 - of all this different callback classes
620:43 - is because this is kind of like the modern class,
620:48 - all that abstract based class,
620:50 - which can be used in building new callbacks.
620:53 - So if you want to build a callback,
620:54 - which isn't listed here,
620:57 - you could always get back to this
620:59 - and then build that callback from scratch.
621:02 - So here we have this callback class,
621:05 - which has attributes as you could see, params and model.
621:10 - And then it also has this methods,
621:13 - which we'll see how to implement very easily.
621:16 - If you look at this on batch begin and this on batch end,
621:20 - you'll see that the take similar arguments
621:22 - like here we have the batch, and then we have this logs.
621:26 - Then for the next, the epoch begin and the epoch end,
621:30 - they're actually taking the epoch and the logs.
621:34 - That said, we'll import the callback.
621:37 - So here we have from tensorflow, Keras, callbacks,
621:43 - we're going to import callback.
621:46 - This class, which we're going to be using
621:49 - and creating our callbacks, we run that.
621:52 - Then we define this callback class,
621:54 - which we'll note as call last callback.
621:57 - We have this last callback class,
621:59 - which inherits from the callback, which we've just imported.
622:03 - And then we will make use of the different methods
622:06 - which have been given to us in the documentation.
622:09 - Let's start with the on epoch end.
622:12 - So on epoch end, we're taking the epoch.
622:15 - And then what we're going to do is we are going to
622:19 - print out the loss values at the end of an epoch.
622:23 - So what we'll be doing is kind of similar
622:24 - to what we have already here.
622:26 - So let's do that and have print epoch number,
622:34 - let's say epoch number this,
622:37 - and then has a loss of for epoch number this,
622:42 - for epoch number this, our method,
622:47 - the model has a loss of this,
622:52 - and then we just format.
622:54 - So there we go, we pass in the epoch.
622:56 - Let's have this here and the locks.
623:00 - So just as in the documentation,
623:02 - where we add in the epoch and the locks.
623:04 - Now we are going to pass in the epoch and then the locks,
623:08 - but since we want to get just the loss,
623:10 - we could get the loss from this.
623:12 - So we have this dictionary here
623:14 - and we'll pick out just the loss.
623:16 - Now let's run this and then we'll see how to include
623:22 - a callback in the training process.
623:25 - So just right here, we're going to have callbacks.
623:28 - So the callbacks argument and we have this list.
623:31 - So in this list, we're going to insert this callback
623:34 - we've just created here.
623:35 - So we have the last callback,
623:37 - we should just pass in here and that's fine.
623:39 - So now let's rerun this,
623:41 - let's take this for say three epochs
623:43 - and then see what we get.
623:45 - We get in this error, we click search stack overflow
623:48 - and then click on this.
623:50 - You'd see that we have a solution just here.
623:55 - And what it said here is,
623:57 - what you're going to pass is the object
624:00 - and not the class itself.
624:02 - So that said, instead of passing the loss callback,
624:05 - as we just did, we're going to pass on this object.
624:08 - So we're going to put in the brackets.
624:10 - Let's modify those prints here
624:16 - and pass in a space.
624:17 - So we're going to have that to the next line.
624:21 - And then we rerun that again.
624:23 - So let's run that and that's fine.
624:26 - Then after training for all our three epochs,
624:28 - here's what we get.
624:29 - We see that unlike before where we just have this output,
624:34 - now we have this message output that is for epoch number zero,
624:39 - the model has a loss of this for epoch number one
624:42 - and then for epoch number two.
624:44 - Now what we could do is we could add this.
624:46 - So we have this formatted normally.
624:49 - So we don't start from zero, but it ends up from one.
624:51 - So we could say plus one right here.
624:53 - So that's it.
624:54 - Now another thing we could do is we could have on batch end.
625:00 - So on batch end, we have that and then we have the batch.
625:05 - We pass in the locks.
625:06 - This time around what we want to do is kind of similar
625:09 - to what we've seen already here.
625:10 - So we just print this out.
625:13 - That's fine.
625:14 - Take that off.
625:15 - So for the batch number, for this batch number,
625:19 - the model has a loss of this.
625:21 - So there we take in the batch.
625:23 - So that's it.
625:24 - And then we log out the loss.
625:26 - Now, yeah, let's just pick up this locks totally.
625:29 - So we have the locks and then that's fine.
625:32 - So yeah, we've put in the batch
625:34 - unlike here we'll pass in the epochs.
625:36 - We run this again and run this on this.
625:42 - You'll notice that this time around
625:44 - we have much more information, which has been locked out.
625:48 - Let's take this year and get right to the top.
625:51 - What we have here is for batch number one,
625:53 - the model has lost.
625:55 - So you see that this is after each and every batch.
625:58 - So here the first batch, we have this log
626:02 - and then the next batch we have this log
626:05 - and so on and so forth.
626:07 - And since our batch size equal to 32,
626:10 - this simply means after working on 32 different data points,
626:16 - we're going to have this locked out.
626:19 - And then the next 32, we have this locked out
626:22 - right up to the end.
626:24 - Now for the epochs, you'll notice that as we go on,
626:28 - so we're moving on to, let's get right up to 689.
626:32 - Oh, we have that and finally here we have 689.
626:39 - Okay, so for this last year, that's at the end of the epoch,
626:43 - we now have this locked out.
626:45 - So here we have for epoch, whereas previously we had for batch
626:48 - for all the different batches.
626:51 - In case you want to understand how we got this 689 right here,
626:55 - you could take the total data set size
626:58 - and then divide by the current batch size, which is 32.
627:02 - You should get this 689, which is given right here.
627:07 - So from this, we can look at the CSV logger.
627:10 - Now with a CSV logger, what we're actually doing is
627:13 - we are logging out this information in a CSV file
627:17 - or in some file, which we're going to define.
627:21 - So what we'll do is just simply copy this.
627:23 - Now this time around, we wouldn't actually have to
627:27 - recreate a class as we just did with the callbacks
627:30 - because there's kind of like a general way
627:33 - of creating these callbacks.
627:35 - Now with the CSV logger,
627:36 - we'll actually create the callback more easily.
627:40 - So here we have this
627:43 - and then we're going to specify the file name.
627:45 - So yeah, let's have the CSV callback equal that.
627:49 - Let's take this off and we specify our file name.
627:53 - So yeah, we just have file locks.csv
627:58 - and that should be fine.
628:01 - So we take that off.
628:04 - Take that off and then up right here,
628:07 - we have to import CSV logger.
628:13 - That's fine.
628:14 - We run that, we think it's okay.
628:17 - We get back to our callback
628:19 - and then we run this cell right here.
628:24 - Now note that this append is,
628:28 - as described in the documentation,
628:31 - a Boolean which tells us whether
628:34 - the locks we are currently putting in the CSV file
628:37 - or in the file in general are going to be appended
628:42 - on previously locked content or not.
628:45 - So when we have this as false,
628:47 - we're supposing that this is empty
628:49 - and so we are going to be putting information
628:54 - in this file for the very first time.
628:56 - So we've run this and then now all we need to do
629:00 - to take this new callback into consideration
629:03 - is to have your CSV callback.
629:07 - So let's make sure, let's take this batch locks out from here.
629:11 - So we are not going to take this last callback
629:15 - into consideration any longer.
629:17 - We run this.
629:19 - After three epochs, we're going to open this up
629:22 - and then we have this locks of CSV file
629:25 - which has been created.
629:26 - So let's have that.
629:27 - And as you could see, we have this CSV file
629:30 - which we could now download and then view it later.
629:34 - So here we have the accuracy, AUC
629:37 - and all the other metrics and lost values
629:41 - which we want to store.
629:43 - So that's fine.
629:45 - Next thing we could do is we could get back again here
629:49 - and then I'll select append true.
629:52 - So if we've done training the first time
629:55 - I want to redo the training process,
629:57 - we don't want to erase all the values we had previously.
630:00 - So here we set this append true around this again.
630:06 - After three epochs, we open this lock.csv file
630:09 - and what do you have here?
630:10 - You see, we have this new information
630:14 - which has just been appended on the previous information.
630:18 - From this CSV logger, we could now move on
630:21 - to this early stopping callback right here.
630:26 - To better understand early stopping,
630:28 - let's get back to the plots which we had previously.
630:32 - So let's take out this plot of the model's accuracy
630:35 - where we see how the train accuracy keeps increasing
630:39 - while after a certain point, let's see this point here.
630:43 - Let's take this off.
630:45 - After say this point, our models, even this point here,
630:52 - our model's validation accuracy doesn't increase any further.
630:57 - So what we have in here is something like this.
631:01 - We have this train accuracy which increases
631:05 - and goes towards one and then the validation accuracy
631:10 - which is something like this.
631:15 - Now in some other cases, you would even have situations
631:18 - where this starts to drop.
631:20 - Nonetheless, in this case, we have this plot
631:23 - where it just kind of like stabilizes
631:26 - and doesn't increase any further.
631:29 - Now note that this kind of situation is known as overfeeding.
631:34 - In overfeeding, the model starts to overfeed the training data.
631:42 - So because the model has been trained on the training data
631:45 - and not on the validation data, at certain points,
631:49 - the model stops or ceases to generalize
631:53 - because the aim of this training process
631:56 - is not to come up with a model
631:59 - which only performs well on the training data.
632:01 - We're trying to come up with a model
632:03 - which performs well on any type of data,
632:06 - be it a train, be it a validation or the test data.
632:10 - So if we're able to have a model which does the same
632:14 - or which has the same performance with a train
632:16 - and with a validation, with a test,
632:18 - then that model is an ideal one.
632:21 - But in this case, we see that as we keep on training,
632:25 - the model's parameters have been modified
632:28 - to suit only the training data.
632:31 - And this is very dangerous because at a certain point,
632:35 - you may feel like because you're having
632:38 - high training accuracy, your model's performing well.
632:42 - Whereas this isn't the case because when your model
632:46 - will be shown new data, like the validation in this case,
632:51 - and the test later on, this model wouldn't perform
632:55 - as well as it will do,
632:58 - or as well as it's doing with the training data.
633:01 - So to avoid this kind of false measurements,
633:05 - we tend to stop the training
633:07 - once this overfeeding status will occur.
633:11 - So this means that if you're training
633:14 - and then your validation, let's take,
633:16 - let's suppose we're stopping at this here.
633:18 - So we're training and then your validation data
633:22 - or your validation accuracy seems to be constant,
633:25 - whereas that of the training seems to kind of increase,
633:30 - then it's better for you to stop training at this point.
633:34 - Because after this point, the model parameters
633:38 - are just being modified to suit the training data
633:41 - and it doesn't really generalize, which is the case here
633:45 - because we're trying to extract some information
633:50 - from this data and make the model intelligent.
633:53 - So the model doesn't become intelligent
633:56 - by only modifying its weights or parameters
634:01 - based on the data it's been trained upon.
634:03 - It's intelligent because after being trained,
634:07 - it can perform well on data it has never, ever seen.
634:12 - So here we have this early stopping
634:15 - where after we notice that the validation accuracy
634:20 - doesn't seem to increase any further,
634:22 - we just kind of like stop the training
634:25 - and then use the model parameters
634:27 - from this number of epochs.
634:29 - So we could see that after say 12 epochs,
634:32 - we just stop the training.
634:34 - Now let's take this off and then do replicate
634:37 - something similar for the loss.
634:40 - So if we're having a loss,
634:42 - we could have something like this and then that.
634:46 - So yeah, we have the number of epochs
634:49 - and then we have the loss.
634:50 - So you could have a situation where you're having
634:53 - your training data, your training loss
634:56 - which keeps reducing whereas for the validation
635:00 - you see you would have something like this.
635:02 - So this is a typical plot for over feeding.
635:08 - Nonetheless in our case, the model over feeds
635:11 - but not that much.
635:12 - In some cases you will have a situation
635:14 - where this even starts to drop
635:16 - and where the loss starts to increase
635:19 - after a certain point.
635:21 - Here is the validation loss
635:23 - and then here is the training loss.
635:25 - Obviously the training loss will always keep reducing
635:29 - because we're training on this training data.
635:32 - So what we're saying is at this point
635:35 - where the validation stops reducing,
635:38 - it's important to just stop this training
635:41 - and this is known as early stopping.
635:44 - Now recall that the aim of callbacks
635:48 - is to be able to modify the training process,
635:52 - the evaluation process or the test process
635:55 - as a prediction process in an automatic manner.
636:00 - So that said, we shall make use of this early stopping
636:05 - callback right here which will permit us
636:08 - stop training automatically once we notice
636:12 - that a given parameter like say the loss,
636:15 - the validation loss doesn't drop any longer.
636:19 - So here we're just gonna copy this
636:21 - and then we just apply it similarly
636:25 - to while we are done with the CSV callback.
636:28 - So here we add this text and then we add that code,
636:32 - we just paste this out.
636:34 - Now we define this AS callback,
636:36 - that's early stopping callback
636:38 - and then we look at the significance
636:39 - of each and every one of these arguments.
636:42 - Now coming back to documentation,
636:44 - we have this monitor, quantity to be monitored.
636:47 - So by default, here we have this valid loss.
636:51 - This means that this callback will simply check
636:55 - on this validation loss right here
636:59 - and then once it stops reducing like see at this point,
637:04 - we're gonna stop the training.
637:07 - Whereas if we change this to say validation precision
637:11 - or validation accuracy, then what we'll be monitoring
637:15 - will be that accuracy or the precision value.
637:20 - So if you have them like this,
637:21 - we'll see we'll stop on this right here
637:24 - to ensure that we don't go and over feed
637:27 - on the training data.
637:29 - The next argument is this mean delta argument right here.
637:34 - So with the mean delta argument,
637:36 - we are defining a minimum change
637:38 - below which any change is considered as no improvement.
637:45 - So if we have a loss like this
637:49 - and that our mean delta is say a value of 0.1,
637:54 - then even if this loss reduces by a value of 0.5,
638:00 - then this callback will consider
638:03 - that there has been no decrease in the loss
638:07 - because the mean delta is 0.1.
638:10 - Now by default, the mean delta is set to zero.
638:13 - This means that any slight change is considered as a drop.
638:16 - So if we have even 0.0005,
638:19 - then we consider this as a drop in the loss.
638:25 - Now this is important because this has a patience,
638:28 - this callback makes use of this patience.
638:31 - With this patience, we are defining the number of epochs
638:35 - above which if we don't have a decrease
638:41 - in the validation loss,
638:43 - like in this case of the validation loss,
638:45 - we consider that we could stop that training process.
638:50 - And for the accuracy is the number of epochs above which
638:55 - if we don't have an increase in the validation accuracy,
638:59 - if we've picked validation accuracy for the monitor,
639:03 - then we'll have to stop the training.
639:05 - So we define this, well, we predefined this
639:08 - so that this could run automatically.
639:12 - Now for the mode, by default, we have the auto mode,
639:16 - but we could specify mean or max.
639:18 - Notice that when speaking about the loss,
639:21 - we spoke of a value or the number of epochs
639:27 - above which if the loss doesn't decrease.
639:31 - So here we're supposing that the loss
639:32 - is meant to be decreasing.
639:34 - And in that case, we are having a mode of mean.
639:38 - Now for the accuracy, we spoke of,
639:40 - for the patience, number of epochs above which
639:43 - if the accuracy doesn't increase.
639:46 - So in this case, we're having the max.
639:49 - Now, what TensorFlow permits us to do is to use an auto.
639:53 - And with this auto, TensorFlow automatically infers
639:56 - whether it's dealing with a mean or max.
639:59 - So this means that if you place in, for example,
640:02 - a valve position, this auto should be able to understand
640:06 - that a position should be increasing.
640:09 - And so it's going to use a max.
640:11 - Then we move to the baseline where the training stops
640:15 - if the model doesn't show improvement over the baseline.
640:19 - And finally, we have this restore best weights.
640:22 - With the restore best weights, which by default is false,
640:27 - we are simply saying that the model is going to take up
640:31 - its final state.
640:32 - So this means that if we start monitoring the model,
640:37 - say at this point here, or at this point where we have
640:40 - the lowest possible valve loss, and then the model,
640:45 - let's say we have a patience of five.
640:47 - So we are going to train for four to five epochs
640:50 - before stopping.
640:52 - So if after five epochs, we are on this,
640:54 - let's suppose that the feed epoch, we are on this.
640:57 - So we've added from here plus five epochs, we are on this.
641:00 - And then we are having this loss.
641:03 - It's clear that this model with this loss
641:06 - is less performant than this one.
641:08 - Now, if this restore best weights is set to false,
641:12 - then we'll just take the model's weights here,
641:16 - or the model weights which give this loss value here.
641:20 - Whereas if it's set to true, then it means we're going to
641:23 - take the best weights we've had throughout the training
641:27 - process and which happens to be the weights
641:30 - which provide this loss right here.
641:34 - That said, here we go.
641:35 - We have this, we have, let's say the patience to three,
641:39 - verbosity one, mode auto, baseline known,
641:45 - restore best weights false.
641:46 - So let's run that, and then all we need to do
641:50 - is to just include this year.
641:52 - So here we're going to have ES callback.
641:56 - Now we could take off the CSV callback,
641:58 - but you could always put all this together.
642:00 - Let's just let it, so we could see how all that works.
642:04 - So we just have this list right here,
642:06 - and we have this ES callback with the CSV callback.
642:10 - Now we run the training.
642:12 - We didn't train this for long enough to be able
642:14 - to observe any callback changes.
642:19 - So let's take this to 10 epochs,
642:21 - and then we reduce this to one or two.
642:26 - Let's take this to two.
642:28 - We run that again, so that's fine.
642:31 - And then we fit our model.
642:35 - After training for eight epochs,
642:37 - we see clearly here how the early stop in callback
642:41 - stops the training process.
642:43 - Now let's understand why this is,
642:46 - all this training process has been stopped.
642:48 - If you take a look at this validation loss right here,
642:53 - you would find that there was a drop here, drop increase,
642:58 - but after this increase, there was a drop.
642:59 - And since the patience is equal to,
643:03 - that's the patience we had defined here equal to,
643:07 - we have to get two successive increases
643:11 - or two successive same loss values
643:15 - before the training process could be stopped.
643:18 - So since we, after this, we have a drop,
643:21 - the training process continues,
643:22 - then we have this increase,
643:24 - and then we have this drop, so it continues.
643:26 - Then here we have this increase,
643:28 - and then here again, we have this other increase.
643:30 - So because now we have had this two successive increases,
643:35 - as we could see in the plot right here,
643:37 - you see right here,
643:40 - where we have this increase, drop, increase,
643:43 - and then increase.
643:46 - We now have the training process,
643:48 - which has been stopped, as we could see here.
643:52 - That said, we now move on to the learning rate scheduling.
644:00 - Up to this point, we've been used to training our models
644:04 - with one fixed learning rate throughout the process.
644:08 - So we could fix our learning rate as we did to say 0.01,
644:13 - and we use this same learning rate
644:15 - throughout our whole training process.
644:18 - But it happens that if this learning rate is too large,
644:21 - then we reach diverging,
644:25 - and if the learning rate is too small,
644:29 - it would take too long for our model to converge.
644:32 - So let's consider this plot right here.
644:35 - It's actually a very simplified plot,
644:36 - as actually what really happens
644:39 - is way more complex than this.
644:41 - So let's consider this plot, and we have this,
644:47 - and here we have the loss.
644:49 - Recall our aim and the weights.
644:53 - So here we have loss and then weights or parameters.
644:57 - And our aim is actually to modify this weights
645:00 - such that the loss is minimized.
645:03 - So our aim is to get to this position right here.
645:08 - Now we start with a case where we have a high learning rate.
645:11 - So if we were dealing with a high learning rate,
645:13 - then it would be easier for our model
645:16 - to find its way to this minimum position right here,
645:20 - to some position close to this minimum position,
645:23 - let's say at this point here.
645:26 - But the problem here is once it gets to this position,
645:29 - it could also very easily diverge from it.
645:33 - So it could very easily get back
645:35 - to another point around here,
645:37 - and then repeat this kind of process again
645:39 - where the model just kind of diverges.
645:42 - So you could have something like this,
645:45 - it could come down here and then get back
645:48 - to some point around this and so on and so forth.
645:51 - So we may have this case where the model
645:54 - doesn't really converge because the learning rate
645:56 - is too high.
645:57 - And then for the small learning rates,
646:02 - we may start training and then if say the model
646:05 - or if we find ourselves at this point here,
646:09 - that is we've modified the weights
646:11 - so that the loss value happens to be at this point here,
646:14 - it becomes difficult for us to get to this ultimate
646:21 - or global minima.
646:25 - Since the learning rate is too small
646:27 - and it changes, we're making very small changes.
646:30 - So we may find ourselves just staying around
646:34 - this local minima here instead of going
646:37 - towards this global minima, which is this.
646:41 - And so to bring in a balance, what we could do is
646:44 - when we start the training process,
646:47 - we could use a relatively high learning rate
646:51 - so that the model kind of approaches
646:55 - this global minima faster.
646:57 - And then after a certain number of epochs,
647:00 - we start or we modify the learning rate
647:04 - so that it becomes or it takes in very small values.
647:07 - And since it now takes in small values,
647:09 - we now start taking up very small changes
647:13 - such that we can get towards this global minima now
647:17 - without risking divergence.
647:21 - Now, one way of doing this is by say you could fix,
647:25 - let's say you could suppose that for the first 10 epochs,
647:28 - you train your model at a learning rate of say 0.1.
647:33 - And then from 10 to 20, you're gonna train the model
647:36 - a learner of say 0.01.
647:39 - Let's say we're dividing by a factor of 10.
647:41 - So we now move to 0.01 and the next 10 again,
647:45 - let's say we're training all this for 30 epochs
647:47 - and then the next we go to 0.001.
647:51 - So you could train your model and then after 10 epochs,
647:54 - you restart the training by modifying the learning rate
647:57 - in your optimizer and then again, you do this same year.
648:01 - But now the problem with this is you always have to be there
648:05 - to ensure that after the training,
648:07 - as after the 10 epochs, you modify this manually.
648:11 - Now, what if we're able to do this automatically?
648:15 - As usual, this is made possible by TensorFlow callbacks.
648:18 - With this callback, that's the learning rate
648:21 - shadow log callback, we could define a function
648:24 - which takes in the number of epochs
648:27 - and then modifies the learning rate
648:29 - based on the current epoch,
648:34 - based on a mixture of the current epoch
648:37 - and some predefined function.
648:41 - So as you could see here, we have the learning rate
648:44 - shadow log, it takes in a meta shadow
648:47 - and then we could specify the verbosity.
648:50 - Now, this is an example of this shadow log method
648:54 - which has been defined here.
648:56 - Here what it do is if the number of epochs is less than 10,
648:59 - then you're gonna use this predefined learning rate.
649:02 - And then in the case where the number of epochs
649:06 - is greater than or equal 10,
649:08 - then we start to reduce this learning rate
649:12 - in an exponential manner.
649:14 - So there we go, let's take this off.
649:17 - Now, what we're saying here is we're modifying
649:20 - the learning rate such that after that,
649:23 - we have in before 10 epochs,
649:25 - we have in this fixed learning rate.
649:27 - That's it.
649:29 - And then after this, we start to reduce this.
649:33 - So the learning rate starts dropping
649:35 - as we continue with the training.
649:37 - So now we don't really need to be monitoring
649:41 - the training manually because this callback
649:44 - will automatically modify the learning rate for you.
649:48 - We could simply copy out this example
649:50 - which has been given to us right here
649:52 - and then make use of it in our training process.
649:56 - So here we have to include this text
650:00 - and then our code.
650:01 - So here we paste out the scheduler.
650:04 - Let's do this.
650:06 - And then for this one,
650:08 - we have our learning rate scheduling callback.
650:15 - So learning rate scheduler.
650:19 - And then we do this import.
650:21 - So here again, we just have learning rate scheduler.
650:25 - We run that and then get back to this position right here.
650:31 - Let's do this.
650:33 - One, two, three.
650:34 - So that's fine.
650:35 - So notice that here we have this order.
650:38 - So we have the callbacks, learning rate scheduler.
650:42 - And we have CSV logger,
650:43 - LSTopping, learning rate scheduler under the callbacks.
650:47 - So that's fine.
650:48 - Let's get back to that.
650:49 - And then we are going to define.
650:52 - So here we've had the learning rate scheduler
650:55 - or this scheduler method,
650:56 - but we're yet to define our learning rate scheduler callback.
651:00 - So we have our, let's say scheduler.
651:05 - Scheduler callback equals learning rate scheduler.
651:11 - And then it takes in this scheduler method right here.
651:15 - So that's fine.
651:16 - Notice how the scheduler method takes in the current epoch
651:19 - and the current epoch number and the learning rate.
651:23 - So here we have this given to us already.
651:26 - And so we'll modify this to take, say for example, three.
651:31 - So after three epochs, we're going to modify the learning rate
651:34 - and then we could bring out the learning rate.
651:38 - So we could say, for example, the current learning,
651:42 - actually what we could do is let's take this off.
651:44 - Let's not have that.
651:46 - Let's specify the verbosity to be equal to one.
651:50 - So here we have the verbosity specified as one.
651:54 - And then we'll take off this.
651:59 - So you could also check out the CSV logger,
652:02 - the locked CSV file.
652:05 - So you'll notice how through our login these values
652:09 - since we just adding up or stacking up the values
652:12 - on the previous values we've had already.
652:15 - So for now, what we could do is take this off.
652:18 - Now let's take this off all this
652:22 - and then focus on just the scheduler.
652:25 - So we have scheduler callback and that's fine.
652:29 - Let's ensure that the cells have been around already.
652:32 - So that should be okay.
652:34 - Now we have in this arrow, verbosity,
652:38 - unexpected keyword arguments.
652:40 - Let's come back, get back to this year.
652:42 - And then we have always verbose.
652:44 - So let's modify that as verbose equal one.
652:48 - We run that again.
652:50 - That should be fine now.
652:51 - We could now get back to our training.
652:54 - So we're expecting that below three epochs,
652:57 - let's say equal three epochs.
652:59 - We have a given learning rate and then above that
653:01 - we have a learning rate which decreases exponentially.
653:05 - So that's it.
653:08 - You see here that as the training process starts,
653:11 - we did not really need to print out any value
653:13 - because when we set our verbose to one, it outputs this.
653:18 - So here we have learning rate,
653:19 - scheduler setting the learning rate to 0.00999.
653:23 - That's practically 0.01,
653:25 - which is what we had given here in this learning rate.
653:29 - So it's practically this learning rate setting.
653:31 - And then as time goes on, it's going to decrease its value
653:34 - and then always output the current learning rate.
653:39 - So as we carry on the straining process,
653:42 - recall that the aim of having to work
653:46 - with these kinds of learning rate schedulers
653:48 - is that we want to actually get the best of both worlds.
653:54 - So what we want is speed because a very slow
653:58 - or a very small learning rate doesn't actually speed.
654:01 - And so we want speed.
654:03 - And then we also want stability when training.
654:08 - So because we want this tool,
654:10 - we are going to use or modify our learning rates
654:14 - such that we always get this through our training process.
654:19 - And so that's why whenever we started our training,
654:22 - we have high learning rates, which could ensure speed.
654:25 - And then as soon as we have trained
654:28 - for a given period of time or for a given number of epochs
654:32 - and that we're trying to approach
654:33 - this global minima right here,
654:36 - we now seek for stability by reducing the learning rate
654:40 - such that we don't get to this point and have to diverge.
654:43 - So if you want a more stable kind of training process,
654:48 - what we'll do is reduce this learning rate.
654:52 - After the training is complete, here's what we get.
654:55 - You could see that we have this learning rate,
654:58 - which is now modified after a given number of epochs.
655:02 - So here you see how the learning rate starts decreasing
655:06 - as we go on with the training process.
655:11 - And so that's how we implement learning rate scheduling.
655:14 - But before moving on, let's check out this tutorial
655:19 - provided by the MXNet project developers,
655:23 - which talks of other different learning rate
655:26 - scheduling techniques.
655:28 - So here we have this learning rate scheduling with warmup
655:32 - as a slanted triangular.
655:35 - And as you could see, the learning rate
655:39 - actually falls between these two values here.
655:42 - So we have a learning rate between one and two.
655:47 - So you could always define your max learning rate
655:51 - and then your mean learning rate.
655:53 - So you could always have this.
655:55 - And then with the warmup, what we'll do is
655:59 - we are going to increase this learning rate linearly
656:04 - up to the max for a given number of epochs.
656:10 - And then once we get to this number of epochs,
656:14 - we now start decreasing this learning rate.
656:18 - So here the decrease is linear.
656:21 - So it's a linear function we're using.
656:23 - And then once it gets to the minimum learning rate,
656:25 - we just maintain that constant value right up to the end.
656:29 - So another learning rate scheduling technique
656:34 - we could use here is we have this linear increase.
656:38 - That's the warmup.
656:39 - And then we decrease this exponentially
656:44 - right up to this minimum value.
656:47 - Now this technique of warmup was developed in this paper
656:51 - by Priya Goyal et al.
656:54 - And they found that having a smooth linear warmup
656:57 - in the learning rate at the start of the training
657:00 - improved the stability of the optimizer
657:02 - and led to better solutions.
657:05 - Then from here, we move on to the cosine
657:07 - aligning scheduling method,
657:09 - where there is this smooth decrease in the linear rate,
657:14 - which is kind of resembling the cosine function.
657:18 - Now this is what a cosine function actually looks like.
657:21 - So here we have this and that.
657:25 - So here is our cosine function.
657:28 - And then if you notice, you'll find that this portion
657:32 - actually looks similar to what we have here.
657:36 - And now after we get to a given number of epochs,
657:39 - we just maintain that.
657:41 - So we have defined the mean and then the max.
657:43 - And then once we get to the mean,
657:44 - we just maintain that mean throughout.
657:47 - Then we also have this stepwise decay scheduling.
657:51 - Let's take this off.
657:52 - Here you see how we start with a warmup.
657:55 - So this warmup is kind of like a method
657:59 - used very much in practice.
658:01 - And then from here we have this stepwise reductions.
658:05 - So we go in for this fixed learning rate
658:09 - after a given number of epochs,
658:11 - we now drop the learning rate
658:13 - and then we drop it and so on and so forth.
658:15 - So here's our stepwise scheduling with warmup.
658:21 - Then from here, we have this cool down
658:26 - where we follow the stepwise method.
658:28 - And then after a given number of epochs,
658:30 - we now reduce the learning rate linearly
658:33 - and the term cool down.
658:36 - We also have this one cycle scheduling technique
658:41 - proposed by Leslie and Smith and Nicolet Topin.
658:46 - And here's what it looks like.
658:48 - You see that we increase that we have this warmup
658:50 - and then this linear decrease.
658:53 - Once we get to this initial position,
658:55 - we also have again this linear decrease
658:58 - with a different slope, with a smaller slope.
659:03 - And then once we get to this minimum
659:06 - or this final minimum,
659:09 - we now just actually just maintain the learning rate.
659:13 - Now finally, we have the cyclical scheduling methods
659:18 - originally proposed by Leslie and Smith.
659:21 - The idea of cyclical increasing
659:23 - or cyclically increasing and decreasing
659:26 - the learning rate has been shown
659:27 - to give faster convergence and more optimal solutions.
659:31 - So as you can see here, we are having the learning rate
659:35 - which is going to be bouncing
659:37 - from the minimum to the maximum, as you can see here.
659:40 - So we have this linear increase to the max
659:43 - and then linear drop, linear increase and your drop
659:46 - and so on and so forth.
659:48 - And then finally, we have this cyclical cosine aniline
659:53 - scheduling right here.
659:54 - We see how we have the cosine aniline
659:57 - and then we have this full cyclical process
660:00 - as we go from the highest to the lowest.
660:03 - And then this next cycle from the meat
660:05 - between the mean and the max,
660:08 - that's this right to the mean learning rate.
660:12 - And then we take up this final cycle
660:15 - from this quarter of the total.
660:17 - So we had 0.5, which is a quarter of two.
660:20 - So we start from this and then right to the mean.
660:25 - Now notice also that as we go into the cycle,
660:29 - the cycle lengths keep increasing.
660:31 - So we move from this to this length and finally this length.
660:37 - Based on the scheduler you want to implement,
660:39 - all you need to do is to modify
660:41 - this scheduler method right here.
660:43 - The next callback we'll be looking at
660:45 - is that of model checkpointing.
660:48 - In this model checkpoint callback,
660:50 - we are able to save the model's weights at some frequency.
660:55 - So unlike previously where after training,
660:59 - like with this after training our model,
661:02 - we get to load or rather we get to save the model like here.
661:07 - So here we save the model, we save our weights.
661:12 - After we're done with the training here,
661:15 - we could be doing this weight saving
661:19 - during the training process.
661:21 - And thanks to this model checkpoint callback,
661:30 - now as usual, we just copy this and then we could check out
661:34 - on this documentation for the significance
661:38 - of each and every one of those arguments.
661:40 - Here we have the file path.
661:42 - So this is the file where we are going to be saving our model.
661:49 - So here we have our checkpoint callback and that's it.
661:54 - We have model checkpoint here.
661:56 - We're going to define this file path.
661:59 - And then what are we going to be monitoring?
662:00 - We're going to be monitoring the validation loss,
662:03 - which is what's given to us by default.
662:07 - Just like with the early stop-in,
662:10 - where we had this monitor right here,
662:13 - we are going to look at our validation loss.
662:16 - And supposing we have this validation loss
662:19 - and then our training loss.
662:21 - So we're going to look at our validation loss,
662:23 - which is this, and then save the model weights
662:28 - when we have this mini model
662:30 - or the smallest validation loss right here.
662:33 - And then if you set the save best only to true,
662:38 - then we'll save only this weights,
662:40 - whereas when it's false, we'll save this
662:43 - and also save the latest weights.
662:48 - And then for this argument save weights only,
662:51 - if it's set to true, then we're going to save only the weights,
662:56 - whereas if it's set to false,
662:57 - then we're going to be saving a model.
663:00 - That's a full model with its parameters.
663:04 - Then we have the mode, which is automatic,
663:06 - like since it's validation loss,
663:09 - since it's a loss automatically, this would be a mean.
663:12 - Whereas if this is an accuracy or precision,
663:15 - we're going to have a max right here.
663:18 - So let's take this back to auto
663:20 - and then we have validation loss.
663:24 - Now the save frequency when set to epoch simply means
663:27 - we're going to do the saving after each and every epoch,
663:31 - but we could modify this to say three.
663:33 - So after three epochs,
663:35 - we're going to verify if our current model
663:38 - is the best of all the previous models,
663:41 - that's if we've set this to true.
663:44 - So if we've set this to true,
663:46 - we're going to verify if this model,
663:48 - the current model is the best, if it's the best,
663:50 - then we override this file path,
663:53 - that's override the file, the weight file we've saved.
663:56 - And if it's not the best, we continue training.
663:59 - So that said, let's modify our file path
664:02 - and then let's call this check points.
664:06 - That's it.
664:07 - So we have the checkpoints
664:08 - and then we run this.
664:11 - So we have that.
664:15 - And then we add this callback right here.
664:17 - So we have the checkpoint,
664:20 - checkpoint callback.
664:23 - That's fine.
664:23 - We run the training now and see what we get.
664:28 - After training for over 10 epochs,
664:30 - we have this two logs, which you could notice here,
664:33 - this one where we've been told assets
664:35 - reaching to checkpoints assets.
664:39 - So you could open up this checkpoints here.
664:40 - And then you'll see that we actually saving this model.
664:44 - And then we also have this log here,
664:47 - that's after the seed epoch.
664:49 - Now let's understand why this login is done
664:52 - after the first and also after the seed epoch.
664:57 - Just when training starts,
664:58 - we have a validation loss of 0.47.
665:01 - And so we're supposing that since this is the first then,
665:04 - the model sets its best state.
665:07 - Then it moves to one, which is greater than this.
665:09 - So the model state or the best state
665:12 - is this one after the first epoch.
665:15 - This, the model, the first epoch
665:17 - or the model state of the first epoch is to the best.
665:20 - This doesn't beat the record.
665:22 - This doesn't.
665:23 - And then you'd see that here we have this validation loss
665:28 - of 0.24.
665:30 - That is why the model is now saved.
665:33 - From this model checkpoint and section,
665:36 - we'll look at the reduced learning rate on plateau.
665:40 - And then later on, we'll have a dedicated section
665:44 - for tensor board.
665:46 - The way this learning rate or rather reduced learning rate
665:50 - on plateau works is that,
665:52 - if you start training with a fixed learning rate,
665:55 - and then say you've trained for over 100 epochs,
666:00 - and that the model performance after 110 epochs,
666:05 - so let's say we have 110 right here.
666:09 - So after this 110 epochs,
666:11 - our model performance doesn't improve.
666:14 - So we still fix our learning rate.
666:17 - But since for the past 10 epochs,
666:21 - we haven't had an improvement in the model's performance,
666:23 - what we'll do is we'll reduce the model's learning rate
666:28 - by a given factor.
666:30 - So here is the factor.
666:32 - The patience, our patience here is 10 million
666:35 - that we're gonna wait for 10 epochs
666:37 - before deciding whether we're gonna reduce
666:39 - the learning rate or not.
666:40 - The verbosity as usual,
666:42 - the quantity to be monitored,
666:45 - in this case, the default is the validation loss,
666:48 - the mode automatic.
666:50 - So if you want it to be mean,
666:52 - you could specify mean or max.
666:56 - But setting it to automatic allows TensorFlow
666:58 - to automatically decide whether it's a mean or max.
667:01 - In the case of validation loss,
667:04 - obviously it's gonna be a mean
667:05 - since our aim is to reduce the validation loss
667:08 - as much as we can.
667:09 - Then we have the mean delta,
667:12 - which takes in a default value of 0.0001.
667:16 - And this means that if after say 10 epochs,
667:21 - we have a change in the validation loss,
667:24 - but this change is less than this mean delta,
667:28 - then we'll consider that that isn't a change.
667:31 - And hence we are gonna still reduce the learning rate
667:35 - by this factor right here.
667:37 - We also have the cool down
667:38 - and then the minimum learning rate
667:41 - below which we wouldn't wanna drop our learning rate.
667:45 - As described here,
667:46 - this cool down is a number of epochs to wait
667:49 - before you resuming normal operation
667:52 - after the learning rate has been reduced.
667:54 - We are gonna create this callback very easily.
667:58 - And this time around,
667:59 - we're gonna monitor the validation accuracy.
668:01 - So if the validation accuracy doesn't increase
668:05 - after two epochs,
668:07 - we are gonna reduce the learning rate by a factor of 0.1.
668:11 - Meaning that if we had a learning rate of say 0.01
668:18 - and that this validation accuracy
668:21 - hasn't increased after the two epochs,
668:24 - we are gonna reduce this such that
668:27 - this becomes 0.01 times 0.1,
668:32 - which is this factor right here.
668:36 - Just as before,
668:36 - we are gonna add this plateau callback
668:39 - and then train our model.
668:42 - After training for five epochs,
668:44 - we can observe that we have this learning rate
668:48 - which has been reduced after the third epoch.
668:52 - And this is because after these two epochs,
668:57 - there is no improvement in this initial validation accuracy.
669:02 - And so that's why the accuracy is gonna drop
669:05 - from 0.01 to 0.001.
669:10 - Now that's it for this section.
669:12 - Thank you for getting up to this point
669:14 - and see you next time.
669:28 - Hello everyone and welcome to this new section
669:32 - in which we're gonna look at different strategies
669:34 - of combating overfitting and underfitting.
669:37 - This strategies will include data augmentation
669:41 - as you could see here, dropout, regularization,
669:44 - early stopping, smaller network usage,
669:47 - hyperparameter tuning and normalization.
669:50 - So far in this class,
669:51 - we've mentioned the terms overfitting and underfitting
669:56 - without really getting in depth
669:58 - to what this two actually mean.
670:01 - Now for overfitting, we'll consider this plot
670:04 - of the loss versus the number of epochs
670:08 - and then precision versus number of epochs.
670:11 - With a loss versus number of epochs,
670:13 - what we'll generally have when a model
670:15 - overfitting something like this.
670:17 - So here we have the training, the validation loss
670:22 - and that here we have the training loss.
670:25 - So this is just like a general way of looking at this
670:31 - though this may take different forms.
670:34 - Now that said, as you could see,
670:36 - we have this two sets and the loss
670:40 - versus number of epochs plots.
670:43 - Clearly we could see that the validation
670:47 - and the training set initially start up
670:50 - with a similar pattern.
670:52 - Sometimes you may even have the validation
670:55 - which comes up like this.
670:56 - So you can even have a situation like this
670:58 - where the validation performs even better
671:00 - than the training set initially.
671:03 - But generally when a model overfeits,
671:06 - what we'll have will be this kind of model
671:09 - where at some point the model keeps doing well
671:15 - at the level of the training
671:16 - and starts doing very poorly in the validation set.
671:21 - We could replicate this with a precision epoch.
671:25 - This could be precision accuracy recall
671:27 - or some other metric which we've chosen.
671:30 - And what we could have will be something like this.
671:33 - Obviously you saw in the practice,
671:35 - it isn't always like this, but something in this sense.
671:40 - So you would have this here, you see you have the training
671:45 - and then you have the validation right here.
671:48 - So what goes on is your model keeps on doing very well
671:52 - for the training and then with a validation
671:55 - at some point it starts doing even a very, very poorly.
672:01 - And so the danger with this is if you are considering
672:04 - working only with a training set,
672:06 - you may feel like you're more epochs
672:09 - or training over more epoch is a good idea
672:12 - because you keep having this great results.
672:15 - Like supposing we have fixed this year
672:18 - and that your training is like say 99%.
672:21 - So you have 99% precision score on your training data
672:27 - and you're like, wow, this model will do very well
672:30 - in the real world.
672:32 - But this isn't generally the case
672:34 - because this model actually has been over-fitted
672:40 - on your training data.
672:41 - Your model has learned instead to modify its weights
672:45 - based on the training data
672:48 - instead of being able to extract useful information
672:54 - or some intelligence from the data
672:57 - which has been used in training this model.
673:00 - The main cause of overfitting is having a small data set
673:06 - and a large and complex model
673:10 - which contains so many parameters.
673:13 - Now, if a model like say deep neural network
673:17 - has many parameters and you're giving it a small data set,
673:22 - then obviously it's gonna adjust its parameters
673:25 - such that on this small data set,
673:28 - it performs exceptionally well.
673:31 - So you may come across a model which has say 99.9%
673:37 - precision or accuracy
673:39 - just because the data you've used to train the model
673:43 - has or was very small.
673:47 - Now, in another case, you may have say a moderate sized data
673:51 - set and then not just this kind of model
673:55 - but a very large model.
673:57 - So at the end of the day, we notice that there has to always
674:01 - be a balance between this data set and the model size.
674:06 - So this means that even if you increase the data size
674:09 - and then you also increase the model size,
674:12 - your model may still risk overfitting.
674:15 - To better understand this concept of overfitting,
674:18 - let's take this simple example.
674:20 - Supposing that you have three subjects to master a school,
674:24 - let's say math, English and sports.
674:28 - But when kids get to school,
674:30 - they are only taught mathematics
674:33 - or they only taught English or only taught sports.
674:36 - Let's choose for example, mathematics.
674:38 - So these kids get to school and from the first year
674:42 - to the last year, they taught only mathematics.
674:46 - It's clear that when you evaluate this kid
674:49 - or when you pick a kid at random and evaluate that kid
674:52 - in mathematics, the kid would tend to have a better
674:56 - or an above average result in mathematics
675:01 - compared to kids from other schools.
675:04 - But when tested on subjects like English and sports,
675:09 - that may not be the case.
675:11 - And this is because the English
675:13 - and sports weren't taught at school.
675:17 - And so if you are evaluated on only what you were taught,
675:20 - you would tend to have very high scores like this.
675:24 - And then what if we started to evaluate
675:26 - on stuff the kids were not taught?
675:29 - You'll notice that those kids will then start having
675:32 - poorer results because those kids haven't taken out
675:36 - some time to master English and sports.
675:40 - And so you may end up with a kid who was able to show
675:43 - for example, that sine squared x plus cos squared x
675:48 - equal one, but can't tell you the past tense of it.
675:54 - Now it's clear that to readjust this situation,
675:57 - the kids have to be taught all the subjects
676:01 - such that there is a balance that those kids need.
676:05 - And obviously this balance will come in a way
676:09 - that the kids now perform better in these two subjects.
676:13 - And because they have had part of the time
676:17 - they used to study maths allocated for English and sports,
676:21 - they may perform slightly less than before in maths,
676:26 - but at least what's important to notice this time around,
676:30 - they can now express themselves better in English
676:33 - or practice some sports.
676:35 - Let's now take this other example.
676:37 - Supposing we're training a model which predicts
676:41 - the presence of a car in an image,
676:43 - and then you feed your model with this kind of data.
676:48 - Then after training your model, where you get to test
676:51 - that model is on this kind of real world data.
676:57 - It's clear that even if you had very great results
677:01 - with a previous data, in this working with this or testing
677:05 - on this new data set right here,
677:09 - you wouldn't have those amazing results you had previously.
677:13 - And so it's important that not only should your data
677:16 - be large enough or get as much useful data as you can,
677:22 - but ensure that your training data represents
677:26 - or looks like your test data or looks like
677:30 - what you're going to be having in production,
677:33 - as that's really what matters.
677:35 - What matters is the fact that you have a model in production
677:38 - which works well, or which has a high accuracy
677:43 - and not a model in your notebook,
677:46 - which has say 100% accuracy on your training data,
677:50 - whereas on production, it doesn't perform very well.
677:56 - Now moving on to under feeding,
677:58 - it turns out that here our model becomes way too simple
678:03 - for it to even be able to extract information from our data.
678:08 - So we may have our validation data
678:11 - and then our training data like this.
678:15 - And then there is this huge gap between our current loss
678:21 - and the minimum possible loss.
678:23 - We could also have this at a level of say,
678:26 - let's say accuracy.
678:27 - So yeah, we could have accuracy
678:30 - and then we have our validation
678:32 - and then training and we have say accuracy 100%.
678:38 - Let's say we have one right here
678:39 - and then our model is still too simple
678:43 - that we just end up say at 0.6 or 60% accuracy.
678:50 - In this kind of situation,
678:52 - our data or the relative size of our data
678:57 - as compared to the model may be too large.
679:00 - So you could even have a situation
679:02 - where this data is smaller than what we had here.
679:07 - So your data could be small like this,
679:10 - but if your model is way too simple,
679:14 - say we have just this very small model,
679:17 - then you may face this problem of under feeding.
679:21 - It also turns out that sometimes you may have a situation
679:24 - where you have even a very complex model,
679:28 - but that model still under feeds.
679:31 - And that's because that model hasn't been built in a way
679:35 - that it could extract useful information from this data.
679:40 - Now, if you could recall in the section
679:42 - where we're predicting the car price,
679:44 - we had a situation where a model,
679:46 - we use a simple dense layer.
679:49 - We use a simple single dense layer
679:52 - with just two parameters and we have this fixed data set.
679:57 - But once we increased our stacked up more dense layers,
680:05 - we found out that we're able to get better training
680:09 - and validation mean average error values.
680:13 - There are several ways in which we could mitigate
680:16 - this problem of over feeding.
680:18 - The very first one we'll look at
680:19 - is that of collecting more data.
680:22 - So it's important to lay hands on as much data
680:27 - as you can.
680:28 - This data has to be representative
680:30 - of what the model would see in real life.
680:33 - And this data should be as diverse as possible.
680:37 - Even after collecting more data,
680:39 - to solve this problem of over feeding,
680:41 - we could use data augmentation.
680:44 - Now, what is data augmentation all about?
680:47 - Supposing we have this image cell right here,
680:50 - the cell image we have here,
680:53 - which happens to be pasteurized.
680:55 - Now, instead of having just this in our data set,
681:00 - we could have this image right here modified
681:04 - such that we now have more data to train on.
681:07 - So this means that in the case where initially
681:10 - we had say 20,000 images,
681:15 - so we have 20,000 images.
681:17 - Now, after doing data augmentation,
681:19 - after modifying each and every image,
681:21 - we now have a data set of 80,000.
681:25 - Now we're considering 80,000
681:27 - because we're supposing that each image
681:29 - is gonna be flipped as we've just done right here.
681:32 - So we take this image, we rotate it,
681:34 - we have this other image,
681:36 - the same level obviously still parasitized.
681:39 - And then we flip again and get this image.
681:42 - We flip, get this image, flip, get this image.
681:44 - We see that we now have,
681:46 - instead of just this one, we have four others.
681:50 - This actually means we're multiplying this by five.
681:54 - So this is 100,000 since we have it now,
681:58 - one, two, three, four, five examples
682:01 - for this single example we had initially.
682:05 - It should also be noted that
682:07 - there are many other data augmentation strategies
682:10 - for this kind of image data.
682:13 - So apart from flipping as we've just done,
682:15 - we could crop just a portion,
682:18 - we could add some noise to this data,
682:20 - we could modify the contrast,
682:21 - we could modify the brightness
682:23 - and carry out so many other operations.
682:27 - And there is no particular data augmentation strategy
682:30 - which works for all problems.
682:33 - This means that when you have a particular problem,
682:36 - you will have to try different augmentation strategies
682:40 - and then be able to select
682:42 - the one which works for your data.
682:44 - Now, that said, we have dropout.
682:48 - To better understand this notion of dropout,
682:51 - we'll consider this simple neural network right here.
682:55 - Now, if you could recall,
682:57 - the reason why we have models which overfeed
683:00 - is because we are working with very complex models
683:03 - with many parameters.
683:06 - Now, in order to reduce the complexity
683:08 - of this neural network,
683:10 - what we could do is take off, for example,
683:13 - this interaction between this neuron
683:16 - and all those previous neurons right here.
683:19 - So this means that when training our model,
683:22 - we are only going to consider
683:24 - that we have in this hidden layer
683:28 - just this two neurons right here.
683:31 - So all those connections here become useless.
683:34 - Now, this has the effect of simplifying our network
683:37 - as what we have as output.
683:40 - Now, that's after carrying out the dropout operation,
683:44 - looks like this.
683:46 - So we have now a connection which look like this.
683:50 - Now, this particular case is an example of a dropout.
683:54 - We drop our ratio equal 0.3,
683:59 - or let's say 0.333.
684:02 - As we're dropping out exactly one third
684:05 - of all the connections,
684:08 - or rather one third of all our neurons right here.
684:12 - And if r equal two third,
684:14 - what we'll be left with will be this.
684:16 - So we'll take this off
684:18 - and we'll be left just with this neural network
684:21 - to train right here.
684:22 - We see that we could leave from this very complex model
684:26 - to a simplified model via this dropout operation.
684:33 - And this has an overall effect of mitigating overfeeding.
684:39 - The next step we could take is that of regularization.
684:42 - To better understand regularization,
684:45 - suppose we have this model with weights WJ.
684:50 - So we have say n weights,
684:55 - and this weights are free to take up any value.
684:59 - As we've seen previously,
685:00 - the fact that this weights can take up just any value
685:05 - may lead to overfeeding.
685:08 - As now, this weights can be adjusted
685:11 - to fit on the training data in a very perfect manner.
685:16 - So this means that we could have a model
685:18 - which picks out each and every point like this.
685:21 - So we have this kind of model and we have this.
685:26 - So we have this model which picks up every point like that.
685:30 - Whereas if we restrain this weights
685:36 - to stay in a given range,
685:39 - then we may end up with something like this.
685:42 - So we may end up with a model
685:43 - which looks simplified like this
685:46 - because it doesn't have as much freedom
685:49 - as this other model.
685:51 - Now the problem with this model is
685:53 - if now you're putting this new data,
685:57 - you'll find that this will try to pull out like this
686:01 - and then your prediction will be somewhere around here.
686:04 - And so if, for example, we're having horsepower
686:09 - in the x-axis and then year,
686:11 - want to predict the price of a car,
686:14 - let's do same year.
686:15 - We have the price and then the horsepower.
686:20 - Then it happens that we have this car
686:22 - with this very high horsepower.
686:26 - This model, because it is overfitted on this training data,
686:30 - will predict this very low price.
686:33 - Whereas this model which is generalized
686:36 - on this training data
686:39 - will tend to predict a more reasonable price.
686:44 - Now it should be noted here that
686:46 - because this model was able to go through each
686:48 - and every point, we would have a training loss
686:51 - of almost zero.
686:53 - Whereas now when we give it this new data,
686:56 - it doesn't perform well.
686:58 - Whereas with this, we wouldn't have a training loss of zero.
687:01 - But when given new data,
687:05 - at least we're going to have reasonable predictions.
687:08 - So coming back to regularization,
687:10 - our aim is to ensure that this weights,
687:14 - because this model can be represented as this function
687:18 - and this function is made of this weights.
687:23 - And so since when we're doing training,
687:25 - our aim is to ensure that we minimize the loss,
687:30 - then we could include this weights
687:33 - in the computation of the loss.
687:35 - This means that we have our loss, which is now equal.
687:38 - The loss would have normally
687:40 - plus the regularization constant
687:44 - times the sum of this weights of each and every weight square.
687:48 - Now this is known as L2 regularization.
687:51 - Whereas here we have L1 regularization
687:53 - where we're summing up the absolute value
687:56 - of each and every weight.
687:58 - For now, we're just going to explain
688:00 - how regularization helps in mitigating
688:03 - that problem of overfitting
688:04 - by restraining this weights in a given range.
688:08 - So let's have your loss equal L does initial loss
688:15 - plus let's call this R.
688:18 - Now, if our aim is to minimize this loss,
688:22 - then obviously this L will be minimized
688:26 - and R will be minimized.
688:29 - And so if we're trying to minimize this year
688:32 - or this sum in general,
688:35 - then it would have that overall effect
688:37 - of restraining these weights in a given range.
688:43 - Especially as we know that when we square very large values,
688:47 - these values become even larger.
688:50 - And so to avoid this,
688:51 - our weights will tend to take up smaller values
688:56 - which fall on the smaller range.
688:59 - This L2 regularization is also known as weight decay.
689:04 - It should be noted that the main difference
689:06 - between this L2 regularization and L1 regularization
689:10 - is that in trying to restrain the range of values
689:14 - which this weights can take up,
689:16 - the L1 regularization has that negative effect
689:20 - of making many of those weights
689:24 - to take up values around zero.
689:27 - That's values very, very small
689:29 - or take up many zero values.
689:32 - So this will lead to sparse models
689:35 - as compared to the L2 regularization.
689:38 - And that's why in practice
689:39 - we generally use the L2 regularization.
689:43 - That said, we move on to L stopping,
689:46 - which we've seen already.
689:47 - In L stopping, as we have seen,
689:49 - if we have our model, let's say we have precision,
689:54 - validation precision and then the training precision
689:57 - which keeps on increasing.
689:58 - And then we have this limit year of one or 100%.
690:03 - So we have the precision and we have our epochs.
690:06 - And then after a while this starts dropping
690:08 - and this starts dropping simply because our model
690:12 - is now trying to over feed on this data
690:16 - it's been trained on.
690:18 - And so we have to stop training once we notice
690:22 - that the validation performance isn't improving any longer.
690:27 - So this means that after a certain number of epochs
690:30 - we are going to stop the training.
690:32 - And we've seen this already in the previous section,
690:34 - we've seen it both to really go like this
690:36 - and then we had seen it practically.
690:39 - Then another thing to do is to reduce the size
690:42 - of the network or use a less complex network.
690:47 - Our next step will be to properly tune our hyper parameters.
690:51 - Hyper parameters like batch size, drop our rate,
690:55 - regularization rate and the learning rate
690:58 - can affect our model and dictate whether this model
691:03 - will overfeed or not.
691:05 - Now, if we look at the batch size,
691:07 - training with a larger batch size
691:09 - may speed up our training process,
691:12 - but working with smaller batch sizes
691:15 - have a regularization effect which help reduce overfeeding.
691:21 - And so according to Yann LeCun,
691:23 - frames don't let frames use many batches larger than 32.
691:28 - For the drop our rate, we'll see this already.
691:30 - Increasing the drop our rate means
691:33 - we are making the model simpler
691:36 - and the regularization rate,
691:38 - increasing the regularization rate means
691:40 - we're reducing the effect of overfeeding.
691:44 - And then finally, picking too small of a learning rate
691:48 - may lead to overfeeding.
691:50 - So in general, we have some hyper parameters to tune
691:54 - and they are not only limited to this
691:56 - as you may have many other hyper parameters
692:00 - depending on your problem.
692:02 - Now the fact that normalization
692:04 - introduces extra parameters, mu sigma,
692:08 - which bring in some noise in the model
692:11 - has that regularization effect
692:14 - which help reduce overfeeding.
692:16 - So if you're including batch norm in your model,
692:22 - then you could feel free to reduce the drop our rate.
692:27 - Since this normalization layer
692:29 - already brings in that regularization effect.
692:32 - From here, we look at ways of mitigating
692:35 - the problem of over underfeeding.
692:37 - So with underfeeding, we could use more complex models.
692:42 - We could also collect more data.
692:46 - We see that this solution falls in the tool
692:49 - that's both overfitting and underfitting.
692:52 - Here is always a good thing to collect more data
692:56 - or more clean and representative data.
693:01 - So from here we have,
693:02 - you could also improve the training time.
693:05 - Now note that you could have this model,
693:08 - like let's take this,
693:09 - and then we have here you've trained
693:12 - and then you have the validation.
693:15 - So here you have the valve and then you have the train
693:17 - and then you've been training for over say a thousand epochs.
693:23 - And then you feel like the model may not perform any better.
693:28 - Now, several scientists have reported that many times
693:32 - they've given up on a model and then come back later
693:36 - after forgetting to stop the training process
693:38 - and notice that this model kept on performing
693:42 - were much better.
693:43 - So sometimes you don't have to give up on your model.
693:47 - So you could train or you could increase this training time.
693:51 - Then again, we have hyperparameter tuning,
693:53 - which could help in making your model more performant.
693:57 - And we have normalization,
694:00 - which stabilizes the training process
694:02 - and leads to better performance in the model.
694:05 - We now see practically how the dropout
694:08 - could be implemented with tensorflow.
694:10 - So here we have this dropout layer,
694:13 - which takes as argument the rate, the dropout rate,
694:17 - noise shape, and the seeding.
694:20 - To better understand how and why we need to use seeding,
694:25 - is simply in the case where one reproducible experiment.
694:30 - So if wanna apply dropout in this layer
694:33 - with a dropout rate of 0.2,
694:36 - then we'll be taking off one neuron out of this five neurons
694:40 - to make our model simpler and avoid overfeeding.
694:43 - Now in doing so, we may take this one,
694:45 - or this, or this one, or this, or this other one.
694:49 - So it's a random choice.
694:51 - Now, if we wanna fix this choice
694:54 - so that this experiment can be reproducible,
694:56 - then we can set this seed
694:58 - so that each time we run the experiment,
695:00 - it's going to be exactly the same neuron,
695:02 - which is going to be taken off.
695:04 - Getting back to the code, the way we could use this
695:07 - is by importing the dropout layer.
695:11 - So here we just have this dropout,
695:13 - we run that and it's fine.
695:15 - And then we could include this dropout right here.
695:18 - So we could have here dropout,
695:20 - let's say dropout rate, dropout rate equal 0.2.
695:28 - Now you could always increase this rate.
695:30 - So let's have that.
695:31 - And then we have the dropout and then dropout
695:35 - or add a rate equal dropout rate.
695:39 - That's fine.
695:40 - We could piece this out here,
695:42 - but we wouldn't wanna add dropout here.
695:45 - Anyway, you could always include the dropout
695:48 - at this level, depending on how the model responds
695:52 - to this dropout, which has been added here.
695:55 - So let's add this dropout
695:56 - and then add the dropout right here.
695:59 - Bearing in mind that we could always add more dropout layers
696:03 - and increase the dropout rate or even reduce this rate.
696:06 - So that's fine.
696:07 - We run our model.
696:09 - And as you could see right here,
696:11 - this dropout has no parameters.
696:15 - From this, we look at regularizers.
696:17 - We'll see how to implement the L2 regularizer
696:20 - and the L1 regularizer.
696:22 - So yeah, we have TF Keras
696:24 - and then we have this regularizers right here.
696:27 - So if you select L2, you should follow on this page.
696:31 - Now, once you get this,
696:33 - you see you have this TF Keras, the regularizers, L2,
696:36 - and then you specify the regularization rate.
696:40 - So let's copy this out and then get back to our model.
696:45 - At the level of our model,
696:47 - let's come back to the definition of the Conf2D.
696:51 - So here we have this kernel regularizer
696:54 - and this kernel regularizer that we use
696:57 - in carrying out regularization.
697:00 - So here we have this Conf2D
697:02 - and then we simply specify the kernel regularizer.
697:07 - We have kernel regularizer,
697:11 - which is equal this regularizer right here.
697:15 - Now let's take this off.
697:16 - And there we go.
697:17 - We have this kernel regularizer,
697:19 - which is now our L2 regularizer.
697:21 - You could always take this off
697:23 - and then from here,
697:26 - and you have from tensorflow.keras.regularizers
697:33 - import L2.
697:37 - So that's it.
697:38 - You could also import L1.
697:39 - So run that.
697:41 - Let's correct that.
697:42 - That's regularizer, regularizer.
697:47 - Okay, so that's fine.
697:50 - We get back to our model right here
697:52 - and then we have just L2.
697:55 - Now you could have this taken off from here
697:59 - and then you add it up here.
698:01 - So pattern valid, activation relu and that.
698:05 - Okay, so we have this and then we add our regularizer
698:10 - and that's fine.
698:11 - Now you could also do this for the dense layers.
698:13 - So just right here,
698:15 - you could have kernel regularizer
698:17 - and that should be fine.
698:18 - So this is how we implement the weight decay with tensorflow.
698:23 - Now you could always modify this parameters.
698:25 - So let's have decay or regularization rate 0.01.
698:38 - So that's it.
698:39 - And then we run our model, that should be fine.
698:42 - And we could go ahead and retrain this model.
698:46 - That's it for this section.
698:47 - Thank you for getting up to this point and see you next.
698:55 - Hello everyone and welcome to this new section
698:58 - in which we'll delve deep into implementing data augmentation
699:02 - with TensorFlow 2.
699:04 - The first method we'll use in implementing data augmentation
699:08 - will be using this TensorFlow image model,
699:12 - which is made of these different functions.
699:15 - Now this is just a few of those functions
699:17 - and you could check out the rest of the documentation.
699:20 - So we'll be able to do stuff like adjusting brightness,
699:23 - contrast, gamma adjustments, saturation adjustment,
699:27 - flipping left, right, flipping up, down and rotation.
699:30 - Using this TF image is known to be a more flexible way
699:35 - of implementing data augmentation
699:38 - as we could alter an input image
699:40 - with all these different functions given to us.
699:44 - The next method we'll use or we'll implement in this course
699:48 - is by working with the Keras layers.
699:51 - And although we're limited by the number of data augmentation
699:55 - layers made available to us,
699:58 - this method permits us to carry out data augmentation
700:01 - more efficiently and hence speeds up the training process.
700:05 - Now to find a balance between these two methods,
700:07 - we could implement our own custom Keras layers.
700:12 - And that's exactly what we're going to do in this section.
700:15 - We're now looking at how to implement data augmentation
700:17 - with TensorFlow.
700:19 - So we're going to get into TensorFlow images.
700:23 - So here we have TF, here is layers,
700:27 - we actually in Keras closes up and TF.image actually.
700:32 - So we have the TF.image with all these methods
700:36 - which we could use in data augmentation.
700:38 - We call that in order to do data augmentation,
700:41 - we're basically modifying the images
700:45 - while keeping the labels fixed.
700:48 - So that said, as you could see here,
700:50 - we have this several methods.
700:52 - Let's check out on this overview
700:54 - and we should be able to get this list of methods.
700:58 - So here we have the list of methods
701:00 - and we have the categories.
701:03 - So here we have image adjustments
701:05 - working with bounding boxes.
701:06 - Here is for object detection where you have to get into this.
701:10 - Here is cropping, flipping, and decoding and encoding.
701:17 - So this is what we have.
701:19 - Now if we get back to the top, we'll have these adjustments.
701:26 - OK, so we have this resize, which we've seen already.
701:30 - And then we have these adjustments.
701:32 - We could adjust the brightness, contrast, gamma, hue,
701:36 - JPEG quality, saturation, brightness, contrast, hue, saturation,
701:41 - and per image standardization.
701:43 - Now notice that these ones here are random.
701:46 - So we are adjusting our image randomly.
701:50 - As you could see, we have random brightness.
701:52 - Whereas here we're actually adjusting this brightness
701:55 - with some fixed parameters.
701:59 - So if you click on this, I just brighten this right here.
702:02 - You see, we have this image and we
702:04 - have this fixed delta, which we choose.
702:07 - Whereas if it's random, let's get back.
702:10 - If it's random, that will be fixed randomly.
702:13 - So click on this random brightness.
702:16 - You see, we are just given a max delta
702:19 - because all we need to pass in here is a range.
702:22 - And then we are going to randomly pick a delta in that range.
702:28 - So that's it for this image adjustments.
702:31 - We also have this cropping.
702:33 - So we could crop out some parts of the image.
702:36 - We could crop or do a central crop.
702:38 - We could crop and resize.
702:39 - So this means if we have this image, so we have this image.
702:46 - And it's 224 by 224.
702:49 - And then we want to do a crop.
702:52 - We could do a center crop like this.
702:54 - We could take this center crop.
702:58 - Sorry for that.
702:59 - We have this center crop.
703:01 - And we have this new image right here, which is this center.
703:05 - Now after getting this image, let's
703:07 - suppose we have now this smaller image of shape,
703:10 - say, 150 by 150.
703:15 - And then what we could do now is to resize this so that we
703:19 - get this shape, which we need to pass into the model.
703:24 - So we could resize this.
703:25 - Hence the reason why we're using the crop and resize right here.
703:33 - And you could as well just crop and then do the resize manually.
703:39 - So that's it.
703:40 - We have flipping, rotating.
703:42 - So flipping, we have an image.
703:44 - We flip it left, right, up, down, random, randomly.
703:49 - So let's get the same thing.
703:50 - But this time around, we're doing it randomly.
703:53 - Rotation and then the transposition of the image.
703:58 - So as you can see, TensorFlow gives us
704:00 - all these methods which we could use in modifying our images
704:05 - and hence augmenting our data.
704:09 - So here we will define this method, visualize.
704:13 - So you could see this clearly.
704:15 - We have the original image.
704:16 - And then we have the augmented image.
704:19 - Now in here, we have subplot.
704:24 - We want just one line, two columns.
704:27 - And this will occupy the first position in that two-colon space.
704:32 - So here now we have this image show,
704:35 - which takes in the original.
704:37 - And then we repeat this.
704:38 - So here we have this second position.
704:42 - And then we have the augmented.
704:46 - Augmented.
704:48 - OK, so that's our visualized method.
704:51 - Now what we'll do is we are going to get the original image.
704:55 - So let's say we have this original image, which
704:59 - is equal an element which we take from our data set.
705:03 - So we have train data set.
705:06 - And then let's just pick one element.
705:08 - So we have this.
705:12 - And then we take just one element from our data set.
705:15 - Now that's it.
705:16 - And this should output a level.
705:18 - So we should have the level.
705:19 - And there we go.
705:21 - So we have the original image here.
705:24 - And then we now work with augmented image.
705:27 - We get this augmented image.
705:29 - And to obtain this only augmented image,
705:32 - we are going to modify this original image.
705:36 - To do this modification, we are going
705:38 - to make use of these methods, which we've seen already.
705:41 - So yeah, let's pick out this flip left right.
705:44 - Click on this flip left right and see what we get.
705:47 - So all we need to do is just pass the image.
705:49 - So let's copy this.
705:52 - And then while we have here is our flip left right
705:55 - and our image, which has been passed.
705:58 - That's fine.
705:58 - We have original image.
706:01 - So that's it.
706:02 - Now we have this augmented image.
706:04 - We run that.
706:05 - And then we could go ahead and visualize this augmentation.
706:09 - So here we have original image and augmented image.
706:16 - OK.
706:16 - So we run this and see what we get.
706:19 - So that's it.
706:19 - You see that now in your data or in your data set,
706:23 - you will not only get this image right here.
706:26 - You will not only get this one, but you
706:28 - would get this and this.
706:31 - So this means our data set now will be multiplied by two.
706:36 - Let's check on other augmentation strategies.
706:39 - Let's get back.
706:42 - Here we go.
706:42 - We have flip up down, this similar to what we've seen already.
706:47 - OK, let's check out.
706:48 - Let's do this random flip up down.
706:50 - And then let's click on this.
706:52 - And then rotation by 90 degrees.
706:55 - So here we have this image, similar.
706:57 - And then they could specify the seed.
707:00 - So here we have the random flip up down from that.
707:04 - And then here we have random flip up down.
707:16 - OK, so we run that again.
707:18 - We have augmented image.
707:20 - And then we visualize.
707:22 - You see that it happens that it's exactly the same image,
707:25 - which is outputted.
707:26 - Let's run this again.
707:31 - And now what we get is instead this.
707:33 - So notice how there's a difference
707:35 - between these two images.
707:37 - So that's it.
707:38 - The next will be this rot 90 degrees.
707:40 - Click on that.
707:41 - You could always feel free to click and then understand
707:45 - what it actually means.
707:46 - Like, yeah, we're told to rotate image counterclockwise
707:50 - by 90 degrees.
707:51 - Now let's take this off.
707:54 - We know clockwise is this direction.
707:57 - So rotating counterclockwise means
707:59 - we'll be rotating it in this direction by 90 degrees.
708:04 - So that said, let's take this off.
708:06 - And then we have rot 90.
708:09 - We run that.
708:10 - And there we go.
708:12 - OK, so as you can see, we've rotated this by 90 degrees
708:16 - in the anticlockwise direction.
708:19 - We can also try out this adjust brightness and random
708:23 - saturation.
708:24 - So let's get back to this notebook.
708:27 - We have your adjust brightness.
708:33 - And we run this visualize.
708:39 - We have this error missing positional argument.
708:42 - So let's get back to this and understand how it's used.
708:46 - We check on adjust brightness.
708:49 - And there we go.
708:50 - We need to pass in a delta.
708:51 - So we have to pass in this delta.
708:55 - Now this delta should be in the range negative 1, 1.
709:00 - And as you can see, when you add this delta,
709:03 - it basically just adds up to each and every pixel we have.
709:07 - So this 1 turns to 1.1.
709:09 - This 11 turns to 11.1, and so on and so forth.
709:12 - And we told you that this delta is a scalar.
709:16 - And it's an amount to add to the pixel values.
709:19 - So let's go ahead and add this delta.
709:22 - Let's have that.
709:23 - We add the delta.
709:25 - And we have 0.1.
709:29 - So we run that again.
709:30 - And this should be fine.
709:31 - OK, so that's fine.
709:33 - We now visualize.
709:35 - And that's what we get.
709:36 - You'll notice that there's some difference with this,
709:39 - like this one appears brighter than this.
709:43 - And let's go ahead and increase this.
709:45 - Let's take 0.8 and then have that.
709:48 - OK, so you see, we are able to modify this brightness.
709:54 - We could also include the random saturation.
709:58 - Random saturation.
709:59 - And we have the original image.
710:02 - Let's get back to documentation for random saturation
710:05 - and see what we need to pass in.
710:07 - So there we go.
710:08 - We have this image.
710:09 - We pass in the lower and the upper limit.
710:12 - So here, let's find this lower upper limit.
710:16 - We are told here, we're going to get an error
710:19 - if the upper is less than the lower, which is logical.
710:22 - The lower should be less than the upper.
710:24 - And then if the lower is less than 0,
710:26 - so we have to ensure that we're dealing with values greater
710:28 - or equal to 0.
710:30 - So getting back here, we have saturation, that.
710:33 - And then we have lower.
710:36 - Let's say 2.
710:37 - And then upper, let's say 12.
710:41 - Take this off.
710:42 - Run again and visualize.
710:46 - OK, so that's fine.
710:47 - So you see, we've added some random saturation.
710:50 - And this is what we get.
710:52 - We could obviously reduce this.
710:54 - We could make this fall in the range 0, 1.
710:58 - And here is what we get.
711:00 - OK, we now check out on cropping.
711:04 - Let's check out the central crop.
711:07 - Here we have central crop, central crop,
711:12 - and the original image.
711:15 - So that's what we have.
711:16 - Let's make sure we understand exactly how it's
711:19 - meant to be used.
711:21 - So here we have this.
711:22 - And then there's a central fraction.
711:24 - So here they explain how this works.
711:27 - Now, where x is the central 50% of the image,
711:33 - it is a float which lies between 0 and 1.
711:36 - As you can see here, 0.5 has been picked.
711:39 - Meaning that we're going to pick 50% of the image.
711:42 - But this portion of the image we're
711:45 - going to pick has to be centralized,
711:48 - has to be surrounded in the center.
711:50 - So that said, we've understood how that works.
711:54 - We can now put that, let's say, 0.3.
711:57 - Run that and see what we get.
712:01 - OK, so you see, we get this portion right here.
712:04 - Now, what if we expand this to, like, say, 0.8?
712:08 - We should.
712:08 - We expect to have some black regions now.
712:12 - Run that, and you see, we have some black regions.
712:16 - So it's actually centralized.
712:19 - Let's take this off.
712:20 - You see, it's actually centralized.
712:22 - And the reason why we have something like this,
712:25 - it's actually cutting out like this and something like this.
712:31 - So this is what we get in here, something like this.
712:36 - OK, so this is what we get when we do this central crop.
712:42 - Now, the way we're going to integrate this augmentation
712:44 - in our data pipeline is going to be
712:47 - similar to the way we did with the resize rescale.
712:50 - So just like we used this map method,
712:54 - we are going to reuse this same method for augmentation.
712:59 - Here, let's suppose we are going to add this code
713:03 - and then define our augment method.
713:09 - So we have this.
713:11 - Let's put this down.
713:12 - And OK, so let's just have your augment.
713:17 - We have this augment method, image, level.
713:22 - And then what we'll do is we'll take the image.
713:26 - And then for that image, we're going
713:27 - to have the rotate 90 degrees of that image.
713:35 - Then next, we are going to have adjust saturation.
713:42 - And then pass in the image and specify the saturation factor.
713:47 - So here we have our saturation factor equal, say, 0.3.
713:52 - Now you could always feel free to visualize this
713:55 - so you don't get to poorly augment your data.
714:00 - That said, here we could have this saturate.
714:04 - Always don't copy that.
714:05 - Anyway, this flip left, right, let's come back to this.
714:08 - We have this adjust saturation.
714:12 - We could check that out.
714:14 - And let's have it here.
714:17 - OK, we have that.
714:19 - So we run this and visualize to see exactly
714:22 - what it's going to look like.
714:24 - We modify that image to original image and then run it again.
714:29 - OK, so this is what we get.
714:31 - Now that's fine.
714:32 - We could also include the flip left, right.
714:35 - So here we have image, tf.image, flip left, right.
714:43 - And then what we do is we could return the image and the level.
714:49 - So that's fine.
714:51 - Then we could also just simply include this resize
714:54 - with scale in this augment.
714:55 - So here we have image and the level equal resize with scale,
715:03 - which takes in the image and the level.
715:06 - So we resize and rescale before doing the augmentation.
715:10 - So that's it.
715:11 - Now we have this augments meta defined.
715:14 - We could now, instead of doing this here,
715:17 - so here we have augments.
715:21 - Now another thing we want to do before doing the map
715:24 - is actually shuffling.
715:26 - So we'll modify the order in which we're doing things.
715:29 - Now here we have this train data set, could have that.
715:34 - And then we have the train data set right here.
715:39 - .shuffle,.match, and.refresh.
715:44 - So yeah, that's what we have.
715:46 - And then we just include the mapping right here.
715:48 - So instead of doing the mapping before the shuffling,
715:51 - we'll just do it after the shuffling.
715:54 - And there we go.
715:55 - Let's now have this, it's basically this with augments.
715:59 - So we just have that.
716:01 - And then there we go.
716:03 - We have this map.
716:04 - OK, so we now have that for the training.
716:07 - And then we could close this up.
716:10 - So that's fine.
716:12 - We repeat the same process for the validation and the testing.
716:15 - So yeah, we're going to modify just as we did with the training.
716:19 - We have here our validation.
716:23 - Then we include this map right here, map, resize, rescale.
716:29 - Now notice how we are not augmenting the validation
716:31 - and test data sets.
716:33 - So here we have the validation.
716:34 - And then we now do the same for the testing.
716:39 - Anyway, for the testing, we only did the map.
716:43 - That is, we only did the resize, rescale.
716:46 - So that's fine.
716:48 - We will now take this off, train validation.
716:51 - That's off.
716:53 - That's fine.
716:56 - OK, so here we have our test data.
716:58 - We have our train data.
717:00 - And then we have our validation data.
717:04 - At this point, everything is set up.
717:06 - We could now rerun ourselves so we start out with training.
717:11 - So that's fine.
717:12 - We will come back, get back to this.
717:14 - We have our data.
717:16 - We run all the cells and our model created.
717:22 - So yeah, we're going to work only with this sequential API
717:26 - and keep out all this other methods of creating models.
717:30 - Once we're done with that, we get right up to this level.
717:36 - OK, so now we get into training.
717:40 - That's fine.
717:41 - Compile the model.
717:43 - And we wouldn't use any callbacks for now.
717:46 - Take that off and start training.
717:50 - We're now done with training.
717:52 - And we'll see the results right here.
717:55 - So as you can see, the model performs very poorly.
717:59 - And we'll try to understand why the model doesn't perform
718:03 - as well as it performed without the data augmentation.
718:09 - Now, if you can recall, what we did for the data augmentation
718:13 - was that we rotated the image by 90 degrees anti-clockwise.
718:18 - We adjusted the image, the image's saturation.
718:22 - And then we flipped the image left right.
718:25 - But taking a look at the kind of data set
718:28 - we're dealing with, this saturation
718:31 - shouldn't be a great idea.
718:33 - And this is simply because if we look at each and every one
718:36 - of this, what differentiates a parasitized cell like this one
718:41 - and this uninfected cell is this patches we have right here.
718:47 - So you'll notice that with a parasitized cell,
718:49 - we generally have these kinds of patches.
718:52 - Whereas with an uninfected cell, we particularly have no patch.
718:56 - And with this, whereas here you could see these patches.
719:00 - And so rotating or flipping left right
719:03 - wouldn't change much about that.
719:06 - So that is acceptable.
719:08 - Now, the problem with modifying the contrast is
719:11 - when you modify the contrast, you
719:13 - tend to make the parasitized cells and uninfected cells
719:19 - less differentiable.
719:21 - And so it isn't a good idea in this case
719:24 - to modify the contrast or saturation of the image.
719:30 - Let's take this example so we could clearly
719:32 - understand why this saturation data augmentation strategy
719:37 - isn't a good idea.
719:39 - So yeah, right here, what we do is we just copy out this
719:42 - and then paste here.
719:44 - We take out just two elements from our dataset.
719:47 - And then here we have this 1, 4, and 2i plus 1.
719:57 - And we have the image show.
720:00 - From here, we create another subplot.
720:03 - There we go.
720:04 - We have that.
720:06 - And this one now is the saturation.
720:09 - So we have image.adjustsaturation.
720:13 - That's it.
720:14 - And then we'll pass in the image.
720:16 - And that's what we have.
720:17 - So here we have this first subplot and this other subplot.
720:21 - We have that, too.
720:22 - Now we could run this and see what we get.
720:27 - I'm using the positional argument.
720:28 - That is the 0.3, the saturation rate.
720:33 - So here we have 0.3.
720:35 - We're using exactly what we used previously.
720:37 - So here we have that.
720:39 - And this is what we have.
720:40 - In this example, we have both an uninfected cell.
720:44 - So we wouldn't actually see much difference.
720:47 - It's true this was modified.
720:49 - This was modified.
720:50 - But there isn't much difference.
720:51 - Let's take a parasitized cell.
720:54 - Let's rerun this again.
720:56 - Hopefully we get a parasitized cell.
720:57 - OK, as you could see here, let's take this off.
721:02 - As you could see, in this parasitized cell,
721:05 - what could clearly show the model
721:07 - that this is a parasitized cell is this patch we have right
721:10 - here.
721:11 - But when we adjust the saturation, it turns into this.
721:15 - And you'll notice that this parasitized cell now
721:17 - looks more like an uninfected cell.
721:20 - So the model isn't able to differentiate
721:23 - between these two clearly.
721:24 - So we see clearly that using the adjust saturation
721:30 - de-developmentation strategy isn't a great idea.
721:33 - Let's comment this part.
721:34 - And that's fine.
721:36 - Now we're going to modify our strategy.
721:39 - So just right here, what we had previously,
721:42 - we're going to take this off.
721:44 - So we comment this.
721:45 - And then we'll retrain our model.
721:49 - And so here we retrain our model after modifying
721:52 - our de-developmentation strategy.
721:54 - We run this again.
721:56 - And after training, as you could see, we get the results.
722:00 - Now we check out this loss.
722:01 - This is what we get for the loss.
722:03 - And if we look at the accuracy, we
722:05 - see that it looks more like what we had previously.
722:08 - Though we still have that maximum validation
722:13 - accuracy of about 94% and the trained accuracy of about 99%
722:20 - right here.
722:21 - And though the de-developmentation strategy
722:24 - hasn't closed this gap we had between the training
722:27 - and validation accuracy, we're seeing other sections
722:31 - how de-developmentation will be very instrumental
722:34 - in helping reduce this gap between the training
722:38 - and the validation scores.
722:40 - So far, we've been applying de-developmentation
722:43 - by using the image methods.
722:47 - Now the other way we could apply de-developmentation
722:51 - is by directly making use of Keras layers.
722:54 - So you come right here, tf.keraslayers, as you could see.
722:59 - And then you would notice that among those layers,
723:03 - we have those layers which permit us
723:07 - do modifications on images, like with this random contrast,
723:13 - which permits us randomly adjust contrast during training.
723:18 - Here we have the definition of this layer,
723:20 - this random contrast layer.
723:22 - And we have those attributes right here.
723:28 - Now, apart from this random contrast,
723:30 - we have the random crop, we have the random flip,
723:33 - and you could always get all these definitions,
723:35 - get back to, yeah, let's have this flip.
723:37 - Now the mode horizontal and vertical,
723:40 - so we're going to flip it both horizontally and vertically.
723:43 - We could also have other modes like horizontal,
723:46 - that's kind of a left-right flip.
723:49 - We have vertical, kind of up-down flip.
723:52 - We have horizontal and vertical,
723:53 - which is a mixture of the two.
723:55 - And the default is horizontal-vertical,
723:57 - as we've just seen right here.
724:00 - We also have this random rotation,
724:02 - which you could see, random translation, random zoom.
724:06 - So you could randomly zoom the data you're training on.
724:11 - And you also have this resizing.
724:13 - So we had looked at resizing previously,
724:15 - but not actually under the documentation.
724:17 - So we did the resize using TF image.
724:19 - Now you could also do this resizing,
724:21 - yeah, this resizing layer.
724:23 - Now, here you have the height and the width,
724:25 - as we had, similar to what we had in the image.
724:29 - We could scroll down and check on others.
724:34 - We don't really find anyone here.
724:35 - We could scroll back up, and that's it.
724:38 - Anyway, what's important to note is that you could create,
724:42 - you could implement data augmentation
724:44 - via two main ways, using the TF image
724:46 - and using the Keras layers.
724:48 - Now, we are going to look at how to re-implement
724:51 - this data augmentation strategy we had previously
724:54 - via the Keras layers.
724:56 - So here we had this rotation.
724:59 - So we had rotation and we had the flip left, right.
725:02 - So we'll get back to the documentation.
725:04 - And then what do we have here?
725:05 - We have this random flip right here,
725:10 - and then we're going to select the mode to horizontal,
725:12 - since we're doing left, right flip.
725:14 - So we'll copy this out.
725:16 - We have that layer, random flip.
725:18 - Let's put this just below.
725:20 - So this is the augmentation.
725:22 - This is augmentation using TF image.
725:26 - So TF image, augments,
725:29 - and then this is augmentation using Keras layers.
725:33 - So TF.keras.layers, augments.
725:38 - Okay, so here we have our augments.
725:41 - It takes the image and the level.
725:45 - But since we actually built in Keras layers,
725:48 - we are not going to take this this way.
725:49 - We're going to have augment layers.
725:53 - It's just a way of differentiating between
725:54 - these two augment and augment layers,
725:56 - which is a sequential model.
725:59 - So we have the sequential.
726:01 - We're going to make use of the sequential API,
726:03 - TF.keras.sequential.
726:05 - And then just right in here,
726:07 - we are going to specify all these different steps.
726:13 - And so we copied this out from the documentation.
726:17 - They just space it out here.
726:18 - We have this random flip.
726:20 - So we're going to use this random flip right here.
726:22 - Let's take this.
726:24 - Let's import random flip from the layers.
726:25 - So we have your Keras layers.
726:29 - There we go.
726:30 - We'll import random flip.
726:32 - Now we could have this.
726:33 - We have random flip.
726:36 - Okay, so we have the random flip imported.
726:39 - And then the other one is that rotation.
726:42 - Let's get back and we have your rotation.
726:46 - Let's check on that.
726:47 - We have scroll up and let's just do a quick search.
726:52 - So here we have rotation.
726:54 - Okay, so here we have random rotation.
726:57 - And don't forget,
726:58 - we will rotate in 90 degrees in the anti-clockwise direction.
727:01 - So here we have to be careful to make sure we
727:04 - re-implement that same kind of rotation we had seen before.
727:09 - Now, here we have this definition.
727:11 - We have a factor, fill mode,
727:12 - reflect interpolation by linear,
727:15 - the seed for disability, fill value.
727:18 - And that's it.
727:19 - Now let's check down here
727:21 - and understand better this factor.
727:25 - Now this factor we have here is actually this tuple.
727:29 - And this tuple is going to represent our range.
727:32 - To better understand this range,
727:34 - you should note that it's like each and every one of this
727:37 - represents a fraction of two pi.
727:39 - Now what is pi?
727:41 - Pi equals 180 degrees.
727:43 - So two pi is 360 degrees.
727:46 - So we're looking for a fraction of 360 degrees.
727:49 - Now recall that we're going anti-clockwise by 90 degrees.
727:54 - Like we're just trying to redo the exact same thing
727:58 - we have with a TF image.
728:00 - So here we're gonna look for a way to get 90 degrees
728:03 - from this 360.
728:04 - And to do that, we need to divide this by four.
728:07 - In other words,
728:08 - it means we're gonna get 25% of this 360
728:13 - and 25% of 360 is 90 degrees.
728:16 - So 90 degrees, not percent.
728:19 - So that said, we wanna get 25% and that's fine.
728:23 - Now from what is given to us here,
728:26 - this is, we represent as a single flow.
728:28 - This value is used for both the upper and the lower bound.
728:31 - For instance, this results in an upper rotation
728:34 - by a random amount in this range.
728:38 - A positive value means rotating counterclockwise
728:42 - while a negative means rotating clockwise.
728:44 - Now we're interested in rotating counterclockwise
728:46 - as anti-clockwise
728:48 - that we're actually interested in rotating this way.
728:50 - Recall, it was this, this is clockwise.
728:53 - So we're not interested in this,
728:54 - we're interested in this anti-clockwise 90 degrees rotation.
728:58 - So we take positive values
729:00 - and then we'll try to ensure that it lies in that given range.
729:05 - Now, given that we are actually picking just random numbers
729:08 - and unlike the ROT90 method we have seen,
729:11 - which was somehow fixed.
729:13 - Yeah, what we'll do is we could just pick a range
729:15 - between say 90 degrees and say 90.1 degrees
729:23 - or any value very close to 90.
729:25 - So that said, recall it's gonna be positive,
729:28 - unlike if it was clockwise, it will be negative.
729:33 - So yeah, we're gonna pick the 0.25
729:37 - because we wanna get 25% of this 360.
729:41 - So we have 0.25, that's positive 0.25
729:44 - up to say 0.2, let's say five, zero, one.
729:52 - So let's pick out this range.
729:53 - So we'll work in this range
729:55 - so that we'll be around this 90 degree
729:58 - that we're actually looking for.
730:00 - Now that said, since we've taken 0.25, 0.205, two zero one,
730:06 - what we'll have here is 25% of two pi,
730:10 - which is 90 degrees, so it's gonna be translating it
730:12 - to 90 degrees and a value very close to 90 degrees.
730:18 - So it'll be like 90 point, a value very close
730:22 - or 0.00, whatever, degrees.
730:26 - So it's gonna be around this.
730:27 - So we pick this up and that's fine.
730:30 - Now we're sure it's anti-clockwise
730:33 - because as we've picked it to be positive.
730:36 - Now the fill mode and the fill values
730:38 - will just be the default values, even the interpolation tool.
730:42 - So that said, we get back to the code.
730:45 - Let's copy this out right here, random rotation,
730:48 - copy this out, get back to the code.
730:51 - Let's include random rotation here.
730:53 - So we have random rotation.
730:56 - Okay, random rotation, that's fine.
730:59 - We could now run this, let's run this cell
731:03 - and then get back to our method
731:06 - or this layer we're trying to construct right here.
731:11 - So here we have the random flip
731:12 - and then the random rotation.
731:14 - Now we started in this other method with the rotation.
731:18 - So yeah, we're gonna redo that.
731:20 - Now we have this random rotation, random rotation.
731:26 - Okay, and then we have this factor.
731:30 - So we just do exactly what we just agreed to put.
731:34 - So we have the factor, a triple take 0.25
731:37 - and 0.62501.
731:41 - Okay, now we have that, which is,
731:43 - for the rest we take the default value.
731:44 - So we don't need to put those values.
731:46 - So that's fine.
731:47 - We've done with this first layer.
731:49 - The next layer will be this random flip.
731:53 - So we just put out this random flip.
731:55 - Let's copy this, copy that and then paste it out here.
732:00 - Now we have our random flip, we've decided on the mode,
732:03 - take that off and that's fine.
732:06 - Now this is horizontal.
732:07 - So let's have this as horizontal.
732:09 - Okay, so we have this random flip and that and there we go.
732:15 - We have our layer, sequential layer.
732:18 - Okay, now let's take this off and that's fine.
732:23 - So we have this augmented layers or this augmented layers
732:26 - which we run this time around.
732:28 - Horizontal is not defined.
732:29 - Now let's use this small case, horizontal and run again.
732:39 - Horizontal still not defined as pass as a string.
732:42 - Horizontal, run again.
732:45 - Then we could redefine an augment method.
732:48 - Let's call this method augment layer
732:50 - and then this takes in the image, image layer, sorry, levels
732:55 - and then what we'll do is we're going to return
733:02 - this augment layers which we just created
733:05 - and then pass in the image,
733:07 - specify that training is equal true.
733:10 - That's okay and then we send out the levels.
733:12 - So here we're going to take this augment layers
733:15 - as we're going to take the image
733:16 - and pass into this layer right here.
733:21 - It's going to rotate the image and then into this other layer
733:25 - which is going to flip that image.
733:27 - So we get our output which has been augmented
733:29 - and we have our levels.
733:30 - We run this, run this and then we get to this year
733:35 - where we have our augment and we have augment layer.
733:39 - So we call the augment layer method right here.
733:42 - We call this method, that's fine.
733:44 - We run that and everything is okay.
733:48 - Now after running this training, we have this error.
733:51 - So it shows that we have an error at a level of the shapes.
733:55 - Now let's get back.
733:57 - The problem here is we forgot to do this resizing
734:01 - before the data augmentation.
734:04 - So we could copy this out
734:05 - and then put out this resizing here.
734:08 - We have the image and levels,
734:11 - level, let's say image and level, we have that.
734:14 - And then we resize and rescale.
734:16 - After resizing and rescaling, we now do the augmentation.
734:19 - So we run this again and then we run the stringing.
734:25 - As you could see all as well,
734:27 - we will now look at another way of creating
734:30 - of doing data augmentation.
734:33 - Before doing that, we should also note
734:35 - that instead of doing this resize rescale
734:38 - using the TF image as we had done here,
734:41 - that is using this TF image resize
734:44 - and then by dividing by 255 as we just did,
734:48 - we could also use the resizing and rescaling layers.
734:52 - So right here, we're gonna include those layers.
734:56 - Here we have resizing and then rescale.
735:03 - We run that and then just here,
735:06 - we're gonna include that resize rescale.
735:08 - So here we say resize rescale layers.
735:13 - Okay, so now we have this sequential API
735:16 - and then we always start with resizing.
735:18 - So we have the resizing, take this off and have this.
735:24 - Yeah, we're gonna put in size.
735:27 - We have in size, in size.
735:30 - Okay, and then we do the rescaling.
735:33 - So we have rescaling, take this off
735:37 - and we do the rescaling.
735:39 - So here we have this taken off, one divided by 255.
735:46 - Okay, so now we have those layers
735:48 - which are responsible for resizing and rescaling.
735:52 - Resizing and rescaling.
735:56 - So we'll see how we'll replace this method
735:59 - in doing the resize rescale.
736:01 - So that said, let's get back to this augment.
736:03 - We had augment layer right here.
736:06 - After getting this image,
736:09 - we do resize rescale and layer.
736:14 - So we do the resize rescale on the image
736:17 - before doing the augmentation.
736:19 - Let's take this off now and this should be fine.
736:22 - Now, since we've built this resize rescale layers,
736:24 - we could replace this here.
736:26 - Resize rescale layers, even for the test set.
736:29 - And then we do same for this.
736:32 - This is already fine.
736:34 - Augment layer is what we've seen already.
736:37 - For the validation, we have resize rescale layers.
736:41 - Okay, so that's fine.
736:42 - We run this, run the validation, run the train
736:45 - and that's fine.
736:48 - We then train our model and see what we get.
736:52 - So that's it.
736:53 - Our model is training.
736:54 - Everything is working well.
736:55 - Now let's pause this and then check out
736:59 - this other interesting way of doing this data augmentation.
737:03 - So as of now, we've seen how to do data augmentation
737:06 - by using TF image and then by building these layers.
737:11 - Now we'll see the importance of building these layers.
737:15 - But before looking at this importance,
737:17 - it's important to note this and that
737:22 - is a fact that when you're working with a TF image,
737:25 - you have more flexibility as with these layers,
737:28 - you are kind of like restrained to what you could do
737:32 - with data augmentation.
737:33 - Though these layers already have many
737:38 - common data augmentation strategies like the rotation,
737:41 - the zoom, the crop, the flipping and the others.
737:46 - Nonetheless, when you're working with a TF image,
737:49 - you'll see that you have many more operations you could do.
737:52 - Like if you open up this TF image right here,
737:56 - you see you have many more methods
737:58 - and you could do many more stuff with this.
738:00 - Anyway, getting back to our code,
738:04 - we have to note that the advantage of these layers
738:07 - is that you could embed this into the model itself.
738:12 - Now let's explain.
738:13 - Supposing you have, let's take this off.
738:16 - Supposing you have your training pipeline here.
738:19 - So you have that TF data, you do the shuffling,
738:22 - you do the mapping, you do prefetching, caching
738:25 - and everything you need to do with your training data.
738:28 - So yeah, you're doing your data.
738:30 - And then next, after doing all this resizing,
738:33 - reskilling and everything,
738:35 - you now pass this into your model.
738:37 - So you pass this into your model
738:41 - and then you train your model and have all your results.
738:45 - Now, when you want to test this model,
738:50 - you will actually still need to do resizing and reskilling.
738:54 - So you still need to ensure that you resize your image
738:59 - and then you also ensure that you reskill your image
739:03 - before passing to the model.
739:05 - Now, if you recall, like, let's go down,
739:08 - let's get down here.
739:09 - You would see under the testing section,
739:12 - let's look at this, under the testing section,
739:17 - that we made use of this test data.
739:20 - And this test data, as we have seen from here,
739:25 - was already resized and rescaled.
739:27 - So if you check here, you see,
739:29 - we had done some resizing and reskilling right here.
739:33 - Now, what if you have to take this,
739:36 - this your model, which you've trained into another setup?
739:40 - And obviously in that new setup,
739:42 - you will still need to resize and reskill
739:45 - because just like here, before testing your model,
739:49 - you had to resize and reskill.
739:52 - So what if we put this resizing and reskilling in the model?
739:59 - So if we embed this resizing and reskilling in the model,
740:02 - it means that no matter where we go with this model,
740:07 - we don't need to resize and reskill.
740:09 - All we need to do now, let's, we'll take this off,
740:13 - right, I will take this and put in the model.
740:16 - So all we need to do now is just to pass in your data
740:18 - without the resizing and reskilling and the job is done.
740:23 - So now we're going to see how to do this with this augment
740:26 - or rather with the resize, rescaled layers,
740:29 - which we've just seen.
740:30 - We also do with the augment,
740:31 - but the augment isn't very useful since in testing time,
740:34 - we don't make use of the augment.
740:37 - What's more important here to notice
740:38 - the resizing and rescaling,
740:40 - although if you can include this augment layers
740:43 - into your model and make the training faster.
740:45 - Anyways, let's go ahead and see how this is done.
740:49 - So firstly, you have the layers, which you've built up,
740:52 - you've had the augment layers,
740:54 - and then you've had the resize, rescaled layers,
740:57 - which we have had here.
740:59 - So all we need to do is to get to this model right here.
741:03 - And then after the input layer,
741:06 - you just have the resize, rescaled layers, and that's it.
741:11 - And then you have augment layers.
741:16 - So here you have resize, rescaled layers,
741:18 - and augment layers, and that's all.
741:21 - So with this, you don't need to do any resizing or rescaling
741:24 - on your input because this is going to be done.
741:27 - All these layers, which are now part of your model.
741:32 - And then here too,
741:33 - you don't really need to specify this image size.
741:35 - You could now have this as known.
741:38 - And then yeah, you have that.
741:41 - So you could put in any image
741:42 - and then the resizing and rescaling will be done
741:45 - in that model such that you could put this model
741:49 - or use this model in any environment.
741:51 - And all you need to do is to pass in the image
741:53 - and you have your corresponding output.
741:55 - Now we've modified this model.
741:57 - We need to also modify the way we're working
741:59 - with our testing and training validation datasets.
742:03 - So let's get back to this right here.
742:07 - We have the down here, we have this,
742:11 - augment, augment, and then we have this test dataset.
742:15 - So with the test dataset,
742:16 - now we do not need to do this again.
742:18 - So we just comment this
742:20 - because we don't need to resize and rescale anymore.
742:23 - Now with the validation, you see with the validation,
742:27 - we do not need to do this mapping anymore.
742:30 - You see, we take this off.
742:31 - We don't need to do this resize and rescaling.
742:33 - And then with the training,
742:35 - get back to the training, this training,
742:39 - we do not need to do this augment
742:42 - because we've included this already.
742:44 - So all we need to do now is just to shuffle,
742:45 - put in some matches and do the profession.
742:48 - So that's it.
742:49 - So we have this, okay, we will now rerun this
742:53 - and see how the training goes.
742:56 - We run this training.
742:58 - As you can see, we have this very long trace
743:02 - and with all this, with this message right here,
743:05 - which isn't very readable.
743:06 - So what we could do is generally
743:08 - when you're trying to get or understand an error
743:12 - and you don't seem to have an idea of what's going on,
743:16 - you could run the model eagerly.
743:18 - So we use an eager mode.
743:20 - So run eagerly equal true.
743:22 - And here you see when we run this again,
743:25 - you would have a shorter error message
743:28 - or more readable error message.
743:30 - So you check this out
743:31 - and you see you just have this shorter error message.
743:33 - It should be noted that TensorFlow operates
743:35 - into main modes.
743:37 - That is the graph mode and the eager mode.
743:40 - So as you could see here, when you don't specify this,
743:44 - you run the training in the graph mode.
743:46 - The graph mode is actually faster
743:48 - and more efficient way of training our model,
743:51 - whereas the eager mode is slower
743:53 - but more easily debuggable.
743:55 - So when you have errors like this,
743:57 - you advise to run in this eager mode
743:59 - and when everything is okay,
744:01 - you could take this off or set this simply to false.
744:05 - But by default, it's not true.
744:08 - So by default, we are not in eager mode
744:10 - while doing the screening.
744:12 - Now that said, let's scroll down and check on our error.
744:17 - And what it reads is we cannot batch tensors
744:19 - with different shapes in component zero.
744:22 - First element had the shape
744:23 - and element one had this other shape.
744:26 - So we try to, we understand from this
744:29 - that since we're doing the rescaling in the model
744:33 - that is in here, we're actually doing the rescaling,
744:35 - the resizing and rescaling in here.
744:37 - We have this batching, that's what we had here.
744:41 - We had doing this batching on elements
744:44 - which do not have exactly the same shape.
744:47 - And so to avoid this,
744:49 - we'll just fix this batch size to be equal one.
744:52 - So we work with batches of one and yeah,
744:55 - work with batches of one, we run that.
744:58 - And we run our model again.
745:01 - We run this and that's fine.
745:04 - So you'll notice that training starts
745:06 - and everything now works well.
745:08 - Those gonna be slower since we're no more working
745:11 - with batches of 32, but now batches of one.
745:14 - And also to ensure that this goes even faster,
745:17 - let's take this off.
745:18 - So let's pause this and then let's restart training
745:23 - with the graph mode.
745:24 - So we run this and run this
745:26 - and this should now be faster than what we had previously.
745:30 - Now training is going on, we could pause with the training
745:33 - and see how it's gonna be very easy now
745:36 - with this model to run inference.
745:40 - To test this, we've had this cell here, the cell image
745:44 - which we'll run inference on.
745:46 - So we have the cell image
745:48 - and then we'll make use of OpenCV to read this image.
745:52 - So yeah, we have our image equal CV2 in read,
745:59 - in read this image.
746:01 - So, and yeah, you could start by printing out image.
746:07 - So you would have that not defined.
746:11 - Let's import CV2 right here.
746:15 - Okay, so we go ahead and import CV2.
746:17 - We can close this and we could import that just here.
746:21 - So we import CV2, that's OpenCV, we run that.
746:25 - And then we now rerun this cell.
746:29 - OpenCV actually stands for Open Computer Vision.
746:32 - So this is a very popular image processing library
746:36 - which normally you cannot do image processing
746:40 - or computer vision today without really working with this.
746:46 - So that's how with OpenCV you could read an image.
746:49 - So image read, and then here we go.
746:52 - You have the sprint of the shape.
746:54 - So you see that it matches up with what we expect.
746:58 - We have that.
746:59 - Okay, so here's the shape of our image.
747:01 - And now we are ready to, let's add a batch.
747:05 - So here we're gonna have image equal tf.expanddems
747:12 - and then we have the image
747:13 - and then we specify the zero axis.
747:16 - So we wanna add the batch dimension
747:19 - and then we print out this image.
747:21 - So here we have our image.
747:24 - Okay, that's fine.
747:25 - Let's print out the shape.
747:26 - So we should see that.
747:29 - Okay, we have this here.
747:32 - And now you see that we're gonna pass this
747:34 - into our model directly.
747:36 - So here we're just gonna do a lunette model
747:40 - that predicts image and that's it.
747:46 - So all you need to do is just do this.
747:48 - We don't need to divide by 255.
747:50 - We don't need to resize
747:51 - and the model is gonna understand.
747:53 - Now note that we are passing this image of this input,
747:57 - which is 265 by 262 by three.
748:01 - And our model is gonna automatically resize this
748:05 - because we've included resizing in the model.
748:08 - So let's run this now and see what we get.
748:11 - And that's fine.
748:12 - So it tells us that zero, that is, it's parasitic,
748:17 - but which is actually wrong, but this is normal
748:19 - because we have not really trained the model
748:22 - for a long period of time.
748:24 - And clearly from here, our results show that
748:28 - we should expect very poor results.
748:30 - Why is this disturbing?
748:31 - Let's scroll down.
748:32 - Okay, so let's scroll this way and then scroll back up.
748:36 - Okay, so you see that here we have this poor accuracy,
748:39 - 48%, if we train for longer,
748:42 - we can get to the 99% wish we had already.
748:45 - So this is actually done to show how easy it's gonna be
748:50 - for you to run inference on this model
748:54 - anytime you wanna in any environment you find yourself.
749:01 - So far, we've seen two methods of creating,
749:06 - of implementing data augmentation with TensorFlow.
749:09 - We've seen the TF image and then we've seen the TF Keras.
749:13 - Now with the TF Keras, it's actually interesting
749:17 - when you want your pre-processing that is,
749:19 - say, resizing, rescaling, and the augmentation
749:22 - to be in the form of layers.
749:24 - And with the TF image, it's more flexible to work with.
749:28 - Now, we're gonna look at a way in which we could
749:33 - build a custom Keras layer
749:36 - based on the TF image operations.
749:40 - So recall that when we're working with a TF image,
749:43 - we had this rod 90 degrees,
749:47 - which was right here.
749:49 - And when we wanted to work with this layers,
749:51 - we had to play around with this
749:54 - to give us that same rod 90 degrees effect.
749:58 - Now, what we'll do is in order to create a layer,
750:02 - which takes, to create this rod 90 degrees Keras layer
750:07 - and not just use this random rotation
750:09 - because it's not exactly this,
750:11 - we are gonna define a class
750:14 - which inherits from Keras layers.
750:17 - So right here, we have this class,
750:21 - which we'll call rod 90.
750:24 - Let's call it rod 90.
750:26 - And then it inherits from layer.
750:29 - We have all in it.
750:33 - There we go.
750:35 - Self.
750:35 - Super.
750:39 - There we go.
750:40 - dot init.
750:43 - And that's fine.
750:44 - So yeah, we have this rod 90, which inherits from layer.
750:49 - And then we'll define the call method.
750:51 - So we'll define the operation,
750:53 - which is gonna be carried out in this rod 90 layer,
750:57 - which we're about to create.
750:58 - Yeah, we have self and then we have the input image.
751:02 - So what we simply wanna do here is we wanna have,
751:05 - we wanna return the tf.image, the rod 90.
751:10 - So we're carrying out the same operation,
751:12 - but now we're converting this into a layer.
751:14 - So yeah, we take that and then we have image.
751:17 - So that's what we output now.
751:20 - That sounds fine.
751:21 - So yeah, we've had this rod 90, which we've created.
751:24 - We run this.
751:25 - And then instead of this random rotation,
751:29 - we are gonna use this rod 90.
751:31 - So yeah, we have rod 90.
751:33 - Take this off, not in its past, and that's it.
751:38 - So that's it.
751:39 - We have our rod 90 now,
751:41 - and we are no longer using the random rotation layer.
751:48 - So that's it.
751:49 - We get back and then run the cells and get to training.
751:55 - And as you can see, everything just works fine.
751:58 - And we'll see how to get the best of both worlds.
752:02 - As now we have the flexibility of tf.image
752:06 - and the speed and portability of Keras layers.
752:11 - Before we move ahead, there's this point
752:14 - which needs to be clarified.
752:16 - So we've already seen that when we have a single sample
752:20 - like this one, with data augmentation,
752:23 - we're able to produce different samples.
752:25 - In this case, we have the rotated version of this.
752:30 - In this case, we have another version
752:32 - in which we've modified the saturation.
752:35 - In this, we have this version
752:37 - where we've flipped this horizontally.
752:41 - That said, in practice, we are not gonna feed
752:44 - or we're not gonna create, let's say,
752:47 - this three other extra samples.
752:50 - What we do is, if we suppose we wanna implement
752:54 - three augmentation strategies,
752:56 - that is rotation, saturation, and flipping, for example,
752:59 - then for one epoch, let's have this year.
753:03 - For one epoch, we may decide
753:05 - that we do not wanna carry out rotation.
753:08 - And then from here, we may decide to adjust its saturation.
753:12 - So let's have from this, we have its saturation adjusted.
753:17 - And then in the last step,
753:19 - we'll decide that we do not wanna flip.
753:22 - So at the end, we have this input
753:25 - and then we have this as our output.
753:30 - Now in another epoch, you would have this instead.
753:34 - You may decide that you wanna rotate.
753:37 - So you may decide that you wanna have something like this.
753:40 - Let's rotate this by 90 degrees.
753:43 - Again, here, this is random.
753:45 - So we randomly rotate in this.
753:48 - We decide a random whether we wanna rotate this
753:50 - at 90 degrees or not.
753:52 - Then once we have this,
753:53 - we may decide that we do not want to adjust the saturation.
753:58 - And then finally year, we have flipping.
754:01 - So let's flip this.
754:04 - There we go.
754:05 - We flip this and we have this as output.
754:07 - So in this case, this is what we pass in the model.
754:11 - In other case, we may decide that we still wanna rotate.
754:16 - So we may decide I wanna have this rotation.
754:18 - Then still no saturation,
754:24 - but year we may want to maintain this.
754:28 - So we may still have this, you see.
754:31 - Now we do not flip this.
754:32 - And this is instead what this example or this model sees.
754:37 - Now in another case, we may have the input itself.
754:40 - That's this input, it gets past this.
754:44 - No rotation, gets past this.
754:47 - No saturation, gets past this, no flipping.
754:50 - And this is what the model sees.
754:52 - So we can see that the model has now seen
754:56 - four different examples of this same input
755:01 - without us necessarily having to create this
755:06 - four different examples separately before starting to train.
755:10 - And so this helps us carry out the data augmentation
755:13 - much more efficiently.
755:14 - That said, when we get back to the code,
755:16 - you see that that's what we actually did.
755:19 - Yeah, we decide to, or we may rotate or not.
755:22 - We may flip this or not.
755:24 - And if we had to flip, it's gonna be horizontally.
755:27 - And then year we may modify the contrast or not.
755:30 - So these are all augmentations
755:33 - which are randomly carried out during training.
755:35 - Now for the other alternative,
755:37 - which is the TensorFlow image,
755:39 - we need to make sure that this rotation year
755:43 - is carried out in a way that
755:44 - sometimes we have the image rotated
755:46 - and some other times the image remains intact.
755:49 - Now get into documentation,
755:51 - you'll see here we have this route 90,
755:55 - which takes in the image and this K right here.
755:58 - Now this K is simply a scalar integer tensor.
756:02 - And the number, and it's actually the number of times
756:04 - the images, the image of the images
756:06 - are rotated by 90 degrees.
756:08 - And so here we're gonna define this K,
756:11 - which is a random number,
756:12 - which could either be a zero or a one.
756:15 - And that's it for this rotation.
756:18 - Now getting back to documentation,
756:20 - you could check the same for the adjust saturation.
756:25 - Here is actually stateless random saturation.
756:29 - So you click on this.
756:30 - As usual, we pass in the image
756:32 - and then we have this low and upper bounds,
756:36 - which are those of the random saturation factor.
756:40 - So let's take this here,
756:43 - it's basically this.
756:46 - We copy this out here.
756:48 - So instead of this, we would have this.
756:50 - Well, let's just put this out here.
756:53 - We have image, which is equal this.
756:57 - Okay, so we taken the image and then we could have,
757:02 - let's say 0.3 and then 0.5.
757:06 - Okay, so that's it.
757:07 - We are gonna have this.
757:09 - And then now for the flipping left, right,
757:12 - we get back again here.
757:14 - Here you have random flip left, right.
757:17 - Then we copy this out.
757:19 - See, so we're gonna randomly decide
757:21 - whether we're gonna flip the image or not.
757:24 - Let's paste this out here.
757:26 - Instead of this, we have this now.
757:28 - There we go.
757:30 - Take this off.
757:31 - And that's it.
757:33 - So now we have this augment method,
757:37 - which randomly selects which augmentation strategies
757:43 - we are gonna use for a given instance.
757:46 - And this permits us carry out augmentation
757:49 - much more efficiently.
757:50 - Now let's do the same here.
757:52 - Let's copy this and paste out right here.
757:57 - So here we have this.
758:03 - Hello everyone and welcome to this new session
758:06 - in which we'll treat mixed sample data augmentation.
758:09 - Previously, we saw how to do data augmentation
758:13 - on a single image like this or this one.
758:18 - And now we'll learn how to create new samples
758:21 - based on a combination or mixture of different images
758:26 - or different samples from our dataset.
758:29 - And more specifically,
758:30 - we'll treat the mixed up data augmentation strategy
758:34 - where we'll pick two samples from the dataset.
758:38 - We're gonna mix up the samples
758:40 - and then we're gonna include the strategy
758:42 - in our TF data pipeline.
758:45 - Up to this point, we've implemented
758:47 - data augmentation strategies
758:49 - which involve modifying input samples like this one.
758:53 - So we could take this input sample,
758:55 - we could do a zoom, we could do a center crop,
758:58 - we could rotate, we could translate
759:00 - and do many other stuff with this
759:02 - in order to augment our already existing data.
759:07 - Now, in this section,
759:08 - we'll look at another data augmentation strategy
759:11 - which is known as mixed up.
759:13 - Now, mixed up doesn't only involve just one sample
759:19 - as we had seen previously.
759:21 - With mixed up, we are gonna make use of these two samples
759:26 - instead of just one, mix them up
759:29 - and then use this output sample.
759:33 - And if you look very carefully here,
759:35 - you'll notice that this image contains this one,
759:39 - you could see here, you could see it
759:40 - carved out this dog, carved out like this, you see here
759:45 - and then this other dog right here.
759:48 - So we also have this one
759:50 - which is also carved out like this.
759:52 - So we could see the mixture of this two
759:54 - to form one input.
759:57 - So we have this output image
760:00 - which we could define as X prime
760:02 - which is a mixture or a combination
760:05 - of this image X1 and X2.
760:08 - So yeah, let's have X1 and X2.
760:12 - But this actually happens to be a weighted addition.
760:16 - That is we have a certain factor lambda
760:19 - which is a value between zero and one
760:23 - and drawn from the beta distribution
760:26 - such that we have X prime equal lambda times X1
760:30 - plus one minus lambda times X2.
760:35 - This means if lambda equals 0.5,
760:37 - then we'll have 0.5 X1 plus 0.5 X2.
760:42 - Lambda equals 0.3 for example,
760:44 - we have 0.3 X1 plus 0.7 X2.
760:50 - So some sort of weighted addition.
760:52 - Now we've created this new input,
760:54 - it's logical that we need to modify the labels
760:57 - because this doesn't belong to either
761:01 - or you cannot really say that this belongs
761:03 - to this class or this class.
761:05 - It actually is a mixture of both classes.
761:08 - So all like previously,
761:09 - when we do data augmentation on this image,
761:13 - we are gonna maintain the label
761:15 - because it still remains this particular class.
761:19 - But when we mix classes up together
761:21 - or images from two different classes together like this,
761:24 - we need to modify the level.
761:26 - And so that said, we have a new level Y prime equal lambda
761:32 - and you guessed that right, Y1 plus one minus lambda Y2.
761:40 - And from here, we could now dive into the code
761:42 - and implement this mix up data augmentation strategy.
761:47 - Recall we said that lambda was to be drawn
761:50 - from a beta distribution.
761:53 - But if you come to the documentation
761:56 - we've been using so far,
761:57 - where we have this tensorflow.org API docs,
762:02 - you wouldn't really find this beta distribution.
762:06 - So you're advised to look out in this one year.
762:10 - So go to tensorflow.org slash probability instead.
762:14 - It's here you would find this distribution.
762:17 - So here we have this tensorflow probabilities
762:21 - and then you have tensorflow distributions.
762:27 - When you click here,
762:28 - you could find many probability distributions,
762:32 - including this beta distribution,
762:34 - which we want to work on now.
762:36 - Now, just right here, you could see the beta distribution,
762:38 - you have the definition.
762:39 - And then notice how we have this two parameters,
762:43 - which we must pass.
762:44 - So the beta distribution is defined over 01,
762:47 - this interval 01 is in parameters concentration one,
762:50 - aka alpha and concentration zero, aka beta.
762:53 - So we have the parameters alpha and beta to pass right here.
762:58 - Now, if we look up in this mix up paper,
763:01 - you'd see that the parameters alpha,
763:04 - they use like here from this experiments is 0.2.
763:08 - And then sometimes they use 0.42,
763:11 - but most times they tested on 0.2.
763:14 - So it's this parameter we'll be using.
763:16 - And then getting back here,
763:20 - all we need to do now is copy this out.
763:22 - So we copy this, get back to our code.
763:26 - And then we have the mix up down here.
763:30 - So here we have this mix up.
763:33 - Okay, mix out the documentation, let's reduce this.
763:37 - And then we reduce this part.
763:39 - So you wouldn't run the cells
763:42 - because we're not going to make use of this now.
763:45 - So here we have this mix up, let's test out this.
763:49 - We run it and we have this error,
763:52 - name concentration is not defined.
763:54 - Anyway, we could get back here
763:57 - and then import TensorFlow probability.
764:00 - So let's import TensorFlow probability
764:05 - and then put that as TFP, run that
764:09 - and then get back to our mix up.
764:11 - Okay, so here we have this mix up.
764:14 - Let's take this off.
764:16 - And let's take all this off actually, 0.2, 0.2.
764:20 - Okay, so we got that from the paper.
764:23 - That's understood.
764:24 - And then we have Lambda.
764:26 - So here we have Lambda.
764:28 - If we spell it this way, we'll have the keyword,
764:30 - Python keyword.
764:31 - So let's just keep that simple
764:32 - and just have this Lambda spelled,
764:34 - although spelled wrongly.
764:36 - And now we'll do Lambda, let's print out Lambda,
764:40 - Lambda.sample.
764:42 - So we take out one sample from that beta distribution
764:46 - and we run that and here's what we get.
764:49 - So you see, if you run this again,
764:51 - you would obviously get different outputs
764:53 - and all this drawn from the beta distribution
764:56 - and the range 01.
764:59 - So here I'm just going to do this,
765:01 - pick out this zero item and then we have this output.
765:05 - We could also put this NumPy to get it.
765:10 - See, you have this now.
765:11 - So you're not having the tensor.
765:14 - But anyway, we prefer to use it as a tensor
765:17 - and we'll explain why it's preferable to work with tensors
765:21 - in the function we're trying to build.
765:24 - From here, we just simply apply the formula.
765:27 - So we'll have the output image,
765:31 - which is equal Lambda times the image one
765:36 - plus one minus Lambda
765:42 - times the image two.
765:45 - So that's it for the image.
765:46 - We'll repeat the same process for the labeling.
765:50 - Then using OpenCV, we'll test this on this two images,
765:54 - which we've added here.
765:55 - We have this dog and this cat image right here.
765:59 - So we have this two, you're going to test this on it.
766:01 - Let's take this off right here.
766:04 - We're going to read the images.
766:06 - So we have image one.
766:09 - We're going to do the reading in read
766:13 - and then we'll do the same for in two.
766:15 - So right here, we could print out image that shape
766:19 - and level that shape.
766:23 - All right, let's print out the level.
766:25 - So we have that.
766:27 - We get in this error.
766:29 - Let's get back up and we correct this.
766:32 - So this actually Lambda, let's modify Lambda.
766:36 - Lambda equal this.
766:37 - So let's have that out.
766:38 - Okay, so we have that right.
766:40 - We run this again and require broadcastable shapes,
766:46 - which happens because we haven't yet resized this
766:49 - because here we have image.
766:51 - If we print out the shape of image one
766:54 - and image two before doing this operation,
766:57 - you'll see there are two different shapes.
767:00 - So we have to ensure that they are both the same.
767:02 - See, they're two different.
767:03 - Now let's resize this with CV2 resize,
767:07 - resize and that will specify the shape.
767:11 - So let's have your image size, image size, image size.
767:17 - Okay, we have that done.
767:18 - We just copy this out, paste out here.
767:22 - We have image size and then CV2 resize.
767:27 - Okay, so we've read, we've resized
767:30 - and that should be fine now.
767:32 - We run that again, image size is not defined.
767:35 - Let's have that to be defined here.
767:39 - We actually defined this previously,
767:40 - but we haven't run those previous cells
767:42 - since we restarted the notebook.
767:43 - So that's why it isn't recognizing the image size.
767:48 - Okay, now level one are defined.
767:50 - This looks great already.
767:52 - For this levels, let's say we have level one.
767:57 - So here we have level one equals zero
768:02 - and then level two equal one.
768:05 - So we have just these two levels.
768:08 - Okay, we run that again and this is what we get.
768:12 - You see, we have this output image
768:14 - and then we have this final level
768:18 - and which happens to be neither zero nor one.
768:21 - From here, we could plot this out, PLT in show
768:26 - and then we pass in the image and normalize this.
768:30 - We run that and this is what we get.
768:32 - So you see, we have a mix up of these two images.
768:37 - Now that we have succeeded to do this,
768:39 - let's make this part of our TensorFlow pipeline.
768:44 - So we could take all this out now
768:47 - and then define this mix up method.
768:51 - We have this level.
768:53 - Okay, we'll take this off.
768:54 - Okay, we're gonna define this method.
768:57 - Let's call it mix up and the way this method works is
769:01 - we're gonna take in our data.
769:02 - So train data set, here we have one
769:06 - and then train data set two.
769:11 - So we have this two data sets
769:14 - which contain the same elements
769:16 - but which have been shuffled
769:17 - so that we could have this kind of mix up.
769:21 - Then we can now make this data sets available
769:25 - which we'll do the mix up on.
769:28 - So let's add some code, take this up
769:31 - and then here we have our first train data set.
769:35 - We have train data set one
769:38 - which is actually our train data set we have built already.
769:43 - And so we're getting our train data set right from here.
769:45 - We've run this already.
769:47 - We get back.
769:49 - Be careful we are not running this
769:52 - but we may make use of one or two methods from this.
769:55 - So we have that and then we get back to this.
769:58 - So here we have train data set.
770:00 - We do some shuffling.
770:02 - We specify the buffer size
770:05 - and then we also specify that we're gonna reshuffle
770:09 - after each iteration.
770:11 - We just copy this and paste out here
770:13 - to have our train data set two.
770:15 - Now, once we have this train data set two
770:17 - we now have our train.
770:20 - Let's call this mix data set or mixed data set.
770:25 - Okay, we have our mixed data set
770:28 - that will make use of the zip method.
770:31 - And with the zip we're gonna pass in the train data set one
770:36 - and then the train data set two.
770:41 - Now, if you could remember we had an arrow
770:43 - when we passed into images we had two different shapes.
770:47 - So we have to ensure that we do some pre-processing
770:50 - before doing the mix up.
770:52 - So that said, after doing the shuffling
770:54 - we could do the pre-processing.
770:56 - So let's say pre-process.
770:59 - Let's get back up while we define this pre-processing.
771:02 - And we had here.
771:05 - Okay, so we had at this level of data augmentation
771:07 - we had pre-processing.
771:09 - Although we had inserted this in our augment
771:13 - but it's practically this resize, rescale method here.
771:16 - So we could run this, that'll be fine.
771:19 - And then we just do resize, rescale.
771:21 - So we wouldn't call that pre-processing again
771:23 - we just have resize, rescale as we've done already.
771:28 - We do the same mapping.
771:29 - Yeah, we have resize and rescale.
771:32 - Okay, so now we shuffle, we resize and rescale
771:36 - and then we have our data
771:38 - which is now a combination of data set one and data set two.
771:43 - This GIF was gotten from gifi.com.
771:47 - Now we have this mixed data set formed.
771:50 - We run the cell, that's fine.
771:52 - And then in here, let's take this off.
771:55 - We've run this already.
771:56 - So we have that.
771:58 - And then yeah, we're gonna take in image one.
772:01 - So image one and level one.
772:06 - Level one, we have that.
772:09 - Yeah, this suppose we have image two and then level two.
772:14 - This closes up.
772:16 - Okay, so we have that.
772:18 - And then we get this from the train data set one,
772:23 - data set one and the train data set two.
772:29 - So that's how we get this.
772:31 - We have image one, level one, image two, level two.
772:34 - The image one that we had here, we don't need this again.
772:37 - We have Lambda, we get Lambda, we get the image,
772:40 - we have the level and then we have our output.
772:43 - So we return image and level.
772:46 - So this is all about the mix up.
772:47 - Now we run this cell
772:49 - and then we create this other new cell.
772:52 - Yeah, we have this error, should have this.
772:57 - And then we create this new cell right here.
773:02 - Then we pass this out from what we had done already.
773:05 - And then here we have our mixed data set.
773:08 - So we now have this mixed data set.
773:10 - We shuffle again, we do the mapping
773:12 - with the augment layer, we do batching and prefetching.
773:17 - But since our augment is no more
773:19 - the previous augment layer we had,
773:20 - here we have now the mix up.
773:22 - So yeah, we replace that with mix up
773:24 - and that should be fine.
773:25 - We have the train, we could do the same for the validation.
773:29 - We get an error or spell an error, train data set.
773:34 - Let's get back to this, train data set.
773:38 - Okay, so here we have the train data set.
773:41 - We run that, run this again.
773:43 - We have your input Y of mall operation has type in 64
773:48 - that doesn't match the type float 32 of argument X.
773:52 - So this is where we multiply in the Lambda by the levels.
773:56 - So clearly we have the Lambda
773:59 - which is a float and levels which are ints.
774:04 - So yeah, we're just going to cast this.
774:06 - So we have this casting, we specify the D type, float 32.
774:13 - That's fine.
774:14 - And then right here, we do a same casting,
774:18 - specify the D type again.
774:23 - And that's it.
774:23 - Let's run this again and see what we get.
774:27 - See, it works fine.
774:28 - We have this warnings.
774:30 - Also note that here we were trying to experiment
774:33 - and change this.
774:34 - So let's get back to point two, run that again.
774:37 - Okay, so now we have our training data as you can see here,
774:41 - we have the batch and then the image
774:43 - and then the batch and then the level.
774:45 - So that's it.
774:46 - We've now created this data set
774:50 - which happens to be a mixed data set.
774:52 - Then from here, we are going to prepare our validation data
774:55 - set is actually the same as what we had already.
774:57 - So you could just simply run this previous cell right here.
775:01 - This cell is data loading.
775:04 - Just simply run this, run this and you should be fine.
775:08 - Instead of doing this augment layer,
775:09 - we meant to just do resize with scale.
775:11 - We just have to resize and rescale our validation data.
775:15 - We don't really need to shuffle.
775:16 - We could take that off.
775:18 - We could take the profession off and that's fine.
775:20 - So we run our validation and then check it out here.
775:23 - See, you have a validation data.
775:25 - Now the reason why we have this is because
775:27 - we run this twice.
775:28 - So let's re-initialize this right here.
775:33 - Let's get back to the splits.
775:34 - We created train data and validation data.
775:38 - Okay, let's run this again here.
775:41 - Now everything should be fine.
775:42 - Okay, so you have the batch dimension and that's good.
775:47 - So we now get back to training
775:49 - and make sure everything is okay.
775:52 - We rerun this and everything is now fine.
775:55 - Okay, so we're now set to train our model.
775:58 - So we run our sequential API right here.
776:02 - We run this.
776:03 - We then compile our model
776:05 - and then we could get ready to train the model.
776:08 - Have this poor results.
776:10 - Reason being that the mix-up data augmentation strategy
776:14 - isn't adapted to the data set we're working with.
776:18 - Even if the mix-up data augmentation strategy
776:20 - we've just applied wasn't very helpful
776:23 - for this particular problem,
776:25 - it's important to note that this mix-up strategy
776:28 - could be used in many other problems.
776:30 - And with that, we've come to the end of the section.
776:32 - Thank you for getting up to this point
776:34 - and see you next time.
776:39 - Hello everyone and welcome to this new session
776:41 - in which we're going to implement
776:43 - the cut-mix data augmentation strategy
776:45 - with TensorFlow 2.
776:47 - The cut-mix data augmentation strategy,
776:49 - though based on the fact that
776:51 - we are combining two different samples,
776:54 - is different from that of the mix-up.
776:56 - Instead, with a cut-mix data augmentation,
776:58 - we are going to take a random patch
777:01 - from one of the samples and attach to the other sample
777:05 - while modifying the levels accordingly.
777:08 - We've looked at how to implement data augmentation
777:10 - with TensorFlow and also how to implement
777:14 - more advanced data augmentation strategies
777:16 - like the mix-up.
777:18 - In this section, we'll look at this
777:20 - cut-mix data augmentation strategy.
777:24 - Here, if we suppose that we have these two images,
777:27 - image one and image two,
777:30 - we are going to randomly crop a part of this image.
777:34 - So just like you can see in this output here,
777:37 - they randomly crop this section from this image
777:40 - and then attach that to this other image
777:44 - such that what you have in the output
777:47 - is this one with this patch.
777:50 - So this is how this cut-mix data augmentation is implemented.
777:56 - If we take this example where we have this cut
777:59 - and this dog right here,
778:02 - we'll try to randomly crop a part from this dog
778:06 - and then attach that part at the same position
778:11 - on this cat image here.
778:14 - To do this cropping operation,
778:15 - we get to TensorFlow image.
778:17 - Let's have this.
778:19 - We have this TensorFlow image
778:21 - and then we have the crop to bound in box.
778:23 - So we'll click on this
778:25 - and then we have this definition.
778:30 - We see the arguments, we're passing the image
778:32 - and then we are going to see,
778:34 - or we're going to specify the offset height,
778:36 - offset width, target height and target width.
778:39 - Now let's explain what all this means.
778:43 - If you have an image like this one,
778:45 - just as it's giving you the offset height
778:47 - is a vertical coordinate of the top left corner
778:50 - of the bounding box in the image.
778:53 - And so this means that if we randomly select,
778:57 - for example, this box right here,
779:00 - let's suppose we've randomly selected this box
779:03 - and our offset height will be this distance
779:09 - that is, our reference is this top left corner
779:12 - and our offset height will be this distance here.
779:16 - This distance and our offset width
779:19 - will be this other distance.
779:23 - So that's how we have this offset height
779:26 - and offset width.
779:30 - And then the target height is a height of the bounding box.
779:33 - So here we have this bounding box's height
779:36 - and target width, the width of the bounding box.
779:41 - So once you provide this,
779:43 - it will be able to automatically crop out
779:47 - this zone from the image.
779:50 - So coming back to the code,
779:51 - we are gonna add this other extra subplot.
779:55 - Let's have this other subplot.
779:59 - We have, let's paste this code first.
780:02 - Could just copy this out.
780:03 - We have this.
780:05 - And then we have the subplot, third position.
780:08 - What we're doing here is we're not gonna be having this.
780:11 - We're gonna have image,
780:12 - let's call that image three.
780:14 - And then what we'll be doing here
780:15 - is we'll be making use of this method.
780:17 - So we have this method
780:19 - and then we'll specify this offset height,
780:21 - offset width, target height, target width.
780:24 - Let's suppose that our offset height is,
780:28 - let's say 20.
780:29 - So here we have 20,
780:32 - 20, say 15.
780:34 - So we have 20, 15.
780:37 - And then let's suppose this target height is 100.
780:40 - And here, say 98.
780:46 - Okay, so we have this specified
780:49 - and then we now pass in the image.
780:51 - The image we'll be using here is image two,
780:53 - this image here.
780:55 - And that will be it.
780:56 - So here we take this off
780:58 - and then we could simply show this passes here
781:02 - and that's fine.
781:03 - Now we could take this off.
781:05 - Okay, so we have this.
781:06 - We can now plot this out and see what we get.
781:09 - Okay, as you could see,
781:11 - what we obtain here is the correct,
781:16 - it's a cropping of this zone,
781:18 - as you could see.
781:19 - You see that we have something around this,
781:22 - we crop out this zone.
781:25 - So actually it takes more of this.
781:26 - So it's something like this.
781:28 - Now we could modify this,
781:29 - like we could shift the height and the width.
781:31 - So we could actually maintain the height
781:33 - but shift the width so that we could get more of the dog.
781:36 - So let's do just that.
781:37 - We shift the width.
781:39 - Let's say we take 100 too.
781:41 - We run that and here is what we get.
781:44 - You see that we get the dog's face this time around.
781:48 - And basically this is how we crop out a region
781:51 - from this image.
781:52 - Now, once we've done this cropping,
781:54 - we want to create another image
781:58 - which is made of only this crop
782:00 - while the remaining zones are actually left out.
782:05 - Now to do that, we'll make use of this other method
782:09 - which is the path to bound in box.
782:12 - So here we have this image path to bound in box.
782:15 - We'll copy it out and you see how it works.
782:17 - So here we have this image path to bound in box.
782:20 - We're going to create another plot.
782:21 - So let's increase this number of plots.
782:24 - We have four and year four and year four
782:28 - and let's place this out first.
782:30 - So we have that, copy this,
782:33 - paste this out and then create this foot plot.
782:37 - So we have this foot plot.
782:38 - And now what we'll do is just copy this here
782:44 - and paste it out.
782:46 - So here we paste this out and then let's get back.
782:51 - And then what we pass in as image now is this cropped image.
782:54 - So let's actually copy this.
782:56 - Let's say we have this crop.
782:59 - Let's call it a crop.
783:00 - And then we have, let's take this off.
783:04 - We have the crop, paste it out.
783:07 - Okay, so here we have the crop
783:09 - and now we're going to take in the crop now.
783:12 - So after, let's look at this.
783:14 - So after we've, oh, we've had an error before.
783:18 - So let's do this so you could see it better.
783:25 - Okay, so actually what we're seeing here
783:27 - is we have this padding to be done
783:30 - and then we're taking this image
783:32 - and then we want to pad it or like stamp it on another image
783:38 - which contains only zero pixels.
783:41 - So let's look at that.
783:44 - We have that.
783:45 - We have this pad.
783:47 - We pass in the crop.
783:48 - We have the offset height and width
783:51 - and then the target height has to be given.
783:55 - So yeah, we're going to put in the image size
783:58 - because we want this to be padded on an image
784:01 - with this dimensions.
784:03 - So we have the image size there.
784:05 - Now we could run this and see what we get.
784:09 - We have this error.
784:10 - Let's modify that quickly.
784:12 - Okay, so yeah, we have this taken off
784:16 - and then we have 20, 100, 100.
784:21 - Okay, so let's run that
784:23 - and we should have something unreasonable.
784:25 - Offset width again.
784:26 - Oh, okay, we should take this off.
784:29 - Okay, so let's look at this and there we go.
784:34 - As you could see, we have exactly what we expect.
784:37 - You see that we have this somehow the same image
784:40 - but we've taken out only this crop.
784:42 - So that's what we actually want.
784:44 - We want to be able to take only this crop
784:46 - and then take this like this crop
784:49 - and then add it with this image here.
784:53 - So what we want to do is we want to take this image now
784:56 - and then add it with this image
784:59 - so that we could be able to create
785:02 - our data augmentation pipeline.
785:06 - Now we could call this image four.
785:08 - So let's have this as image four equal this here.
785:15 - Stick this off.
785:17 - Image four, paste it out.
785:20 - And then once we paste it out,
785:21 - we could now let's also do another subplot.
785:25 - So we keep on doing the subplots.
785:27 - Let's copy this and paste it here.
785:30 - That's fine.
785:31 - We take the crop as we have the,
785:36 - or rather we have this image four
785:38 - plus our initial image, image one.
785:41 - So let's have that and then we put it out.
785:44 - Okay, we get in this plot but it isn't very clear.
785:47 - So let's increase that figure size.
785:49 - Let's add the figure size here and that's it.
785:53 - So we run that again and then let's look at this now, Clara.
785:58 - Okay, here's what we get.
786:00 - So as you could see here, we have this patch,
786:03 - like it looks like someone is working
786:05 - but there is a problem as we have some sort of mixture
786:09 - of this and the initial image.
786:12 - Now this is logical since when you do this addition
786:16 - for this black region, you just have only the cut
786:20 - but for this region, you still have the path
786:24 - of this cat image.
786:26 - So what we need to do is we need to remove this path
786:29 - such that when you take this and add to this,
786:32 - it just fits in like a puzzle.
786:35 - Then to crop out just this part from this image,
786:39 - we are going to use the same process we've used
786:42 - for this dark image right here.
786:44 - We'll simply copy out what we had already here.
786:47 - So we had this cropping, could copy this out
786:50 - and paste it out here.
786:53 - We run that and here's what we get.
786:55 - So you see, we have this crop again.
786:58 - Now we will take this crop and then add to bound in box
787:02 - as we had done here previously.
787:05 - So let's copy this and then what do we have?
787:08 - We have the seventh and then we're going to pass
787:12 - in this crop cat.
787:13 - So pass in the crop cat and that's it.
787:17 - But we'll print out this image.
787:19 - So let's call this image five
787:21 - and here we have image five, we run that
787:25 - and this is exactly what we get.
787:27 - Now the aim is for us to be able to take this
787:31 - and subtract like take this and subtract this image
787:38 - from it such that we'll be left with the full cat.
787:43 - We doubt this portion, without this portion,
787:46 - this portion right here.
787:50 - So that said, if just here you do image one
787:54 - minus this image five, you would get this answer.
787:59 - So as you could see, you have this whole cat
788:02 - without this portion and this is exactly what we want.
788:06 - Now that we have this part, we could add it up
788:10 - with what we wanted to add initially here.
788:12 - So we added up with this image four.
788:17 - So here we have plus image four
788:22 - and we get the response and there we go.
788:25 - So we've completed this process of cutting out this portion
788:30 - from here and then fitting on this image one.
788:36 - We now take off this other part, take this off
788:40 - and there we go, we now have the crop,
788:45 - the image four, crop cat, image five
788:49 - and then our final image.
788:51 - We copy out this mix up code we had done previously.
788:56 - So we get back to this.
788:59 - You see that now we could easily integrate this
789:01 - since we have image one and image two,
789:04 - we just left with the levels.
789:05 - Let's just copy this out, cut that
789:09 - and then here we take this off, paste it out.
789:14 - There we go and we now have the output image.
789:19 - We now modify this variable name such that crop is crop one.
789:23 - So here we have crop one instead of crop
789:27 - and then here we have crop two.
789:30 - So everywhere we meet crop cat, we have crop two,
789:34 - here we have crop one and then this image two is maintained
789:38 - from here, we have image two.
789:40 - Then the image four, we could tend this to pad one.
789:45 - So let's call this pad one.
789:48 - Here we have pad one, but this is actually pad two
789:52 - since we're working with the image two.
789:54 - So let's call this pad two and crop two.
789:56 - We have the crop two, then from this we have crop one
790:00 - and here image one, here we have crop one
790:03 - and here we have this image five, we'll call it pad one.
790:09 - Then coming right here, image five, image five is pad one
790:13 - and then image four, pad two.
790:17 - Okay, so we have that done.
790:19 - Now we could focus on how to get this bound in boxes.
790:23 - So yeah, we just pick this bound in box,
790:26 - but how do we get this bound in box?
790:29 - To get an answer to this question,
790:31 - we are gonna make reference to this formulas
790:33 - which were given in the paper.
790:35 - Right here, we are told that rx is drawn
790:39 - from the uniform distribution,
790:42 - which takes parameters zero and the width.
790:44 - And then ry is drawn from the uniform distribution,
790:47 - which takes parameters zero and the height.
790:50 - Now if we consider this image
790:52 - and say this box randomly picked,
790:56 - then our rx is a center that is this distance
791:01 - from this as based on this origin actually.
791:06 - So we have this vertical distance to this center,
791:09 - which is our ry and then this horizontal distance,
791:13 - which is our rx.
791:15 - So that's how we obtain ry and rx,
791:18 - which we draw from the uniform distribution.
791:21 - Now to obtain rw and rh, we have this formula right here.
791:29 - rw equal w, w is the width of the image.
791:32 - So we have w times the square root of one minus lambda.
791:36 - Recall how we obtained lambda
791:38 - with a mix up data augmentation strategy.
791:42 - So this is exactly the same way we get lambda.
791:45 - So here we have w times square root of one minus lambda,
791:49 - rh equal the height of the image
791:53 - times square root of one minus lambda two.
791:56 - Now note that when you multiply this two,
791:59 - that is if you take rw times rh,
792:03 - you would have the numerator right here.
792:06 - Let's have this.
792:08 - rw times rh gives you this numerator.
792:14 - And then if you multiply this two,
792:17 - you would have wh times the square root of one minus lambda.
792:29 - So if you multiply this times this,
792:30 - it gives you wh, this times this,
792:32 - square root of one minus lambda
792:34 - times the square root of one minus lambda.
792:37 - And you have the square root of one minus lambda.
792:41 - Okay, so this is equal rw rh.
792:48 - Now we'll see how they obtain this formula.
792:51 - You see here that since you have rw rh,
792:54 - you could divide here by wh
792:59 - and divide here by wh.
793:03 - This goes away and we left with this.
793:07 - Obviously, if I have the square root of x
793:09 - times the square root of x,
793:13 - then it is equal to x.
793:16 - Since I'm having the square root of x squared,
793:19 - now the square root gives you x.
793:22 - So in this case, our x is one minus lambda.
793:24 - So this is equal one minus lambda.
793:28 - And that's how they obtain this relationship right here.
793:31 - Now that said, we have rh.
793:34 - Oh, we've taken this off.
793:36 - Let's go one step back.
793:39 - We have rh.
793:40 - Let's take this.
793:41 - We have, sorry, rw and then we have rh.
793:46 - So this is rw, the width,
793:48 - and then here's rh, the height, r height.
793:55 - But recall that what we have to pass into the method,
793:58 - which permits us crop out the zone, for example,
794:03 - is actually this point right here,
794:09 - the top left corner of this bounding box.
794:14 - So we are not going to use the center,
794:17 - but instead it's top left corner.
794:18 - Now, how do we get a top left corner?
794:20 - To leave from the center to the top left corner,
794:24 - what we need to take into consideration is the fact that
794:27 - we notice this width, for example.
794:30 - Now, if we know this distance,
794:33 - and we have this distance from this to this point here,
794:38 - we're supposed to use the center,
794:40 - then we could obviously get this distance here.
794:44 - We could get this distance.
794:46 - To obtain this distance,
794:47 - we need to take all this minus just this portion
794:55 - to obtain this portion right here.
794:58 - Now, to obtain this portion, it's easy
795:01 - because we already know the width, r, w.
795:05 - So all we need to do is divide this by two.
795:07 - If we divide rw by two, then we have this distance.
795:10 - And since we already have this distance, rx,
795:13 - then we can obtain the x coordinate
795:19 - for this point right here.
795:21 - Now, the next thing to do is to find rh.
795:26 - And we're going to use, sorry, we're going to find ry.
795:30 - And we're going to use the same method we did to find rx
795:34 - from base in the top left corner.
795:36 - For ry, we know this distance.
795:40 - Now, do we know this distance?
795:42 - Yes, we know this distance
795:43 - because this is half of the total height
795:46 - since we are found at the center.
795:49 - Now, if we have this distance,
795:50 - then we could find this distance
795:52 - because this distance plus this distance
795:56 - gives us this distance.
795:58 - And so to get this distance,
796:00 - we need to take this distance minus this distance
796:04 - to obtain this distance.
796:06 - And if we have this distance,
796:07 - then we have the y coordinate of this point right here.
796:12 - Recall that the reason why we're going through all this
796:14 - is simply because the methods offered by TensorFlow
796:18 - consider that the bound and box coordinates are given
796:21 - based on this top left corner right here.
796:25 - So now we define this function box,
796:28 - which takes in the lambda.
796:31 - It takes in lambda.
796:33 - And then what this does is,
796:36 - it makes use of this uniform distribution to obtain rx, ry,
796:40 - and then makes use of lambda to obtain rwrh.
796:45 - Now, getting back to our uniform distribution,
796:47 - we could copy this right here.
796:50 - And then put in our code.
796:53 - For now, let's keep the function aside.
796:55 - Let's just have this.
796:56 - So here we have our uniform distribution.
796:59 - Recall that our low is zero.
797:02 - Let's take this back.
797:03 - Our low is zero and our high is the width.
797:08 - So we have your aim size and we have that.
797:12 - Okay, so we've defined this
797:15 - and then we have rw,
797:19 - or rather rx.
797:21 - That's rx.
797:22 - Now we want to have ry.
797:25 - Let's have it rx, ry.
797:29 - And then we could simply copy this out.
797:32 - There we go.
797:33 - We have this.
797:34 - Let's print this out so we see what we get.
797:38 - Print out rx.
797:40 - For example, there we go.
797:41 - We have rx.
797:44 - Aim size is not defined.
797:45 - We restarted the notebook.
797:46 - So let's get back to redefining this aim size.
797:51 - We run it again.
797:52 - And this time around, everything looks fine.
797:54 - Now we could draw a sample from this distribution.
797:57 - So let's have sample.
797:59 - We draw a single sample and see what we get.
798:02 - You see, we have that.
798:03 - We could take this zero element and there we go.
798:08 - So now we're able to draw a sample from our distribution.
798:11 - We could do the same for ry.
798:13 - So yeah, basically we have this.
798:15 - And then what we want to do is to ensure
798:17 - that all this coordinates are integers.
798:21 - So we could cast this.
798:24 - We cast that.
798:25 - We have the dtype equal int 32.
798:30 - That's fine.
798:32 - Copy this out and paste it here.
798:34 - Okay, so we have that and then we cast this too.
798:37 - Now we have rx, ry.
798:40 - Let's just do a sampling directly.
798:42 - So we've taken this and then we do the sampling right here.
798:50 - Sample, take a single sample, take zero element.
798:55 - And that's fine.
798:57 - We do the same here, paste it out and everything looks okay.
799:01 - So yeah, we could now print out,
799:03 - we could now print out rx and ry.
799:08 - Okay, it looks great.
799:09 - We have now our random rx and ry.
799:13 - Run again so you could see the response.
799:16 - Now we have to obtain m size
799:18 - times the square root of one minus lambda.
799:21 - So we suppose that we have lambda.
799:24 - We actually going to use the same method
799:25 - we had used previously.
799:26 - So suppose here we have our lambda
799:29 - and then right here to obtain rw,
799:33 - we have the m size
799:36 - times tf mat square root of one minus lambda.
799:44 - Okay, one minus lambda.
799:46 - Okay, that's fine.
799:48 - Now we have rh, r height, rh same in size,
799:54 - one minus lambda, that's okay.
799:57 - So now we have rh, rw and rx and ry.
800:01 - What we'll have to do is to modify this rx.
800:06 - So we will have rx equal rx minus the width divided by two.
800:16 - Now we want to have a whole number.
800:18 - So we have that minus the width divided by two,
800:22 - but the width is the m size.
800:24 - So we want to have this m size, there we go.
800:27 - Now, if you don't get why we're using this,
800:32 - just get back to this image to obtain this distance,
800:36 - that's to obtain this coordinate, this top left corner,
800:39 - we simply take this distance to the center
800:42 - minus the width divided by two
800:45 - and we do the same for the height.
800:47 - So that's it.
800:48 - Now we're doing that.
800:50 - We have rx equal rx minus m size divided by two.
800:53 - We'll repeat the same and then we have it for y.
800:59 - Now this is actually this width divided by
801:05 - to the width of the box, not the width of the whole image.
801:08 - So we're making an error here.
801:10 - Let's get back to this.
801:11 - We're making use of this width actually,
801:14 - not the width of the whole image.
801:16 - So let's get back and we'll put this code after this.
801:19 - So because we're going to be making use of this rh
801:22 - and rw.
801:25 - Now we have here, this is rw, rw, that's fine.
801:32 - Okay, now yeah, we're going to make use of rh.
801:35 - So we have the rh, ry, ry.
801:41 - Okay, so now we have rx and ry.
801:45 - We could now print this again
801:47 - and then also print out rw and rh.
801:53 - We get in this error.
801:55 - We told that in this competition right here,
802:00 - the rw is meant to be an int that was passed as a float.
802:05 - Since after this competition here,
802:06 - we would have that as a float.
802:08 - Now we could always print out rwd type.
802:14 - Let's run that and see what we get.
802:16 - You see, we have a float.
802:17 - So we'll modify that and cast it
802:21 - to ensure that we have an int.
802:24 - There we go.
802:25 - We have dtype equal, let's just copy this,
802:30 - tf ints 32.
802:32 - So we paste this out here and paste it out this way.
802:36 - tf.cast and that's fine.
802:39 - So now we expect to have the expected response
802:44 - and there we go.
802:44 - You see, we have rx, we have ry,
802:48 - we have rw and we have rh.
802:54 - When we run the cell several times,
802:57 - you'll notice that you have some negative values
803:01 - popping up from time to time.
803:03 - And this actually happens when, for example,
803:06 - we could have our center at this level,
803:09 - but our weight is so large that it doesn't fit in the image.
803:14 - So it goes in a negative direction.
803:16 - And this kind of situations are,
803:18 - say in this kind of situation,
803:20 - you could have our box going out of the image.
803:24 - And so we have to ensure that
803:26 - each time we create in this box,
803:29 - we limit it to the image.
803:30 - So this box now, let's take this off.
803:34 - This box now will be this one.
803:38 - And this other box right here will be this box.
803:44 - So we redraw this box, but it wouldn't go out of the image.
803:48 - Our aim is to ensure that we take this part off.
803:51 - Okay, so that's it.
803:53 - We're now going to clip this values programmatically
803:56 - by using the clip by value method.
803:58 - So here we have tf.clipByValue.
804:03 - There we go, we have that.
804:05 - And then once we pass this in, we're going to specify the range.
804:09 - So we're making sure that the values
804:11 - always fall in this range zero to the image size.
804:15 - So that's fine.
804:17 - Place it out and we have this clip by value right here.
804:22 - So that's it for our X, our Y.
804:24 - And now after running,
804:25 - you should have only values fall in that range.
804:29 - After clipping, what we have is,
804:32 - if we had say initially this box right here,
804:37 - we now have this box.
804:40 - So this is what we get after clipping.
804:43 - Of course, we get the coordinates of the top left corner,
804:47 - but the width in this particular case has changed.
804:51 - In another situation, we could have this, for example,
804:55 - if we have this, in this case, the height changes
804:59 - because we no longer have this height,
805:03 - but now this new height.
805:06 - In a situation where we had a box, which was like this,
805:10 - for example, here, but the height and the width
805:13 - will have to change.
805:15 - And so based on these new modifications,
805:18 - we have to ensure that we actually pass the right width
805:22 - and the right height.
805:25 - And so what we could do is make sure that
805:27 - once we have the center and we subtract the width,
805:32 - or we subtract half of the width
805:34 - and half of the height to obtain this,
805:36 - because we had the center, we had X minus the width
805:40 - divided by two to obtain the X coordinate of this point.
805:46 - And then Y minus the height divided by two
805:52 - to obtain the Y coordinate of this point.
805:55 - Now, what we'll do is we'll obtain this coordinate right here,
806:01 - but while implementing the clip function.
806:04 - And so this means that if we had a box,
806:06 - like normally centered in the image like this,
806:10 - and then we have this, we'll obtain this point,
806:14 - top left corner, and then we'll obtain this point,
806:17 - bottom right corner, just by using X plus W
806:22 - plus W divided by two,
806:26 - and Y plus W divided by two.
806:31 - So this point is of has coordinate X plus W divided by two
806:35 - and Y plus W divided by two.
806:38 - And that said, after clipping, we'll have just this now.
806:43 - But what's interesting about the method of clipping
806:47 - after getting this is that now we know exactly
806:51 - where this bottom left is found
806:53 - or rather this bottom right is found.
806:55 - And if we know we have this coordinate
806:58 - and we have this coordinate,
806:59 - then we could recalculate the width and the height.
807:02 - To recalculate the width and the height,
807:04 - it suffices to take, for example, for the width,
807:06 - we take this point here, the X axis,
807:11 - minus this X axis right here,
807:14 - or minus the X axis at this point,
807:16 - because this point and this for the X remain constant.
807:21 - Whereas for the Y axis, this point
807:26 - and this also remain constant.
807:28 - So that said, all we need to do now is to take this Y
807:33 - minus this Y to obtain the height,
807:37 - and then this X minus this X to obtain the width.
807:41 - So based on that, we'll re-copy this out and paste.
807:47 - So here we have X bottom right,
807:52 - X bottom right, and Y bottom right.
807:57 - All we need to do is to change this and add a plus,
808:00 - and here we add a plus.
808:01 - We keep it by value,
808:02 - so always ensure that it's found in the image.
808:06 - Then also now to obtain this final R, W.
808:11 - So our R, W now is equal,
808:14 - our Y bottom right, minus our R, Y,
808:20 - and then the R, H, we equal our Y instead.
808:28 - This is actually X, and here is X,
808:30 - because that's H, that's W, sorry.
808:33 - So you have a Y, B, R, minus R, Y.
808:38 - Okay, so now we've modified the way we calculate this
808:43 - based on the fact that the box may be out of the image,
808:48 - and that now it has some modifications to be done
808:51 - on the width and the height.
808:54 - Now the next step we have to take is,
808:58 - if this R, W is equal to zero,
809:03 - then we have to make sure that R, W becomes one.
809:08 - And then here we repeat the same process.
809:12 - If R, H equals zero,
809:17 - then R, H becomes one.
809:20 - So that's it.
809:21 - Now we have our R, X, R, Y, R, W, and R, H.
809:26 - That said, let's now create our box method.
809:29 - We have our box method right here.
809:32 - Let's have that.
809:33 - And this is what we return.
809:36 - So we return in this.
809:38 - But note that here we're taking the height,
809:42 - the offset height, offset width,
809:44 - target height, target width.
809:46 - And so this means that what we have to output
809:49 - in this method right here has to be R, Y, R, X,
809:55 - and then R, H, R, W.
809:58 - So there we go.
809:59 - We return that and normally everything should be fine.
810:04 - So we have this box method defined.
810:06 - We run that.
810:08 - We get in this arrow.
810:09 - We should pass in our Lambda.
810:13 - Let's go down.
810:14 - We have here Lambda.
810:17 - Okay.
810:18 - So we've passed in Lambda and that's fine.
810:21 - Now next thing to do is get back to our mix up.
810:25 - And then we like we did before with this Lambda,
810:31 - we actually could get this.
810:32 - We pass in the Lambda so we could take this off from here.
810:36 - So we run this again.
810:39 - And then we'll define this Lambda creation just right here.
810:43 - We have Lambda.
810:45 - Let's go back.
810:46 - We have Lambda.
810:47 - And then now we have the box.
810:49 - So this box is going to produce this output we need here.
810:53 - Let's call it R, Y and that other in the documentation,
810:59 - R, Y, R, H, R, W equal box of Lambda.
811:08 - So now we have box of Lambda
811:10 - and then we have this outputs here.
811:12 - Now instead of passing this, we have R, Y, R, X, R, H,
811:19 - and R, W.
811:21 - Let's take this off.
811:23 - Those boxes we fixed initially.
811:26 - We have this.
811:29 - Yeah, we have R, Y and R, X.
811:35 - Scroll down, that's R, X.
811:36 - Okay, now we repeat the same process here.
811:40 - We just simply copy this out and paste here, paste it out.
811:46 - Then also we have R, Y, R, X again.
811:49 - So here R, Y, R, X, let's take that off.
811:53 - Okay, so that's what we have now.
811:55 - And the mix up seems to be fine.
811:58 - So we could get back, run this method
812:02 - and then run our mix up method again.
812:04 - Now as usual, we are going to mix up these two data sets.
812:10 - So it suffices to run this.
812:13 - And then for this one, we are going to,
812:15 - instead of using the mix up,
812:17 - so yeah, we command this part
812:19 - and then we have the map and cut mix.
812:25 - So yeah, we have the cut mix method.
812:27 - We run this and everything should be fine.
812:30 - We get in this arrow.
812:31 - Let's check on how we call this.
812:33 - Oh, we call this mix up still.
812:35 - This should be cut mix, cut mix.
812:38 - We run that, fine.
812:41 - Let's get back to this year and run it again.
812:46 - Okay, that looks fine.
812:49 - Train data set, everything looks fine.
812:51 - Now we'll try to plot out some values
812:54 - so you see clearly what this looks like.
812:57 - Let's create this new code cell down here.
813:01 - This is out and we are going to show this image
813:06 - from our training data.
813:08 - And there we go.
813:08 - We could notice this patch, we could run this again.
813:12 - And this time around, we even have a bigger patch.
813:15 - So that's it, we've seen how to come up
813:18 - with this data augmentation strategy.
813:21 - But yeah, we get to do with the level.
813:25 - So I think if we print out this level,
813:28 - let's print out the level.
813:30 - Yeah, see, we print out the level.
813:33 - We just get in all zeros.
813:35 - Anyways, let's go ahead and implement
813:39 - this section for the level.
813:42 - And to do that, if you recall this formula,
813:46 - you have one minus lambda equal this.
813:50 - So this means that lambda is equal
813:53 - one minus RWRH divided by WH.
814:02 - And the reason why we need to do this again
814:04 - is simply because the RH, like at this level
814:09 - where we had this RH and RW modifications,
814:14 - all those clippings, we have to ensure that
814:16 - when creating the level, that condition is verified.
814:20 - So let's get back.
814:21 - Let's check on this formula.
814:22 - Lambda equal one minus RH, RW.
814:26 - So here again, we have lambda equal RH
814:31 - times RWRH, anyway, we have this RWRH, then divided by WH.
814:44 - So here we have in size, in size times in size.
814:50 - Okay, now this is one minus all this.
814:54 - Then we apply the same formula for the mix up.
814:57 - We have this lambda.
814:59 - Okay, we've got no lambda.
815:01 - We now pass this.
815:03 - We have level one and level two.
815:06 - Okay, so there we go.
815:08 - We have our cut mix.
815:09 - And then let's get back.
815:12 - Actually, we will have to rerun this, run our cut mix.
815:18 - And then for our training data, we would have to rerun again.
815:22 - So we have to run this cells again, run this,
815:27 - and then recreate our training data set.
815:32 - Again, this error, meaning that lambda is of type float 64.
815:37 - We get back to this and then we do some casting
815:42 - to send this, make this float 32.
815:44 - Okay, that said, we run our cut mix, cut mix run.
815:50 - We run this, there we go.
815:54 - Your cut mix, we run this again and this is fine now.
815:59 - So we have our training data set
816:02 - and then we'll go ahead and visualize it.
816:05 - We have this double knowns right here.
816:07 - So it's preferable for us to actually rerun this
816:12 - from this point.
816:15 - Okay, so we run that from that point
816:17 - and then we get back to this, we run this.
816:22 - Well, that's fine.
816:25 - We can run this and this should be okay now.
816:29 - Okay, that's fine now.
816:30 - We have everything intact.
816:32 - So that's fine.
816:33 - We can now visualize our data.
816:37 - So you can see we have our patch
816:39 - and then unlike before where we had add a zero or one,
816:42 - now we have values between zero and one as our levels.
816:47 - So let's now run this model, compiling and training.
816:56 - After the training is completed, we obtain those results.
816:59 - We see here how this accuracy, let's scroll down here.
817:04 - We have this accuracy, which doesn't really change much.
817:09 - So we just around this 46, 48 and high is this 50%.
817:17 - So it's around this 45 to 50% range
817:21 - and the last two doesn't really change much.
817:24 - Now, this is due to the fact
817:26 - that the model is getting confused.
817:29 - And the reason why this model gets confused
817:31 - is because if you have say this uninfected cell right here
817:35 - and this parasitized cell, it's actually this portion
817:40 - which permits the model know that the cell is parasitized.
817:44 - And so when you come and crop this part right here
817:50 - and then attach it to this cell here at this position,
817:56 - the actual part of this parasitized cell image
818:01 - which makes this parasitized isn't taken into consideration.
818:06 - And so the model gets confused
818:09 - as now it doesn't really know how to differentiate
818:12 - between an uninfected and a parasitized cell.
818:15 - Again, this cut mix data augmentation strategy
818:19 - isn't adapted for our model
818:21 - though it could be applied in many other problems.
818:24 - We thank you for getting around to this point
818:26 - and see you next time. Hello everyone.
818:32 - And welcome to this new section
818:34 - in which we'll see how to implement data augmentation
818:37 - using a specialized data augmentation library
818:39 - called albumentations.
818:41 - We'll see how to use albumentations with TensorFlow
818:45 - and also PyTorch which will permit us to see how easy it is
818:49 - for us to integrate albumentation with just any library.
818:53 - Note that this session was inspired by a question
818:56 - posed by one of us.
818:58 - Feel free to always ask questions
819:00 - as this will permit us better discourse.
819:03 - We shall be looking at the albumentation tool
819:05 - which is a specialized data augmentation library.
819:08 - Albumentation is a Python library
819:11 - for fast and flexible image augmentations.
819:13 - It efficiently implements a rich variety
819:16 - of image transform operations
819:17 - that is optimized for performance
819:19 - and those so while providing a concise
819:22 - yet powerful image augmentation interface
819:25 - for different computer vision tasks
819:27 - including object classification, segmentation and detection.
819:31 - Now we will look at why we need a dedicated library
819:34 - like albumentations.
819:35 - Here in the documentation they argue that
819:38 - if you do this kind of usual data augmentation
819:41 - there are a host of libraries like TensorFlow
819:43 - which we've seen already
819:44 - which do this kind of data augmentation.
819:47 - But then when carrying out data augmentation
819:49 - with different tasks like say object detection
819:52 - like in this case or let's take this example here
819:55 - which is more illustrative.
819:56 - You have this image as input and this bounding boxes
820:02 - which will permit you build a model for object detection.
820:06 - But then if you were to do data augmentation
820:09 - and then you want to augment this data
820:11 - by applying cropping like as you could see here
820:14 - notice how this image right here has been cropped
820:18 - like what we have now is something like this part
820:22 - or rather than this part actually.
820:24 - Now notice you have cropped out this whole image
820:27 - and you're left only with this.
820:29 - I think it's it goes right up
820:30 - so it should be something like this.
820:33 - Yeah, something like this.
820:35 - Okay, so you've cropped this image
820:37 - and it's part of your data augmentation pipeline.
820:39 - So let's take this off and this off.
820:43 - As we were saying, you crop this image
820:45 - and here's where you get.
820:47 - Now with the usual data augmentation methods
820:50 - or working with the usual libraries
820:52 - what you have to do is after doing this cropping
820:56 - you would have to manually modify each
821:00 - and every bounding box you find here.
821:02 - You see this bounding boxes right here
821:03 - or this one's not taken into consideration
821:05 - because it's not part of this crop.
821:07 - So you have to modify each and every bounding box
821:09 - you find here, this red, brown and boxes right here.
821:12 - And this is because this bounding boxes
821:16 - they get the positions with respect
821:18 - to this origin right here or in this case
821:21 - with respect to this origin.
821:22 - And so when you do this cropping
821:24 - obviously those positions change.
821:26 - Now, instead of doing this manually
821:29 - augmentation permits you to get this restructuring
821:35 - of this bounding boxes automatically
821:38 - without having to manually modify them.
821:42 - And apart from object detection
821:44 - another very common use case is in image segmentation.
821:48 - We have this original image
821:50 - which has been transformed into this one
821:53 - and then the image has a mask.
821:55 - So you are trying to segment
821:56 - the different parts of this image.
821:58 - And so after you've applied this rotation
822:01 - you now get augmentation to apply the same rotation
822:06 - in the outputs.
822:07 - Another reason why using augmentations is advantageous
822:11 - is the fact that it has this declarative definition
822:14 - of the augmentation pipeline
822:16 - and provides a unified interface.
822:19 - So basically this is what it takes
822:21 - to build augmentation pipeline.
822:24 - Now notice how with this
822:26 - we can also include this probabilities.
822:29 - So you see a probability 0.3 and this probability 0.5.
822:34 - Now stating that this brightness contrast augmentation
822:39 - will get a probability of 0.3 symptom means that
822:42 - you're gonna apply random brightness contrast
822:45 - three times for every 10 images you process.
822:48 - And this means that if you have a data set of let's say
822:52 - let's say we have a data set of 10,000 images
822:56 - we have this data set of 10,000 images
822:59 - then what our model will see
823:01 - are the probability of our model
823:05 - getting the original images is in this case
823:09 - of P equals 0.3, 0.7.
823:12 - So we have a probability it is most probable
823:16 - that we will get 7,000 out of this 10,000
823:22 - from the original data set which our model sees
823:25 - and then we have 3,000 which is gonna be augmented.
823:29 - Then you also have this horizontal flip
823:31 - so you have the 0.5.
823:33 - So this means that you could pass in an image
823:35 - and then there is no brightness contrast
823:38 - and there is no horizontal flip.
823:40 - Same as you could pass in an image
823:41 - and you have the random brightness contrast
823:44 - and you don't have this or you may not have this
823:46 - and you have this or you may not, we've seen this already
823:50 - or you may have this and have this.
823:52 - But with this random crop you always have it
823:54 - because here there's no probability specified.
823:57 - Then also it's advantageous to make use of augmentation
824:01 - because it has been rigorously tested.
824:05 - As you can see, it has been battle tested,
824:07 - used widely in the industry, deep learning research,
824:10 - machine learning competitions and open source projects,
824:13 - high performance, diverse set of supported augmentation,
824:16 - extensibility and rigorous testing.
824:19 - We also have here this list of transforms
824:22 - and their supported targets.
824:23 - As you could see, this list is broken up into two parts.
824:27 - We have the pixel level transforms
824:29 - and we have the spatial level transform.
824:32 - So if you want to get more information
824:34 - for each and every one of these transforms,
824:36 - it suffices just to click on this.
824:38 - So you could say, for example,
824:41 - let's pick this sharpen right here.
824:44 - Click on the sharpen and you have here the arguments
824:48 - and the description.
824:50 - Let's take another example from the spatial level transforms.
824:54 - Here we could take the vertical flip.
824:57 - So click on this vertical flip, you see,
824:59 - we have this float probability of applying the transform
825:02 - default to 0.5.
825:03 - Now note that if you're dealing with a very large dataset,
825:06 - that is, if your initial dataset is very large,
825:08 - then you could use probabilities
825:10 - between 0.1 and 0.3.
825:14 - Reason being that since your dataset is already large,
825:17 - it doesn't need data augmentation that much.
825:20 - Now, if you're dealing with a small, medium sized dataset,
825:22 - you could use probabilities between 0.4
825:26 - to 0.5.
825:28 - Nonetheless, you could always pick whatever value
825:30 - depending on how it affects your model performance.
825:33 - Getting back to the code,
825:34 - we're going to make use of this example,
825:36 - TensorFlow data augmentation pipeline
825:39 - built with augmentations.
825:41 - So here we have the transforms
825:44 - and then we have the augmentation function,
825:49 - then data preprocessing,
825:51 - and finally integration with TensorFlow datasets.
825:56 - Now, based on the kind of dataset we're dealing with,
825:59 - we have to be very careful
826:01 - in the dataset augmentation strategies we're going to be using.
826:05 - So like, yeah, we could use this random rotates
826:08 - because rotation doesn't wipe off those sections
826:11 - which contain information that permits us to differentiate
826:15 - between a parasitized and an unaffected cell.
826:19 - Getting back to our random rotate 90,
826:22 - you have here this argument, a float,
826:25 - which is a probability of applying this transform
826:27 - default 0.5.
826:29 - So we could make use of this random rotate right here.
826:32 - Then just below, we have other different rotations.
826:35 - We have the random rotate 90 apply,
826:38 - which rotates the image a certain number of times.
826:41 - We have the geometric rotates
826:45 - where we can select the angle
826:48 - and which will do the rotation.
826:50 - Then we have this safe rotate
826:53 - would avoid this kind of data augmentation strategies
826:56 - like the cropping because here you could crop out information
827:00 - which permits us differentiate
827:01 - between the parasitized and uninfected cells.
827:06 - You also have this resize here,
827:08 - which we were using preprocessing our images.
827:13 - We have the vertical flip and the horizontal flip,
827:18 - which we are gonna use.
827:19 - So here we have horizontal and vertical flip.
827:22 - Let's add that here.
827:23 - We can also do just a flip.
827:25 - We also have this random grid shuffle,
827:27 - which we could make use of.
827:29 - This randomly shuffles grid cells in an image,
827:32 - meaning that if we have this kind of image
827:35 - and then we've picked a grid size of three by three,
827:37 - we could break this up this way.
827:40 - You break this up and then you have this three by three
827:43 - grid cell and then you simply randomly shuffle this position.
827:47 - So this one can end up here.
827:50 - This ends up here.
827:51 - This ends up here and so on and so forth.
827:54 - Could also have this random brightness contrast.
827:57 - So with this one, we'll take this all
828:02 - and then paste it right here.
828:04 - Random brightness contrast.
828:06 - The next one, let's take this sharpen.
828:11 - So that's it.
828:13 - You could use other data augmentation strategies you want.
828:16 - Always make sure that you visualize the outputs
828:21 - to better understand exactly what you're using.
828:25 - Now we define our transforms.
828:27 - We have our transforms, augmentation, compose,
828:31 - and then we have this list
828:33 - which is made of the different augmentations.
828:35 - Yeah, we just copy this out.
828:38 - Let's have this copied.
828:40 - But before this, let's do a resize.
828:42 - So yeah, we could have a dot resize
828:46 - and then we specify the image size and that's it.
828:51 - So we specify the image size.
828:54 - We're going for the horizontal and vertical flip.
828:57 - So let's copy this out.
828:59 - There we go.
829:00 - You have that horizontal vertical flip.
829:03 - For those random brightness contrast,
829:05 - we're gonna use the default parameters.
829:08 - Sharpen, we're gonna use the default parameters
829:10 - and that's it.
829:11 - We now have our transforms.
829:13 - So we could run the cell.
829:16 - We get in this error and this is because right here
829:19 - we have to have this.
829:21 - So it's augmentation.
829:22 - This is going from augmentation.
829:24 - We run this again.
829:26 - We get in some errors.
829:28 - Augmentation has no attribute sharpen.
829:30 - Even when we comment this one,
829:32 - we will also get this error.
829:34 - Augmentation has no attribute random grid shuffle.
829:38 - So we'll just comment this tool and then run that again.
829:41 - Okay, that's fine.
829:42 - That looks fine.
829:43 - We can also implement this augmentation one off.
829:48 - With this one off, either the vertical flip
829:50 - or the horizontal flip.
829:52 - So let's take this out of this year
829:54 - and put it right here.
829:56 - We have one off horizontal flip or the vertical flip.
830:00 - Now we're gonna have this year and then,
830:04 - so we have this list actually.
830:05 - We're gonna create this list
830:07 - and it's gonna be made out of this two transformations,
830:11 - which is the vertical and the horizontal flip.
830:14 - Let's have this year and close this.
830:17 - Okay, so we have defined that one off
830:20 - and then we could also specify a probability.
830:22 - So let's take P equals 0.3, for example.
830:26 - And there we go.
830:28 - This probability year actually defines
830:31 - whether the one off will be applied or not.
830:34 - And so we have that.
830:35 - We could run this again and that's fine.
830:37 - Drawing inspiration from this method
830:39 - given the documentation,
830:41 - we are gonna create this own method,
830:45 - aug-albumant, which takes in an image,
830:49 - creates this dictionary,
830:51 - feeds this information in the transforms,
830:54 - which we've created here,
830:56 - and then normalizes the image.
831:00 - So this is what we do with this aug-albumant method.
831:05 - From here, we could have our train data set
831:09 - similar to what we've been doing already.
831:11 - But what the difference is that instead of this,
831:14 - we have now process data,
831:16 - getting back to the documentation.
831:18 - In this process data year,
831:20 - we have, let's copy out this process data.
831:22 - In this process data,
831:24 - what we're actually doing, let's add this cell here.
831:28 - In this process data, we're taking the image,
831:30 - we're taking the level, we're taking the image size,
831:33 - and then actually modify this image
831:37 - and have this and then take this level
831:40 - and pass it to the output
831:42 - since the level remains unchanged.
831:43 - Now for this image size, we wouldn't need this
831:45 - so we could take that off.
831:47 - We're just taking the image and the level size,
831:49 - and then as this input, we pass in the image.
831:54 - The tensor out is gonna be updated type flow32.
831:57 - The function is gonna be arc albument.
832:00 - So arc albument is our function.
832:03 - And then we are gonna make use
832:05 - of this tensorflow NumPy function.
832:07 - Now getting to documentation,
832:08 - we see it wraps a Python function
832:11 - and uses it as a tensorflow operation.
832:15 - That said, we could still work in the graph mode
832:18 - even though we are having this Python code right here.
832:23 - And this is because tensorflow is gonna convert
832:27 - everything that goes on here as a tensorflow operation.
832:31 - So we could run this.
832:33 - We have the process data that looks fine.
832:36 - We could run this too.
832:37 - And then finally, we have our train data set.
832:40 - So we run our train data set, batch size not defined.
832:44 - We should have run this here.
832:47 - Okay, we run this cell.
832:49 - And then getting back, we run this again.
832:51 - It should be fine now.
832:53 - Okay, so here we have train data set.
832:56 - We could look at that, and that looks fine.
832:59 - We can quickly visualize our data set.
833:01 - We have the image, the level,
833:05 - and then we have an element pick from our data set.
833:09 - So now let's im show this, im show im.
833:13 - Around that, we get this error
833:16 - because we are dealing with batches of 32.
833:18 - So let's just pick one of these elements.
833:21 - Let's run that now, and that should be fine.
833:23 - Okay, so here's what we obtain.
833:26 - Now we could have many more plots.
833:28 - So we have this figure.
833:30 - We define the figure size for i in range one to 32,
833:36 - one and a half, plot dot subplot,
833:41 - subplot, eight, four, and then i.
833:45 - And then plot dot im show our image, i.
833:50 - Okay, we run that.
833:52 - And here's what we get.
833:55 - So we have this, and now what we could do is,
833:58 - let's get this cut out,
834:01 - which we have here in the documentation.
834:03 - Specify a number of holes, maximum height size,
834:07 - maximum width size, the field value.
834:09 - Always apply false and the probability.
834:12 - So when we apply this, you're gonna see exactly
834:16 - or better understand all these different arguments right here.
834:19 - So let's go ahead and apply cut out,
834:21 - which should be more visible as compared
834:24 - to the other transformation like the rotations that we did.
834:27 - So yeah, let's have this year, let's,
834:30 - you see how easy it is now to integrate
834:33 - any data augmentation strategy you have.
834:37 - So yeah, we have a, is that a cut out?
834:44 - And then we take the default parameters.
834:46 - We have that cut out, let's run that again.
834:49 - And what we need to do, because the train data set
834:52 - has been modified.
834:53 - So we need to like get the initial train data set we had
834:56 - after getting from here.
834:58 - Let's get back down and then check this out.
835:02 - Okay, so we have done the transform,
835:04 - all couple moments, run this, run this,
835:07 - and now we could visualize.
835:10 - Okay, yeah, the cut out hasn't been implemented.
835:13 - That is, obviously there's a probability of 0.5.
835:15 - So it's possible that we don't have cut out here.
835:18 - Now, if you look at this, you see clearly that
835:21 - in some parts of this, let's increase the size.
835:24 - Let's see, 15.
835:27 - Okay, as we're saying, if you look at this,
835:29 - you'll notice that in some parts or in some images,
835:33 - we have this portions which have been cut out.
835:36 - So you could see, for example, this one,
835:38 - you have this part which is cut out
835:39 - and then getting back to the documentation,
835:44 - we have your default number of holes eight
835:46 - and we could copy all this out
835:48 - and then modify it so you see how this could change
835:52 - the kind of output we get.
835:54 - Because here you see, when there's cut out,
835:55 - we have one, two, three, four, five.
835:58 - Surely you have the other holes
836:00 - or cut out regions in the black spot
836:02 - so you can identify them.
836:04 - And then here you have one, two, three, four.
836:06 - You see, you even have this, looks like a cut out.
836:08 - Anyway, that's the idea.
836:10 - You specify the number of cut out,
836:12 - number of regions you want to cut out.
836:14 - You also specify the value is going to tick.
836:17 - So like in this case, when you specify zero,
836:19 - it simply means you're going to be
836:22 - having this black spots here.
836:24 - So let's have that back here.
836:29 - Okay, so let's put this in here
836:30 - and then you have number of holes.
836:33 - So that's it, you could modify this
836:34 - and observe exactly what goes on.
836:37 - To increase the size of the cut out region,
836:39 - you could simply specify the values here.
836:43 - And then you have the fill value.
836:44 - You have this Boolean, always apply
836:47 - and then you have the probability.
836:48 - So if you want to always have cut out,
836:50 - you could simply send this probability to one
836:53 - and that's how it works.
836:54 - So we've seen how this works.
836:56 - We could go ahead and train our model
836:59 - using this augmented data.
837:01 - But before training, let's take out the cut out as
837:03 - it was just meant to show you how this works.
837:08 - Okay, so let's, we run this again,
837:11 - run this and then we visualize.
837:14 - As you can see now, there is no cut out region.
837:17 - So that's as expected.
837:19 - That said, we'll move forward to training our model again.
837:23 - So we run that training process.
837:25 - So the other results we get after training
837:28 - for several epochs, you'll notice that the accuracy
837:32 - doesn't go up to 99% as it used to be before.
837:37 - And also with this validation accuracy,
837:41 - we even get better results.
837:44 - Here's the result we get after training for several epochs.
837:47 - Notice how here the accuracy doesn't get
837:50 - as high as it used to be before.
837:52 - So the accuracy we're getting now is,
837:55 - the highest value of accuracy we get is like 94.5%.
838:00 - And the validation accuracy we get in year,
838:04 - it's about that, like the highest we get in year
838:07 - is like 94.48%.
838:10 - And this is the accuracy versus epoch plot we now get.
838:13 - Also, after checking our implementations GitHub page,
838:16 - we found this solution to those problems we're getting here
838:20 - where we weren't able to make use of the cut out
838:23 - or rather we were able to make use of the sharpen
838:25 - and the random grid shuffle.
838:27 - So yeah, when we do this install, which you have here
838:30 - and then uncomment the sections,
838:32 - you should now have this working.
838:38 - Hello, dear friends and welcome to this new session
838:41 - in which we'll be building custom losses and metrics.
838:44 - The first method we'll look at
838:46 - is building a custom loss method without parameters,
838:49 - the next with parameters.
838:51 - And finally, we'll build a custom loss class.
838:55 - And then from here, we'll go on to build custom metric method
838:58 - with parameters, custom metric method without parameters
839:01 - and custom metric classes.
839:04 - Most times when building models and training them,
839:07 - all you need to do is to pick this loss function
839:10 - from one of these losses available in the documentation.
839:14 - Now, in a case where you have to build a loss function
839:18 - from scratch or a custom loss function
839:22 - in which defining the loss isn't as easy
839:25 - as just specifying this loss function name right here.
839:29 - So right now, let's look at how to build
839:32 - such custom loss functions.
839:34 - Here we're gonna have this custom binary custom.
839:38 - Let's call it custom BCE. BCE stands for binary cross entropy.
839:42 - So we have the custom BCE loss right here
839:45 - and then we'll define it just above this cell.
839:49 - Now we have our custom BCE loss.
839:53 - There we go.
839:55 - And here, what we're gonna do is we take our,
839:58 - we define this BCE object,
840:01 - the binary cross entropy loss object.
840:04 - And obviously here we have binary cross entropy.
840:08 - There we go.
840:09 - So we've defined this object of this class
840:12 - and then we return this computation
840:15 - of the binary cross entropy of the Y true and the Y red.
840:21 - So here, this Y true is the actual output levels
840:25 - and here's what the model predicts.
840:28 - You could understand this better
840:30 - by taking a look at the documentation
840:32 - where here you see the define this object
840:35 - and then in computing this binary cross entropy loss,
840:39 - you see specify this Y true and Y red right here.
840:44 - Now the binary cross entropy could take several arguments,
840:47 - for example, from logits, levels smoothing,
840:50 - the axis reduction and the name.
840:53 - So that's it.
840:53 - We have this custom binary cross entropy
840:56 - and just here, all we need to do is to specify this name.
841:00 - So let's take this off and we're fine.
841:02 - Okay, now we just run this.
841:04 - We run this binary cross entropy.
841:06 - We run this metrics not defined.
841:10 - Let's run the cell, run the cell
841:12 - and then finally run the training.
841:14 - We get this error.
841:16 - This function takes zero positional arguments
841:19 - but to a given.
841:20 - So we have to have this year Y true and Y predicted.
841:27 - Okay, we run that and run that.
841:30 - We could see here how this training process looks similar
841:33 - to what we've had already
841:35 - when we're dealing with a preconceived
841:38 - binary cross entropy loss.
841:41 - Now that said, let's suppose that we are going to pass
841:45 - in a parameter here.
841:46 - So we want to parameterize the way
841:49 - in which we calculate this binary cross entropy.
841:52 - That is want to have this binary cross entropy
841:55 - multiplied by a given factor.
841:57 - Let's say we want to multiply this by a factor
841:59 - of say 0.5.
842:01 - In that case, we could have this year as a factor
842:04 - and then we'll pass in this factor right here.
842:09 - So we pass this in, let's stop the training
842:11 - and then we train to see how that works.
842:15 - We run this year and run this factor not defined.
842:19 - We could include this factor here.
842:23 - We run it.
842:24 - Factor still not defined.
842:25 - And this brings us to the way
842:27 - in which we define models or rather define loss functions
842:31 - in which we have a given parameter.
842:34 - So right here, we're going to define a loss year,
842:37 - a loss function, and then we pass in our Y true, Y pride.
842:42 - So it's going to be like similar to what we had
842:43 - with a custom BCE without a factor.
842:46 - We have that.
842:47 - And then in year we have this BCE
842:51 - and then we return this right here.
842:54 - Now notice how we take this tool off.
842:57 - So we have in just this factor
843:00 - and that's what we're going to be passing here.
843:01 - And then here what we return is this loss method right here.
843:08 - So we define this custom binary cross entropy.
843:12 - We return this loss method and this loss method.
843:15 - We take in the Y true, the Y pride
843:17 - and we carry out the computation as we had done before.
843:21 - But now note that we're multiplying this by a given factor.
843:24 - So here we could define our factor.
843:26 - Let's have this factor and your factor.
843:30 - Then before this, we'll define this factor.
843:34 - Let's take this factor to be equal one
843:36 - because knock is actually equal one.
843:38 - But supposing you are having a different problem,
843:40 - you may want to modify this parameter as you wish.
843:43 - Let's have this factor here.
843:45 - That's fine.
843:46 - Now let's rerun this and hopefully we should have no error.
843:49 - So that's it.
843:50 - No error, that's fine.
843:52 - And then we run our training.
843:54 - As you can see, the training process continues normally.
843:57 - And we could now check out on another method
844:00 - of building custom loss functions,
844:04 - which is that of actually building a loss class,
844:07 - which inherits from the Keras loss class.
844:12 - So that said, let's add this year.
844:15 - And then we have our class.
844:17 - We'll call this custom BCE
844:21 - and then we'll inherit from Keras losses loss.
844:28 - Okay, so that's it.
844:30 - Now, once we have this right, we have our init method.
844:34 - And just like we did here, we're going to pass in a factor.
844:37 - So let's have this factor.
844:38 - Then we inherit methods from the parent class.
844:41 - There we go.
844:42 - We have our custom BCE,
844:44 - custom BCE,
844:47 - and that's it.
844:48 - Okay, so we have this right.
844:50 - And then we now go straight forward
844:52 - to creating this factor variable
844:56 - and take this value factor.
844:59 - And that's it for the init method.
845:01 - From here, we're going to define a call method
845:03 - where those operations are going to be carried out.
845:06 - Now, here we have this call method.
845:08 - And then what we take in is ytrue and then ypred.
845:12 - Okay, so we've taken this.
845:13 - We now do exactly the same thing we did here.
845:17 - We could copy this out and paste it right here.
845:21 - We have our BCE defined operation carried out.
845:25 - And here we have self dot factor.
845:29 - Okay, so everything looks fine.
845:31 - We have our custom BCE defined invalid syntax.
845:36 - There should be a dot just right here.
845:38 - We run that again, that's fine.
845:40 - Okay, so we have that right.
845:42 - And what we do now is take this off.
845:45 - So here we have custom BCE,
845:48 - which is this class where define
845:50 - how we just defined right here.
845:52 - Okay, now we should also note
845:54 - that you could pass in the factor here.
845:56 - We actually have to pass in the factor.
845:58 - That's it, because we didn't specify the default value.
846:01 - So we have to pass that.
846:03 - Now let's run this.
846:05 - That's fine.
846:05 - And then we rerun our training
846:07 - and make sure everything works well.
846:09 - We get in this error.
846:10 - The call takes two positional arguments
846:12 - but three were given.
846:13 - Let's get back to this and we see this error.
846:16 - We do not put in the self.
846:18 - Okay, so that's it.
846:19 - And let's rerun this.
846:21 - As you can see, the training went on successfully.
846:24 - Now we know how to build custom loss functions
846:27 - with tensorflow.
846:29 - We then move on to building this custom metrics
846:33 - just similar to the way we deal with a loss.
846:36 - So here we have a custom metric.
846:38 - We'll start with a custom method.
846:39 - So we'll take this off.
846:41 - And then just right here, we have custom metric.
846:45 - Let's add this and this.
846:47 - We paste this out here and have custom accuracy.
846:51 - We now get back to the documentation,
846:53 - tensorflow, Keras, and then metrics.
846:56 - So right here, you would have this accuracy
846:59 - which has been defined.
847:00 - Here we go.
847:01 - We have binary accuracy.
847:03 - So that's it.
847:04 - We have binary accuracy.
847:05 - We specify why true, why proud, and the threshold.
847:08 - There we go.
847:09 - We can paste this out here.
847:11 - This is the loss.
847:13 - Let's go back up to this metric.
847:15 - Okay, let's paste it out here.
847:16 - So now we have this accuracy
847:18 - which takes in the why true and why proud.
847:20 - So we just return, we basically return this.
847:23 - We return binary accuracy, accuracy,
847:26 - which takes in why true and why proud.
847:31 - Okay, so let's take this off and that looks fine.
847:34 - Now we have our custom accuracy, custom accuracy.
847:37 - We run this as well.
847:40 - We get to this compile custom accuracy, fine.
847:43 - We compile our model and we train the model.
847:46 - Everything looks good.
847:47 - You could see this custom accuracy.
847:49 - Now let's suppose we had a factor.
847:52 - We're gonna use this same kind of approach we had
847:55 - with the loss method.
847:57 - So we just simply copy this out.
847:58 - And then right here, we have this paste that.
848:03 - Then we have our custom metric right here.
848:06 - Here again, we have custom accuracy, custom accuracy.
848:09 - We're taking a factor.
848:11 - We have this metric, we change it to metric.
848:14 - There we go.
848:15 - And then we simply have this.
848:17 - So let's just copy this out and paste right here.
848:21 - So here we have this here, let's copy this out.
848:25 - Okay, so we have this and we now multiply this
848:28 - by the factor.
848:29 - So yeah, again, we call on the metric method and that's fine.
848:33 - We run that, we have this custom accuracy.
848:35 - Oh, okay.
848:36 - We haven't stopped this training yet.
848:38 - So let's stop this training.
848:39 - That's fine.
848:40 - We get back here and run this.
848:43 - Looks good.
848:45 - Compile and train a model.
848:47 - We're told this takes one positional argument,
848:50 - but two are given.
848:51 - So let's get back to this and then include this factor.
848:54 - Run that.
848:55 - And the training looks fine.
848:56 - We can see our metric right here.
848:58 - Our next approach will be the class-based approach
849:01 - similar to what we deal with the loss.
849:03 - So here we have this custom loss class.
849:05 - We'll build a custom metric class
849:08 - beside this and this code.
849:11 - So we have that and then we have our custom metric class.
849:14 - This class here is a subclass of the class metric.
849:18 - We have our class metric and then just right here,
849:22 - we have custom accuracy.
849:26 - So we have this custom accuracy
849:28 - and then yeah, we specify the name.
849:31 - Let's have this name.
849:33 - Let's say we want to put here custom accuracy.
849:37 - Okay.
849:38 - And then we pass the factor too,
849:40 - which by default was set to one.
849:42 - So that's it.
849:43 - Now we have custom accuracy.
849:45 - We have our factor.
849:47 - And then what we'll do is define this accuracy
849:51 - by calling on the add weight method,
849:53 - which comes with this metric class right here.
849:57 - So we have this add weight and then we specify the name,
850:00 - which is the name we've passed.
850:02 - We have the initializer equals zeros.
850:06 - So that's it.
850:07 - You'll notice that this will look slightly different
850:10 - from what we saw with the custom loss
850:12 - as unlike with the loss class, which has this call method,
850:16 - yeah, would instead use three other different methods.
850:20 - The first we'll be looking at
850:22 - will be the update state method.
850:25 - We have update state.
850:27 - Then next we will have result method
850:30 - and then the reset method.
850:33 - So we have this reset right here.
850:35 - The update state method here
850:37 - permits us update our metric state.
850:41 - The results permits us to output the metric values
850:45 - and then the reset permits us reset the metric states.
850:50 - So we have here reset states.
850:52 - Here we go.
850:53 - We start with this update state method.
850:56 - In this update state,
850:57 - we're going to be assigning a value to this accuracy variable.
851:01 - So just right here, we have self.accuracy.
851:03 - We assign a given value.
851:06 - Now the value we're going to assign
851:07 - is exactly the same we had here with this binary accuracy.
851:12 - So let's take this factor and then paste out here.
851:16 - We paste it here.
851:18 - Okay, so this is our self.factor.
851:22 - That's fine.
851:22 - So that's it.
851:23 - We assign this value to this accuracy variable.
851:29 - This method to takes in the Y true, the Y bright.
851:34 - And then with the result, we have a return.
851:38 - So here we have this return self.accuracy.
851:42 - That's it.
851:43 - And then here for the reset states, reset states,
851:47 - we are going to reset the state at the end of each epoch.
851:51 - So once we finish with one epoch, we reset the state
851:54 - and then re-update the state and then output this result.
851:59 - So here again, we have self.accuracy.assign.
852:04 - The value is zero.
852:05 - That's fine.
852:06 - Let's run the cell.
852:07 - We get this error because this is actually metrics.
852:10 - So let's take this off.
852:12 - We have here metrics.
852:14 - We run that again.
852:15 - It should be fine.
852:16 - Okay, so we have our custom accuracy built.
852:19 - We now get into this year.
852:22 - So let's take this off now and specify our custom accuracy.
852:28 - Then here we specify the name.
852:30 - But anyway, we've got in default value.
852:32 - So we could just pass it this way.
852:34 - Let's run the cell and then we will start the training again.
852:39 - We get this error of this state,
852:41 - got an unexpected keyword argument, sample weight.
852:44 - Now we get into this error because year,
852:47 - we didn't put the sample weight as one of our arguments.
852:52 - So yeah, we should have the sample weights.
852:55 - We're not going to be using that.
852:57 - So we just set it to known.
852:58 - Let's run this again.
853:00 - Fine.
853:01 - We recompile and then train our model.
853:06 - We get this other error.
853:07 - The input Y of equal op has type float 32.
853:11 - That doesn't match type 64 of argument X.
853:16 - So this is actually an error which comes up
853:19 - because right here when competing this binary accuracy,
853:24 - we have Y true and Y paired of different data types.
853:26 - So let's print this out and you could see that.
853:29 - Y true and then Y paired.
853:35 - We run this again.
853:37 - Looks fine.
853:39 - We get down to this year, this compilation.
853:42 - Recall when we want to debug,
853:45 - we could run eagerly.
853:46 - So let's run eagerly, set that to true.
853:50 - And that's fine.
853:51 - We run this again and try to understand what's going on.
853:56 - So as you could see, that's exactly what we expected.
853:59 - We have this int 64.
854:01 - So this is an int and this is a float 32.
854:04 - We have the Y true.
854:06 - So let's, we could cast this year.
854:08 - So let's cast this value tf.cast.
854:11 - And then we specify the D type to float 32.
854:16 - Float 32, that's fine.
854:19 - And then just year we take this off.
854:22 - Okay, so we run this again.
854:24 - There we go.
854:25 - We now compile and then train.
854:28 - We get this other error,
854:29 - cannot assign value to variable custom accuracy.
854:33 - The variable shape and the assigned value shape
854:35 - aren't compatible.
854:37 - We get back to this year.
854:40 - And what we do is we're going to print out
854:43 - this output, which we assign to the accuracy.
854:47 - Yeah, we define this output.
854:49 - And then we simply take this out from year
854:53 - and have this as our output.
854:55 - And now we're going to assign this output.
854:58 - So since we now have the output,
855:00 - we could print it out so we understand exactly
855:02 - what we're assigning to the accuracy.
855:04 - Let's run this cell, that's fine.
855:07 - And then we get to this compile and train the model.
855:13 - Okay, there we go.
855:14 - So now we see exactly what we're assigning,
855:18 - but you'll notice that
855:20 - instead of just assigning a single value,
855:23 - we assign this list.
855:25 - And this is the reason why we're getting that error.
855:28 - To solve this problem,
855:31 - we'll also have to understand that
855:33 - this output actually corresponds
855:36 - to whether the model's prediction was correct
855:40 - or not for each and every element of the batch.
855:43 - So when we have a one like this,
855:45 - it means the prediction was correct,
855:47 - or the model's prediction was correct,
855:49 - and one and so on and so forth.
855:51 - So because we've trained a model for quite a while,
855:55 - you see that for all this,
855:56 - or for this particular batch,
855:58 - we have kind of like 100% accuracy.
856:03 - So instead of printing out 100%,
856:06 - what a model gives out is this list.
856:09 - Now let's look at this again.
856:11 - We see now that with this other batch,
856:13 - we have some zeros.
856:15 - So out of 32, we've had three zeros.
856:18 - And hence, our accuracy in this case is 90% for this batch.
856:25 - That said, we have to figure out a way of
856:28 - counting the total number of ones
856:30 - and then dividing by the total or by this length,
856:34 - which in this case is 32,
856:36 - or by the batch size, which is 32,
856:39 - and then multiplying by 100,
856:42 - or we could just let it like that.
856:44 - So we make use of this method, count non-zero,
856:48 - where here we could count the number of non-zeros values
856:52 - very easily by just calling it, and I'll be fine.
856:55 - So let's copy this here,
856:58 - and then instead of having this output,
857:02 - we'll count the number of non-zeros.
857:05 - There we go, we count that.
857:08 - And now we divide by the length of this output.
857:12 - So we divide by the length of the output.
857:14 - But this length is gonna be an int,
857:17 - so we're gonna cast this so we get a float.
857:20 - We have that, and then D type equals here, float 32.
857:24 - Looks fine.
857:25 - Okay, so now we have that, and then we could,
857:28 - let's take off this output, and then we run again.
857:31 - We then compile, and then train the model.
857:35 - We still get another error, so what we'll do is
857:38 - we're gonna just check in the documentation,
857:41 - and then here, what do we see?
857:42 - We see that by default, this is an int.
857:45 - So here, we could also cast this,
857:48 - we cast this into a float.
857:50 - There we go, we have this output.
857:52 - We don't really need to cast that
857:53 - because we could specify the data type.
857:55 - So here we have the output,
857:57 - and then D type equals TF, float 32.
858:01 - Okay, so that's it.
858:04 - Hopefully this should be okay now.
858:06 - We check, we compile the model, start the training,
858:09 - and everything looks okay now.
858:13 - We could now stop this and then get back to graph mode.
858:18 - So let's run again so that we could now train even faster.
858:22 - And that's it for this section.
858:23 - We'll see how to build custom losses and metrics.
858:26 - Thank you for getting around to this point
858:28 - and see you next time.
858:33 - Hello everyone, and welcome to this new session
858:36 - in which we'll see how to switch between the ego mode
858:39 - and the graph mode in TensorFlow.
858:42 - We'll first start by understanding what these two modes mean
858:46 - and also when to use either of them.
858:49 - Then finally, we'll see how to switch between these modes
858:52 - by simply adding this TF function decorator right here.
858:58 - So far in this course,
858:59 - we've been building methods like this one in the ego mode.
859:04 - That is, we've been following a Pythonistic approach
859:08 - in creating these methods.
859:11 - Apart from this ego mode or this ego way
859:14 - of manipulating data, we also have the graph mode.
859:19 - In the graph mode, we have data which is passed
859:23 - into these different nodes right here.
859:26 - Now, these nodes are operations.
859:29 - And so when we pass in our data, which could be a tensor,
859:33 - this data gets modified and then passed to the next node
859:38 - and right up to this output right here.
859:41 - This means that if we consider this line of code
859:44 - where we have X equal Y times Z,
859:48 - TensorFlow is capable of converting this
859:51 - into a graph with a single node
859:54 - where this node will represent this multiplication operation.
859:58 - So here we have this multiplication operation
860:01 - and then we have the two inputs, Y and Z.
860:08 - Then our output X.
860:11 - Let's draw it this way.
860:13 - We have Y, Z, the multiplication operation.
860:17 - Then we have X.
860:18 - Now this X, this data we get can be passed
860:21 - into two other operators or two other operations.
860:26 - So here we could have addition
860:28 - and here we have subtraction.
860:30 - Then we could combine this to finally have,
860:34 - say an addition operation and then get the output.
860:38 - Now let us come up with some code
860:41 - which will represent this rest of our graph.
860:45 - Here we've had X and then we could have, for example, R
860:50 - which is equal X plus a certain constant.
860:57 - So X plus a constant, let's call that constant key.
861:01 - We have X plus a constant key, let's just say constant.
861:05 - So this addition we have here is actually an addition
861:08 - to a fixed constant and that's it.
861:11 - So here we have R and then we could define S
861:14 - to be equal Y minus this same fixed constant.
861:20 - There we go, we have S.
861:22 - Now we have S.
861:24 - We could get an output by adding up S and R.
861:28 - So let's call this output T.
861:31 - We have T which is equal R plus S.
861:37 - Now this addition here is different from this one
861:40 - as this one takes in just one input.
861:42 - Since this input is gonna be added to the constant
861:45 - and the subtraction here takes in just one input
861:47 - as this input is gonna be subtracted
861:49 - or the constant will be subtracted from the input
861:53 - which in this case is meant to be X actually.
861:56 - Let's take this off, this is X
862:00 - because here you have, after this multiplication,
862:02 - you have X and this is X that gets in this
862:05 - and the same exact goes this way.
862:07 - So here it's meant to be X and that's it.
862:10 - So here we have now S.
862:12 - Once we get S, we have S here, this tensor S
862:18 - and then here we have our tensor R.
862:20 - Now R plus S gives us an output T.
862:23 - So we see that under the hood,
862:26 - what TensorFlow is capable of doing is
862:28 - taking this Python code you write
862:31 - and convert it into a graph like this one.
862:33 - One advantage of working with the graph mode
862:37 - is that this now becomes portable
862:40 - or this code now becomes portable
862:43 - since in a case where you don't have the Python interpreter,
862:49 - you have now this data structure
862:52 - which could be used in any environment.
862:55 - Hence making our code or this method more portable.
863:00 - Another advantage is that since we now dealing
863:03 - with this data structure,
863:05 - it can be broken up into several or separate blocks
863:10 - hence making it easy for parallelization
863:13 - leading to faster computations on devices like GPUs.
863:18 - The good news here is in order to convert this method,
863:22 - for example, into this graph,
863:26 - all you need to do is to add this TF function decorator.
863:32 - And when once this is done,
863:34 - when TensorFlow gets this block right here,
863:38 - it does what we call tracing.
863:41 - During the tracing process,
863:43 - this graph right here is generated.
863:45 - And once generated, each time you'll be calling this method,
863:49 - you now have your inputs passed into this data structure
863:55 - and your outputs generated
863:57 - without you going through each and every step in this method.
864:01 - Getting back to the code, all we need to do right here
864:04 - is to include our TF function decorator.
864:07 - And this automatically is turned to graph mode.
864:11 - That's fine.
864:12 - Run this.
864:14 - And then for this augment, we just add TF function
864:19 - and that's it.
864:20 - So there we go.
864:22 - Now right here in this Route 90 class,
864:25 - what we'll do is at this level of this call,
864:29 - that is when we compute in this Route 90,
864:33 - we will have this decorator put out here.
864:37 - So that's it.
864:39 - That's fine.
864:40 - We run the cell, augmented layers.
864:42 - Okay.
864:44 - Then this augmented layer method too, we have TF function.
864:48 - Now note that previously we had seen
864:51 - this Resize Rescale layers.
864:52 - So let's get back.
864:54 - We have Resize.
864:56 - Okay, that's the Resize Rescale layers.
864:58 - But if we were calling instead this Resize Rescale,
865:03 - and that we had already applied this decorator right here,
865:07 - then it would have been needless
865:09 - putting this decorator here.
865:11 - So in fact, what we're saying is if you have a big function
865:14 - and you have this function,
865:17 - say let's say we have A equal
865:21 - a function call to small function.
865:24 - There we go.
865:25 - And we return some value.
865:28 - We have that.
865:29 - Let's say we return known.
865:30 - Okay, so we have this function of this method.
865:35 - We have this method defined
865:37 - and then we have the small function right here.
865:40 - Then we could also, let's define small functions.
865:44 - We have small function
865:46 - and then we have return whatever value is in known.
865:50 - So what we're trying to say is
865:52 - if you have this decorator,
865:55 - this TF function decorator put out here,
865:58 - it's needless having it in this small function
866:03 - because once you put this,
866:05 - then all the methods which will be called in this method
866:10 - will automatically be converted to graph mode.
866:13 - So that's how this works.
866:15 - We take this off.
866:17 - Let's explore another interesting effect
866:18 - of working with a graph mode.
866:21 - So right here we have print.
866:23 - Let's say I was here.
866:25 - Okay, and then what we're doing here
866:28 - is we're going to call this method.
866:30 - We're going to call this resize method the first time.
866:34 - We have resize, rescale.
866:37 - We pass in the original image, original image
866:40 - and the level which we got from here.
866:44 - So that's it.
866:45 - We run this and we'll see we have this I was here.
866:50 - Now we'll repeat this several times.
866:53 - So let's have this.
866:54 - And repeating this, we should expect
866:57 - to have this printed out three times
867:00 - since we run this actually three times.
867:03 - Let's run this cell.
867:05 - And we see that this is printed out only once.
867:08 - Okay, that's what we notice.
867:10 - Now let's take off this TF function decorator
867:13 - and run this again.
867:16 - We see that this is printed twice.
867:18 - And the reason why we have this is because
867:20 - in the graph mode, what we have is a data structure,
867:24 - a graph data structure.
867:26 - So we have this graph, we have this nodes
867:31 - which have been linked to one another.
867:34 - And so when we write code, input output.
867:37 - So when we write code like this
867:39 - and it's converted into this graph format,
867:43 - the very first time we make a call on this method,
867:47 - we actually carrying out tracing.
867:49 - Now tracing permits us to convert this
867:53 - into this graph format.
867:54 - And then the next time we make this call,
867:58 - since in the graph mode, we basically storing
868:00 - this operations and the data that's gonna be passed
868:04 - in between the operations,
868:07 - we are gonna focus on only this portion
868:10 - of this method right here.
868:14 - So this print is not gonna be taken into consideration
868:17 - for the second time and for any other time.
868:21 - So that said, we could run this for as many times
868:25 - as we wish and we wouldn't.
868:28 - Okay, let's take this back to the graph mode.
868:31 - So this is the eager mode,
868:33 - taking it back to the graph mode,
868:34 - you see it's gonna be printed only once.
868:37 - And then you could convert all the methods we're using
868:41 - into the eager mode by making use of this,
868:44 - config run functions eagerly function.
868:48 - So yeah, we have tf.config.run functions eagerly
868:53 - and then we set this to true.
868:55 - So we're saying that we're gonna run all these functions
868:57 - now eagerly and then let's copy this out.
869:00 - So let's copy, let's say we take this three
869:02 - and then paste right here.
869:04 - You see, once we run this now,
869:06 - you see it's printed out twice.
869:07 - This will tell you that even though we have this tf.function
869:11 - decorator right here, all those methods now are run eagerly.
869:17 - We could also set this to false.
869:18 - So yeah, let's take this.
869:20 - If we have this false, we run it and there we go.
869:25 - You notice that we have nothing printed out
869:27 - and this is simply because the tracing
869:29 - has already been done.
869:30 - The next point we wanna make is in the case
869:33 - where you're working with the graph mode,
869:35 - then there is a specialized print you could use.
869:38 - So yeah, instead of using the print,
869:41 - some sort of this print for the eager mode,
869:44 - the usual Python print, you could now use this print
869:47 - which is specially made for working in the graph mode.
869:51 - So here we have this print here
869:54 - and you see that the output of this tf.print
869:57 - is different from that of the print
869:58 - because now this tf.print makes this method
870:03 - look like a normal method as it goes through
870:06 - this print each and every time.
870:09 - Let's come in this and have this.
870:11 - So you see how this is printed.
870:13 - You see it's printed only once
870:14 - whereas if we're having the tf.print
870:16 - it's gonna be printed twice.
870:18 - Then from here, always ensure that if you turn
870:20 - away in the graph mode, make sure all your operations
870:25 - are tensor flow operations.
870:27 - So if you are able to resize, for example, with OpenCV,
870:32 - it's actually a good thing but you have to make sure
870:36 - that that resizing or you have to try as much as possible
870:40 - to have that resizing done with tensor flow operations.
870:45 - And so instead of CV2, the resize, for example,
870:48 - is preferable to use this image resize method.
870:52 - And then in order to keep everything in this function
870:56 - right here based on tensor flow,
870:59 - we should avoid passing in Python variables
871:02 - into this method right here.
871:05 - So you should make this method depend only
871:08 - on tensor flow variables.
871:10 - In conclusion, we've looked at the graph
871:13 - and eager mode with tensor flow.
871:16 - How to leave from the eager mode to graph mode
871:18 - and vice versa.
871:20 - Thank you for getting right up to this point
871:21 - and see you next time.
871:25 - Hello everyone and welcome to this new session
871:28 - in which we're gonna go under the wood
871:30 - and understand exactly what goes on
871:33 - when we call on this feed method right here.
871:36 - In fact, we're gonna be training our own model
871:38 - without making use of this feed method.
871:41 - So you're gonna have this custom training loop
871:44 - which we're gonna build.
871:46 - As you could see here, we have this training block.
871:48 - We have this validation block.
871:51 - And then we have this method which we named neural learn.
871:55 - And this method which replaces the feed method
871:57 - which we used to get in.
871:59 - TensorFlow is not only a great machine learning library
872:01 - because it permits us build
872:03 - and deploy machine learning models.
872:05 - Its greatness also comes from the fact
872:07 - that it permits us build these models
872:11 - and train them without getting to know everything
872:14 - which is going on under the wood.
872:16 - So all you need to do to train a TensorFlow model
872:19 - is to specify that model
872:22 - and then make use of this feed method
872:26 - while passing in your data set
872:28 - which is made of the inputs and the output.
872:32 - That said, in this section,
872:34 - we'll see how to create our own feed method.
872:37 - That is, we'll go a step further
872:40 - in understanding how TensorFlow works under the wood.
872:43 - Recall that when you make use of this feed method right here,
872:47 - what goes on essentially is TensorFlow
872:52 - applies the gradient descent method
872:54 - to update this weight data right here.
872:57 - So all the status have been updated
873:01 - until the model converges.
873:03 - Recall that for given inputs X1, X2
873:07 - and outputs Y1, Y2, Y3,
873:11 - our aim is to update this weights
873:14 - such that when we pass in this inputs into the model,
873:19 - we get an output right here
873:21 - which looks practically the same as this outputs.
873:26 - And to know whether this model outputs
873:30 - are the same or similar to this actual outputs,
873:34 - we apply or make use of a loss function
873:38 - which computes the difference between these two.
873:41 - And generally, initially,
873:43 - when you start training the model,
873:45 - that's when you start updating these parameters,
873:48 - there will be a great difference between these two.
873:50 - But as you keep on updating these parameters,
873:53 - we notice that the loss starts dropping.
873:56 - And so once this loss drops until it starts,
873:58 - it converges, we then stop the training.
874:03 - But recall also that the way we update this weights
874:08 - is such that we take the initial weight
874:12 - minus the learning rates,
874:14 - which is a constant we define
874:16 - times the partial derivative of this loss.
874:19 - Now the loss, which we've seen here,
874:21 - which we understand that is the difference
874:24 - between the model's predictions and the actual predictions.
874:28 - So here we have this partial derivative of the loss
874:31 - with respect to the parameter or the weight in question.
874:35 - And so if we want to update say this theta double prime
874:39 - to one, we'll simply use this same approach.
874:45 - Now the way tensorflow computes
874:47 - this partial derivative right here
874:50 - is by automatic differentiation.
874:53 - And so when you have this model with the inputs,
874:58 - as this inputs are being passed into the model
875:02 - in this forward pass right up to the output,
875:06 - tensorflow records this gradient
875:09 - or this partial derivatives.
875:12 - And this is done for each and every parameter
875:15 - such that once we get the output and we compute the loss,
875:18 - we could now get this whole partial derivative right here.
875:23 - That is the partial derivative of the loss
875:25 - with respect to each and every weight.
875:28 - And then each and every weight has been optimized
875:32 - or been updated via this formula.
875:36 - So in general, we can have theta, whatever.
875:39 - And then we have, let's say I G,
875:42 - we could have I comma J equal theta I comma J,
875:51 - minus linear rate times partial derivative
875:53 - of the loss with respect to theta I G.
875:56 - And so as we go through this forward pass,
875:59 - what tensorflow does is it keeps in mind
876:03 - the thetors and the gradients.
876:07 - Let's call this gradients the theta.
876:09 - So we have the theta and the theta
876:12 - which have been stored or which have been recorded
876:14 - as we go through this model that's in this forward pass.
876:19 - That process of recording this gradients here
876:25 - as we pass in our inputs in the model
876:28 - is similar to what we would have with a tape recorder
876:31 - where here you'll just need to speak in this mic,
876:35 - the information is recorded
876:36 - and then it could be replayed later on.
876:39 - And so here as information has been passed in this model,
876:43 - this gradients have been stored
876:46 - and they could be used in the gradient descent algorithm
876:51 - later on.
876:52 - Getting back to the code,
876:53 - we'll now see how to build a fit method from scratch.
877:00 - So here we have this, let's have for epoch in epochs.
877:05 - You see that now,
877:06 - unlike here where we just needed to pass
877:08 - in this number of epochs and TensorFlow does the job,
877:12 - here you really need to do much work on your own.
877:17 - Okay, so we have for epoch in epochs,
877:20 - what we're gonna do here is
877:22 - go through each and every batch of our data set.
877:25 - So we also have that for each epoch,
877:28 - we are now going to go through each and every batch.
877:30 - So for X, batch, Y, batch in train data set,
877:38 - let's have this tuple.
877:41 - So we have X batch, Y batch in train data set.
877:44 - What we're gonna do now is pass in this X batch
877:49 - into our model and then after getting the model output,
877:55 - compare that model output with actual output
877:58 - and obtain the loss. So here we have Y pred,
878:02 - which is what a model outputs.
878:03 - We have the model, here we've defined the model already.
878:07 - So we working with a sequential model
878:11 - we had built previously.
878:13 - Here we go, we have this sequential model.
878:16 - And then in this model, we're gonna pass our X batch.
878:20 - There we go.
878:21 - We specify that we're training.
878:23 - So in training mode, training equal true.
878:25 - Okay, so now we have Y pred and what we could do next
878:30 - is compute the loss.
878:31 - So here we have loss equal loss function,
878:36 - which we're gonna define loss function,
878:38 - which is gonna take in our Y pred
878:42 - and the models and the actual Y.
878:46 - So we can make use of this custom BCE right here.
878:49 - Now this takes in the Y true, Y pred.
878:52 - So we should ensure that we have Y true before Y pred.
878:56 - And here we have, we should have Y batch and then Y pred.
879:00 - Okay, so let's take this off and that's it.
879:04 - So here we're gonna use our custom BCE.
879:07 - We could take this off and then specify custom BCE.
879:12 - Now we have our custom BCE, we could go ahead
879:15 - and update this model's weights.
879:18 - But in order to update our model's weights,
879:21 - we need those gradients.
879:24 - The way we get those gradients is by recording
879:27 - or taping these gradients as we pass the input
879:31 - into the model.
879:32 - To do this, this part of our code has to be put
879:36 - in a particular scope.
879:37 - So here we have with gradient or tf.gradient,
879:42 - gradient tape as tape.
879:46 - We are gonna have this, let's have that.
879:49 - We're gonna have this two lines of code.
879:54 - Now the effect of doing this is we are recording
879:57 - the gradients in this tape or in this recorder.
880:01 - We could say this as recorder.
880:03 - And now all the intermediate values of tater
880:07 - and intermediate gradients have been recorded
880:10 - and now could be used in computing
880:12 - this partial derivative right here.
880:16 - That said, we can now obtain the partial derivatives.
880:19 - So we have partial derivatives equal the tape.
880:22 - So this, or rather equal the recorder
880:25 - because we changed that to recorder.
880:26 - So equal this recorder dot gradient of the loss
880:33 - with respect to all the model's trainable weights.
880:37 - And so this line of code here represents this operation
880:42 - that is computing this partial derivative of the loss
880:45 - with respect to each and every weight.
880:49 - Then we move on to optimize our model
880:53 - by going through this stochastic gradient descent
880:56 - or some other gradient descent based optimizer
881:00 - like the Adam optimizer which we've used so far.
881:04 - That said, we get back up to this compiler.
881:07 - We have this optimizer.
881:08 - Let's take this from here, copy that.
881:10 - And then what we'll do is we are gonna add a cell.
881:15 - Let's take this up.
881:16 - We have our optimizer here.
881:19 - So let's have this optimizer.
881:22 - That's it.
881:23 - Our Adam optimizer defined.
881:25 - We run that cell.
881:26 - Okay.
881:27 - And then what we do now is we have optimizer,
881:32 - optimizer dot apply gradients.
881:35 - So we use this apply gradients method
881:37 - in order to update the weights
881:41 - based on the gradient descent algorithm.
881:44 - In here, we have zip of gradients
881:48 - or rather the partial derivatives.
881:50 - So we are passing in the partial derivatives
881:53 - and the model trainable weights.
881:57 - So recall we have some untrainable weights actually.
881:59 - So we have this model trainable weights right here
882:03 - and the derivatives.
882:04 - Let's scroll.
882:06 - Okay.
882:07 - Let's have this here.
882:11 - This is linked.
882:14 - So this part is linked to this.
882:18 - And then for this, it's the whole algorithm,
882:22 - the whole gradient descent based algorithm
882:26 - which takes in this partial derivative
882:29 - and the model trainable weights, that's the Thetters.
882:34 - Obviously while defining the optimizer,
882:36 - we have already set the learning rate.
882:37 - So there we go.
882:39 - Then from here, we could print out our loss values.
882:42 - Here we could print out the loss.
882:45 - Print of the loss.
882:48 - Okay.
882:48 - So now we said we could now run this
882:51 - and see how this works.
882:53 - Let's not forget to specify the number of epochs.
882:56 - So let's have number of epochs equals say three, four stat.
883:00 - Let's have this epochs.
883:03 - And then we run the cell.
883:06 - We are told here that the model is not defined.
883:08 - So let's modify this
883:10 - and have a lunette model run that again.
883:16 - We get in this error where we told this expects the shape
883:20 - while what we get is this.
883:22 - Now the fact that we have this means
883:24 - that we haven't yet resized our training data.
883:26 - So let's get back up
883:27 - and then run this cells here for resizing.
883:32 - And there we go.
883:33 - Let's run the cells.
883:36 - Okay.
883:37 - We have that.
883:39 - And now it looks fine.
883:41 - Okay. So we have the cells run.
883:43 - Let's get back to our custom training loop.
883:47 - Okay. So let's run this.
883:50 - As we can see the training goes on smoothly.
883:53 - You have those loss values we should drop in
883:55 - and we run this actually for three epochs.
883:58 - So there we go.
883:58 - We have our loss, which is dropping.
884:01 - And then we could decide to print this out
884:04 - after a given number of steps.
884:06 - So here we could add this step
884:08 - and enemy rate, enemy rate.
884:11 - There we go.
884:12 - We could check out our free Python costs on neuralland.ai
884:16 - to understand this in case you're new to all those keywords.
884:21 - So there we go.
884:22 - We enemy rate.
884:23 - We have step.
884:24 - And then what we're going to do here is we're going to print
884:26 - our loss values only after a certain number of steps.
884:30 - So given that we have 689 steps,
884:33 - that's if our batch size is 32,
884:36 - we'll suppose that after say 300 steps,
884:39 - we're going to print this loss out.
884:41 - So here we're going to have,
884:42 - if the step modulo 300 equals zero,
884:47 - then we'll do the printing.
884:48 - So there we go.
884:49 - We have this print.
884:52 - And then for every epoch stat,
884:54 - we're going to print out
884:57 - training stars for epoch number.
884:59 - And then we format that.
885:00 - So here we have the epoch.
885:02 - Okay.
885:03 - Let's rerun this again.
885:05 - Here's what we get now.
885:07 - So we have this training stars,
885:09 - training stars, training stars.
885:11 - And then we have this intermediate loss values
885:13 - which have been printed out.
885:15 - Now, what if we include the validation?
885:17 - So here notice we finish like we train the model.
885:22 - And then after training for one epoch,
885:25 - we don't, we then get into validation.
885:27 - So for X batch,
885:30 - val,
885:32 - Y
885:33 - batch, val in val data set,
885:36 - we'll simply copy this out.
885:38 - So here we have this year.
885:41 - Let's paste it out here.
885:43 - Okay. So here we have the Y PRED
885:46 - and our loss vowel.
885:49 - It's still the same model,
885:50 - but your training is set to false.
885:52 - We have training false.
885:53 - We have Y PRED vowel.
885:56 - And then we have Y batch vowel.
885:59 - Here's X batch vowel too.
886:01 - And then we print out the loss vowel.
886:04 - So here we have validation loss.
886:09 - Okay. Let's run this cell now.
886:12 - While training is going on,
886:13 - we could include the metric.
886:15 - Here we have this metric,
886:18 - which is binary accuracy.
886:22 - Binary accuracy.
886:25 - Now while training is going on,
886:27 - we could deal with the metric.
886:29 - So here for this, for each batch we're working with,
886:35 - we are going to update the state.
886:37 - We are going to print out the result
886:39 - and then we are going to reset the states.
886:42 - So let's get it back to this part.
886:44 - And then just after this,
886:46 - that's for this particular batch,
886:48 - what we have here is we have our metric
886:51 - that update state,
886:53 - and then we pass in our Y, our Y batch.
886:58 - That's our Y batch, true Y, and then Y PRED.
887:03 - So once we update this metric,
887:04 - we can now print out this metric value
887:08 - out of this for loop.
887:10 - So after each epoch,
887:12 - we're going to print out the metric value.
887:15 - And so here we have print of metric dot result.
887:22 - There we go, print metric dot results.
887:25 - And then we should have here the accuracy is,
887:30 - there we go, accuracy is, that's it.
887:33 - Once we print out the result,
887:35 - we now go ahead and reset states.
887:37 - So we have metric dot reset states.
887:41 - And that should be good.
887:42 - So we reset the states and that's fine.
887:44 - We could repeat this exact same process for the validation.
887:47 - With the validation just in here in this batch,
887:52 - we are going to repeat the same step.
887:54 - So here we have the metric, let's pass it out here.
887:57 - And then here we're going to have metric vowel.
888:00 - Metric vowel, we update the states.
888:03 - You're going to use Y batch vowel and Y PRED vowel.
888:08 - Okay, once we update the states,
888:10 - we now do this same process here.
888:13 - So we print and then we reset the states.
888:16 - Let's have this year print and then reset states.
888:19 - Let's take that off.
888:21 - Okay, so we have here metric, vowel and then metric,
888:26 - vowel and that's it.
888:28 - Let's define metric and metric vowel.
888:31 - Here we go, we have metric and then we have metric vowel.
888:36 - So that's it.
888:37 - So here we see how to recreate this feed method
888:41 - that we have here.
888:42 - Now that we've seen this,
888:44 - we see the training is going out well.
888:46 - We see the training loss, the validation loss
888:50 - and that's it.
888:51 - But the training loss, we're getting years
888:53 - for each and every 300 batch.
888:58 - So let's take this off from here.
888:59 - Let's say we print this out after each epoch.
889:05 - So the training loss, we print it out after each epoch.
889:08 - And that's it.
889:10 - This means that we don't really need to have this again.
889:13 - Let's take that off.
889:14 - So we print out the training loss and the accuracy.
889:17 - This should be good.
889:18 - We now run this.
889:19 - And here's what we get.
889:21 - You see, we have the train loss.
889:23 - We have the train accuracy, validation accuracy.
889:25 - We've changed this to vowel accuracy.
889:28 - And then we have the validation loss.
889:30 - For epoch two, we have the same.
889:32 - For epoch three, we have this.
889:34 - For now, we're in the eager mode.
889:36 - So obviously this training process is gonna be slower
889:40 - as compared to if we're working in the graph mode.
889:42 - Now, what we're gonna do is we are gonna pick out
889:46 - those competition expensive units of this cell right here.
889:51 - Now, we know that this is competition expensive
889:55 - because here we have to pass in our input into the model,
889:58 - compare this output, compute the loss,
890:01 - get the gradients, optimize the model, update the metrics.
890:05 - So we look at this as one of those blocks.
890:09 - And then we have this other block.
890:11 - Let's scroll up.
890:14 - We have this.
890:15 - Let's take this off.
890:16 - So here we have this block.
890:19 - And then we also have this other block.
890:21 - Now, you'll notice that this is what we call
890:24 - the training step, and then this is the validation step.
890:27 - So what we could simply do here is create these two methods.
890:32 - So we have train.
890:34 - Let's call this training block.
890:38 - We have the training block.
890:40 - And then what goes on this training block is we take in a X.
890:43 - So you have X batch and then Y batch.
890:48 - Basically, we just copy all this.
890:50 - So here we copy this, let's cut that.
890:53 - And then we paste it right here.
890:56 - Okay, so here we have this training block.
890:58 - And then here, instead of having this,
891:01 - we just say for each step,
891:03 - we're gonna go pass through the training block.
891:06 - Pass through the training block.
891:07 - We pass in X batch and Y batch.
891:12 - Then we'll repeat the same process for the validation.
891:15 - Here we have this copied out.
891:18 - And let's copy this.
891:20 - Okay, so here we have the validation block,
891:23 - or let's say, wow block.
891:24 - Here we have the vowel block,
891:26 - which takes L batch vowel and Y batch vowel.
891:32 - There we go.
891:33 - We have the string block, validation block.
891:36 - Now, just here, we could paste out.
891:38 - Let's create our validation block.
891:41 - And then take X batch vowel,
891:45 - X batch vowel, Y batch vowel.
891:49 - There we go, we paste this out.
891:51 - So for this, we have that.
891:55 - Now, since we compute this loss in the training block,
892:00 - we wanna return this loss.
892:03 - So we return this loss.
892:05 - Okay, we have the loss returned.
892:08 - And then just right in here,
892:10 - we are gonna say loss equal this.
892:13 - So we call this loss,
892:14 - and now we could make use of it right here.
892:17 - And then we'll do same for the validation block.
892:19 - With the validation block, we take this here,
892:22 - and then we return our loss vowel.
892:25 - Now that sounds great.
892:26 - We should now be able to change this in our graph mode.
892:30 - So we have TF function.
892:32 - That's all it takes to convert this to graph mode,
892:35 - TF function.
892:38 - That's it.
892:39 - So there we go.
892:40 - We've converted this to graph mode.
892:41 - We can now run this.
892:42 - We're getting this error.
892:44 - Let's check on what's going on.
892:46 - Okay, let's run this now.
892:48 - We told on and then doesn't match
892:49 - any outer indentation level.
892:52 - Okay, yeah, we should put this to match up with this width.
892:56 - And that should be good.
892:58 - Let's make sure we don't have the same error here.
893:00 - So we have this four and that's it.
893:02 - Okay, so let's run again.
893:04 - And obviously we should have no problem.
893:07 - Now we could retrain our model
893:10 - and we should get faster training this time around.
893:13 - But still we're getting this error.
893:15 - Loss vowel must be defined before the loop.
893:18 - So we get back to the code.
893:19 - And what we could do is comment this section,
893:22 - which is for the training because this works already.
893:24 - So let's comment the section
893:26 - and then focus on the validation block.
893:28 - The first remark we have here is we've made an error
893:33 - of putting this for loop in this valve block.
893:36 - It's not actually a syntax error, but more of a design error.
893:39 - So it's better for us to have this valve block
893:43 - in this for loop instead.
893:45 - So that said, we're gonna cut out this from this valve block.
893:49 - And then what we'll have here now is for this.
893:53 - So we have for XY in the valve data set.
893:57 - You see, we're gonna pass this in here
893:59 - and then have this valve block.
894:01 - So here we have this valve block now.
894:03 - Everything looks fine.
894:05 - We should have that.
894:06 - And then let's now run the cell again.
894:08 - You now see how just recorrecting that error
894:11 - automatically solve that issue.
894:13 - And then we could go ahead and uncomment the section
894:17 - and we run our training.
894:19 - While the training is going on,
894:21 - we are gonna create this method.
894:23 - So this method, basically we're gonna copy from this.
894:27 - So here we should add the cell.
894:30 - We add the cell.
894:32 - And then we'll define this method,
894:34 - which we'll call train.
894:36 - Or let's call it neural learn.
894:38 - So we'll call this method neural learn.
894:40 - And our neural learn method,
894:42 - we are gonna take in a training data set.
894:44 - So we have train data set and then our validation data set.
894:47 - We could also get in the model.
894:49 - So we have the model.
894:50 - We could pass in the number of epochs.
894:53 - Let's have that year.
894:54 - So it looks like the model feed,
894:56 - which comes with tensorflow.
894:58 - So here we have the number of epochs.
895:00 - And then after the model, we have the last function,
895:03 - last function, metric, vowel metric.
895:07 - And that seems okay.
895:08 - So here we have that.
895:11 - We didn't have this call.
895:12 - So here we just have neural learn.
895:15 - And then we pass the model, the net model.
895:18 - We pass the last function.
895:20 - Last function.
895:21 - We pass the metric.
895:23 - We pass the vowel metric.
895:26 - Vowel metric.
895:28 - We pass the train data set.
895:30 - We pass the vowel data set.
895:32 - And then we pass the number of epochs.
895:35 - So all we need to do now is just run this
895:37 - and then our training process is gonna start.
895:39 - Getting back to the training year,
895:41 - you see how this training loss,
895:43 - you see we have the training loss.
895:44 - We have the accuracy.
895:46 - We have the validation loss
895:47 - and we have the validation accuracy.
895:51 - Let's stop this.
895:52 - And then we could even take off the cell.
895:55 - Let's take off the cell.
895:57 - Okay.
895:58 - And then we have that cell off.
896:00 - Let's delete that cell.
896:02 - And then focus on this.
896:03 - So now we have our neural learn,
896:05 - which takes in the model.
896:07 - So here, as you could see, we have this optimizer.
896:10 - We have the metric.
896:11 - Metric vowel, epochs.
896:13 - Let's get back to the optimize.
896:15 - Let's add the optimizer here.
896:16 - So we should have after this last function metric
896:20 - and then let's have optimizer.
896:23 - Optimizer.
896:24 - There we go.
896:25 - Now after the metric year, we have our optimizer.
896:30 - It was fine.
896:31 - Let's run this.
896:32 - And this loss function not defined.
896:35 - So let's scroll up and we have this custom BCE.
896:39 - So let's just put out the custom BCE right there.
896:42 - Custom, custom BCE.
896:45 - We run that.
896:46 - Training is now complete and you're the results we get.
896:49 - You now know how to create your own custom training loops
896:53 - and make them run in graph mode.
896:56 - Thank you for getting around to this point
896:58 - and see you next time.
897:03 - Hello everyone and welcome to this other amazing session
897:06 - in which we are going to see how to integrate
897:08 - TensorBot callbacks with TensorFlow.
897:12 - In this session, we are going to look at
897:13 - how to log in information from our training process
897:18 - or from our different experiments into TensorBot,
897:21 - how to view model graphs,
897:24 - how to do hyperparameter tuning with TensorBot,
897:27 - how to view distributions, histograms, time series,
897:32 - how to log image data like confusion matrices,
897:36 - RSC plots, and finally how to do profiling with TensorBot.
897:42 - In one of our previous sessions,
897:43 - we saw the importance of working with callbacks
897:46 - as they permitted us to modify certain key information
897:51 - during the training process
897:52 - and also store certain information
897:56 - and do certain modifications during the training.
897:58 - That said, we have this TensorBot callback right here,
898:02 - which we spoke of last time but didn't really get into.
898:06 - And so in this session, we are going to go in depth
898:08 - and see how to make use of TensorBot
898:11 - to visualize on a web interface vital training information.
898:16 - You'll notice also that this TensorBot callback
898:20 - is going to be used in a similar way to the way we had done
898:23 - with the previous callbacks.
898:24 - So yeah, we define the callback.
898:26 - And then just here, you see in this callbacks argument,
898:31 - we pass in the TensorBot callback right here.
898:34 - So let's go ahead and copy this out.
898:36 - We have this, we just copy from here.
898:38 - We've copied this clip bot.
898:40 - And then now we go ahead and see how to pass this
898:44 - in our training process so we see exactly how it works.
898:49 - So yeah, we open up this callbacks.
898:52 - You see the different callbacks we have created previously.
898:54 - And then now we're going to include our TensorBot callback.
898:58 - So just yeah, let's add this text and add a code cell.
899:02 - We paste this out.
899:04 - Then let's take all this off and simply go,
899:08 - let's run this cell, that's fine.
899:10 - And now let's move on to our training process.
899:14 - Here we're going to have the callback.
899:16 - So we have the callback equal TensorBot callback.
899:20 - And then we're ready to train our model.
899:25 - So let's check on this.
899:27 - We are getting an error,
899:28 - unexpected keyword argument callback.
899:31 - Okay, you should have callbacks.
899:33 - We'll run that again.
899:36 - Now while our model is training,
899:37 - you'll notice that there is this logs folder
899:39 - which has been created.
899:41 - And the reason why the logs folder has been created
899:43 - was because we actually specified that
899:45 - in this TensorBot callback call.
899:48 - So here we have this log directory argument.
899:52 - And we're going to store in some information
899:55 - or some training information.
899:56 - And this is training information that TensorBot will use
900:00 - to display very important training information
900:03 - on a web interface.
900:04 - Now you could click open here
900:06 - and you'll see that you have these two folders.
900:08 - In this train folder, you see you have this information
900:11 - for content in the training data,
900:13 - which has been stored here
900:14 - and information for the validation,
900:16 - which has been stored right here.
900:18 - The next thing we'll do is copy out this here,
900:21 - this command, and then run it just below.
900:25 - So let's get to where we have visualizations,
900:28 - like reduce this one.
900:30 - And we have the visualizations here.
900:32 - Now we're going to have this.
900:33 - We're going to see how we're going to replace
900:35 - these visualizations with our TensorBot visualizations.
900:39 - So it passes out.
900:39 - We have TensorBot, log, dire, path to logs.
900:44 - Then we created this log dire variable,
900:47 - here, which takes in our path.
900:49 - So let's get back.
900:51 - And then instead of putting our path directly,
900:53 - we just have actually this log dire.
900:55 - So we have the log dire.
900:58 - Now before running this,
900:59 - we are going to add this code cell
901:02 - and then load this TensorBot notebook extension.
901:07 - So here we have load extension TensorBot.
901:13 - We run this and then you see already loaded.
901:16 - We are loaded already.
901:18 - Anyway, we have that and then TensorBot now.
901:21 - So we run this and then we should expect to have
901:24 - some interface which contains all our log data.
901:27 - No dashboard active for the current data set.
901:31 - Check out on scalars, what do we have?
901:33 - Anyway, let's do this.
901:35 - Let's check on this.
901:36 - Let's have logs.
901:38 - Logs, run this again.
901:41 - Okay, and there we go.
901:43 - We see now that we have this interface,
901:46 - which pops up or what do we see here?
901:49 - We have this logs.
901:51 - We have both training validation.
901:53 - You could pick this out as you would pick
901:56 - only the training data.
901:57 - You could see only training data.
901:59 - And then here you have the scalars.
902:01 - The scalars basically stores all this information
902:05 - which we pass in here,
902:06 - like the loss and the metrics.
902:08 - Call that we had defined this metrics here.
902:10 - So we are going to get all this metrics information.
902:13 - So unlike previously where we had to manually do this step
902:17 - and do this step for the loss and the accuracy,
902:20 - now this is done automatically.
902:22 - So let's run this and then we will compare
902:24 - what we get here with what we get from TensorBot.
902:27 - You can see here we have this loss
902:29 - and then we get back to TensorBot.
902:31 - We have this level of scalars.
902:35 - Let's reduce this accuracy.
902:37 - Let's view the loss first.
902:38 - So let's reduce this.
902:39 - Okay, we check out, we have this epoch loss.
902:42 - Now this is what we get for the loss.
902:44 - Let's include the validation.
902:46 - So you see, you have this plot right here.
902:51 - Click on this, you could also expand.
902:53 - So you see, this is the plot we get.
902:55 - Now, what do you notice?
902:56 - You notice that it's exactly the same as what we had here.
903:00 - But this time around, we didn't have to write any code.
903:02 - All this information was automatically locked
903:05 - in this file here and TensorBot took care of the rest.
903:09 - So that's how this works, it's really very interesting.
903:14 - And it's a kind of tool we want to master how to use
903:16 - because when working on different machine learning
903:20 - experiments, you wouldn't want to always have to lock
903:23 - all these values by hand or manually,
903:26 - you want to have this done automatically.
903:28 - Now, one of the interesting point is,
903:30 - you have all the metrics here.
903:33 - You just have to select any one.
903:34 - So let's look at the accuracy which we've seen already.
903:37 - We can check out this.
903:39 - Let's click here, okay.
903:40 - So we see this accuracy and then let's scroll down.
903:44 - You see, you could compare it with what we have here.
903:47 - See, we have this zip around the,
903:49 - this should be the ninth epoch or let's say eight epoch.
903:52 - Anyway, let's come back and check here.
903:55 - Let's scroll up from here.
903:57 - So actually an interface in this other interface.
904:00 - So we have this, you see, at this ninth epoch,
904:03 - what do you see here?
904:04 - You see, notice on this here, you will have this name,
904:07 - trains, modad, value, what do we have,
904:12 - step, ninth step, the time, and that's it.
904:16 - Okay, so there we go.
904:19 - We see how we could plot all this automatically
904:22 - and then you could get at any point.
904:25 - So you could go through each and every point
904:27 - and then get all the exact values.
904:30 - So that's how we look at this.
904:32 - Let's reduce this.
904:32 - We could take now, let's say a precision.
904:35 - So you could see, monitor the precision.
904:39 - You could also check out the number of false negatives.
904:41 - You see how this, as you keep on training,
904:43 - the number of false negatives keep reducing.
904:45 - And then the false positives, what do we have here?
904:48 - False positives, that's loading.
904:52 - Why that's loading, let's scroll down
904:55 - and we have the loss.
904:57 - We've seen this loss already.
904:59 - We have, let's look at true negatives.
905:02 - That's loading true positives.
905:05 - Here's a plot we get for the true positives
905:07 - and the true negative.
905:09 - And in the section where we have this evaluation,
905:11 - evaluation, evaluations, practically the validation metric
905:16 - and loss that we're plotting
905:17 - against the number of iterations.
905:19 - So that's what we have here.
905:20 - It's just like the validation accuracy versus iterations.
905:23 - If you take off the train, you see nothing really changes
905:25 - here, but when you do this, you see all that goes.
905:28 - So that's our validation and we could monitor this
905:31 - and observe that the highest accuracy we have is 94.09%.
905:36 - No, it's 94.21%.
905:38 - Scroll down, we have precision.
905:41 - What's our highest precision here?
905:42 - It's 93.6, no, the value is 94.39%.
905:48 - As of now, we've been able to log this information
905:52 - just from compiling our model.
905:54 - So because we passed our loss and the different metrics,
905:57 - we're able to log the information
905:59 - and visualize it on tensile board.
906:01 - But there are other possibilities.
906:03 - That is, it's also possible for us
906:06 - to log information manually
906:08 - instead of just logging all of this information.
906:11 - So what we could do is log, for example, image data.
906:16 - We could log even this different learning rates here.
906:20 - So we are gonna log or we can log the learning rate values
906:26 - for each and every epoch.
906:28 - And this gives us that freedom to log in
906:31 - just any kind of scalar or quantity we want.
906:34 - So here we have this metric.
906:37 - We're gonna create a metric directory.
906:41 - This is gonna be logs and then here we have metrics.
906:45 - And what we're gonna do now is create this train writer
906:50 - because now we are doing this like manually.
906:53 - So we create our train writer.
906:55 - We have tf.summary.createFileWriter.
907:01 - And then in here we have our metric directory.
907:04 - Okay, so that's good.
907:05 - We have our train writer created
907:08 - based on this metric directory,
907:10 - which is gonna be in the logs.
907:12 - So in those logs, we're gonna create a metrics directory.
907:16 - So that's it.
907:17 - We have our train writer, we could run the cell.
907:20 - Then just in here we have width.
907:24 - So once we're done with this,
907:27 - with train writer as default,
907:34 - with train writer as default,
907:36 - we wanna have this logged.
907:39 - So we have tf.summary.scalar
907:42 - and then we specify that we're dealing with the learning rate.
907:46 - So our learning rate.
907:48 - And then we're gonna pass in the learning rate actually.
907:51 - So here we pass in the learning rate
907:52 - and then we pass in the epoch.
907:54 - So this is like, as you could see here, you have data.
907:57 - Look at this popup.
907:59 - You see here you have data
908:01 - and then here you have steps, or rather step.
908:03 - So you have the name of the scalar,
908:06 - you have the data and you have the step.
908:07 - You also have the description.
908:09 - So here we have the name, which is learning rate.
908:14 - We have the data, which is this learning rate.
908:17 - And then we have the step, which is the epoch.
908:21 - Now we'll have to do this, like with this,
908:25 - we have to put it in each and every one of this
908:28 - since either we get into this or get into that.
908:31 - But to avoid writing this twice,
908:33 - we just have to set a learning rate.
908:35 - So in here, we define this learning rate, learning rate.
908:42 - And then here too, we have our learning rate.
908:46 - There we go, we have the learning rate.
908:48 - And then out of this, we return our learning rate.
908:54 - Okay, so that's it.
908:55 - Now we return the learning rate
908:56 - and then we are also logging this data.
908:59 - So here we should change this and have learning rate.
909:03 - So that's it.
909:04 - Now we have the set and then we could go on and train.
909:07 - Before training, recall each time you want to log
909:10 - in this kind of data or this kind of custom data.
909:14 - First thing you do, you create your writer
909:17 - as you create this file writer.
909:18 - After creating the file writer
909:20 - based on a given directory that you set,
909:23 - you now go ahead and then put this
909:25 - in this train writer scope right here.
909:30 - So with this, we can run this now.
909:33 - We've run this already, we can run this now.
909:35 - And then since this is our shadow log callback,
909:38 - we'll have to add this in our feet.
909:41 - So let's go ahead and add it in this our feet method here.
909:44 - Let's take now just five epochs.
909:46 - So yeah, we have tensor bot callback
909:49 - and then we have scheduler callback.
909:53 - Okay, so we run this now and see what we get.
909:56 - That's training.
909:58 - Our training is now complete.
910:00 - Let's go ahead and check this here.
910:02 - We have our logs.
910:03 - You see, we have this metrics and we have this locked.
910:07 - Next step, we go to visualize.
910:09 - So here we have this tensor bot and then here let's do,
910:14 - anyway, let's run this first.
910:16 - So you'll see what we get.
910:17 - As you can see right here, we now have this learning rate
910:20 - which has been logged and which we can visualize.
910:22 - So you see, as we go from this epoch to this epoch
910:28 - to this, this, if we set this at zero smoothing in,
910:32 - we have this.
910:33 - Now, the reason why we, after this,
910:35 - we don't get any value is because of what we have here.
910:39 - So we sent the learning rate into this tensor
910:41 - and what we should be doing here is getting that NumPy.
910:45 - So we should have this learning rate right here,
910:49 - learning rate equal the learning rate
910:53 - because this is going to be converted into a tensor.
910:55 - So we have learning rate at NumPy and that should be cool.
910:58 - Okay, we run this again and that should work.
911:01 - And then to view or to have immediate response,
911:03 - let's set this to just one.
911:05 - So if the number of epochs is greater than one,
911:09 - then we would have the learning rate being modified.
911:11 - So let's get back again to training.
911:15 - This time around, let's see, we just have three epochs.
911:18 - Okay, this time around, we have this actual value locked.
911:23 - And so we get back, let's reduce this custom training loop.
911:27 - We get back to our visualizations.
911:29 - We will run this again.
911:31 - Getting back here, we see we start with this loss
911:35 - and then this drops after this second epoch right here.
911:41 - As you may have noticed,
911:42 - each and every time we run new training process,
911:46 - the previous values or the previous locked input data
911:51 - is being deleted.
911:52 - So what we could do now is we could modify this file name
911:55 - as this logs file we're using here,
911:58 - where this folder name now depends on the current time.
912:03 - So yeah, we're going to have date time,
912:06 - date time dot date time dot now.
912:13 - And then we get a string, string from the time
912:20 - and then format this output.
912:21 - So yeah, we're going to have percentage,
912:24 - the day percentage, the month, the year
912:30 - and then we specify the exact time.
912:32 - So you're going to have the minute and the second.
912:38 - Now let's go ahead and import the time up here.
912:41 - We have the time.
912:43 - So yeah, we import the time simply that should be imported.
912:49 - Okay, so we run that and that should be fine.
912:51 - Okay, we've imported the time.
912:53 - Let's get back.
912:55 - And then let's print out this log diet right here.
912:58 - Let's have log diet printed out.
913:00 - See what we get.
913:01 - See, we have this logs and this is actually on your folder.
913:05 - Now, if you print out this again,
913:08 - you see we're going to have a different folder.
913:11 - And this is important because each and every time
913:13 - we do not need to cancel our previous runs.
913:18 - From here, let's take this off
913:21 - and then set this as our current time.
913:26 - So current time, current time
913:31 - these equal this, take this off.
913:34 - Here's our current time.
913:35 - And then we have this plus current time.
913:43 - Okay, and then yeah, we'll do the same.
913:45 - We have this plus current time.
913:47 - Let's take this off and then have the current time here.
913:52 - Current time.
913:55 - If we get back to the straining, we can run this again.
913:58 - If we're now complete, as you could see here in this logs,
914:01 - we have this new folder created,
914:04 - which is dependent on the time in which we decided
914:08 - to do the training.
914:10 - And then in this metrics, we also have this.
914:12 - But what we want to have is actually just this one photo
914:15 - which contains a train validation and the metrics.
914:17 - So we should modify this right up here.
914:22 - So instead of having this metrics before,
914:25 - we should take this off.
914:26 - We should take this off and then add it later on.
914:30 - We have plus and then we add this.
914:34 - Okay, so we have this slash and then slash.
914:37 - Then we should take this now.
914:39 - So that's it.
914:40 - We have recreated this and then we will rerun again
914:44 - to avoid this kind of error.
914:47 - So let's go ahead and retrain our model.
914:50 - Let's have it to be, yeah, it's fine.
914:54 - Let's say two epochs and then we'll run that again.
914:57 - Okay, the training is complete.
914:59 - Now you'll see that if you open this up,
915:01 - you have train validation.
915:02 - This is what we had previously,
915:03 - but now we have metrics, train, validation.
915:06 - This is exactly what we want.
915:07 - We want to be able to lock all this into this one directory.
915:11 - And you'll now notice how we do not have
915:14 - to erase previous logs.
915:17 - So let's go down to running this again.
915:21 - We run this.
915:22 - Take this off.
915:25 - Okay, here we go.
915:26 - We have this information now locked.
915:29 - You see, we could take all this previous logs out
915:32 - and focus on just this log here.
915:35 - So let's have this one.
915:37 - This should be 0.5, same, n3.7.1.
915:41 - So we want to focus on this one, which n3.7.1.
915:46 - And okay, here we go.
915:48 - So we have this metrics, this train and this validation.
915:50 - For this, we are not interested in logging this.
915:53 - So that's it.
915:54 - Let's take this off now and then get back.
915:56 - So you see here, we have the learning rate.
915:58 - We have the epoch accuracy.
916:00 - We have this true negatives.
916:04 - We have the recall and that's it.
916:06 - So here we'll see now how to create this directories,
916:10 - which are dependent on the current date and time.
916:14 - Now the next step we'll be doing is
916:16 - how to actually do this logs we doubt
916:19 - or when we're doing a custom training.
916:22 - So let's get back to where we did this custom training loop.
916:25 - We had this custom training loop
916:26 - and then we have this feed method,
916:29 - which comes directly with tensorflow.
916:32 - So what if we now try to use,
916:35 - or what if we actually use the custom training loop
916:37 - and we do not have the possibility
916:39 - of just simply saying, okay, callbacks,
916:43 - tensorback, callback, and then the job is done.
916:45 - And what if we just have this custom training loop?
916:50 - In this case, we're gonna use exactly the same process
916:53 - we've just followed here.
916:56 - So we're just gonna create this file writer.
917:00 - So let's copy all this.
917:02 - And then just as we did here, actually,
917:06 - we're just gonna write in this scaler values
917:10 - and then create the scaler,
917:12 - put in the data and then specify the step.
917:14 - So that's basically how we're gonna function.
917:16 - Now let's get back to this custom training loop.
917:19 - We're gonna add this code cell.
917:21 - And then in here, we have current time as usual.
917:24 - We have now, let's call this custom directory.
917:28 - We have logs, current time, and then we have custom.
917:32 - It's called this custom train writer.
917:35 - Let's call this custom train writer.
917:38 - Custom train writer.
917:40 - And you can also define a custom validation writer.
917:43 - We could have that too.
917:44 - So let's have here custom,
917:49 - custom directory.
917:50 - And in here, we will specify also train.
917:54 - So notice that since you were using this feat,
917:57 - what we got was the immediate automatically,
918:00 - we automatically got this train and validation.
918:03 - So what happens in the background is
918:05 - these two file writers are created.
918:08 - That is the train and the validation.
918:11 - And we're just gonna do exactly that here.
918:13 - We have custom.
918:14 - And then let's say we have custom train directory.
918:19 - Custom train and then custom validation.
918:23 - Okay, so we have that.
918:24 - And then here we have custom validation.
918:26 - Now we specify our writer.
918:28 - We have custom train and custom validation.
918:32 - Then here we have custom train.
918:36 - Custom train, custom validation.
918:39 - Okay, so I think this is okay.
918:42 - We could now run this.
918:44 - And then let's copy out this code we had put out here
918:49 - in the section and the shadow.
918:52 - So let's simply copy this out.
918:54 - You see how easy it becomes when you have already done this.
918:58 - So yeah, you now have to say,
919:01 - instead of just only printing this out,
919:04 - you let's have this.
919:05 - So with our, let's get a name from here
919:09 - with our custom train writer.
919:11 - And then here we have with our custom train writer.
919:15 - Custom train writer.
919:18 - We have the loss.
919:20 - So we have the loss.
919:21 - Let's call it train loss.
919:23 - We have the training loss.
919:25 - We have the data.
919:26 - The data is now this loss here.
919:28 - It's this loss.
919:29 - So we have the data which is passed,
919:31 - which is now the loss.
919:33 - And then the step is the epoch we have here.
919:36 - So that's it.
919:37 - Okay, we've logged this.
919:38 - Let's now go ahead and log for the accuracy.
919:43 - So we'll paste this out.
919:44 - And then we have training accuracy.
919:48 - Accuracy.
919:50 - And then the accuracy.
919:51 - So we kind of paste out.
919:53 - We will see metric the results.
919:58 - Metric the results.
919:59 - Okay.
920:00 - Metric the results.
920:01 - And then we have the step specified.
920:03 - So this is for the training process.
920:05 - We could separate this block and that's it.
920:09 - Okay, so we've done this.
920:10 - We now simply copy this out
920:12 - and then do the same for the validation.
920:15 - So in here, instead of writing this out like this,
920:18 - we could simply put out your custom val.
920:24 - We have custom val.
920:25 - Take this off.
920:27 - Here we have validation.
920:29 - Validation loss.
920:31 - Here we have the loss,
920:33 - but this is loss of val.
920:35 - Loss of val.
920:36 - That's fine.
920:37 - We have metric val.
920:39 - Metric val.
920:41 - That's fine.
920:42 - We have validation accuracy.
920:45 - Validation.
920:47 - Validation accuracy.
920:49 - Now you have this too.
920:51 - Take this off.
920:52 - And then we have the val.
920:54 - Okay, so that sounds fine.
920:56 - Everything looks okay.
920:58 - We could run this here.
921:00 - So let's run this.
921:01 - We run this.
921:03 - And then we run neural learn.
921:05 - And then we start with a training.
921:10 - Training now complete.
921:11 - Let's go ahead and see what we have.
921:13 - You could check out this logs.
921:15 - You see, we have our values now locked in here.
921:17 - Custom, train and validation.
921:20 - So we have this locked.
921:22 - We now go ahead and rerun this tensor board.
921:26 - As you could see, you have all these values here.
921:29 - You could, as usual, let's take this off, toggle our runs.
921:35 - And then let's pick this very last one.
921:38 - So we pick this last one and also pick out this one
921:43 - because there's a train and this is the validation.
921:46 - Then we come right here and check out the training accuracy.
921:51 - Train loss.
921:53 - There we go.
921:53 - We have train accuracy.
921:55 - We have train loss.
921:57 - If we do this, you see, we have, okay,
922:02 - we have the train accuracy.
922:03 - We have the train loss.
922:04 - We have the validation accuracy
922:06 - and we have the validation loss.
922:09 - Now, if we want to take off all the information
922:12 - stored in the logs, we could have this command.
922:15 - So we remove all this information
922:17 - and that will specify the logs.
922:19 - So we run this and then open it up this, you see,
922:23 - you don't have the logs folder anymore.
922:25 - At this point, we'll go ahead and see how to display
922:27 - image data with tensor bot.
922:30 - So unlike previously where we've been displaying
922:33 - information like the loss, the different metrics,
922:36 - now we'll see how to implement or rather,
922:39 - we're going to display image data
922:42 - like the confusion metrics we had seen previously.
922:45 - Let's get back here.
922:47 - We have this confusion metrics right here.
922:49 - And what we'll do now is after each epoch,
922:52 - we are going to display this confusion metrics
922:56 - with tensor bot.
922:57 - That said, we're going to copy out all this code
923:00 - we used in displaying this confusion metrics right here.
923:03 - So we have this code.
923:05 - And then we have this log images callback right here
923:08 - with this on epoch end method.
923:11 - Then in this method, we are going to paste out this code
923:15 - we used in visualizing the confusion metrics previously.
923:19 - So here we have this level input right up to this.
923:23 - We have the confusion metrics based on the threshold.
923:27 - And then we are going to visualize this confusion metrics.
923:30 - But now since we're working with a callback,
923:33 - what we will do is at the end of each and every epoch,
923:37 - we are going to display this with tensor bot.
923:41 - Now that this is set, we are going to,
923:44 - for now we've actually just been able to visualize this,
923:48 - but how do we put this,
923:50 - or how do we make this work with tensor bot?
923:54 - What we're going to have here is we create a buffer.
923:57 - We have this buffer IO dot bytes IO.
924:01 - Here we have bytes IO.
924:04 - And then, so that's our buffer.
924:06 - We are going to save this image,
924:08 - the confusion metrics image in this buffer.
924:11 - So we have this PLT dot save fig,
924:16 - and then we save it in that buffer.
924:19 - And that will specify that the format should be PNG.
924:26 - So we have the PNG format and that's okay.
924:29 - So now we have this buffer.
924:31 - We've saved that into our buffer.
924:35 - The next step we'll take is create an image of this buffer.
924:40 - So from here, we have this image.
924:42 - Let's take this up.
924:43 - We have this image.
924:45 - We use the tensorflow image decode PNG method,
924:51 - which takes in this buffer.
924:53 - So we have our buffer get value,
924:55 - number of channels equal to three.
924:58 - That's it.
924:58 - We have this image.
925:00 - And then once we get this image,
925:02 - we then write this in tensor bot.
925:05 - So we have this image writer,
925:07 - which we've created right here,
925:08 - similar to what we've done already.
925:10 - We create, we use this create file writer.
925:12 - Let's modify this and have your image directory.
925:17 - So we have the image directory,
925:20 - and then we create this file writer,
925:23 - or rather we create this image writer.
925:25 - So from with this image writer's default,
925:28 - what we're going to do now is,
925:29 - instead of having this summary dot scaler
925:34 - as we used to have here,
925:36 - now we're going to use summary dot image.
925:38 - So you could see here that tensor bot permits us
925:41 - not only input or write scalers,
925:46 - but also images.
925:48 - And that's basically all we needed to do here.
925:50 - So let's have this and then run the cell.
925:54 - So we make sure we have this run, run this, we run this.
925:59 - And then we, okay, we have this log images call back here.
926:04 - Copy that.
926:05 - Now that's copied, we have to reduce this one.
926:10 - And then right here, we run this metrics
926:14 - and then compile and run this.
926:17 - We get this arrow where we said,
926:20 - where we have this value arrow, no step set.
926:23 - So let's get back to this call back.
926:27 - And then we specify the step.
926:30 - So right here, we have the step, step equal the epoch.
926:35 - So we run that again, and this should be fine.
926:38 - Training is going on and then the image data
926:42 - has been logged into tensor bot at the end of each epoch.
926:46 - Training now done, we could go ahead
926:48 - and then run this on tensor bot.
926:52 - Once training done, we could now visualize
926:55 - this confusion matrices on tensor bot.
926:57 - So we run this to cells and here's what we get.
927:02 - You'd see here step zero, step one, and then step two.
927:06 - This is because we actually run this for three epochs.
927:08 - So we could notice how, let's come back to the top.
927:12 - We notice how here we have 53.
927:17 - And then as we keep training, these drops to 35.
927:20 - And then finally here, let's go down a little.
927:24 - Finally here, we have 240.
927:29 - So this tells us that the last epoch
927:32 - wasn't helpful in improving the number of false negatives.
927:36 - You could see also even with the validation
927:38 - that here we had 16 false negatives, eight false negatives.
927:43 - And then this value rose up to 111 false negatives.
927:49 - So it's kind of similar to what we have with the test data,
927:55 - which is exactly what we're logging in the tensor bot.
927:59 - And now that you know how to log in image data
928:02 - with tensor bot from the example on the confusion metrics,
928:05 - what you could do is log in directly this ROC plots.
928:10 - You could also log in data like this one right here,
928:14 - where on the test data, you're going to put out
928:18 - the actual value and what the model predicts.
928:22 - And so that's it for the session on logging in image data.
928:25 - Now let's move on to visualizing model graphs
928:29 - with tensor bot.
928:30 - To visualize a graph, we're going to rerun this command
928:33 - to delete all the logs we've stored so far.
928:37 - And then we run this chance about callback once more.
928:41 - So we have that and then let's get back to metrics.
928:44 - We run this and that's fine.
928:47 - Now we have the training done.
928:49 - Let's go ahead and rerun this again.
928:52 - So we run this two cells again.
928:54 - And as expected, here's what we get.
928:57 - So yeah, we have the skillers.
929:01 - And then if you click on this graphs here,
929:04 - or to have this graph coming up, let's reduce this slightly.
929:09 - Okay, so we have this, notice that the graph tab
929:12 - here is up graph.
929:14 - And this actually means we're viewing the graph
929:16 - from the operations level.
929:19 - If you could come right here and zoom in,
929:22 - you see you have the conf layer.
929:24 - You have the conf layer again, dense layer, dense layer,
929:29 - and then zoom in this other way.
929:32 - Here you see you have the Adams optimizer.
929:34 - Let's double click here.
929:35 - Where you have this plus you double click.
929:37 - And then you get to see exactly what goes on
929:40 - in this Adam optimizer.
929:43 - Let's reduce that, zoom in,
929:46 - and then let's double click again to reduce this.
929:50 - Okay, so here you have that.
929:52 - And for this dense layer, you could double click.
929:55 - You see, you have that.
929:57 - You see, you have this kennel, you double click
930:00 - to better understand what goes on.
930:02 - And as you can see here, we have this regularizer.
930:05 - So it's a regularizer we had defined previously.
930:08 - Now let's reduce this by double clicking.
930:11 - Let's scroll here.
930:12 - We double click that, reduced, and then double click this.
930:17 - So basically it's hands to hands are barred.
930:19 - We able to visualize exactly what goes on
930:22 - under the hood when tensorflow creates this graphs,
930:27 - which in turn permeate us do computations even faster.
930:31 - Now, another way we could look at this
930:33 - is by coming right here, the stack and selecting Keras.
930:36 - Once we select this Keras,
930:38 - instead of having the operation graph we had just seen,
930:41 - we now have this conceptual graph.
930:43 - We'll see that this is going to be quite easy
930:46 - compared to our easy to understand
930:48 - compared to what we had done previously.
930:50 - So yeah, this focuses on the Keras model.
930:53 - We had built a Keras sequential model, we had specified.
930:57 - And here what we have is, you see the input.
931:00 - So unlike previously where we had stuff
931:02 - like the Adam Optimizer, the different metrics
931:04 - and the say loss computations with the up graph,
931:09 - here we have just the Keras model.
931:12 - So here you have the input com batch normalization,
931:16 - max pooling, drop out com batch normalization,
931:20 - max pooling, flatten, dense, batch norm,
931:23 - drop out dense, batch norm and finally dense.
931:25 - So that's our conceptual graph.
931:30 - And that's it for the section on graphs.
931:31 - You see, you get to understand exactly what goes on
931:35 - under the hood, thanks to this visualization
931:41 - or better to this graphs visualization
931:43 - made available with tensor board.
931:49 - Now, as we go ahead with building these models
931:52 - and then training them, you may sometimes wonder
931:56 - why this value six, for example, was picked,
931:59 - why the kennel size three was picked,
932:03 - maybe why the 16 was picked year,
932:05 - why 100 was picked, why not say 32,
932:08 - why not say a value like 100 year,
932:11 - why others values were picked.
932:13 - And then looking at the drop out rate,
932:15 - what makes us pick a given drop out rate,
932:18 - what makes us pick a given regularization rate
932:20 - and so on and so forth.
932:23 - Now, although in this particular case,
932:25 - we're building this model based on the Lynette model,
932:28 - which is some like a model,
932:30 - which has already been built and tested,
932:33 - we'll see another technique known as hyperparameter tuning,
932:37 - where we'll be able to select the best values
932:41 - for these different hyperparameters automatically.
932:44 - Now, this hyperparameters will not be only this,
932:47 - which we've picked out here.
932:49 - We could also have hyperparameters like the learning rate,
932:53 - like even the choice of this optimizer
932:57 - and so on and so forth.
932:58 - And this way of deciding this best parameters
933:02 - for our model and model training
933:06 - is known as hyperparameter tuning.
933:09 - Now, we're going to see how to do
933:11 - or carry out hyperparameter tuning with TensorBoard.
933:14 - So here, first in first, we carry out those imports.
933:18 - We have from TensorBoard plugins, HParams, hyperparameters.
933:22 - We're going to import API as HP.
933:26 - Then right here, we're going to redefine this model
933:29 - or we're going to restructure it.
933:31 - So instead of having just the regularization rate
933:36 - drop our rate given to us like this,
933:39 - what we're going to have here is here, for example,
933:42 - instead of having this drop our rate, we'll have HParams.
933:46 - And then we'll have HP drop out.
933:52 - And the aim of this process is to ensure that
933:55 - if we have this, suppose we have this model here,
933:59 - and then after training, we have an output accuracy,
934:03 - we want to be able to modify the parameters
934:06 - which make up this model
934:08 - and see how they affect the accuracy
934:10 - such that we are going to pick out the best parameters
934:14 - or we're going to pick out the parameters
934:16 - which permit us get the best possible accuracy.
934:20 - So that said, instead of having the drop out,
934:23 - we just have this variable drop out.
934:27 - And then for the regularization, we're going to do same.
934:29 - So you're going to have HParams.
934:32 - Instead of regularization rate, we're going to have this.
934:36 - And then we have HP regularization rate.
934:41 - Then apart from this regularization rate
934:44 - and drop out hyper parameters,
934:46 - we're going to include just right here.
934:49 - Let's start with this one.
934:50 - So let's copy out this.
934:52 - We have HParams and then we have HP number of units.
934:58 - So here we have number of units
935:00 - and then we'll call this one
935:02 - because just after this,
935:04 - we're going to have number of units too.
935:06 - So here we are going to be able to pick out
935:09 - the best possible values for this hyper parameter here.
935:14 - So let's have this here too.
935:16 - Okay, so that's it.
935:18 - The other hyper parameters will be included
935:20 - when doing the model compilation.
935:22 - So let's include this model compilation
935:24 - and then we're going to carry out the training in here.
935:28 - So we have this, let's take out the summary
935:31 - and then take this back.
935:32 - Okay, so that's it.
935:34 - Now we have this model compilation right here
935:37 - and then we have the optimizer.
935:39 - But what we're going to do here
935:40 - is we're going to fix this optimizer.
935:41 - So we're going to have the item optimizer
935:44 - and then we'll specify the learning rate to be HParams.
935:47 - And then we have your HP learning rate, learning rate.
935:53 - Okay, so there we go.
935:55 - This looks fine.
935:56 - We have the last binary cross entropy metrics accuracy.
936:00 - And then we do the model training.
936:02 - So once we do this model training,
936:04 - we are going to get the accuracy
936:07 - that we're going to train the model.
936:09 - We're going to evaluate the model.
936:10 - We obtain the accuracy
936:12 - and then TensorFlow will permit us
936:15 - modify all these different parameters
936:18 - such that we now get the parameters
936:21 - or the specific values which permit us
936:24 - maximize this accuracy.
936:26 - So here we're going to create a function
936:29 - which is going to return the accuracy.
936:31 - Let's have this to the right and that's it.
936:35 - We have this method which we'll call model tune,
936:40 - call it model tune.
936:42 - And then it's going to take HParams.
936:43 - So it's going to take this HParams
936:45 - and then it's going to like
936:47 - take different value for HParams.
936:49 - And then based on the accuracy,
936:51 - we're going to know after this different training steps
936:55 - which values for this hyper parameters best,
936:59 - gives us the best results.
937:01 - So that's it.
937:01 - We have that.
937:03 - And then we return the accuracy right here.
937:05 - Then we're going to define the range of values
937:08 - this different hyper parameters can take.
937:11 - So like this one, let's copy this out.
937:13 - Let's say we want to get this HP number of units one.
937:18 - Paste this out here.
937:19 - We're going to have hp.hpparams.
937:26 - There we go.
937:27 - And then we have the name specified.
937:32 - So we have number of units one.
937:36 - And we also specify now this range.
937:39 - So we have hp.discrete.
937:41 - So let's have discrete.
937:43 - And then let's say we want to pick out those values
937:47 - between the range, say 10 to 100.
937:51 - Now let's take this powers of two.
937:54 - So we have 16, 32, 64, 128.
938:00 - Okay.
938:01 - So that's it for the first.
938:03 - And then for this next one, we have two right here.
938:07 - Let's have that.
938:08 - And yeah, units two, there we go.
938:11 - We have that fine.
938:12 - Let's copy this again.
938:13 - And then we repeat the same for the dropout.
938:16 - So here we have dropout, take this off.
938:19 - And then here we have dropout.
938:25 - For the dropout, we're going to take values
938:27 - between 0.1, 0.2, 0.3 and 0.4.
938:34 - Anyway, let's say we want to take between 0.1 and 0.3.
938:37 - And then from the dropout,
938:39 - we have the regularization rate.
938:41 - So make sure you see that we've taken typical values.
938:45 - So your typical values for this number of units
938:47 - will be this and for the dropout will be this
938:50 - for the regularization, regularization rates
938:54 - will have different range of typical values.
938:58 - So here we will have 0.00, yeah, say 0.001, 0.01
939:05 - and then let's say 0.1.
939:07 - So that's it.
939:09 - What's next?
939:10 - We have the learning rate.
939:11 - So let's copy this out, HP learning rates.
939:16 - Here we have one E negative four, one E negative three.
939:23 - Okay, so that's it for the learning rates.
939:24 - And that should be all.
939:26 - So we have nothing left actually.
939:30 - So that's fine.
939:31 - So there we go.
939:31 - We've defined all those different ranges.
939:33 - So we're going to search in this range,
939:36 - search in this range, search in this range,
939:38 - search in this range, search in this range.
939:41 - Now, notice that this is actually discrete value.
939:44 - So it's not like we're going to pick
939:46 - between this value and this value.
939:48 - We're just going to basically pick this value
939:51 - or this value or this value or this outer value.
939:55 - And now to perform this grid search,
939:57 - we are going to go for number of units,
940:03 - one in this range that we've specified here.
940:07 - So we're going to have in HP num units one,
940:13 - there we go, dot domain dot values.
940:19 - Here we have values, there we go.
940:23 - And then we're going to do the same for this.
940:24 - So we have for num units two in HP num units two,
940:29 - HP num units two,
940:35 - dot domain dot values.
940:39 - And then for the dropout rate
940:45 - and then the regularization rate
940:47 - in the regularization domain,
940:48 - learning rate in the learning rate domain.
940:51 - We are now going to create this HP RAMS dictionary.
940:56 - So we have HP RAMS right here,
940:59 - which is going to be our dictionary
941:00 - and it's going to contain the different values here.
941:04 - So we will start with this HP num units one.
941:11 - And then what we're going to put in here
941:13 - is this value we have here.
941:16 - So the value we pick from this domain
941:18 - is what we're going to pass in here.
941:19 - So we have num units one, one, there we go.
941:26 - We now repeat the same process.
941:28 - So we just have your two and then the rest.
941:32 - And there we go, we have the remaining parameters.
941:35 - And then from here, we are going to create a file writer.
941:39 - So for each run, because here we're going to have
941:42 - different runs for the different values.
941:45 - So for each run, we're going to define the file writer.
941:50 - So here we have the file writer, TF summary,
941:55 - create file writer, just as we've seen already.
941:59 - So here we have the file writer
942:01 - and then we specify this directory.
942:04 - Now we want the directory to have different names
942:07 - based on the exact run number.
942:10 - So yesterday we want to have run number here,
942:13 - which initialize to zero.
942:14 - And then we have this.
942:16 - So let's have this logs slash,
942:21 - let's just put in the run number in your string.
942:25 - We have the run number.
942:27 - Okay, so that's it.
942:28 - We have that and then after each and every run,
942:36 - we want to increment this run number.
942:38 - So yeah, we're going to increment the run number,
942:40 - run number equal, plus equal one, plus equal one.
942:48 - Okay, so that's it.
942:49 - Now we've defined this file writer.
942:52 - That looks fine.
942:53 - Next thing we want to do is
942:56 - with this file writer as default.
943:04 - So we have this default as default.
943:08 - We'll now log in the current hyper parameters.
943:11 - So we have hp.hperrams
943:14 - and then we pass in this current hyper parameters.
943:19 - Recall, we are going to sweep through all these different values
943:24 - and then for a particular run,
943:25 - we want to know exactly what we're passing in.
943:28 - And that's what we're going to pass in here.
943:30 - So we have hp.hperrams, that's fine.
943:33 - And then once we've notified Tensor Bar
943:37 - that this is the parameters
943:40 - or the hyper parameters we're working with,
943:42 - the next thing we want to do
943:43 - is to pass this hyper parameters
943:45 - in this model tune method right here.
943:48 - So here we're going to pass in now this hp.hperrams,
943:51 - this hp.hperrams which we've just defined here,
943:54 - which is a function of these different values
943:57 - we are going to be sweeping through.
944:00 - So here we're going to have a model tune, model tune,
944:07 - which is going to take in the hp.hperrams
944:09 - and then it's going to output,
944:11 - like from here it's going to output this accuracy.
944:14 - So here we're going to have the accuracy.
944:16 - So we have the accuracy, which is that.
944:20 - And then once we get this accuracy,
944:22 - we are going to log in its value.
944:24 - So we have tf.summary, scalar.
944:26 - So log in the scalar, accuracy, accuracy,
944:30 - and then we have the accuracy value logged in.
944:33 - So that looks fine.
944:34 - We have this run number.
944:36 - Why do we have a red here?
944:39 - That's actually okay now.
944:41 - So let's get back to this.
944:43 - Now we could run this cell.
944:46 - We run this cell.
944:48 - Everything looks fine.
944:49 - And then we now go straight into this hyperparameter tuning
944:53 - where we are going to pick out the best values
944:57 - for our hyperparameters,
944:59 - which actually give us the best accuracy.
945:02 - So for each step, now we want to let's say print out this.
945:07 - So we'll print out the hp.hperrams.
945:11 - So our hp.hperrams is,
945:16 - so we have hp.hperrams, there we go.
945:20 - And then we also want to print out the current run number.
945:25 - So what we're going to have here is the hp.hperrams
945:29 - are for the run,
945:35 - let's specify the run number.
945:37 - Our hp.hperrams is that, so our hp.hperrams is this.
945:42 - And then we format, so we have that.
945:45 - And then we have the run number, there we go.
945:49 - So let's have this here and then I'll run this cell.
945:54 - So here's what we get.
945:55 - We also modified this code here
945:58 - to contain each and every value in this hp.hperrams.
946:03 - So we get this kind of output
946:05 - and we make this run for like, let's count this,
946:10 - for like 287 times.
946:13 - So we have 287 different runs.
946:15 - And then from here we run the cell and then run this too.
946:21 - So from here we have the scalars,
946:24 - you have the different accuracy to get.
946:26 - Anyway, we're not very much interested in this for now
946:29 - because we're interested in looking at this hp.hperrams here.
946:33 - Now let's start with this table view.
946:35 - You could see here that with this table view
946:38 - we could see the different values,
946:40 - the different hyper parameters we made use of
946:43 - or we tuned throughout this process.
946:46 - So you could see, let's scroll up a little.
946:50 - Here you see we have the regularization rate,
946:53 - number of units, drop out rates.
946:56 - And then let's scroll down again and then go this way.
947:00 - You have, okay, so we have number of units too,
947:04 - learning rate and then accuracy.
947:06 - So you see here that the accuracy changes
947:10 - based on the different values we have
947:13 - for this different hyper parameters here.
947:16 - Now, another better way of looking at this
947:18 - is this parallel coordinates view.
947:20 - As you could see in this parallel coordinates view,
947:23 - you get to like pick out the highest accuracy value.
947:27 - And then if you follow this path, you get to see that
947:30 - taking a learning rate of 0.0001
947:35 - and then here, let's just look at this part.
947:38 - So we see, notice this red part here,
947:42 - which you can actually turn to green by clicking on it.
947:44 - So you see it turns to green, you have 0.0001
947:48 - and then the number of units too, it's like around 32.
947:52 - Here we have drop our rate 0.2,
947:55 - number of units one, 32,
947:58 - regularization rate 0.01.
948:01 - So these are the best hyper parameter values we have
948:06 - and we can now make use of this information
948:10 - to better create our model.
948:14 - So now instead of having a model like this,
948:18 - so now we will modify this and then instead of having,
948:22 - let's get back to what we had up here.
948:25 - So what we're saying is instead of having this,
948:28 - here where we had 10, now we could take this to 32
948:32 - and then right here to 32,
948:35 - then we could pick the regularization rate to 0.01.
948:39 - So I have the 0.01 drop out rate.
948:44 - We didn't take, we didn't not drop our rate.
948:47 - Let's get back to that.
948:49 - The drop out rates, 0.1, no, no, no, 0.2 actually.
948:54 - So drop out rate 0.2, that looks fine.
948:58 - Learning rate 0.0001, so that's it.
949:02 - So we now see how to better pick these values
949:07 - thanks to this hyper parameter tuning,
949:09 - which we can do easily with tensor board.
949:12 - So let's get back to this and then we have 0.2 right here.
949:17 - Now we should be noted that the method we've used so far
949:20 - is grid search, that is we've,
949:23 - specified those different ranges
949:26 - and then we've some sort of search
949:29 - through those different ranges
949:32 - and then we've gotten the best possible parameters
949:35 - which maximize the accuracy value.
949:40 - Now another very popular technique is the random search.
949:44 - With a random search, we'll just define a range
949:48 - and then pick our random values in that range.
949:51 - So unlike the grid search where we have some defined
949:55 - or predefined values which we have to search through,
949:58 - with a random search, we kind of like pick values at random.
950:02 - But then they both have the advantages and disadvantages.
950:06 - And one thing you could take out from this is that
950:09 - with a grid search, since you have a fixed range,
950:12 - so you have a fixed range of values
950:15 - which you can pick from, it means if this fixed range
950:18 - of values is very large, it becomes problematic
950:21 - as this process of searching is going to take
950:24 - a very long time.
950:26 - So you may take a very long time before finding
950:29 - your best hyperparameter values, which we connect.
950:33 - Whereas for the random search,
950:36 - since we're not going in a particular order,
950:39 - it happens that we may even get the best
950:42 - hyperparameter values after just one step.
950:46 - So after just one run or a few runs,
950:51 - we may already get the best hyperparameter values
950:54 - just like we may get these values after 200 different runs.
950:59 - So if you have enough computation power,
951:02 - you could make use of the grid search
951:04 - since you're more sure of the different values
951:06 - you're going to be picking from.
951:08 - And if not, the random search will be a better option.
951:11 - Another very important aspect of TensorBoard
951:14 - is TensorBoard's profiler.
951:16 - With TensorBoard's profiler,
951:18 - we're able to evaluate the TensorFlow code
951:21 - and based on this evaluation,
951:24 - modify this code to ensure that it runs
951:27 - as efficiently as possible.
951:29 - That said, to make use of the profiler,
951:31 - we're going to start by installing
951:33 - this TensorBoard profiler plugin.
951:35 - So we have your pip install this,
951:38 - you run this cell.
951:39 - The profile plugin now installed,
951:41 - we now go ahead and run the cells.
951:45 - Right here, we're going to have this profile batch.
951:48 - So notice that we have this profile batch right here.
951:51 - And this value specified here has to do with
951:56 - the range of batches to be profiled.
952:00 - And so here we could take say 100 to let's say 132.
952:05 - Okay, so that's it.
952:06 - So we have that said, we are now around the cell too.
952:09 - Looks fine.
952:11 - And then we could start with the training.
952:13 - We have this TensorBoard callback
952:15 - which has been added here.
952:16 - So we run this to start with the training.
952:19 - Training now complete.
952:20 - We go ahead and run these two cells.
952:24 - Just as expected, we have the scalars,
952:26 - we have the graphs,
952:27 - we have these distributions,
952:29 - we have these histograms, time series.
952:34 - And then in here, we have this profile.
952:38 - And you're in this overview page.
952:44 - So you have different pages in this overview page.
952:46 - We have this performance summary.
952:48 - We're given the average step time,
952:50 - all the time, compilation time,
952:53 - output time, input time.
952:55 - Notice how this input time is relatively larger than the others.
952:59 - And you'll see that,
953:01 - or you'll notice that here,
953:03 - the input time occupies the largest part of the step time.
953:07 - So that's why you have a recommendation
953:12 - to first focus on reducing this input time.
953:17 - And then we have 7.5% of total step time
953:21 - sampled is spent and cannot launch.
953:23 - It could be due to CPU contention with TF data.
953:27 - In this case, you may try to set the environment variable
953:30 - TF GPU trend mode to GPU private.
953:34 - Then we have 6.6% of the total step time
953:37 - sample is spent on all other times.
953:40 - This could be due to Python execution overhead.
953:43 - Only 0% of device computation is 16 bit.
953:46 - So you might want to replace more 32 bit operations
953:50 - by 16 bit operations to improve performance.
953:53 - So this actually means we could make use of
953:56 - mixed precision training.
953:58 - Now we're going to look at mixed precision training
954:00 - in subsequent sections.
954:02 - We also have this other tools we could use
954:04 - for reducing the input time.
954:06 - This input pipeline analyzer.
954:08 - You could click here.
954:09 - You have this input pipeline analyzer.
954:12 - Now notice that you have these different tools here.
954:14 - So we have the overview page.
954:16 - Let's go back to overview page.
954:20 - And then here we have this input pipeline analyzer.
954:24 - Let's scroll down here.
954:26 - We have the input pipeline analyzer.
954:27 - We have this TF data bottleneck analysis.
954:31 - You could click on this.
954:33 - You see we have this TF bottleneck analysis.
954:35 - And then you let's get back to the overview again.
954:39 - Scroll up.
954:41 - And then you have this trace viewer right here.
954:44 - So you would also find this trace viewer in here.
954:48 - You could scroll here and you have the trace viewer.
954:51 - Now get into summary of the input pipeline analysis.
954:55 - We get to see exactly the breakdown of the input process
954:59 - and time on the host because we've seen already from here
955:02 - that the input processing time is kind of taking up
955:06 - close to 68.4% of the total step time.
955:10 - So here you could see we have the data processing,
955:14 - which is like the main reason why this input processing time
955:19 - is that large.
955:20 - And we have this different steps we could take.
955:23 - So what can be done to reduce above components of the host
955:26 - input time in current data?
955:30 - That is, you may want to combine small input data
955:32 - chance into fewer of the larger chunks.
955:35 - Data processing.
955:36 - You may increase number of parallel calls in the dataset
955:40 - map or preprocess the data offline.
955:43 - Reading data from files in advance.
955:46 - Reading data from files in demand.
955:48 - Other data reading or processing.
955:50 - And then here we have a more detailed input operation
955:53 - statistics.
955:54 - So let's click on this.
955:56 - Scroll up and then you could have this statistics
955:59 - given to you right here.
956:00 - So as you could see, we know exactly why
956:05 - our input processing is taking up much time.
956:07 - And based on this different suggestions,
956:10 - we could reduce this time.
956:13 - Now from here we have the kennel stats.
956:17 - Then we also have this memory profile.
956:21 - And then we have the part viewer.
956:25 - From this part viewer we have the TensorFlow stats.
956:30 - And then here we have this TensorFlow data bottleneck analysis.
956:36 - You could also look at this from here.
956:39 - You see we have the root prefetch.
956:41 - Look at the self duration here.
956:43 - 10 microseconds, 36 microseconds.
956:46 - See this one now is very large.
956:48 - So here's our bottleneck, a level of the mapping and batching.
956:52 - Shuffling, just 118, 373.
956:57 - Pre-fetching and so on and so forth.
957:00 - So we now know that the problem comes from the mapping
957:04 - as we have seen previously.
957:07 - Now selecting this trace viewer, we have this year.
957:12 - And we'll make use of this year.
957:15 - You could carry this around by clicking on this.
957:18 - And then you have the arrow.
957:20 - You have this to pull this from place to place.
957:24 - You have the zoom.
957:25 - So you have the pan.
957:27 - You have the zoom.
957:29 - You have this timing.
957:31 - So let's click on the zoom and then you see click.
957:34 - You click and you drag to the top.
957:37 - You could zoom in and zoom out.
957:39 - Now you notice the string.
957:41 - We have values from 100.
957:43 - Like let's get back to what we had defined previously.
957:47 - Here we have 100 to 132.
957:50 - And that's why you notice here we have the string from 100 to 132 to.
957:55 - Getting back here, you can zoom.
957:58 - See you have that.
958:00 - And then you could click here on the pan and then you pull this to one side.
958:05 - Now stopping right here, you could zoom again.
958:08 - And then you get to see all those different operations
958:12 - carried out during a single process.
958:15 - All those different operations carried out during a single training step.
958:19 - And with this timing tool you click on this timing tool.
958:23 - You can be able to like let's zoom this again
958:28 - and zoom and then drag this here.
958:33 - Zoom again.
958:37 - As we're saying measure the timing for a given operation.
958:41 - So once you click here, you just click and then you see you drag
958:45 - and you can measure the timing for different operations.
958:48 - You see you have the time here.
958:50 - 166.5 microseconds.
958:54 - And that's it. We now go on the distributions.
958:57 - Here we have batch normalization, batch normalization.
959:01 - Let's check out on this comp 2D.
959:03 - You see that different biases and weights.
959:08 - You see the kernel here is the weights have values which fall under this zone.
959:16 - The values fall under.
959:18 - The values are between 0.3, negative 0.3, 0.3.
959:23 - For the biases between negative 1.2, about 0.4.
959:30 - And then for comp this dense, we have this other dense layer here.
959:35 - This comp 2D2, which you could see here.
959:38 - See the range of values.
959:40 - And then we have the dense 1.
959:42 - We have the dense 2.
959:43 - We also have the different value ranges for both the kennels and the biases.
959:49 - So that's it for this distributions.
959:51 - We have the histograms too.
959:53 - You could check this out.
959:56 - Time series.
959:57 - And the way this time series is kind of similar to information we've seen already.
960:03 - So here we have this all.
960:05 - We could select just the scalars.
960:07 - So here we have the epoch loss.
960:09 - We have the different evaluation accuracy and so on and so forth.
960:13 - Now click on images.
960:15 - You select only images.
960:17 - Nothing to be shown.
960:19 - See that's why you have no information here.
960:21 - Click on histogram and then you have the histogram data we've just seen already.
960:25 - That's it for the section of TensorBot.
960:27 - TensorBot has other functionalities which we shall explore subsequently.
960:31 - And thank you for getting right up to this point.
960:39 - Hello everyone and welcome to another amazing session in which we are going to see how to work with weights and biases
960:47 - and integrate it with our already existing TensorFlow code base.
960:52 - Weights and biases help practitioners in experiment tracking, collaborative reports,
960:57 - dataset and model versioning, interactive data visualization, and hyperparameter optimization.
961:04 - It's trusted by over 100,000 plus machine learning practitioners around the world.
961:10 - In this session, we are going to focus on experiment tracking.
961:14 - It's one thing to build a model, train this model and evaluate it on a given dataset.
961:20 - And at least as we've seen throughout this course, this is pretty easy with TensorFlow.
961:25 - When we work in large teams and we have to collaborate, we have to produce reproducible results.
961:31 - We need to debug those ML models as a team and we also need to enforce transparency.
961:38 - Then a machine learning operations platform like weights and biases becomes indispensable.
961:45 - Weights and biases permits us to build better models faster with experiment tracking,
961:51 - data versioning and model management.
961:54 - As of now, the different products which Weights and Biases offers to us are experiment tracking,
962:01 - reporting that's producing collaborative dashboards, artifacts, dataset and model versioning,
962:08 - just like how you would do code versioning in Git, interactive data visualization,
962:15 - and hyperparameter optimization.
962:18 - Also, you could see here that this has been used by modern 100,000 machine learning practitioners around the world.
962:26 - Some key aspects of the Weights and Biases tool are the fact that you could integrate very quickly.
962:32 - You see that with Keras.
962:34 - So here we're supposing we are building a Keras model, a TensorFlow Keras model,
962:39 - and that all you need to do is to import this 1DB callback right here, start a new run as it's given here.
962:47 - So if you have some configurations, you set those configurations and then in the place of the callbacks,
962:54 - you simply pass this Weights and Biases callback which we imported right here.
962:59 - So it's quite easy to integrate with already existing frameworks.
963:04 - Now you see with any framework, you just need to do 1db.log and you could log any information you want to log.
963:12 - We could also visualize useful information very seamlessly
963:16 - and then we could collaborate in real time.
963:19 - So if you're working on a project, you could all as part of the team
963:24 - discuss the project's progression and see how to eliminate any bugs or any problems.
963:31 - Now Weights and Biases is designed for all use cases.
963:34 - So here we have a practitioner that's supposing just a single person.
963:39 - You have this dashboard, central dashboard.
963:42 - You see this hyper parameter sweeps artifacts.
963:46 - You could do dataset and model versioning just like you would do with GitHub code reports to share updates very transparently.
963:55 - Throughout this course, we'll look at these different products
963:58 - and in this section, we'll focus on experiments tracking.
964:02 - We're now going to go straight forward into signing up.
964:05 - So we click right here.
964:07 - We want to sign up with GitHub.
964:09 - Click here.
964:10 - And then we authorize 1db to get access to our neural learn account.
964:15 - So authorize 1db.
964:18 - We have the full name, organization, neural learn, and that's fine.
964:23 - Okay, so I agree to the terms and conditions.
964:26 - You could always read out the terms and conditions very carefully.
964:31 - And then from here, we also have to put in this username.
964:35 - So let's say neural learn.
964:37 - Okay, so we continue.
964:40 - How often do you train models?
964:42 - Let's say every week.
964:44 - So we have that and then get started.
964:46 - Here is now a homepage.
964:48 - Here you could create a new project.
964:50 - You could modify a profile, invite your team.
964:53 - You have this documentation.
964:55 - Click right here.
964:57 - Docs.1db.ai with the different guides, references,
965:01 - which you could always make use of in case you are having any difficulties
965:06 - or as a starter, you want to master how all those work.
965:10 - Then you also have this fully connected right here,
965:13 - which brings ML practitioners together.
965:16 - Now here you have curated tutorials, conversations with industry leaders,
965:21 - deep dives into newest ML research and a whole lot more.
965:26 - So you would always have this information or this curated information at your disposal.
965:32 - Then we also have the community and then we have this quick start
965:36 - for the different frameworks.
965:37 - So we could view all frameworks here.
965:40 - Getting back, you see we have PyTorch Keras.
965:43 - We're working with Keras.
965:44 - Click on Keras.
965:45 - You would have this quick start for Keras users, which we actually.
965:49 - So here you see how easy it is to get started with 1db in Keras.
965:56 - The very first step will be to install and log into 1db.
966:01 - So we'll simply copy this and then get back to our Clap Notebook.
966:06 - We paste this out right here and then run this cell.
966:09 - We have the syntax error.
966:11 - Let's have this and then we run.
966:14 - There we go.
966:15 - As you can see, 1db has been installed.
966:17 - From here, we are going to get into the login.
966:20 - Now you see you can find your API key in your browser here.
966:24 - So we could click on this link, get the API key and then paste it out here.
966:28 - Now let's get back to this and then copy out this API key.
966:33 - There we go.
966:34 - We have that and then we paste it out here.
966:37 - So we paste it out there or you could click on this link and then still get the key,
966:42 - which we can paste in here and then simply press and enter.
966:46 - So we hit enter and we should be able to log in since we have this key put in right here.
966:51 - Now moving on to the next step at the top of your training script,
966:55 - start a new run and to start this new run,
966:58 - we are going to make use of this init method.
967:00 - Getting back to the documentation,
967:02 - you have your run is a unit of computation locked by 1db.
967:07 - Typically, this is an ML experiment.
967:11 - So we create a run with the 1db init.
967:15 - So before moving on, you should note that you could create a project
967:20 - and in this project, you have several runs.
967:23 - Now one run could be for training.
967:26 - So we could have a training run like some sort of ML experiment as defined here
967:31 - and then we have evaluation and you could have other different runs
967:37 - or other different training processes which will act as different runs.
967:42 - Here, as you can see, when you just import 1db,
967:46 - the run is known and now you have 1db init.
967:51 - Once you make a call on this method,
967:56 - you have a run which is automatically created.
967:59 - And so everything you're going to log in into 1db
968:04 - will be sent to that particular run.
968:07 - So if you have say this project,
968:10 - let's call this project malaria prediction project.
968:13 - If you have a malaria prediction project
968:15 - and then we have this training run,
968:17 - everything like once we create this run, everything we log in
968:21 - will be sent into this particular run right here.
968:25 - And then if we create another run, that's evaluation run,
968:28 - everything we log in will be stored in this particular run here.
968:34 - So getting back here, you see you could create this run
968:37 - and you could finish or you could stop that run.
968:41 - And that's why once you have this init, you've created a run,
968:44 - you stop the run and then here normally this should be known
968:49 - as there should be no run going, no run created.
968:53 - Now, after doing this, you could also create this like here,
968:58 - like 1db init, you could put this in a with block.
969:04 - So we have with 1db init as run,
969:07 - then you now have all the data to be logged in here
969:12 - such that out of this with block, you have no run.
969:18 - From here, you could check out on the different attributes
969:21 - and then the information related to this attributes.
969:24 - Now let's get into this 1db init.
969:26 - 1db init you have as definition with all the different arguments it takes
969:31 - and then you have this information
969:35 - concerning all those different arguments.
969:37 - So here we could define like we have this run,
969:41 - which we could create by simply doing 1db init
969:44 - and then we'll specify the project, specify the entity,
969:49 - I would say neural learn, the project malaria detection,
969:53 - the configuration, all the information will be needed.
969:56 - Let's say for training, we specify a safe code
970:00 - to permit 1db safe code.
970:03 - By default, this is actually false.
970:05 - So by default, your code is not going to be saved to 1db.
970:09 - You could check this out here.
970:11 - Let's have safe code.
970:13 - Let's search that.
970:14 - Save code.
970:16 - Go up and there we go.
970:18 - So by default, we don't allow this
970:23 - and you could flip this behavior by going to the settings page.
970:26 - Now you could check on all these other arguments.
970:29 - You have this job type argumental, which is also very, very important
970:34 - and this is very useful when your grouping runs together
970:38 - into larger experiments using this group argument right here.
970:43 - Now that said, let's copy this part of this code
970:46 - and then get back to our notebook.
970:49 - We will notice that we have this import.
970:51 - So we would have to finish, put out this import here.
970:56 - Let's have this.
970:57 - We have import 1db from 1db Keras, import 1db callback.
971:02 - Let's take this off.
971:04 - So we put this tool here and then we run this cell.
971:09 - That should be fine.
971:11 - Now let's run.
971:12 - We scroll down.
971:14 - We have the model.
971:16 - Let's run this again and then let's actually get back here
971:20 - and create this run.
971:21 - So let's have this here.
971:23 - So 1db init.
971:24 - We're going to specify the project.
971:26 - We're going to create this.
971:27 - So this is 1db install login
971:31 - and install login and initialization.
971:36 - So we have that.
971:37 - Now, as we've said already,
971:39 - this is going to permit us create a run.
971:41 - So here let's have malaria detection, malaria detection
971:46 - and one way the like to write this out,
971:49 - like prefer to put this hyphen instead of the space.
971:53 - So let's have that that way.
971:55 - So we have your malaria detection, entity in neural learn
972:00 - and let's have that for now.
972:01 - So let's run the cell and then let's add this code below here.
972:07 - Currently login is neural learn and that's fine.
972:10 - From here, let's now do 1db run.
972:13 - See what we get.
972:15 - From here, we see that we have no metrics locked yet.
972:18 - So that's fine.
972:20 - We could now take this off and then go straight away
972:24 - to add our 1db callback to tensorflow.
972:28 - Before that, we have saved model inputs and hyperparameters.
972:33 - So 1db config, let's have this copied
972:36 - and then we have that before defining the model.
972:39 - Let's copy that and then just here,
972:43 - we'll add this code cell again
972:46 - and then let's have your initialization configuration
972:50 - and there we go.
972:51 - Okay, so we have that.
972:52 - 1db config learning rate specified,
972:55 - number of epochs, batch size.
972:57 - We'll include the dropout rate,
973:00 - the image size, regularization rate,
973:03 - number of filters, canal size, number of strides,
973:06 - pool size, number of outputs for the first dense layer,
973:10 - number of outputs for the second dense layer.
973:12 - It was us finally now around the cell.
973:14 - So we've now started this configuration.
973:17 - Then the next and last step is to simply have this callback
973:22 - in this fit method.
973:25 - So let's copy this out and then get back.
973:28 - So we have that, we've run our model.
973:31 - Let's be sure we've run this model
973:35 - and just here, let's have this configuration.
973:40 - Let's say we have our configuration,
973:43 - configuration equal 1db.config.
973:47 - And then right here, we have configuration and image size.
973:53 - And we have same for the other hyper parameters
973:56 - like the dropout rate, regularization rate,
973:59 - number of filters, canal size, pool size,
974:02 - and number of strides.
974:04 - We now go ahead to the training process.
974:07 - So we copy this out and paste just right here.
974:11 - So here we would have this callback.
974:14 - Let's take this off.
974:16 - Instead of this TensorFlow, TensorBoard callback,
974:19 - we now have this 1db callback.
974:21 - So we have this 1db callback.
974:22 - And you could always even include the TensorBoard callback.
974:26 - So you could also have the TensorBoard callback we had previously.
974:29 - Now there we go.
974:30 - We have this 1db callback.
974:32 - Let's have that.
974:33 - And then we run this cell for metrics.
974:36 - We compile the model.
974:38 - All the models compiled.
974:40 - Let's include this learning rate.
974:43 - So let's have this configuration
974:46 - and then specify the learning rates.
974:49 - So we have that learning rate there.
974:52 - And then we compile the model and start with the training.
974:56 - Now the training is complete.
974:58 - We could go to our weights and bias dashboard right here
975:02 - and see exactly what went on during the training process.
975:06 - So here we have these projects.
975:08 - You could see in your learn, we have the projects.
975:11 - We click on this malaria prediction project.
975:14 - And then we select this run.
975:16 - The run we selected here is the sandy water run.
975:19 - So there we go.
975:20 - Now you see we have 19 different chats.
975:23 - AUC, validation loss, false positive, false positive.
975:30 - Well, this is epoch versus epoch.
975:32 - So that's why you have this kind of straight line.
975:35 - And then you have this precision recall loss
975:40 - to negative to positive accuracy.
975:43 - In fact, all this is basically gotten
975:46 - from all those metrics we had here.
975:49 - So this means that with this simple callback
975:53 - we have put right here, weights and biases
975:56 - is able to get all this or capture all this information
975:59 - during the training process and give it to us
976:02 - or present it to us after we're done with the training.
976:05 - So here we can see those different charts
976:09 - with what we already used to see in this already.
976:11 - So you should be already familiar with these different chats.
976:15 - Now, apart from these charts, you have the system information.
976:20 - So here we have the CPU utilization.
976:22 - We have the system memory utilization,
976:25 - process memory news, process memory news.
976:29 - While this is megabytes,
976:31 - here we have this information in the percentage
976:35 - with respect to the total memory available.
976:38 - Then here we have process memory available,
976:41 - process CPU trace and use, disk utilization,
976:45 - network traffic, GPU utilization, GPU temperature.
976:49 - So you could have this information here,
976:51 - GPU time spent, GPU memory allocated,
976:54 - GPU power usage.
976:56 - And so you see how easy it is to get all this system information
977:01 - without writing any extra line of code.
977:04 - Now from here we could go on to the model.
977:07 - You see, we have this table right here,
977:10 - which shows our model.
977:11 - It's kind of similar to the model summary
977:13 - we had pulled out previously.
977:15 - So yeah, it's kind of similar to what we had here.
977:18 - So now we have, instead of that,
977:20 - we have this beautiful table right here,
977:22 - output shape, number of parameters, type and the name.
977:26 - So that's it.
977:27 - And then from here we have this logs.
977:30 - So we have everything that was logged out,
977:33 - like all we had here.
977:36 - So you see everything logged out.
977:39 - We have it in this run.
977:41 - Now recall that when doing this 1DB init last year,
977:47 - once we do this 1DB init, we actually create a run.
977:51 - And once that run is created, everything we do after that
977:55 - is going to be stored in that run.
977:57 - And that's why if you notice here,
977:59 - like you don't only have this information,
978:02 - not only the string information,
978:04 - but even this data which was logged,
978:07 - even this model because we run this several times.
978:10 - So you see here that even this information which was logged
978:15 - is actually stored by 1DB.
978:19 - And so you see that experiment tracking here is done very easily
978:23 - and actually seamlessly.
978:25 - Now you look at the files will start.
978:28 - You have the model best.
978:30 - See you have automatically this Keras model file will just start.
978:36 - You have this metadata.
978:38 - You could open this up and you have this information start.
978:41 - You see here that we're using a Tesla P100 GPU.
978:46 - So that's it.
978:47 - Python version, operating system version,
978:51 - and then other information like the GPU count, CPU count,
978:55 - and so on and so forth. So that's it.
978:58 - Let's get back to this overview. See it's still running.
979:01 - That's because we have our Collab Notebook still running right here.
979:06 - Now one thing you could always count on is this documentation right here.
979:10 - So you just come straight to this integration and then you pick out Keras.
979:15 - You would have your Keras and then you have this 1DB Keras 1DB callback.
979:20 - And right here you have this.
979:23 - Now you could see this arguments of those different arguments we have here
979:27 - and you have the explicit definitions here.
979:31 - So let's check out on this arguments.
979:34 - You see we have monitor the validation loss,
979:39 - which plays a similar role to this model checkpoints callback right here.
979:45 - And also we could specify the mode.
979:48 - So let's get back to the documentation where we are.
979:52 - Okay, so we get back to this mode.
979:55 - So you could pick out the mode by default is automatic.
980:00 - We could also select mean or max.
980:03 - In the case of validation loss obviously would select a mean
980:07 - and the case of if we are dealing with a validation accuracy
980:10 - or validation precision or recall, then we'll select the mode to max
980:14 - such that we're saving our model when we have the maximum precision
980:19 - or maximum recall for example.
980:21 - Here we have the safe model, safe graph, safe weights only, lock weights,
980:25 - gradients, training data, validation data.
980:28 - Now we should know that with this we're able to pass in our data set to 1DB.
980:35 - And the fact that we pass in this data to 1DB can permit us
980:40 - come up with some visualizations of what the model is predicting
980:45 - since now we have the data.
980:47 - Then we also have the generator, validation steps, levels, predictions,
980:52 - input type, and so on and so forth.
980:54 - So you could always check out on this.
980:56 - Now the next thing we'll be doing is take the validation data,
981:01 - pass into this 1DB callback right here,
981:05 - and then be able to visualize the different predictions
981:09 - as we go through the training process.
981:11 - So let's break this up a little.
981:13 - We have this training validation, number of epochs.
981:18 - This should actually be configuration, configuration,
981:23 - number of epochs, number of epochs.
981:27 - Okay, so we have that.
981:29 - And then we have the verbosity callbacks.
981:33 - Now in this 1DB callback right here we have validation,
981:38 - validation data, which is all of our data set.
981:44 - And then from here, let's check out the documentation.
981:48 - And then we have the levels and the data type.
981:53 - So let's specify the levels.
981:56 - From here we have levels.
981:59 - And the levels we have here, we have parasitized
982:05 - and uninfected.
982:09 - Anyway, we could create this like we could have the classes.
982:13 - So we could have your levels and then specify this.
982:18 - So let's just take this off from here.
982:21 - Cut that and then paste it out here.
982:24 - Okay, so we have that and then let's have this levels.
982:27 - Now we have the level.
982:30 - We could also specify the data type.
982:32 - So that said, we specify this to be image.
982:35 - Okay, we have that, number of epochs and all of that.
982:39 - Let's go ahead and modify this number of epochs here.
982:43 - So number of epochs, 100.
982:46 - Let's change this.
982:47 - Let's just have three epochs for now.
982:49 - So let's get back 1DB config.
982:52 - We should have this up here.
982:55 - So once we make this, we get this new configuration.
982:59 - So we run that, looks fine.
983:01 - We get back to the training.
983:03 - From here now, we could run this training again.
983:06 - Training complete.
983:07 - Let's go ahead and check out what has been logged in our dashboard.
983:11 - So let's have this here.
983:13 - We pick up this run.
983:15 - There we go.
983:16 - We see we have still our 19 different charts.
983:18 - And then we have this media, which has been added here.
983:22 - We click on this and you see what we get.
983:25 - As you could see here, we have the different images
983:28 - and the predictions.
983:30 - We should view full screen mode by doing this.
983:34 - And then just here, you could select the step.
983:38 - So if you pull this to the end, you see we have 36 steps,
983:42 - meaning we have trained for over 36 different epochs.
983:47 - Now, selecting this malaria detection project
983:50 - and having all these three different runs right here,
983:55 - you see you could view all the runs simultaneously.
983:59 - So I could click on this one.
984:01 - This is one run actually.
984:02 - This is another run.
984:03 - So I could view all this now simultaneously right here.
984:07 - See, we have that.
984:09 - You see the difference in GPU power usage
984:12 - with these two different runs.
984:14 - That is the sandy water to run and this exalted night to rerun.
984:19 - And then from here, if you want to stop a run,
984:22 - if you want to stop this current exalted night to rerun,
984:26 - what we could do is come right here.
984:29 - That is, let's say we have this code here.
984:33 - We could come right here and just simply put out 1db.finish.
984:39 - So we call this and we should be able to stop the current run.
984:44 - You can always check out on this 1db.run documentation right here
984:49 - to get more information about this.
984:51 - You see here this example given where the create the run
984:56 - and then stop the run.
984:58 - And then this shows that there is no current run.
985:02 - And then after recreating the run, you see that there is now a run.
985:07 - So this is simply how we could stop this run.
985:11 - And that's what we've just done here.
985:13 - We could simply run the cell.
985:16 - And after running the cell, you have this run summary.
985:19 - The accuracy and the other different metrics and loss values.
985:25 - Notice how the different metrics and losses
985:28 - have been put out in this command line formatting.
985:32 - So you could see how this loss, for example, here drops.
985:36 - And then getting back to our dashboard,
985:38 - we see that there is no current run.
985:40 - Then we've again recreated a new run with a 1db init right here.
985:46 - We've created this new run on this elevator.
985:49 - We could check it out here.
985:51 - We created this new run.
985:52 - And you can see this little green circle right here.
985:56 - Coming back to the notebook, we have this 1db callback,
986:00 - which we've used so far in logging information into weights and biases.
986:06 - But then this is limited because we are not yet able
986:11 - to define our own custom callbacks.
986:14 - Now that's set, let's get back to this callbacks we have defined here.
986:18 - Here we have this log images callback.
986:20 - We're going to actually try to log images into weights and biases
986:24 - using this callback, which we have created.
986:27 - Now recall that to create this kind of custom callbacks,
986:30 - you inherit from the callback class in terms of flow.
986:34 - And then you get to put in some code in here,
986:39 - which defines that custom callback.
986:43 - Now we've just copied this and paste it right here.
986:46 - The difference here is this is 1db.
986:49 - So this is what we had for tensor board and this one of 1db.
986:54 - You're going to see how easy it is, even though we're building
986:58 - these custom callbacks.
987:00 - So right here, so let's suppose we want to predict,
987:04 - rather we want to log this confusion matrices
987:07 - and we want to log this to weights and biases.
987:11 - Now what we'll do is we're going to take off all this here.
987:15 - So let's get back.
987:16 - This is what we did with tensor board.
987:18 - We saw that we had to make use of matplotlib
987:21 - and then log this information into tensor board.
987:24 - But here what we have or what we need to do to log
987:28 - the confusion matrix is simply just this part of this code right here.
987:35 - So here what we have is we'll take all this off.
987:39 - See, we take all this code right here off.
987:43 - Take this off.
987:45 - Let's have that.
987:47 - So we have this code right here taken off
987:50 - and then we're just left with this.
987:52 - So you see that instead of having to write all this code,
987:56 - all we need now to write is just this one.
988:00 - Now if you're wondering why or how to get the other plots
988:05 - or what other plots we could get automatically like this with weights
988:08 - and bias, we get into this weights and bias GitHub repository.
988:13 - And here we have, we have this link,
988:17 - 1DB client, TreeMaster, 1DB plot.
988:21 - You see all the different plots we have now.
988:24 - And since it's constantly under development,
988:27 - surely in some time to come,
988:30 - we will have many more of this different plots
988:32 - we could do very easily.
988:34 - So here we have a bar.py, confusion matrix.
988:38 - You see, we have the confusion matrix,
988:40 - histogram, line, line series, position recall curve,
988:44 - ROC curve and scatter PY.
988:48 - So this means that already we could plot out this confusion matrix
988:52 - and the ROC curve very easily with weights and bias.
988:56 - So let's get back to this.
988:58 - We'll check this out.
988:59 - First thing we have noticed is log, 1DB.log.
989:02 - So whenever you're using this 1DB callback here,
989:05 - what's actually happened, what happens under the hood is
989:09 - this information has been logged like this.
989:12 - So we have, we make use of this 1DB log
989:15 - and then here we are going to have, let's change this.
989:18 - Let's say confusion, confusion matrix.
989:23 - Anyway, let's write like this.
989:25 - Okay, here we have the confusion matrix
989:27 - and then you have 1DB plot confusion matrix.
989:30 - You could modify this and put PR curve,
989:33 - that's position recall curve or ROC curve.
989:35 - That's it.
989:36 - And then this props, then we have Y true,
989:40 - prets, the predictions and then the class names.
989:44 - So here we should change this class names
989:47 - and have parse size, parse size.
989:53 - And what do we have here?
989:55 - We have uninfected.
989:57 - And then you can either pass the probabilities
990:01 - or the predicted score.
990:03 - So here we are going to have props equal predicted
990:07 - since what our model predicts or what our model outputs
990:10 - are the probabilities.
990:11 - And then the Y true is equal levels.
990:14 - From here, we're going to take out the spits
990:16 - because we've done that already.
990:18 - So let's have this here.
990:20 - There we go.
990:21 - This doesn't take into consideration the threshold.
990:23 - So we take off this threshold.
990:25 - That said, now we have this callback 1DB
990:29 - and then we're ready to train our model.
990:32 - After the training process,
990:33 - we have those results which look great.
990:36 - And we could now go ahead to look at the confusion matrix
990:41 - loaded in the dashboard.
990:43 - So let's get to our dashboard
990:45 - and this is what we should have.
990:46 - Let's get back to runs and then click on this current run
990:51 - and this is what we get.
990:52 - Now we see that we have this confusion matrix
990:55 - which doesn't actually show us what we expect to get.
991:01 - And this is simply because the way this was conceived
991:05 - or the way this callback here, this login was conceived
991:10 - was such that we have a multi-class problem.
991:15 - So even in the case where we have a binary classification problem
991:20 - like in this case, we expect to have an output of two values.
991:25 - So that's why when you have this output,
991:28 - let's get down here.
991:31 - Click on this.
991:33 - Let's add this code.
991:35 - So that's why when you have the value like say
991:38 - in the output 0.9,
991:42 - it's considered to be parasitized
991:44 - although we defined already that parasitized
991:48 - is meant to be zero, uninfected equal one.
991:52 - And so when we have 0.9,
991:54 - this is greater than the midpoint of this two,
991:57 - which is 0.5.
991:58 - So we should consider this as uninfected.
992:01 - But the way as we said already,
992:02 - the way this has been conceived is such that
992:05 - even in the binary classification problem,
992:07 - we shouldn't or we don't have a single output
992:11 - but two outputs.
992:12 - That is if we have a value of 0.9,
992:15 - what 1 dB expects to see is something like this,
992:19 - something like 0 and 0.9.
992:23 - So showing that here,
992:25 - let's even put here zero one,
992:27 - even 0.8 and 0.2.
992:32 - Okay, so let's have this.
992:34 - So even in this case,
992:35 - it goes back to the same output.
992:38 - So this simply means that we are having
992:43 - an uninfected cell since this first index here
992:46 - has a higher value.
992:49 - Now to solve this issue,
992:50 - what we're going to do is,
992:52 - we're going to take each and every output we have
992:54 - and convert it to this format.
992:56 - So in the case where we have, for example,
992:58 - 0.1 as output,
993:00 - we're going to convert it into 1, 0
993:03 - because this means that the higher,
993:07 - this one means that it's parasitized cell
993:10 - and this two means the same
993:12 - because here we're seeing that this zeroth index,
993:15 - which is the parasitized index,
993:18 - has a higher value.
993:20 - Hence, this is a parasitized cell.
993:24 - So that said, what we're going to do now is,
993:26 - for all values less than 0.5,
993:29 - we could say less than, yeah, 0.5,
993:31 - we are going to convert it into 0,
993:34 - sorry, we're going to convert it into 1, 0.
993:37 - And then for all values greater than 0.5,
993:40 - we're going to convert them into 0, 1.
993:44 - So this is the transformation we're going to make
993:47 - in order for this 1DB log method
993:50 - to correctly log our values.
993:53 - But note that if we're having a multi-classification problem,
993:57 - it will be needless doing this transformation.
994:00 - So let's get straight away and see how we're going to transform this
994:03 - into this required format right here.
994:06 - Let's start by copying out this first part.
994:09 - Let's copy this into this other cell here.
994:12 - And once we run this, we have this output right here.
994:15 - Then we can go ahead and modify this predicted.
994:18 - So what we now have is, we will define the spread,
994:22 - this other list.
994:23 - And then for i in range,
994:27 - range the length of predicted,
994:30 - we're going to make sure that if the value,
994:35 - the i, we're going to take this zero index.
994:40 - If it's less than 0.5,
994:44 - then what we have to do is append this
994:49 - to this list spread right here.
994:51 - So what we're going to append is 1, 0
994:55 - because this is less than 0.5.
994:57 - So if it was a multi-class problem,
995:00 - we would have the highest or the higher value in this case
995:05 - since we just have in two classes,
995:06 - we have the higher value representing this output
995:10 - which was predicted,
995:11 - which happens to be less than 0.5.
995:13 - So we have here 1.
995:15 - And then here we'll have else,
995:18 - spread, append.
995:21 - What do we have next?
995:23 - We have 01.
995:25 - So here we have 01.
995:26 - Let's take this off, 01 and that's fine.
995:29 - So here we've modified this spread
995:31 - and then we could then print out the spread.
995:35 - So with this now,
995:36 - we should be able to have the expected output.
995:39 - There we go.
995:40 - We have this output.
995:41 - Let's add this code cell
995:44 - and then print out our pride shape.
995:48 - We run this and what do we get?
995:50 - You see, we have exactly what we expect.
995:52 - So that said,
995:53 - let's copy out this part from here
995:56 - and then we're going to add this here.
996:00 - So we have this pride now.
996:02 - Let's have this forward
996:04 - and then we're going to have
996:06 - pride equal np.array of pride.
996:10 - So convert it into a non-py array
996:12 - before passing into this log method console.
996:16 - That should be fine.
996:17 - Okay, so that's it.
996:19 - Now we have this set.
996:21 - Everything looks fine.
996:22 - Let's take this off and have that pride.
996:25 - So we have this pride now.
996:27 - Everything looks okay.
996:28 - And we could now rerun or restart our training.
996:32 - So let's have this year.
996:34 - Let's take this off.
996:35 - From here we train again for just about two epochs
996:39 - and then the results we get.
996:41 - So we can click on this run year
996:44 - and then you could have the tables
996:46 - and the custom chats.
996:48 - Let's start with the custom chats.
996:50 - You could expand this this way.
996:52 - So here you see that we have
996:55 - the predicted and the actual.
996:57 - So we have the model.
996:59 - You see now that we have
997:01 - this number of true
997:03 - let's say true positives
997:05 - which increases.
997:06 - Number of true negatives
997:08 - which also increase.
997:09 - While we have this
997:11 - number of false negatives to be 112.
997:14 - While here we have 62.
997:16 - This is 1278.
997:19 - 1305.
997:21 - And while you compare this with this previous run
997:23 - you see that
997:24 - now we have reasonable outputs.
997:26 - As with this previous run
997:28 - we had that error
997:30 - where the confusion matrix method of 1 dB
997:33 - considers all the outputs
997:35 - to be parasitized.
997:37 - Let's have this back and take this off.
997:40 - Then from here we could go ahead
997:42 - and to look at how to plot the ROC plots.
997:45 - Let's get back here.
997:47 - Scroll up.
997:48 - What we do is here we just have to
997:51 - let's comment this here
997:53 - and then
997:54 - now do the same for the ROC plots.
997:57 - Let's get back to the GitHub repo
997:59 - and click on this ROC curve here.
998:02 - Here we have now those arguments
998:05 - which have been described.
998:06 - So here we have why true
998:08 - why probabilities.
998:10 - Obviously this is the level
998:12 - predicted
998:13 - and then the classes, the class names.
998:15 - Classes to plot.
998:17 - That is a set of classes
998:19 - which we are going to
998:21 - plot.
998:22 - We are going to
998:23 - exclude all classes
998:24 - which aren't specified
998:26 - in this classes to plot argument.
998:28 - Now this is actually optional
998:30 - so you don't necessarily need to have this.
998:32 - We also have this title.
998:34 - So let's copy this out.
998:36 - Let's copy out this
998:38 - here.
998:39 - Let's copy this out
998:41 - and
998:42 - paste
998:43 - just right here.
998:45 - We then specify the levels
998:47 - why true levels.
998:49 - Here we have
998:51 - red and then
998:52 - right here we have this
998:54 - class names.
998:55 - So let's copy this out
998:57 - and paste here.
999:00 - OK, so that's what we have.
999:02 - Let's run this cell.
999:04 - Everything looks fine.
999:06 - And then we start with the training.
999:08 - Training now complete.
999:10 - Here are the results we obtained.
999:11 - We get back to our dashboard
999:13 - and this is what we have.
999:14 - You could go back to the runs
999:16 - and you see that
999:17 - you click on this custom chart right here.
999:19 - No, let's click on this run here first
999:22 - and then we have this two custom charts.
999:24 - So we have this previous
999:26 - confusion matrix which we locked in
999:28 - and then now we have this ROC
999:30 - curve which we just plotted out
999:32 - because we check out the tables
999:34 - for the ROC curve
999:36 - and the confusion matrix.
999:38 - So here we go.
999:39 - We have this ROC plot
999:41 - and then we have this
999:42 - two plots here where
999:44 - one is for the uninfected
999:46 - and the other is for
999:48 - the parasitized class.
999:50 - I haven't seen how to plot the
999:52 - confusion matrix and the ROC curve.
999:54 - We could check out the documentation
999:56 - and look at the other plots.
999:58 - So here we have basic charts,
1000:00 - lines, color,
1000:02 - bar chart, histogram, multiline
1000:04 - and the next
1000:06 - model evaluation charts
1000:08 - like what we've just done, the confusion matrix,
1000:10 - ROC curves, PR curves,
1000:12 - that's precision recall curves.
1000:14 - You have them here, interactive custom
1000:16 - charts, matplotlib
1000:18 - and plotlib plot. So this means
1000:20 - you could actually come up with
1000:22 - your own plot with matplotlib
1000:24 - and then log that
1000:26 - to 1db
1000:28 - and then apart from this
1000:30 - method in which we used the 1db
1000:32 - callback here,
1000:34 - not this one, the 1db
1000:36 - callback we had defined previously,
1000:38 - apart from using that 1db callback
1000:40 - to log the loss, metrics
1000:42 - and other information
1000:44 - we could directly
1000:46 - just do 1db
1000:48 - dot log and then
1000:50 - we say for example we want to log
1000:52 - the loss, so we just put out loss
1000:54 - and then since we got the
1000:56 - loss from this locks, we have this
1000:58 - locks here, we
1001:00 - scroll down and then we put out
1001:02 - your locks, locks
1001:04 - and then we specify the loss.
1001:06 - So this is all what it suffices to
1001:08 - log this loss
1001:10 - values. From here you could also log
1001:12 - the accuracy and all the
1001:14 - metrics.
1001:16 - This actually a dictionary, so let's have that
1001:18 - and there we go.
1001:20 - So we've seen how to log these
1001:22 - values. Now let's also see how
1001:24 - to log images like
1001:26 - with this, we have this image
1001:28 - and then
1001:30 - we log this to tensorboard.
1001:32 - Now we'll see how to
1001:34 - get this image and then log
1001:36 - it to 1db.
1001:38 - To log those images, we'll then check out
1001:40 - your documentation and you'll see
1001:42 - that you could log rich media files
1001:44 - as really any kind of
1001:46 - media file, 3D
1001:48 - point clouds,
1001:50 - molecules, HTML
1001:52 - and histograms.
1001:54 - So that said, you'll
1001:56 - scroll down and you have
1001:58 - this code here which
1002:00 - permits us to log image data.
1002:02 - So here's login arrays
1002:04 - as images, login
1002:06 - PIL images,
1002:08 - login images from files,
1002:10 - image overlays, segmentation
1002:12 - masks, bound in boxes
1002:14 - and so on and so forth. So
1002:16 - you have this, you also have these histograms,
1002:18 - you have 3D visualizations
1002:20 - as you could see, point cloud
1002:22 - and molecules. So you see that
1002:24 - 1db actually permits
1002:26 - just any practitioner
1002:28 - be able to log any
1002:30 - data or any kind of data they really want to
1002:32 - log, hence making them more efficient.
1002:34 - So that said, let's get back to
1002:36 - this here. We're logging arrays. Let's
1002:38 - copy this out. Let's click here.
1002:40 - Just simply copy that way
1002:42 - and then you get back to the code.
1002:44 - So here, what we're saying is, we want
1002:46 - to be able to log
1002:48 - these images at this level. So
1002:50 - let's paste this out somewhere here
1002:52 - and then copy out this
1002:54 - part again. We're going to copy this
1002:56 - part. Recall that
1002:58 - with this 1db we had defined
1003:00 - here, we
1003:02 - kind of like got, we ended at this
1003:04 - level where we got the levels and the predicted
1003:06 - and automatically
1003:08 - we got this ROC curve
1003:10 - and coefficient metrics.
1003:12 - Now what we want to do is
1003:14 - get right up to this point where
1003:16 - we actually
1003:18 - have the image that is our
1003:20 - own image which we've created
1003:22 - and then from this image
1003:24 - log it to 1db
1003:26 - and so right here we have this
1003:28 - piece of code here and
1003:30 - we're going to integrate this.
1003:32 - One thing we could do again,
1003:34 - let's copy this again. No, let's copy this
1003:36 - other one because here we have the
1003:38 - image. So we'll copy this again. We consider that
1003:40 - this is for TensorBoard. Let's
1003:42 - write here TensorBoard,
1003:44 - TensorBoard. Okay,
1003:46 - we have that for TensorBoard. We have this
1003:48 - for 1db. Now
1003:50 - 1db plot and
1003:52 - then this next one is just 1db.
1003:54 - Let's add this here.
1003:56 - There we go. Okay, so here we have
1003:58 - this 1db.
1004:00 - That's fine.
1004:02 - So what we're saying is
1004:04 - we're going to take off all this
1004:06 - here. So we take all this off
1004:08 - and then we
1004:10 - copy this out.
1004:12 - Let's cut that out
1004:14 - and then paste it here.
1004:16 - So there's how we're going to log this to
1004:18 - 1db. As usual
1004:20 - it's going to always be simpler than what we
1004:22 - have with the TensorBoard. So just
1004:24 - all we need to do here is just to take our image
1004:26 - and then log as simple
1004:28 - as that. Now our image array
1004:30 - here. Let's see this image array.
1004:32 - Our image array is this output
1004:34 - which we have and then we pass
1004:36 - this image array into this
1004:38 - caption. Let's put a
1004:40 - caption. Let's say
1004:42 - confusion metrics
1004:44 - for
1004:46 - epoch. And then
1004:48 - we get the epoch from here. Like
1004:50 - we have this epoch. So we're going to be logging
1004:52 - this confusion metrics for each and every epoch.
1004:54 - So we just put out this epoch
1004:56 - here and that's fine. So
1004:58 - that's our caption.
1005:00 - We have 1db image
1005:02 - and we log it in here.
1005:04 - Let's have those, let's say
1005:06 - confusion
1005:08 - metrics.
1005:10 - Yeah,
1005:12 - confusion metrics. Okay, let's have that.
1005:14 - Now we run this.
1005:16 - We copy. Anyway, we had that
1005:18 - already. Let's copy this out.
1005:20 - We have an error
1005:22 - here. This should be closed.
1005:24 - Okay, so that's fine. We have this
1005:26 - 1db log right here
1005:28 - and everything looks okay. Okay,
1005:30 - so let's run that again and
1005:32 - check this out. Looks fine.
1005:34 - Now we could go ahead and start
1005:36 - with the training. Let's run this.
1005:38 - This should be okay now.
1005:40 - And compile the model.
1005:42 - After compile the model, we now
1005:44 - go ahead and train the model.
1005:46 - Let's say we want to have 3
1005:48 - years. Let's keep that aside
1005:50 - for now. And then here we have
1005:52 - this. Let's paste it out. It should be the
1005:54 - same. Okay, here we have it. Now
1005:56 - let's run the training process and
1005:58 - see what we get. Training now complete.
1006:00 - We can get back
1006:02 - to our dashboard and see what we
1006:04 - have as plots.
1006:06 - We'll click on this year.
1006:08 - This run,
1006:10 - current run. We have
1006:12 - this image here we logged
1006:14 - previously. Now let's check in this hidden
1006:16 - panels and see what we get.
1006:18 - It happens that we have this
1006:20 - completion matrix logged in these hidden panels.
1006:22 - Anyways, we have that.
1006:24 - What do you notice? For the 3 different
1006:26 - epochs, you could go to
1006:28 - the second and then go to
1006:30 - the third epoch. Start with the first.
1006:32 - We have the completion matrices which should be
1006:34 - logged per epoch. Now
1006:36 - let's do this so it appears clearer.
1006:38 - And we see we have
1006:40 - this number of true positives
1006:42 - true negatives
1006:44 - false positives false negatives
1006:46 - and so on and so forth. So that's it.
1006:48 - From here you can notice how this value
1006:50 - leaves some 68
1006:52 - to 52.
1006:54 - And then finally we have 53.
1006:56 - Now another interesting
1006:58 - functionality is you could simply
1007:00 - have that. You could download it
1007:02 - and now you have this full screen
1007:04 - and you can see that clearly.
1007:06 - So let's press escape
1007:08 - close and then get back.
1007:10 - So that's it. That's how we
1007:12 - log this image very easily with
1007:14 - 1 dB. Now getting back to the
1007:16 - documentation, you might have noticed that
1007:18 - you have Keras and then you have
1007:20 - TensorFlow somewhere
1007:22 - around. You have TensorFlow
1007:24 - you see that these are kind of like
1007:26 - considered to be two separate libraries.
1007:28 - But then just note
1007:30 - that if you're building Keras
1007:32 - model, that's if you're building this kind of
1007:34 - model where you have
1007:36 - or you're making
1007:38 - use of this method, this feed
1007:40 - method to train your model
1007:42 - then making use of this
1007:44 - Keras documentation
1007:46 - right here is appropriate. But then
1007:48 - sometimes you
1007:50 - want to have
1007:52 - control or full control over what
1007:54 - you're doing and then you want to
1007:56 - be able to do custom training
1007:58 - like we have seen previously
1008:00 - let's scroll down. Okay custom
1008:02 - training loop. So if you're having a custom
1008:04 - training loop like with this year
1008:06 - you would find that
1008:08 - you would not be
1008:10 - able to use the 1 dB callback as
1008:12 - easily as you had done with the
1008:14 - Keras code.
1008:16 - So in that case you'll see
1008:18 - that instead of having like for example like
1008:20 - here we have training block
1008:22 - and then we have this loss
1008:24 - here all you need to do is
1008:26 - come and put in the 1 dB
1008:28 - you log and then
1008:30 - you log the loss. So here you just
1008:32 - say loss and then you simply log
1008:34 - this loss, let's say loss numpy
1008:36 - and that's all. So this way it
1008:38 - takes now to log this loss values
1008:40 - the different metrics
1008:42 - and so on and so forth. The integration
1008:44 - with TensorBoard has been made quite
1008:46 - easy too. So if you're already using TensorBoard
1008:48 - it's easy to integrate with 1 dB
1008:50 - now you're going to
1008:52 - see how 1 dB is different
1008:54 - from TensorBoard. The ability
1008:56 - to reproduce models
1008:58 - automatic organization
1009:00 - fast flexible integration, persistent
1009:02 - centralized dashboard, powerful
1009:04 - tables and
1009:06 - tools for collaboration.
1009:08 - That said let's copy this
1009:10 - out, let's copy this again and now
1009:12 - when we'll be creating our run with the
1009:14 - init method we're going to instead have
1009:16 - this. So let's get back to our
1009:18 - code
1009:20 - let's reduce this year
1009:22 - and then stop this run
1009:24 - so we have this 1 dB
1009:26 - finish we stop the run
1009:28 - the run stopped and then we're going to create
1009:30 - this other new run which will take into
1009:32 - consideration our TensorBoard
1009:34 - logs. So let's
1009:36 - get back to the top
1009:38 - and just right here we'll paste
1009:40 - this out. We've already had
1009:42 - this so we can take this off. Now let's
1009:44 - comment this and then let's
1009:46 - have project entity
1009:48 - just copy that and paste out
1009:50 - here that's fine
1009:52 - configuration we have that
1009:54 - same TensorBoard that's fine
1009:56 - okay let's run this now
1009:58 - we get this error so
1010:00 - let's take this off here
1010:02 - now this isn't compatible
1010:04 - with version 2 of TensorFlow
1010:06 - which is what we're using so
1010:08 - let's run this again and this
1010:10 - now should be fine. We now have created
1010:12 - this new run let's
1010:14 - scroll down and we
1010:16 - get to our custom training loop
1010:18 - so we're going to run this
1010:20 - we have the optimizer, metric
1010:22 - metric validation, IPOCs
1010:24 - number specified
1010:26 - and then let's say we have
1010:28 - this as configuration
1010:30 - and then we have
1010:32 - an epochs
1010:34 - an epochs
1010:36 - okay so that's it
1010:38 - we have all this set and then
1010:40 - we run this
1010:42 - we've seen this already under the section
1010:44 - TensorBoard. We get
1010:46 - this running when using
1010:48 - several event log directories
1010:50 - like what we're doing right here
1010:52 - please call
1010:54 - 1DB TensorBoard patch
1010:56 - and then specify the root log directory
1010:58 - before the 1DB init
1011:00 - so let's take this
1011:02 - from here, copy this out
1011:04 - and even if we check in the documentation
1011:06 - you should have that
1011:08 - so here you have this
1011:10 - the 1DB TensorBoard
1011:12 - patch and then
1011:14 - you have this root directory
1011:16 - right here
1011:18 - so this means again we're going to
1011:20 - stop the current run
1011:22 - where are we exactly
1011:24 - let's stop this current run
1011:26 - it should be at the level of training
1011:28 - we're going to stop this current run
1011:30 - here, we run this
1011:32 - again, we stop that run
1011:34 - and then what we do
1011:36 - now is we have
1011:38 - this here and then we specify
1011:40 - logs
1011:42 - so we have that
1011:44 - root directory specified now
1011:46 - we have the root directory specified
1011:48 - that's logs
1011:50 - documentation, we have that
1011:52 - specified before the init
1011:54 - so we have to run that before the init
1011:56 - looks fine, let's run this
1011:58 - now
1012:00 - let's run this cell
1012:02 - there we go
1012:04 - here we go, TensorBoard
1012:06 - already patched, removed this
1012:08 - TensorBoard true from 1DB init
1012:10 - since we've already done this patch
1012:12 - we should remove this
1012:14 - from 1DB init
1012:16 - let's take that off and run again
1012:18 - we're still getting the error, so what we'll do
1012:20 - is we just get back, and then
1012:22 - have that, and then we'll just
1012:24 - go as we started initially
1012:26 - so let's run this, this should work
1012:28 - now, from here
1012:30 - we're going to continue
1012:32 - what we're about to do
1012:34 - with a custom training process
1012:36 - so let's scroll down
1012:38 - okay, so we are
1012:40 - going to this point, we
1012:42 - have to, anyway we'll run
1012:44 - this already so that's fine, now
1012:46 - let's recommend this one
1012:48 - and then run this
1012:50 - cells, run this
1012:52 - neural and method, and then
1012:54 - start the training process, as you can see
1012:56 - training now completes, let's go ahead and
1012:58 - launch TensorBoard, we have the
1013:00 - loss values and accuracy values
1013:02 - which have been logged in, you can see them
1013:04 - here, let's reduce this, validation
1013:06 - accuracy, and the
1013:08 - validation loss, now
1013:10 - from this, we are now
1013:12 - going to look at
1013:14 - our 1 dB logs, so we
1013:16 - take on this run, and you see
1013:18 - we have this log data in here
1013:20 - you see we have the train, we have
1013:22 - the plot for the loss, we have the
1013:24 - plot for the accuracy, and we have the
1013:26 - plot for the global step
1013:28 - then for the validation, you see we
1013:30 - have this here, we have the
1013:32 - global step, validation loss
1013:34 - and validation accuracy, exactly
1013:36 - what we have with TensorBoard without
1013:38 - adding any extra
1013:40 - line of code for the 1 dB
1013:42 - that is the only thing
1013:44 - we actually did was at this
1013:46 - point here, at this point
1013:48 - here, we said we wanted
1013:50 - to sync TensorBoard, so this is all
1013:52 - we need to sync TensorBoard
1013:54 - with 1 dB
1013:56 - so if you have been using TensorBoard
1013:58 - or if you are using TensorBoard
1014:00 - on a particular project, that's all
1014:02 - you need to log your information
1014:04 - now to 1 dB, apart from
1014:06 - this, you will notice this TensorBoard
1014:08 - here, so you can click on that
1014:10 - we are spinning up your TensorBoard
1014:12 - instance, hang tight, take about 30 seconds
1014:14 - and we will keep it online as
1014:16 - long, before we have been completing the sentence
1014:18 - you have this already, so you see we have
1014:20 - this TensorBoard which has been logged here
1014:22 - we have this PyTorch profiler
1014:26 - and then time series
1014:28 - scalers, just exactly what we have
1014:30 - with TensorBoard, so that sounds cool
1014:32 - we have the logs, we have the
1014:34 - different files which have been saved
1014:36 - system information
1014:38 - and
1014:40 - we have these charts
1014:42 - and then overview, where we could see
1014:44 - all this information and then
1014:46 - get back to our runs, so that's
1014:48 - it, we have seen how to sync
1014:50 - TensorBoard with 1 dB
1014:52 - and so now you are ready
1014:54 - to track your experiments with
1014:56 - 1 dB, thank you for
1014:58 - getting around to this point and see you next time
1015:04 - Hello everyone and
1015:06 - welcome to the session in which we are
1015:08 - going to treat hyperparameter tuning
1015:10 - with Weights & Biases
1015:12 - there are 3 methods available for
1015:14 - us to implement hyperparameter tuning
1015:16 - in Weights & Biases, that is
1015:18 - the random search, the grid search
1015:20 - and the bias and search
1015:22 - method. At the end of the session
1015:24 - you will be able to search for the most suitable
1015:26 - parameter values, which
1015:28 - optimize the accuracy. Previously
1015:30 - we saw how to implement
1015:32 - hyperparameter tuning with
1015:34 - TensorBoard. We created
1015:36 - this model tune method
1015:38 - which takes this argument
1015:40 - HPROUNDS as hyperparameters
1015:42 - which are actually going to be tuned
1015:44 - as you could see here and then
1015:46 - we have as
1015:48 - output of this model
1015:50 - tune method the accuracy
1015:52 - so what we are trying to do here is
1015:54 - we are trying to tune or
1015:56 - we are trying to obtain the optimal
1015:58 - values for these different hyper
1016:00 - parameters which maximize
1016:02 - the accuracy of the model
1016:04 - the exact method we
1016:06 - used was a grid search
1016:08 - with a grid search method
1016:10 - we actually go through each and
1016:12 - every option or each and
1016:14 - every possibility and for
1016:16 - each possibility we are going
1016:18 - to log its accuracy
1016:20 - as we did right here
1016:22 - now with Weights & Biases
1016:24 - we are going to see how to
1016:26 - redo this but
1016:28 - in an easier and more reliable manner
1016:30 - so let's check out another
1016:32 - documentation right here
1016:34 - we have this documentation
1016:36 - here we have hyperparameter tuning
1016:38 - click on the sweep
1016:40 - start which shows
1016:42 - globally what are hyperparameter
1016:44 - tuning with Weights & Biases
1016:46 - we set up Weights & Biases
1016:48 - we configure the sweep
1016:50 - we initialize a sweep
1016:52 - we launch agents
1016:54 - we visualize the results and then finally
1016:56 - stop the agent. From here we will
1016:58 - see how to run these sweeps
1017:00 - in Jupyter. So the Weights &
1017:02 - Biases sweeps allow you
1017:04 - to easily try out a large number
1017:06 - of hyperparameters while
1017:08 - tracking model performance and
1017:10 - logging all the information
1017:12 - you need to reproduce experiments
1017:14 - now we are going to focus on this
1017:16 - pure Python method
1017:18 - where
1017:20 - the sweep configurations are
1017:22 - in the form of a dictionary like this
1017:24 - the way these
1017:26 - sweeps work
1017:28 - the way Weights & Biases sweeps work is
1017:30 - we have a central sweep server
1017:32 - that is this one year
1017:34 - the central sweep server
1017:36 - and then we have
1017:38 - these different agents
1017:40 - right here we could have many more
1017:42 - agents. Then once we
1017:44 - set the configurations
1017:46 - the sweep configurations
1017:48 - in this central sweep server right
1017:50 - here the agents now take
1017:52 - over and do the actual
1017:54 - hyperparameter tuning
1017:56 - that is a search for the best
1017:58 - hyperparameters which
1018:00 - help in optimizing the model
1018:02 - and this parallel method
1018:04 - of implementing sweeps in
1018:06 - Weights & Biases help make
1018:08 - the hyperparameter tuning process
1018:10 - even more efficient. As you could see
1018:12 - here this could be done in
1018:14 - just two steps. Initialize
1018:16 - the sweep and
1018:18 - put all necessary information
1018:20 - or give all necessary information
1018:22 - for this central
1018:24 - or to this central sweep server
1018:26 - and then we run
1018:28 - these different agents right
1018:30 - here. Now that's set we have
1018:32 - the sweep configuration which looks
1018:34 - similar to what we had
1018:36 - when we were working with
1018:38 - TensorBoard. So here we
1018:40 - have this configuration
1018:42 - right here
1018:44 - where we specify for
1018:46 - each hyperparameter
1018:48 - the values
1018:50 - you can take given that we're having
1018:52 - a grid search or making use of a grid
1018:54 - search algorithm
1018:56 - or specifying simply all those
1018:58 - different values and that was it.
1019:00 - Now coming back to Weights & Biases
1019:02 - you see here you just
1019:04 - need to specify for example
1019:06 - the hyperparameter, that number of
1019:08 - epochs, give the values, learning rate
1019:10 - for example, you give the minimum
1019:12 - and maximum value
1019:14 - and so on and so forth.
1019:16 - But then note that here
1019:18 - in this example
1019:20 - given the documentation, the method
1019:22 - you're using is not a grid search. That is
1019:24 - you're not going through each and every
1019:26 - possibility in the list of values.
1019:28 - What you're doing here is actually a
1019:30 - random search which happens to be
1019:32 - more efficient than the grid search
1019:34 - algorithm. Since
1019:36 - it's possible in a shorter period of time
1019:38 - to get very
1019:40 - optimal values of the
1019:42 - hyperparameters
1019:44 - as compared to the grid search
1019:46 - algorithm where you need to go through each and
1019:48 - every value.
1019:50 - Now let's get back here. We have
1019:52 - the name, we have the method
1019:54 - and we have the parameters. To
1019:56 - understand where all this comes from
1019:58 - check out in this sweep configuration
1020:00 - right here. So clicking on the sweep
1020:02 - configuration we have the structure
1020:04 - of a sweep configuration
1020:06 - then here we have the different
1020:08 - keys and their descriptions.
1020:10 - You'll notice this method
1020:12 - which we had seen here.
1020:14 - Let's open this another tab, open
1020:16 - a new tab and have that.
1020:18 - So let's have it this way.
1020:20 - There we go. We have this name
1020:22 - method and parameters
1020:24 - which we could see here.
1020:26 - We have name, method
1020:28 - parameters
1020:30 - and then for each of these configuration
1020:32 - keys like this method
1020:34 - for example, there is this
1020:36 - more explicit description.
1020:38 - So here we have the method
1020:40 - you're going to use. If it's a grid
1020:42 - grid search iterates over all possible
1020:44 - combinations of parameter values
1020:46 - the random search chooses a random
1020:48 - set of values on each iteration
1020:50 - and weights and biases also comes
1020:52 - with this other method
1020:54 - which is the bias method.
1020:56 - This bias in hyperparameter
1020:58 - search method uses a Gaussian process
1021:00 - to model a relationship between the parameters
1021:02 - and the model metric
1021:04 - and chooses parameters to optimize
1021:06 - the probability of improvement.
1021:08 - The strategy requires the metric key to be
1021:10 - specified.
1021:12 - Here we see now we have these three
1021:14 - different methods and that is why
1021:16 - when we wanted to
1021:18 - work with a random it suffices to
1021:20 - just put this out here.
1021:22 - Either you put it random or you put grid
1021:24 - or you put bias. Now we move
1021:26 - to the next. We have the parameters.
1021:28 - We have the parameters.
1021:30 - Anyway the name, you could
1021:32 - put the name for the sweep.
1021:34 - That depends on you. Here is
1021:36 - my sweep. Then the parameters
1021:38 - here it gets a little bit more
1021:40 - tricky because here you have
1021:42 - this here
1021:44 - this key and then the value
1021:46 - is a dictionary which itself
1021:48 - is having its own keys
1021:50 - and values. We have
1021:52 - these parameters. Let's get
1021:54 - back to parameters here.
1021:56 - We have the parameters. Where are we?
1021:58 - We have parameters and then
1022:00 - we have these different
1022:02 - possible values and the descriptions.
1022:04 - Getting back here
1022:06 - you will notice that this hyperparameter
1022:08 - epochs takes values in this
1022:10 - list that is added
1022:12 - 10, 20 or 50 while the
1022:14 - learning rate can be chosen
1022:16 - between 0.001
1022:18 - 0.0001
1022:20 - and the maximum
1022:22 - 0.1. Each
1022:24 - hyperparameter has
1022:26 - its own distinct way of
1022:28 - describing the values it can take.
1022:30 - Now getting back here
1022:32 - we have
1022:34 - different parameters
1022:36 - and then for these values
1022:38 - you can say
1022:40 - a value.
1022:42 - Sometimes
1022:44 - here we may decide and say
1022:46 - we want this epoch to take only just one
1022:48 - value. There we just have value
1022:50 - and then in the case where we want
1022:52 - several values you see it specifies
1022:54 - all values for this hyperparameter
1022:56 - compatible with grid and all of that.
1022:58 - Now here in some cases
1023:00 - you have a distribution
1023:02 - and this selects a distribution
1023:04 - from the distribution table
1023:06 - below. So you can see here
1023:08 - this specifies how values will be
1023:10 - distributed if they are selected randomly
1023:12 - e.g. with the random or biased methods
1023:14 - so when working with the
1023:16 - random or biased methods
1023:18 - you may want to
1023:20 - select your
1023:22 - values based on the given
1023:24 - distribution and your other list of distributions
1023:26 - you can use. You have constant, categorical
1023:28 - int, uniform
1023:30 - uniform distribution, cure uniform
1023:32 - log uniform, cure
1023:34 - log uniform, normal distribution
1023:36 - cure normal, log normal
1023:38 - and cure log normal.
1023:40 - Then from this distribution we now move to
1023:42 - min-max. Min-max is what you actually
1023:44 - saw here
1023:46 - here you had this min-max
1023:48 - so here you are simply
1023:50 - saying you want your learning rate to
1023:52 - to have a minimum value
1023:54 - of this and a maximum value of that
1023:56 - so we randomly pick values between
1023:58 - this range
1024:00 - and that's it. You have
1024:02 - MU, MU mean parameter
1024:04 - for normal or log normal
1024:06 - distributed hyperparameters. So here you
1024:08 - have a
1024:10 - normally distributed hyperparameters
1024:12 - and you are specifying the mean
1024:14 - while here you are specifying the standard
1024:16 - deviation. Then for cure
1024:18 - you have the quantization step size
1024:20 - for quantized hyperparameters
1024:22 - Our next key is the metric
1024:24 - and with the metric
1024:26 - we have to define the name of the
1024:28 - metric, the goal, and
1024:30 - the target. Now here
1024:32 - you could have for example like
1024:34 - your validation loss
1024:36 - you have this metric validation
1024:38 - loss. You could say you want to
1024:40 - optimize your parameters or you want to
1024:42 - choose the hyperparameters
1024:44 - which minimize the
1024:46 - validation loss. Now you could
1024:48 - also change this into an
1024:50 - accuracy. Let's say validation accuracy or
1024:52 - just the accuracy or just the trained accuracy
1024:54 - in that case you would want to
1024:56 - choose the hyperparameters which
1024:58 - maximize the accuracy
1025:00 - and hence here at the level of the
1025:02 - goal you either minimize or
1025:04 - maximize. The default is
1025:06 - minimize. So if you have a validation loss
1025:08 - it's needless specifying the goal because by
1025:10 - default it's minimized
1025:12 - and for now as
1025:14 - this documentation there's
1025:16 - really no automatic way of
1025:18 - deciding whether it's minimize
1025:20 - or maximize. Anyway
1025:22 - you can always do that
1025:24 - manually by specifying.
1025:26 - Now the next we have
1025:28 - is a target. So here with the
1025:30 - target as you could see here for
1025:32 - example 0.95
1025:34 - it means that if you happen to
1025:36 - get a set of hyperparameters
1025:38 - which permit you to get
1025:40 - this validation
1025:42 - accuracy to this target value
1025:44 - then at that point you will
1025:46 - stop the process of
1025:48 - searching for the
1025:50 - optimal hyperparameters
1025:52 - and what happens
1025:54 - exactly is all agents
1025:56 - with active runs will finish
1025:58 - their jobs but
1026:00 - no new runs will be launched in
1026:02 - the sweep since we've already
1026:04 - attained the objective.
1026:06 - From here we have
1026:08 - early terminate which
1026:10 - is an optional feature that's a piece of hyperparameters
1026:12 - searched by stopping poorly
1026:14 - performing runs. We get back
1026:16 - here, copy out this code
1026:18 - just copy it, okay copy it
1026:20 - we get back now to
1026:22 - hyperparameter tuning
1026:24 - we paste out the code here
1026:26 - and then we just going to
1026:28 - try to replicate what we have done
1026:30 - already with TensorBoard
1026:32 - so here we paste this
1026:34 - out here and then
1026:36 - we have this numUnits1
1026:38 - we have this numUnits1
1026:40 - this here and have
1026:42 - that numUnits1 values
1026:44 - there we go let's copy
1026:46 - this out and paste here
1026:48 - so we replace these values
1026:50 - which we had already
1026:52 - and then the next will be numUnits2
1026:54 - it's kind of similar so we should
1026:56 - numUnits2
1026:58 - so here we should just
1027:00 - copy this and paste
1027:02 - numUnits2
1027:04 - values
1027:06 - it's kind of similar, okay we have that
1027:08 - and then there we go
1027:10 - so we have numUnits1, numUnits2
1027:12 - and then the next
1027:14 - drop out rate
1027:16 - we're going to have learning rate so let's just have
1027:18 - the drop out rate here so we have
1027:20 - the drop out rate
1027:22 - and this drop out rate
1027:24 - we take values
1027:26 - 0.1 to 0.3
1027:28 - now what we're going to use is like
1027:30 - this min max so let's just copy this out
1027:32 - here and then
1027:34 - let's copy this out
1027:36 - and then paste it out here
1027:38 - okay so we have drop out
1027:40 - rate but here we're going to go from 0.1
1027:42 - to 0.4
1027:44 - let's say 0.4
1027:46 - and this random actually is not agreed
1027:48 - so we have that and then here
1027:50 - okay name, lesson, name
1027:52 - malaria
1027:54 - prediction, sweep
1027:56 - okay
1027:58 - mental random parameters is fine
1028:00 - so we're getting each and every parameter now
1028:02 - we have the drop out rate set
1028:04 - we now move to regularization
1028:06 - rate
1028:08 - regularization rate
1028:10 - 0.001 and 0.1
1028:12 - okay so
1028:14 - we have 0.01, 0.1
1028:16 - and then what we could do now
1028:18 - is we make use of
1028:20 - distribution and then we have
1028:22 - your uniform so we specify
1028:24 - that we want to make use of
1028:26 - your uniform distribution and then
1028:28 - here so we're going to
1028:30 - use the same again here for
1028:32 - the learning rate distribution
1028:34 - uniform
1028:36 - there we go
1028:38 - we have that uniform and that's fine
1028:40 - here we're going
1028:42 - to go from this let's say
1028:44 - 1e
1028:46 - negative 4
1028:48 - min and then max
1028:50 - 1e negative 2
1028:52 - 1e negative 2 okay
1028:54 - so looks good
1028:56 - let's take this off now
1028:58 - let's take off what we've set here
1029:00 - and then we have that so from this we'll
1029:02 - be able to create now this
1029:04 - sweep id we'll get our sweep id
1029:06 - by running this 1db sweep
1029:08 - and passing in the sweep configuration
1029:10 - to run an agent
1029:12 - the first step is we're going to
1029:14 - define a function to run the training
1029:16 - based on those hyper parameters
1029:18 - and then we're going to pass that function
1029:20 - with the sweep id here
1029:22 - in this
1029:24 - 1db agent
1029:26 - method so now let's copy
1029:28 - this code out and then paste
1029:30 - it out here
1029:32 - paste it out here and you'll
1029:34 - also notice that this is kind of like
1029:36 - similar to what we had done here because
1029:38 - here we had defined
1029:40 - this method
1029:42 - which takes in the hyper parameters
1029:44 - which we're trying to
1029:46 - tune and then we go
1029:48 - through this
1029:50 - method here for each
1029:52 - and every sweep
1029:54 - now let's get back here we have the model
1029:56 - we could simply make use
1029:58 - of this model so let's copy
1030:00 - out some part let's just copy
1030:02 - out this here let's copy
1030:04 - that out and paste
1030:06 - this here paste it out
1030:08 - and we have model
1030:10 - tune
1030:12 - the net model which we create
1030:14 - here hyper parameters
1030:16 - and love that and then instead
1030:18 - of what we had here where we
1030:20 - pass the we had the compile
1030:22 - and the feed method
1030:24 - what will return here
1030:26 - will be just this model so we
1030:28 - return the net model
1030:30 - right here so let's have this
1030:32 - new net
1030:34 - model let's take out
1030:36 - this part of the code and that's fine
1030:38 - okay so we have this
1030:40 - model tune here
1030:42 - instead of make model we have
1030:44 - model tune and
1030:46 - we'll pass the configuration
1030:48 - in here we add
1030:50 - the project and the entity in our
1030:52 - init method
1030:54 - then we've modified this
1030:56 - keys right here to match with
1030:58 - those of our 1db
1031:00 - configurations which we have seen already
1031:02 - let's get back here let's just
1031:04 - copy this and put right here
1031:06 - so you could see that so
1031:08 - as you could see instead of number of units 1
1031:10 - we have number of dense 1
1031:12 - and here we have number of dense 2
1031:14 - so that is it we
1031:16 - have the different values you could take
1031:18 - we have the dropout
1031:20 - the regularization rate
1031:22 - and the learning rate
1031:24 - now right
1031:26 - here in this model this our model
1031:28 - we should call model tune we shall
1031:30 - make use of this 1db
1031:32 - configurations which we've already
1031:34 - passed in here and so here
1031:36 - we have config
1031:38 - and
1031:40 - config and we're going to do
1031:42 - the same for all these other
1031:44 - different hyper parameters
1031:46 - like here
1031:48 - we'll have number of filters
1031:50 - here config
1031:52 - number of filters
1031:54 - and then you should note that
1031:56 - in this example we are
1031:58 - not going to turn all the hyper
1032:00 - parameters so the
1032:02 - hyper parameters which we want to turn should be
1032:04 - in this parameter
1032:06 - or parameters
1032:08 - dictionary in
1032:10 - our sweep configuration
1032:12 - the method is random
1032:14 - and the metric is the accuracy
1032:16 - with the goal of
1032:18 - maximizing it so
1032:20 - if you want to tune any parameter or
1032:22 - hyper parameter you put that in here
1032:24 - for the rest we're just going to use
1032:26 - this configuration
1032:28 - which we've set already so it'll just have some
1032:30 - fixed values so
1032:32 - that said we would modify
1032:34 - all this
1032:36 - and there we go so we now have all
1032:38 - these different modifications
1032:40 - run this, run this
1032:42 - and then back here
1032:44 - on this train
1032:46 - we could now replace all
1032:48 - this with the compilation
1032:50 - and the
1032:52 - fit method
1032:54 - now again here we'll
1032:56 - have this learning rate so we
1032:58 - have learning
1033:00 - rate
1033:02 - and then
1033:04 - we could modify
1033:06 - this number of epochs to say
1033:08 - three or actually the config
1033:10 - so we have config
1033:12 - number of epochs
1033:14 - and you should note that we're carrying this
1033:16 - only on the validation set so
1033:18 - you could try this
1033:20 - on the full training data set that would
1033:22 - take much more time and also
1033:24 - you could feel free to
1033:26 - carry out this hyper parameter
1033:28 - tuning on more
1033:30 - parameters so
1033:32 - you could take up say
1033:34 - the image size like this
1033:36 - parameter here, you could add that up
1033:38 - and then see
1033:40 - how this image size
1033:42 - affects the model performance
1033:44 - so that said
1033:46 - we have all this already
1033:48 - we have our agent which
1033:50 - takes the sweep ID which we've
1033:52 - defined already, we have the function
1033:54 - which is the string right here
1033:56 - and then we have this count which is the number
1033:58 - of runs to execute. Before we proceed
1034:00 - note that we specified
1034:02 - this configuration which is essentially
1034:04 - this configuration we have here so we could
1034:06 - just take this off and save it, this is our
1034:08 - configuration, ok
1034:10 - so that's our configuration
1034:12 - and now we
1034:14 - are ready to
1034:16 - let our agents do their job
1034:18 - so let's run this
1034:20 - now after training runs
1034:22 - here's what we get, start from here
1034:24 - you see that it starts by
1034:26 - picking the drop array, the learning array
1034:28 - the number of dense 1
1034:30 - number of dense 2 and then
1034:32 - the regularization rate
1034:34 - while obviously keeping the other
1034:36 - parameters constant
1034:38 - here we have this loss and
1034:40 - its corresponding accuracy
1034:42 - and you could check out for the other sweeps
1034:44 - down here, you see we have
1034:46 - 20 different
1034:48 - runs and
1034:50 - we could get right here
1034:52 - you see and we click on
1034:54 - this to view the sweep
1034:56 - and now on this page
1034:58 - you could see the different runs we have here
1035:00 - from 1 up to 20
1035:02 - the epochs or rather the
1035:04 - run, the accuracy
1035:06 - and its corresponding loss and then
1035:08 - we'll skip
1035:10 - this tool for now, let's look at this
1035:12 - we'll go ahead and check
1035:14 - out the hyperparameters which
1035:16 - produce the highest accuracy which is
1035:18 - this here, let's
1035:20 - highlight that, we have this
1035:22 - well let's pick this out from here
1035:24 - there we go
1035:26 - we have this, we
1035:28 - have this, we have
1035:30 - this
1035:32 - that comes down here, we have
1035:34 - this and then we have
1035:36 - this
1035:38 - so here we have the hyperparameter values
1035:40 - which give us the highest
1035:42 - accuracy score
1035:44 - then coming into this
1035:46 - let's take this off
1035:48 - we could check out these different parameters
1035:50 - or parameter importance
1035:52 - with respect to the accuracy
1035:54 - so you see that the most important is the learning rate
1035:56 - followed by the run time
1035:58 - number of dense 2
1036:00 - dropout rate,
1036:02 - regularization rate, number of dense 1
1036:04 - and here we have
1036:06 - this other hyperparameters
1036:08 - which we did not modify
1036:10 - now let's
1036:12 - click on this here and see what we have
1036:14 - we told automatically
1036:16 - shows the most
1036:18 - useful parameters, so
1036:20 - let's check this out
1036:22 - you see that after clicking on that
1036:24 - the other fixed hyperparameters
1036:26 - and even the run time
1036:28 - is taken off, so we left with only
1036:30 - this hyperparameters which we had fixed
1036:32 - or better still which we had
1036:34 - put in this slip configuration
1036:36 - then one other
1036:38 - point you could note from here
1036:40 - is this correlation
1036:42 - so apart from importance we could check out
1036:44 - the correlation and here we
1036:46 - told that this learning rate has a
1036:48 - negative correlation because you could
1036:50 - see with the red, the red
1036:52 - indicates negative correlation while the green
1036:54 - indicates positive correlation
1036:56 - so you could check out the value, you see this is 0.051
1036:58 - while this is negative 3.53
1037:00 - now what this
1037:02 - means is the lower the learning
1037:04 - rate then the higher the accuracy
1037:06 - and then in this plot to the left
1037:08 - we have the accuracies
1037:10 - for each and every
1037:12 - run
1037:14 - so that's it for this section, thank you for getting
1037:16 - up to this point and see you next time.
1037:46 - So we are getting into this data
1037:48 - here and then this
1037:50 - other preprocessed data
1037:52 - or this preprocessed version
1037:54 - is again preprocessed
1037:56 - to give us this one
1037:58 - and then finally, we have this
1038:00 - last step, this last preprocessed
1038:02 - step which gives us
1038:04 - this data set version
1038:06 - which happens to be an augmented
1038:08 - data set version
1038:10 - so this shows us that
1038:12 - if at any point in time we are
1038:14 - identified with this preprocessing
1038:16 - which was done right here to produce
1038:18 - this data set version
1038:20 - we could simply preprocess
1038:22 - from or making
1038:24 - use of this data set version
1038:26 - right here and so this greatly
1038:28 - simplifies our data set
1038:30 - management when working
1038:32 - in our different machine learning project
1038:34 - just as gate pyramids us
1038:36 - do code versioning, weighting
1038:38 - biases gives us the
1038:40 - possibility of doing
1038:42 - data set versioning and model versioning
1038:44 - and this can be done using
1038:46 - weights and biases artifacts
1038:48 - which help us save and organize
1038:50 - machine learning data sets throughout
1038:52 - a project's life cycle. We are going to
1038:54 - start with the data set versioning
1038:56 - and the most common ways in which
1038:58 - weights and biases artifacts have been used
1039:00 - for data versioning are to version
1039:02 - data seamlessly, pre-package
1039:04 - data splits like
1039:06 - training, validation, and test sets
1039:08 - iteratively refine
1039:10 - data sets, juggle multiple
1039:12 - data sets, and finally
1039:14 - visualize and share a data workflow
1039:16 - before getting into seeing
1039:18 - how artifacts could be used in
1039:20 - data set versioning
1039:22 - let's look at this simple example
1039:24 - in order to obtain the
1039:26 - malaria data set we have to start by
1039:28 - loading this data. We have
1039:30 - this data loader which is represented
1039:32 - by the square and now once we've
1039:34 - loaded this data we have
1039:36 - our data set so
1039:38 - let's say we have this original
1039:40 - data set right here
1039:42 - we now go ahead
1039:44 - to split this data
1039:46 - so from here we can split
1039:48 - up this data and then have
1039:50 - the three different parts
1039:52 - we have the train data, the
1039:54 - validation data, and the
1039:56 - test data. At this point
1039:58 - all three data sets
1040:00 - have been passed in this
1040:02 - pre-processing units right here
1040:04 - so we pre-process the train
1040:06 - pre-process the validation and pre-process
1040:08 - the test and this
1040:10 - gives us an output here. So for the train
1040:12 - we have a pre-processed
1040:14 - training data. Here we
1040:16 - have this pre-processed training data
1040:18 - we have this pre-processed
1040:20 - validation data and
1040:22 - here we have also this
1040:24 - pre-processed test data
1040:26 - again at this point we are
1040:28 - now going to carry out
1040:30 - augmentation on this pre-processed
1040:32 - training data. This PTRD
1040:34 - means pre-processed
1040:36 - training data
1040:38 - and then pre-processed validation data
1040:40 - pre-processed testing data
1040:42 - ok so we have that and then
1040:44 - we pass this through
1040:46 - this augmentation
1040:48 - process. So we have
1040:50 - this process was rolled
1040:52 - with that of carrying out data augmentation
1040:54 - on this data set
1040:56 - to produce another
1040:58 - data set
1041:00 - which is actually now
1041:02 - an augmented version of this
1041:04 - so here we have pre-processed
1041:06 - training data. Now we have
1041:08 - augmented training
1041:10 - data and this is kind of like
1041:12 - the life cycle of our
1041:14 - data set and this particular problem
1041:16 - in which we are working on. Imagine that
1041:18 - this original data set
1041:20 - contained
1041:22 - mislevelled examples
1041:24 - in that case what you
1041:26 - want to do is now to
1041:28 - carry out this level incorrectly
1041:30 - such that we have
1041:32 - this data set here
1041:34 - which now has been cleaned.
1041:36 - Overall this leads to a
1041:38 - high level of accountability
1041:40 - as when working on a
1041:42 - team and let's say you have done
1041:44 - some modification, let's say you have cleaned
1041:46 - the data, all the people in your team
1041:48 - can now view this
1041:50 - cleaned data set and decide
1041:52 - whether to modify it
1041:54 - delete it or keep
1041:56 - using it. So if we suppose
1041:58 - the team accepts this
1042:00 - cleaned data set and everyone is happy
1042:02 - with this newly cleaned data
1042:04 - set, we now see that instead of passing
1042:06 - this directly into the split, we will
1042:08 - now pass this one into the
1042:10 - split. So we'll have
1042:12 - something like this, we'll go this way, let's take this
1042:14 - off. In that
1042:16 - case we will have to go this way
1042:18 - we go this way
1042:20 - this and then
1042:22 - into the split. And then talking
1042:24 - about data set versioning
1042:26 - for each and every data set
1042:28 - we've created here, like for this
1042:30 - one, this one, this
1042:32 - this, this, this, this, or this one
1042:34 - we have different versions
1042:36 - so we could have for example
1042:38 - this PVD here, that is the
1042:40 - preprocessed validation
1042:42 - data and
1042:44 - it could have a version 0
1042:46 - or version 0. So we have
1042:48 - the version 0, right, yeah
1042:50 - and then later
1042:52 - on you may modify this
1042:54 - preprocessing and it leads us
1042:56 - to have another version of this
1042:58 - validation data. So you could
1043:00 - have another version, version 1
1043:02 - and the version, version 2
1043:04 - and the version, you could say
1043:06 - version best, you could have
1043:08 - version latest
1043:10 - and so on and so
1043:12 - forth. And a good
1043:14 - thing is when weights and bias
1043:16 - or when using weights and bias
1043:18 - to store this data
1043:20 - or when we using weights and bias
1043:22 - artifacts
1043:24 - the data is taught in such a
1043:26 - way that if we have
1043:28 - data in this original data
1043:30 - set, which is exactly the same
1043:32 - as what we have in this preprocessed
1043:34 - validation data
1043:36 - then that data
1043:38 - wouldn't be duplicated. Now it's true
1043:40 - that if we preprocess this data
1043:42 - obviously we'll have all
1043:44 - the data set or all the
1043:46 - elements of the data set changing. Now
1043:48 - let's take this example for the cleaned
1043:50 - data set. So here we have
1043:52 - this original data set. Let's suppose
1043:54 - we have say we have
1043:56 - 10, we have 10 different
1043:58 - examples, different
1044:00 - samples in this original data
1044:02 - set and then after
1044:04 - cleaning our data set only
1044:06 - two of these samples have been modified.
1044:08 - So we have 8
1044:10 - unmodified and 2
1044:12 - modified. So we have
1044:14 - modified. We have changed the levels
1044:16 - of these two samples here.
1044:18 - What weights and bias will do is
1044:20 - it will ensure that
1044:22 - this 8 year
1044:24 - aren't duplicated. That is
1044:26 - we don't create extra space for those 8
1044:28 - orders which haven't changed
1044:30 - from this previous original data
1044:32 - set here. And so in that case
1044:34 - we're going to only
1044:36 - store these two new
1044:38 - samples here while weights and
1044:40 - biases keep track of the fact that
1044:42 - these other 8 samples
1044:44 - haven't been modified and so
1044:46 - don't necessarily need to occupy
1044:48 - extra space in the
1044:50 - storage unit which
1044:52 - weights and biases makes available for
1044:54 - us for free. That said
1044:56 - we could look at those different processes
1044:58 - right here. Those processes
1045:00 - in the square
1045:02 - boxes as 1 dB
1045:04 - runs while
1045:06 - these different forms
1045:08 - and even the versions which
1045:10 - are data takes are the
1045:12 - artifacts. And so we could consider
1045:14 - those key here where this represents
1045:16 - the artifacts and the
1045:18 - runs. Also we see that
1045:20 - the artifacts are connected
1045:22 - together by these different
1045:24 - runs. So these
1045:26 - two artifacts, this
1045:28 - trained data and the original data set
1045:30 - has been connected together
1045:32 - by the split run. And
1045:34 - then these two PTRD
1045:36 - and trained data
1045:38 - are connected together
1045:40 - by the pre-processing run.
1045:42 - Getting back to the documentation, we'll see
1045:44 - how we create this
1045:46 - artifacts here which is called
1045:48 - the new data set
1045:50 - of type raw data and then
1045:52 - this is created within
1045:54 - this run right here.
1045:56 - So we'll see how we create this run and
1045:58 - specify the project, my project and
1046:00 - once we create this artifact
1046:02 - we're going to add data into it.
1046:04 - Now one thing you could do with
1046:06 - 1 dB is you could
1046:08 - simply add a whole directory.
1046:10 - We're supposing that all your data is
1046:12 - in a given directory and so you just all you
1046:14 - need to do is specify this path and then
1046:16 - you make this data
1046:18 - part of this 1 dB
1046:20 - artifacts which we've called my
1046:22 - data and then finally you
1046:24 - log this artifact to
1046:26 - 1 dB. Let's copy out this
1046:28 - sample code, paste it out here.
1046:30 - We have that sample code
1046:32 - and then we could get started with
1046:34 - our data set versioning. Now
1046:36 - we are going to put this in a width statement
1046:38 - so we have here width
1046:40 - 1 dB in it and then we specify
1046:42 - the project, project we're working on
1046:44 - which is this malaria detection
1046:46 - project, the entity you will learn
1046:48 - and that's it. So we
1046:50 - have this here and then we are
1046:52 - going to create our original data.
1046:54 - So we have original data
1046:56 - there we go. We are going to create
1046:58 - this artifact actually. We have
1047:00 - that original data, 1 dB artifact
1047:02 - new data set type raw data.
1047:04 - Now to check out the different
1047:06 - arguments we could pass in here
1047:08 - get back to documentation
1047:10 - let's check out here and then
1047:12 - you could scroll down. So here you
1047:14 - have this references
1047:16 - let's reduce this and
1047:18 - have this clearly. We have references
1047:20 - on the references you have this Python
1047:22 - library and then you have 1 dB
1047:24 - artifact so you could click on this
1047:26 - and what do you get? You have this
1047:28 - documentation right here. So we have
1047:30 - the different arguments name, type, description
1047:32 - metadata, incremental
1047:34 - and use us. Now note that
1047:36 - this 4 are optional
1047:38 - so that's why
1047:40 - in the example we just had the name and the
1047:42 - type. We'll now add up this
1047:44 - here. We have name
1047:46 - and then type
1047:48 - and we have description with
1047:50 - metadata. So let's have this description
1047:52 - right here. Description
1047:54 - simply we could say malaria
1047:56 - data set or tensorflow
1047:58 - tensorflow
1048:00 - malaria data set
1048:02 - so that's it. For the description it's actually
1048:04 - a string and then we have this metadata
1048:06 - which is this
1048:08 - dictionary which contains
1048:10 - information related to our data set
1048:12 - so here we define this dictionary
1048:14 - we're going to start for example the source
1048:16 - so we could have your
1048:18 - source
1048:20 - and then we say
1048:22 - tf.data.set. Okay
1048:24 - so we have that source. We could also add
1048:26 - other information from this homepage
1048:28 - so let's even copy out this
1048:30 - description here and then
1048:32 - paste it out here. So
1048:34 - in place of this description
1048:36 - we have that and there we
1048:38 - go. That's fine. Now we check
1048:40 - out on the homepage
1048:42 - we have homepage source code. Let's
1048:44 - copy this out and paste out
1048:46 - here and now we have all this
1048:48 - necessary metadata information
1048:50 - so here is it. We've created this
1048:52 - artifact. We then paste out this
1048:54 - code which we had seen previously
1048:56 - and which premiers us load
1048:58 - the malaria data set from
1049:00 - TensorFlow data set and then we'll
1049:02 - save this data set
1049:04 - in the NumPy compressed
1049:06 - format. So with our
1049:08 - original data
1049:10 - which is this one here, this
1049:12 - artifact.new file
1049:14 - new file
1049:16 - so unlike here where we add directory
1049:18 - here we add in our
1049:20 - we created this new file which now
1049:22 - contains this data set
1049:24 - so with original data
1049:26 - data.new file we'll
1049:28 - call it original
1049:30 - original data
1049:32 - .npz
1049:34 - so that's our file name and then
1049:36 - the mode is going to be
1049:38 - a write mode and we have
1049:40 - this as file. So we
1049:42 - have this artifact. We
1049:44 - add this new file with a file
1049:46 - name and then we save this file
1049:48 - while putting in the
1049:50 - appropriate content. So right here we
1049:52 - have np save
1049:54 - that's our compressed format
1049:56 - call here we have npz
1049:58 - and then here we have
1050:00 - this file and then what
1050:02 - we pass in is our
1050:04 - data set which is this one
1050:06 - and so at this point we've read in
1050:08 - our information or
1050:10 - this data set in this artifact
1050:12 - and then we're now ready
1050:14 - to log this. So here
1050:16 - we have rounded log
1050:18 - artifact. This basically
1050:20 - this here. So let's
1050:22 - do this. Let's take this off.
1050:24 - We have
1050:25 - there we go. We have rounded log
1050:27 - artifact and then what we pass in here
1050:29 - is original
1050:31 - data. Okay. So that's it.
1050:33 - So now we've seen how to
1050:35 - create this run and
1050:37 - in this run we have this
1050:39 - artifact and then we
1050:41 - put in the information in the artifact.
1050:43 - Now one thing we could do is put
1050:45 - all this in a method. So we'll
1050:47 - define the method load
1050:49 - original
1050:51 - data. So we load original
1050:53 - data and simply
1050:55 - does it. So let's send this
1050:57 - one step and that's fine. There we go.
1050:59 - So we have that load original
1051:01 - data method defined. Let's add this
1051:03 - code cell and then we can call it
1051:05 - right here. So we can call load
1051:07 - original data
1051:09 - and there we go. So let's get
1051:11 - back to this diagram we had previously. You
1051:13 - see that in this diagram if we
1051:15 - take off. If you don't
1051:17 - consider this path that this information
1051:19 - flows this way and then gets
1051:21 - pleats and so on and so forth. So
1051:23 - here what we have is
1051:25 - we've had this load original
1051:27 - data method which takes into consideration
1051:29 - this tool. Here we have this run
1051:31 - which is the data loader. So we have
1051:33 - the little run and then
1051:35 - what it does is it
1051:37 - recuperates this original
1051:39 - data set from TensorFlow data sets
1051:41 - and then produces an artifact
1051:43 - which is this one now. So the
1051:45 - artifact we have just created here. This
1051:47 - artifact original data
1051:49 - is this which we
1051:51 - had drawn previously here. We then
1051:53 - run this cell and
1051:55 - load the original data. We get
1051:57 - this output. Unfortunately this
1051:59 - process has failed. Let's check out
1052:01 - the reason why this failed.
1052:03 - Cannot convert a tensor of d-type
1052:05 - variants to a non-py array.
1052:07 - Now
1052:09 - since this variable
1052:11 - data set we have here is
1052:13 - of d-type variant. What we're going
1052:15 - to do is we're going to take out
1052:17 - each and every element of
1052:19 - this data set
1052:21 - and save it
1052:23 - in a directory. So from
1052:25 - here we're going to copy out
1052:27 - this code. Let's copy out this code
1052:29 - and paste it out here. Click back. Now
1052:31 - what we're having here is we're going to
1052:33 - go through this data set.
1052:35 - So we have for
1052:37 - dean data set or for
1052:39 - data in
1052:41 - data set. Now we have
1052:43 - a list here and then we can pick
1052:45 - out the zero element. So pick out the
1052:47 - zero element and then for that
1052:49 - we're going to create this folder data set
1052:51 - right here. So let's have this new
1052:53 - folder and call
1052:55 - it data set. Okay so we
1052:57 - have that new folder data set. In
1052:59 - that folder data set we're going to put
1053:01 - in these different tensors which are
1053:03 - going to be stored in this non-py
1053:05 - compressed format.
1053:07 - So that's it. We have
1053:09 - this. We modified the name. So let's say
1053:11 - we have Malaria
1053:13 - Malaria data
1053:15 - set and then we give it
1053:17 - a number. So let's have
1053:19 - that plus
1053:21 - we'll give it a number plus
1053:23 - this. So here now we have this
1053:25 - K. We've initialized K and then
1053:27 - we're going to continually be
1053:29 - incrementing this K. So here we have
1053:31 - Malaria data set. We have
1053:33 - K and then we have
1053:35 - this extension right here.
1053:37 - So that's how we're going to save this
1053:39 - file and then the data we're going to be
1053:41 - saving is going to be this. So we have data
1053:43 - which we're going to be saving in
1053:45 - each and every one of this file.
1053:47 - Now let's test this out and let's
1053:49 - put out a break here so we see exactly
1053:51 - what we're getting. We run this cell
1053:53 - original data not defined.
1053:55 - We instead need to do open
1053:57 - here. So we're going to open this
1053:59 - file and then save
1054:01 - that. Let's run this again.
1054:03 - We check out now this directory.
1054:05 - We have your data set and there we go.
1054:07 - You see we have this information
1054:09 - here saved. Now let's
1054:11 - run this for all the data
1054:13 - set. So let's take out the break
1054:15 - and then run this.
1054:17 - Let's print out
1054:19 - K and then
1054:21 - like after a thousand steps
1054:23 - so if K
1054:25 - modulo a thousand
1054:27 - a thousand equals
1054:29 - zero we should print out K.
1054:31 - Okay so let's run again. We have
1054:33 - now all this data locked. Let's
1054:35 - take off this one
1054:37 - now and then get back to
1054:39 - documentation and you have
1054:41 - my data.add directory
1054:43 - there. So you pass in the directory
1054:45 - right there. So from here we have
1054:47 - now instead original
1054:49 - data.add directory
1054:51 - and then we're going to pass
1054:53 - in the directory which in this case is
1054:55 - data set. So we have that data set
1054:57 - passed. We don't really need to have this
1054:59 - again so we can take that off.
1055:01 - Okay so that's it. Everything looks
1055:03 - fine. Let's now run this cell
1055:05 - right here. We run that and
1055:07 - run this. And this time around
1055:09 - the artifact is loaded successfully.
1055:11 - So now we are going to check out
1055:13 - in our dashboard
1055:15 - and here you see that we have this
1055:17 - raw data, new data
1055:19 - set and then you have this version
1055:21 - two different versions of this new data
1055:23 - set. We have version zero
1055:25 - and version one. Now click on
1055:27 - this version one. You should be able to have this.
1055:29 - Let's check out the overview. You see you have
1055:31 - this data set.
1055:33 - Malaria data set on all this.
1055:35 - In fact this is metadata we put in already.
1055:37 - So that's it. We have this
1055:39 - API. We're going to come back to this
1055:41 - actually because to use this artifact
1055:43 - we're making use of this API.
1055:45 - Then we check out on metadata.
1055:47 - We have this metadata
1055:49 - which we logged in. We should put in
1055:51 - in the notebook. We have the
1055:53 - files. You see those different files
1055:55 - here. This
1055:57 - directory. You see
1055:59 - you have the root and then you have basically
1056:01 - all those files. Now you have this
1056:03 - graph view which for now is very simple.
1056:05 - So here you have a run.
1056:07 - The run is
1056:09 - actually this run here.
1056:11 - So this run and then
1056:13 - we have this artifact which has been
1056:15 - created. Now this artifact contains
1056:17 - the raw data.
1056:19 - So we could click on explode
1056:21 - and basically that's it. Here you have the name
1056:23 - of this run. Vividwater
1056:25 - name of the run and then the data
1056:27 - is this new data set and it's
1056:29 - version one. Then the next thing we
1056:31 - want to do is to move to the next step. That is
1056:33 - be able to split this data set
1056:35 - into this other, into these three different
1056:37 - parts. Now what we could
1056:39 - do other than this is
1056:41 - process this before doing the splitting
1056:43 - so we don't have to do the split processing tries.
1056:45 - So instead of having this let's take
1056:47 - this off here. We'll modify this
1056:49 - such that here is what we now
1056:51 - obtain. So coming back to
1056:53 - this we have this run
1056:55 - and this artifact
1056:57 - which our raw data set is basically
1056:59 - what we have here. And then the next
1057:01 - step will be to preprocess
1057:03 - this. So we'll do the preprocessing
1057:05 - on our raw data set
1057:07 - and then produce this preprocess
1057:09 - data and then from here
1057:11 - we'll now do the splitting. We'll have this run
1057:13 - which whose role will be to split
1057:15 - our data into the training validation and testing
1057:17 - and then for the training we'll have another
1057:19 - run. Whose role will be to
1057:21 - do augmentation on our
1057:23 - training data. So in
1057:25 - fact in summary this is what
1057:27 - we want to achieve.
1057:29 - Like when we're done with all this
1057:31 - we want to have a graph view which looks like this.
1057:33 - We now go straight forward into
1057:35 - the preprocessing and that's
1057:37 - copied. We get back to the code
1057:39 - that's fine. We paste this out here.
1057:41 - We then put this in the wait statement
1057:43 - and then run this code and here's what we get.
1057:45 - We've now downloaded this
1057:47 - 27,000 files. We check out
1057:49 - here and we should have this
1057:51 - artifacts new data set version
1057:53 - one and then see we have all those files
1057:55 - which we unlocked in previously.
1057:57 - So now this means that the next
1057:59 - time you want to work on this
1058:01 - data set you don't really
1058:03 - need to come and run this
1058:05 - year. Like we don't need to
1058:07 - run this again. So we don't have to do this
1058:09 - again. All we need to do now
1058:11 - is just to use
1058:13 - this artifact which was in
1058:15 - biases starts for us.
1058:17 - And so now that we have this
1058:19 - we'll get back here and then
1058:21 - we do this resize,
1058:23 - reskill. Particularly our
1058:25 - processing here is resizing and reskilling.
1058:27 - So you're going to do a resize
1058:29 - and a reskill of
1058:31 - all our images.
1058:33 - You can copy this and then
1058:35 - get back to
1058:37 - where we actually have loaded
1058:39 - our artifact. While we print
1058:41 - this out we have this path
1058:43 - to our different files.
1058:45 - So we will have your artifacts.
1058:47 - We can check out all those files here
1058:49 - and then right now we're
1058:51 - going to create this
1058:53 - auto run. So let's copy this
1058:55 - out and then have this
1058:57 - pasted here. Now here we have
1058:59 - this preprocess
1059:01 - preprocess
1059:03 - the data. So this is
1059:05 - this method we're going to be defining
1059:07 - 1DB init project
1059:09 - entity as well. We've created this new
1059:11 - run and then we have now this
1059:13 - preprocessed data
1059:15 - preprocessed
1059:17 - data which is now
1059:19 - this new artifact and the name is
1059:21 - preprocessed
1059:23 - preprocessed data
1059:25 - set. And then the type is
1059:27 - preprocessed data.
1059:29 - We have our preprocessed
1059:31 - data. Okay. Description
1059:33 - we'll say
1059:35 - a preprocessed
1059:37 - version
1059:39 - of the malaria data
1059:41 - set. Of the malaria
1059:43 - data set. Let's take this off
1059:45 - and then for the metadata
1059:47 - we could let's take all this
1059:49 - off. Let's have this taken off
1059:51 - but you could always put
1059:53 - information in the middle of the data.
1059:55 - We are now going to go through each
1059:57 - and every file we have
1059:59 - in this directory
1060:01 - here. So we will have
1060:03 - for file
1060:05 - in let's say for F
1060:07 - for F in OS
1060:09 - the list of
1060:11 - this directory
1060:13 - here. Let's copy this from here.
1060:15 - So we're going to create
1060:17 - a list from this directory and we go through this
1060:19 - list. So we go through this list
1060:21 - and then for each and
1060:23 - every file in this list
1060:25 - what we'll be doing is
1060:27 - open up that file. So here you have
1060:29 - artifact directory which we've just
1060:31 - defined and then we have this artifact
1060:33 - directory plus F to specify
1060:35 - the current file
1060:37 - and then we read that file
1060:39 - as file. So from here now
1060:41 - once we've read this as file we have
1060:43 - x y outputs. Recall
1060:45 - this is our output. So for each
1060:47 - file since each file we had
1060:49 - has an x and a y
1060:51 - we'll take this from here
1060:53 - and then we have this load
1060:55 - file. Now once
1060:57 - we have this we're going to have
1060:59 - x or better still x
1061:01 - full. That's x data set. Let's
1061:03 - call this x data
1061:05 - set or data set
1061:07 - data set x. So
1061:09 - we have our data set x
1061:11 - dot append. We're going to create
1061:13 - this as a list. So
1061:15 - we have here data set
1061:17 - x a list
1061:19 - and then we have data
1061:21 - set y and
1061:23 - not a list. So that's it.
1061:25 - We've created these two lists
1061:27 - and then we want
1061:29 - to take each and every
1061:31 - element and append it to
1061:33 - this data set x and data set
1061:35 - y respectively. Here we
1061:37 - have for x
1061:39 - and then we have data set
1061:41 - y that append
1061:43 - y. Now
1061:45 - recall what we have to preprocess
1061:47 - as this is our preprocessing
1061:49 - method. So here we have
1061:51 - this resize. Let's make sure we run this
1061:53 - resize rescale and it takes in the
1061:55 - image or it takes an x basically.
1061:57 - So here we have resize rescale
1061:59 - instead of passing x
1062:01 - we'll do resize
1062:03 - rescale
1062:05 - and then we pass in x.
1062:07 - Now we ensure that we'll define this
1062:09 - aim size. Let's
1062:11 - have this aim size defined here
1062:13 - aim size
1062:15 - equal to 24.
1062:17 - Run that again. That should be fine.
1062:19 - We need to also specify
1062:21 - the fact that we'll allow pickle
1062:23 - here. So we have this argument
1062:25 - which is done to true. You could
1062:27 - always check out the documentation
1062:29 - right here. And then
1062:31 - from here we are not going to
1062:33 - take this directly. Here what
1062:35 - we get is npz array
1062:37 - and then to obtain
1062:39 - x and y we have
1062:41 - x
1062:43 - and y
1062:45 - which is equal
1062:47 - npz array.
1062:49 - We get the numpy array
1062:51 - and we get the values.
1062:53 - So that's what we do to get this.
1062:55 - You could always check out the documentation to
1062:57 - understand how all this
1062:59 - numpy load and all of this work.
1063:01 - Now once you have x and y
1063:03 - this is where we pass in here.
1063:05 - So from here we can
1063:07 - run this. This looks fine. We have
1063:09 - all dataset and that's ok.
1063:11 - But before running we have
1063:13 - to ensure that we
1063:15 - convert this now into
1063:17 - TensorFlow dataset. So let's
1063:19 - get this done by having
1063:21 - this called
1063:23 - dataset
1063:25 - and we have tf.data
1063:27 - dataset
1063:29 - from
1063:31 - TensorSlices
1063:33 - and what we're going to pass in here is
1063:35 - dataset x and
1063:37 - dataset y. Now we're
1063:39 - going to be saving this as a file. So we're
1063:41 - going to take out this and we have
1063:43 - this portion
1063:45 - of this code right here which is
1063:47 - from the documentation. So here we
1063:49 - have with artifact. The artifact
1063:51 - here is pre-processed
1063:53 - is pre-processed
1063:55 - data. So we
1063:57 - pre-processed data
1063:59 - the new file. We specify the
1064:01 - file name. Let's call this
1064:03 - pre-processed
1064:05 - dataset
1064:07 - as file. We then save
1064:09 - this with numpy.
1064:11 - We have np.save
1064:13 - z in the compressed format
1064:15 - we specify the file and
1064:17 - the data to be saved. In this case all data
1064:19 - is this dataset right here which
1064:21 - is this TensorFlow dataset.
1064:23 - This looks okay
1064:25 - we can now do
1064:27 - log artifact pre-processed
1064:29 - dataset
1064:31 - or rather pre-processed
1064:33 - data. Then before running
1064:35 - we are going to take out
1064:37 - just a part of all this
1064:39 - dataset. So we'll take out only
1064:41 - a thousand elements. And the reason why
1064:43 - we're doing this is because we do
1064:45 - not have enough memory to store
1064:47 - 22,500
1064:49 - 2500
1064:51 - 2500 different data
1064:53 - points as a single
1064:55 - variable. So we'll have that
1064:57 - for now let's have this 1000
1064:59 - elements so you could see how
1065:01 - this is done. Now if you have
1065:03 - a real world problem where you have say
1065:05 - 100,000 different elements
1065:07 - or 100,000 different data points
1065:09 - then you could break them up into
1065:11 - simpler parts. So here we
1065:13 - have that a thousand
1065:15 - we take out just a thousand
1065:17 - and then one thing we'll do is
1065:19 - copy out this part here. We'll copy
1065:21 - out this part and then include
1065:23 - it in this run. So
1065:25 - just before this
1065:27 - so we include this here
1065:29 - in this run. And the fact that
1065:31 - you include this in this run will link
1065:33 - up this new dataset
1065:35 - with a pre-processed
1065:37 - dataset. Now with that set
1065:39 - we could run this and then run the
1065:41 - next cell. What we get is this
1065:43 - error saying cannot
1065:45 - convert a TensorFlow of D type variance
1065:47 - to a non-pyre. So here
1065:49 - we are having this Tensor of
1065:51 - D type variance and we're trying to
1065:53 - store it as a non-pyre. And to solve
1065:55 - this problem now what we'll do is
1065:57 - we'll just ignore this and then
1065:59 - save the dataset
1066:01 - X and Y as this list
1066:03 - we have dataset X and
1066:05 - dataset Y.
1066:07 - So let's run this now
1066:09 - and pre-process our data.
1066:11 - We now have successfully
1066:13 - logged this to our
1066:15 - artifact. Let's get to
1066:17 - our dashboard, 1DB dashboard
1066:19 - let's refresh this page.
1066:21 - Here you'll see your artifacts
1066:23 - you could click on artifacts,
1066:25 - media detection artifacts and then you could
1066:27 - select from this. So this is
1066:29 - what we had previously this new dataset
1066:31 - under this raw data. Now
1066:33 - pre-processed data we have this pre-processed
1066:35 - dataset and this almost recent
1066:37 - version there we go. You could check out the files
1066:39 - you see this file right here
1066:41 - metadata API
1066:43 - which you could use now to do
1066:45 - or carry out all the operations
1066:47 - the overview, graph
1066:49 - view, this graph view right here
1066:51 - now let's explode this graph view
1066:53 - ok we have exploded this graph
1066:55 - view, let's zoom
1066:57 - ok now you'll see that
1066:59 - let's drag this one here and then
1067:01 - what you'll notice is we have
1067:03 - this first path which has to do
1067:05 - with the creation or rather with
1067:07 - the loading of our initial original dataset
1067:09 - and then once we load this original
1067:11 - dataset the next thing we
1067:13 - did was to now pre-process
1067:15 - this dataset
1067:17 - now we'll carry out all the
1067:19 - runs previously that's why you have this
1067:21 - so you don't really need to take this
1067:23 - into consideration from here we'll just
1067:25 - continue from this point here
1067:27 - we copy out this API here
1067:29 - we have that copied
1067:31 - and then we get back to
1067:33 - our code where we're going to start now
1067:35 - where the data is splitting
1067:37 - you see that we're going to
1067:39 - have, we've already had
1067:41 - two artifacts created
1067:43 - one was original data, the other
1067:45 - the pre-processed data
1067:47 - the next will be the trained data
1067:49 - validation data and
1067:51 - test data and so that's why we
1067:53 - call this section the data splitting
1067:55 - we'll again copy out this
1067:57 - part from the pre-processing
1067:59 - data and paste here
1068:01 - now we have this
1068:03 - let's go ahead and
1068:05 - take this off from here
1068:07 - and then replace this one
1068:09 - here we have this artifact
1068:11 - ok
1068:13 - we have this artifact now which is
1068:15 - the artifact we're using now is pre-processed
1068:17 - dataset and then from here
1068:19 - we have three artifacts
1068:21 - which we're going to create we have
1068:23 - trained data
1068:25 - we'll just call this trained data set
1068:27 - trained data set type let's say
1068:29 - pre-processed data, description trained
1068:31 - data set and then the artifact
1068:33 - directory we could get it from this year
1068:35 - so when we create this artifact
1068:37 - we're going to get this artifact
1068:39 - directory for now let's just create
1068:41 - the other artifacts so we copy
1068:43 - this out, trained data, we have
1068:45 - validation data and then the
1068:47 - test data to obtain the artifact
1068:49 - directory we're going to run this
1068:51 - we have this output here
1068:53 - click on this artifact and see
1068:55 - we have this pre-processed dataset which has been loaded
1068:57 - and we have our
1068:59 - prep dataset.npz file
1069:01 - here so this
1069:03 - here we copy this
1069:05 - path and then
1069:07 - at the place of this artifact
1069:09 - directory we're going to place this
1069:11 - path let's take this off
1069:13 - we have this path
1069:15 - now which has been placed
1069:17 - we have now the artifact let's
1069:19 - call this artifact
1069:21 - file and then
1069:23 - let's paste this out
1069:25 - here take this
1069:27 - off
1069:29 - and there we go
1069:31 - we've now taken all this off
1069:33 - ok so we have this
1069:35 - here and then
1069:37 - we're going to have our
1069:39 - artifact file instead
1069:41 - past here, artifact
1069:43 - file, so we read
1069:45 - that as file and then we're going to look
1069:47 - this file, allow pickle
1069:49 - and get the array
1069:51 - then at this point
1069:53 - we define the trained split, valid split
1069:55 - and test split
1069:57 - from here we're going to paste this out
1069:59 - we have our trained array because
1070:01 - we're trying to create these three different
1070:03 - arrays, we have trained array
1070:05 - which goes, takes values from
1070:07 - zero to the trained split
1070:09 - we define a data length
1070:11 - data length
1070:13 - which is the length of
1070:15 - array zero
1070:17 - now recall that the array
1070:19 - the array we have in
1070:21 - this array is made of
1070:23 - is at least actually made of two parts
1070:25 - the x and the y
1070:27 - this x has a length of a thousand
1070:29 - the y length of a thousand, that's why we're doing
1070:31 - array, the zero
1070:33 - index of array and then we're taking
1070:35 - its length, so here we're going to get data length
1070:37 - to a value of a thousand
1070:39 - and once we have this data length
1070:41 - now we're seeing that we're picking out
1070:43 - this x
1070:45 - we're picking out this x right here
1070:47 - and then we're taking values from zero
1070:49 - to eighty percent
1070:51 - of the total length
1070:53 - and so we have trained split which is
1070:55 - zero point eight times
1070:57 - data length
1070:59 - so that's what we have
1071:01 - and then we repeat the same for
1071:03 - this y
1071:05 - then for the
1071:07 - validation array, we're going to start
1071:09 - from this trained split
1071:11 - we're going to start from here, times
1071:13 - that, so from the each
1071:15 - index
1071:17 - with respect to the total data set, that's why
1071:19 - we're multiplying by the data length
1071:21 - we're going to go from here, right up
1071:23 - to
1071:25 - the trained split
1071:27 - plus the validation split, so now we're going to
1071:29 - from zero point eight to zero
1071:31 - point nine and we have
1071:33 - that set, we'll just repeat
1071:35 - this for the validation
1071:37 - for the y right here, so let's
1071:39 - copy this out
1071:41 - we'll copy this out
1071:43 - and then
1071:45 - paste this here
1071:47 - that looks fine
1071:49 - we have that paste that, we have
1071:51 - it here and this here
1071:53 - this looks fine, let's
1071:55 - repeat the same process for the test
1071:57 - array, copy
1071:59 - this and paste out
1072:01 - your test array
1072:03 - we're going to go from
1072:05 - trained split plus valid split
1072:07 - right up to the end, so we're going
1072:09 - from zero point eight plus zero
1072:11 - point one, that's zero point nine
1072:13 - times the total data length from the nine
1072:15 - hundredth value right to the end
1072:17 - we'll just copy this again
1072:19 - out here, copy that
1072:21 - out and then paste this
1072:23 - for the y value
1072:25 - so we have this here
1072:27 - take this off, paste it
1072:29 - and then we go right to the end
1072:31 - so that looks fine, again
1072:33 - we have now our trained array
1072:35 - validation array and test
1072:37 - array, we're now set to
1072:39 - write this information in our
1072:41 - artifacts
1072:43 - so here we have with
1072:45 - preprocess data, this is instead now
1072:47 - we train data, we call our artifacts
1072:49 - we just created here is trained data
1072:51 - trained data, valid data, and test data
1072:53 - so we train data, the new
1072:55 - file, we have our
1072:57 - we'll call this trained data set
1072:59 - npzat mode wb
1073:01 - save the file and then
1073:03 - we're saving
1073:05 - what we're saving is actually just trained array
1073:07 - so we're just saving the string array
1073:09 - and then we repeat the same
1073:11 - process for the validation
1073:13 - and the testing, okay
1073:15 - we have that, validation
1073:17 - validation
1073:19 - test
1073:21 - test, and then
1073:23 - here we have test
1073:25 - validation
1073:27 - okay we have
1073:29 - okay, here we have
1073:31 - validation, and then
1073:33 - here too we have test
1073:35 - then now we log
1073:37 - our different artifacts, so we
1073:39 - not only log in this one artifact
1073:41 - but the three different artifacts
1073:43 - stick this back, and then
1073:45 - here we log trained data
1073:47 - trained
1073:49 - data
1073:51 - valid data
1073:53 - and test data
1073:55 - okay we have that set
1073:57 - we could run this, let's change
1073:59 - it to split data
1074:01 - we run this cell here
1074:03 - everything looks fine, and then
1074:05 - we move on to split all data
1074:07 - split data, we run that cell
1074:09 - and wait for the response
1074:11 - here's the output we get, now
1074:13 - the reason why we're having this is
1074:15 - because we must have
1074:17 - integers and not floats at this
1074:19 - level, so when we
1074:21 - have this indices
1074:23 - we have to convert this all
1074:25 - into integers, so
1074:27 - we have this int here
1074:29 - there we go, int
1074:31 - and
1074:33 - from here we now run this
1074:35 - cell again, and then
1074:37 - split all data, the data has now
1074:39 - been split successfully, let's get back
1074:41 - to our dashboard right here
1074:43 - we have our 1db dashboard
1074:45 - artifacts, let's
1074:47 - refresh this page
1074:49 - as you can see we have our data
1074:51 - preprocessed data, and now we have the test
1074:53 - data, validation data
1074:55 - and our training data
1074:57 - let's click open this training data
1074:59 - so you can look at the file here
1075:01 - metadata, API
1075:03 - we can always make use of
1075:05 - this in creating other
1075:07 - artifacts, and let's get
1075:09 - to the graph view, in this graph view
1075:11 - now we'll be able to see the link
1075:13 - between this original
1075:15 - data set, the preprocessed data
1075:17 - and the training data
1075:19 - set, here's what we get, let's click
1075:21 - and explode and you can see this clearly
1075:23 - now, so here you see you have
1075:25 - this original data set
1075:27 - this artifact here, we have
1075:29 - this run which produces this data
1075:31 - preprocessed artifact, and then we
1075:33 - have this run which produces this
1075:35 - training data set, validation data
1075:37 - set, and test data
1075:39 - set, so let's
1075:41 - draw this, take this off
1075:43 - we have this
1075:45 - path here, you see
1075:47 - we have this here, we take
1075:49 - this, this, and
1075:51 - this, let's copy out
1075:53 - this code right here, and then
1075:55 - start with the data augmentation
1075:57 - so from here we now download the training data
1075:59 - set, we can check this out in our artifacts
1076:01 - you should have training
1076:03 - data set right here, there we go, we have
1076:05 - our training data set, we can now
1076:07 - copy this path
1076:09 - let's copy
1076:11 - this path and paste here, now
1076:13 - we can click on this one to be finished to stop
1076:15 - that run, and that should be fine
1076:17 - now let's go ahead and copy
1076:19 - out this part of the code
1076:21 - which was used for preprocessing
1076:23 - so here we have this here, preprocessed
1076:25 - data, we copy this
1076:27 - and then, there we go
1076:29 - let's paste this here, we have
1076:31 - our augment data
1076:33 - and then project artifacts
1076:35 - we're going to use this artifacts
1076:37 - here, so
1076:39 - let's come right here
1076:41 - and then get
1076:43 - this path
1076:45 - ok, so we're going to get that
1076:47 - path and then replace this one
1076:49 - with this op-on part
1076:51 - or the path to the training data set
1076:53 - this paste is out
1076:55 - and this should be fine
1076:57 - this is actually a file
1076:59 - so we have artifact file
1077:01 - now we have this
1077:03 - take this off
1077:05 - preprocessed data, instead of preprocessed
1077:07 - data, we're going to use augmented
1077:09 - augmented data
1077:11 - wanted to be
1077:13 - artifact, augmented data set
1077:15 - augmented
1077:17 - data set
1077:19 - type, let's say
1077:21 - preprocessed data
1077:23 - an augmented version
1077:25 - augmented version
1077:29 - of the malaria
1077:31 - train data set
1077:33 - so everything looks fine
1077:35 - for now, we have that
1077:37 - and then here, let's get back to this
1077:39 - and then copy out this here
1077:41 - and then paste this
1077:43 - right here, so we're getting this
1077:45 - from our training data, obviously
1077:47 - there we go
1077:49 - and everything looks fine
1077:51 - next thing to do is
1077:53 - do the actual augmentation
1077:55 - and then log the
1077:57 - data set to our artifact
1077:59 - so let's take
1078:01 - this back
1078:03 - and then we take this off now
1078:05 - we have this, we open up
1078:07 - this artifact file
1078:09 - right here, artifact
1078:11 - file, we open that up
1078:13 - we obtain our array
1078:15 - let's call that array
1078:17 - then for
1078:19 - images, or for image
1078:21 - we obtain the array
1078:23 - our array
1078:25 - O, we've taken out the X
1078:27 - so for image X
1078:29 - before moving on, we're going to create this
1078:31 - empty list here, data set
1078:33 - X, and then we have that
1078:35 - ok, we have data set X
1078:37 - and then data set X with a pen
1078:39 - augment of whatever image
1078:41 - we want to pass in
1078:43 - let's send this one step, and that's fine
1078:45 - so for all images, we
1078:47 - are going to do this augmentation
1078:49 - then after this, we have data set
1078:51 - Y, which is simply
1078:53 - the unchanged
1078:55 - levels we've had already
1078:57 - so from here now, we have all data set
1078:59 - X and R data set
1079:01 - Y, then we can now run
1079:03 - the salt, and then we
1079:05 - run the augment data
1079:07 - we have here augment
1079:09 - data, and
1079:11 - they should be fine
1079:13 - we obtain this error, preprocessed data
1079:15 - is not defined, so let's check back here
1079:17 - and we see that this should be
1079:19 - augmented data instead
1079:21 - so let's change this, and we have
1079:23 - augmented data
1079:25 - there we go, we run this cells
1079:27 - and see what we get
1079:29 - now those artifacts have been
1079:31 - locked successfully, we could
1079:33 - get to 1 dB, and check this out
1079:35 - here we have this, and then
1079:37 - we click on augmented data set
1079:39 - so loading the artifacts
1079:41 - now let's load that
1079:43 - let's click on this, and
1079:45 - check out on this graph, so let's
1079:47 - explode, and this is what we have
1079:49 - now, so you see again that we have
1079:51 - this path, we have our data
1079:53 - we split this
1079:55 - into train validation and testing
1079:57 - you see here you have the training, validation and
1079:59 - testing, and then after the training
1080:01 - we have this run, which
1080:03 - converts this training data
1080:05 - into an augmented data, which
1080:07 - is this one right here
1080:09 - and so at this point, we
1080:11 - have different versions of
1080:13 - our data, which we could make use
1080:15 - of depending on our needs
1080:17 - thank you for getting around to this point
1080:19 - and see you next time
1080:25 - hello everyone, and welcome
1080:27 - to this new session in which
1080:29 - we'll treat model version
1080:31 - with 1 dB
1080:33 - now, in the previous session, we had
1080:35 - looked at data set versioning, in
1080:37 - which we had created an augmented
1080:39 - version of our data, which happens
1080:41 - to be this version right here
1080:43 - from previous versions
1080:45 - like a preprocessed version
1080:47 - another preprocessed version
1080:49 - and the original
1080:51 - data, and so from here
1080:53 - we are now going to
1080:55 - see how to implement
1080:57 - model versioning using
1080:59 - this augmented data
1081:01 - and the untrained
1081:03 - model version, so we have this
1081:05 - version of the model, which is untrained
1081:07 - and the augmented data, which
1081:09 - are going to be passed in this run
1081:11 - to produce our trained
1081:13 - model version. In the previous
1081:15 - session, we saw how to
1081:17 - implement data set versioning with
1081:19 - 1 dB artifacts. We
1081:21 - left from this original raw
1081:23 - data right here, onto
1081:25 - this preprocessed data
1081:27 - we created a trained validation
1081:29 - test sets, and then finally
1081:31 - we had this augmented
1081:33 - data right here, and
1081:35 - now we are going to see how to
1081:37 - make use of this augmented data
1081:39 - in training our model
1081:41 - but while training this model
1081:43 - we are going to implement
1081:45 - model versioning, and
1081:47 - the two model versions
1081:49 - we will have here will be the
1081:51 - model before training
1081:53 - and the model after training
1081:55 - and so we are making use of this
1081:57 - sequential model
1081:59 - which we had created previously, and
1082:01 - the good news here is the
1082:03 - way we implement model versioning
1082:05 - is quite similar to the way
1082:07 - we implement data set versioning
1082:09 - we will start by copying out this
1082:11 - code, and then we will simply modify that
1082:13 - and here we have this 1 dB
1082:15 - model versioning, let's paste it out
1082:17 - and then here instead of augment
1082:19 - data, we are going to log
1082:21 - out model
1082:23 - so here we have
1082:25 - log model, which is
1082:27 - in fact this Lynette model
1082:29 - right here, we have the log model
1082:31 - 1 dB in needs, our project
1082:33 - entity specified, the
1082:35 - artifacts
1082:37 - to be used here is known, so you are not
1082:39 - going to make use of any artifact
1082:41 - to log this model
1082:43 - artifact right here, and so with
1082:45 - that we have this, take off this augmented
1082:47 - data, we have our model
1082:49 - that's called
1082:51 - untrained
1082:53 - model, and then
1082:55 - we specify that
1082:57 - it is this untrained
1082:59 - model
1083:01 - type model, so we have
1083:03 - that type model
1083:05 - the initial version
1083:07 - of our
1083:09 - Lynette model
1083:11 - there we go, now
1083:13 - we have that done
1083:15 - this artifact file
1083:17 - here, we are not going to make use of this
1083:19 - we are not making use of any previous artifacts
1083:21 - we just log in this model
1083:23 - initially, and then here
1083:25 - we will take this off
1083:27 - we are not going to make use of this
1083:29 - just before
1083:31 - writing in our model
1083:33 - we are going to save that model
1083:35 - so here we have this model to be saved
1083:37 - we call, this is our Lynette model
1083:39 - actually, so we have Lynette model
1083:41 - save, and then
1083:43 - we will give it a file name, so let's call it
1083:45 - Lynette.h5
1083:47 - and that's it, so we will save this
1083:49 - model, let's have
1083:51 - this file name to be Lynette
1083:53 - h5, so let's define this
1083:55 - Lynette h5
1083:57 - file here, and take this off
1083:59 - so we have now
1084:01 - our file name
1084:03 - now, since we have already saved this file
1084:05 - instead of going through this
1084:07 - process here of creating a new file
1084:09 - and all of that, we are just going to
1084:11 - add a file, so here we
1084:13 - have our untrained
1084:15 - model.add
1084:17 - file, and then we
1084:19 - specify the file name
1084:21 - so that's all we need to do, and then
1084:23 - we are also going to do 1db.save
1084:25 - that file
1084:27 - based on the file name, and
1084:29 - that's it. So let's take off this
1084:31 - part here, and then we are going to
1084:33 - log our artifact, which happens to be
1084:35 - our untrained model
1084:37 - so we have your untrained model
1084:39 - and that's fine. Now we could
1084:41 - add in some metadata, so
1084:43 - let's have metadata
1084:45 - which equals our configuration, which
1084:47 - we have defined already. In here
1084:49 - let's modify this name, this is
1084:51 - our untrained model
1084:53 - so here is untrained model
1084:55 - and that's fine. So with this
1084:57 - we could now run this here
1084:59 - with this
1085:01 - successful run right here
1085:03 - and then we can now go back to
1085:05 - 1db. Getting back here
1085:07 - we have this under
1085:09 - our artifacts, we have the model
1085:11 - so here we could click
1085:13 - on this model, and then we specify
1085:15 - the untrained model
1085:17 - so let's click on this
1085:19 - and we have this overview
1085:21 - you see we have that API
1085:23 - metadata, you see the metadata we've
1085:25 - logged in, we have the files
1085:27 - you see here the Linux model
1085:29 - and then we have as usual our
1085:31 - graph view which we could explode
1085:33 - notice that it's quite simple because
1085:35 - for now it isn't linked to our
1085:37 - data set, so all we have is just
1085:39 - this run which produces
1085:41 - this untrained model or
1085:43 - this initialized model
1085:45 - from this point we'll go
1085:47 - ahead to train our model
1085:49 - and log that
1085:51 - trained version of the model. So again
1085:53 - here we have this artifact
1085:55 - which we are going to use in
1085:57 - training the model which happens to be our
1085:59 - augmented data set. We call
1086:01 - that we had logged this augmented
1086:03 - data previously, we could have this overview
1086:05 - from here, we have this augmented data set
1086:07 - and then
1086:09 - it's of type preprocessed data, we've seen that already
1086:11 - we downloaded data and then
1086:13 - we create this new artifact
1086:15 - which is going to be our trained
1086:17 - sequential model
1086:19 - we call this trained
1086:21 - sequential model
1086:23 - so of type model, description
1086:25 - a trained version of our model, simple
1086:27 - as that. So that's it. We could also
1086:29 - pass in the metadata
1086:31 - just as we had seen here
1086:33 - with this log model
1086:35 - where we pass in the configuration
1086:37 - so metadata equals our configuration
1086:39 - and
1086:41 - that's fine. Getting back to
1086:43 - this augmented data method we had seen previously
1086:45 - we're going to follow this same
1086:47 - pattern that is we're going to download
1086:49 - the data as we've done already
1086:51 - we download the data
1086:53 - and then we have this artifact file
1086:55 - here the artifact file we're
1086:57 - using was a trained data set
1086:59 - because we needed to take this and
1087:01 - convert it into an augmented
1087:03 - data set and then we had this
1087:05 - processing right here to produce
1087:07 - data set X and data set Y. Now
1087:09 - we're going to do this
1087:11 - exact procedure
1087:13 - for training our data on the augmented data
1087:15 - so let's paste this out right
1087:17 - here and then here our artifact
1087:19 - file will be
1087:21 - this org data set
1087:23 - so here we have org data set
1087:25 - and we get this
1087:27 - right from here. Let's get back
1087:29 - to org data set, files
1087:31 - and there we go. We see we have this org
1087:33 - data set.mpz which we
1087:35 - started previously and so
1087:37 - once we are going to download this
1087:39 - we're going to have this file
1087:41 - which is going to be found in this folder
1087:43 - which is also found in this
1087:45 - artifacts folder. So we have artifacts
1087:47 - augmented data set version 0 which is from
1087:49 - this. This means that if we want to use the version
1087:51 - 1 we just simply have to specify your version
1087:53 - 1 and so on and so forth. So
1087:55 - that said we have our artifact file
1087:57 - data set and then we go to
1087:59 - this processing to obtain data set X
1088:01 - and data set Y
1088:03 - and then from here we'll go through this series
1088:05 - of steps to convert this
1088:07 - data into a tensorflow
1088:09 - data set type data. So here
1088:11 - we have DX, DY
1088:13 - this data set X
1088:15 - we convert to tensor so we
1088:17 - convert this data set X to a tensor
1088:19 - and data set Y to a tensor. Then
1088:21 - from here we apply this
1088:23 - from tensor slices method which
1088:25 - takes in DX and
1088:27 - DY. That's our data set X
1088:29 - and our data set Y to
1088:31 - create this new data set
1088:33 - D. Now we go
1088:35 - to the same processes we have seen already
1088:37 - that we shuffle, we batch, we
1088:39 - prefetch and then we have our
1088:41 - training data set. Now note that
1088:43 - you could repeat the same process for
1088:45 - the validation but here we just
1088:47 - do this for the training and
1088:49 - you could take it as an exercise to
1088:51 - do the same for the validation.
1088:53 - Note that for validation we didn't
1088:55 - make use of the augmented data set
1088:57 - we instead made use of the validation
1088:59 - data set itself. Let's get to the graph view
1089:01 - so you could see that clearer.
1089:03 - And as you could see here we have this
1089:05 - data which is pre-processed
1089:07 - pre-processed data, the training data set
1089:09 - pre-processed data, the validation
1089:11 - data set, pre-processed data, test data set
1089:13 - and then from here we have the augmented
1089:15 - data which is this one.
1089:17 - And so when you want
1089:19 - to carry out validation
1089:21 - you're not going to be making use of this
1089:23 - training data obviously. So
1089:25 - we're going to make use of this one instead.
1089:27 - And so if you are to
1089:29 - put in the validation
1089:31 - in here, let's convert the validation data
1089:33 - into a TensorFlow data set
1089:35 - type data and then use
1089:37 - it for training and
1089:39 - evaluation
1089:41 - then you would have to
1089:43 - make sure you modify this here.
1089:45 - So that said we
1089:47 - continue with this process we have
1089:49 - the trained data which we just created
1089:51 - is now a TensorFlow data
1089:53 - set type data
1089:55 - and so training
1089:57 - can now go on smoothly.
1089:59 - So you see here we have the metrics we've used
1090:01 - to seeing this already
1090:03 - compiling and then
1090:05 - we fit the model. So here we
1090:07 - are not going to include the validation
1090:09 - we have that already
1090:11 - we're going to do the training and after training
1090:13 - our model we would have this
1090:15 - so let's simply
1090:17 - just have that lunet model
1090:19 - that fits and then
1090:21 - after this we will have
1090:23 - similar to what we have seen already
1090:25 - here in this log model method
1090:27 - the file which has been saved
1090:29 - and then
1090:31 - saved to 1db and then
1090:33 - the artifact which has been
1090:35 - locked. So let's copy this out
1090:37 - and then get back here
1090:39 - and paste this. Now let's
1090:41 - change this name let's say lunet
1090:43 - trained. lunet
1090:45 - trained that's our file name
1090:47 - untrained model.add file
1090:49 - it's actually
1090:51 - here we have this
1090:53 - trained sequential model so
1090:55 - let's have this as
1090:57 - trained
1090:59 - sequential
1091:01 - model
1091:03 - there we go. We have the trained
1091:05 - sequential model.add file file name
1091:07 - 1db save file name
1091:09 - run the log artifact
1091:11 - the trained sequential
1091:13 - model. Trained sequential
1091:15 - model. Okay
1091:17 - so that's it. So we have this
1091:19 - set we could now run this method
1091:21 - and then see what we get. We obtained
1091:23 - this error where we've been told
1091:25 - name lunet model is undefined.
1091:27 - Now as you had seen previously
1091:29 - with this log
1091:31 - model method we made
1091:33 - use of the lunet model which we
1091:35 - had defined right here. So we
1091:37 - defined this lunet model
1091:39 - using the sequential
1091:41 - API and that's what we had
1091:43 - and this led us to having
1091:45 - this year where we had a run
1091:47 - so initially we had
1091:49 - this run which produced
1091:51 - our untrained model
1091:53 - so we have this year untrained
1091:55 - model. Let's call it um
1091:57 - untrained model and since in our
1091:59 - case we're trying to build our
1092:01 - trained model it's going to be
1092:03 - reasonable for us to make use
1092:05 - of this untrained model
1092:07 - which we've started already
1092:09 - in 1db since we're doing
1092:11 - model versioning and so
1092:13 - instead of us making use of this
1092:15 - lunet model we're going to make
1092:17 - use of this version of
1092:19 - the untrained model which was
1092:21 - thought already in 1db. So from here
1092:23 - we're going to run another process
1092:25 - which now will lead us
1092:27 - to this trained model
1092:29 - but then to run this process we will
1092:31 - also need data. So
1092:33 - as you could see you have the data
1092:35 - that's our augmented data
1092:37 - from here we have the trained
1092:39 - data. We have different
1092:41 - other nodes which came before
1092:43 - this so we call
1092:45 - what we've seen before having
1092:47 - the untrained augmented data
1092:49 - which is this one here and then
1092:51 - this augmented data together
1092:53 - with this untrained model
1092:55 - will produce
1092:57 - our trained model and once
1092:59 - we have this trained model we can
1093:01 - now use it for prediction and so
1093:03 - coming back again if
1093:05 - we had to use this lunet model
1093:07 - which was defined
1093:09 - here in the notebook would
1093:11 - have this
1093:13 - data set that will have
1093:15 - our data which will pass into
1093:17 - the run and then
1093:19 - we'll have our outputted
1093:21 - trained model and then
1093:23 - on the other side we'll just have
1093:25 - this outputted
1093:27 - like we have this run
1093:29 - and then we have this outputted untrained
1093:31 - model. So we'll have the untrained model
1093:33 - and the trained model
1093:35 - which will exist
1093:37 - like two separate entities in our
1093:39 - global graph whereas what we want
1093:41 - to do is be able to link this
1093:43 - two up and so that's why we're
1093:45 - not going to make use of this lunet model which was
1093:47 - defined here and this is why we're going to
1093:49 - make use of the model which has already been
1093:51 - stored in 1db. That said,
1093:53 - getting back to the code we're going to
1093:55 - copy this out here
1093:57 - and then in the place
1093:59 - of this lunet model as
1094:01 - instead of using
1094:03 - the lunet model we've defined
1094:05 - here we are now going
1094:07 - to make use of our artifact which
1094:09 - was stored already. So let's
1094:11 - have this here. So let's have that
1094:13 - pasted here and then
1094:15 - we'll make use of this artifact
1094:17 - which is in fact called
1094:19 - the untrained
1094:21 - model. So you see we have
1094:23 - this here. Let's copy this out
1094:25 - we copy that out and
1094:27 - then we paste this here.
1094:29 - Let's paste this here
1094:31 - and there we go.
1094:33 - Just after downloading the
1094:35 - model the next step will be to have
1094:37 - this artifact
1094:39 - file which is simply
1094:41 - our untrained
1094:43 - model which we should have
1094:45 - downloaded. So in that case we're
1094:47 - going to have this here
1094:49 - artifacts stored in the artifacts
1094:51 - directory artifacts and then
1094:53 - in here we'll take this
1094:55 - out from here. Let's copy this
1094:57 - and paste out here our untrained
1094:59 - model and then to be more
1095:01 - specific we could get the
1095:03 - file name from here. So here you see
1095:05 - we have the file name you could get back here
1095:07 - overview and then you have files
1095:09 - in the net h5. So
1095:11 - this is a file we're going to download
1095:13 - and here is it here. So
1095:15 - we have that. So now we have the
1095:17 - artifact file which we want to download
1095:19 - we have this path and then now
1095:21 - we could make use of TensorFlow's
1095:23 - load model
1095:25 - method. So here we have now
1095:27 - the net model which is equal
1095:29 - tf.keras, models
1095:31 - and
1095:33 - load model which takes in the
1095:35 - artifact file
1095:37 - there we go. So now we have
1095:39 - this lunette model which
1095:41 - is not gotten from
1095:43 - this definition here
1095:45 - but which is gotten from a previous version
1095:47 - of our model which has
1095:49 - stored in 1db. So
1095:51 - let's now run this and
1095:53 - see what we get. After training
1095:55 - for over 3 epochs
1095:57 - here are the results we get and
1095:59 - then we get to 1db
1096:01 - you see here we have the model
1096:03 - trained sequential model
1096:05 - and then the untrained model. So let's click
1096:07 - on this trained sequential model loading
1096:09 - artifacts. There we go
1096:11 - we could look at this
1096:13 - API metadata which
1096:15 - we loaded and the
1096:17 - files. So here we have lunette trained
1096:19 - and then we also have our graph
1096:21 - view. Explode in this view
1096:23 - here's what we get. Now
1096:25 - let's focus on this part here
1096:27 - and as you can see
1096:29 - we have this run which produces
1096:31 - this untrained model
1096:33 - here. We have this untrained model
1096:35 - and then this untrained model
1096:37 - together with our
1096:39 - augmented data set
1096:41 - been passed through this run
1096:43 - now produce
1096:45 - our trained sequential
1096:47 - model. And that's it for
1096:49 - the section in which we'll see how to
1096:51 - implement model version with
1096:53 - 1db.
1096:59 - Hello everyone and welcome to this new
1097:01 - and exciting session.
1097:03 - In this session we shall
1097:05 - be building a system which permits
1097:07 - us automatically detect
1097:09 - weather and input
1097:11 - which in this case is an
1097:13 - image is that of
1097:15 - an angry, happy
1097:17 - or sad person. So
1097:19 - this person may be sad as you can
1097:21 - see here. This person may be
1097:23 - angry as you can see
1097:25 - here. All the person
1097:27 - may be happy
1097:29 - so in fact we want to be able
1097:31 - to have this kind of input
1097:33 - pass it into our system
1097:35 - which we want to build and then
1097:37 - automatically infer that
1097:39 - this is a happy person
1097:41 - and so to train
1097:43 - the system we'll be making use
1097:45 - of this data set
1097:47 - which has been made available to us
1097:49 - by Muhammad
1097:51 - Hanan Asgar
1097:53 - and since this data set is available
1097:55 - on Kegel just anyone could
1097:57 - have access to it. We are now going to
1097:59 - take a closer look at this data set
1098:01 - as you can see here we have this
1098:03 - test and train directories
1098:05 - you open up the train
1098:07 - open up the test you see you have
1098:09 - these three different folders angry, happy,
1098:11 - sad. Here for the train you have
1098:13 - angry, happy, sad too.
1098:15 - So what we do is
1098:17 - generally when trying to create
1098:19 - this kind of data set for classification
1098:21 - problems what you want to do is
1098:23 - make sure that you put
1098:25 - each or you put the different
1098:27 - images in separate
1098:29 - directories. So all images
1098:31 - representing angry people are in
1098:33 - this directory, happy in this
1098:35 - directory and sad in this directory.
1098:37 - Now this way it becomes easy
1098:39 - for us to build our model
1098:41 - taking this data set
1098:43 - as input. Also in the previous sessions
1098:45 - we explained the reason why
1098:47 - it's very important for us to split
1098:49 - our data set in this kind of
1098:51 - manner such that
1098:53 - while you use part of the data
1098:55 - set as a training to
1098:57 - actually train or build your
1098:59 - model you're going to use the
1099:01 - other part to evaluate this
1099:03 - model. So
1099:05 - although in this case we've just been
1099:07 - given this data which is
1099:09 - very unlike
1099:11 - what you have in
1099:13 - real world scenarios where you will be
1099:15 - asked or tasked to
1099:17 - build your own data set like this
1099:19 - one thing or
1099:21 - one very important thing you have to
1099:23 - note is the fact that if for
1099:25 - example you're building this kind of data set
1099:27 - where say we want to
1099:29 - monitor the usage
1099:31 - of an app or
1099:33 - better still monitor
1099:35 - how the users of an
1099:37 - app react
1099:39 - to a certain feature then
1099:41 - it's important to get a data
1099:43 - that reflects
1099:45 - or is very similar
1099:47 - to what the data or
1099:49 - rather what the model will be
1099:51 - seen during
1099:53 - inference. And so this
1099:55 - means that if
1099:57 - you have this kind of
1099:59 - data or if your model is going to be
1100:01 - seeing this kind of data during
1100:03 - the inference or when it's
1100:05 - going to make use of the model to predict
1100:07 - whether there's a happy image
1100:09 - or not then
1100:11 - training on this kind of
1100:13 - data
1100:15 - isn't a good idea although
1100:17 - this is
1100:19 - images or these are images of
1100:21 - happy people. So
1100:23 - you have to ensure
1100:25 - that the data you train on
1100:27 - is representative of
1100:29 - the data or the kinds of
1100:31 - inputs the model will be seen
1100:33 - when
1100:35 - it's going to be deployed and left
1100:37 - to predict or
1100:39 - make predictions from
1100:41 - these kinds of images.
1100:43 - Then if you have noticed
1100:45 - not all classes
1100:47 - we have here have the same
1100:49 - number of files. So here you
1100:51 - have a happy 1000, angry
1100:53 - 500, sad
1100:55 - 757
1100:57 - for the train is
1100:59 - 1525
1101:01 - year 3000
1101:03 - sad
1101:05 - 2255. So this shows us that
1101:07 - when solving
1101:09 - your real world problems
1101:11 - it may happen that
1101:13 - some class or
1101:15 - it's easier to gather
1101:17 - data from a particular
1101:19 - class as compared to the other.
1101:21 - So here
1101:23 - maybe the other of this
1101:25 - dataset found it easier to
1101:27 - gather happy images
1101:29 - as compared to angry
1101:31 - images.
1101:33 - Another very important point to note here
1101:35 - is the fact that the kind of problem
1101:37 - we're trying to solve is that of
1101:39 - multi-class classification.
1101:41 - So unlike previously where
1101:43 - we had a model,
1101:45 - we had some data which when passed
1101:47 - in we would say
1101:49 - whether this is a
1101:51 - parasitic or non-parasitic
1101:53 - input cell
1101:55 - whereas here
1101:57 - what our model outputs
1101:59 - isn't just either
1102:01 - one class or the other but
1102:03 - a model which
1102:05 - outputs a given class
1102:07 - out of several options.
1102:09 - So those options could be
1102:11 - 3, 4, 5, 6, 7
1102:13 - or even
1102:15 - 1000 different possibilities.
1102:17 - Now all the different possibilities are
1102:19 - termed classes. So in this case
1102:21 - we have 3 classes.
1102:23 - The first class is
1102:25 - that of the angry
1102:27 - person, the next class
1102:29 - happy person, the other class
1102:31 - a sad person. Now to be able to
1102:33 - download this data, the very first thing
1102:35 - we want to do is create
1102:37 - this new API token right here.
1102:39 - So you get to your
1102:41 - profile, click on account
1102:43 - and you'll have this here. Click here
1102:45 - and save
1102:47 - this Kaggle.json file.
1102:49 - So we'll get that saved.
1102:51 - Then getting back to the Colab notebook
1102:53 - we'll simply copy this in here.
1102:55 - Okay
1102:57 - and then we should have this here.
1102:59 - Now before moving on, it's
1103:01 - important to note that you will have to sign up
1103:03 - for a Kaggle account.
1103:05 - And then the next step will be to install
1103:07 - this Kaggle
1103:09 - package. So
1103:11 - pick install
1103:13 - Kaggle, run that cell
1103:15 - then we
1103:17 - create this directory. So we make this
1103:19 - directory and then we copy this
1103:21 - JSON file in this directory.
1103:23 - So again we run this cell.
1103:25 - Then from here we give the user
1103:27 - full rights to read and write into this
1103:29 - Kaggle file right here. We run
1103:31 - the cell
1103:33 - and then get back to Kaggle where we
1103:35 - are going to copy this API command.
1103:37 - Now after copying
1103:39 - this API command, we're going to
1103:41 - paste this just here. So here we
1103:43 - have Kaggle datasets
1103:45 - download. So we're going to download this
1103:47 - dataset, this human emotions
1103:49 - dataset. Let's run this cell.
1103:53 - Our dataset has been downloaded.
1103:55 - Now we're now left to
1103:57 - unzip this downloaded file.
1103:59 - Let's click this here. You see we have
1104:01 - the emotions dataset.zip. We now
1104:03 - unzip this and store in this dataset
1104:05 - folder. So you're
1104:07 - going to run this cell. You'll
1104:09 - see that it's going to create this folder
1104:11 - dataset and in this dataset
1104:13 - you have the emotions dataset
1104:15 - which we saw in Kaggle. This is called
1104:17 - the dataset right here. From
1104:19 - this point we'll go on to generate
1104:21 - a TensorFlow dataset
1104:23 - based off the
1104:25 - images which are found in the
1104:27 - directories. So we're going to make use
1104:29 - of this image dataset
1104:31 - from directory method
1104:33 - which is in the
1104:35 - Keras utils
1104:37 - package right here. So here
1104:39 - we have this
1104:41 - description. Here we
1104:43 - have these different arguments
1104:45 - directory. Now the directory
1104:47 - let's copy this. So we
1104:49 - see clearly this in the code
1104:51 - we copy and paste this out
1104:53 - here. This directory
1104:55 - is going to be simply
1104:57 - one of these directories here. So here
1104:59 - first thing we'll do is create our train
1105:01 - dataset. So that's our train
1105:03 - dataset which is a TensorFlow dataset. We've seen
1105:05 - this already. We're
1105:07 - going to specify the directory. So let's
1105:09 - call this
1105:11 - train directory
1105:13 - and
1105:15 - we'll simply copy this path.
1105:17 - So let's copy this path
1105:19 - and then paste it out
1105:21 - here. Copy that path.
1105:23 - Paste that. Now we have
1105:25 - our validation
1105:27 - directory.
1105:29 - We're going to do the same thing. So
1105:31 - basically again we're going to copy this
1105:33 - here. Copy here
1105:35 - and paste.
1105:37 - Now here we're going to use
1105:39 - this as our train dataset and we're going to use
1105:41 - this as our validation dataset.
1105:43 - So here's our train. Here's our
1105:45 - validation and that's it.
1105:47 - So with this let's run this cell.
1105:49 - That's it.
1105:51 - We get back here. We
1105:53 - change this to train
1105:55 - directory. Now here
1105:57 - the levels are inferred
1105:59 - and this means that the
1106:01 - levels will be generated from the
1106:03 - directory structure. So if you
1106:05 - could look at this here
1106:07 - this example. Okay
1106:09 - you have this main directory and
1106:11 - you have the directory structure. That's why
1106:13 - at the beginning we made
1106:15 - mention of the fact that it's important to
1106:17 - maintain this kind of directory structure
1106:19 - when dealing with classification problems.
1106:21 - So because we have this kind of directory
1106:23 - structure where we have a class
1106:25 - we have images
1106:27 - we have class and then
1106:29 - images like we can see here we have this
1106:31 - in the train we have the class
1106:33 - we have the images. We have
1106:35 - this class and this images. This class and this
1106:37 - images. We are able to
1106:39 - automatically create this
1106:41 - kind of dataset.
1106:43 - And that's the role of
1106:45 - this inferred right here.
1106:47 - Then the next one we have is
1106:49 - level mode. Now for the level
1106:51 - mode we have several
1106:53 - modes. Here
1106:55 - the default we have the int
1106:57 - we have categorical, we have
1106:59 - binary unknown. Now let's explain
1107:01 - what these different modes mean.
1107:03 - For the integer mode we simply
1107:05 - meaning that when we design
1107:07 - or the way we design our dataset is
1107:09 - such that we have an image
1107:11 - and we have
1107:13 - a level. So let's say we have this
1107:15 - image of this person. The person
1107:17 - is happy.
1107:19 - And we have three different possibilities
1107:21 - either
1107:23 - the person is angry
1107:25 - happy or sad.
1107:27 - So here
1107:29 - what we're going to have is angry
1107:31 - angry we're going to give it a value of zero
1107:33 - happy a value of one
1107:35 - sad a value of two.
1107:37 - So here are the three different possibilities.
1107:39 - And so when designing this
1107:41 - dataset or when creating this dataset
1107:43 - we'll criticize that we have the image and
1107:45 - this integer which reflects
1107:47 - what that
1107:49 - the emotion in that image.
1107:51 - So here in our case in the case of happy
1107:53 - would simply have an output
1107:55 - of one. Now that
1107:57 - said either we create
1107:59 - our dataset in this way or
1108:01 - we use a one heart representation
1108:03 - of this
1108:05 - levels. So what this
1108:07 - means is instead of having a zero
1108:09 - we'll create a vector of
1108:11 - size three which has three different
1108:13 - positions. Where
1108:15 - if we have
1108:17 - an angry input
1108:19 - so if the person is angry
1108:21 - we'll put a one on this position
1108:23 - and zero zero
1108:25 - here. If the person is
1108:27 - happy we'll put a
1108:29 - zero here a one
1108:31 - and a zero. And if the
1108:33 - person is sad we'll put a
1108:35 - zero here a zero
1108:37 - and a one. And the way
1108:39 - this encoding works is that
1108:41 - where we have a zero
1108:43 - you see the position is one for
1108:45 - the one heart representation
1108:47 - where we have a one happy person
1108:49 - we put a one at the first position
1108:51 - where we have a
1108:53 - sad person who put a two
1108:55 - we have a two and so we put
1108:57 - a one at the second position
1108:59 - and then
1109:01 - with this there will be some differences in the designing
1109:03 - of the last function which we are going to see
1109:05 - in subsequent
1109:07 - sessions. Now we have this
1109:09 - binary for the binary is like the
1109:11 - previous example of the previous
1109:13 - project we worked in where we have
1109:15 - two classes
1109:17 - so where we have two classes we just
1109:19 - have the level mode to be binary
1109:21 - in our case we may pick out
1109:23 - the integer or the categorical
1109:25 - then from
1109:27 - here we have the class names. Now
1109:29 - if you decided to infer directly
1109:31 - from the class directory then it's
1109:33 - important to make sure that those class names match
1109:35 - the subdirectories. So
1109:37 - you should ensure that
1109:39 - if you're for example you have this
1109:41 - class names let's put
1109:43 - out the class names here. So
1109:45 - here we'll have class names
1109:47 - and let's say
1109:49 - we want to have
1109:51 - angry
1109:53 - we have angry, we have
1109:55 - happy, we have
1109:57 - sad, we have
1109:59 - sad. Okay so here's our
1110:01 - class names. Now
1110:03 - if we don't
1110:05 - put this exactly
1110:07 - as it is here we should have an error
1110:09 - so let's run this. We have class
1110:11 - names there. We get back
1110:13 - here and instead of having this here
1110:15 - we're going to specify our class names
1110:17 - so here we have
1110:19 - class names
1110:21 - there we go. So we
1110:23 - have that. The next is the color
1110:25 - mode, RGB, the batch
1110:27 - size by default is 32
1110:29 - the image size, here
1110:31 - the default is 256 by 256
1110:33 - we could always modify this. Shuffling
1110:35 - by default is true so by default
1110:37 - we're going to shuffle our dataset to
1110:39 - we do not need to explicitly
1110:41 - do this shuffling so we
1110:43 - just have to specify this to be
1110:45 - true and that's done. Now
1110:47 - for reproducibility we could
1110:49 - give a seed such that
1110:51 - we always have the same
1110:53 - shuffling. Then
1110:55 - we have this validation split
1110:57 - so here in a case
1110:59 - where let's say we have just one
1111:01 - directory. Let's say we just have the dataset
1111:03 - directory. We do not have this test. We
1111:05 - may want to split this directory into
1111:07 - the training and validation. So here
1111:09 - we could have your
1111:11 - 0.2 and
1111:13 - automatically we're going to have a split
1111:15 - of this data
1111:17 - set into the training and validation
1111:19 - data sets. Now once
1111:21 - you have the split, since you're creating the
1111:23 - train data set, you specify that this
1111:25 - is training. So training
1111:27 - is actually in the documentation here
1111:29 - you have your
1111:31 - validation split
1111:33 - an option of float between 0 and 1
1111:35 - which is logical since it was split
1111:37 - in our dataset. And then
1111:39 - we specify either training
1111:41 - or validation. So let's
1111:43 - get back here. So in this case
1111:45 - it will be training. The interpolation
1111:47 - bilinear, following is false
1111:49 - crop to aspect ratio false.
1111:51 - So these are the default values.
1111:53 - We're not going to use this. Take this off
1111:55 - and in this
1111:57 - case, or in the case where we would have had
1111:59 - validation, we just have your validation.
1112:01 - So anyway, we're not going to use
1112:03 - this. Let's take this off.
1112:05 - That said, by default
1112:07 - the validation split is
1112:09 - known. So optional
1112:11 - float, there's no, we're not going to do any
1112:13 - validation. So that said, let's
1112:15 - have this off
1112:17 - and let's run this and see
1112:19 - what our dataset, our training dataset
1112:21 - is about. So here we have
1112:23 - 6799 files
1112:25 - belonging to three classes. Now
1112:27 - let's go ahead and
1112:29 - modify this. Let's
1112:31 - add that. This is that
1112:33 - let's run this now
1112:35 - and see what we get.
1112:37 - You see already we have this error
1112:39 - because there's no match and the class name
1112:41 - is passed in a match. The names
1112:43 - of the subdirectories of the target directory
1112:45 - so expected this, but instead
1112:47 - received this. So let's get
1112:49 - back, modify it here.
1112:51 - There we
1112:53 - go. That's fine.
1112:55 - We run this here
1112:57 - and that's
1112:59 - fine. Okay, so that's our
1113:01 - training data. We could do the same
1113:03 - for our validation data.
1113:05 - So validation
1113:07 - or let's say a vowel
1113:09 - and your string. Okay,
1113:11 - so your string, your vowel
1113:13 - and we have the vowel
1113:15 - directory. We have
1113:17 - the vowel directory. Let's
1113:19 - ensure that here we call this vowel
1113:21 - vowel directory.
1113:23 - We get back
1113:25 - and that's it. Okay, here's
1113:27 - inferred, level, mode, int, class
1113:29 - name, same class names, RGB,
1113:31 - 32, image size, 256.
1113:33 - Now let's change
1113:35 - this and have
1113:37 - configuration
1113:39 - and batch size.
1113:41 - Okay, so
1113:43 - that's our batch size.
1113:45 - Here we have this
1113:47 - image size. So
1113:49 - image size. Take
1113:51 - this off, paste that out.
1113:53 - We have here image
1113:55 - size. Okay, so we have
1113:57 - this configuration and then
1113:59 - we could create this here.
1114:01 - Configuration
1114:05 - there we go.
1114:07 - We have the batch size
1114:09 - which will take
1114:11 - a value of 32. We have
1114:13 - the image size which will
1114:15 - take a value of
1114:17 - 256.
1114:19 - So we have that. Let's
1114:21 - run this. We get
1114:23 - back here.
1114:25 - This configuration
1114:27 - let's copy this here.
1114:29 - And that's fine. We have
1114:31 - 2,278 files
1114:33 - belonging to three different classes for our
1114:35 - validation data set.
1114:37 - From here we're
1114:39 - going to look at our data set.
1114:41 - So let's say
1114:43 - for image
1114:45 - and level
1114:47 - in
1114:49 - for I
1114:51 - in the
1114:53 - data set. Let's take one
1114:55 - batch. We're going to print out
1114:57 - I. So run that.
1114:59 - Scroll down
1115:01 - see we have the images.
1115:03 - We're interested in looking at the levels for now.
1115:05 - So let's go down.
1115:07 - Scroll
1115:09 - up to this.
1115:11 - You see the levels? We have
1115:13 - two, zero, zero,
1115:15 - one. You see the level is between zero and one
1115:17 - because we have three different classes.
1115:19 - Now let's modify
1115:21 - this. Instead of
1115:23 - having the level mode to ints, let's
1115:25 - know this is ints already. Let's
1115:27 - change this to categorical. Categorical
1115:29 - and this here
1115:31 - categorical.
1115:33 - Run that. Run this.
1115:35 - And okay.
1115:37 - So we'll see what we get. So here
1115:39 - instead of having two, here instead of having
1115:41 - two, two, what we have
1115:43 - is this one hard representation
1115:45 - here where
1115:47 - everywhere is zero except at
1115:49 - the second position. So here instead
1115:51 - of having zero, we have
1115:53 - everywhere zero except at the zeroed
1115:55 - position. So that's it.
1115:57 - Now we shall go on to visualize
1115:59 - this there. So here we have
1116:01 - this figure
1116:03 - and then we specify the figure
1116:05 - size. So we have here fixed
1116:07 - size
1116:09 - and we have this 12
1116:11 - by 12.
1116:13 - So we have that. And then
1116:15 - for images
1116:17 - and levels
1116:19 - in our train data set
1116:21 - we have this in
1116:23 - our train data set. You could always
1116:25 - change this and put, for example,
1116:27 - the validation data set. So
1116:29 - we have that. Now train data set
1116:31 - we take just one. Then we
1116:33 - create a subplot here
1116:35 - plt.subplot
1116:37 - it's going to be
1116:39 - 4 by 4 and
1116:41 - i plus 1.
1116:43 - So we have that and then we'll do
1116:45 - an image show. So we plot
1116:47 - out the image
1116:49 - basically this image
1116:51 - right here and then
1116:53 - we select a given image.
1116:55 - So we have that and then
1116:57 - we divide all
1116:59 - the pixels values by
1117:01 - 255. So we have that
1117:03 - and then next one, next
1117:05 - step is to plot out
1117:07 - the title. So now we plot out the images
1117:09 - then we then go ahead and plot
1117:11 - out the different titles.
1117:13 - Then here we have
1117:15 - the outputs, the levels
1117:17 - basically. Select the level
1117:19 - so select the particular level
1117:21 - just as we selected the image
1117:23 - and then we have
1117:25 - let's, because the levels
1117:27 - will be a 1 heart representation
1117:29 - then we have to take the
1117:31 - max. Now if you're new to
1117:33 - this you could check out the previous sessions where
1117:35 - we talk about this
1117:37 - kinds of methods here. So
1117:39 - we have the argmax of this and we
1117:41 - have an axis. Let's specify this
1117:43 - axis 0
1117:45 - that's fine.
1117:47 - I think here we should also
1117:49 - just do the plotting. So we should take
1117:51 - the axis sorry. So axis
1117:53 - and off. So let's run
1117:55 - this. So you see what we get before
1117:57 - we have that.
1117:59 - Anyway here we have numpy
1118:01 - so let's convert this to numpy
1118:03 - run that again.
1118:05 - You have this output
1118:07 - there we go. You see that we have the image
1118:09 - and it's class above. Now to convert
1118:11 - this into some words let's
1118:13 - make use of the class names. So we
1118:15 - have class names
1118:17 - class names and
1118:19 - then here we have
1118:21 - this. We run this again.
1118:23 - We now get the different images and
1118:25 - your levels. At this point
1118:27 - our dataset is now ready
1118:29 - for training. We just have to
1118:31 - include this prefetching
1118:33 - here for a more efficient usage
1118:35 - of our data. We saw
1118:37 - these kinds of prefetching.
1118:39 - We explained what prefetching
1118:41 - was all about in some previous
1118:43 - sessions. So here we have the prefetching.
1118:45 - We're not going to include the batching because
1118:47 - here already we've included the batch
1118:49 - size previously. So
1118:51 - in this data
1118:53 - loading right here we specify
1118:55 - the batch size. So
1118:57 - our data is already bashed. Now
1118:59 - we have that. We
1119:01 - run this solve for the training
1119:03 - and then we simply
1119:05 - redo this for the validation.
1119:07 - So this is our validation
1119:09 - data validation
1119:11 - and now
1119:13 - we're ready for building our model.
1119:15 - But before going on to build our model
1119:17 - we shall copy out this code here
1119:19 - from the previous session
1119:21 - and paste it out here. So
1119:23 - we have this resize with scale layers
1119:25 - which we're going to include in the model.
1119:27 - Now recall that
1119:29 - we could do this resizing
1119:31 - like we have our data set here.
1119:33 - So supposing you have your data set. You could
1119:35 - do the resizing and
1119:37 - reskilling. Here there's a reskilling
1119:39 - and there's a resizing to the
1119:41 - required size
1119:43 - before passing this data
1119:45 - into the model.
1119:47 - So this could be done before passing
1119:49 - into the model such that you
1119:51 - could train your model
1119:53 - on this
1119:55 - resized and reskilled data.
1119:57 - Now another method will be
1119:59 - to pass in your data
1120:01 - here. So let's suppose that we have
1120:03 - let's take this off.
1120:05 - We suppose that
1120:07 - here we have
1120:09 - in this middle we have
1120:11 - the resizing and reskilling unit.
1120:13 - So we resize and reskill before passing
1120:15 - into the model. Now
1120:17 - instead of doing this what we could do is
1120:19 - we could pass the data into the model
1120:21 - directly and then
1120:23 - carry out the resizing and
1120:25 - reskilling in
1120:27 - the model as a
1120:29 - layer in the model. Now
1120:31 - we've seen this already but I'll just explain
1120:33 - here again that doing
1120:35 - this is great for
1120:37 - deployment because when you
1120:39 - have to deploy this kind of system
1120:41 - you no longer want
1120:43 - to resize it again. So here
1120:45 - all you do now is just pass
1120:47 - in this image and then
1120:49 - the model on its own
1120:51 - the model which has been deployed
1120:53 - takes care of resizing
1120:55 - and reskilling. Unlike in this
1120:57 - kind of system where when you deploy
1120:59 - this model you
1121:01 - have to do the resizing and
1121:03 - reskilling on your own. So this means
1121:05 - that if you deploy this model on some cloud
1121:07 - solution and then you're calling it
1121:09 - from a JavaScript
1121:11 - client for example
1121:13 - or say some Android
1121:15 - client then you would
1121:17 - have to carry out this resizing
1121:19 - and reskilling in
1121:21 - JavaScript. Unlike here
1121:23 - where you just pass in the image
1121:25 - and the model takes charge
1121:27 - of resizing and
1121:29 - reskilling. So that
1121:31 - said let's take this off
1121:33 - get back to our code
1121:35 - here we're
1121:37 - going to run this simply
1121:39 - and then we start with
1121:41 - building our model. Here we have
1121:43 - this error resizing not defined
1121:45 - so simply we have Keras layers
1121:47 - here so we should
1121:49 - just as we did here
1121:51 - to the resizing
1121:53 - so we have resizing
1121:55 - there we go run that cell again
1121:57 - and then get back
1121:59 - to our model.
1122:01 - At this point our data
1122:03 - set is now ready for training
1122:05 - we just have to include
1122:07 - this prefetching here for
1122:09 - a more efficient usage of our data
1122:11 - we saw this kind of
1122:13 - prefetching how we explained
1122:15 - what prefetching was all about
1122:17 - in some previous sessions.
1122:19 - So here we have the prefetching we're not
1122:21 - going to include the batching because here already
1122:23 - we've included the batch size
1122:25 - previously. So in this
1122:27 - data set loading
1122:29 - right here we specify the batch
1122:31 - size so our data is
1122:33 - already batched. Now we have that
1122:35 - we run the cell
1122:37 - for the training and then
1122:39 - we simply redo this
1122:41 - for the validation. So
1122:43 - this is our validation data
1122:45 - validation
1122:47 - and now we're ready for
1122:49 - building our model. But before going
1122:51 - on to build our model we shall copy out
1122:53 - this code here from the previous
1122:55 - session and paste it out
1122:57 - here. So we have this resizing
1122:59 - scale layers which we are going to
1123:01 - include in the model. Now recall
1123:03 - that we could do this resizing
1123:05 - like we have our data
1123:07 - set here. So supposing you have your data set
1123:09 - you could do the resizing
1123:11 - and rescaling
1123:13 - here there's a reskilling and there's a resizing
1123:15 - to the required size
1123:17 - before passing
1123:19 - this data into the model.
1123:21 - So this could be done
1123:23 - before passing into the model such
1123:25 - that you could train your model
1123:27 - on this
1123:29 - resized and rescaled data.
1123:31 - Now another method
1123:33 - will be to pass in your data
1123:35 - here. So let's suppose
1123:37 - that we have let's take this off
1123:39 - we suppose
1123:41 - that here we have
1123:43 - in this middle
1123:45 - we have the resizing and rescaling
1123:47 - unit. So we resize and rescale
1123:49 - before passing into the model. Now
1123:51 - instead of doing this what
1123:53 - we could do is we could pass the data
1123:55 - into the model directly
1123:57 - and then carry out the resizing
1123:59 - and rescaling
1124:01 - in the model as
1124:03 - a layer in the model.
1124:05 - Now we've seen this already but
1124:07 - we'll just explain here again
1124:09 - that doing this is
1124:11 - great for deployment because
1124:13 - when you have to deploy
1124:15 - this kind of system you no
1124:17 - longer want to resize
1124:19 - it again. So here all you do
1124:21 - now is just pass in this image
1124:23 - and then the model
1124:25 - on its own this model which has been
1124:27 - deployed takes care of resizing
1124:29 - and rescaling
1124:31 - unlike in this kind of system where
1124:33 - when you deploy this model
1124:35 - you have to do
1124:37 - the resizing and rescaling on your own.
1124:39 - So this means that if you deploy this
1124:41 - model on some cloud solution and
1124:43 - then you're calling it from
1124:45 - a JavaScript
1124:47 - client for example
1124:49 - or say some Android
1124:51 - client then you would
1124:53 - have to carry out this resizing
1124:55 - and rescaling in
1124:57 - JavaScript unlike here
1124:59 - where you just pass in the image
1125:01 - and the model takes
1125:03 - charge of resizing
1125:05 - and rescaling.
1125:07 - So that said let's take this off
1125:09 - get back to our code
1125:11 - here we're
1125:13 - going to run this simply
1125:15 - and then we start
1125:17 - with building our model. Here we have
1125:19 - this error resizing not defined
1125:21 - so simply we have Keras
1125:23 - layers here so we should
1125:25 - just as we did
1125:27 - here to the resizing
1125:29 - so we have resizing
1125:31 - there we go run that cell again
1125:33 - and we're now ready for
1125:35 - modeling
1125:39 - Hi guys
1125:41 - welcome to this session on modeling
1125:43 - in which we are going to build our own model
1125:45 - which permits us
1125:47 - pass in these kinds of inputs
1125:49 - and it tells us whether
1125:51 - the input is that of a
1125:53 - person who is sad, angry
1125:55 - or happy. In the
1125:57 - session we are going to start
1125:59 - with the lunette model which
1126:01 - we saw in the previous session
1126:03 - while modifying some parameters to
1126:05 - suit the problem we're trying to solve
1126:07 - and then move on to
1126:09 - even more complex and better
1126:11 - computer vision models.
1126:13 - We copy the code from the previous session
1126:15 - and paste out here.
1126:17 - So there is it
1126:19 - we have this lunette model which we
1126:21 - saw in the previous example
1126:23 - and then we also
1126:25 - have this number
1126:27 - of classes here which we have to set
1126:29 - so that
1126:31 - set here we change this number of
1126:33 - classes from 1 to 3.
1126:35 - Now the reason why we're doing this is simply
1126:37 - because in the previous session
1126:39 - we're building a lunette model
1126:41 - which takes in images and outputs
1126:43 - whether these images
1126:45 - are those
1126:47 - of a parasitic
1126:49 - or a known
1126:51 - parasitic cell. Now in this
1126:53 - example we're taking in images
1126:55 - and our model has
1126:57 - to decide whether is that of
1126:59 - an angry person
1127:01 - a sad person or
1127:03 - a happy person. So we see
1127:05 - that here we have 3 different classes
1127:07 - and because of that we're going to
1127:09 - change this to from 1
1127:11 - to 3. So we have this
1127:13 - we run the cell that's fine
1127:15 - we also have this other
1127:17 - configuration the learning rate, number of
1127:19 - epochs, the dropout rate
1127:21 - the regularization rate, number
1127:23 - of filters, kernel size
1127:25 - number of strides, pool size
1127:27 - number of
1127:29 - layers
1127:31 - or rather outputs in this
1127:33 - dense layer and number
1127:35 - of outputs in this other dense layer.
1127:37 - So with that we're just going to
1127:39 - run this cell. Now note that
1127:41 - you could always get back
1127:43 - to the previous sessions to understand
1127:45 - each and every parameter we have here
1127:47 - as we've discussed this already.
1127:49 - So we just seem to replicate in this
1127:51 - Lunet model we've seen previously.
1127:53 - Now that cell you see the kind
1127:55 - of output we get here. We have 6
1127:57 - million parameters. We have our
1127:59 - different layers going from
1128:01 - count of layers to our
1128:03 - fully connected layers.
1128:05 - Now the next
1128:07 - point here is this
1128:09 - activation right here.
1128:11 - So this activation
1128:13 - now with the previous
1128:15 - case study was the sigmoid
1128:17 - and this was because we were actually
1128:19 - deciding whether an
1128:21 - output would be one class
1128:23 - or the other. It was a binary classification
1128:25 - problem wherein
1128:27 - we could use this kind of sigmoid
1128:29 - activation
1128:31 - function. With the sigmoid activation
1128:33 - function if you can recall we have this input
1128:35 - and we have this output. So
1128:37 - let's say x gets in and
1128:39 - y goes out. And
1128:41 - here's 0
1128:43 - the 0.5
1128:45 - here we have
1128:47 - this and
1128:49 - 1. So what goes on here is
1128:51 - as we take in higher
1128:53 - values of x the sigmoid
1128:55 - function approaches 1
1128:57 - while as we take in
1128:59 - high negative values of x
1129:01 - the sigmoid function approaches 0
1129:03 - and so the range of values
1129:05 - here is simply 0 to 1
1129:07 - and this is logical since
1129:09 - in the binary classification
1129:11 - kind of setting we wanted
1129:13 - to output a 0 for one
1129:15 - class or a 1 for
1129:17 - another class. Now intermediate values
1129:19 - like 0.7
1129:21 - will be taken to be
1129:23 - either one of these
1129:25 - classes depending on our threshold.
1129:27 - But then the role of the sigma
1129:29 - here is to make sure that
1129:31 - all our values that's based
1129:33 - on all the inputs we are
1129:35 - able to have an output
1129:37 - which always lies between
1129:39 - 0 and 1 as we
1129:41 - could see here. See that we are always
1129:43 - in this range here 0
1129:45 - to 1. Now
1129:47 - in a case where we are having
1129:49 - a multi-class problem
1129:51 - like in this case where we have
1129:53 - three different classes like
1129:55 - this and that
1129:57 - what we don't want is to just
1129:59 - say for example here we have
1130:01 - an output 0.1
1130:03 - output 0.7
1130:05 - output say
1130:07 - 1. So we do not want
1130:09 - this kind of
1130:11 - outputs right here since we are dealing
1130:13 - with a multi-class
1130:15 - problem with
1130:17 - single level. Since
1130:19 - here our output cannot be
1130:21 - two of those
1130:23 - classes so in some problems
1130:25 - or some kind of problems
1130:27 - we may have a situation where the person is
1130:29 - maybe sad and
1130:31 - say angry at the same time.
1130:33 - Now in those kinds of
1130:35 - situations you could
1130:37 - thought to have
1130:39 - these kinds of outputs where say
1130:41 - this two here, these two classes
1130:43 - have high values. Nonetheless
1130:45 - in the kind of problem we are trying to
1130:47 - solve we want
1130:49 - the model to choose or
1130:51 - pick only one out of
1130:53 - the three different classes. So we
1130:55 - are not going to pick two
1130:57 - one. We are going to pick
1130:59 - only one. And because
1131:01 - we are going to pick only one
1131:03 - what we will have here or what we
1131:05 - will try to do here is to
1131:07 - make sure that this output
1131:09 - sums up to one. So instead of
1131:11 - having
1131:13 - let's take this off, instead of having
1131:15 - this kind of output we will have an
1131:17 - output which sums up to one
1131:19 - such that
1131:21 - this here, if you have that
1131:23 - such that one, the one
1131:25 - with the highest class or
1131:27 - the one with the highest value is considered
1131:29 - as that class
1131:31 - which the model has
1131:33 - selected. And so we
1131:35 - could have different kinds of values
1131:37 - we could have say 0.1
1131:39 - here 0.2
1131:41 - but here
1131:43 - since we already have 0.1, 0.2
1131:45 - we are going to have 0.7
1131:47 - since we want to ensure
1131:49 - that the sum of all these values
1131:51 - should give us one. We look at
1131:53 - this 0.3 plus 0.7
1131:55 - gives one. So
1131:57 - we make sure that
1131:59 - our values here lie
1132:01 - between 0 and 1
1132:03 - and when we sum them up it gives
1132:05 - us one. Now
1132:07 - an activation function which we could
1132:09 - use in achieving this is the
1132:11 - softmax function. So
1132:13 - let's have here softmax
1132:15 - so instead of the sigmoid, now
1132:17 - we have the softmax. And to
1132:19 - better understand how the softmax
1132:21 - works so you clearly see that difference
1132:23 - let's take this example from
1132:25 - the analyticsvideo.com
1132:27 - website. In this example
1132:29 - you're considering that we have
1132:31 - three classes
1132:33 - in this output. Let's take this off
1132:35 - here instead of 10 like what we have
1132:37 - now here they have just these three classes
1132:39 - right here. Now
1132:41 - after applying the softmax we're going to
1132:43 - have
1132:45 - outputs here. Just like when
1132:47 - you have some output
1132:49 - after applying the
1132:51 - sigmoid you have this other output.
1132:53 - So just like for example
1132:55 - if you have an output let's say 5
1132:57 - after applying the sigmoid of 5
1132:59 - you have a value like
1133:01 - 0.99 whatever
1133:03 - so it's a value very close to 1. Whereas
1133:05 - if you pass in a value
1133:07 - say negative 10
1133:09 - after passing in the sigma
1133:11 - you're going to have a value very close
1133:13 - to 0 say 0.001
1133:15 - so this is how the sigmoid
1133:17 - works. Now for the
1133:19 - softmax function
1133:21 - is quite different.
1133:23 - Here what goes on is
1133:25 - we're going to make use of this
1133:27 - formula here. So we have
1133:29 - EX let's say
1133:31 - XI or let's say XC for a particular
1133:33 - class divided by
1133:35 - the sum of
1133:37 - EXC
1133:39 - for all the classes.
1133:41 - Now this formula shouldn't scare you
1133:43 - as we're going to explain how it works in
1133:45 - detail here. That said
1133:47 - we're having this input just like with the sigmoid
1133:49 - where you have the
1133:51 - input coming from the
1133:53 - output of the dense layer.
1133:55 - Now here we have this here
1133:57 - we have obviously three inputs because we have
1133:59 - two different classes so we have this one
1134:01 - this one and this one.
1134:03 - Now you should note the values here
1134:05 - let's take this off so you can see the value here.
1134:07 - So you note the values
1134:09 - this one is
1134:11 - 2.33, negative 1.46
1134:13 - 0.56
1134:15 - Now once we have this
1134:17 - we're going to simply apply this formula and
1134:19 - the way it's applied is as such. You take this
1134:21 - value 2.33
1134:23 - you have the exponential of that
1134:25 - value so you have e to the power of
1134:27 - 2.33 as you see here
1134:29 - of that particular class divided
1134:31 - by the sum
1134:33 - of all these exponentials
1134:35 - for all the different classes. So here
1134:37 - you have e to the power of 2.33
1134:39 - there is it divided
1134:41 - by e to the power of 2.33
1134:43 - plus
1134:45 - here e to the power of
1134:47 - negative 1.46
1134:49 - plus e to the power of
1134:51 - 0.56. So you have all
1134:53 - this summed up. That's basically what
1134:55 - this formula here means. For all the different
1134:57 - classes sum up e to the power of
1134:59 - the values that these classes
1135:01 - take up. Now once you have that
1135:03 - for this first class, class 1
1135:05 - you obtain this value. Now for
1135:07 - the next class you have
1135:09 - e to the power of negative 1.46
1135:11 - divided by this same
1135:13 - sum here. So basically this sum
1135:15 - is the same everywhere
1135:17 - but the only difference is the numerator
1135:19 - so you have this numerator which changes
1135:21 - here we have 0.56
1135:23 - we have this other numerator and we have this
1135:25 - value. Notice how as
1135:27 - the value goes to us
1135:29 - negative, high negative values
1135:31 - this value is approaching 0
1135:33 - while when the value is going to us a high positive
1135:35 - value it's approaching 1. So
1135:37 - this means that if we replace this and put a value
1135:39 - like say 10 we would have a value
1135:41 - even very close to 1 like say
1135:43 - 0.9. So that's
1135:45 - it and that's basically how
1135:47 - the softmax works. So this means that
1135:49 - at every given
1135:51 - point when you sum
1135:53 - up all these outputs you have
1135:55 - a total value equal 1.
1135:57 - So you could take this 0.83
1135:59 - plus 0.01 let's say
1136:01 - 0.84. Let's work in two
1136:03 - decimal places. So 0.84
1136:05 - 0.84 plus 0.14
1136:07 - is like 0.98
1136:09 - so you add this other
1136:11 - parts up to give you 1. So
1136:13 - basically what you're doing with the softmax
1136:15 - is you're taking some inputs
1136:17 - for let's say 3 classes
1136:19 - and then
1136:21 - you're sharing the values
1136:23 - so you have 1 that
1136:25 - you want to share. You have this number 1
1136:27 - which you want to share
1136:29 - you're going to give this one
1136:31 - a fraction of this
1136:33 - whole 1. In this case
1136:35 - the fraction is 0.83
1136:37 - and then you're going to give this other one's own
1136:39 - fraction, this other one its own
1136:41 - fraction. Such that
1136:43 - at the end all this here sum
1136:45 - up to be equal to 1. From here we're
1136:47 - going to get straight into the training and
1136:49 - start by designing our loss
1136:51 - function. So we have this loss
1136:53 - function which is going to be the
1136:55 - categorical cross-entropy
1136:57 - function. So unlike before where we
1136:59 - had the binary, now we have this
1137:01 - categorical cross-entropy. We
1137:03 - have it right here on the documentation.
1137:05 - You have TFK-RAS losses
1137:07 - categorical cross-entropy
1137:09 - and then one of the arguments
1137:11 - is this from logits. Now
1137:13 - the from logits as
1137:15 - given here,
1137:17 - let's get back here. The from logits by
1137:19 - default is false. So by default we
1137:21 - suppose that we have
1137:23 - from logits to be false.
1137:25 - And as given here, this from
1137:27 - logits says whether the
1137:29 - y-pred is what
1137:31 - the other y which is
1137:33 - outputted by the model. So
1137:35 - this is whether the y-pred is expected
1137:37 - to be a logits tensor
1137:39 - or not. And by
1137:41 - default we assume that
1137:43 - y-pred encodes a probability distribution.
1137:45 - So by default
1137:47 - we're supposing that
1137:49 - what the model is going to
1137:51 - output, in our case we have
1137:53 - three classes, what the model is going to
1137:55 - output is going to be such that when
1137:57 - we sum all these probabilities
1137:59 - or when we sum all these outputs
1138:01 - here, it's going to give us
1138:03 - a value of 1.
1138:05 - And so
1138:07 - when we have
1138:09 - by default that's
1138:11 - from logits to be false.
1138:13 - So by default we have from logits to be
1138:15 - false. When we have this
1138:17 - it's simply meaning that we're supposing
1138:19 - that what is going to get into this
1138:21 - loss function is going to be a logits tensor.
1138:23 - And that's our case here
1138:25 - because we've had
1138:27 - or we have this softmax
1138:29 - activation. Now in case we
1138:31 - didn't have the softmax activation,
1138:33 - then we will have needed to
1138:35 - specify this or set this to true
1138:37 - such that what gets into
1138:39 - the categorical cross entropy is a
1138:41 - logits tensor.
1138:43 - Now that's set, let's go
1138:45 - ahead and test
1138:47 - this example here.
1138:49 - We have this example, let's take this example
1138:51 - copy this and then
1138:53 - just paste it out here.
1138:55 - So we have this example
1138:57 - there we go.
1138:59 - Here we have CCE, categorical cross
1139:01 - entropy. We're going to print this
1139:03 - result. So we print out
1139:05 - this result and see what it
1139:07 - gives us. We run that and
1139:09 - see we have a value of 1.17.
1139:11 - Now this value here
1139:13 - tells us
1139:15 - how close the model's
1139:17 - prediction is to the true
1139:19 - values of y. Now
1139:21 - let's modify our model's
1139:23 - prediction such that it's very close
1139:25 - to the true values of y. So here we have
1139:27 - 0, might be 0.05
1139:29 - okay, 0.95 okay
1139:31 - 0, 0, 0, 0.1
1139:33 - 0, 0.8. Now we should reduce
1139:35 - this to 0.08.
1139:37 - So it's very close to 0. Now here instead of
1139:39 - 0.1 let's say we have
1139:41 - 0.0. Now let's change
1139:43 - this to 0.05
1139:45 - and then here we have 0.85.
1139:47 - Okay, so all
1139:49 - this sums to 1.
1139:51 - This too sums to 1.
1139:53 - That's it. You see they are very close now.
1139:55 - Now with this we run this cell
1139:57 - again and what do you notice? You see the value
1139:59 - drops by almost
1140:01 - a factor of 10. So this shows us that
1140:03 - these two are very close to each
1140:05 - other. Now let's change this
1140:07 - and say 0.0
1140:09 - let's have here 1.0
1140:11 - and here
1140:13 - 0, here 0,
1140:15 - here 1.0
1140:17 - we run that
1140:19 - and what do we get? You see you have a value which is
1140:21 - practically 0. And this is because
1140:23 - these two values are very close. Now if you
1140:25 - change this again, so let's have
1140:27 - here 0 and have this
1140:29 - 1.0. Let's now make
1140:31 - these values completely different from each other.
1140:33 - So here we have 1.0
1140:35 - and that. So this means
1140:37 - that this year the actual
1140:39 - prediction is this
1140:41 - position here while
1140:43 - this one or what the model predicts
1140:45 - is a different position. Now here the actual
1140:47 - is this but the model predicts this. So
1140:49 - the model fails
1140:51 - to predict for each case
1140:53 - here. So here let's run this
1140:55 - again and see how
1140:57 - high this value is. So this shows us
1140:59 - how this
1141:01 - categorical cross entropy loss
1141:03 - actually works.
1141:05 - Now to gain even more
1141:07 - in depth understanding of how this works
1141:09 - let's consider this following formula.
1141:11 - So this, the categorical cross entropy
1141:13 - loss defined here is simply
1141:15 - we have this negative here
1141:17 - the sum of
1141:19 - y true log y pred.
1141:21 - Now we take this example
1141:23 - here for this case
1141:25 - or let's modify
1141:27 - this
1141:29 - such that this first prediction
1141:31 - here matches this prediction
1141:33 - and or rather
1141:35 - this second
1141:37 - prediction matches
1141:39 - this second prediction and
1141:41 - this year doesn't match.
1141:43 - So this example
1141:45 - this doesn't match.
1141:47 - So the first example doesn't match
1141:49 - while this other example
1141:51 - matches. So what the model
1141:53 - predicts matches with what
1141:55 - the model was meant to predict.
1141:57 - Now that said let's get into
1141:59 - this formula and see how it works.
1142:01 - So we understand why
1142:03 - we're getting high values when
1142:05 - there is no matching and very low values
1142:07 - when there is a match. So here
1142:09 - for example we have 0
1142:11 - this y true just use the
1142:13 - formula 0
1142:15 - then log
1142:17 - of 0.01
1142:21 - Now we have this summation so plus
1142:23 - this next example
1142:25 - 1 this is y true 1
1142:27 - log
1142:29 - of 0.05
1142:31 - you have
1142:33 - that plus
1142:35 - next one 0 log
1142:37 - of 0.96
1142:39 - Now
1142:41 - obviously this cancels because we have 0
1142:43 - here till we have 0 this goes
1142:45 - away this goes
1142:47 - away we left only with this let's
1142:49 - take this off so
1142:51 - we left only with this year
1142:53 - after this computation
1142:55 - ok now log
1142:57 - of 0.05
1142:59 - gives us negative
1143:01 - 1.3 approximately negative 1.3
1143:03 - so we have
1143:05 - here approximately negative 1.3
1143:07 - but with this negative here it gives us 1.3
1143:09 - so we have this value of
1143:11 - 1.3. Now let's take
1143:13 - for example here for this case where
1143:15 - there is a matching we have
1143:17 - 0 log
1143:19 - of 0.1 because we have 0
1143:21 - it's not going to be considered here we have 0
1143:23 - again 0 here it's going to multiply
1143:25 - that it's going to be 0. Now this other one
1143:27 - we have 1 so this other
1143:29 - one we have 1 log
1143:31 - of 0.7
1143:33 - 1 log of 0.7
1143:35 - what do we have
1143:37 - here
1143:39 - this gives us negative
1143:41 - 0.15 so we have
1143:43 - approximately
1143:45 - 0.15
1143:47 - so we see that with where there is a
1143:49 - matching the loss is reduced and when there is no
1143:51 - matching the loss value is increased
1143:53 - so with that we just simply take this
1143:55 - off since it's our default and that's fine
1143:57 - so we have this now
1143:59 - but before continuing let's also
1144:01 - consider a case where our
1144:03 - outputs like let's get back here
1144:05 - we're going to run this again
1144:07 - get the loss we have
1144:09 - so now we're going to consider that
1144:11 - this Y true that's our
1144:13 - dataset is going to be designed such that
1144:15 - instead of having this categorical
1144:17 - kind of output we're going to have the
1144:19 - integers so here instead of
1144:21 - 0.1.0 we're going to simply have
1144:23 - 1. Now let's
1144:25 - get back to why this is
1144:27 - so we have 0.1.0
1144:29 - this is translated
1144:31 - that's this for categorical
1144:33 - for integer
1144:35 - this is translated as 1
1144:37 - because the 1 is at the first position
1144:39 - starting from 0.1.2
1144:41 - now in the case where we have
1144:43 - 0.0.1 this is
1144:45 - translated as 2 since this is at the
1144:47 - second position 0.0.1.2
1144:49 - so that's it so
1144:51 - we have seen this already in the previous session
1144:53 - now getting back here
1144:55 - let's modify this
1144:57 - instead of having this let's put
1144:59 - 1 and instead of having this
1145:01 - let's put 2 so we are
1145:03 - saying that if your dataset is designed in this
1145:05 - way recall we have seen it here
1145:07 - just
1145:09 - here let's get into dataset
1145:11 - loading just here if the
1145:13 - liberal mode is int then you should have
1145:15 - this kind of design
1145:17 - there we go
1145:19 - right here okay so we have this kind of
1145:21 - design instead so we just have these integers
1145:23 - so if you have this let's run
1145:25 - this here you see it doesn't work
1145:27 - now what we will do in the
1145:29 - case where we have this kind of
1145:31 - output is we're going to use
1145:33 - a sparse categorical so instead of
1145:35 - the categorical we have the sparse
1145:37 - categorical you see you run that and you
1145:39 - have the exact same answer you will have
1145:41 - in the case where it was
1145:43 - categorical let's get back
1145:45 - here run that again
1145:47 - so you have the exact same answer
1145:49 - so that said we
1145:51 - could make use of the sparse categorical
1145:53 - it depends on how we
1145:55 - created our dataset
1145:57 - so here we have sparse categorical
1145:59 - and we should just comment this
1146:01 - since we're not making use of that
1146:03 - let's take this off now and then
1146:05 - we move on to our
1146:07 - metrics so here we have the metrics
1146:09 - the metrics we'll be
1146:11 - using here will be the categorical
1146:13 - accuracy so we have
1146:15 - categorical accuracy
1146:17 - let's give it a name
1146:19 - let's call that
1146:21 - accuracy
1146:23 - and then we will also have the top
1146:25 - k accuracy there we go
1146:27 - we have our top k
1146:29 - categorical accuracy
1146:31 - we'll give it a name
1146:33 - no before giving it a name we need to give it
1146:35 - the value of k so we'll give
1146:37 - a value of k of 2 and then we'll give it
1146:39 - a name
1146:41 - top k
1146:43 - accuracy
1146:45 - now before we move on let's explain this
1146:47 - top k categorical
1146:49 - accuracy metric right here
1146:51 - so unlike with this
1146:53 - accuracy with this categorical accuracy
1146:55 - where if we have
1146:57 - for example this four
1146:59 - stations here
1147:01 - for example
1147:03 - this doesn't blue
1147:05 - what the model predicts and those
1147:07 - in red are what the model was
1147:09 - expected to predict
1147:11 - accuracy will be computed as such
1147:13 - so we'll start with this
1147:15 - first one
1147:17 - the highest here is this
1147:19 - 0.5 the highest here is this
1147:21 - since there is no match we have a
1147:23 - 0 plus
1147:25 - we move to this next one
1147:27 - the highest here is this
1147:29 - the highest here is this
1147:31 - and the highest is this
1147:33 - at this position there is no matching
1147:35 - so we have a 0
1147:37 - and that's because the class 0
1147:39 - was picked whereas the
1147:41 - expected what should have been
1147:43 - class 1
1147:45 - now we get here the class 0
1147:47 - is predicted by the model
1147:49 - and the class 0 is also
1147:51 - what was expected
1147:53 - the actual output
1147:55 - so here we have
1147:57 - plus a 1
1147:59 - because we've gotten this correctly
1148:01 - so this is correct, this is wrong
1148:03 - this is wrong
1148:05 - now we get here we have the same situation
1148:07 - the highest here is 0.7
1148:09 - the highest here is this
1148:11 - and the positioning is such that there is a match
1148:13 - so we have plus 1
1148:15 - now because we have 4 different examples
1148:17 - we divide all this by 4
1148:19 - and multiply by 100
1148:21 - that gives us accuracy
1148:23 - of 50%
1148:25 - now
1148:27 - we get back to this case
1148:29 - here for the top k categorical accuracy
1148:31 - for this first case
1148:33 - the highest class here
1148:35 - is this one
1148:37 - the second highest class
1148:39 - is
1148:41 - this other one here
1148:43 - so we have this highest class
1148:45 - second and we have
1148:47 - this third so it's in this order
1148:49 - for this one here
1148:51 - this is our first
1148:53 - we have two
1148:55 - of them and then this one
1148:57 - this is our first
1148:59 - and this is 0.05
1149:01 - so here we have two of them
1149:03 - this here is our first
1149:05 - and this is our second
1149:07 - because we've selected k equal to
1149:09 - we're interested in just our first
1149:11 - two highest predictions
1149:13 - so with the top
1149:15 - k categorical accuracy
1149:17 - we are not interested in
1149:19 - making sure that the
1149:21 - highest prediction here matches the highest
1149:23 - prediction here what we're interested in
1149:25 - here is if
1149:27 - any of the two highest
1149:29 - matches the highest prediction
1149:31 - we expect so if
1149:33 - any of the two predictions of the model
1149:35 - matches the
1149:37 - exact prediction the model
1149:39 - should have predicted then
1149:41 - we'll give that or we consider
1149:43 - that as a correct prediction
1149:45 - so in this case here this first
1149:47 - two do not match this so
1149:49 - this is a wrong prediction and we have
1149:51 - a zero plus
1149:53 - we move to this next one
1149:55 - here the highest
1149:57 - here is this and it matches with the second
1149:59 - so we have a one so this is
1150:01 - considered now a correct prediction
1150:03 - unlike previously where we would have
1150:05 - considered this to be a wrong prediction
1150:07 - now for this one the
1150:09 - first two of these three
1150:11 - are there so obviously this will be a
1150:13 - prediction it also matches here the first
1150:15 - matches with this first year so we
1150:17 - have that plus one
1150:19 - and then this
1150:21 - other one year we have
1150:23 - the highest prediction here matches
1150:25 - with this so we have plus one so
1150:27 - here we have three
1150:29 - divided by four
1150:31 - times a hundred this means
1150:33 - that the top two
1150:35 - categorical accuracy in
1150:37 - this case is equal 75
1150:39 - percent unlike
1150:41 - the categorical accuracy
1150:43 - which is 50 percent
1150:45 - now with that we go ahead and compile
1150:47 - the model learn that model
1150:49 - let's run this we have
1150:51 - item learning rate
1150:53 - which we specified in the configuration
1150:55 - the loss function here the
1150:57 - metrics this and that's
1150:59 - it so we compile our model
1151:01 - and then we set now to train
1151:03 - there we go let's paste
1151:05 - this out and
1151:07 - here instead of this here we should
1151:09 - have configuration
1151:11 - number of epochs
1151:13 - then obviously we have
1151:15 - our training data set and we have our validation
1151:17 - data set so let's run this
1151:19 - and see what we get go down
1151:21 - and that's it see our losses dropping
1151:23 - accuracy increasing
1151:25 - and top key accuracy which is
1151:27 - clearly higher than the accuracy
1151:29 - training now complete
1151:31 - let's plot out the loss
1151:33 - curves for the validation and the
1151:35 - training as you can see here
1151:37 - both the validation and training losses
1151:39 - all drop together
1151:41 - while the accuracies
1151:43 - for the validation and training also increase
1151:45 - up to this point
1151:47 - here so it's almost getting
1151:49 - to value of one now you could
1151:51 - check this out here you see the accuracy
1151:53 - 97.8 percent
1151:55 - year
1151:57 - 98.19 percent
1151:59 - so the model is performing quite
1152:01 - well we could evaluate this model
1152:03 - on our validation data
1152:05 - and here we have the lunette model
1152:07 - dot
1152:09 - evaluate so we
1152:11 - call the evaluate method and then
1152:13 - we pass in the validation
1152:15 - data set
1152:17 - okay
1152:19 - so we run this
1152:21 - and there we go we have a loss of
1152:23 - 0.35
1152:25 - accuracy 98.33 percent
1152:27 - top
1152:29 - key or rather or better still
1152:31 - top two accuracy 99.88
1152:33 - percent
1152:35 - now note that given that the model
1152:37 - isn't over fitting the model keeps
1152:39 - or the models metrics keep
1152:41 - increasing what you could do
1152:43 - here is increase the number
1152:45 - of epochs so we could train for
1152:47 - more epochs to get even better results
1152:49 - now we're ready to test
1152:51 - out this model on some
1152:53 - image in our testing data
1152:55 - set so here we're going to have this
1152:57 - image or let's call it test
1152:59 - image we have test image
1153:01 - which is going to be read
1153:03 - which is going to be this image we're going to read
1153:05 - using OpenCV library
1153:07 - so we have here cv2.imread
1153:09 - and then just
1153:11 - in here let's open this up and the test
1153:13 - let's take
1153:15 - up say happy
1153:17 - we copy this path here
1153:19 - copy this path
1153:21 - paste it out here there we go
1153:23 - and then we're going to convert
1153:25 - this into a tensor
1153:27 - let's close this
1153:29 - we have here now
1153:31 - our test image
1153:33 - let's say im is equal
1153:35 - tf.constant
1153:37 - test image
1153:39 - and then we're going to specify the data type
1153:41 - so here is
1153:43 - float32
1153:45 - and then with that we're just going to pass this
1153:47 - so let's print out the output
1153:49 - let's first of all
1153:51 - print out the shape
1153:53 - see the image shape
1153:55 - 90 by 90 by 3
1153:57 - pass this into our model
1153:59 - directly because as we have designed
1154:01 - this we have put in
1154:03 - the resizing in the
1154:05 - model and the rescaling
1154:07 - tool so we're going to resize
1154:09 - and then we're going to rescale in the model
1154:11 - such that now
1154:13 - we do not need to do that out of
1154:15 - the model so
1154:17 - that's sad let's get back here
1154:19 - and all we need to do is call
1154:21 - our line model but before calling
1154:23 - that we need to add
1154:25 - one dimension since
1154:27 - we're passing this input in the
1154:29 - model as batches
1154:31 - so we add the batch dimension
1154:33 - here tf.expand
1154:35 - dimensions
1154:37 - and then we have the
1154:39 - image we specify the axis
1154:41 - 0
1154:43 - so once we have this now
1154:45 - let's have our line model
1154:47 - model which takes
1154:49 - in that image so here we're going to
1154:51 - print out this output
1154:53 - or print out what our model
1154:55 - gives us we're getting this error here
1154:57 - where we're told that there's this
1154:59 - incompatibility issues
1155:01 - between the input image
1155:03 - and what
1155:05 - the model expects
1155:07 - that said we get back to the model
1155:09 - and we noticed that we didn't actually put
1155:11 - the resizing so let's
1155:13 - get back here and we have
1155:15 - this resized rescale layer which we have
1155:17 - built already and we
1155:19 - put that instead of this
1155:21 - reskilling we have this
1155:23 - resized rescale so we make sure we
1155:25 - resize and we reskill
1155:27 - ok now
1155:29 - next thing we have to do is
1155:31 - we have to modify this here because
1155:33 - here by doing this we suppose
1155:35 - that the input is going to be
1155:37 - 256 by 256 but
1155:39 - here what we will have is that we will have
1155:41 - this known so our
1155:43 - input could be any of any dimension
1155:45 - but we are going to
1155:47 - do the resizing here
1155:49 - so we have the resizing in this
1155:51 - resized rescale layer and then
1155:53 - we also have the rescaling so that's
1155:55 - it let's run this
1155:57 - there we go as you could
1155:59 - see you have from here
1156:01 - we have this
1156:03 - 256 by 256 by
1156:05 - 3 and that's because
1156:07 - we've actually passed this input
1156:09 - into our resized rescale layers
1156:11 - so let's go ahead
1156:13 - and retrain our model
1156:15 - training and validation
1156:17 - plots for the loss and accuracy
1156:19 - and with this we could
1156:21 - go ahead and test
1156:23 - our image so let's run that
1156:25 - and this is what we get
1156:27 - as you could see here we have
1156:29 - 0.99
1156:31 - and then almost
1156:33 - 0 so this shows us clearly
1156:35 - that the class
1156:37 - 1 in this case
1156:39 - because here this is our
1156:41 - class 0
1156:43 - I'll take that off
1156:45 - this is our class 0
1156:47 - class 1 and class 2
1156:49 - so this
1156:51 - image is of class 1
1156:53 - and it's correct because
1156:55 - this is a happy image
1156:57 - so basically you see
1156:59 - how to
1157:01 - create this
1157:03 - image here, this image
1157:05 - array from the
1157:07 - file path and then
1157:09 - convert this into a tensor
1157:11 - which is then passed into the model
1157:13 - without any pre-processing
1157:15 - another thing we
1157:17 - want to do is to actually print out
1157:19 - the class so what we can do here
1157:21 - is instead of this lieutenant model
1157:23 - we'll have tf.argmax
1157:25 - so we'll look for the class
1157:27 - with the highest probability
1157:29 - which in this case is this one
1157:31 - we'll look for that class so tf.argmax
1157:33 - we'll specify the axis
1157:35 - now if you're new to this you can check our
1157:37 - previous sessions where we treat
1157:39 - these kinds of functions
1157:41 - authorically so here we have that
1157:43 - let's run this
1157:45 - you see we have 0
1157:47 - no this is tf.argmax
1157:49 - this is negative 1 or 1
1157:51 - let's say negative 1 is the last axis
1157:53 - ok we have that you see
1157:55 - it picks out this here and then from here
1157:57 - let's convert this to numpy
1157:59 - and then
1158:01 - from here we use the class
1158:03 - names to get its name
1158:05 - so there we go, class names
1158:07 - we run that
1158:09 - we're given this
1158:11 - let's see what we
1158:13 - obtained before this class names
1158:15 - let's take this off
1158:17 - and also take this one off
1158:19 - we run that
1158:21 - ok we have this list so we should take
1158:23 - the 0th element
1158:25 - there we go now we have that
1158:27 - we put in the class names
1158:29 - and
1158:31 - we get the name
1158:33 - so that's it you see we get the name happy
1158:35 - ok so with that now
1158:37 - let's do one last test
1158:39 - let's take sag for example
1158:41 - and we see how easy
1158:43 - it is to carry out such tests
1158:45 - let's pick out an image here
1158:47 - let's take this one you could actually view
1158:49 - this image here so that's it
1158:51 - this one
1158:53 - the same image
1158:55 - take out this other one
1158:57 - ok so let's copy this path
1158:59 - take this off
1159:01 - take this off
1159:03 - scroll up and then simply
1159:05 - paste it here so this is
1159:07 - the path we're trying to know
1159:09 - exactly from the model
1159:11 - what kind of image it is
1159:13 - so you see here it's a sad image
1159:15 - and this is from the test set
1159:17 - so make sure you're doing this
1159:19 - kind of testing with data the model
1159:21 - has never ever seen
1159:23 - ok so that's it our model
1159:25 - is performing quite well
1159:27 - we can do something similar to
1159:29 - what we had here but with the
1159:31 - difference that instead of just giving out these levels
1159:33 - we'll give out not only these levels
1159:35 - but also what the model
1159:37 - predicts so let's copy this code here
1159:39 - get back to
1159:41 - our testing
1159:43 - there we go and paste it out here
1159:45 - ok so again we're
1159:47 - not going to use the train but the validation
1159:49 - data set
1159:51 - so there we go we have our validation data set
1159:53 - we have this plot and then
1159:55 - at the level of the title we'll plot
1159:57 - let's have this true
1159:59 - level
1160:01 - level right here and then
1160:03 - we'll have
1160:05 - the predicted level so let's have
1160:07 - this move to the next line
1160:09 - and then we have
1160:11 - predicted
1160:13 - level
1160:15 - yeah
1160:17 - predicted level there we go
1160:19 - and
1160:21 - that's fine so we have
1160:23 - this predicted level
1160:25 - and we'll get it from here
1160:27 - so we could simply have the net model
1160:29 - the net model
1160:31 - here we have
1160:33 - our net model
1160:35 - which takes in the images
1160:37 - so it takes in the image
1160:39 - selects a particular image and
1160:41 - let's do the let's
1160:43 - expand them so we have
1160:45 - expand dimension
1160:49 - take in that image
1160:51 - axis
1160:53 - equals zero and we close that
1160:55 - ok so we have
1160:57 - this, we pass in the
1160:59 - net model and we have this output
1161:01 - now we are going to
1161:03 - do something similar again to what we had here
1161:05 - basically it's even
1161:07 - this here so let's just copy this
1161:09 - let's just copy this and
1161:11 - replace it right here so it's
1161:13 - the same thing we're trying to do
1161:15 - all we're trying to do here is
1161:17 - actually passing
1161:19 - the image into the model and get
1161:21 - it's class so we compare
1161:23 - with the actual
1161:25 - level so here we have
1161:27 - this class names and this
1161:29 - we have plus
1161:31 - that should be fine
1161:33 - and that's it
1161:35 - with this now let's run this cell
1161:37 - and see what we get
1161:39 - scroll
1161:41 - here is what we get you see the
1161:43 - model this is supposed to be true
1161:45 - the model here doesn't
1161:47 - perform quite well unlike
1161:49 - what we had in this evaluation
1161:51 - so let's check out our code and see if
1161:53 - there's any errors
1161:55 - scroll this way
1161:57 - here you see we have image
1161:59 - so we have that
1162:01 - that's why you see the predicted is always sad
1162:03 - so we have always
1162:05 - predicted level always sad because
1162:07 - we had picked just one
1162:09 - image and it's not dynamic so here
1162:11 - we have those images we select the particular
1162:13 - image and
1162:15 - we run this again
1162:17 - this should be fine
1162:19 - we get in this error
1162:21 - because we didn't add the
1162:23 - batch dimension unlike here
1162:25 - we added a batch dimension before passing in here
1162:27 - so let's get back
1162:29 - and then we're going to replace that
1162:31 - images
1162:33 - the image with this code here
1162:35 - so let's get back here and then
1162:37 - replace this with this code
1162:39 - note that we've treated this already in
1162:41 - previous sessions so you can always
1162:43 - check out if you're new to methods
1162:45 - like expand deems
1162:47 - we run that
1162:49 - and see what we get
1162:51 - ok so that's it
1162:53 - you see happy happy
1162:55 - angry happy true level
1162:57 - happy predicted happy
1162:59 - oh let's see if there's
1163:01 - any errors
1163:03 - the model does quite well
1163:05 - see no errors so it's almost
1163:07 - 100% in fact it's 100%
1163:09 - although the evaluation here
1163:11 - shows 98%
1163:13 - 98.33%
1163:15 - the next thing we'll do is plot out the confusion
1163:17 - matrix
1163:19 - so we're going to go through our validation data
1163:21 - for EAM level
1163:23 - in our validation
1163:25 - data set
1163:27 - we are going to
1163:29 - start those levels in
1163:31 - this list right here so we have
1163:33 - levels that append the level
1163:35 - and then we also
1163:37 - have predicted
1163:39 - that append
1163:41 - the lunette model
1163:43 - which takes in the images
1163:45 - so here we have what the model predicts
1163:47 - and what the model
1163:49 - should predict
1163:51 - so here we have predicted
1163:53 - create this list
1163:55 - and then levels we also have
1163:57 - this list there we go so we have
1163:59 - now this
1164:01 - that's it so with this now let's run
1164:03 - the cell
1164:05 - and then before moving on we'll convert this
1164:07 - to numpy format so we could easily
1164:09 - manipulate it so we have this as
1164:11 - numpy we run this
1164:13 - again that's fine
1164:15 - then here
1164:17 - we could print out the levels
1164:19 - we have levels
1164:21 - let's print this out
1164:23 - there we go you see the levels the different
1164:25 - levels now let's try to flatten
1164:27 - out all these different levels right here
1164:29 - so let's have this to be
1164:31 - flattened now let's scroll up
1164:33 - see this
1164:35 - they're in batches of 32
1164:37 - so
1164:39 - we can actually see that here
1164:41 - scroll down a bit you see we have this
1164:43 - batch here then we have the next batch
1164:45 - and so on and so forth but then this
1164:47 - output format isn't exactly what
1164:49 - we want what we want is
1164:51 - the classes
1164:53 - the class with the highest
1164:55 - core so instead of this one representation
1164:57 - we want the integer representation
1164:59 - so what we're going to do here is
1165:01 - use the Agmax so let's
1165:03 - print out the Agmax of
1165:05 - this
1165:07 - let's get back up let's print out the
1165:09 - Agmax of this
1165:11 - there we go and then we specify
1165:13 - the last axis
1165:15 - so we have that and
1165:17 - we run that
1165:19 - or we get an error or
1165:21 - we have this error now let's
1165:23 - let's do this let's pick
1165:25 - out or let's simply
1165:27 - select here
1165:29 - up to the last value
1165:31 - now this error should be coming in because
1165:33 - we have batches of 32
1165:35 - but it happens that
1165:37 - if you have a data let's suppose you have a data
1165:39 - set of 48 items
1165:41 - now if you break this in batches of 32
1165:43 - then you have let's even
1165:45 - say 98
1165:47 - so here you have the first batch 32
1165:49 - the next batch 32
1165:51 - the next batch 32 so here you already
1165:53 - have 96 elements
1165:55 - and then the last batch will
1165:57 - be 2 because you have
1165:59 - you want to have 98 elements here you have
1166:01 - 96 plus 2 given 98
1166:03 - so because of this last batch here
1166:05 - we get in this error
1166:07 - here so because of that we
1166:09 - get the error so what we could do
1166:11 - is print right up to this last
1166:13 - batch so let's
1166:15 - run that so we're not going to pick
1166:17 - out the last batch here we're not picking out the last
1166:19 - batch so we go from
1166:21 - the first batch to right up to
1166:23 - the batch before the last
1166:25 - one so let's run this now
1166:27 - and see that that's fine so this works
1166:29 - out well we could even print out
1166:31 - say the first two batches
1166:33 - let's print out the first two batches
1166:35 - so you see what that looks like see
1166:37 - the first two batches you have this here
1166:39 - and this
1166:41 - now from here you could actually flatten
1166:43 - so you could do this you flatten
1166:45 - this so you get all
1166:47 - this in this single
1166:49 - or one dimension list
1166:51 - so this is what we want to get
1166:53 - now let's move on
1166:55 - so let's get back to this we have
1166:57 - right up to the last batch
1166:59 - we run
1167:01 - we run this
1167:03 - and there we go so this is
1167:05 - what we get so we've actually flattened out
1167:07 - all these elements now if you print out the length
1167:09 - of this here
1167:11 - see you print out this length
1167:13 - you see you have 6784
1167:15 - elements so now basically
1167:17 - we want to compare these different
1167:19 - predictions here we want to compare
1167:21 - this predictions let's take this length off
1167:23 - we want to compare this
1167:25 - levels sorry we want to compare
1167:27 - all those levels with the
1167:29 - models predictions so here we could
1167:31 - repeat the same
1167:33 - process with the predicted let's have
1167:35 - predicted
1167:37 - or predicted we run that
1167:39 - you see we have these two lists
1167:41 - here you see already that they are quite similar
1167:43 - although this one misses out so what
1167:45 - we're doing is we're having this list and
1167:47 - this other list this is for the predicted
1167:49 - and this is for the levels so now
1167:51 - that we have this set
1167:53 - let's
1167:55 - or let's redefine our pred let's say
1167:57 - pred we will call
1167:59 - pred to be this year so this
1168:01 - our pred which will flatten out so
1168:03 - these are the different predictions by the model
1168:05 - and then this is what
1168:07 - the model was supposed to predict so
1168:09 - here are the levels so pred
1168:11 - and levels
1168:13 - okay so let's
1168:15 - have that we run that cell
1168:17 - and then we get back to the code
1168:19 - for the confusion matrix which we had
1168:21 - previously so basically here we had
1168:23 - seen in the previous sessions we
1168:25 - defined the threshold because we have
1168:27 - in binary classification problem here
1168:29 - since we have different classes we wouldn't
1168:31 - define that we will just
1168:33 - simply pass in as year
1168:35 - this different
1168:37 - predictions so for the
1168:39 - levels and for the predicted
1168:41 - we have that let's simply
1168:43 - copy this out and
1168:45 - paste out here so we've seen
1168:47 - this already let's take this off
1168:49 - now we have
1168:51 - level and then
1168:53 - here we have the predictions
1168:55 - there we go we're going to print
1168:57 - out this confusion matrix
1168:59 - we have the figure and
1169:01 - that's it so let's run this
1169:03 - and see what we get there we go
1169:05 - we see already here we have this confusion matrix
1169:07 - and one thing you can notice
1169:09 - straight away is that
1169:11 - the most values or the leading diagonal
1169:13 - has those
1169:15 - elements with the highest values so it's here
1169:17 - we have the highest values here
1169:19 - and this is normal actually
1169:21 - because when you have this confusion
1169:23 - matrix like this let's redraw this
1169:25 - confusion matrix when you have this confusion
1169:27 - matrix like this this is a class zero
1169:29 - so let's say class zero is angry
1169:31 - so this is class angry
1169:33 - yeah happy
1169:35 - and sad now here it's angry happy
1169:37 - and sad so whenever
1169:39 - the prediction or whatever
1169:41 - what the model predicts matches with what
1169:43 - it was supposed to predict you have
1169:45 - an additional one which is added here
1169:47 - so we simply go through all the
1169:49 - different model predictions for the validation
1169:51 - and you see that
1169:53 - 1472 times
1169:55 - for 1472
1169:57 - times the model
1169:59 - predicted angry when
1170:01 - it was actually angry
1170:03 - so this is correct now for
1170:05 - happy you see 2000
1170:07 - here is matches here
1170:09 - 2890 times the
1170:11 - model predicts happy
1170:13 - and the actual was happy
1170:15 - and here we have in
1170:17 - 600,198 times
1170:19 - model predicts sad when it's
1170:21 - actually sad
1170:23 - and then here we have
1170:25 - 21 times the model predicts
1170:27 - angry when it was
1170:29 - happy and then 26 times
1170:31 - the model predicts angry when it was sad
1170:33 - then we have
1170:35 - 20 times the model predicts happy
1170:37 - when it was angry
1170:39 - here we have
1170:41 - 27 times the model predicts happy
1170:43 - when it was sad. Here we have
1170:45 - when he was angry and we have a hundred times the model predict sad when it was
1170:50 - instead happy so there is you see this this is the highest score we obtained
1170:55 - for the wrong predictions and so this means that the model has that tendency
1171:02 - of predicting sad when it's actually happy. You can also observe this plot
1171:08 - here you see this lighter colors you see here as we go up we get to this higher
1171:17 - values see here and then this and this so the lighter the color the higher the
1171:23 - score and then the darker the color the lower the score. Now obviously the ideal case
1171:30 - will be where we have this purely white so we would have this purely white this
1171:34 - purely white and all this you're completely dark with all zero values so
1171:41 - that said we've looked at how to have to obtain the confusion matrix which is an
1171:47 - important evaluation metric and here we could also change this to training so
1171:52 - anyway you just have to have your training and that will be fine. Now we have to deal
1171:59 - with that last batch which we did not take into consideration so what we
1172:05 - could do is we could do some concatenation here so let's do
1172:09 - concatenate concatenate and then we have that then let's copy this we just copy
1172:16 - this and paste out your copy and then paste it out here there we go but then
1172:23 - we're taking the last element so instead of the all elements before the last now
1172:28 - we're taking the last element and that's it so we add in we add in the last
1172:32 - element here so that we could actually flatten it out separately before
1172:37 - concatenating it with the previous elements we saw that having all of this
1172:42 - joined together will cause an error so that's why we're doing this here
1172:47 - again we copied this here let's copy this and then paste it out here that
1172:52 - should be fine this for the predicted so let's change this to predicted
1172:59 - okay so we should have that fine scroll this way and then let's have concatenate
1173:09 - okay so let's run this and see what we get we get in this error let's add this
1173:16 - here there we go we run that again that's fine and the same should be for
1173:22 - the predicted so let's run this again get that error scroll this way and add
1173:29 - this here okay so we have this the last batch which has been added up and then
1173:36 - we'll simply copy this and paste here so now instead of having just the all the
1173:43 - values before the last we have now all the different batches together so here is
1173:48 - for the predict now just for the level so let's put this here we have the
1173:52 - levels and then this is for the predicted let's copy this get right to
1173:57 - the end and we have that okay so here we have this for the predicted space this
1174:03 - year and that should be fine let's run this again run that and then we run this
1174:09 - and there we go see we have slightly different answers now okay so that's it
1174:15 - we've seen how to plot out the confusion matrix for multi-class
1174:19 - classification before we move on we noticed that we've made a very big
1174:24 - mistake here as with this validation data set we actually pass in the train
1174:29 - data set so let's modify this and make sure you never make this kind of error
1174:35 - as if you make this kind of error you feel like your model is performing well
1174:39 - whereas we are validating on the train data so that we run this again I have to
1174:45 - run this again now training is complete and we see that the model wasn't
1174:50 - performing as well as we thought you see here the last drops and then at some
1174:55 - point stats increasing while that other training keeps dropping and then for the
1174:59 - validation we have its accuracy going to us one as we had previously whereas for
1175:05 - validation your it plateaus at around 75% as you could see here in those values
1175:14 - so you see the validation accuracy the highest we have goes like 75% although
1175:19 - the top key accuracy is about 90% okay so that said we see that we see clearly
1175:26 - the models and performing that well and the next sessions will see how to better
1175:31 - this model performance here we could also run this evaluation now the
1175:39 - evaluation CR 7590 and loss of one let's run this testing correctly classifies
1175:49 - this next we're gonna try this out to see previously we had generally 100%
1175:56 - because that was on our same train data now here you see let's start from up you
1176:04 - see wrong prediction right wrong wrong you have here wrong right right right
1176:13 - wrong see you see that out of the 16 different predictions we have 10 out of
1176:21 - 16 rights so running that you see you have that 62% on this little sample which
1176:28 - you took here now we could also plot the confusion matrix let's run this and you
1176:35 - could see clearly from here that the model isn't performing as well as it
1176:38 - used to perform when we're making that error
1176:44 - hello everyone and welcome to this new session in which we are going to treat
1176:50 - data augmentation previously we had seen how to load our data set from this data
1176:56 - set directory right here and then we trained a Lynette model which performs
1177:02 - very well on the training data but didn't perform as well on the validation
1177:07 - date and then we're able to evaluate this model on different evaluation
1177:12 - metrics like the accuracy top key accuracy and the confusion matrix in
1177:17 - this session we are going to focus on data augmentation that is we're going to
1177:21 - see the effect of augmenting our data artificially without actually getting to
1177:27 - add an element in this data set right here and then seeing how this affects
1177:33 - our model performance in the session we'll see how to augment our data like
1177:42 - those data right here and see how this technique of data augmentation helps in
1177:48 - making the model even more performant now we are looked at the augmentation
1177:53 - previously but if there's one thing you have to note about the documentation is
1177:59 - simply the fact that it promotes diversity in your data set and so if you
1178:04 - have data like this one here let's open this we consider this original data so
1178:11 - this is our original data this is the data we actually gather and then we have
1178:16 - this brightness here so we modify this data or this image's brightness and we
1178:22 - obtain this other data point and then we modify from this we rotate this and we
1178:28 - obtain this other data point you see that is exact same image which has been
1178:33 - modified and so now the model doesn't only get used to seeing this image right
1178:38 - here but now you could see this one or this one and so data augmentation is
1178:45 - this method of this technique for promoting robustness in models hence
1178:52 - fighting over fitting as now the model can see different versions of certain
1178:57 - data points so let's close this up and get back to the code here let's get back
1179:04 - up here we have data augmentation we're gonna simply get back to this year we
1179:09 - have data augmentation we had looked at this already in the previous session so
1179:13 - you could get back and try to understand exactly how this is carried out
1179:19 - previously we have seen that we could carry out data augmentation by using
1179:24 - these kinds of TensorFlow image methods like this one you see you could rotate
1179:28 - the image you could flip left right you could adjust the saturation or you could
1179:33 - use Keras layers you're gonna use Keras layers and then also we saw how to use
1179:38 - albumentation library which is this amazing library which permits us
1179:41 - carry out data augmentation on different types not only classification very easily
1179:46 - so you could check out in that video now that said let's simply copy this out
1179:50 - here then we paste it here so we have this we're gonna we have this augment
1179:57 - layers we have random rotation random flip random contrast let's actually get
1180:03 - back here and this layers and then add the random contrast so we have random
1180:09 - contrast okay so we have that and you could feel free to get back to
1180:14 - documentation on the TensorFlow Keras layers let's get to Keras layers and
1180:21 - then check out those different augmentation strategies here you can see
1180:26 - random brightness contrast crop flip height rotation translation random
1180:31 - weight random zoom and you could check out the documentations in case you have
1180:35 - any doubts so you could just have this year and you see how to use each and
1180:42 - every one of those augmentation strategies now getting back to the code
1180:46 - we have this three year now what was generally done is you could actually run
1180:52 - one and then test this or test how it helps in making the better model perform
1180:58 - better and then you could add the other and see whether it helps and so on and
1181:03 - so forth so it's kind of like it's not a fixed kind of method where you just have
1181:09 - some fixed methods or some fixed right augmentation strategies which we just
1181:14 - place in this order sequentially and then it will always work magically
1181:18 - generally you will have to test this out different different strategies out and
1181:23 - then see which one works well for the data you're working with now we have
1181:30 - this set let's run this cell right here we have this is not defined let's let's
1181:38 - get back here let me show that this we run this cell let's run this cell
1181:42 - normally that should be fine get back and we run this one year and that's fine
1181:49 - so we have that and then now at a level of this dataset preparation we're gonna
1181:55 - include this we're gonna do the mapping so we have our augment layers not
1182:01 - that we're not gonna do this for the validation data set we're gonna only do
1182:04 - this for the training data set we have this augment layers and then we're gonna
1182:09 - specify number of parallel calls so we have known parallel calls and then this
1182:17 - is gotten via auto tune so we have that automatically then before we move on
1182:23 - you're gonna copy this out here and then simply paste this here so we create this
1182:31 - augment layer which takes this and then has that's it outputs the augmented image
1182:38 - and the level you're not gonna have this year so let's just take this off we have
1182:43 - just this image okay so that's it we have this augment the layers and then
1182:49 - we define this function as augment layer so right here let's take this one off
1182:53 - and run this again with this we're now set to train so let's go ahead and
1183:00 - retrain our model. Training is not complete and we could see that the model
1183:06 - doesn't perform as well as it used to do without augmentation so you see here we
1183:12 - did augmentation the model performs even poorer as compared to when there was no
1183:18 - augmentation let's run this here so you could see I could compare this with the
1183:24 - previous results we got other previous evaluation you see we go from 75% to 54%
1183:29 - and then you will go from 90% to 83% to understand the reason why we have this
1183:35 - drop in performance let's look at this visualization of our data set we'll see
1183:41 - that after carrying out the rotation we have images which are rotated at very
1183:47 - unusual angles so you see like this image here is unusual this angle too
1183:54 - looks very unusual as compared to the kind of data we would have in our test
1184:01 - all validation set so we have to ensure that when carrying out this random
1184:08 - operation we limit the angle at which we could carry out this rotation so that
1184:15 - said if we have a face like this or let's say we have an input image like
1184:20 - this we should limit this rotation such that this image cannot be rotated at say
1184:28 - 180 degrees so let's have this here so you see that better we have this this
1184:34 - and this so we do not want these kinds of rotations what we want is rotations
1184:39 - where we have the face be tilted like this and that's it so we want this kind
1184:46 - of rotations but not this type so to solve this problem what we're gonna do
1184:51 - is we're gonna get back to the documentation that's random rotation we
1184:55 - check out this factor right here we see that the value we put here takes us from
1185:01 - negative 20% of 2 pi, pi is 180 degrees, pi is in regions convert this to degrees we
1185:08 - have pi which is 180 degrees so we have two times 180 which is 360 so when you
1185:15 - say 0.2 or negative 0.2 what you're in fact saying is negative 20% of 360 so
1185:22 - when you have let's get back to the code so when you have your let's add this cell
1185:28 - here when you have your 0.25 what you're having is 0.25 times 360 run on this you
1185:36 - see you're going 90 degrees so this means that if you had an image which was
1185:40 - already somehow tilted this image we end up in a very unusual position where the
1185:47 - face will look something like this and so what we'll do here is we are gonna
1185:54 - limit this rotation so we're gonna we're gonna go from 0.025 for example to this
1186:01 - so let's limit that and negative we'll go from negative 0.025 meaning that we
1186:07 - have here as 0.025 that will be 90 degrees so limiting this to 90 degrees
1186:12 - and then going from negative 0.025 to 0.025 simply means that if you have this
1186:19 - axis here and then you have the face like this put them out this then after
1186:27 - rotation you can only go 90 degrees in this direction or 90 degrees in this
1186:37 - or rather 90 degrees in this direction so you have a limit so you can you can
1186:41 - only pick a random value between negative 90 degrees and 90 degrees in
1186:47 - this direction so you can only go this way 90 degrees or this way 90 degrees so
1186:52 - that said the extreme will have a face like this that's after rotation so we'll
1186:58 - go from this blue to this red or you could also get something like this so
1187:03 - this what we are gonna get after rotation unlike with the case of 90
1187:07 - degrees where if you have a face which is already let's let's change this let's
1187:13 - do this if we have a face which is already say tilted like this you have a
1187:19 - face tilted like this after rotating 90 degrees what you have now let's let's
1187:26 - have here this 90 degrees what you have now be a face tilted this way and this
1187:34 - is in a very usual position when taking an image so or when taking a photo so
1187:40 - the image is no validation or test data set wouldn't look like this and so
1187:46 - that's why we actually limited this year so let's get back to our code now and
1187:51 - we run this let's run this training data and then let's visualize our data set
1188:00 - there we go as you can see you do not have images which are upside down as we
1188:06 - had before and that's it so we have this now we now go ahead and retrain our
1188:12 - model and see what we get after training for over 20 epochs your results we get
1188:19 - you see that we go up to 78 percent so those are highest we get and when we run
1188:26 - the evaluation you see here we have this when we run the evaluation what we get
1188:31 - is 77.8 percent for the accuracy and the top case 91 percent so improved
1188:38 - compared to what we had before or what we had before the data augmentation now
1188:44 - we are going to use another data augmentation strategy which is a cut mix now
1188:49 - the cut mix isn't like this other data augmentation strategies where we just
1188:55 - modify a single image with a cut mix as we have seen in the previous sections
1189:00 - we actually combine two images so what we're going to do here is we're going to
1189:04 - simply copy out this code and then put it out in this code base right here now
1189:11 - if you haven't or if you're new to the cut mix data augmentation you could
1189:15 - check out the previous sessions where we treat this in detail so let's get here
1189:20 - and there we go we're going to apply cut mix and see the effect it has after
1189:26 - training our model so we have data augmentation in here let's let's add this
1189:31 - cell and have your cut mix augmentation add a code cell and let's base out that
1189:40 - part of the code we also paste out this part where we have the train data set 1
1189:46 - and train data set 2 then we create this mixed data set so from here we have
1189:51 - augment augments layer stick this off and here we have augment layer so we
1190:01 - carry out augmentation for this tool separately and then once we have this
1190:06 - two year we're not we're not gonna shuffle on the shuffling already so we
1190:10 - just do the mapping we do the mapping and that's what we're saying once we
1190:16 - have this two year you're gonna combine this into this one mixed data set so
1190:20 - that's it let's have that and then from here now we build our training data set
1190:27 - so let's take this year let's take this up there we go
1190:33 - data set preparation and then we're now gonna comment this part so we could
1190:39 - comment this one and we have a validation data set that's fine
1190:44 - everything looks fine and that's okay so we have this set let's run this now for
1190:51 - cut mix you could always try to mix up all the cut out augmentation strategies
1190:56 - and see how it better or how it ameliorates your model performance so
1191:03 - let's have this let's run this there we go this our cut mix here and we combine
1191:12 - the two data sets and then once they combined we now apply the cut mix
1191:16 - augmentation here we have this inside is not defined let's actually you should
1191:27 - have okay this should be our configuration so we should have this
1191:30 - configuration run this cell now run that again run the cut mix and we run the
1191:39 - cells and now everything should be fine so there we go we have all this fine and
1191:44 - then we get to validation run that training data set and validation data
1191:52 - sets okay so that's it let's go ahead and retrain our model and see what we get
1192:00 - after training for 20 epochs we noticed that unlike previously where the training
1192:06 - was went to about 99% while the validation was about 77% year the
1192:14 - trainings about 80% and the validation let's scroll this way the validation is
1192:22 - about or the highest we have here is 78% so this shows us clearly that the model
1192:29 - isn't overfitting because the training and the validation data set both are
1192:36 - evolving in a similar manner so let's scroll down here and look at this curve
1192:41 - you see here before look at the accuracy plot before what we had was something
1192:48 - like this we had the training and here we had one and then the validation was
1192:58 - like this but now we have these two curves which are evolving in similar
1193:04 - manner though sometimes we have these kinds of peaks anyway clearly our model
1193:11 - isn't overfitting so what we'll do is we're gonna train for more epochs so
1193:16 - that said let's go ahead and retrain this model for more epochs so there we're
1193:24 - gonna modify the number of epochs so here we just have to our work and those
1193:30 - in place we just keep training so we train for 20 more epochs so let's run
1193:33 - this and then we wait for 20 more epochs train are complete the other results we
1193:38 - get as you could see the validation accuracy starts to stagnate around this
1193:44 - and then after evaluation we obtain an accuracy of 79.71% but
1193:51 - what's interesting to note here is the fact that our training accuracy is still
1193:56 - having this value of 86.25% unlike previously when where our model was
1194:02 - overfitting we had this accuracy of about 99% so that's it we have that
1194:08 - evaluated we see these values and then testing on these values here you see
1194:15 - that we have 14 out of 16 images predicted correctly and here's our
1194:20 - confusion matrix right here so that clearly this model performs best or
1194:26 - performs better than all the other previous models
1194:33 - hello everyone and welcome to this new and exciting session in which we are
1194:39 - gonna treat tensorflow record tensorflow records helps us build more
1194:44 - efficient data pipelines as they help us store data which we are to use to train
1194:51 - our models more efficiently and also the help in parallelizing the reading of the
1194:57 - data hence helping in speeding up the overall training process and so in the
1195:03 - section together with the tensorflow data sets which we've seen already we
1195:08 - are going to see how to implement an efficient training pipeline with tensorflow
1195:14 - in the previous sections we've been working with tensorflow data sets in
1195:19 - the session would see how to convert our tensorflow data set into a tensorflow
1195:27 - record and then get this tensorflow record and convert it back to tensorflow
1195:34 - data sets to be used for training now the very first question you access
1195:39 - office given that we've already carried out the stringing process successfully
1195:43 - without any problems why do we need to work with tensorflow records now there
1195:50 - are two major problems tensorflow records come to solve or their two
1195:55 - advantages of working with tensorflow records the very first one is a fact
1196:00 - that you can now store your data more efficiently now notice that every time
1196:06 - you have to create this data you're creating this data from this data set
1196:10 - which will load a year which is made of little files of like say a few kilobytes
1196:16 - let's open this up so you could see for yourself we have this year let's take
1196:21 - this one open this folder and then we will see the size of this so you can see
1196:26 - the size is about 17 point 26 kilobytes now the fact that we have to do with
1196:33 - deal with this kinds of files means that we are not going to always have to load
1196:39 - this data very efficiently now it's true working with a tensorflow data sets
1196:43 - brings in some efficiency but what if we start this data in a very efficient
1196:50 - manner that is instead of storing let's say for example 17 kilobytes files like
1196:56 - this what if we just store say 10 megabyte files or say a hundred megabyte
1197:05 - files so every time we want to read from a file we don't have to read from many
1197:10 - of this kind of files but from a single file like this one then another good
1197:17 - thing with having to work with tensorflow records which is stored in this
1197:22 - kind of form is that this time around you could carry out the pre-processing
1197:30 - before storing the data so you could augment your data so we have augmented
1197:37 - data which is now stored as tensorflow records so instead of having the images
1197:44 - and then getting here and each and every time you have to carry out the
1197:47 - augmentation or some pre-processing before training you store the data which
1197:52 - has been augmented already so suppose you have your initial data so you have
1197:58 - your initial data here it passes through some pre-processing so it passes through
1198:03 - some pre-processing and then let's change this color so here we have the
1198:08 - data and then here we have the pre-processing and then from the
1198:12 - pre-processing we have the augmented data so after this the data has been
1198:18 - augmented what we do is instead of having to go through this each and every
1198:24 - time what we do is we pass through it will go through this once and we store
1198:28 - our data in this is augmented form such that the next time we want to train our
1198:33 - model all we need to do is just make use of this augmented data right here
1198:40 - apart from this it should be noted that sometimes we have models or we have
1198:46 - sections of a model let's draw it this way we have let's suppose that this is
1198:52 - our model here so this is our model is made of section 1 and section 2 the
1198:57 - section 1 is fixed meaning that we are not going to train this part but we're
1199:02 - going to train only this part this means that when you pass in data right here
1199:07 - when you take this data and you pass in here the output you would have here is
1199:12 - going to be the same each and every time given that this year is fixed the
1199:18 - weights here are fixed so what we could do is instead of storing the data we
1199:23 - are going to store this outputs which we could call embeddings so instead of
1199:28 - storing the data we store this embeddings here such that we now make
1199:33 - use of the embeddings directly and train our model or train this part of the
1199:39 - model which is actually trainable on this embeddings instead of working with
1199:43 - a data so this time around we see that we have data the pre-processing the
1199:49 - augmentation and then we have the embedded or embeddings from this data
1199:54 - so here we have the embeddings so we can now store this as tensorflow records so
1199:59 - you see that gives you it gives us some kind of flexibility as to what we can
1200:04 - store and be able to retrieve the stored data and make use of it as we wish the
1200:12 - second major advantage of working with tensorflow records apart from this
1200:18 - efficiency in storing data is the fact that they encourage the parallelizing of
1200:24 - reading data now this means that if we're having to train our model on
1200:30 - several hosts let's say we have four different hosts or say we have four
1200:36 - different machines on which we train our data like this and we have our data set
1200:41 - right here what we could do is we could create shards of our tensorflow record
1200:49 - data so here for example this let's suppose that this are our data set our
1200:54 - complete data set we could break this up into several parts so let's say we break
1201:01 - this up such that each host takes care of two so we have one two one two one
1201:06 - two one let's add another part here so each of this takes care of this so this
1201:12 - this host here trains on this data this sort of one trains on this this sort of
1201:17 - one trains on this this one trains on this now as a rule of thumb is generally
1201:24 - advisable to make sure that each host has about 10 of this packs of 10 to 100
1201:35 - mega bytes of our data set so that said if we have a 10 gigabyte data set like
1201:45 - suppose that all this now is 10 gigabyte let's take this off we have 10 gigabyte
1201:51 - data set let's write this in here so we have your 10 gigabyte data set what we
1202:00 - want to do is have here nice each and every host taking care of at least 10
1202:06 - packs of our tensor flow of our tensor flow record which we've created so let's
1202:15 - take this off it's no longer two we have to create 10 packs or 10 charts of our
1202:22 - tensor flow records and allocate that to each and every host we have here now
1202:29 - that said given that we have four hosts then we will have 4 times 10 charts to
1202:38 - create so we'll break this 10 gigabyte data set up into 40 different parts so
1202:46 - we have 40 different part packs or charts of our tensor flow data set all
1202:54 - right off our tensor flow record right here and then each and every one of them
1203:01 - will be approximately 250 megabytes since 10 gigabytes that's if we convert
1203:12 - this to megabytes we would have 10,000 suppose that a thousand megabytes equal
1203:17 - a gigabyte so we have your 10,000 megabytes divided by 40 which will give
1203:24 - us 250 megabytes per chart or per pack that we've created your split of our
1203:31 - tensor flow records and so this will lead to some reasonable gains because we
1203:39 - now can train our model on this parallelized data and then we could also
1203:50 - prefetch huge chunks of our data set to be precise 250 megabytes such that once
1203:59 - the model is ready the model just fits on this data which has already been
1204:03 - prefetched now you could check out the previous sessions where we talked about
1204:08 - prefetching under tensor flow data sets that said what we actually store in this
1204:16 - tensor flow records are this protocol buffers and the way tensor flow manages
1204:22 - this is by making use of this tensor flow example class which is defined here
1204:30 - let's check this out your tensor flow example which is defined here as a
1204:36 - standard proto storing data for training and inference and so if you have to
1204:43 - convert your data into this into the proto files you would have to make use
1204:50 - of this tensor flow example class and then you would have to understand this
1204:57 - representation right here and so in our case where we're dealing with an image
1205:02 - and its corresponding level let's say we have the image of this person smiling we
1205:09 - have the level one so we have this image and the presence corresponding level
1205:15 - would have to convert this data into this format before creating our tensor
1205:24 - flow records now here we have this dictionary as I said here it contains a
1205:30 - key value store example features where each key which is a string maps to a
1205:37 - feature now note that here we have this feature with s and here without s so we
1205:47 - have this features and each and every one of them is a feature so we could
1205:54 - click here you click here and you see this feature you check out the
1205:57 - documentation and we have the this content let's take this off the
1206:03 - content list can be one of the three types a byte list generally this is our
1206:08 - information flood list or an int64 list so you would pick your feature depending
1206:18 - on the kind of data you're having now here you have this features which is
1206:23 - like a combination of these different features here so you have here the int
1206:28 - list you have the float feature the float list the byte list and then here
1206:35 - you create the you create the features from these different features here so
1206:43 - from this from each and because this is a feature this one year this one
1206:49 - year is a feature let's take this off this one year is a feature this is a
1206:53 - feature and this is a feature this has been defined already here because the
1206:57 - ints feature is of type feature there is it float feature the same there is it
1207:03 - bytes feature the same that is it now all this combined forms features which
1207:09 - is of type features see the difference you have this and this we doubt the s
1207:14 - obviously is a single on this plural so that said if we want to create our
1207:21 - tensorflow records I want to convert our data sense tensorflow records we have to
1207:25 - take into consideration this formatting of our data so we get back to the code
1207:31 - and then we add this two imports here here we have this tensorflow train we
1207:37 - import by list float list and in 64 list so these are the types of our feature
1207:44 - and then we have from tensorflow training again we import example we
1207:50 - employ features and we import feature so with that let's run this cell get back
1207:57 - to our code we have your tensorflow records now what we'll start by doing is
1208:03 - unbatching our data so we'll start by unbatching our data note that to run
1208:10 - this we've taken we've taken off this prefetching as we will not need this we
1208:14 - just take our data as it is so we have our train data set which has been
1208:18 - augmented you could carry out any pre-processing you want before storing
1208:22 - this data and then the validation data remains validation so we have that
1208:27 - validator set there then from here we'll run the cells already from here we'll go
1208:33 - ahead and unbatch this data there we go we have training data set we run this we
1208:39 - have this year then we do the same for the validation so here we have
1208:45 - validation data set and validation data set unpatched there we go we have that
1208:53 - you could see for yourself training data set training data set this is gonna be
1208:58 - unpatched so you have the unbatch data set notice that the batch dimension is taken
1209:03 - off could simply have this for the validation there we go validation you
1209:08 - run that see that taken off too and then we get back to this documentation recall
1209:16 - that before creating the tensorflow records we need to put our data in the
1209:21 - sediment format more specifically we need to create proto files and the way
1209:26 - we do this is by making use of tensorflow example and then to create this
1209:32 - tensorflow example we have this features which we need to combine to create this
1209:38 - tensorflow examples and in this documentation here we have all that is
1209:43 - needed to create this so here you see for example this example here we could
1209:47 - copy this out simply and then we paste this right here so you could see the
1209:53 - int the floats the bytes but since we are not gonna use the floats here we
1209:59 - could take this off we're gonna use this float feature what we're interested is
1210:02 - the byte feature because recall we have the image and we have the level so we
1210:10 - have the image of the person happy and then we have the level one so this level
1210:18 - will be this int feature and the image will be the bytes feature so let's go
1210:25 - ahead and get this done we have here let's put this first so here we have our
1210:32 - bytes and then we have our int now we're gonna create an example from here you
1210:38 - can see clearly we've imported this already so we just have that even here
1210:42 - could have this features there we go here we have bytes list see here we have
1210:49 - feature here we have feature here we have int 64 list feature of type in 64
1210:57 - list and that should be it so we have all this here we need to take this off
1211:03 - we don't need that so here we have we change this name we put we call this
1211:09 - image we'll call this image our images and then we'll call this levels so that
1211:17 - is it okay so we have this year back that's fine we have that the bytes does
1211:26 - images and we have the levels now once we have this the next thing to do is to
1211:32 - put in the correct values in here so instead of having this year we take this
1211:38 - off and then here we're gonna create a method which will call create example
1211:46 - call this method create example is going to take our image and also the level so
1211:52 - it takes in the image and the level and then what it returns is our serialized
1211:58 - example so we've had this example which have created and this are serialized
1212:04 - example so we pass we call the serialized to string method right here
1212:09 - now that's set instead of this year we'll take in our image and then instead
1212:15 - of this year we'll take in our level so that's that should be fine we now run
1212:21 - this and the next thing we'll do is define the number of charts here we have
1212:27 - 10 charts and then the path so you call this tensor flow records let's create
1212:34 - this new folder here TF records that's okay TF records that's fine and then we
1212:42 - will have the charts with your specific number so here we have the chart or
1212:49 - basically we have this name let's have your file name and then here's the
1212:55 - extension TF record okay so that's it and then we can run this and then the
1213:03 - next thing we want to do is to get back to documentation TF.io and then we get
1213:10 - this TF record writer where we are going to see how to write in a TF record file
1213:16 - so we have this year here's a simple definition arguments to specify the path
1213:25 - and you also use this example so we have here write the records to a file so let's
1213:32 - copy this simply and then we paste this year so we have with this TF record
1213:40 - writer will specify the path here our path is gonna be this path we have that
1213:47 - path and then as file writer we're gonna write our information in that now we
1213:53 - want our file to get those different names so here we have path.format and
1213:59 - then we'll specify a given chart so a given part of our data so let's get back
1214:08 - here recall that when we create our tensorflow record we could create this
1214:12 - as a block like this and then we'll later on chart this break it up into
1214:18 - different parts so here exactly we have 10 charts so break this up into this 10
1214:24 - different paths 1 2 3 4 5 6 7 8 9 10 okay so there we go we have this 10
1214:34 - different charts and then we want each chart to have a different file name and
1214:40 - that's why if you look at this year you see we have this formatting such that we
1214:47 - pass in a given chart number in here so we have the chart number so that's it
1214:54 - and what we'll do now is for each chart as out of this 10 charts here for a
1215:02 - chart number in range number of charts that's more of
1215:11 - shots will specify here on more shots 10 so it's basically for short number in 10
1215:16 - so we are gonna go through we're gonna loop through this and we are gonna
1215:20 - create a file for each chart so breaking up here so for each and every one of
1215:26 - this we're gonna create a file this one a file this a file and so on and so forth
1215:31 - so let's take this off there we go now getting back here once we have had this
1215:38 - in place or once we've set this up now recall that we had created this create
1215:44 - example matter right here and if you notice you find that in this year we
1215:50 - actually all this example it's kind of like doing exact same thing with this
1215:57 - create example where we have those different features which are created and
1216:00 - then at the end we have this serialized to string meta which is called so you
1216:04 - could see here those different features see those features and then they all
1216:09 - combine to form the example and then serialization so this means that all we
1216:15 - need to do here is pass in we have here instead of this we have create example
1216:21 - and then we'll be passing in the image and the level now we'll take this off
1216:25 - yeah we don't need this physically we don't need that and then we will go for
1216:31 - image and level in our data set in our tensorflow data set let's get back up
1216:38 - the what we're doing with now is our training data set so for this in our
1216:43 - training data set that's it we're gonna write this so we're gonna have to write
1216:51 - this image and this level in our file will be created so in this our tensorflow
1216:58 - record file now given that we are we have to write a given shard and not just
1217:05 - a full data set here would change this to shard sharded data set and then let's
1217:13 - have this year we have our sharded data set which is going to be equal you have
1217:20 - that our training data set and then we'll shard so you could get to the
1217:26 - definition or you could get to the tensorflow data let's scroll this
1217:31 - tensorflow data data set here you could have this or you could find the shard
1217:38 - method right here let's scroll down and click here see we have let's click on
1217:44 - shard shard okay so you see here we have the definition takes the number of
1217:50 - shards and then it takes the specific index so here we have 10 shards and then
1217:55 - for each index we're going to pass in the value here dynamically so here you
1217:59 - see that this creates different packs or parts of our data set so here we have
1218:06 - each pack which is going to be created so we specify the number of shards there
1218:11 - we go and then we specify the shard number so that's all we need to create a
1218:16 - part of our data set so that's it once we have this let's now run this and see
1218:23 - what we get get in this arrow for sharded number run that again what do we
1218:31 - get this but expected one of time bytes so what we pass in years tensor but we
1218:38 - expected a byte now the next question is how do we convert this tensors to bytes
1218:44 - so we do a quick go search here convert image to bytes in tensorflow there we go
1218:50 - we have decode image but in fact what we'll be using to convert this image into
1219:00 - bytes is this encode gpeg right here so we haven't gpeg images so we could use
1219:07 - this here using it is quite simple we just simply pass in the image and then
1219:13 - we'll consider all this to be default values so here we have encode gpeg and
1219:21 - then we're gonna create this encoder method here encode image takes in the
1219:28 - image and the level and then what we have here is that we're gonna start by
1219:35 - having getting the image we'll say image is gonna be the encoded version so we
1219:39 - have tf.io encode gpeg that's it we pass in the image and then we're gonna
1219:48 - return the image and the level so once we have this let's add this code so let's
1219:53 - run this and then we're gonna create a new encoded data set encoded data set
1220:00 - equal this we have training data set there we go we're gonna map encode image
1220:10 - that's it so that each and every time we want to pass this into this file
1220:18 - writer we're gonna make sure that we have this in the form of bytes so that
1220:24 - matches up with this here with this feature which is defined in our create
1220:33 - example method right here so we need to match this up so that said let's get back
1220:40 - we have encode encoded data set take this off we run we have encoded data set
1220:48 - we told that this image has type float 32 that doesn't match expected type of
1220:53 - unsigned int 8 so what we need to do here is we need to convert our image
1220:59 - first into this unsigned it so we have here now to get a solution we'll get
1221:06 - into tf image convert image dtype so that we could convert let's actually
1221:12 - convert let's get to see convert image dtype so as we're saying we could
1221:18 - actually convert our float 32 to the unsigned int so here all we need to do as
1221:24 - you could see here you see you you have the tensor and then you have the dtype
1221:28 - specified so that said let's copy this and we have that anyway we are we're
1221:36 - going to take the default value for the saturation so let's get back here and
1221:40 - then at a level of this or before the encoding we're going to convert that so
1221:46 - here we have image image equal that and then we pass in the image here so we
1221:54 - have tf dot unsigned int 8 int 8 and close that so that's it
1222:03 - then set we can now let's run this and run this again so we could see our
1222:09 - encoded data set see that works fine now so we have our encoded data set and then
1222:14 - in here instead of having this we will call this encoded data set so we have
1222:21 - encoded data set and then we'll run this and see what we get value must be
1222:28 - iterable so we must convert this into an iterable data set so to do this we just
1222:34 - have your as non-py iterator we have as non-py iterator we run that again we are
1222:43 - getting here 255 has type int but expected one of bytes this error is
1222:50 - coming from our create example method so let's get back to create example
1222:54 - everything looks fine but here we should have a list so let's have this year there
1223:01 - we go and run this again and that's it the creation of the different files now
1223:10 - complete you could click open year and you see we have all this 10 different
1223:14 - charts at this point what we could do is we could save this files in the drive
1223:20 - such that we could use it next time for training so let's go ahead and see how
1223:25 - we could make use of this for our training process now what we want to do
1223:31 - here is to convert this back to a tensorflow data set so we had our tensorflow
1223:36 - data set we converted it into a tensorflow record like we see here
1223:41 - we converted this into some different tensorflow record files and now want to
1223:46 - reconvert this into our tensorflow data set now to get this reconstructed data
1223:52 - set we are going to pass the different file names in this TF record data set
1223:57 - right here so what we have here is this list made of all these different file
1224:01 - names right here so this list in essence let's call it L will be our path which
1224:08 - will format and then we'll pass in this variable P for P in range the number of
1224:16 - charts for P in range number of charts let's print out this L run that and we
1224:23 - get this list which is made of all the different files here so that said what
1224:28 - we're gonna have here is we're gonna copy this we're gonna copy this here and
1224:34 - then simply replace this list with it so we have this list now let's have this we
1224:41 - have this list now and then we run this and get our reconstructed data set but
1224:46 - then we need to parse this TF record data set such that we could get our
1224:52 - original data which was in the form of the image and the level where the image
1224:57 - was an array and the level was some integer and so with that we have here
1225:03 - parse single example method which we're gonna make use of which takes an example
1225:08 - which is basically what is contained in our reconstructed data set takes the
1225:12 - example and permits us to split this into the image and the level so right
1225:19 - here we'll have example and then we'll get the image we have the image then
1225:25 - we'll make use of the decode gpeg method so previously we encoded the into gpeg
1225:30 - or we convert it into bytes now we're gonna convert from bytes back to the
1225:35 - unsigned integer so here we have decode gpeg and then we're gonna pass in the
1225:43 - example image so we have that image we specify number of channels to be three
1225:50 - now we have all the set what we're left to do is specify this feature
1225:55 - description right here nonetheless this feature description is basically what we
1226:00 - had in this create example and so here we have this dictionary let's have this
1226:07 - images levels your images we have this dictionary which is made of the images
1226:14 - and the levels and then we have the data types of the images and the levels
1226:19 - respectively so we need to pass in this feature description in this parts single
1226:25 - example method now we have that always set to return our output so we take
1226:31 - example and we have images and then example and we have levels so what we're
1226:39 - doing here is we take an input example and then we're breaking it up into the
1226:43 - images and to the levels while converting these images from the bytes
1226:50 - back to the unsigned int so we have that now let's run the cell and then here we
1226:57 - have our parsed our parsed data set there we go we have parsed data set and
1227:04 - we have recounts data set there we go we map this method here so we have parsed
1227:13 - TF records parse TF records so we have this method let's run this that's fine
1227:21 - and then let's see what is contained in our parsed data set so let's take a
1227:27 - single value and then print this out from that we get in this arrow for I in
1227:35 - this let's run that and there we go as you can see we have our input and then
1227:42 - our output level right here now the next thing we could do is specify the batch
1227:48 - size so let's have this batch size here we have our batch configuration and then
1227:57 - batch size that's fine and then we could do some prefetching so let's carry out
1228:03 - the prefetching let's just do auto tune auto tune and then run this again there
1228:13 - we go now we should have 32 elements let's look at our batch our parsed data
1228:19 - set looks fine you see we have this four different dimensions and then we have
1228:26 - output here so that's it we now have our parsed data set are we ready to train so
1228:34 - as we've seen already it's important for you to save this in some location say
1228:42 - for example in the drive such that the next time when you want to do training
1228:45 - all you need to do is start from here so all you need to do is to reconstruct
1228:51 - this so you just come and reconstruct this data set you will parse it and then
1228:57 - you're good to go with this parsed data set right here so you don't longer have
1229:01 - to load all these images from memory and all of that now before we move on you
1229:07 - should also note that while encoding the image or while encoding the data that
1229:12 - the image and the level what we could also do is take the argmax of the level
1229:17 - so if you have a level so let's suppose we have an input image like this and
1229:22 - then you have an output level say 0 1 0 instead of taking the considering the
1229:30 - levels to be this we could take the position with the highest value so this
1229:36 - is 0 1 2 so it tends to 1 now if we have 0 0 1 then after getting the argmax in
1229:47 - this case it's going to be 2 because here we have 0 position or first position
1229:52 - second position and this is the one with the highest value so this is another way
1229:57 - of encoding our levels and that's what we are going to do so we will take this
1230:00 - and we run again and create our data set or create our tensorflow records and we
1230:07 - are still going to run this cells again so let's rerun this and we have our
1230:13 - parse data which you could see here see now it's different from what we had
1230:17 - before so now we have the image we have the images and then we have the levels
1230:23 - then we could go ahead and run the model we have the loss function defined now
1230:29 - the loss function is a sparse categorical cross entropy and this is
1230:33 - simply because instead of the one hot notation that is instead of representing
1230:40 - our outputs like this for example we convert them into this single integer so
1230:46 - like this one for example would be 2 if we had 1 0 0 then this will be 0 if we
1230:54 - have 0 1 0 then this will be 1 so we've seen already that when we have this
1231:01 - this kinds of outputs then we use this parse categorical cross entropy and so
1231:07 - that's it we have the sparse accuracy categorical accuracy and now let's go
1231:12 - ahead and start with the training so there we go you can see that training
1231:17 - has begun and we training as we usually do so that's it for this section on
1231:23 - tensorflow records see you in the next section
1231:28 - hello everyone and welcome to this new and exciting session in which we are
1231:34 - going to look at other state-of-the-art convolutional neural network based
1231:39 - model and talking about state-of-the-art models ten years ago in the image net
1231:46 - visual recognition challenged the Alex net convolutional neural network beat
1231:52 - all state-of-the-art solutions previously or before this Alex net solution
1231:58 - state-of-the-art methods achieved a top error rate of our top 5% error rate of
1232:03 - 30 25.3% but with Alex net we drop this error rate to 15.3% now this is 25.2
1232:16 - this breakthrough has led to a widespread adoption of convolutional
1232:22 - neural networks in solving recognition tasks like this one and although today
1232:27 - we would hardly use this kind of model that's to say the Alex net model we are
1232:32 - gonna discuss this model because it was a precursor to most of the modern
1232:38 - confidence we have today like the mobile nets dense nets and efficient nets that
1232:44 - said we are gonna see what makes or what made the Alex net model so powerful the
1232:50 - Alex net model was first published in this paper entitled image net
1232:54 - classification with deep convolutional neural networks just from the title you
1233:00 - could get some idea that this was one of the first times conv nets were used for
1233:06 - this image net challenge this paper was by Alex Krasinski, Ilya Sutsicker and
1233:12 - Jeffrey Hinson in the abstract the start by presenting the results as you could
1233:18 - see here on test data they achieved top one and top five error rates of 37.5%
1233:23 - and 17% which is considerably better than the previous state-of-the-art now
1233:29 - this model was a 16 million parameter model composed of conf layers and max
1233:36 - pulling layers with some final three fully connected layers as we shall see
1233:42 - in the model section shortly now here 1000 ways softmax because we
1233:49 - have 1000 classes they also try to reduce overfitting by implementing
1233:54 - strategies like dropout and data augmentation that said here to discuss
1233:59 - the data set which was used obviously the image net which is a 15 million
1234:04 - labeled high-resolution image data set although the finally used roughly 1.2
1234:12 - million training images 50,000 validation and 150,000 for testing then
1234:19 - it's also important to note that the images which were used for training were
1234:23 - down sampled to 256 by 256 images as for the overall architecture as we could see
1234:30 - here we have the conf layers followed by max pulling layers sometimes you see your
1234:35 - conf layer max pulling conf layer max pulling then we have several conf layers
1234:41 - this max pulling layer and then we have three dense layers you see here we have
1234:47 - this dense layer this dense layer and this final dense layer with 1000 way
1234:52 - output then another point to note here is given that at a time many times the
1234:58 - non-linearity used was the tangent or the sigmoid let's get back to this top
1235:04 - year you see here they talk about the non-linearity which was used just here
1235:10 - the previously were the mostly use a tangent or the sigmoid as the same year
1235:16 - but it turns out that after working with a relu the relu if you can recall we've
1235:22 - seen this in the previous section the relu is simply this function which takes
1235:26 - in a value x and then if x is negative the value is 0 if x is positive remains
1235:31 - the same value so basically we have x f of x f of x here which is our relu
1235:40 - function which is 0 if x is less than 0 and it is x if x is greater than or
1235:48 - equal to 0 so this is our relu function right here and what the discovered was
1235:54 - that the relu permitted them to train your model much faster than this
1236:01 - previously used non-linearities like the tangent of x so as you could see here
1236:07 - just after a few epochs just after let's say five epochs they attained this
1236:13 - training error rate as compared to this other non-linearity here so this is what
1236:20 - we get when we use the relu and this is what we get when we use some other
1236:27 - non-linearity like the tangent x and it's important to note that till date most
1236:33 - conf nets we build make use of this relu non-linearity another thing the
1236:41 - deed to speed up the training was to use multiple GPUs then the actual number of
1236:47 - GPUs they use here is two and the device a method of communication between these
1236:54 - two GPUs to speed up calculation from here the authors make use of this
1236:59 - normalization strategy for regularization known as a local response
1237:04 - normalization and this normalization strategy was used alongside the relu
1237:10 - non-linearity so from here we have some input let's take this schematic from
1237:18 - this post by Akhil Anwar where he shows this even more clearly here we have some
1237:26 - input and then we normalize it based on its surroundings hence the term local
1237:36 - response normalization here he explains that there is inter channel local
1237:42 - response normalization and there's intra channel local response normalization
1237:47 - here as you can see this is between pixels of a given channel or neurons of
1237:54 - a given channel and here this is inter channel so this is carry out between
1238:00 - pixels of different channels now that said the exact mathematical formula use
1238:06 - here is this one so here we have given neuron and then we divide its value by
1238:15 - this summation right here which is a square of some neighboring values and
1238:21 - according to the authors terms here you see this sort of response normalization
1238:28 - implements a form of lateral inhibition so take note of this and is inspired by
1238:34 - the type found in real neurons create and competition for big activities
1238:39 - amongst neuron outputs computed using different kernels so this means that if
1238:47 - we consider this three neighboring channels from or rather the three
1238:51 - neighboring neurons from these three different channels if we take this
1238:56 - particular neuron right here and we try to normalize it or pass it through a
1239:02 - normalization layer given that it is surrounded by this pixel whose value is
1239:08 - relatively high because of this square term right here and this is a square right
1239:15 - here meaning that you take this value say 1 divided some by some summation and
1239:23 - then you have this alpha obviously you have this K right here let's omit that
1239:27 - let's just put it right here we have this alpha and then we have this value
1239:33 - squared so obviously this would be a function or basically this will be this
1239:37 - value here so when you square this value it means that this overall value here
1239:42 - will become very small hence the term lateral inhibition and so for a neuron
1239:48 - to maintain a relatively high value after going through this local response
1239:54 - normalization layer right here it has to ensure that it has one of the highest
1240:00 - values among the surrounding neurons nonetheless this local response
1240:07 - normalization as compared to other normalization techniques like the batch
1240:12 - normalization layer normalization and the group normalization hasn't proven
1240:18 - to be very effective when it comes to regularizing a neural network hence not
1240:22 - used by modern conf nets we had seen in the previous sessions that the pulling
1240:28 - layers permeate us down sample information from the inputs such that as
1240:34 - we go deeper in the neural network we have a reduced number of features now on
1240:40 - this paper they make use of this pulling layer more specifically the max pull
1240:45 - layer and the way it works is quite straightforward so we're supposing we
1240:50 - have this kind of input and then we have a 3 by 3 max pull layer with initially
1240:57 - a stride of 1 what we're gonna have here is we have this positions which we are
1241:04 - gonna fix here let's get back and then fix some values so let's say we have a
1241:09 - value of 1 2 and then all this other values if we want to carry out the max
1241:17 - pull operation with a stride of 1 we'll start with this year see and because
1241:23 - this max pull we have we're gonna pick the max of all this so we have the
1241:28 - highest value yours 11 and so here we're gonna have 11 and then the next thing
1241:34 - we'll do is we are gonna shift this so we shift this here this is a stride of
1241:40 - one we're gonna go one step to the right and so we have this now it's here
1241:46 - notice how we still pick out 3 by 3 pixels let's not take this one off so
1241:53 - we've done this shift and now we have this position we take the max here the
1241:57 - max here again is going to give us 11 so we have 11 year and then we're gonna do
1242:03 - another shift so from here we're gonna take this year let's take this off this
1242:09 - other shift and then we still have this the max here again is gonna be 11 so you
1242:14 - see at this top we have 11 11 11 and then from here we'll move on to this
1242:21 - next one so we will go downward one step downward we'll have this here and the
1242:31 - max here is gonna be here's 11 still so we're gonna have 11 year and then we'll
1242:37 - move this way this way we'll go downward and all of that so we will move this way
1242:43 - I think we should have 11 still the way 11 we go downward still have 11
1242:49 - practically we will have 11 everywhere so we'll have 11 and your 11 so this is
1242:56 - going to be our output from this input here after the max pull operation now
1243:01 - when we modify this stride number from one to two take this stride number from
1243:08 - one to two as it was illustrated in the paper instead of having or instead of
1243:13 - moving through one step we move through two steps so if we take this off here
1243:19 - you would see that we'll start with this so from the first one we're gonna get
1243:24 - 11 so this was tried equal to so the first one we get 11 and then from here
1243:30 - instead of moving just one step like previously we had this year we have this
1243:34 - year and then we move one step now we're gonna move two steps so we move this way
1243:39 - and then again we have 11 and then instead of going one step downward we're
1243:45 - gonna go two steps downward and so we'll end up here and then we have a maximum
1243:52 - of 11 and then we'll go two steps again this way so let's take this one off take
1244:00 - this one off we go two steps again and then we get a maximum of 11 right here
1244:07 - so as you could see here when you talk of overlap and pulling they actually
1244:14 - use a stride of two just as we have described and then the founders to give
1244:19 - them to give an improvement in the results though this improvements aren't
1244:24 - very much so in practice we generally use the classical max pulling with S
1244:32 - equal one does stride number equal one and we also use two by two kennel size
1244:40 - so instead of using three by three kennel sizes as you see here most times
1244:45 - or in modern conf nets generally use two by two pulling size now getting back to
1244:51 - the general architecture we could see here that this very first conf net has a
1244:57 - kennel size of 11 by 11 and although these kinds of kennels permit the
1245:07 - network capture much larger spatial context we'll see that they are
1245:15 - computationally much more expensive compared to this kennels with smaller
1245:21 - filter size and as we'll see in subsequent sections the confidence
1245:27 - developed after this didn't use these kinds of large kennel sizes as they were
1245:32 - able to make use of these kinds of smaller filters to still capture this
1245:37 - large spatial context the 11 by 11 filters capture then to overcome
1245:44 - overfitting the others make use of data augmentation and the dropout technique
1245:51 - so you could check out on our previous sessions where we talked about this two
1245:54 - different techniques now that said you would see here the training details and
1246:00 - then one very interesting advantage of working with the 11 by 11 kennel size
1246:07 - filters is the fact that we could have visualizations like this so because
1246:12 - those kennel sizes are large enough we could visualize them in this manner and
1246:18 - then clearly from here we see how our conf layer captures these kinds of low
1246:24 - level features like here we have a slanted line here we have yeah many slanted
1246:30 - lines we have this vertical line we have this horizontal lines right here and
1246:39 - then we have this this checkerboard pattern we have this colors sometimes
1246:44 - dwell sometimes single color and so we'll see how this first count layers
1246:50 - permit us capture low level features in this session they discuss this record
1246:57 - breaking results as you could see we have this top one error rate which for
1247:04 - now or at that time was about 45.7 percent and then with this curve net
1247:11 - model this was dropped to 37.5 percent then for the top five we have dropped
1247:18 - from 25.7 to 17 percent now they also developed this other variant which comes
1247:26 - with an even better top five error rate of 15.3 percent and so as you could see
1247:35 - here we move from this previous method does a shift plus FVs which had 26.2 top
1247:42 - five percent error rate to the CNN which has 15.3 percent error rate then in the
1247:49 - session on qualitative results we see the different inputs the correct levels
1247:56 - and then what the model predicts on the top five best predictions see the model
1248:01 - does well here does well here correct correct prediction yours wrong see it
1248:09 - predicts convertible when it's actually a grill here it does this wrongly here
1248:16 - it's also wrong but unlike here this level doesn't even occur among the top
1248:25 - five best predictions and so that's it for this breakthrough model we're gonna
1248:29 - look at other confidence models in the next sections
1248:34 - hello everyone and welcome to this new and exciting session in which we are
1248:42 - going to discuss the VGG model VGG actually stands for visual geometry
1248:47 - group and this was presented in the paper by Karen Simone and Andrew
1248:53 - Zimmerman entitled very deep convolutional networks for large-scale
1248:58 - image recognition in the session we are going to discuss different methods which
1249:03 - the authors of the VGG paper used to drop the top one validation error rate
1249:12 - from 38.1 to 23.7 where this 38.1 was achieved by the breakthrough
1249:22 - conf net model which is the Alex net model now in the previous session in
1249:28 - which we treated the Alex net model and we saw the power of working with conf
1249:34 - nets and solving recognition tasks one thing we could notice already or very
1249:39 - clearly from this model is that it's quite shallow and so Simone and
1249:44 - Andrew Zimmerman go even deeper with the VGG model in this paper the authors
1249:51 - investigate the effect of the conf net of the convolutional network depth on
1249:57 - its accuracy notice words depth and accuracy so unlike the Alex net where
1250:04 - the depth is relatively small and is actually a shallow network here the
1250:11 - authors use a deeper conv convolutional neural network and make use of smaller
1250:20 - convolution filters now recall with Alex net from the very first layer we
1250:26 - already had 11 by 11 filters and we argue that this helped in capturing large
1250:35 - spatial dependencies now we'll explain how is possible for us to make use of
1250:40 - the smaller and more economical convolutional filters while still
1250:46 - capturing large spatial dependencies like the bigger five by five and eleven
1250:53 - by eleven filters will do to better understand why it's better to work with
1250:59 - two three by three comp layers as compared to working with a single five
1251:04 - by five comp layer let's consider the following examples so here we have this
1251:09 - three examples let's start with this first one here so let's have this we
1251:15 - start with this part here and this one if you notice is five by five so here
1251:21 - you have kennel size of five input size of ten and no padding dilation of one
1251:29 - and stride of one so here we have this year and you can see we have this output
1251:34 - which is six by six so you have the six by six output and the way each and every
1251:40 - pixel on the output is garden is quite simple you have the kennel right here
1251:45 - let's have this kennel we have this kennel which has been passed on a
1251:50 - particular patch on the input and then this produces the output so simply take
1251:58 - this kennel values multiplied by these values add them up and then get this
1252:03 - output now this is the case of five by five so the receptive field Spaniard is
1252:11 - quite great see we could get we could capture this information in this patch
1252:18 - in the input right here now for the number of parameters you could simply
1252:23 - count this we have five times five which is 25 parameters here we have 25
1252:29 - parameters and then for the learning capabilities this one is quite limited
1252:36 - here here we have 25 and here we'll say this is quite limited because if we
1252:44 - suppose that we have an input let's say we have this input which is passed
1252:48 - through a single conf layer and then obviously the conf layer ends with a
1252:54 - non-linearity in our case the relu non-linearity so let's say we have the
1252:59 - non-linearity line right here and then we get the output now this doesn't
1253:05 - capture as much complex information as we would be able to capture if we had
1253:12 - two conf layers stacked now here when you stack this first one this is a case
1253:18 - of three by three and this is a case of five by five so here after this first
1253:21 - three by three we then have this other three by three with this other
1253:26 - non-linearity so here we are able to capture much more complex information
1253:32 - from our input information of my input data as compared to when we just have
1253:37 - one single conf layer so that's why here let's take this off here we the learning
1253:44 - capabilities isn't as much as this two three by three now let's get to the
1253:50 - receptive field span for the two three by three to understand this we are going
1253:55 - to take this example right here so take note that here we have input size of 10
1253:59 - so we're gonna maintain this we have input size 10 okay now the candle size
1254:05 - is three unlike here where we have five padding dilation and stride number the
1254:10 - same now notice that the output we have here is eight by eight unlike here where
1254:15 - six by six now since we are having two three by three conf layers we're gonna
1254:20 - get this output just like the output we get here is this so let's draw let's put
1254:26 - this here we have this input the here is it and then we get this output which is
1254:32 - gonna be input of another three by three so this is the first three by three and
1254:36 - then this is the next three by three so this will be the input of another three
1254:42 - by three comp layer and then we should produce the output so what we're trying
1254:46 - to show here is when we have this output here which is the input of this next
1254:50 - layer so this is this two here are combined as one this forms one so this
1254:58 - year this eight by eight now is an input so here instead of having ten we have
1255:03 - eight so we take this input size of eight there we go same candle size
1255:07 - patterns dilation stride the same and what we'll want you to notice let's
1255:13 - take this off what one you notice here is the fact that this output is six by
1255:18 - six and so this means that this year as if we follow this let's take the mouse
1255:24 - if we follow you see this year captures the same information as you would as
1255:30 - this one year would capture and so we could confidently see that the recessive
1255:36 - field spannier is quite great now for the number of parameters here we have
1255:42 - nine and here we have nine nine plus nine is 18 so 18 you see clearly that
1255:51 - this model now is cheaper as compared to a model which uses a five by five or
1255:57 - even eleven by eleven now for the learning capabilities we've seen this
1256:02 - already because we stack two we able to capture much more complex information
1256:06 - from the inputs and so this is great so in all we see that is better to use
1256:13 - this conf layers with smaller kennel sizes we want to thank Edward young for
1256:22 - providing this convolution visualizer which you can find on as young that
1256:26 - github.io so at this point we've understood why the authors of the paper
1256:32 - prefer to work with this smaller kennel size convolutional layers that's three
1256:38 - by three and with a smaller conf layers they were able to push the depth to 16
1256:46 - that's between 16 and 19 layers where the 16 layer version is VGG 16 and then
1256:56 - the 19 layer version is VGG 19 in this table we have the summary of this
1257:04 - models focus on the 16 and 19 weights layer models so yet we have 16
1257:11 - weight layers you see we start with two conf layers and then max pull and then
1257:16 - to conf layers and the max pull and in three couple layers max pull three
1257:20 - comf layers max pull and then three conf layers then from Europe we have the max
1257:25 - pull we have a flattened layer and then we have this three fully connected
1257:30 - layers which end up with a softmax since we are dealing with a multi-class classification
1257:36 - problem.
1257:38 - The authors also noted that the usage of the RNN normalization as a local response normalization
1257:45 - which we saw in the AlexNet did not improve performance but instead led to increased memory
1257:51 - consumption and computation time.
1257:53 - In this section to describe the training process then from your testing and we could get some
1258:01 - results.
1258:02 - Here we see we have this top one validation error and we also notice that we have this
1258:11 - ConvNet layer or this ConvNet model here A. Now the ABCDE you could get them by checking
1258:17 - on this table here you have A with the RNN normalization B, C, D and E right here.
1258:26 - So basically these are the different models and your other results.
1258:32 - So we are working with the local response normalization we notice that this seems to
1258:38 - have even the highest error.
1258:41 - So that's why the authors did not make use of this normalization technique.
1258:46 - Now we get the best results with the VGG19 so this year VGG19 we get 25.5 for top one
1258:56 - and then 8 for top five.
1258:58 - Now if you are new to this notion of top one valid error and top five valid error you could
1259:03 - check out the previous section where we discussed the top one accuracy and top five accuracy.
1259:09 - Here again now we have this comparison with the state of the art solutions at the time
1259:15 - you see here AlexNet, Overfit, Inception, MSRA, Clarify, this model by Clarify AI and
1259:25 - the Zeller and Fegris model.
1259:29 - So here we see that the VGG at the time had the best results and with this we can conclude
1259:36 - that stacking up those Conv layers with smaller kennel size actually helps in getting better
1259:44 - results and in subsequent sections we'll see the limit of just stacking up many Conv layers
1259:49 - as with the VGG.
1259:55 - Hello everyone and welcome to this new and exciting session in which we are going to
1260:00 - discuss the ResNet model.
1260:03 - This model was first introduced in this paper entitled Deep Residual Learning for Image
1260:08 - Recognition by Kaimin Hayao and up to date that's about seven years later this model
1260:15 - is still greatly used and the high performances gotten when working with the ResNet model
1260:22 - come due to the fact that the ResNet model relies on this residual block right here which
1260:28 - permits us get even better error rates as we could see here as compared to the VGG and
1260:36 - Google Net models.
1260:38 - In this section we are going to focus on understanding how this residual block works and how the
1260:45 - ResNet model is constructed based off this model.
1260:50 - This curve right here depicts the limit of models like the VGG which are just based on
1260:57 - stacking up Conv layers.
1261:00 - To best understand this plot, recall that with the AlexNet we had fewer number of layers
1261:06 - so we started out with AlexNet fewer number of layers and then we moved on to VGG where
1261:12 - we had the VGG 16 version then the VGG 19 version and then we expect that if we keep
1261:21 - increasing this number of layers then the error rates should be dropping but what happens
1261:27 - actually is the opposite.
1261:30 - So what goes on here is you see for this 20 layer network we have a lower error rate as
1261:37 - compared to this 56 layer network.
1261:41 - So we expect that this instead should be lower than this because this we've stacked more
1261:47 - Conv layers but that's instead the opposite.
1261:50 - Now this same phenomenon is witnessed with a test set.
1261:54 - So here there's a test and there's a train.
1261:57 - So for the test tool we have the 20 layer performing better than the 56 layer.
1262:04 - And so it's clear that just blindly stacking up Conv layers wouldn't help in making this
1262:12 - or in dropping the training on test errors even though they are more expensive.
1262:18 - And this is why the ResNet model introduces this residual learning which is based off
1262:25 - the residual block which we've seen already here is the residual block.
1262:29 - Now note that this weight layer here or this weight layers here are simply convolutional
1262:34 - layers.
1262:35 - And so now unlike before where we will just stack this here let's call this WL for weight
1262:42 - layer stack this weight layer and then we'll stack this other one and then we'll keep stacking
1262:48 - up that way just keep stacking up like this.
1262:52 - Now what we'll do is we create a connection between the input and the output.
1263:00 - So we create this connection and then here there's some addition.
1263:04 - So we get this output and we add it with this other output right here to produce now this
1263:10 - new output.
1263:12 - So here if we suppose that this input is x and then what goes on in here is f of x.
1263:18 - So let's change this to red what goes on in here is f of x that's this f of x here we
1263:27 - have this f of x then our output can be given as h of x which is simply equal f of x plus
1263:37 - x that's the input plus its output.
1263:42 - Now to produce this new output h of x.
1263:46 - But again to better understand why we need this residual block right here.
1263:54 - We need to first understand why models which are based on just stacking up the comp layers
1264:00 - like the VGG actually under fit even when you add up or when you increase the number
1264:06 - of layers.
1264:07 - Now the reason for that is exploding and varnishing gradients.
1264:14 - So let's explain what it means for gradients to be vanishing.
1264:20 - Now recall that in the gradient descent process we have a weight and this weight the way this
1264:27 - weight is updated is such that we take its previous value.
1264:31 - So we have the previous value of the weight minus the learning rates here let's call it
1264:37 - our LR really denoted alpha minus this learning rate times the partial derivative of the loss
1264:45 - respect to that given weight.
1264:49 - Now during the training process in order to compute this partial derivative right here
1264:54 - very efficiently the method used is back propagation or one of the most common methods used is that
1265:01 - of back propagation.
1265:03 - Now the way back propagation works is that you have say this model let's call this model
1265:08 - M and then you have some input right here with the output obviously you have what the
1265:13 - model outputs and let's let's call the model output y cap and then y that's not from model
1265:22 - we have y right here which is what the model is expected to output.
1265:27 - Now is this difference that produces the loss and well finding that partial derivative of
1265:33 - this difference with respect to each and every weight which makes up this model.
1265:43 - Then if we split this up into different layers let's just say we have different layers like
1265:47 - this note that each layer is composed of several weights but let's let's say we just split
1265:53 - this up like this now the layers this layer has its own weights but one point to note
1266:00 - is that during the back propagation process to obtain the partial derivative of the loss
1266:06 - we respect to this weight here we make use of the partial derivative of the loss with
1266:13 - respect to weights which come after the layer or which come after this layer right here.
1266:21 - So we to get this year we will need this different partial derivatives to get let's let's get
1266:29 - back let's get back to get this for example to get the partial derivative with respect
1266:36 - to this weights here we will need this others right here now the problem is if we have this
1266:45 - year and and also before before going to explain the problem with this is that we need to understand
1266:53 - that to get for example let's take this one to get for example the partial derivative
1266:59 - of the loss we respect to this different weights there are many weights in this layer let's
1267:05 - say weight for this layer is actually equal some values let's say alpha 1 times whatever
1267:12 - value times this partial derivative of the loss we respect to this weights here so let's
1267:23 - consider that this is the sit layer 1 2 3 4 5 6 then L6 here we have L7 layer 7 so because
1267:31 - we're multiplying here it means that if while getting this partial derivative we obtain
1267:39 - a value very close to 0 if we get a value very close to 0 say 0.00001 for example it
1267:49 - means that it's going to affect this other partial derivative in the sense that this
1267:54 - two will be a very small value and if this partial derivatives are too small then we
1268:01 - will not get a change in this weight because you have the the new way you're trying to
1268:07 - get being equal the previous weight minus a very small value here and so there will
1268:14 - be no there will be little or no changes in the weights and that is why even though you
1268:21 - keep increasing this number of layers let's take this off even though we keep increasing
1268:27 - this number of layers we cannot achieve better performances due to this vanishing gradient
1268:33 - problem as the model is now finally difficult to update its weights such that the training
1268:40 - error can be decreased since the gradients right here are vanishing that is getting towards
1268:48 - 0 so now we've just seen that making our network deeper or increasing the number of layers
1268:56 - makes it difficult to propagate information from one far end to the other end and so what
1269:04 - the authors suppose is that if the added layers can be constructed as identity mappings a
1269:13 - deeper model should have training error no greater than its shallower counterpart so
1269:20 - this means that you have this model let's suppose this model and then we this is the
1269:25 - shallower model and then this is the deeper model right here and we're saying that if
1269:33 - we construct this deeper model such that it's identical to the shallower model so basically
1269:39 - it's the same as this here the changes in blue so we have this same shallower model
1269:45 - and then the remaining layers here this other layers here are constructed such that this
1269:52 - is the identity function or a group of identity functions which are stacked together then
1270:00 - the training error of this one although deeper shouldn't be greater than this training error
1270:08 - right here or the training error of this shallower model and so this means that if we want to
1270:15 - pass information from this point to this point right here and that this weights or the values
1270:24 - in your dampen this information then there is this path right here which permits us simply
1270:35 - copy this input information to the output and obviously this is simply the identity
1270:43 - function and so here if after passing through say 20 layers and we get to this point so
1270:50 - let's consider each and every one of this is a single layer so we've gone through 20
1270:54 - layers and then we've gotten to this point where the values we get here are almost zero
1270:59 - such that when this information passes is going to be also or practically zero then
1271:07 - there is this path which at least restores this exact same input we have here and so
1271:15 - this means that just as the authors of the papers supposed in this example we took here
1271:23 - if we make our model or neural network deeper by adding this residual blocks then there
1271:32 - will be no increase in the error rate and in practice this instead leads to a decrease
1271:40 - in the error rate which is exactly what we want and one other argument which accounts
1271:47 - for the fact that this residual blocks help in improving the performance of the model
1271:52 - is the fact that since we have several paths this residual model now looks like a combination
1272:01 - of several shallow model so it looks like you're combining different let's let's draw
1272:06 - it better let's get back it looks like we're combining this shallow model here with this
1272:13 - other shallow model with this other shallow model and then producing what we call an ensemble
1272:23 - of models or an ensemble of shallow models to be more precise which help in making the
1272:31 - overall model much more performant as compared to when we just have a single path.
1272:38 - So another way you could look at this is that for this let's take this first shallow model
1272:43 - you could have information which passes this way and then gets here and then goes this
1272:48 - way so that's the first model and then another time you could have information which goes
1272:54 - this way goes this way goes this way and you have this other model you could have an illustration
1273:02 - where the model goes this way it goes this way and then goes just straight and giving
1273:10 - us this other model later in this paper entitled visualizing the lost landscape of neural nets
1273:19 - Lee et al produced this visualization here which shows a resnet without skip connections
1273:29 - and a resnet with skip connections and this shows how easy this or how easier it is for
1273:37 - the weights to find their way or to get the optimal weights which minimize the loss as
1273:45 - compared to when there are no skip connections here it should also be noted that this addition
1273:50 - we have here is an element wise addition and so we have to ensure that the dimensions of
1273:57 - this input should match the dimensions of the output we have here for this operation
1274:04 - to be valid now in the cases where on the case where this two aren't equal then we need
1274:10 - to do some modifications in this skip connection right here now when we look at this three
1274:16 - models compared we have this VGG 19 we have this 34 layer plane convolutional network
1274:26 - then we have this 34 layer residual network we find that we have this skip connections
1274:34 - that's our residual block so this is our residual block right here we stack this residual blocks
1274:40 - now instead of just stacking the conf layers as we do with the VGGs now we stack the residual
1274:46 - blocks and then sometimes you could see here sometimes this line is solid sometimes it
1274:51 - is dotted now when is dotted like this it's simply because there's going to be a change
1274:58 - in the dimension so if you notice here you find that every time we have this dotted lines
1275:03 - you'll find that there's a change here the number of channels so here you have 64 channels
1275:08 - and then here you have 128 and so since we're getting this 64 channel input and we want
1275:17 - to match this with 128 output we're getting here then we need to do some adjustments here
1275:25 - now as we've seen here or as we can see in the paper here there are actually two ways
1275:31 - of making these adjustments the first way is this A the next one is this B for the A
1275:37 - the shortcuts still perform identity mapping with extra zero entries padded for increasing
1275:44 - the dimensions so to get from 64 to 128 we add this extra zero entries then either we
1275:52 - do that or we take the B that's the projection shortcut which was presented in equation two
1276:01 - is used to match the dimensions but this is actually done using one by one convolutions
1276:08 - now to better understand how the one by one convolutions work let's take this example
1276:13 - where we have this input size 10 by 10 cannot size now since it's one by one then the cannot
1276:18 - size is one so this is we have just this one weight right here and then you see just goes
1276:25 - through each and every pixel value so here we have this and then notice that the input
1276:34 - is the same shape as the output and so if for example you have this input made of two
1276:42 - channels let's add a second channel let's suppose this input has two channels so we
1276:47 - have this one channel and this other channel right here then to obtain an output of say
1276:56 - four channels so we have this input two channels so it's ten by ten by two two channels and
1277:03 - if we want to have this output to get to four channels all we need to do now is just make
1277:11 - use of the one by one convolution and then we add the we have four of this different
1277:20 - weights or this fall for the different kernels since obviously one by one if we're three
1277:24 - by three then we'll have something like this would have three of the four of this now is
1277:32 - one by one we have is here four of this and then in that case this one will give this
1277:38 - output then this other one will give this other channel here this other one let's change
1277:47 - the color this other one will give another channel which we'll just add here then this
1277:54 - other one here stick that to be green will produce this other channel so that's how we
1278:04 - can move from these two channels to four channels and so in the case of the resnets where we want
1278:11 - the inputs we get some inputs let's say 64 channel input and we want that at the end
1278:20 - we want that this input what we have here should match with this output which is already 128
1278:27 - oh yeah 128 year then we need or we could add this one by one convolutions with a certain
1278:36 - number of filters such that we could get this desired number of channels here so here now we
1278:43 - could have 128 one by one filters and then this now will match up with this output right here
1278:52 - such that we could carry out the element wise addition and now the difference with the vgg and
1278:59 - other previous conf nets is that instead of making use of the max pool as you will have here you see
1279:06 - this pooling layer with pool size two what we do here is we use a three by three convolutional
1279:14 - layer but we use strights so we specify the stride number of two and this permits us to
1279:21 - down sample this feature maps right here again we could check this out here let's suppose we have
1279:28 - this three and if we're to do or if we want to down sample this what we could do is increase
1279:35 - number of strides let's take that to two and you see this is going to be down sampled and if we
1279:40 - take the padding to one you find that the output is half of what we have here then the authors also
1279:48 - make use of augmentation and then batch normalization where they apply this batch
1279:54 - normalization layer right after each convolution and before the relu activation now batch
1280:03 - normalization is this technique for accelerating deep neural network training by reducing internal
1280:10 - covariate shift to better understand batch normalization we'll start by explaining the notion
1280:18 - of covariate shift to better understand the notion of covariate shift let's suppose that
1280:24 - we're trying to build a model which classifies or which says whether an input image like this one
1280:32 - or say this one is not of a car so it's not a car or it is not a car now if you're building this kind
1280:45 - of system and then you start by or you create batches of these kinds of toy cars and you pass
1280:52 - through the model and model learns how to see this and know that it's a car and see some other image
1280:58 - and knows that that's not a car then later on when you take a car from this other distribution
1281:08 - and you pass into our system it becomes difficult for the weights of this model to adapt to this
1281:17 - change in distribution though the inputs are all cars and to visualize this let's consider this
1281:26 - plot right here what we're gonna have is uh someone like this so we'll have this uh year for car
1281:36 - and then this for not car and then we'll build this classifier or this model which distinguishes
1281:44 - a car and an image which is not a car by say this function for example now when you bring
1281:53 - some other distribution like say this distribution you would have uh something like this you see you
1282:00 - have something like this here it's other um distribution and not car let's say not cars
1282:09 - about this and then you need to have something like this to separate the cars from images which
1282:17 - are not cars and this then makes it difficult for us to have a function which separates uh
1282:25 - images which are cars from those which are not cars when those images come from this two different
1282:31 - distribution now this shift here is known as the covariate shift and so that's why most times
1282:40 - before passing the image into the model what we do is we normalize this input so let's suppose
1282:46 - we have an input x we generally carry out some normalization in order to account for this
1282:53 - covariate shift so now after normalization what we're gonna have is that all those images be from
1283:01 - this distribution or this other distribution right here will now have been normalized to reduce the
1283:10 - effect of the shift and so now we could have our single or could have this uh uh function which
1283:18 - separates the cars from the non cars and with much more ease now that said what if this kind of
1283:29 - covariate shift instead happens in the hidden layers that's those layers which make up the
1283:37 - model right here so let's suppose that we have some confidence like this stacked with the activation
1283:43 - functions and then we have this weights that's these parameters which make up or which are part
1283:51 - of the layer now coming from different distributions then in this case we have an internal
1284:01 - covariate shift and to remedy this situation we now make use of the batch normalization
1284:10 - and the algorithm for the batch normalization is described in the paper so here we have a mini batch
1284:15 - and we obtain its mean so here we try to obtain the average value of the different weights
1284:22 - then we also obtain the standard deviation which is sigma and the variance which is sigma square
1284:30 - so basically we obtain the mean and we obtain the variance and it's this that we make use now to
1284:36 - normalize our data so now you take every weight you subtract by the mean that's here which is
1284:43 - calculated here and then you divide by this standard deviation and then we add this small
1284:52 - epsilon to avoid having a very small number or zero at the denominator so that's it this is how
1285:00 - the normalization or the batch normalization process goes on and it should be noted that
1285:06 - there are other normalization techniques like the layer and group normalization which are kind of
1285:11 - similar to this but different in a sense that with a batch normalization this mean is calculated
1285:19 - over a given mini batch so like here this is the mean of values calculated in the mini batch and
1285:27 - the standard deviation from that mini batch now after getting this new value of x x shuffle
1285:35 - what we now do is we multiply it by gamma and add better now this gamma and better are actually
1285:44 - trainable parameters so when working with batch normalization in say TensorFlow or PyTorch
1285:52 - you'll notice that the batch norm layer will also have its parameters now here the role of this
1285:59 - gamma and this better is to scale and shift and these parameters are learned along with the
1286:08 - original model parameters and restore the representation power of the network and so
1286:14 - when we set gamma to be the square root of the variance and beater to be the to be the
1286:19 - expectance or the mean of x then we could recover the original activations if that were the optimal
1286:28 - thing to do so essentially what they're saying here is if it's instead optimal for us not to use
1286:36 - the batch normalization then we could adapt the value of gamma and better such that we get this
1286:44 - original value of extra here and the way this can be done is quite simple all we need to do here
1286:53 - is multiply this x by let's say gamma square plus epsilon so we have gamma square plus epsilon
1287:04 - and then once we've multiplied this you see once you take this here I multiply by this you set this
1287:15 - to like this do this this cancels out with this and you're left with xi minus the mean now when
1287:23 - you're left with xi minus the mean if beater is equal to mean then you will have xi minus the mean
1287:31 - plus beater which in this case is the mean and you see it cancels out and gives you xi which is
1287:42 - this original value of x so if we set our beater to be this our gamma to be this and our beater
1287:50 - to be the mean then in that case we retrieve our original value of x and so that's why you see
1287:56 - these two parameters are trainable such that we get the best values or the most optimal values for
1288:05 - gamma and beater then there's also this initialization that's the model or the network
1288:14 - is trained from scratch stochastic gradient descent is used with a mini batch size of 256
1288:20 - linearity starts from 0.1 and is divided by 10 when the error plateaus so basically
1288:25 - uh when we get to the points where let's have this when we get to a point where the arrow
1288:30 - starts to arrow starts to plateau then at that point we could update the learning rate from 0.1
1288:39 - to 0.01 and then if it plateaus it if it drops you see any plateaus let's go this way it drops
1288:46 - and plateaus again then we carry out this same computation now that said is for over 60 times
1288:55 - 10 to the 4 iterations weight decay and momentum are used and there is no use of dropout so again
1289:04 - in testing year we have different scales which are used and average so we pass the image at these
1289:10 - different scales and then the average value or the average scores are recorded now before we
1289:16 - move forward to check out some results it's important to note here that after this last
1289:24 - comp layer we do not carry out flattening instead what is done here is average pooling
1289:31 - in order to better understand how the global average pooling works let's consider this example
1289:38 - from peltarion.com so right here we're supposing that we have this as the output of the final comp
1289:46 - layer then what we do is instead of just flattening that's just picking all those values and
1289:52 - flattening them out and then passing to a fully connected layer what we are going to do is
1289:59 - depending on the number of neurons we want the next fully connected layer we are going to create
1290:05 - a certain number of channels so here for example we have this depth here the number of channels
1290:12 - here is three we have the height and we have the width and then since we're dealing with we're
1290:18 - actually doing global average pooling then for each and every one of these channels for this
1290:24 - channel this channel and this channel we're going to get the average value so you have the average
1290:29 - value is this eight here the average value is this three here the average value is this five and so
1290:35 - if you want a thousand of this then you should have a depth of a thousand right here and now with
1290:42 - this you see that it looks quite similar to the flattened layer as now you have all these different
1290:49 - values or these different single values which cannot be passed into a fully connected layer
1290:55 - here now it should be noted that in certain tasks like in classification which we are trying to do
1291:02 - this global average pooling will be great as the position of the pixels don't really matter
1291:11 - now this simply means that here since you get this average and here you get this average you
1291:16 - get this average it means that this position or this pixel wouldn't be close to this other pixel
1291:25 - as in the case of the flattening but since like in our case we're interested in saying whether
1291:32 - a person is angry happy or sad the positions of this output values right here won't really matter
1291:42 - as much as would would have mattered in the case where we're dealing with an object detection or
1291:50 - say object counting problem where the particular position of the person or of whatever we are
1291:58 - trying to detect actually counts so to better explain this again let's consider these two
1292:04 - examples example one example two right here for classification problems all we're interested in
1292:11 - detecting that this person is happy so whether we have a face this way or this way the position
1292:18 - doesn't really matter all we're interested in is in knowing whether this person is happy angry or
1292:23 - sad now for object detection the exact position of the person matters and so the exact position of
1292:33 - this neurons here will matter and so employing global average pooling for such tasks isn't
1292:40 - a great idea so in summary if you have a task where the position doesn't matter that much
1292:48 - then you could use a global average pooling if not then your advice to use the flattened layer
1292:53 - from here you could see the different variants of the resnet you see here we have the 18 layer
1292:58 - resnet as a resnet 18 resnet 34 resnet 50 resnet 101 resnet 152 and here we could see that with the
1293:07 - plain networks that's without the res residual block you find this 18 layer performing better
1293:14 - than the 34 layer but once we have the resnet block you find the 34 layer performing now better
1293:21 - than the 18 layer meaning that we could now go deeper we also have this table right here
1293:26 - which shows the vgg model the google the net the prlu unit and then the different plain networks and
1293:37 - the residual networks it shows clearly here that the resnet 150 performs best regardless of the
1293:45 - fact that it is deeper than the 101 and 50 counterparts and before we move on also note
1293:53 - that here this resnet block as you can see here is composed of these two conf layers for the 34
1293:59 - layer while from this 50 to 150 layers the resnet blocks are composed of three layers
1294:09 - or three conf layers as you could see right here
1294:15 - hello everyone and welcome to the session in which we are going to implement
1294:20 - the code for resnet 34 in tensorflow 2 now here is a resnet 34 model right here we have other
1294:27 - variants like the 50 101 and 152 after going through the section you'll be able to implement
1294:35 - this other variants right here and also you'll be able to get results like this where we can see
1294:43 - clearly an improvement in the accuracy of our model we are going to construct our resnet 34
1294:49 - model while making use of model subclassing so you could check out in the previous section
1294:55 - so you better understand how model subclassing is implemented in tensorflow 2 now that said here
1295:02 - we have this resnet 34 model right here and then the first layer we have is our convolutional
1295:12 - layer which has seven by seven filters and there are 64 in number also we know that the stride here
1295:21 - number of strides equal two so you could get all this from the paper we have information from the
1295:25 - paper seven by seven 64 stride two and then followed by a three by three max pool with a
1295:31 - stride of two so we get back here we have this three by three stride of two now from this we
1295:38 - then get into this residual blocks so we'll get back to the paper you see we have three of this
1295:46 - residual blocks now that's it's actually this because this is a 34 layer we're implementing
1295:51 - right here meaning that if you are implementing a 50 layer resnet then you would have one by one
1295:58 - three by three and one by one but for the 34 version we have three by three and three by three
1296:04 - now these are repeated tries so that's why you will notice here we have this repeated three times
1296:10 - and each one of them is our residual block now our residual block here our residual block is this
1296:17 - block right here so we are going to later on implement implement this residual block here
1296:23 - let's get back to the code we have simply our residual block you see the parameters here
1296:28 - number of filters 64 64 and year 64 just exactly as we have it in the paper you see for each block
1296:38 - we have number of filters to be 64 now if we want to get into our into this residual block
1296:45 - we could or we would see exactly how it's implemented i would see that we're gonna have
1296:52 - this two count layers one three by three and another three by three but for now let's do it
1296:58 - this way let's just consider that that has been implemented now another reason why i want to
1297:04 - implement this is this way is because now if you want to convert this to a resnet 50 all we need
1297:10 - to do now is just to update the code for the residual block since what makes this different
1297:15 - year is just this residual blocks right here so that said let's get back again here we have now
1297:25 - this four resnet blocks so we have this four residual blocks actually here you see you have
1297:33 - oh this is four and it's similar like here you have the three by three three by three
1297:38 - and then here the number of channels equals 128 so you notice here that we have 128 and we have
1297:46 - four of them now because uh here we're living from or we're modifying the number of channels
1297:53 - we need to take into consideration this number of strides we have right here as this permits us
1298:00 - to down sample our features now getting back to the code you have this year oh we here we have
1298:07 - the down sampling and then we move on 128 128 128 again here we have down sampling now we go to 256
1298:15 - exactly as is in the paper oh we look at this directly from here here we have 256 and we have
1298:24 - a certain number of them which are aligned we could see that in the summary year six of them
1298:29 - check this out here you see we have six of this aligned and then again we have 512 three of them
1298:38 - as is in the paper you see here 512 and that's it now from here we have the global average pooling
1298:46 - 2d global average pooling and then we have this final fully connected layer which with an activation
1298:52 - which is softmax as we've seen previously now that said uh we just in this our call method we just
1298:58 - simply gonna call all those different layers which we just created by passing in the input
1299:05 - so here we have our input x which goes through each and every layer and we get the output right
1299:12 - here now that said we're going to move on to uh looking at this residual block right here so
1299:19 - we're going to implement this residual block and now this is basically our residual block let's
1299:25 - increase this so we could see that clearly this is basically our residual block right here
1299:30 - so we could take one of this let's let's take this one for example we have this residual block
1299:36 - right here and then we'll get back to the code see it here now this is our residual block it's
1299:43 - a layer unlike the full model here we have this residual layer and then you would see that we
1299:51 - have this um dotted brilliant right here which is true when the number of strides is driven from one
1300:00 - so let's uh run this here let's let's have this we have dotted and then let's specify number of
1300:08 - strides say equal one we let's print out dotted after this so there we go we have your dotted
1300:18 - we run this take this off see it's false now when we set this to true turns to true so basically
1300:27 - that's what dotted here does and if you get back you will notice that we let's get back to this
1300:34 - we have this we selected this part but since this is this isn't a dotted uh link you see this link
1300:42 - is a full line so here our dotted variable will be false and when we get to this our dotted variable
1300:49 - will be true now let's get back to the code you see we have let's take this off you see here we
1300:56 - have um this which we understand already and I will get back here then after we have our two
1301:02 - convolutional layers now we'll define this custom come to d which again we are going to break up
1301:08 - uh subsequently so let's just understand that we have this custom come to d which is presented by
1301:14 - this uh year so when we have this this is uh it right here and this is the other one right here
1301:22 - now you'll notice that the number of channels has been passed here and the number of strides
1301:27 - has also been passed and that's exactly what was done here so you see we passed in the number of
1301:32 - channels and number of strides now since by default our number of strides is equal one it means when
1301:38 - we don't pass we simply say number of stride equal one but in cases where we have these transitions
1301:44 - we have number of strides equal two and so our value here is going to be changed now that said
1301:50 - we see we define this uh conv layer which has number of channels the kind of size three as in
1301:57 - the paper number of strides and then we have the padding same so we ensure that the height and
1302:03 - width of our input features remain unchanged now that said also notice that we have this number
1302:11 - of strides here for the second which is equal one and getting back to the paper that's simply because
1302:18 - even when we're getting or even when we have these transitions here where we're getting from 64 to 128
1302:25 - and that we're also doing a max pooling we have this stride or rather that we have in the striding
1302:31 - not max pooling we have this stride value change for only one of the conv layers and not the two
1302:38 - so you see here only one another two and so that's why right here you see only this one
1302:44 - which actually changes for this other one it remains fixed always one now that said we have
1302:49 - the activation layer and then uh if it's dotted then we're gonna have this link right here so if
1302:58 - it's dotted we're gonna have uh let's draw this here we're gonna have uh one by one conv layer
1303:05 - uh one by one oh it's actually here so we're gonna have our one let's take this one off
1303:11 - you're gonna have our one by one conv layer just right here to ensure that this two number of
1303:19 - channels match that's the number of channels we get as input here and as output actually match up
1303:26 - so um that's the role of this we've seen this already now we get back to the code we see here
1303:32 - let's take this off we see here this here and then we see the number of the the kernel size here is
1303:37 - one unlike here where we have a kernel size of three and then we also specify the number of
1303:41 - channels to ensure that it matches up with uh what we expect now the number of strides here
1303:47 - is gotten from this so if it's two you're gonna have two if it's one you're gonna have one
1303:52 - from here we have uh this set and then we can go ahead and do the calling so again please check
1304:00 - out on the previous sessions where we treat models of class and so you understand exactly
1304:04 - what's going on so here we have uh the input see the input it gets into the first conv layer
1304:10 - then gets to the second conv layer that's the output from the first gets as input to the second
1304:16 - and then we get this output and now let's suppose that uh we have a normal layer let's say we have
1304:22 - in this one let's get back to this let's suppose that we are working with this one right here
1304:29 - in that case then the input will be added to the output directly so here you see we have this add
1304:36 - layer right here this add layer then so flow takes the output which is this and adds this
1304:44 - to the input and we now get x add goes through the activation the relu and that's our output
1304:51 - now in the case where we have this the case where we have this one by one convolutional layer
1304:56 - let's specify this in the case where we are at this position then you'll see that we will take
1305:04 - the input and modify it before passing it to the output or before adding it with the output so you
1305:11 - see here we have this output there we go it remains here and then we modify this or we modify this
1305:20 - input sorry so we take this input there there is it here let's take all this off and try to redraw
1305:27 - it so we have this we have our red block then we have our output we have the addition which is
1305:34 - going to be right here addition and then we are going to have our one by one conf layer right here
1305:42 - which comes and adds up with this now this one by one convolution is exactly what's going on
1305:48 - right here and that's what we define here since kennel size one now that said you see we add this
1305:54 - up and then we get our output x add so if we are having dot it we have that else we go through the
1306:03 - normal path and that's basically why you see here we specify this sometimes and we don't know the
1306:10 - times now we have understood how this works let's go ahead to look at the custom conf 2d layer now
1306:17 - the custom conf 2d layer is basically made of a usual conf 2d layer and with a batch norm remember
1306:24 - the resnet model the resnet paper makes use of the batch normalization layer so basically here
1306:31 - instead of writing batch norm batch norm every time in our code what we just want to do is
1306:36 - combine this two and then we have our batch normalization with our conf 2d layer together
1306:42 - so that's it we now run the cell there we go we run the cell we again run the cell
1306:52 - and then we can define our resnet 34 which is our resnet model which we've just seen
1306:58 - resnet 34 there we go we have the resnet 34 summary let's run this
1307:09 - we get in this error we need to build our model so what we're going to do here is quite simple
1307:16 - we will now take this resnet 34 and then we'll call this resnet 34 so we will pass some inputs
1307:24 - into this our resnet 34 model so here we'll suppose tf0s and then we have 1 by 256 by 256 by 3
1307:34 - so we have this kind of input we run that and we see that now we have our model so our model
1307:42 - summary so this our model summary 21 million parameters and that's it now we go to the
1307:49 - training but this time around we're going to include some checkpointing so we got this from
1307:54 - our previous session where we treated checkpointing the model checkpointing so here we're going to
1308:00 - ensure that as we train we save our best model weights so that said here we have this checkpoint
1308:09 - callback again you could check back on our previous sessions where we treat these callbacks
1308:15 - so here we have this callback which will permit us total weights for our best or our best performing
1308:20 - model our best performing weights actually so here we have a monitor we want to monitor the
1308:27 - validation accuracy so let's take this off enter validation accuracy save best only true so that's
1308:36 - it let's run this our last function but before we move on let's get back to the section where we
1308:43 - have this parameter training right here now we have this custom conf2d model let's delete the cells
1308:53 - we have this custom conf2d model which we've seen already and then we have this batch norm layer
1308:59 - but it should be noted that with the batch norm layer we have to specify whether we are in training
1309:06 - mode or in inference or testing mode now the reason why we are doing this is because the
1309:12 - parameters of our batch norm layer will react differently or behave differently in these two
1309:20 - different modes this means that during training this layer will normalize the inputs with the
1309:28 - mean and variance of the current batch of inputs now we've seen this already and then when we're
1309:35 - not training that's when training equals false we're in inference mode the layer will normalize
1309:40 - this input using the mean and variance of its moving statistics learned during training so this
1309:46 - simply means that we have this layer right here with some parameters let's call the parameters
1309:52 - say p let's call the parameters p so you have some parameters right here and then during training
1310:01 - our layer updates these parameters but then during inference we do not want to update
1310:08 - these parameters as they were learned during training and so we have to specify or we have to
1310:16 - pass in the training or set the training to false when we are not training the model or when we are
1310:22 - evaluating or testing the model now what this simply means is that here we'll have this to
1310:28 - training so here you see we've passed in training and then here we have training so by default we
1310:34 - could set this training to true so by default in training mode and that's what we have now let's
1310:39 - run the cell and then get into our residual block for residual block here we have training again so
1310:47 - we have training and then since we're calling this we would have training there we go we have
1310:54 - training and that's it we have training let's run this we get back to our complete network and here
1311:05 - we're gonna have this training so paste that out here and that should be fine so now when we are
1311:13 - not in training mode we could specify the training parameters such that the batch norm layer or the
1311:19 - batch norm layers parameters aren't modified so we have that and that's it okay so we have that
1311:28 - set now let's run this and this time around we're going to set this training so let's set training
1311:34 - to be true we have that true let's make sure that this was passed in here so let's have this
1311:41 - training and the default is true okay we run that we run this and that looks fine now we could set
1311:51 - this to be false or we don't we don't put any value it means it's true so we could set this to be
1311:55 - false it's training to be false and that's it so there we go we have this set now let's get back
1312:03 - to our last function so we were at this point we have our metrics we run the metrics we compile
1312:10 - the model but this time around we use a higher learning rate so one thing you could also do is
1312:16 - as described in the paper decrease this learning rate as soon as the model starts plateauing
1312:22 - so we could start with a learning rate in the paper it should be 0.1 although here we're going
1312:26 - to start with 0.01 so here the say you you have this learning rate and then when it starts plateauing
1312:32 - you drop to 0.01 and then you go with that and then it starts plateauing you drop and so on and
1312:40 - so forth so this is what they proposed in the paper and you could always implement this and
1312:46 - we've seen this in some previous section where we implemented this kind of callback which permitted
1312:52 - us to schedule our learning rate so that's it let's take this off get back to the code
1312:58 - and we have we've run this already so we could start with the training now yeah we're going to
1313:04 - train for 60 epochs and we're going to include the callbacks so let's have this callbacks and
1313:11 - we have our model checkpoint callback which we have defined here let's copy this there we go
1313:19 - copy this and we have it here so that's it we can now run this cell now we've launched this training
1313:28 - and we've had this error here saving the model to the HDF5 format requires a model to be a functional
1313:34 - model or sequential model it does not work for subclass models like in our case because such
1313:40 - models are defined we had a body of a python method which isn't safely serializable hence
1313:46 - consider saving to the tensorflow saved format by setting save format to tf or using save weights
1313:53 - so now we're going to save these models in the tensorflow format all we need to do here is
1313:58 - specify this folder and that should be fine the mode is max now since we want to store the weights
1314:04 - which have the highest validation accuracy that's fine let's now run this again training
1314:12 - now complete and we achieve an accuracy or the best accuracy of 83.6 we have our accuracy plot
1314:22 - right here and then we could evaluate the model here we get 82.3 percent and 94.6 percent
1314:31 - for the top k accuracy now what if we load our best model because the model we have in
1314:39 - here is the latest model the very last one now let's load our best model and we evaluate this
1314:44 - let's add this code cell then we go ahead and load our best weights here we have resnet 34
1314:54 - dot load weights then here we have our best weights so this should be a string since our folder
1315:03 - there's a photo where we store the weights resnet we run this that's fine and then we
1315:11 - evaluate our model see here we get in this accuracy of 83.6 and then top k accuracy of 95.6
1315:22 - percent now we go on to test this that's fine and here are some results we get happy angry sad happy
1315:36 - yeah we miss we miss one we miss this one oh that's two three and that's it so we miss three
1315:46 - meaning we have let's add the cell we have 13 out of 16 angles correct 81.2 percent correct
1315:58 - okay so that's it we've gone from 79 percent to 83 percent by modifying or changing our model
1316:07 - now let's plot out this confusion matrix and see what we get
1316:12 - there we go your results which are much better than what we have had so far
1316:21 - hello everyone and welcome to this new and amazing section in which we are going to treat
1316:27 - the mobile net architecture this was first developed by google researchers in 2017 and
1316:34 - we had the mobile net version 1 and 2019 the again developed the mobile net version 2
1316:41 - and after this there was a mobile net version 3 but yeah we are just going to focus on this
1316:47 - mobile net version 2 entitled inverted residuals and linear bottlenecks by just looking at the
1316:55 - title we could guess the type of environments for which this model was built for in fact the mobile
1317:04 - nets have been built for environments with low compute resources like the mobile and edge devices
1317:12 - so in this section we are going to focus on what permits this model that's the mobile net v2
1317:21 - to perform quite well in terms of speed while producing high quality results there are two
1317:28 - major techniques which make the mobile net version 2 very powerful or which permits them
1317:35 - or which permits us work at higher speeds while still maintaining reasonable quality results
1317:42 - now these two are the depth wise separable convolutions and the inverted residual bottleneck
1317:52 - which we have right here here's the separable convolutions this is a regular convolution
1317:59 - your separable convolution and we'll start by explaining what a depth separable convolution
1318:07 - is a depth separable convolution is simply a combination of a depth wise convolution
1318:14 - and a point wise convolution now this point wise convolution is not in different from a normal
1318:22 - convolution layer but with countout size of one so one by one convolution here that's a point wise
1318:31 - convolution and before this we have the depth wise convolution now to understand like this now the
1318:37 - depth separable convolution which is this two put together in sequence or sequentially now to
1318:46 - understand what this depth wise convolution is actually let's get back to this demo where we saw
1318:55 - how usual convolution operation works as you could see here we have this input basically let's have
1319:03 - this let's toggle the movement so here we have some input now this input is three-dimensional
1319:11 - as you see here we have one two three so zero one two we have these three dimensions let's change
1319:18 - the color so it's clear for you to see we have this first dimension we have the second dimension
1319:26 - and then we have this other third dimension now what goes on during a convolution operation
1319:35 - is that we have this uh kennel so we have this here and then we have also some kennel so in this
1319:43 - case three by three kennel uh we have this on a kennel here three by three and the reason why it
1319:51 - is three by three is because we have three channels here in the input so because the inputs
1319:57 - are three channeled or have three channels we have a three channel kennel now here we could
1320:06 - add this we could add a foot channel right here add this foot channel and then we will also have here
1320:11 - four of these kennels and then what happens is going uh here during the convolution operation
1320:20 - is exactly what we're seeing here so we have this one which uh is placed at a given position
1320:29 - let's pick its color it's placed at this position for example and then we multiply all the values
1320:36 - like you could let's uh say we add this point here you see at this point we can see how this
1320:43 - filter in this case this filter so this filter is actually we match this filter with this one
1320:49 - we match this one with this in red we match this one uh with this in green uh now since you're
1320:56 - three obviously we don't have four now uh let's get back to this operation where we take this one
1321:02 - and multiply with all the values we have here so when we multiply all the values we have here
1321:08 - we get for example in this case one by zero plus zero times zero plus one times zero you see all
1321:16 - this at the top are canceled then we have zero times zero zero times one canceled we have one
1321:23 - times one here so we have one and then we have one times zero zero negative one times one negative
1321:28 - one so we have negative one and then negative one times two we have negative two so this gives us
1321:36 - a value of negative two and then we move to the next one this one let's change the color you see
1321:45 - we have one times all this at the top is zero so when we multiply all this by because it's just
1321:50 - simply matching so when we have one times zero we have zero zero times zero zero one times zero zero
1321:56 - one times zero zero negative one times two negative two then negative one times zero we have zero
1322:05 - negative one times zero zero negative one times two negative two so this gives us negative four
1322:11 - at this point we have negative four we move to the next one
1322:15 - here we have negative four all this is zero obviously so we just get to this one one times
1322:20 - 1, we have 1, 0, negative 1, and then 0. So we have 1, let's write that, we have 1, and
1322:32 - minus 1, because this is negative 1 times 1, minus 1, it gives us 0. So here we have
1322:38 - negative 6, when you add all this up, basically we're taking this, we multiply, and then we
1322:43 - add, here we take this, multiply, we add, take this, multiply, we add, and we add all
1322:48 - these values, and this gives us here negative 6. Now because we have this bias of 1, we
1322:54 - add plus 1, it gives us negative 5. That's how we obtain this value here. That's how
1322:58 - we obtain this 1 right here. Now, we'll repeat the same process for all these filters, all
1323:07 - the different positions. So basically, let's toggle this, so you see what happens. You
1323:13 - see, we repeat this process, we move, we move, we go down, and that's it. There we go. So
1323:22 - we repeat this to the end until we have this final value right here. Then if we want to
1323:27 - have, you see that we get this, so what we have is, let's take this off, let's take this
1323:33 - orange off. What we have is, now we have this output, so we have the input which is of the
1323:42 - channel, which has number of channels 3, here we also have this number of channels 3, and
1323:47 - then we have an output which has number of channels 1. But if we want the number of channels
1323:53 - to be equal, say 2, like in this case, for the output we have 1 channel and this other
1323:58 - channel, then we need to increase the number of filters we have here. So basically, here
1324:02 - we have how many filters? We could create this again, so we will have now 2. If we want
1324:08 - to have output 2, then we will need to have 2 of this, of this 3 dimensional filter. So
1324:14 - we will have this again, we will have this with its own weights, obviously, and we will
1324:22 - have this, and that's it. So because we have this 2 now, we will no longer have an output
1324:28 - with 1 channel, but now an output with 2 channels, as you could see here. Now notice how, as
1324:35 - we take this, we move to this next. So this year is this one year, let's pause this. So
1324:42 - this year is this one right here, and this one is this one right here. Now with this
1324:53 - year, we are able to get this other channel for the output. And so that's how the convolution
1324:59 - operation works for a normal convolution. Now if we get to the depth-wise convolution,
1325:05 - we will find that it will be different from this method. For the depth-wise convolution,
1325:12 - as the name goes, these computations are done depth-wise. So first of all, now here, this
1325:21 - output will only be gotten from interactions of this channel and this channel right here.
1325:33 - So what goes on here is, we take this, and then we pass it around as we usually do, and
1325:39 - then we obtain this new output right here. So if this is 3 by 3, then we obtain some
1325:47 - values 1, 2, 3, 4, 5, 6, 7, 8, 9. So these values will be different from this one because
1325:56 - the way we compute these values is different from the way these values were computed. The
1326:02 - way we computed this value was, we take this year, pass it here, take this, pass this,
1326:08 - take this, pass this here, and then add all our resulting sums to obtain this value of
1326:16 - negative 5, as we saw. But in this case, what we'll obtain for this first value wouldn't
1326:22 - be this 5. What we'll obtain here will be 1 times 0, 0 times 0, plus 1 times 0, 0, 0,
1326:31 - 1 times 1, we will have 1, and then here we'll have 0, here 1 times 1, negative 1, negative
1326:37 - 2, negative 1, negative 2, it gives us negative 2. So what we'll obtain here will be negative
1326:43 - 2. And then we'll move on to the next, we'll move on to the next, we'll go to this next
1326:50 - position, we'll get some value as we've seen here, and up to this last value right here.
1326:56 - So the way we got this was, we took all this for different channels, and then we added
1327:02 - them up to get this by year. We just get this directly by taking for each and every channel
1327:08 - and just producing the output like this. So this means that with this we are going to
1327:13 - have, like here, let's take this off, with this, because we are having each channel for
1327:21 - the filters producing its own output, we're going to have this three producing three different
1327:27 - outputs. So here already we have three outputs, unlike here where we had two outputs. And
1327:33 - the two outputs here were controlled by the number of canals we used, because here we
1327:38 - used two canals, we have two outputs. But here we, this doesn't matter. Here the number
1327:45 - of output or the number of input channels we have here will dictate the number of outputs.
1327:51 - So here we have three channels. So we just obviously have these three different outputs.
1327:56 - Now since we have these three outputs and we want to be able to control the number of
1328:00 - output channels, what we'll do now after the depthwise convolution, which is in fact what
1328:05 - we've just explained here is we're going to add now the one by one convolution as a point
1328:11 - wise convolution after adding the one by one convolution, we are going to specify the number
1328:18 - of channels here and this number of channels of this one by one convolution that will permit
1328:23 - us leave from a certain number of channels, like in this case three to another number
1328:29 - of channel or to a given number of channels, let's say two. So after getting these three
1328:35 - channels or getting this output with three channels, we now get, we pass to this point
1328:39 - wise convolution and now we're going to get just two channels. To better understand this,
1328:47 - let's take this depthwise convolution image from paper suite code. So here, let's try
1328:52 - to reduce this. Anyway, let's have this. You see we have one, two, three, you see this
1328:58 - one, two, three, this is a five by five canal. And then we pass this, notice how each and
1329:04 - every one is now responsible for its own output. See this, see we have this orange with this
1329:13 - for this particular channel gives us an output, the red gives us an output, the yellow gives
1329:18 - us an output. Unlike previously where we'll take this pass this year and then add all
1329:25 - this together. But now instead what we do is we just simply carry out that addition
1329:33 - at the level of the channel and then we get this output right here. Now let's see why
1329:40 - the depthwise or the depth separable convolution is more efficient. And we'll do this by calculating
1329:46 - the number of filters. So here we find the number of filters we need to get from this
1329:52 - input, which is one, two, three, four, five, six, seven. So we have this seven by seven
1329:59 - by three inputs, which want to convert to this three by three by two output. And here
1330:10 - the number of filters or number of parameters we used can be calculated as here we have
1330:15 - nine times three. Obviously in this nine times three comes from the fact that we have for
1330:21 - each of this, we have nine and times three. The three is from here actually, because we
1330:26 - have three input channels, then we will have three filter channels. And so we're going
1330:32 - to have the filter size, which is three by three. I think we should change this color.
1330:39 - So it makes it clearer. Let's change this. So here we have seven by seven by three, the
1330:45 - three is for this number of channels. Here we have three by three. And then this is three
1330:50 - by three for a single one, then times three, this number of weights. But now because we
1330:56 - want to have, let's change this color again here, because we want to have an output with
1331:00 - two channels, then we would multiply this again by the number of output channels too.
1331:08 - So this is like some general formula to get a number of parameters. We're going to omit
1331:13 - the biases. So here we have three by three by 327 times 254 parameters. Now this means
1331:22 - that if we have, if we want to have number of channels of say 16, then this will give
1331:29 - us 432. Now let's consider that we're dealing with depth wise convolution and the point
1331:40 - wise convolution, which would form the depth separable convolution. With a depth wise,
1331:46 - what we'll have, first of all, we'll have to note that this is no more needed, we just
1331:51 - need this. So what we'll have will be three by three by three. Now this three comes from
1332:02 - the number of inputs by now to get the number of outputs, we wouldn't carry out any multiplication
1332:11 - here because basically the output from the depth wise convolution is a three by three
1332:20 - by three tensor. In this case, or we could just say it's a three channel output. Since
1332:27 - we have three channel inputs, we'll have three channel output. So here to obtain the outputs,
1332:33 - we just need this, we saw this already, we just take this multiplier, get the output,
1332:38 - get this first one, we take this multiplier, get this next one, take this multiplier, get
1332:45 - this next one, and we're good to go. So once we have this, we now add the weights for the
1332:52 - point wise convolution. Now for the point wise, it's a one by one canal. So you see
1332:58 - it's quite cheap as compared to the three by three canals. So one by one, here we have
1333:03 - one by one. So let's just put this here, we have one by one. Now times the inputs, just
1333:15 - like with the usual convolution, it's the same thing actually, because here for the
1333:19 - usual convolution, here we have to calculate the number of weights, just get the canal
1333:23 - size, then like one of the canal size times the canal size times the number of input channels
1333:30 - times number of output channels. So here we have the canal size times the canal size times
1333:35 - the number of input channels, which in this case is three, and the number of output channels,
1333:40 - which is two. So that's what we have. Now if we multiply this, we have 27 plus six that
1333:48 - gives us something like 33. You see that this gives us 33. Now one interesting point to
1333:55 - note here is if we modify this and say we want to have 16 output channel, then we will
1334:04 - change this to and put 16. In that case, our answer will be 27, here we have 27 plus 48.
1334:16 - Now 27 plus 48, we get that quickly. 5, 7, that gives us 75. So you see that here we
1334:23 - have increased this number of channels from 2 to 16, number of weights 75. But when we
1334:30 - did that year, number of weights went to 432. So clearly, the depth wise or the depth separable
1334:37 - convolution is one that is way cheaper than the normal convolution. And in the paper,
1334:45 - the authors argue that this kinds of convolution permits us to reduce the computational cost
1334:52 - by eight to nine times than that of a standard convolution while we have only a small reduction
1335:01 - in the accuracy. That said, from this diagram here, you should now understand why when representing
1335:09 - a regular convolution, the authors have this filter, you see the filter here, which has
1335:17 - this depth into the input, you see this depth right here, you see this. Whereas when representing
1335:25 - the separable convolution, we have this filter which doesn't have any depth. And that's
1335:32 - because here we have no inter-channel computation or calculations as compared to this one. And
1335:38 - then after we have this point wise. Now the point wise is a regular convolution. So we
1335:44 - see we have this depth gain. But now it's smaller in size because it's just a one by
1335:49 - one filter. The next improvement we should look at is this inverted residual block right
1335:55 - here. So it's first of all called inverted in comparison to the residual block. Now the
1336:02 - residual block, as you may notice, you have this large, relatively large channel, and
1336:09 - then it becomes small in the middle, and then it becomes large in the output. So this means
1336:15 - that we have some input, we have some input, and then we have residual block and then we
1336:21 - have some output. Then we have obviously this link right here from the input to the output.
1336:28 - Now with this, you see this becomes the data or the inputs big, it goes to small and then
1336:36 - big. But here what we have is we pass in a relatively small channel, small number of
1336:43 - channels on input with relatively small number of channels. So here is small and then in
1336:49 - our block, it becomes the number of channels increased. And then as output number of channels
1336:55 - reduced. Hence the term inverted residual block. Now in addition to the fact that we're using
1337:03 - depth wise convolutions, instead of the normal convolutions, the fact that we have relatively
1337:10 - lower dimensional data getting into this block and lower dimensional data getting out means
1337:19 - that we could transport very low dimensional data throughout our mobile network. So here
1337:27 - we have low dimensional data getting in, low dimensional data getting out, and then inside
1337:34 - we have this expansion layer right here. As this expansion layer permits us capture as
1337:42 - much information as possible from our input features. One thing to also notice the fact
1337:48 - that we're using a relu 6. The relu 6 is different from the usual relu in the sense
1337:53 - that with a relu, we have for all x less than zero, the value is zero. For all x greater
1337:59 - than zero, the value is x. So we have this y equals x line right here. But with the relu
1338:05 - 6, as from the value 6, we actually clip this output. So what we have here is all values.
1338:13 - Let's get back. What we have here is we have the values, it remains x, but once we get
1338:19 - to 6, it gets clipped. So for all values of x greater than 6, the value remains at 6.
1338:26 - So that's a relu 6. And then one of the important points is the fact that because we're carrying
1338:32 - out this projection from high dimensional data to low dimensional data, this relu non-linearity
1338:39 - generally will cause us to lose too much information. And because of that, there is no relu activation
1338:48 - in the final layer here. You can see this in the summary right here. You see, we have
1338:54 - the input. Then we have the expansion factor, which is T. This means that now we have this
1339:02 - hyper parameter, which we can tune. So if we want better results, we can tune this expansion
1339:09 - factor so that it permits us to get this better results. So here we have this expansion factor
1339:14 - T. It's here, which expands the morph channel. So we get in with K and then we now move to
1339:20 - TK. And then we have this TK, which now takes us to K prime. Now also note that here we
1339:30 - have the relu 6, relu 6, but here we have no activation. Now that said, this is a summary
1339:36 - of our mobile net version 2. So here you have mobile net version 2. You have the different
1339:41 - bottlenecks. There we go. And then we have this conf to D average pool and then conf
1339:49 - to D. This figure right here also shows us how the mobile net version 2 outperforms the
1339:56 - mobile net D1 shuffle net and the NAS net. You see that with a mobile net version 2 right
1340:04 - here. If you pick this, let's pick, for example, this two, you see the number of operations
1340:12 - or the competition cost here is almost similar, but we see this great difference in accuracy
1340:20 - where the mobile net V2 outperforms the mobile net V1. Then apart from classification, the
1340:28 - mobile net V2 has been using other tasks like object detection, semantic segmentation and
1340:36 - other computer vision tasks where we have low compute resource.
1340:44 - Hello everyone and welcome to the session in which we treat this modern convolutional
1340:50 - neural network architecture known as the efficient nets. In this efficient net paper, the authors
1340:57 - proposed a more controlled manner of designing convolutional neural networks such that it
1341:05 - suits our demands in accuracy and speed. And as you can see in those plots, you see that
1341:13 - we could choose suitable parameters such that we could modify or increase our accuracy while
1341:23 - taking note of how this affects the speed. That said, in this section, we'll see how
1341:29 - Ming Xing Tang and Kwok Lei built the system for automatically scaling our convolutional
1341:37 - neural networks much more efficiently. Conf nets are commonly developed at a fixed resource
1341:44 - budget and then scaled up for better accuracy if more resources are available. So with the
1341:50 - case of the ResNet, we had ResNet 34. Then after we had ResNet 50, ResNet 152. And depending
1342:05 - on the kind of setting, we are going to pick this ResNet model which will permit us to
1342:15 - run without any problems of latency while maintaining reasonable accuracy. So this means
1342:23 - that if we are working in a high compute environment, then we could afford to work with this. Whereas
1342:30 - if we are working in a low compute environment, then we would have to work with this model
1342:36 - with fewer conv layers. Now that said, in this paper, the authors propose a more systematic
1342:47 - study of how this model scaling can be done. And unlike other methods where we just scale
1342:54 - by increasing the depth, here the proposal scaling by increasing the depth, increasing
1342:59 - the width, the number of channels and the resolution. That is the size of the input
1343:06 - image. And so here the proposal new scaling method that uniformly scales all dimensions
1343:13 - of the depth with resolution using a simple yet highly effective compound coefficient.
1343:22 - You can see the results right here. You see, for example, the ResNet 50. Let's extrapolate.
1343:29 - Let's pick this here, although this has more parameters on the ResNet 50. Let's take instead
1343:36 - the B4 because it has less parameters. So you see, it has fewer parameters than the
1343:40 - ResNet 50. But its accuracy, that's the top one accuracy on the image net, is much greater
1343:48 - than that of this ResNet 50 here. So we have the EfficientNet B4 version, which is about
1343:54 - 83% to 1% accuracy, while this is only at about 76% to 1% accuracy. Now in this figure,
1344:06 - we see how we have this baseline, some sort of baseline, like in the case of the ResNet,
1344:11 - we could say this is ResNet 18. And then we have this deeper model depth scaling. This
1344:17 - could be ResNet, say 50. Now, in this case, they have this baseline. First of all, this
1344:23 - baseline is gotten by carrying out an automatic network architecture search. So we get this
1344:31 - baseline and the different layers we have for this baseline. And then note that this
1344:39 - baseline has a depth, as you can see, it has a depth. And when we scale deeper, when we
1344:46 - carry out depth scaling, you see, we have much more layers added to this one. And then
1344:53 - when we do width scaling, we increase the number of channels. So you see, we have this
1344:59 - smaller channels for the baseline. And then the width scaling permits us to increase this
1345:04 - number of channels. Then also, we have the resolution scaling, which has to do with the
1345:09 - inputs. So there we have this input height times width. And now after currently resolution
1345:14 - scaling, we see we increase this resolution. This means that we may work with a base of
1345:20 - 224 by 224. And then after scaling, we may get to say 640 by 640. Then from here, we
1345:33 - also have the compound scaling, which is what is used in this paper, where we don't only
1345:39 - focus on the width or the depth or the resolution, but we scale all this systematically to achieve
1345:46 - the best possible results while maintaining reasonable speeds. That said, we could see
1345:55 - from these different plots that when you increase, like here we have the width, there's number
1346:02 - of channels, which is increased. You notice that as we increase this number of channels,
1346:07 - at some point it starts to plateau. And then when we increase the depth at some point,
1346:12 - it starts to plateau. Then when we also increase this input size as a resolution, at some point
1346:18 - it starts to plateau. And so this is why the authors proposed a technique where we could
1346:24 - combine all this such that we get even better results. And there we go. We see the effect
1346:31 - of compound scaling. You see that we have this D, depth, and then our resolution. You
1346:38 - see when the depth is one, our resolution is one, we at this blue here, see we have
1346:44 - worst results here. Whereas when we double this depth and then increase the resolution
1346:50 - by 1.3, you see we have this best results right here. That said, we'll now dive a bit
1346:58 - more deeper and look at this compound coefficient, which they spoke of at the very beginning.
1347:03 - So we go down here and we have this formula right here. See this formula right here. All
1347:12 - right at this equation, which is equation three, where we have this depth, we have these
1347:17 - different formulas, these three formulas. The depth equals alpha times phi. And now
1347:23 - these phis are user specified coefficient that controls how many more resources are
1347:28 - available for model scaling. So this is some sort of scaling coefficient right here. So
1347:34 - it's phi, phi, phi. And then here we have alpha, beta, and gamma. Now this is designed
1347:40 - such that alpha, beta squared, gamma squared, is approximately equal to 2. And alpha is
1347:46 - greater than or equal to 1, beta is always greater than or equal to 1, and gamma always
1347:49 - greater than or equal to 1. So now we are going to carry out a grid search. So we're
1347:54 - going to search for the best values for this alpha, beta, and gamma, and then fix them.
1348:01 - Obviously, they are constants, so we're going to fix this. And then now start varying phi
1348:06 - such that we carry out the scaling in a more systematic manner. And there we go to carry
1348:13 - out or to find the values for alpha, beta, and gamma, the fixed phi to be equal to 1.
1348:19 - And then they obtain alpha 1.2, beta 1.1, and gamma 1.15, all of this such that we have
1348:26 - this constraint. Now they didn't fix alpha, beta, and gamma as constants and scale up
1348:32 - the baseline network with the different phi as we already explained. And it's based on
1348:38 - these different values of phi that we obtain the different versions of the efficient net
1348:43 - going from B1 to B7. Now before moving on, it's important to take note of this efficient
1348:50 - net B0, which is our baseline network. Remember, we have some baseline network, which we have
1348:56 - seen here. Let's go this way. We have this baseline network right here, this one, this
1349:03 - baseline network, which we are going to scale such that we have better results while working
1349:12 - with compute constraints. Now that said, let's take this off and then scroll down back to
1349:20 - this baseline model, which is giving just here in this table. You see we have this baseline
1349:26 - model, efficient net B0. And then you'll notice first that the resolution is 224 by 224, meaning
1349:34 - that we're going to start with image sizes of 224 by 224. But note that different image
1349:40 - sizes could be used for the different models, although the best or the most adapted resolution
1349:49 - for each model should be preferably used. Now that said, here you see we have a usual
1349:54 - conf layer and then we have this MB conf right here. Now before getting to the MB conf, also
1350:00 - note that after carrying out the neural architecture search, the authors notice that we could also
1350:08 - make use of this 5x5 kennel or 5x5 kennel size filters. So unlike what we had discussed
1350:17 - in previous sessions, this 5x5 kennel size filters are still very useful. Then getting
1350:25 - to the MB conf we find here, here to say its main building block is this mobile inverted
1350:33 - bottleneck. So recall the mobile inverted bottleneck, which we found in Sandlai R. Sandlai R is
1350:39 - the MobileNet version 2 paper we had seen already, to which they also add the squeeze
1350:44 - and excitation optimization. Now in the MobileNet version 3, the squeeze and excitation optimization
1350:51 - was added. So here we have basically the MobileNet inverted residual block, which we had seen
1350:59 - already. And then if we check out in this MobileNet v3 paper, which you can feel free
1351:04 - to look at, you would have this squeeze and excitation right here. Let's zoom into this.
1351:11 - See here we have the MobileNet version 2 with bottleneck residual, this residual, then our
1351:17 - bottleneck as usual. Here we have this low dimension input getting in and then it gets
1351:26 - expanded and then we have this low dimension output, which is produced in this final layer
1351:33 - right here. Now with this squeeze and excitation, to better understand this squeeze and excitation
1351:41 - layer, we should or we could get back to how the conf layers actually work. You see that
1351:48 - to get this output, let's take this off, to get this output, for example, we carry out
1351:57 - multiplications and additions for each and every channel here, that's for each and every
1352:02 - channel on the input and those filters which correspond to this channel. And then to produce
1352:08 - this negative one right here, all this are added up with equal weights. So the output
1352:14 - from this computation, let's call it alpha, will be put here, plus the output from this
1352:19 - computation, let's call it beta, we will put here, plus the output from this computation,
1352:24 - let's call it gamma, will be put here and we'll get this value or this output of negative
1352:30 - one at this position. Now what this squeeze and excitation layer brings in is some weights
1352:38 - on this addition operation right here. So instead of just having a weight of one year
1352:44 - one and year one, we're going to have some modified parameters or some parameters added
1352:50 - here such that certain channels influence the output more than some others. And so here
1353:00 - we could have instead of one, we can have a weight A, you have a weight B and your weight
1353:06 - C. Getting back to this paper, the way this is done is as such, we start by carrying out
1353:14 - some pooling and the result of this pooling will be one by one by C output. Now C is the
1353:22 - number of channels. So if here we have C channels, here we'll have this output here, C. So this
1353:29 - output here will be one by one by C. You notice how this is small and then we have C. This
1353:36 - size, the size C is exactly the same size right here. So we have exactly this size is
1353:44 - the same as this. And then for the height and width is one by one. Now once we get this,
1353:53 - we pass this through two fully connected layers, you see with this radio activation. And then
1353:59 - here we have this hard sigmoid activation after this fully connected layer. And then
1354:05 - here we get this output of this same number of channels, C, which will match with this
1354:15 - one. But now what we get here will be multiplied by each and every channel here. So this now
1354:24 - we serve as the weights, we serve as, because remember we designed this as A, A alpha plus
1354:35 - beta plus gamma. And then we had A, B and then C. So this A, B and C is actually this
1354:46 - output right here. We're supposing that C, the channel size is equal to three. So we
1354:51 - have this three here and these are the values which we get after going through this fully
1354:57 - connected layer. And then we take this now and multiply by each and every channel. So
1355:02 - if you break this up into three parts, let's remove this. Let's erase this. And we could
1355:09 - cut this, let's cut this into three parts. So we have one, two, three. So we suppose
1355:15 - there is one by one by three. So if we have this, this first part will multiply this chunk.
1355:22 - So we'll multiply this chunk and then this other part will multiply this next chunk.
1355:29 - And then this other part here will multiply this other chunk. And so now we have this
1355:35 - channels whose contribution to this output is now weighted. That said, we also have the
1355:44 - expansion factor here is six. Then getting back to the results, we see how the efficient
1355:51 - nets perform better than the corresponding other confidence with similar number of parameters
1356:01 - or even more number of parameters. Like here we see how the efficient net B zero outperforms
1356:08 - the resnet 50 though you see this great difference number of parameters as the efficient net
1356:15 - is more efficient as or has fewer parameters as compared to the resnet 50. We see resnet
1356:22 - efficient at B one compared to resnet 152. You see 60 million years 7.8, but this one
1356:29 - is more accurate than the resnet 152. You could check out from this right up to efficient
1356:35 - net B seven. You see we have this G pipe. They are 97.97, but this one has 557 million
1356:44 - parameters while this is only at 66 million parameters. And we could also look at the
1356:51 - floating point operations. You see here we have fewer floating point operations for the
1357:00 - efficient net B zero while still having higher accuracy. We also see that if we scale the
1357:07 - mobile nets and the resnets will still, they wouldn't still get better results compared
1357:14 - to the efficient net. And it shows the power of the network architecture search, which
1357:20 - was used in getting our baseline. Now we'll go down and check out this year. We have this
1357:29 - results right here. You see the class activation map, which is a visualization technique, which
1357:36 - permits practitioners understand how the model or rather what portions of the inputs helped
1357:46 - in producing the outputs shows clearly here that when we use compounds killing, we have
1357:56 - the map, which is more focused on relevant regions, as you could see right here as compared
1358:03 - to the baseline model and this other models with the deeper with depth scaling with scaling
1358:10 - and resolution scaling. Hello everyone and welcome to this new and exciting session in
1358:22 - which we are going to treat transfer learning and fine tuning. Transfer learning can be
1358:27 - applied in several domains like computer vision, natural language processing and speech. In
1358:34 - order to better understand the usefulness of transfer learning, we have to take note
1358:39 - that deep learning models work best when given much data. And so this means that if you have
1358:47 - a data set of only a hundred data points, then you're most likely going to have a poorly
1358:53 - performing model. But what if we tell you that it's possible to train your model or
1359:00 - to train a given model on say a million data points and then use that model or adapt that
1359:09 - model such that you can now train it on this very small data set such that you start getting
1359:17 - very great results. This is very possible with transfer learning and that's what we
1359:23 - shall be treated in this section. At this point, one question which may be going through
1359:28 - your minus, how is it possible to train a model made of say a million data points and
1359:37 - then use that same model on a smaller data set which is obviously different from this
1359:44 - data set with about a hundred data points. The answer to this question lies in this figure
1359:54 - we have right here. Notice how you have this image here, this image of this truck and then
1360:03 - we have this model which takes this input and then produces some outputs right here.
1360:11 - This model or the kind of model we'll use for this image tasks are generally the conf
1360:17 - nets as we've seen already in this course. And with the conf nets, we generally have
1360:23 - two main sections. The first section is the feature extractor. And then as we go towards
1360:31 - this final sections here, we have the classifier. And so the very first thing the conf net we
1360:39 - want to do will be to extract low level features. And then as we go towards the end, we focus
1360:48 - on extracting more high level features. Notice how you're with this, let's pick out this
1360:56 - feature right here or this feature map. You see that we pick up these edges. You see for
1361:03 - other feature maps, we actually filtering out some low level features or extracting
1361:08 - this low level features from our input. Then as we go get towards the end, we get more
1361:14 - high level features like for example, whole portions of the image like the tire. And then
1361:22 - after this, we generally have a classifier which now permits us to pick between a set
1361:29 - of options which one the model thinks this image is actually. Now because the conf net
1361:38 - works this way, it means that if we have two data sets which are similar, then we could
1361:47 - build some sort of feature extractor or we could build a model which will extract features
1361:55 - from this very large data set. And then because these weights have been tuned or have been
1362:06 - trained such that the extract features correctly, then when we pass in this very small data
1362:15 - set, this section of the model will do just its job. That is of extracting useful features
1362:24 - from this data set. And you see that because these two data sets are similar, it's going
1362:31 - to do a great job. And so we will not need a very large data set in order to extract
1362:40 - features from this small 100 size data set right here. And so in fact, what we're saying
1362:49 - is we have a model, let's say we have this model, then we have this feature extractor
1362:55 - unit and then we have this classifier unit which we've seen already. Generally this starts
1363:02 - after the flattening all the global average pooling and then here we have some dense layers
1363:08 - while with this we have a ConvNet or some convolutional layers with some max pool and
1363:13 - batch norm layers. So what we say now is we have this small data set which we're going
1363:20 - to pass in here and then we'll get at this point. So it's here that we're going to get
1363:26 - this outputs from our feature extractor unit. And then since generally we have for example
1363:36 - or we pre-trained this model, not the word pre-trained because we are somehow using this
1363:42 - for the first time in this course, we are pre-training or we pre-trained pre that we
1363:48 - did the training before, we pre-trained the model on a large or relatively large data
1363:55 - set like for example ImageNet. Let's suppose that we pre-trained on some large data set
1364:02 - with one billion images, then this unit year has lent to extract features from whatever
1364:11 - image you give it. And so now when you come with just a hundred images, what it does is
1364:18 - it extracts those features and then since at the level of this classifier we have a
1364:25 - different setup for the pre-training, we now have to modify this classifier. So this means
1364:31 - that if before let's suppose that let's suppose that before we had after the global pooling
1364:38 - we have say a hundred unit dense layer and then 1000 output dense layer right here. Then
1364:48 - in our case where we have just three outputs, what we'll do now is we'll simply replace
1364:55 - this here, we'll take all this off and then now we may pass this say directly to the three
1365:05 - output dense layer or we may pass this first to say 128 and then to this three output dense
1365:14 - layer. So from here we see that this new model will focus more on the classification while
1365:23 - allowing the previous pre-trained model to take care of extracting these features from
1365:32 - our data. Now it should be noted that we generally use this concept of transfer learning when
1365:41 - we have a very small data set and obviously since deep learning models perform best with
1365:47 - large data set, we want to get the best out of them and so we want to use the transfer
1365:53 - learning when we have a small data set as we've said and we have this model which has
1366:01 - been pre-trained to extract these useful features from those kinds of images. So this simply
1366:09 - means that we should have two similar data sets. Another advantage of using or working
1366:17 - with transfer learning is that you get to gain in terms of training compute cost. That
1366:25 - is this model which was pre-trained may have been trained for say three days and then now
1366:32 - all you need to do is just get this pre-trained model and then apply transfer learning on
1366:38 - your own specific and smaller task. And so when you're running on limited budget you
1366:45 - find that working with pre-trained models is going to be really helpful. Now apart from
1366:54 - transfer learning we also have fine-tuning which is quite similar in the sense that unlike
1367:02 - with the transfer learning where we have this feature extractor's weights which are fixed
1367:11 - and then during training we update the weights of this classification section with fine-tuning
1367:18 - what we could do is also update the weights of this feature extractor section. Now generally
1367:25 - we start fine-tuning from the top so we suppose that this is the bottom here so we have the
1367:30 - input and then we have this final layer so we would say that we'll start fine-tuning
1367:34 - from this final layers going to this initial layers. So yeah we have this fine-tuning process
1367:41 - we could get this first this top layers with that is we fix this layers here we keep these
1367:48 - layers fixed so their weights aren't updated during the training process and then we update
1367:54 - these weights while obviously we update this weights already but the difference is that
1367:59 - this weights were initialized from scratch that is we randomly initialize this weights
1368:06 - whereas this weights are initialized from the pre-trained model. Now this weights here
1368:15 - are from the pre-trained model but they are not trained. Now we could again depending
1368:20 - on the kind of results we're getting meaning that if we apply fine-tuning to this top layers
1368:25 - or to this final layers and we get better results you see that what we could do is we
1368:29 - could keep fine-tuning so we could keep increasing this or this section of weights we could update
1368:38 - in the feature extractor unit. So yeah we'll take this off take this off and now we have
1368:43 - this part which we can train so this is trainable this is untrainable or not trained so we have
1368:50 - something like this now it's also possible for us to go ahead and just say okay we're
1368:56 - gonna train all our model but while carrying out fine-tuning if there's one thing you need
1369:03 - to note is that you have to use a very small learning rate and the reason why you're doing
1369:10 - this is to avoid disrupting this weight values which have taken very much time to attain
1369:19 - and so as we do the fine-tuning we're gonna update this weights but very slowly and by
1369:26 - getting this by updating this weight very slowly we mean we're gonna choose a very small learning
1369:34 - rate and then observe how this affects our models performance. At this point we'll get
1369:42 - straight to the code and we'll look at some pre-trained models so here you could see you
1369:48 - get into this tensorflow applications you have called next model, dense net model, efficient
1369:54 - net model, efficient net v2 we've seen the efficient net already, inception net, mobile
1370:00 - net which we've seen mobile net v2, v3, net net and the famous res nets which we've seen
1370:07 - already with the VGGs and the exception net. So here you have the choice of speaking out
1370:15 - any one of this we're gonna go straight to the efficient net so we could have this here
1370:22 - or you could pick the efficient net v2 so you could pick any one of this so here we're
1370:28 - gonna pick the efficient net v4 this one since he has slightly fewer number of parameters
1370:35 - compared to the res net 50 and it outperforms the res net 50 by very large margin so we're
1370:43 - gonna pick this efficient net v4 and if there's one thing you can do with tensorflow is simply
1370:49 - the fact that you could use these models without having to code them out from scratch so as
1370:55 - you could see here we have this tensorflow Keras applications efficient net, efficient
1370:58 - net v4 and then we just have this argument with this we get we're gonna define our efficient
1371:04 - net model we paste this right here and we'll call this the backbone so we've seen this
1371:10 - already now here we're not going to include the top recall that as we had seen here we
1371:17 - have this old model and then let's take this fine-tuning part off we have this whole model
1371:28 - and then what we are interested is in this feature extractor unit so we'll set that include
1371:35 - top to false let's get back here we have include top we're not going to include this set this
1371:41 - to false take this off and then the weights have been pre-trained on the image net data
1371:48 - set we're not take this input tensor we have the input shape now this input shape will
1371:55 - have configuration so basically we have configuration the image size the image size there we go
1372:07 - in size by configuration and since we are not including the top as we've said here we're
1372:15 - not going to take into consideration those classes now the classifier activation so we
1372:20 - have that off and then all we could decide whether to include the pulling layer or not
1372:27 - if we want to pick the pulling layer then we'll have to specify what pulling layer want
1372:31 - to work with either the average or the max so with this we are just gonna take this off
1372:37 - and specify that later on so here we have our backbone we run the cell and then what
1372:44 - we do to freeze what we call this is freezing so what we do to freeze this backbone such
1372:51 - that the weights aren't updated during training is by simply setting this here to to false
1372:59 - or setting this trainable parameter to false so backbone the trainable equal false and
1373:07 - that's it so this all we need to do to freeze our model now we're frozen our model the next
1373:13 - thing to do is to add this other layer right here all this other layers so we'll go straight
1373:20 - away then yeah we're gonna define this input with the image size now once we have this
1373:27 - input we now pass in all we have the backbone so we have the backbone here which has been
1373:33 - defined and its parameters have been set to be frozen and then from here we have the global
1373:41 - average pulling so we have global average pulling here then we now have this dense layer
1373:48 - now the configurations will set this number of dense layer 1 to 1024 and number of dense
1373:56 - layers 2 to 128 so let's get back to this there we go we have that and then from here
1374:04 - we're gonna have batch normalization layer there we go let's copy this paste it out here
1374:14 - we have another dense layer now this time around second one that's fine and then finally
1374:20 - we have this dense layer activation softmax so here we have our softmax activation and
1374:26 - then this is number of classes here we have number of classes and that's fine so let's
1374:33 - take this off now and then run this cell right here we're getting this error because we have
1374:41 - to specify this this way that's fine that's it so now we have this model you see total
1374:48 - parameters 19 million the trainable parameters just 1.9 million and the non trainable parameters
1374:56 - are 17.6 million so it means that the backbone itself is 17.6 million and then this additional
1375:03 - parameters here come with the remaining 1.9 million parameters now with this we have our
1375:10 - model already set you see with minimal code and we could go ahead to start the training
1375:17 - then here we can now start by training our model again we compile the model and we run
1375:24 - the training process now training is over we could go ahead and evaluate our model so
1375:30 - let's run this and what do we get we have close to 85% accuracy and 95.3% top key accuracy
1375:44 - and this does slightly better than the previous models which we had worked with let's go ahead
1375:50 - and test this so changes to model and we run this here incompatible found shape this we
1376:02 - are going to resize this before passing into the model so here we have this image and just
1376:09 - here we have let's say we have our test image which will resize so we just open CV to resize
1376:17 - this image we pass in our test image and then we specify this in size so here we have in
1376:28 - size copy this and there we go let's run this again and here's what we get you see we have
1376:37 - the side output now let's go ahead and check this out here we run this here we have one
1376:49 - miss no miss no miss the second miss here we have two misses and that's it so out of
1376:58 - the 16 we have two misses that is 14 divided by 16 about 87.5% accuracy on this small batch
1377:13 - of images which we took from the validation dataset we go ahead and check out the confusion
1377:19 - metrics oops let's run this run this get back and here's our confusion metrics we get even
1377:29 - better results but one thing we have to note here is that our dataset was not that small
1377:37 - and so we may not see this change or this difference between training from scratch and
1377:45 - using transfer learning so what we'll do is we'll take this we'll take just say 10 that's
1377:52 - 320 so we'll have a dataset of 320 data points and then we'll see the difference when we
1378:00 - train from scratch and when we train with a pre-trained model so right here let's get
1378:07 - back to this we take this down and we're gonna use okay use any of this one so let's let's
1378:15 - pick the Lynette quite simple pick the Lynette and then here we have in this let's scroll
1378:25 - down we have training loss function the same metrics the same and here we have Lynette
1378:33 - model so run that and yeah Lynette model that's fine and so yeah we'll train on this small
1378:43 - part and validate on the full validation dataset we'll do this for just 20 epochs so after
1378:50 - training for 20 epochs on that very small dataset you see the model doesn't perform
1378:57 - well you see it doesn't even get up to 50% validation accuracy while the trained accuracy
1379:04 - keeps increasing so the models over feeding now from here we are now going to change this
1379:11 - so we're gonna use our model this pre-trained model so we have this here we just run this
1379:17 - again so we initialize this parameters and then we'll compile the model this is model
1379:26 - so let's let's call this pre-trained pre-trained model let's change this name here to pre-trained
1379:33 - model there we go we have pre-trained model and we'll get the pre-trained model summary
1379:45 - that's fine yeah we have that and then here we have pre-trained pre-trained model okay
1379:55 - so here we're gonna run this pre-trained model compile and then we'll start with the training
1380:00 - again so just note that we we had the accuracy validation accuracy below 50% previously and
1380:11 - now we're gonna check out our validation accuracy when working with the pre-trained model but
1380:16 - already one thing you could notice just after two epochs like here see after this two epochs
1380:24 - we are gonna see this validation accuracy which is already greater than 50 even from
1380:31 - the first epoch it was already greater than 50 it shows you the power of working with
1380:35 - pre-trained models as we are now making use of those extracted features to get this much
1380:42 - more performing model see how the accuracy keeps increasing now we're done with the training
1380:48 - you could see that this model which before we couldn't cross this 50% mark for the
1380:57 - validation accuracy now is able to cross this mark as we'll see here you see we have the
1381:04 - validation accuracy of 71% while just training on 320 data points so let's get here let's run this
1381:14 - and you could see what we have here you see it gets just above 70% so now with pre-training we
1381:23 - get above 70% and we even got greater than 50% just from the very first epoch and so what we
1381:33 - could see from here is the very first thing is get as much cleaned data as possible and if you
1381:39 - can't lay hands on this or try some data augmentation and then from here if your data set
1381:46 - is still very small then you could then apply transfer learning but if you have a relatively
1381:55 - large data set it will be needless applying transfer learning as training from scratch
1382:01 - should normally get you better results so here we could evaluate our model so you could see that
1382:09 - this is our pre pre-trained model with just 10 out of 213 batches which produces 71.3% validation
1382:23 - accuracy we now move on to fine tuning as we've looked at transfer learning and now we get into
1382:34 - fine tuning but before getting to fine tuning we would first convert this R code which was built
1382:42 - with the sequential API now into the functional API so here's our converted model it's basically
1382:51 - the same thing we have the input we have the backbone which takes in this input produces
1382:56 - output with our average pooling dense layer batch norm dense layer and this dense layer right here
1383:02 - then we have our fine tuned model right here let's run this code cell and then we could view
1383:09 - a summary which is meant to be identical to what we had already with the pre-trained model so here
1383:16 - we we have this 17.673 thousand see this is 675 it doesn't match with what we expect so let's get
1383:27 - back here and we noticed that we did not put this here so let's um x we did not include a batch
1383:35 - norm layer so let's run this again and then we now have this summary right here which is exactly
1383:42 - the same as that of our previously built model with a sequential API and then now we want to
1383:50 - fine tune our model that is all these layers which were frozen that is not trained we now want to
1383:58 - make them trainable so right here we'll get back and then we simply have backbone the trainable
1384:06 - and we set that to true then here we are going to set this trainable to false now recall when
1384:18 - we're building this resnet34 model right here we have this trainable parameter which we made use
1384:25 - of because remember here we had training sorry it's not trainable let's get back here that was
1384:33 - training so we should have training here now trainable is different from this one so take note of
1384:39 - that this is trainable and this is training so while we set this training to false and we'll get
1384:47 - back here you would find that this batch norm took in this training parameter and the reason why we
1384:54 - need this specially for the batch norm is simply because the batch norm works differently during
1385:01 - training and inference and then during training the batch norm layer normalizes its output using
1385:09 - the mean and the standard deviation of the current batch of inputs whereas during inference the batch
1385:18 - norm layer normalizes its outputs using a moving average of the mean and the standard deviation
1385:25 - of the batches it has seen during the training so since at inference or when we find tuning
1385:34 - we do not want the batch norm to take the current mean or rather the mean and the standard deviation
1385:42 - from the current batch of inputs we are instead going to compute this from what it saw during
1385:49 - training and so that's why this training parameter is very important we have this
1385:57 - training right here you see it takes the input and the training where we could set training to
1386:03 - be false for training mode and training to be rather training to be true for training mode
1386:07 - and training to be false for inference or let's say fine tuning now before we move on you should
1386:14 - note that this layer the trainable set to false is different from setting training to false
1386:22 - when you set a layer's trainable parameter to false it simply means we do not want to update
1386:28 - the weights when training but when we set training to false it means we work in an inference mode
1386:37 - in the case of the batch norm this gamma and beater are trainable parameters and so
1386:44 - when we say layer the trainable equal false it means that they are not going to be updated
1386:49 - during training but on the other hand this mean and variance aren't trainable parameters instead
1386:59 - they are parameters which adapt to the training data and that is why when we add inference mode
1387:07 - that's when we set training to false we do not want to disrupt the mean and variance values
1387:14 - gotten during the training based on the training inputs and so as we saw already this mean and
1387:25 - variance at inference mode will be simply the moving average of the mean and standard deviation
1387:30 - of the batches it has seen during training and so clearly these two concepts the that is
1387:39 - setting the weights to parameters not to be trainable and so clearly the concept of setting
1387:46 - these weights not to be trainable is different from that of working in inference mode
1387:55 - nonetheless it should be noted that in the case of the batch norm setting trainable to false
1388:01 - on the layer means that the layer will be subsequently run in inference mode now although
1388:08 - we've seen that these two aren't they don't mean actually the same thing now also note here that
1388:14 - setting trainable on a model containing other layers will recursively set the trainable value
1388:19 - of all inner layers and if the value of the trainable attribute is changed after calling compile
1388:26 - method on a model the new value doesn't take effect for this model onto compile is called again
1388:33 - okay so that's it you could check out all the dropout here we have this dropout layer
1388:43 - which doesn't have any trainable parameters but remember that the way the dropout works is that
1388:49 - you have let's say some inputs let's take this off you have some inputs and then if you to pass
1388:55 - to the dropout layer at the end you have some of these inputs which will be not considered so you
1389:03 - have maybe this will proceed but this not taken into consideration maybe this moves and then this
1389:09 - not taken into consideration so the dropout of 0.5 simply means that half of our inputs will be
1389:17 - will move forward and the other half will not be taken into consideration and so you see that
1389:23 - at inference that is when we are actually trying to test our model we do not need to drop out some
1389:32 - of this neurons right here and so generally the dropout also takes in this training parameter here
1389:42 - where when we set training to true it means that we are in training mode and so we could actually
1389:48 - drop out some of these values whereas when we set the training to false then we are in
1389:54 - inference mode and so we do nothing so we just allow the inputs to pass without any modifications
1390:03 - and you could also see clearly from here that the layer that trainable doesn't really apply
1390:10 - because dropout doesn't have trainable parameters whereas with this training we could decide
1390:20 - whether it's true or false that is what is in training mode or inference mode
1390:29 - so in fact what we're saying is we have this model here so in fact what we're saying is
1390:36 - we have this model here we have the backbone and we have the head for classification we apply
1390:44 - transfer learning by freezing all this we freeze all our backbone so no parameter here is updated
1390:53 - during training and then now we move on to fine tuning where in fine tuning we want to update
1391:00 - these parameters with a very small learning rate and then we also want to avoid a situation where
1391:07 - those mean and variance statistics which were gotten during the training process will be
1391:16 - upset during this fine tuning process and so the batch norm is kind of like a special layer where
1391:25 - even during the fine tuning where we want or where we have set the trainable to true that is we want
1391:31 - to update this weights during the training we do not want to modify or upset the batch norms
1391:43 - mean and variance and so we are going to set this training here to false
1391:50 - so it still behaves as if it were in inference mode so getting back here we have our training
1391:57 - which has been set to false and then we could start training our model again but one point to
1392:04 - note here is we do the fine tuning on a pre-trained model which has already been trained
1392:12 - so what we'll do is we're not going to start training this model this way we'll start by
1392:18 - training the pre-trained model so we start by having this backbone to be set to false
1392:23 - and this training set to false so we're going to repeat the transfer learning process again
1392:30 - before then applying fine tuning so each time you want to apply fine tuning
1392:35 - make sure you have this set to false training set to false and then you go ahead so yeah let's run
1392:42 - this again uh here's our fine tune model which achieves uh best validation accuracy of about
1392:50 - 70 percent okay now we're done with transfer learning we're now going to apply fine tuning
1392:57 - to do this we're going to set this to true so all we need to do here is set this to true
1393:02 - we're not going to rerun this again we is the same model and that's even the idea the idea is for us
1393:08 - to start with the backbone which is not trainable and then later on make it trainable
1393:15 - uh while now just simply recompile in the model so don't forget to recompile this model to take
1393:24 - into consideration the fact that some parts of the model are now trainable so let's get back here
1393:30 - recompile the model and see what this gives us now as we start training we notice that
1393:38 - this validation accuracy here isn't uh looking like what we expect because before getting to
1393:47 - the fine tuning we already had a model with a validation accuracy of about 70 but now we're
1393:53 - getting this 33 and the simple reason why this is so is because our learning rate here we still
1394:00 - maintain the same learning rate instead of reducing it before the fine tuning so we would have to stop
1394:06 - this here then we would get back to this here set this to false so we would have to start
1394:17 - back the whole process set this to false and then we retrain the model we get this accuracy of about
1394:27 - 70 now we get back here and we set this to true so we set this to true that's fine we run this cell
1394:38 - backbone now trainable and then we are going to make sure this learning rate here divided by 100
1394:47 - so we're going to make use of very small learning rate now once we have this we're going to run
1394:53 - this again so we're going to compile our model and restart the training process training are
1394:59 - completed the other results we get you could see that the validation accuracy increases up to
1395:06 - 72.2 percent so we make an extra gain of 2 percent for the validation accuracy after fine tuning our
1395:17 - model and this makes sense since fine tuning permits us to squeeze out some extra jewels
1395:25 - from this backbone since this time around it's actually trainable and that said we've just
1395:32 - completed section on transfer learning thanks for getting around to this point and see you in the
1395:38 - next section hello everyone and welcome to this new and exciting session in which we are going
1395:48 - to visualize the convolutional neural networks feature maps one very important part of building
1395:56 - robust deep learning models involves understanding how these models work or understanding what goes
1396:04 - on in the different hidden layers and so in the section we'll focus on taking a model which has
1396:11 - already been pre-trained and then generating these feature maps so we get to see exactly
1396:17 - what goes on under the hood the pre-trained model we'll be using here will be this vgg16
1396:24 - so we'll simply copy this get back to our code paste this out there we go we have our vgg16
1396:33 - we're not going to take the top so we'll take all this off now our input shape and we'll take
1396:40 - this input tensor off we're not going to include the top so we set this to false that's it then
1396:47 - this input shape will define it as our in size here so we have configuration in size that's fine
1396:58 - and we add this here okay so that's it we set this and we give it this name vgg backbone so
1397:10 - we have vgg backbone right here we can check out a summary vgg backbone summary run that
1397:22 - there we go you see it has about 14.7 million parameters we now move on to the next step
1397:33 - where we're going to create this other model which will permit us visualize this feature
1397:38 - maps now to explain how this works let's recall that we have this vgg right here so we have our
1397:45 - vgg and then what the vgg does is it takes in an input image so we have an input image and then
1397:53 - it produces a single output now if we have say not included at the top then we will have this
1398:01 - output which is 8 by 8 by 512 so here we have 8 by 8 by 512 and when it took in this 256 by 256
1398:17 - by 3 input now since we have only this one single output and we are interested in visualizing the
1398:26 - hidden layers that what go that's what goes in the vgg model what we'll do now is we'll create
1398:34 - a new model we'll create a new model right here which instead now has many different outputs
1398:43 - and these different outputs will come from these different hidden layers so we could take this one
1398:49 - and it becomes now an output this one it becomes an output this one output and so on and so forth
1398:57 - so basically this hidden layers now or this hidden or the outputs of the hidden layers
1399:04 - that's our feature maps will become our outputs so we'll now have this model with this as input
1399:12 - and then this as output so this will be now our different outputs instead of just a single output
1399:19 - now we have this different outputs there will be about 17 outputs in total now you may also decide
1399:26 - to pick the specific output so you may want to take only the conf layers the only the outputs
1399:32 - of the conf layers so you omit the max pull layers here here here and here with this one
1399:41 - but it all depends on you and we'll see how to do this so let's get back to the code well we're
1399:46 - now going to build our feature maps so we'll take the feature maps and then we'll put this in a list
1399:52 - so here we'll get the layer output and this for layer in the vgg backbone layers so we we have
1400:03 - this vgg backbone layers right here or this vgg backbone model here we're going to get all its
1400:10 - layers starting from this one we're not going to pick the input so we'll simply have this
1400:18 - and that will be it so we take these layers and then from here we'll build this new model which
1400:24 - we'll call feature map model and sakura's model from here it takes in as inputs the vgg backbone
1400:37 - so we have vgg backbone input here we have the vgg backbone input is the same input
1400:45 - but now what the difference is that the outputs is this feature maps here so we don't have this
1400:52 - just one output but all this other um hidden layer output will now become our output or
1401:00 - be part of our output so here we have feature maps so that's it we'll build this new model we'll uh
1401:08 - view the summary feature map model that summary and there we go so that's it uh it looks similar
1401:17 - to what we have but if we had picked say from model one to just model four or sorry from layer
1401:24 - one to layer four then you see it's shortened because this all we need for this our new model
1401:29 - but since we're getting right up to the end you see that we actually go through the whole vgg model
1401:35 - but the difference is that now we have outputs that we have many outputs and we have just this
1401:43 - one single input unlike before where we have one input and one output so from here we have this
1401:52 - model which we've just this new model which we've just designed start from one run that again you
1402:00 - have that and now let's head on to passing an input through this model so what we want to do
1402:08 - now is we take this input image and then we pass it into our model and now since our model outputs
1402:17 - the different feature maps the different hidden layer outputs we will now be able to visualize
1402:25 - what's going on inside our vgg model now to get this output we are going to use something similar
1402:36 - to the testing which we've seen already recall we did we carry out this testing here where we take
1402:43 - we read this image we could simply copy this where we read this image and then we pass it to
1402:48 - our model to get the output but now in our case the model let's reduce this the model we'll be
1402:54 - working with is our newly created feature map model so let's have this here and that's it we
1403:02 - have our test image we resize we pass this in this feature map model and then you'll see that
1403:11 - oh when we run this we now check out feature maps so we'll say for let's say for i in range
1403:20 - length of the feature maps we want to print out want to print out the fmap shape so we have that
1403:31 - list has no attribute shape oh okay fmaps i so let's pick out this i we run that and it's and
1403:39 - you see this so you see that the output starts from here from this one instead of the input
1403:44 - starts from it actually starts from here as we since we have decided to start from this one
1403:50 - since we decided to start from here because we don't want to include the input as part of our
1403:56 - output so that's logical we have that we start from this right up to this very last one year
1404:03 - now here we've picked the conf layers and the max pull layers so we have all this now so we can now
1404:11 - um visualize this different feature maps right here now let's note this length let's print this
1404:19 - out let's note this length is here you have 18 different outputs we get back here and we modify
1404:25 - this so we'll say that we'll only do this if is conf of layer that name is true so if this is true
1404:37 - if this is true then we are going to attach this to the outputs now oh we could also do this we
1404:45 - could just simply have it like this and that's fine now so what this means is we are only going
1404:49 - to take the conf layers as part of our output now we could define this is conf this is conf takes
1404:56 - in a layer name so basically this layer names uh what we have here does you see why it's important
1405:02 - to always give your layers some names because now you see it's helpful always it's used now
1405:09 - to differentiate between the different um types of layers so here we have this layer name and then
1405:17 - we're going to say that if this layer name if this layer name or rather if conf in the layer name
1405:29 - we return true then else we return false so we run that there we go we run this again it looks
1405:41 - similar to what we had before but one thing you'll notice now is that this length is reduced so
1405:47 - we've taken off five we've gone from 18 to 13 so we've taken off five layers which correspond to
1405:54 - this max pool layers since this conf uh this conf isn't in this name right here so that's it we have
1406:03 - that we could also decide to say okay we want to take only the max pools so we will say if pool
1406:10 - uh pool we run that and we check this legacy now only five there we go so that's it oh let's get
1406:17 - back to conf and we have that okay so now we've run this and we have the different shapes now to
1406:26 - carry out the final visualization you see you have this f maps here so we're going to go through
1406:32 - each and every feature map so for i in range the length of uh feature maps
1406:40 - we now create this figure and then we specify the fixed size so we have fixed size equal to 56 by
1406:52 - 256 now we have that we call this method right here and then from here since we're going through
1407:01 - each and every feature map is important for us to get the feature size so we want to get these
1407:07 - values for each feature map now with this we just simply have f maps we have k and then
1407:15 - all rather i here so we get in this we pick a feature map we get that and then we get the shape
1407:24 - uh one so this will permit us to get this value so this is shape zero shape one shape two shape
1407:31 - three so we get this feature size the feature map size then we now get to this number of channels
1407:39 - so we have n channels equal the feature maps i shape three three because this is zero one two
1407:50 - three so this is how we get number of channels now we have this already set we want to be able to
1407:57 - visualize this such that like here such that all these channels are lined on a single line
1408:04 - so because we have this 512 16 by 16 let's let's check this earlier ones we have like here we have
1408:13 - 64 256 by 256 images so let's suppose that this is one of them here we have this 256 256 by 256
1408:25 - right here and then we have 64 of this for the 64 different channels right here now what we want to
1408:34 - do is take this one let's take this one put it here take this other one and align it
1408:44 - take the next one align it so we could visualize this in one line up to the very last one right
1408:53 - here so to do this now what we'll do is we'll create another array which we'll call joint maps
1409:03 - joint maps uh mp1s we initialize that way and then the size here we take the feature size so
1409:13 - we have f size and then to get the this for the width this for the height actually so we
1409:20 - we know that the height in the case of 256 by 256 this height is 256 so we have this height 256
1409:29 - this distance 256 but then now the width is going to change so the width is no longer 256 by 256
1409:36 - times in this case 64 so here we have um 256 here we have f size times the number of channels
1409:47 - so that's how we do that so with that we now have this joint maps which is initialized to one so we
1409:54 - have this array now set now the next step will be to fill in this value this output this features
1410:02 - in this array now we understand how this joint maps here was created we now go ahead to fill this
1410:11 - um information all these different uh features in this giant maps array so here we'll do we'll go
1410:20 - through the different channels so for j in uh range uh n channels so basically n channels we have that
1410:32 - and then we fill in the joint maps now the way we'll do this is we'll keep the height fixed
1410:38 - so we have the height fixed and then in this width dimension we'll fill this information
1410:46 - in a way that as we go from one channel to another we are going to keep steps of 256
1410:55 - so here we will have um f size our filter size our feature uh map size is 256 f size times j
1411:04 - and then we go to f size times j plus one so what this means is would we fix the height as
1411:16 - we've said already we've we'll fix this height here the height is fixed 256 that is it here
1411:22 - so we'll take all elements in the height and then for the width
1411:27 - if when j equals zero for example we have zero up to zero plus one so the here we have zero
1411:38 - zero plus one so we have zero and then zero times f size is zero then one times f size is 256
1411:48 - so this means that in the width dimension we're going to go in the height we have 256
1411:54 - in the width we have 256 now when j goes to one now here we have one one times f size is 256 so
1412:02 - we have 256 and then one plus one is two two times 256 is 512 so now we'll skip 256 steps
1412:12 - at this point we are at this point now and we get this one and then we repeat the same process again
1412:17 - when when j equals two we have 512 and then we move to this should be 768 um no 256
1412:29 - yeah 768 so now we go from 512 to 768 and so that's how we're going to be filling up
1412:37 - these different positions right here now once we have this already set we now go ahead and
1412:44 - and pass in the data while we have to fill in here so we have f maps and then we take in i
1412:50 - if we consider this case where j equals zero that's we've picked this zone from zero to 256
1412:56 - and we've collected all the height then we have this patch right here and to fill up this patch
1413:02 - we have our feature maps which we've seen already but then we're going to take a particular feature
1413:10 - map obviously this i here picks that particular feature map and then once we pick the particular
1413:16 - feature map we can now go ahead and set this here to the values of the feature maps while selecting
1413:23 - the particular channel so now we when j equals zero for example we take the zero channel and
1413:29 - so we'll take all the values which come before and then pick out now this zero channel for example
1413:36 - now here's j so that's it we have we have our join maps which has now been created
1413:41 - and then we could take this off and we're now ready to plot our image so we have this
1413:48 - image show and then we pass and join maps now if we want to pass all this is going to be very
1413:56 - ram consuming so we just take the we select all the height and then we'll pick some values so
1414:02 - we'll go from zero to for example 512 so we'll have that and now we could run this
1414:09 - but before running this we need to set the different axis here we have axis
1414:15 - then we have plot that sub plot uh the length of the feature maps so here we'll basically
1414:23 - if we have certain feature maps then we're gonna have this um different sub plots here one and
1414:30 - then here we have i plus one so that is it so once we have this set we can now run this
1414:37 - and see what we get takes a while to run and now that's complete we could visualize the results
1414:43 - here let's simply scroll down and you see this result you see for the initial layers we have
1414:51 - this low level features which have been extracted so we have we could see this clearly here and as
1414:57 - we go down or as we go further or deeper into the network this uh the features we extracting
1415:05 - start to become more high level features so you see this see here see this one um focuses see
1415:14 - here we extract this mount here unlike before where we're more focused on um edges
1415:23 - so that's it we keep going deeper and we see the outputs or the results we get in
1415:34 - and that's it we've just visualized a trained model of feature maps now another thing we could
1415:40 - do is suppose at the beginning here that we have known right here so we don't we we don't
1415:48 - want the pre-trained weights so we run this run this again and check out on what we are gonna
1415:56 - what the model is gonna produce here is what we get you let's scroll so you get to see this
1416:05 - you see the inputs um and as we go deeper we'll notice that not much information is yet to be
1416:16 - extracted from the input
1416:23 - hello everyone and welcome to this new and exciting session in which we are gonna see how
1416:29 - to implement the grad cam technique which permits us visually explain how the deep neural networks
1416:37 - work this was first developed in this paper by Ram Persad et al and entitled grant cam a visual
1416:45 - explanations from deep networks via gradient based localization so as you could see here
1416:53 - we have this input for example let's reduce this we have this original image and there we could
1417:00 - see this grad cam output now the task here is a classification task where we're trying to
1417:05 - say whether there's a cut or darken the image and then with a grad cam for the cat we're able to
1417:12 - detect the portion of the image which influenced the model to say that there's a cat in the image
1417:19 - and also for the grad cam dog we have this part here which shows us that this portion influences
1417:27 - the model the most in detecting or seeing that there is a dog in the image now we also have the
1417:36 - grad cam for different models you see this grad cam for the vgg model and we have this resnet
1417:43 - grad cam right here so we see that two different models can produce two different grad cams though
1417:50 - generally they should be similar we said with a resnet we'll have this larger surface as compared
1417:57 - to this vgg and we could also see this for the cat now that said we are going to implement the
1418:03 - grad cam technique which generates or which permits us to generate these kinds of visualizations
1418:10 - which tell us what parts of the input influence the model in taking certain decisions as we could
1418:17 - see in this figure two from the paper given an image and a class of interest for example the tiger
1418:25 - cat or any other type of differentiable output so um like here we have this image you see the image
1418:32 - and we're giving the class tiger cat so here the task is image classification so here we have this
1418:40 - class tiger cat now um for now we'll keep out this year we'll focus only on this image classification
1418:49 - so essentially we have this image as we said already we we have the class then we pass this
1418:55 - through a cnn that's a convolutional neural network you have that right here where let's say we forward
1419:02 - propagate the image to the cnn part of the model remember um this cnn has already been trained
1419:08 - so we forward this to the cnn part of the model and then through the tax specific competitions
1419:14 - to obtain a raw score for the category so task specific year in our case our task is image
1419:20 - classification so this year is specific to um our task which is that of image classification
1419:27 - now the gradients are set to zero for all classes except the desired class then the signal is then
1419:35 - back propagated to the rectified convolutional feature maps of interest so you could see from
1419:42 - here that when we forward propagate this to our cnn we have this output feature maps here you have
1419:50 - rectified conv feature maps so these are feature maps and then we pass this through this fully
1419:57 - connected layers then we obtain the outputs from here we back propagate as we said already
1420:03 - to this rectified feature maps
1420:06 - and you can see that the resultant of this back propagation is what we have here
1420:13 - notice how this is colored compared to this so here's our feature maps here's the output from
1420:19 - back propagation from this output back up to this feature maps so we we obtain this by doing the
1420:28 - the derivative of this output let's say y let's say y tiger cut let's call it tc y tc y tiger cut
1420:36 - with respect to x feature maps let's call it fm so we're finding our we're computing this
1420:49 - derivative of the output with respect to each and every value we have here for the feature maps
1420:56 - and obviously this will give us a corresponding feature map it's just like having uh let's let's
1421:02 - pick any position let's say this random position here we take the output with respect to this
1421:07 - we take the output with respect to another position take the output with respect to another
1421:10 - position and so on and so forth and by doing this we reconstruct somehow those feature maps
1421:16 - but this time around um it's going to be the derivatives of the output with respect to each
1421:21 - and every position then from here as you could see let's take this off from here what we do is
1421:29 - we are going to obtain the mean for each and every one of those features so you see this one
1421:35 - this one that's pinkish you have this here so we obtain the mean of for this we take for this we
1421:42 - obtain this mean um this and so on and so forth so we obtain all this and so all these mean values
1421:51 - now are going to be multiplied by this feature maps so for this for example this one here we'll
1421:59 - take this and multiply um these features we'll take this multiply these features and so on and
1422:09 - so forth then once we get this we're going to combine all this or add all these features up
1422:18 - and then pass this through a relu function which then produces our grad cam and so that's how this
1422:27 - works in theory that's how we obtain this grad cam we have here let's dive into the code and
1422:33 - get this practically so first things first we have this efficient net d5 model which we had
1422:40 - trained already so we'll just um load the weights let's go ahead and load this weights so we have
1422:48 - pretty much a lot of weights and there we go from here we could test out the model so we have this
1422:54 - image here basically what we've seen already in testing so we have the test image um we expand
1423:02 - dimensions and then we have the pre-trained model which now takes in our image array so this is the
1423:09 - output right here you could see that the max is at this um index one which is logical because
1423:17 - it's a happy image we could um see that right here person is happy so there's our image and
1423:26 - here's what our model produces remember as we've seen already we have actually three classes we
1423:31 - have angry happy and sad and um this is the zero index index one and index two so i'll put this
1423:41 - index one meaning this um prediction is correct at this point if you could recall from here
1423:49 - this model was broken up into two parts so we have let's take this off we have this first part
1423:57 - which generates this feature maps and then this other parts which takes in the feature maps and
1424:03 - then outputs the specific class of this input image but given that when defining our model
1424:13 - this was in a block that is we basically define this as one not as two separate entities like this
1424:21 - we are going to look at how to separate this or better still how to create a model which is made
1424:28 - of only this first part and then the model made of this other part and then one thing you should
1424:33 - note is that the way we created this model was a bit different from what we had seen so far
1424:41 - so previously we get back here with those pre-trained models let's get back modeling
1424:47 - we have pre-trained well let's pick this one transfer learning with this you see that we have
1424:53 - a backbone defined and then we have the backbone we have global average pulling and so on and so
1425:00 - forth and then when you look at the summary if you look at the summary you have all this represented
1425:06 - as one so the backbone was uh you don't get into the details of what is contained in the backbone
1425:13 - but now when you look at this other summary let's get down to grad cam when you look at this model
1425:20 - summary now you would find that all the details of the backbone are given and this has gotten
1425:26 - differently because now what we do is the input our x which is passing to the global average pulling
1425:33 - is simply this backbone dot output so when we say when we specify that you want to get a backbone
1425:40 - output this permits us get a summary with all the details like this and then when it comes to the
1425:48 - inputs here you would say you want to have the backbone inputs so let's this input right here
1425:55 - is actually useless you need to take it you could take this off so that's it so once you specify
1426:00 - this and then you have your usual output then you would get this full summary okay so that said now
1426:06 - let's go ahead and create or make this model out of this full model we have here so in order to
1426:16 - create this last conf layer model which is essentially this model here where we would
1426:23 - get the output to be this rectified conf feature maps we have this Keras model which takes us input
1426:32 - the pre-trained model inputs so it's quite similar to what we had already here where we just take
1426:39 - the inputs to be the backbone inputs now the our inputs is the full pre-trained model inputs but
1426:45 - then the output take notice the output is this last conf layer output now what is this last conf
1426:54 - layer the last conf layer is that layer whose name is given as top activation you'll notice that
1427:04 - this is this last conf layer where we have this feature maps it's 8 by 8 by 2048 and then from
1427:11 - here we move on to the global average pooling then there's one then two and then we have our output
1427:17 - so this part from here upwards is this initial conf layer or this conv convolutional neural network
1427:26 - and then this here is this classifier unit to the right and so now that we have this name of this
1427:35 - last conf layer that's a top activation we're going to make use of that to produce our output
1427:42 - you see last conf layer is simply the pre-trained model and we get the layer whose name is last
1427:49 - conf layer name and last conf layer name is top activation so we understand where we get this
1427:54 - top activation from so now we have our last conf layer as we've said already we are now able to
1427:59 - have this last conf layer model or this initial cnn model here which has as input the image and
1428:10 - as output the feature maps so let's run this and see what we get there we go we could we could do
1428:17 - last conf layer model model and check out the summary there you go we have this model let's
1428:30 - check out the output you see here's the model output and here's the model's input see 256 by
1428:41 - 256 by three so we have the input is the image and the output the feature maps now we've built
1428:48 - this first model or we've created this first model from our overall model let's go ahead and check
1428:55 - out on our classifier model so for the classifier model you see we have this inputs the input now
1429:02 - is going to be this feature maps remember the output of this was the feature maps and the
1429:07 - inputs of this is the feature maps from here given that we have the inputs to this classifier model
1429:14 - we are going to simply pass this through each and every layer which makes up the classification part
1429:22 - of the pre-trained model so if we get back up here let's scroll back up you'll notice that we had
1429:30 - this global average pooling we had this dense we had dense one we had this dense two which all
1429:35 - make up the classifier path so let's get back here and then you see this classifier layer names is
1429:43 - given here so we understand where these names are coming from and then we simply go through for
1429:49 - every layer name in our classifier layer names so let's have this so for every layer name in our
1429:56 - classifier layer names we are going to pass the input this input classifier input into these
1430:04 - layers so we take this input pass into the global average pooling then the the output will be x we
1430:11 - take that again pass this into the dense then we take that again pass into dense one and then pass
1430:16 - into dense two and then we have the output which is in this case x so our input is a classifier
1430:21 - input which is this and our output is the output after we pass this input through these different
1430:27 - layers the count block and the classification unit designed the next step will be to compute
1430:36 - the partial derivatives of the output with respect to the features here now we could compute this
1430:44 - partial derivatives or this gradients making use of this gradient method which takes in the top
1430:52 - class channel and the last conv layer output so this year is our top class and then here we have
1431:03 - our feature maps to obtain this feature map that's this last conv layer output we simply pass the
1431:11 - inputs that's the image into our last conv layer model our last conv layer models this conv layer
1431:18 - or this conv model right here take the input pass in here obtain the feature map so here's our feature
1431:23 - maps and then from the feature maps we pass this through our classifier model so we take the
1431:29 - classifier we take the feature maps pass in the classifier model obtain the predictions here
1431:35 - turn those predictions and then get the top prediction index now the top prediction index
1431:42 - will depend on our example now given that the person is happy we expect this index value to
1431:48 - be equal one so we could we could even print this out let's print this out top prediction index
1431:57 - there we go so we would have this top prediction index and then to obtain the exact score we are
1432:04 - going to simply pick that score from the predictions so here we have the pretz obviously the pretz is
1432:12 - from this so we have the pretz and then by specifying top pred index or by simply specifying
1432:19 - one what we're saying is we want to get this score when we pass this input into this classifier model
1432:27 - and now this score we're talking about here should be what we had right here so should be this value
1432:33 - right here so we're computing the partial derivative of the output respect to the feature
1432:39 - maps and we're making use of this gradient method to help us do that so let's take this off you see
1432:45 - here we have this here there we go we run this and then we obtain this output shape you see takes
1432:51 - the exact shape of the feature maps now once we have this here does this gradients the next
1432:58 - thing we could do is simply obtain the mean values at every position right here so uh we
1433:05 - simply make use of this reduced mean while specifying the axis so let's run this and
1433:14 - print out our pulled grads shape there we go you see we have 2048 so basically this vector now
1433:25 - where each position is a mean of a single channel remember from here we had 2048 channels
1433:36 - and each and every one of them was eight by eight so you should all could say one by eight by eight
1433:41 - so we had this for each and every channel and now we've reduced each and every one of this
1433:48 - into a single value so this is just a single value this is a single value which we have here
1433:53 - which we'll see here essentially and so now we have this 2048 dimensional vector and that's what
1434:02 - we've seen here so this is essentially this now the next step will be to take this here and multiply
1434:09 - by our feature maps to carry out that multiplication we'll simply go for iron range 2048 so for each
1434:18 - and every one of these different positions we have here we are going to take the corresponding
1434:24 - last conflayer output which is essentially this feature maps so we take the feature map you see
1434:29 - we specify i so if zero then we're taking this feature map of this channel and then multiplying
1434:35 - by its corresponding pulled grads which we've seen already to be the mean of each and every
1434:42 - channel we have here for this gradients so that's how we obtain this now note that we specify that
1434:49 - one is zeroed element because this is one by eight by eight by 2048 and specifying this
1434:56 - will leave us with a tensor of shape eight by eight by 2048 so we get rid of this here
1435:01 - so that said we now run this and we obtain our last conflayer output so it's going to be
1435:10 - obviously the same shape you could see check that out last conflayer output shape
1435:18 - and there we go now to obtain the heat map we are going to sum up the values at different
1435:25 - positions for all the channels so let's suppose that we have something like this let's say for
1435:31 - example three channels then this position see we have this position we'll take this plus this plus
1435:40 - this and then we'll have this output then we move to the next position this plus this plus this and
1435:49 - then we have this and so on and so forth so that's how we're going to go from this eight by eight
1435:55 - by number of channels output or let's say 2048 output to an eight by eight heat map so this
1436:05 - what we have here we're going to sum this and specify the axis is a channel axis so we run this
1436:10 - already and then now we could visualize our heat map but notice that there's also this
1436:16 - really right here so we have the relu before we visualize the heat map there we go here's our heat
1436:24 - map which we then resize using open series resize method so you see how we get from this heat map
1436:31 - which was eight by eight to this one which is 256 by 256 and the reason why we're doing this is
1436:36 - because we want to add this with the image or purpose this on the image such that we have
1436:43 - an output which shows us clearly where or better still regions of the image where we have
1436:52 - the highest contribution to our output which is that of a happy person so the model pretty
1436:59 - that this person is happy and now we know which parts of the image contributed the most to that
1437:06 - prediction now try now with a different image and modifying this slightly so that we could
1437:14 - get this image here we find that is this part so previously was a small region now is this
1437:22 - this zone we see this wrinkles right here we have this zone and this zone which influence the
1437:31 - prediction now with this we've just implemented this grad cam method thank you for getting right
1437:36 - up to this point and see you in the next section
1437:42 - hi there and welcome to this new and exciting session in which we shall be treating or we shall
1437:48 - be using this transformer network right here to solve problems in computer vision and more
1437:56 - specifically in the task of image classification up on to this point we've seen different
1438:03 - convolutional neural networks like the lunette the vgg the resnet the mobile net the efficient net
1438:11 - and now we'll be looking at the vision transformers this vision transformers were first developed in
1438:17 - this paper entitled and images what 16 by 16 words where they build transformers for image
1438:24 - recognition at scale in this section we'll take a deep dive into how this whole architecture year
1438:32 - has been constructed and how it works and also how and why transformers perform as well as their
1438:39 - convolutional neural network counterparts the very first point you want to note here is the
1438:45 - usage of transformers for computer vision tasks has been developed in very recent times you could
1438:53 - see here from this date this paper was published the authors say here that while the transformer
1438:59 - architecture has become the de facto standard for natural language processing tasks its
1439:05 - application to computer vision remains limited in vision attention is either applied in conjunction
1439:11 - with the convolutional networks or used to replace certain components of convolutional networks
1439:18 - while keeping their overall structure in place we show that this reliance on the convolutional
1439:24 - neural networks is not necessary and a pure transformer that's without any convolutional
1439:31 - neural networks apply directly to sequences of image touches can perform very well on image
1439:37 - classification tasks they even go ahead and tell us that when pre-trained on large amount of data
1439:43 - and transferred to multiple mid-sized or small image recognition benchmarks like the image net
1439:50 - ciphar the vits that is a vision transformer attains excellent results compared to the state
1439:56 - of the art conf nets like the efficient net while requiring substantially fewer computational
1440:03 - resources to train now it's possible that you've never heard of this term transformer or maybe you
1440:10 - form an electrical engineering background and you've only heard of this when it comes to stepping up
1440:15 - and stepping down electric power now we are going to go straight away to explain terms like the
1440:21 - transformer or even this attention which was mentioned to better understand the transformer
1440:27 - and the role it has to play in this vits architecture right here we would have to get
1440:34 - back in time to understand why do we first developed in 2017 this paper entitled attention
1440:42 - is all you need was first developed by vaswaniya al and it has turned out to be one of the most
1440:49 - influential papers in the modern deep learning era with the development of this transformer
1440:56 - architecture right here at the heart of this transformer architecture we have this self
1441:02 - attention models and more specifically in this paper they used the scaled up product attention
1441:08 - that we could see here but then as we said the whole purpose of the domain in which these kinds
1441:15 - of architectures or those kinds of networks were built was for natural language processing
1441:22 - but the question is how does this work in natural language processing to understand how
1441:29 - and why the attention and also the transformers are used in natural language processing
1441:35 - we'll take the following example which is that of translation which we used to already do in
1441:42 - with google translate so here we're going to put in i love the weather or could you see the
1441:50 - weather today is amazing and we'll translate this to french le temps je d'hui and choir now
1441:57 - initially the kinds of deep learning techniques which were used in solving these kinds of problems
1442:02 - that is taking us from one language to another where the recurrent neural networks the way
1442:11 - this recurrent neural networks work is quite simple so we'll start by putting the text here
1442:17 - here we've put out our example from google translate and then we've added this extra
1442:24 - blocks right here now these blocks we've seen here are recurrent neural network blocks
1442:31 - recurrent neural networks generally reading are enhanced here are one of the first deep
1442:37 - learning based models used in natural language processing tasks like the case of translation
1442:44 - we have here now the way this works is we have our initial text or we have our source that's
1442:53 - english text the weather today is amazing and then we have the target which we want to generate
1442:58 - so initially we have this input and this output which we're going to train and later on when we
1443:05 - pass in some random input we expect to get a reasonable output now the way we have this or
1443:13 - the the way this is structured is such that each and every one of this is called a token
1443:20 - so we have this word here which is a token this this is our first token next token this token
1443:26 - this token and this other token then these different tokens have been converted into vectors
1443:34 - and then been passed in this rnn blocks right here now we carry out some simple computations
1443:41 - like multiplication and addition and then some information is been passed in from one block to
1443:48 - another hands the term the recurrent neural network now the importance of passing information
1443:55 - from one block to another is that this token's computations in this block will depend on this
1444:03 - other previous tokens that you could see here so it depends on this depends on this and also
1444:08 - depends on this other one and then once we're done with convert or passing this information from
1444:14 - one block to another up to this we are then going to take this here we're going to have
1444:19 - some information we're going to be passed onto this other rnn block here so this is our encoder
1444:26 - block so here we encode the information and then yeah we decode this information so here we have
1444:34 - the encoder and the decoder and then again here a similar process is repeated where we have
1444:41 - these computations which produce an output here and then we could take this output and feed it
1444:47 - in this one to produce this other output and so on and so forth up to this final output but then
1444:54 - the problem with this technique or with this method is that first of all if we have a very long text
1445:02 - then it may happen that it has becoming difficult for information to flow from this first blocks
1445:09 - here to this final blocks and given that even as humans we know the importance of taking into
1445:16 - consideration some previous context when trying to carry out attacks like for example translation
1445:23 - this kind of problem will lead to very poor results now another problem here is each time
1445:33 - we're training we have to pass this information for one block to another sequentially so here
1445:41 - we pass all this information sequentially and because this information is passed sequentially
1445:46 - it makes it difficult for us to implement parallelization very efficiently and so this
1445:53 - makes the training of these kinds of neural networks very difficult now to tackle the
1445:59 - issue with long-term dependencies attention networks were developed so right here instead
1446:05 - of depending on just this final vector we get or this final output we get from this hidden layer
1446:13 - here which has been passed on here to relay this information from the source to the target language
1446:21 - what we'll do is for each and every unit we have here each and every recurrent neural network block
1446:28 - we have here we are going to take into consideration inputs from each and every block here so this
1446:35 - inputs will be taken into consideration so each and every block now you see all of this is passed
1446:43 - and then we have this attention layer right here which then processes these inputs from this
1446:52 - different source RNN blocks such that the layer that all this attention layer produces an output
1447:01 - vector which is now passed as input into this RNN block and so when we have this source
1447:10 - and this target we pass in the source then we get all we combine those inputs from each and
1447:20 - every RNN block right here pass in this as input into this RNN block get an output in this case
1447:29 - is L then we take this output and pass it as an input in here but again once we shift and go and
1447:39 - get to this time where or this time frame where we want to get this second output what we'll do is
1447:46 - we'll have another tension here which again takes in all these different inputs so we take again
1447:55 - all these different inputs here and here and here and here then carry out some computations based on
1448:07 - the type of attention we are implementing and then from here we get an output which is passed
1448:12 - together with this right here so from here we get this output and then we repeat the same step
1448:18 - that's passing in the tone that is taking this output passing in here and then also taking in
1448:24 - this inputs from these different RNN blocks and so as you can see here for each and every block
1448:33 - we have here it pays attention to each and every input and for this or from this we could even come
1448:43 - up with an attention map where we would have this text this text in English the weather today is
1448:52 - amazing and then this other side we have the tone or the inquiry app so now after training this kind
1449:00 - of model we can be able to see how much attention this pays to each and every input here and
1449:11 - then it's biological that this law will pay the most attention to the and then
1449:20 - actually here is weather so this will pay more attention to almost attention to weather
1449:26 - then we pay most attention to today we pay most attention to east and inquiry app will pay most
1449:35 - attention to amazing and if we get to this paper entitled neural machine translation by
1449:42 - jointly learning to align and translate that is a famous Badenau et al paper you can see some of
1449:50 - this attention maps here let's have this you see some of these attention maps the you see
1449:57 - the economic European then the agreement on the European economic area was signed in august
1450:11 - 1992 so you see this attention maps here where we see clearly which words attend most to one another
1450:22 - so here we have this image which shows exactly what we're describing previously so here you
1450:27 - have this inputs and then to get this output y of t you will find that we are going to take in
1450:34 - or we're going to attend to each and every input here and then pass this year to obtain our yt
1450:42 - now at this point we are going to move on from the attention to self-attention and to better
1450:49 - explain the self-attention we'll consider a whole different problem which is that of sentiment
1450:54 - analysis so here we want to we have this model we could now take this off we don't
1451:02 - make use of this although you should note that we still use self-attention in the translation
1451:09 - problems but it will be easier to grasp this concept in the context of sentiment analysis
1451:14 - so here what we are having is we we have the weather today is amazing i want to be able to
1451:19 - see whether this is a positive or negative statement so now we have this model which takes
1451:28 - in inputs like this and then let's draw this model here like this we have this model and then
1451:36 - outputs or tells us whether the statement we've made is a positive or a negative statement
1451:45 - now here for this self-attention layer we are not going to need this recurrent
1451:56 - neural network hidden states anymore in fact what we could do is we could take all this off actually
1452:02 - because basically we're having this self-attention model which we'll see in a minute how it works
1452:08 - and then what we're passing in here is some vectors so we have this vector we have this
1452:17 - other vector we have this vector this one and finally this one now if we combine all this
1452:26 - we'll find out we have a sequence length so we have one two three four five suppose our sequence
1452:31 - length is five so we have a sequence length by let's say embedding dimension
1452:41 - metrics which we which we get from here now let me explain let's suppose that the sequence
1452:48 - length is five as we've all seen and then the embedding dimension is let's say three
1452:55 - so we have this five by three metrics which we are going to pass into this self-attention
1453:02 - layer right here now this embeddings all these vectors which we pass into the self-attention
1453:10 - unit are going to be designed in a way that words which look alike are going to be close to each
1453:17 - other while words which are opposites are going to be far away from each other now let's since
1453:25 - we're working in three dimensions it means we'll have one two three values here one two three
1453:35 - and then finally here one two three so let's let's do something like this three dimensions
1453:41 - what we'll have is the word happy which in this case can be represented by this vector or this
1453:51 - embedding will be or can be plotted out like this and this will be close to a word like smile
1454:01 - while a word like sad a word like sad will be far away from this uh two words because
1454:08 - they are actually opposites to each other so we have sad and we could have angry right here
1454:16 - now for this one year or for this text here we could pick out these two words which are
1454:22 - most likely to be very close to each other we could have the right here and we have is
1454:28 - somewhere around here and so now getting back to this model we have this five by three input which
1454:37 - is passed into our self-attention layer so we could let's let's have this matrix here five by
1454:44 - three would have the the word there yeah we'll have its own embedding so we will have some value
1454:51 - some value some value suppose that we're working in three-dimensional embedding and then whether
1454:57 - we'll have its own value its own value its own value today its own value this value this value
1455:05 - could you could take say two point three one zero point five negative five one whatever value one
1455:11 - year and then you have this and you have this and you have this this is four already and then
1455:17 - amazing will have its own so you see that each and every one of this year has its own embedding so
1455:24 - this is these are the different words we have here then at this point we'll implement a special type
1455:29 - of attention known as a dot product attention where we'll take this here and multiply it by
1455:37 - the transpose of a matrix which has the same shape at this matrix here so we'll take this
1455:44 - we'll call this the query and then we'll multiply this by the transpose of the key
1455:50 - now this key is going to be three by five since it's going to have the same shape as this query
1455:56 - now this is our query we'll call this a query and so here now we have this three by five matrix
1456:04 - and then this product will give us a five by five matrix now after this after getting this five
1456:12 - by five matrix we could pass this to a softmax layer now we've looked at the softmax layer in
1456:18 - previous sessions but one thing you should note here is once we have this five by five matrix
1456:24 - it produces this attention map similar to what we have seen before where we have this the weather
1456:32 - today is amazing to the side and the same again to the side and then words which are most similar
1456:39 - to each other in a certain context are going to have the highest values and so if we're in the
1456:45 - case where you have uh say let's replace this weather by happy and then we have uh amazing let's
1456:54 - uh no let's let's live amazing so if we have the happy today's amazing uh it sure doesn't make
1457:00 - sense but let's consider this let's suppose that we have the happy today's amazing then this uh
1457:06 - second row of foot column because amazing will be around here so we'll have this value
1457:14 - which is going to be relatively higher than all the other surrounding values and this will be
1457:19 - because after training the model the attention map values would have been modified such that
1457:29 - values or rather words which are similar to one another take higher values while words which are
1457:36 - not similar to one another take very small values now from here we have this uh five by five matrix
1457:45 - which now when multiplied by another five by three matrix will give us a five by three matrix
1457:55 - generally we call this matrix which is multiplied by this attention matrix the value so we have
1458:01 - query we have the key and we have the value of this you see that we have this input which got
1458:08 - in here which was five by three and now we have a five by three output then this year now we pass
1458:17 - through some fully connected layers and then we'll have an output or a fully connected layer
1458:23 - with one neuron in this output which will tell us whether an input statement is a positive statement
1458:30 - or a negative statement and so as you've seen we've gotten rid completely of the recurrent
1458:36 - neural network blocks as now we're just making use of this self-attention blocks to extract
1458:44 - information from our inputs now one of the first papers if not the first paper which made use of
1458:53 - just the attention and getting rid of the RNNs was this attention is all you need paper and it
1459:00 - happens to be one of the most influential papers in modern day deep learning so here in this
1459:06 - attention is all you need the paper or the transformer paper to present this new network
1459:11 - which you could see just right here and then a single block let's take this off a single block
1459:21 - which makes up the transformer model is this multi-head attention so as you could see right here
1459:28 - we have this single block and then here we have this multi-head attention so let's look at this
1459:37 - multi-head attention this actually the multi-head attention here so you have this year which is
1459:42 - this whole block and then in this multi-head attention you have the skill dot product attention
1459:51 - which is this self-attention we just talked about you see we have the query the key and the value
1459:58 - so since it's self-attention you'll notice here that we have this input and all these come from
1460:05 - the same input so we have this input which is split it up into cure k and v query key and value
1460:15 - now this resembles or is analogous to data management systems where data is stored in key
1460:24 - value pairs just like say python dictionary so you have data in this key value pairs data start
1460:33 - this way and then when you want a particular information you have to pass in a query now when
1460:39 - you pass in a query let's change this color when you pass in a query you have a particular key
1460:46 - which is selected once the key is selected we now obtain the value which is the data itself
1460:52 - and is kind of similar to what we have in here and then from here in this a level of the split
1460:59 - note that before the information has been passed into this skill dot product attention
1461:07 - we actually pass this cure k and v into some different linear layers and so this means that
1461:13 - even though we have the same input they'll end up being projected into three different inputs
1461:21 - and so now we have this cure k v we are going to carry out cure k transpose here we have
1461:28 - the matmul as we saw already cure times k transpose and then we have this scalene
1461:36 - which you can see right here in this formula this attention formula we have cure k t divided by
1461:42 - this d k then from here we have softmax of all this and then we multiply by v so let's get back
1461:50 - up to this here that's fine now that we have this output you now see that we have this multi head so
1462:01 - we we got this we have the softmax we have the matmul where we take this softmax of this
1462:07 - multiply by v so that's how we get this recall how we saw that with a example we had previously
1462:14 - and then we have this multi head attention now this multi head attention here simply means you
1462:21 - take this year as you pass in your information like this you get this cure k and v and then
1462:30 - you again pass the same information into this block so let's suppose that this is our skill
1462:36 - dot product attention block this is skill dot product attention which is right inside here
1462:42 - and so once we have this let's let's make that smaller let's suppose that this is what we have
1462:47 - here so this year is actually this now to obtain a multi head attention we would have this other one
1462:56 - year and then we'll have let's suppose that we have three heads if we have three heads
1463:01 - then we would have three of this stacked in this way you have one two let's change the color so
1463:06 - it becomes clear we have this one in red we have this next one in blue here and then we have this
1463:15 - other one in green so there we go we have this three and then when the information gets in so
1463:21 - you have your cure you have your k and you have your v we pass this to uh this separate linear
1463:29 - layers see for for each of this we have some linear layer here all of this came from the same
1463:37 - inputs as you could see here and then now this information is passed here so we have cure k v
1463:43 - passed into this one into this block here and then this same cure k v also is passed into let's
1463:50 - change the color we will now have some other here some other linear layers let's put it besides this
1463:58 - we have some other linear layers here we'll pass v we will pass k we'll pass cure right here
1464:06 - and then this now will be set into this self-attention block right here then we'll also
1464:12 - finally have this for the red so we'll have something like this red we have the k something
1464:18 - like this we have the v something like this so now this cure k v is passed now into this red here
1464:25 - and that's it and then the outputs here the outputs will get at the end of this three
1464:30 - self-attention blocks will now be concatenated and then pass through a linear layer so this
1464:38 - linear layers is like our dense layer in tensor flow now once we have this you see we have our
1464:43 - multi-head attention which is this block and then now we'll take this input add it onto the output
1464:49 - and then go through a layer normalization then from here we pass this through a feed forward
1464:55 - network that's like our fully connected network or dense layer and then we'll again repeat this
1465:01 - addition and normalization a little similar to what we have with the resnets now once we have
1465:06 - this now we can then repeat this n times now you notice that this is similar to this except for
1465:15 - the fact that now we have this two multi-head attentions and we also have this maxed anyway
1465:21 - we're not going to get into all those details what's important is for you to understand how this
1465:25 - encoder here works and now we understand how this works we will now get back to our V paper that is
1465:32 - this paper entitled transformers for image recognition at scale and now you should be able
1465:38 - to understand this transformer block which we presented earlier in this paper and now
1465:45 - with this understanding of how this transformer encoder works let's now get into this
1465:52 - unit here where we break this image into these different patches as you could see right here
1465:59 - to better understand how and why we make use of patches right here let's not forget that
1466:06 - what this transformer encoder takes in is some input sequence so we have this input here
1466:13 - uh initially we had words where each word like this could be represented by this vector or this
1466:21 - embedding vector and then this now combined is passed into the transformer here since our
1466:29 - input is this image in others for us to represent it this way we'll have to break this up so what
1466:37 - we could do or what we could think of at first sight is we have this image let's suppose the
1466:43 - image is 256 by 256 by say three channels then we could take each and every pixel here so let's
1466:55 - let's omit the channel for the channels for now so what we could have here is for each and every
1467:01 - pixel in this 256 by 256 image we would have a vector representing that pixel and then this
1467:08 - other one is vector this other one is vector and so on and so forth but don't forget that
1467:14 - unlike previously where we had only five words now we have 256 times 256 words because if we
1467:22 - have an image like this and we have to get each and every pixel then we'll have 256 by 256
1467:27 - which is more than 65 000 different vectors which we'll have to pass here and so before
1467:40 - we're in our attention model we had an attention map which was five by five recall we saw that
1467:47 - already with words we had a five or an input sentence with five words we had five by five
1467:54 - attention map now we would have a 65 000 by 65 000 attention map you see that working with
1468:05 - these kinds of matrices and memory isn't very feasible and so instead of going pixel by pixel
1468:12 - the authors decide to work patch by patch let's create this again so you get to see that take
1468:20 - this off you see here we go patch by patch so you could see how this image instead of taking
1468:29 - each pixel we bring this up into patches so this is now like a pixel and then you see this patch
1468:35 - you see this patch this patch this other patch this patch and so on and so forth up to this patch
1468:42 - right here now this is what is like the word now here so with images we have to break this up like
1468:50 - this and the authors choose to work with 16 by 16 pixel patches so each patch here is 16 by 16
1468:59 - and so given that we have 16 by 16 if we have this patch for example then we would have 256
1469:07 - different pixels for each patch here we have 256 here we have 256 and so on and so forth
1469:14 - so unlike with the images or rather unlike with the words where we had five by three so we had
1469:21 - five words and each word was represented by the three-dimensional vector here each patch is
1469:26 - represented by the 256 dimensional vector now this doesn't mean that in nlp we generally work
1469:33 - with this this was just done to make it easier for you to understand so getting back to computer
1469:40 - vision you see that we have this 256 256 256 and so on and so forth now when working with the
1469:48 - transformer we may not want to work with this 256 dimensional vectors maybe we want to work with say
1469:59 - 512 dimensional vectors in that case we would have to do this linear projection of the flattened
1470:06 - patches such that we leave from this um say let's suppose that we have one two three we have nine
1470:13 - patches so the sequence line is nine so we have this input which is nine by 256 and then after
1470:19 - going through this linear projection we now get to nine by 512 and this will be the embedding
1470:26 - dimension for our transformer in the previous example our embedding dimension was three
1470:31 - three so if this this permits us to be to to work flexibly as now we could decide on
1470:38 - what size we want for one embedding dimension now that said we have this output you see nine by
1470:44 - 512 and then we're ready to pass this into the transformer encoder but just before passing this
1470:50 - we would add this position embeddings you see there we have this input you see in this different
1470:57 - this color you have them getting in and then we have this position embeddings here is notice zero
1471:03 - one two three and up to nine now the way this works or or let's start by first the reason why
1471:09 - we even have to do this is because unlike with the conf nets where where the convolutional or
1471:16 - the way the convolutional neural networks work is that for computing the feature maps it takes
1471:22 - into consideration locality so this means that you see these two portions here um when passed with a
1471:31 - conf filter will produce a certain output and so this means that
1471:38 - exhaust which belong to a certain or to a small locality like this one will be used to produce
1471:46 - the output and this clearly gives an gives the CNNs an upper hand over the transformers as
1471:56 - when trying to understand an image the positions of particular pixels actually matter so this means
1472:04 - that CNNs already have an inductive bias due to the way they actually work and so to give a helping
1472:14 - hand to the transformer network will now need or will need this position embedding which gives
1472:22 - this transformer encoder an idea of the location of each and every patch which is passed in
1472:31 - but again it should be noted that this will have to be learned automatically by the model
1472:37 - now if you notice we have this extra input right here and the reason why we have this extra input
1472:44 - is simply because we do not want the situation where after going through this encoder or this
1472:50 - transformer encoder right here we pick one of this outputs because we would have outputs here
1472:58 - we don't want we don't want to pick one of these outputs to be used for the MLP head or to be used
1473:04 - for this fully connected network in this classification unit right here so to avoid
1473:13 - this sort of bias where we would be picking one of this the authors add this extra learnable
1473:22 - class embedding right here which will be or whose output will be passed into this MLP head
1473:30 - and then will be used for classification another important point to note here is the
1473:37 - transformer encoder or this visual vision transformers are some sort of hybrid architecture
1473:45 - because we may decide not to pass in those image patches directly but instead pass those image
1473:52 - patches through a convolutional neural network then get the output embeddings and pass in your
1473:59 - directly instead of this image patches it should be noted that the multi-layer perceptron contains
1474:06 - two fully connected layers with a julu non-linearity here's the general julu non-linearity
1474:14 - compared to the relu and the elu so you see we have this relu where all values less than zero
1474:20 - all values less than zero gives output of zero and all values greater than zero give the exact same
1474:27 - value but with the julu we have this curved function right here so that's it the type of
1474:37 - normalization is the layer normalization as we mentioned already and the layer normalization
1474:43 - here we could visualize this in this paper by Sheng et al entitled power norm or thinking
1474:50 - batch normalization in transformers where you see we let's zoom this you see we have layer normalization
1474:58 - here and we have the batch normalization put side by side so over the layer normalization as we were
1475:04 - saying if you consider some inputs let's let's here we have the sequence length or the sequence
1475:10 - dimension we have the features or the embeddings or like a vector actually so we have the different
1475:15 - vectors and then we have the batch dimension so basically what we're saying is we have in this
1475:21 - sequence length or we have these different vectors here which have been passed into
1475:29 - some layer and then instead of doing or carrying out normalization for for throughout the batches
1475:37 - as is in the case of the batch norm here we're carrying out this normalization for each and every
1475:46 - vector and the reason why we do not use the batch norm with the transformers is the fact that the
1475:53 - batch statistics for nlp data have a very large variance throughout training and this variance
1476:03 - exists in the corresponding ingredients as well and so to avoid this kind of situation
1476:10 - it's preferable for us to carry out this normalization on the features instead before
1476:17 - we move on to the experiments let's look at how the vids are being used in real world
1476:23 - so actually the vids are pre-trained on very large these sets and fine-tuned to smaller
1476:31 - downstream tasks obviously when fine-tuning we remove this head and replace with a head which
1476:40 - now correspond to our number of classes so this means that initially we may have a thousand class
1476:45 - head and then we move this to k classes or let's say three class head
1476:51 - to better understand why we're going from while we have a d by k output let's get back here so
1477:03 - after those inputs have been passed in here we have an output sequence length plus one
1477:09 - this plus is one year or let's just say we have here we have say from here a one by d output if
1477:19 - if we're considering all the sequence length will be a sequence length by d output this deer is our
1477:27 - embedding dimension which we had fixed from this linear projection right here so we have this
1477:35 - one by d and then we pass this through obviously it becomes it becomes like simply
1477:41 - uh d neurons so we have one two we now have d neurons since it's just one by d and then we have
1477:52 - this output let's say we have a thousand classes then we'll have this fully connected layer which
1477:59 - brings all this year this d inputs to this k outputs or in this case to this 1000 outputs
1478:09 - now where we want to fine tune we're going to take this off take this off and replace this
1478:15 - now with k outputs so we now have k outputs right here and then we initialize this weight
1478:24 - of this fully connected layer the others also make mention of the fact that during fine tuning
1478:31 - is better to work at higher resolutions so this means that the model could be trained at 256
1478:38 - by 256 and then later on fine tuned with 512 by 512 images
1478:50 - and then since they keep the patch size the same this results in a larger effective sequence length
1478:56 - now let's explain or let's visualize this statement so here we have this input
1479:03 - which is say 48 by 48 let's say we have your 48 by 48 and when we divide this our break this up into
1479:13 - three parts we have 16 16 16 16 16 16 so we have 16 by 16 patches now if we want to fine tune
1479:22 - on a higher resolution image then uh let's say the higher resolution image is say 96 by 96 so
1479:31 - could have something like this so if now we're fine tune on the 96 by 96 image and that we still
1479:39 - maintain the fact that this year or the patches will be 16 by 16 then this means that instead of
1479:45 - three year we're going to have six so we now have or one two three four five and six patches
1479:55 - six patches this way two three four five six and so on and so forth so now we're going to have
1480:05 - 36 different patches instead of nine patches as we have here and that's why they make me have
1480:11 - the fact that the sequence length is going to be increased and that's so long as they can fit in
1480:18 - the memory now due to this modifications the pre-trained that's what we had before the pre-trained
1480:26 - position embeddings may no longer be meaningful so they therefore perform 2d interpolation of the
1480:33 - pre-trained position embeddings according to the allocation in the original image the experiments
1480:40 - here we we could see those different models they have the vid base the vid large and the vid huge
1480:45 - number of parameters 86 million to 632 million then here we have 12 layers recall let's get
1480:53 - back here recall we have this number of layers here so basically you're repeating this you're
1481:00 - repeating this year 12 times so we'll get back so there we have 12 layers for the vid base as
1481:09 - we stated here and then we have 24 for vid large and 32 for vid huge then this hidden size this d
1481:18 - this embedding dimension is 768 for base 1024 for large and 1284 huge the mlp size that's
1481:29 - uh fully connected layers uh to 3072 4096 here 5120 now the number of heads remember the attention
1481:41 - heads 12 16 16 the experiments were carried out on this gft 300 median data set i will see how this
1481:50 - 14 by 14 patch version of the vid outperforms this resnet 152 now this uh performance although not
1482:02 - largely uh greater than that of the resnets requires less computation resources to train
1482:10 - as we see here we have 2500 tpu core days required to train this model as compared to
1482:19 - this one which requires 9900 tpu core days also from the plots you see that when you increase the
1482:29 - number of pre-training samples the model which performs the best is this vid right here see here
1482:38 - we have this vid and this outperforms the resnets whereas uh for a reduced number of samples the
1482:45 - resnets outperforms the vids while here the smaller the patch size like here we have this
1482:53 - fortune fortune by fortune patch size we have the better the results now in order to understand
1483:01 - the reason why as you increase this uh data set size the vids start to outperform the
1483:09 - conf nets we have to recall that when working with confidence like the resnet there is some
1483:16 - inductive bias in the sense that the fact that this resnet takes as input this two-dimensional
1483:25 - image already gives this conf net a helping hand when it comes to extracting features from here
1483:34 - and so even with relatively smaller data sets these confidence can make sense out of this
1483:44 - input image now with the transformers which are some sort of
1483:51 - generic neural network the model doesn't get to see the image in this its natural form what it
1484:00 - sees is some patches which have been converted to some vectors and so at the very beginning or
1484:08 - with small data the transformer model
1484:14 - finds it difficult to make much sense out of these patches but as soon as we increase this
1484:22 - data set to considerable amounts this transformer model now free of the inductive bias can even do
1484:33 - better than the confidence and interesting enough you'll notice that after training a transformer
1484:42 - model this position embeddings we call the position embeddings which are added onto the
1484:48 - patch embeddings before passing to the transformer actually learn on their own to
1484:56 - encode the position of the patches you can see from this uh plot here where we have the input
1485:03 - patch row and the input patch column you see that this one one you see the position this is gotten
1485:11 - by the model or this is learned automatically by the model during the training process
1485:17 - you see two one it goes a step in the direction and maintains this direction or maintains a row
1485:24 - and you see three one maintains a row that goes three steps you see this you see you see that
1485:32 - and then finally here you go you go several steps to the right and then seven steps uh
1485:37 - downward then here to the left you could look at this you see this um this embedding filters
1485:46 - right here we have this embedding filters which we see here which look much alike to the the
1485:53 - conf net filters then to the right you have this plot which summarizes the reason why the vids end
1486:03 - up being more powerful than the confidence to understand this let's take this here who consider
1486:12 - a conf net with a given depth now with a conf net the initial layers let's let's let's have this
1486:18 - conf net and we break this up so we break this up we have our initial layers and then we have our
1486:24 - final layers this initial layers permit us um extract low level features while the final layers
1486:31 - permit us extract high level features and so if we have an image like this like this one and we
1486:37 - have this head and we have this then given that we're passing these filters or this conf net
1486:47 - filters here you see that this pixel for example attends to this other pixels which are found
1486:56 - around its locality and then as we go deeper in the network we would have this pixel here
1487:04 - which now tries to attend to this other pixel here which is much more far away from it
1487:12 - to better picture this remember the example we took for three or rather two three by three filters
1487:19 - compared to a single five by five filter let's let's let's draw this here we compared this with
1487:27 - a single five by five filter and we saw that although this five by five filter had a larger
1487:35 - receptive field compared to a single three by three filter which we have here making or stacking
1487:44 - up this three by three filters that is making our network deeper permitted us to still be able to
1487:51 - capture this part of the image and so this shows us that with the confidence in the earlier layers
1488:02 - when the when is not yet deep enough we still capturing this local information and then as we
1488:11 - go deeper we start capturing much more global information and so if we're to have this kind
1488:19 - of plot here where this we have mean attention distance and here we have the network that
1488:27 - would see that for a conf net will keep increasing this up to a point where we may we will no longer
1488:34 - be able to continue increasing because as this this network depth or this will increase the
1488:41 - number of layers we are able to capture much more global features and so this mean attention
1488:51 - distance keeps increasing but with the attention or with the transformers since each patch attends
1489:01 - to each and every other patch as we have seen already with the self-attention each and every
1489:08 - patch will attend to the other right from the very first attention layer we are not going to
1489:16 - have this but instead this plot we have here and so this means that if we train our VIT with a
1489:25 - very large data set right from the very first layers we are able to capture both the local
1489:32 - and the global features and this is what makes the VITs more powerful compared to the confidence
1489:45 - when we work with big data here we can also visualize what the model sees by looking at
1489:52 - this attention maps you will notice that after training the model you see we have this attention
1489:58 - here see here these are pixels which pay much attention to one another see here
1490:05 - these pixels are paying attention or much more attention to one another as compared to the other
1490:11 - pixels in summary to understand or to visualize what goes on when training to CNN and VIT model
1490:21 - side by side you'll see here that with the VITs as you increase this data site this increase this
1490:27 - site and this is increased data size here so as we increase the the data size or rather when we
1490:34 - start with small data sizes we have this kind of accuracy while for the CNNs we already have
1490:41 - reasonable accuracies even with small data size and then as we keep increasing this data size
1490:48 - as we keep increasing this data size you see this accuracy keeps increasing while for the CNNs
1490:55 - VITs start to plateau at some point and this plateauing is simply comes due to the fact that
1491:03 - this CNNs here are limited by the inductive biases whereas these transformers which are
1491:11 - more generic neural networks are free to learn even better from these larger data sets
1491:25 - hello everyone and welcome to this new and exciting session in which we shall be building
1491:31 - our own VIT model from scratch in the previous section we saw how this transformer model which
1491:39 - previously was used for NLP tasks could be used in computer vision with proper preparations
1491:49 - and so in the section we'll see how to convert or create patches of this image and then
1491:57 - carry out those linear projections and pass this output from those linear projections into the
1492:02 - transformer encoder to then create or to then train an end-to-end model which learns how to see
1492:10 - whether an input that image is that of an angry person a happy person or sad given that we've
1492:16 - been working with 256 by 256 images if we have to split up those images into 16 by 16 images
1492:24 - then we would have 256 different images because here if you have 256 year and then uh break this
1492:35 - up year into 16 by 16 so here you have 16 pixels by 16 pixels then you would have to go you have
1492:43 - to do this 16 times horizontally and 16 times vertically to form a 256 by 256 image and so
1492:52 - um from here you would have 256 different patches like this one and so what we'll use to create
1493:01 - these patches will be this extract patches method right here now this extract patches takes in the
1493:08 - image it takes in the sizes takes in the stripes and the rates with also the pattern so let's look
1493:16 - at how this works from here we have this picture here or we have this output which will help us
1493:24 - picture how this works but before looking at that let's look at its arguments we have the sizes or
1493:31 - we have the images for the tensor we have this sizes which specifies the patch sizes so in our
1493:41 - case we have 16 by 16 patches so our patch size will be 16 we also have the stripes which tells us
1493:48 - how much we should shift while creating these patches and then we have the rates which will
1493:53 - better understand with some figure so let's let's now get back to this year and then we see that
1494:02 - we've been we here we pass in the sizes of one three three one so this list we pass it in
1494:09 - here we got this from here so you see we told it must be this list where we have one the size row
1494:15 - the size column and one but since the size row equals size column equals 16 then here we would
1494:21 - have 16 by 16 now in the case where we have three three it you you could see here let's let's have
1494:28 - this here you could see here let's take this off you could see here that we have this three by three
1494:34 - so here unlike with our example where we're working with 16 by 16 pixels here they have three by three
1494:40 - pixels and then the next one you have the stripes five by five now you will notice that
1494:50 - let's scroll this way you will notice that when this box or when we want to get the next patch
1494:58 - we shift or we go five steps to the right and then get this next position or this next patch
1495:06 - right here so you can see after shifting we get to this so originally we had this image here and
1495:12 - then we're trying to get these different patches now the patches are what we have in stars we can
1495:17 - see and then again we shift five steps downward we go downward again and then we get to this
1495:23 - you will see that if you can count this one two three four five steps and then we get to this
1495:30 - and then we shift again and so on and so forth so that's how we obtain all this
1495:35 - three this four patches right here now the next thing we will look at will be this
1495:43 - rates now the rates again you have this one and this one and then you specify this two rate values
1495:52 - here now we'll look at or we understand these two rate values by looking at a dilation or by
1495:59 - looking at a dilated convolution operation we can see this visualization from this medium posed by
1496:06 - Sig Ho Tsang where unlike with the usual convolutions where when we have in this
1496:11 - filter which passes through the image you see we have this three by three filter which is compact
1496:16 - here so there's no spacing between each element of the filter but with a dilated convolution
1496:23 - you see the spacing here so it's kind of similar to what we will have here as let's get back to
1496:30 - documentation as with this if we decide to have patches where there is some spacing between the
1496:42 - pixels then we could specify how much paste we want with this rates right here now the pattern
1496:48 - here is set to valid and so this means that if some pattern needs to be done to match our
1496:54 - with the extraction then it will be done automatically now that said we could copy this
1497:01 - out copy this simply and then we paste it out right here so here we have this and then we'll
1497:09 - call this patches so we have this patches and then we'll pass in our test image right here
1497:17 - there we go we have test image that's fine and then here we have 16 by 16 now this is the patch
1497:25 - size so we could do well to put that in the configuration so we could we could add that to
1497:31 - the configuration and the strident 2 is 16 because we want to let's get back here the strident let's
1497:38 - see why we need the strident to be 16 now you see that we're interested if we have this kind of
1497:43 - image we're interested in we are not interested in skipping this part so we don't want to skip
1497:47 - any parts we want to have this then we want to have this next you see this what we want to do
1497:54 - then we want to have this next and this next obviously there'll be padding year and year so
1498:02 - account for this space here and then we want to have this next and so on and so forth so because
1498:07 - we want to have this we'll make this value to be the same as this such that when we have this
1498:13 - three by three patch we skip three steps and move to this next and so on and so forth so that said
1498:19 - if you want to have this kind of compact patch extraction then you want to have this this the
1498:26 - value here for the stride the same as the size so with that now let's configuration patch size
1498:34 - we have that the 16 by 16 that's fine and then now let's run this year let's also run let's run this
1498:41 - year this image test image then once that's fine we run this we get this error let's check out
1498:47 - it must be four dimensional okay so what we'll do is instead of taking this test image we'll
1498:52 - do expand deems so we create this extra dimension and then we add this to the zeroed axis so we
1499:02 - add this extra dimension run that again you see we have our patches now look at what we're gonna
1499:08 - have when we print out the patches shape patches that shape what do we get see we have this 16 by
1499:16 - 16 by 768 now let's explain why we have this now recall that we have a 256 by 256 by three image
1499:23 - meaning that we're going to have three channels like this 256 by 256 then by three now since
1499:33 - we're dealing with patches i think it's preferable we should take this at once so this is 16 by 16
1499:39 - patch 16 by 16 patch here we have this 16 by 16 patch and then here again we have this 16 by 16
1499:50 - patch right here so now we have this three 16 by 16 patches and then given that each and every one
1499:57 - of those patches is 16 by 16 and 16 by 16 is 256 you'll see that if we pick a given patch like
1500:06 - here it's a 16 by if we pick a 16 by 16 patch then this third dimension will be 768 simply because
1500:16 - for each patch we have 256 pixels per channel so here we have 16 by 16 256 pixels which make up
1500:26 - this patch now this other one 256 and this one 256 and now if you sum this up it gives you 768
1500:38 - and that's how this value is gotten now to plug this out we are going to go through each and every
1500:44 - patch so we'll take this both vertically and horizontally create the subplots 16 by 16 because
1500:53 - we'll have 16 by 16 different subplots and then for each subplot we have this image is here we
1501:03 - pick ij we pick a given patch out of this 256 patches we pick that patch and then we're going
1501:10 - to reshape this because when you pick this patch you're left with this 768 here when you pick the
1501:16 - patch you're left with these pixels which have all been flattened out to this 768 dimensional
1501:25 - vector that we calculated here and so now what we need to do is reshape this back into a 16 by 16
1501:33 - by 3 pixel obviously 16 by 16 by 3 will give you 768 and so that's it let's run this now you can
1501:40 - always increase this figure size or reduce it depending on how you want to view this so running
1501:46 - this this what we get is output there we go we see this image which initially was compact now we've
1501:54 - breaking this up into several patches now before we go on to create our patch encoder which is
1502:01 - this whole section right here what we'll have to do is convert or reshape these patches here
1502:08 - so recall that in this paper after doing this year that's after creating these patches we have to
1502:17 - take each and every one of them and this will be considered as one element of the sequence so here
1502:22 - we have nine patches and here we have nine values which have been passed or nine different inputs
1502:28 - or vectors which will be passed here and so if we get back here since we have 256 of this
1502:34 - then we'll need to reshape this such that each and every one of this will be considered as
1502:41 - a part of our whole sequence so here would print out let's print out let's let's reshape that so
1502:51 - we have patches equals reshape and then we have the patches and then this will give us patches
1503:01 - that's a batch dimension patches shape zero then the next one with negative one and then here we
1503:09 - have 768 now either we put negative one or we explicitly write 256 so this means that we have
1503:17 - decided to put this now into uh 256 uh sequence length tensor now with this let's print out the
1503:26 - patches shape and you see what that gives us see here we have 256 now if we put negative one year
1503:32 - it's going to automatically give us uh this 256 and that's it so we we have this oh let's let's
1503:40 - rerun this so we get that difference so you see here 216 by 16 and then 256 and that's it so each
1503:48 - and every one of those 256 now will be passed into our transformer now we could modify this
1503:54 - uh print the this plot of the image so let's let's have that and then we'll say we have
1504:01 - uh patches shape uh one so we'll take 256 and then yeah we have system by 16 and then we have
1504:10 - simply i plus one okay so we have that we could take off this k now we don't need this again
1504:18 - take that off and then here we have this i here which will now reshape into this
1504:25 - and this should be fine so there we go let's run this again
1504:32 - and that's it we get the exact same output which is normal since we just uh restructured
1504:38 - those inputs so with this we see that we are now set to create our patch encoder layer
1504:44 - and this patch encoder layer will be similar to the kinds of layers which we've been creating so
1504:49 - we could just copy this here reduce that and then paste it out here so we'll call this our patch
1504:56 - encoder layer there we go we have patch encoder and your patch encoder so this patch encoder will
1505:05 - be responsible for first of all converting our image into patches and then um carrying out the
1505:14 - projection and adding the positional encoding patch encoder that's it and now here we have this
1505:22 - linear projection which we would um create here so let's call this linear linear projection and
1505:31 - we could do this with our dense layer so we have this dense layer stick that off and then
1505:39 - yes we could select our hidden size dimension or embedding dimension now since our input is already
1505:45 - let's uh write this out here we have inputs of uh let's take this off uh batch dimension or let's
1505:53 - let's take the batch dimension off we have inputs of 256 by 768 already so this is our hidden size
1506:04 - dimension for now but what we could do is we could convert this into 512 so let's do just that uh
1506:12 - we could let's no let's let's take 1024 let's make the model bigger but this is too many parameters
1506:19 - already it will take much time to train so let's let's let's stay with this okay so we stick with
1506:23 - that and uh let's get back to the code we're going to specify our embedding dimension uh let's say
1506:31 - hidden size as is in the paper so our hidden size we specify the hidden size and that's it
1506:37 - we're going to pass this in here we have hidden size there we go and we do not need this batch
1506:44 - norm we have the linear projection we have this hidden size okay let's leave it this way for now
1506:50 - so uh we have this call meta we should take this input and that will be it we'll have this
1506:57 - calculation we should saw already here so we would have this patches let's copy this uh from here
1507:03 - okay so we we get the input we get this input now this input we'll be getting here we wouldn't
1507:10 - need to do this expandings because the when training we already have the batch dimension
1507:15 - so we'll just have x let's call this in no let's let's let's have that as x okay so we have that x
1507:20 - and that's fine then we'll do the reshaping that's it and then from this reshaping we'll have our
1507:26 - output uh output will be this uh patch which has been projected to this new dimension and so here
1507:35 - we would have output and self linear projection which takes in the patches then to make this
1507:47 - value dynamic we're going to have let's take this off we're going to have the patches shape
1507:54 - negative one so this value we have here is the last dimension of all patches so we have that
1508:03 - and we get that last dimension now that we have this set we now ready to add up this positional
1508:11 - embeddings we have here so we will add this different positional embeddings here onto our
1508:18 - linearly projected patches we should also note that we are not going to take into consideration
1508:24 - this class embeddings which I mentioned in the paper as in practice this is not really important
1508:31 - we have here Lucas Beyer from the Google brain team who says the main aim of including this
1508:38 - was to reproduce the exact transformer network but on image patches and so in practice we are
1508:44 - going to stick to this linear projections the positional embedding now will be constructed
1508:50 - using TensorFlow's embedding layer so here we have positional embedding as described in the paper
1508:56 - embedding on encoding and then here we have the embedding layer now let's check out on how this
1509:02 - embedding layer is constructed or first of all what it's about now this embedding layer here
1509:08 - as described Tfkrel's layers embedding turns positive integers into dense vectors of fixed
1509:16 - size now getting back to the paper at this point you see we have this linear projections
1509:22 - here we have this linear projections which need to be added up to the positioning coding
1509:29 - which is this once now the linear projection we're going to get from here if we take if we
1509:35 - include the batch dimension we'll have an output batch size by number of patches so let's call
1509:42 - this np number of patches because obviously we have these patches here and each patch is going
1509:47 - to be a vector so it's number of patches by that last dimension which in this case is 768
1509:58 - so you could always fix this hidden dimension this all hidden dimension let's say batch by np
1510:04 - by hidden dimension and then here let's take this off let's take a batch of one
1510:12 - so if we take a batch of one we would have one for the batch dimension then the number of patches
1510:19 - would have will be 256 because if we have 256 by 256 image and we break it up into 16 by 16 pixel
1510:29 - patches then we'll have 256 different patches so here's what we have for this one and now with
1510:36 - the embedding layer what we'll be able to get will be another tensor which is like this in the output
1510:44 - of that embedding layer and so as we can see here it takes this indices and then turns them into the
1510:50 - dense vectors which we're interested in now the arguments we have here the input dimension
1510:56 - output dimension initializes regularizes as we can see here and then we can see this example
1511:02 - where it converts for example this number four into this 2d vector this index 20 into this other
1511:10 - 2d vector and then from this example which we could copy out and test in our code let's test
1511:17 - this out let's add this here so we have this which we could test out so you better you see
1511:23 - better how it works you see the model takes us input this integer matrix by dimension by
1511:30 - input length and then gives this other output now in our case since we're working with image data
1511:39 - we do not really have this vocabulary we should talk about here and so instead of this vocabulary
1511:45 - we replace this by the number of patches now if this were a natural language processing then
1511:52 - this will be the total number of words that our model can treat and so if we could have a vocabulary
1511:59 - of say 300 000 words and that's what we're going to pass in as this input dimension but as we said
1512:05 - here we're going to take this to be our number of patches now this input dimension is as I say here
1512:10 - is a dimension of the dense embedding so in this case they they put in the value 64 which we will
1512:17 - change to 768 shortly now that said we will have this model which you define take that off we have
1512:25 - this input you see the size of this input is 32 by 10 input there's a batch dimension and here are
1512:33 - the different indices so we have this input and then we're going to print out the shape from here
1512:40 - so let's run this there we go you see it takes in this and then outputs this one now as a vector
1512:48 - so basically what we have is we could have let's take this off what we have is we have some
1512:57 - indices let's take this off we have some indices let's say we have like in this case they have 10
1513:03 - indices so we have this this this this seven eight nine ten okay so here we have this 10 indices
1513:15 - and then oh we're not a batch dimension so we just have this 10 indices here so this 10 indices now
1513:23 - could be projected into its two-dimensional version where this one will be represented by a
1513:30 - vector so just like the patches so you have a patch represented by this vector this one presented
1513:35 - by this vector so this is no longer represented by an index say let's say because here this random
1513:40 - values take value between zero and one thousand so it's no more a value like say 900 so this
1513:46 - maybe was 900 now will be converted into a vector and the the size of this vector we see here
1513:56 - will depend on this value we set here so you see this this means that this this this one year now
1514:03 - becomes a 64 dimensional vector and so each and every one of this becomes 64 dimensional vectors
1514:11 - and that's why we go from this 10 to now 10 by 64 which you could see here if we ignore the
1514:19 - batch dimension so what we'll be doing in our case now let's take this off what we're doing
1514:26 - in our case is we have this 768 and then here we have 256 and then the size of this input
1514:39 - which is going to be some random input we take the size to be 32 by 256 since we have 256 patches
1514:48 - so we have that 256 let's take this we could take this off you could check out the the reason why
1514:54 - we need the input length documentation but for now we don't need that so we'll have 32 by 256
1515:00 - and then we expect to have an output which is 32 by 256 now by 768 let's modify this let's take
1515:10 - one by size of one so let's run this which we have 1 by 256 by 768 and you see that it would match
1515:18 - now with this year with this year from the projections or from the predictions of the
1515:28 - inputs now that's set let's simply copy this year we have this layer which we're going to copy
1515:36 - and add that year so instead of this embeddings we have this embeddings there we go and then
1515:42 - here we'll pass in the number of patches so here we have number of patches and then this one is this
1515:50 - hidden size so here again we have our hidden size and that's it so now we have the hidden size we
1515:58 - get here and then instead of having this we have this plus the positional encodings so we have
1516:07 - self dot positional embedding we could call that embedding and then what this will take in will be
1516:17 - an input of length number of patches so we get back here and then we would have
1516:24 - embedding embedding input okay use the tf range and then we have the start we could start from
1516:32 - zero this year the indices so we have this and then we have the limit our limit is going to be
1516:39 - the number of patches because we want to get the this indices going from the
1516:47 - zero value to some n value such that the length of this tensor we're going to create is going to
1516:55 - be such that is equal the number of patches so our limit year is number of patches in that sense
1517:01 - we could have this year so we have number of patches already from here so we could get self
1517:11 - number of patches there we go we have number of number of patches okay so we have our number of
1517:21 - patches that looks fine and then we could make use of that year so we have that number of patches
1517:30 - and then we are going to set our delta so we have delta will take to be one now obviously we set
1517:39 - delta to be two it means you're going to go from zero to number of patches why it's keeping two
1517:43 - steps and that's not what we want so we we want number of patches elements in this embedding
1517:51 - inputs so with this now we could pass an embedding embedding input and that should be fine okay so
1517:59 - we have this year looks fine and we'll take this off so now we return we could return our output
1518:08 - simply now it should be noted that this embedding layer here is similar to the dense layer but with
1518:14 - a dense layer when you have an input x you see with a dense layer when you have an input let's
1518:20 - write it here for the dense layer when you have an input x that input x is multiplied by the weights
1518:27 - and you add the bias so this is how we get the output but with this embedding layer is a simple
1518:33 - metric small application so when you take an input x you multiply it by the weights and you get the
1518:40 - output with that now let's run this cell here and then we move ahead so we have this run and we could
1518:49 - delete this two cells now now to test this out we could define patchank we have our patch encoder
1518:59 - patch encoder which we've just created here so a patch encoder and this takes into the number
1519:06 - of patches and hidden size so let's specify those 256 and 768 so we have this and then now
1519:16 - we could have patch encoder and then pass in an image and see what we get as output
1519:21 - here's image we have the zeros there we go 1 by 256 by 256 by 3 so now when we pass an input image
1519:33 - we expect to get some output of shape batch size by number of patches by the hidden size
1519:42 - or the number of hidden units and now that we've built this year up to the point where we have
1519:50 - our embedded patches let's go ahead and build this transformer encoder right here
1519:57 - as you can see we start with a layer normalization multi-head attention and then we add this this
1520:04 - input to this output we have the layer normalization again then the multi-layer perceptron
1520:10 - and again we have this addition and then we get the output now to build the code for our
1520:18 - transformer encoder we're going to paste out this patch encoder here we remove this we have
1520:24 - transformer uh there we go and then this transformer is made of a first norm layer
1520:33 - and then the second norm layer let's have this layer layer norm one it's our first norm layer
1520:43 - layer normalization there we go that's fine and then we have our second layer normalization layer
1520:54 - so call this layer norm 2 now we've defined our layer normalization layers we can go ahead to
1521:00 - define our multi-head attention layer so here we have multi-head attention and there we go
1521:09 - again your multi-head attention you could check out the documentation for this so in documentation
1521:15 - you have the different arguments which we could pass in the type of values which the multi-head
1521:20 - attention takes in so here uh we will specify number of heads and the key dimension here this
1521:27 - key dimension is the same as the hidden size or the number of hidden units which in our case we
1521:34 - fixed at 768 we could also have the value dimension drop out and this other arguments
1521:41 - now what this takes in as you would see here is a query value key and attention mass which we
1521:49 - will not use here return attention scores will not do this this training will not specify this
1521:54 - but what we'll specify will be this query and this value and then since we are not going to
1522:01 - specify the key it's going to consider that the key is the or rather it's going to consider that
1522:06 - this value here is a key so it'll be the same value that said here we would have the number
1522:12 - of heads let's take this off multi-head attention number of heads and then we'll also need the
1522:22 - hidden size uh yeah so we fully have the hidden size so we'll leave that away it will replace
1522:27 - number of patches by number of heads okay so we have our multi-head attention number of heads
1522:34 - hidden size and then the next will be the dense layers so uh as you saw in this model here you
1522:41 - see we have after the multi-head attention we have layer norm 2 and then we have this MLP here
1522:46 - which is made of two dense layers so we have our self dense one which is our dense layer
1522:55 - and then the number of units here will take it to be hidden size and then we'll specify the
1523:03 - activation to be a JLU activation as specified in the paper so we have JLU and then we'll repeat
1523:13 - the same process for the second dense layer so here we have that and dense 2 okay so that's it
1523:21 - we will specify those two dense layers and we now get our output so we should have that and then
1523:28 - now we could go ahead and call so let's before going to that let's modify this here transform
1523:33 - my encoder that's fine the name transform my encoder there we go that should be fine
1523:43 - okay so we building our transform my encoder layer already and now we set to pass this input
1523:51 - into these different layers we have our input here x there we go we have x let's take this off
1523:58 - and then x gets into the layer norm 1 so we have our output x x equals layer norm 1 which takes in
1524:13 - this x here takes our input x and then our output is this we could call this in and then we have
1524:22 - this input input we have our input which gets in here and then produces an output x and then from
1524:31 - here we get into the multi-head attention layer so we have x again this is multi-head attention
1524:41 - and this takes in x so now we have this input which gets here gives gives us output this output
1524:46 - gets into this multi-head and gives us output now that we've had this remember from the paper that
1524:52 - we have this addition to do so we have this add layer which we need to specify here because
1525:01 - after this output got into the multi-head we got this output which now needs to be added to this
1525:07 - input from the patches so we need to create this link here so that said getting back to the code
1525:13 - we have x now which is add and this add will take in this x and the input then from here again we
1525:27 - take this and pass into our layer norm so now we'll call this let's let's call this x1 because
1525:35 - we'll need this x1 again so we can't just be working with x you understand shortly why we
1525:42 - need this x1 so we have this and we have your x1 okay so we have that and that's fine everything
1525:50 - looks fine then from here we take this and pass or take this output x1 and pass into our layer
1525:58 - norm 2 so we call this layer norm 2 that's it and then this takes in our x1 produces output x1
1526:07 - then from here we take this and pass into our dense layers so we have this x1 which is equal
1526:20 - dense 1 and this takes in x1 and again we have x1 which takes in dense 2
1526:30 - sorry which takes x1 which is gotten from dense 2 which takes in x1 and now let's say this produces
1526:36 - an output x2 remember this like our final output so let's call this output let's let's call this
1526:43 - output okay so that's our output right there now let's do this then for this output again we're
1526:52 - going to do this addition you call from the paper you see after doing this addition you get an
1526:57 - output we get an output that we take this output from here and add onto this that's why you see
1527:02 - we had to create x1 for this and then x2 for this so let's get back to our code we have that and
1527:08 - then here we have our output now which is equal add that's it and then it takes in our output
1527:21 - and this x1 here or rather this x1 after this addition so we need to have this as x2 so let's
1527:30 - change this to x2 so we could make use of this now in this addition because if we if we have this
1527:38 - if we maintain this to be x1 then the value we're getting here would be this final x1 we got here
1527:45 - and that's not what we want what we want is this output from here so that's why we had to change
1527:48 - this variable name so we have x2 this takes in x1 then that's it so from here now we take this
1527:55 - output and add it to this x1 there we go we have that x1 we get now our output so we could close
1528:04 - this up and that's fine so we return our output and we have our transformer encoder right here
1528:11 - now we can go ahead and test this so we have our transformer encoder number of heads
1528:17 - hidden size that's fine and then we also have this input right here recall that the input or
1528:24 - the patches take this form so let's run this and see what we get we'll get an error from this
1528:31 - yeah this is logical because we need to pass in at least two inputs so here if you recall from the
1528:38 - from this year from multi-head attention you see this we have this two which must be passed in
1528:45 - then this key is optional this one optional and all this rest optional so we must pass
1528:54 - at least this two so that said let's get back and modify that so in this multi-head attention we
1529:01 - have this and then we have x1 so that's fine so now we have that let's run this again and see what
1529:09 - we get you see that's fine you see we get exactly what we expect so there's a kind of input we get
1529:14 - and here's our output after going through the transformer encoder now although this works
1529:21 - make sure that you check your work or check this code and be sure that everything is working as
1529:28 - it's supposed to be so be sure that you pass in the right inputs and get the right outputs and
1529:34 - so that you get the exact output you ought to get and not some outputs different from what you're
1529:41 - what you're supposed to have so that's it we have our transformer encoder and now we'll head on to
1529:47 - building our VIT model so here we would have our VIT model let's get back up here let's copy this
1529:56 - rest net from your complete network we had this so just simply copy the same structure
1530:05 - and then paste it out here now although there's not much difference we'll just the main difference
1530:11 - here is that we will have this model instead of layer so here we have a model and then here we
1530:19 - will call this VIT so here's our VIT model our vision transformer model and then here we change
1530:28 - this we have the vision transformer and we'll call this our vision transformer vision transformer
1530:37 - does it okay so there we go we could take this off take this off and all this off
1530:47 - then we're building in such a way that it takes in the number of heads the hidden size
1530:52 - then from this patch encoder the number of patches so we just need number of heads
1530:57 - hidden size and number of patches so with that would specify number of heads
1531:04 - there we go hidden size and then number of patches okay so we have that and then here
1531:15 - we would have our patch encoder which will be defined so we have patch encoder and this
1531:22 - patch encoder which we have in right here is simply what we created already so we just have
1531:27 - patch encoder and what does this take so we get back here and we look at this format see number
1531:36 - of patches and hidden size so that's why it's important for you to test this as you go on so
1531:41 - since we've tested this we show that this works when you pass in an input image so your patch
1531:47 - encoder number of heads and hidden size alright a number of patches and hidden size so we have
1531:56 - number of patches then the hidden size which we'll get from here now we have this patch encoder
1532:05 - we have our transformer encoders recall that we have several transformers here see we have
1532:11 - l transformer encoders so we get back here and then we define this transformer
1532:16 - uh let's call it trans encoders okay so this transformer encoders will be at least made up
1532:26 - different layers and the length of this list will depend on the number of layers so we have your
1532:33 - number of layers and then what we'll do is we'll say for underscore in this number of
1532:40 - layers or rather in range number of layers see we are then going to define the transformer encoder
1532:51 - so we have transformer encoder so we have the separate transformer encoders and then we'll
1532:58 - specify number of heads and hidden size so here again we have number of heads and then
1533:05 - we have the hidden size the input gets through this patch encoder so here we have our patch
1533:12 - encoder that is it this way we have x let's call this input so here we have input and this
1533:21 - passes through the patch our patch encoder and then produces the output x then once we have this
1533:29 - output x what we are going to do is we are going to loop through or we're going to go through each
1533:34 - and every transformer encoder layer so here we have for i in range of self that number of layers
1533:45 - number of layers let's define this here
1533:51 - number of layers equal this number of layers from here so set our number of layers and then we take
1533:58 - this in here so now we're going to go through this and then what we'll be doing is we'll be having
1534:04 - this x which is going to be the output from this transformer encoder layers so we have trans
1534:13 - encoders i which takes in the different inputs x so we'll do this for a number of layers times
1534:22 - and then for each of this we have trans encoders as we've defined already here
1534:27 - and we pick the specific transformer encoder now once we get the output from here see once we get
1534:33 - our output from here we'll now obtain an output which will flatten so from here we'll get
1534:40 - we'll say x equal we'll flatten this out take an x and then from this we'll have this MLP block
1534:49 - right here oh let's get back here oh we have MLP block here okay so we have this MLP block right
1534:59 - here that's after this now on the paper what they had done was since they had this class token they
1535:06 - picked this last or rather they picked this first output here so this means that if we have let's
1535:12 - take from here if we have an output by size one let's take this off one by 256 by 768 then out of
1535:23 - this 256 here or 257 because they'll have this additional so out of this 257 we'll pick out only
1535:31 - one from here and so we'll be left with that one which has 768 heating units so basically this
1535:40 - will be like having a one by 768 output but now since we're taking all this into consideration
1535:47 - we'll just flatten this out and then pass this through our MLP head right here so that's it
1535:53 - let's get back to the code we flatten this out and then we we we specify the dense layers which
1535:59 - make up the MLP head so we have dense one dense layer this dense layer has certain number of units
1536:08 - let's say a thousand or let's let's let's give this dense units here let's let's have it as
1536:15 - this argument so we have number of dense units okay so we have our number of dense units there
1536:24 - uh let's specify this number of dense units and then we have the Jell-O activation
1536:33 - CF and Jell-O okay so we have that for dense one and now we should have or we should write this
1536:42 - code for dense two so we have now dense two and then here same number of dense units you could
1536:48 - always modify this and that's it next from here we have x equal dense one that's it dense one
1537:00 - this is self okay so here we have dense one which takes an x and then dense two which takes an x
1537:11 - now our final this our final output dense layer has to consider the number of classes so just as
1537:18 - what we have been doing so far in this course you let's take this one this complete block here
1537:23 - so so far what we've been doing is we always ensure that our output dense layer has
1537:31 - the number of classes number of units in this output so here we would simply copy this
1537:38 - scroll down back to our code and put this right here so that's it let's call this dense three
1537:47 - dense and that should be fine okay so here we have dense take this off that's fine we have that now
1537:58 - let's run this should be fine let's go ahead and test this so here we have our VIT model
1538:05 - and we'll define VIT because of the IT and the parameters those arguments here
1538:13 - a number of heads hidden size all of this we're going to define this just down here
1538:19 - scroll down and then define this here okay so we have now this number of heads
1538:25 - uh we say eight heads hidden size 768 uh number of patches 256 the number of layers let's say
1538:37 - four layers the number of dense units let's say 1024 okay so there we go we have our VIT and then
1538:45 - this VIT now from the we pass in this um input to 1 by 256 by 256 by 3 and we should get a reasonable
1538:57 - output we could print out the summary so VIT summary and we could check out this model
1539:04 - so you see here we have 283 million different parameters for VIT model now let's reduce this
1539:12 - we could say four heads just two layers and then this number of dense units here we could take 128
1539:22 - now let's run this again and there we go you see the two layers you see the patch encoder and then
1539:29 - you see the dense layers we should follow this uh transformer encoder layers so now we have this
1539:36 - model we'll change this to the batch size so we have configuration and then batch size now the
1539:46 - reason why we're doing this is because when we're training our batch size is going to be known so
1539:51 - we don't want to have this uh here like that so with that now we run this again and run this
1539:58 - summary uh yeah we need to change this to 32 you see when we change this it doesn't work so here
1540:06 - we have 32 same as our batch size that's it and now we can go ahead to compile and train our model
1540:14 - our model on our training note that you we wouldn't get the best results because obviously the VITs
1540:21 - need very large data sets or even extra large data sets to perform as well as a confidence
1540:29 - and so when working with the VITs generally we want to train on a very very large data set
1540:34 - and then later on fine tune on the smaller data set towards the end of the epoch we still get
1540:40 - this error and the source of this error is the fact that since we've fixed the batch size that's
1540:47 - uh if you get backed up here in our training you see if you look at this here we fixed this
1540:55 - and so since this isn't dynamic now we have a data set which has been broken down into batches
1541:02 - of 32 and now towards the end you may have a batch of say for example eight obviously because
1541:11 - the data set the data set isn't necessarily divisible by 32 that is the number of elements
1541:19 - you have isn't necessarily divisible by 32 and so you have a remainder and so when you have this
1541:24 - remainder and that you fixed this here you should get an error because now you've told the model to
1541:31 - always use at this position a value of 32 so to avoid that instead of doing the patches shape
1541:39 - we're going to do tf.shape so with tf.shape before we had patches we got the shape
1541:46 - and that was it but now we instead using tf.shape so we call it the shape method from tensorflow
1541:53 - and then here we're going to pass in our patches so once we have those uh once we have our patches
1542:00 - passed in we now select this batch dimension and that should be it so let's run this again
1542:07 - you would see that even from here we could modify this we could put 32 and it'll still work fine
1542:15 - okay let's see too so with that now let's go ahead and compile and then restart the training
1542:23 - the model is training and as you could see it starts to stagnate around this 44.4 percent
1542:29 - accuracy and 44.16 percent validation accuracy in the next section we'll fine tune a vid which
1542:40 - has been trained on a very large dataset hello everyone and welcome to this new and exciting
1542:51 - session in which we are going to be looking at how to fine tune an already trained transformer
1542:58 - more precisely vision transformer model using the hugging face library and tensorflow tool
1543:04 - hugging face today is at the forefront of practical AI and it permits practitioners
1543:09 - around the world to build train and deploy state-of-the-art models very easily it's also
1543:15 - used by thousands of organizations and teams around the world so here we have a host of
1543:22 - different tasks which we could solve with readily available hugging face models like audio classification
1543:29 - image classification object detection question answering summarization text classification
1543:34 - and translation in our specific case we are dealing with image classification
1543:39 - and if you click open right here you see we have this uh image classification page where you could
1543:46 - already test an image on this vid model here so this is the vid base model with patch size 16
1543:56 - and the image size 224 we also have this facebook's uh DIT base distilled model
1544:06 - and with patch size 16 image size 224 you could browse a host of other image classification
1544:12 - models here as you can see here we have these different models sorted in out of a number of
1544:18 - downloads see 219 000 times this model was downloaded now we'll be working with this vid
1544:25 - oh we're fine tuning this vid by google right here here we have the model description
1544:31 - so you could have that we've seen this already in the paper um the intent that uses and limitations
1544:37 - and then how to use this model without any fine tuning so here you get passing your image
1544:44 - and then um already run classification on this image using this vid model right here vid for
1544:52 - image classification model now here you will notice that this is uh we suppose that we
1544:58 - dealing with a pytorch model so we could check in the documentation here where we would see this
1545:05 - vid model on the left side together with many other different models which are available for
1545:11 - free and you see here we have this vid you you could also check out the DIT the distillation
1545:19 - transformers let's get to D we should have DIT around this there we go here's the DIT you see
1545:27 - you have the DIT and this documentation right here so uh we also have the swing we should have seen
1545:34 - we looked at the swing previously so if we check out the swing somewhere here uh swing s where we
1545:42 - are swc you have the swing transformer right here so that's it see the swing transformer we
1545:50 - have seen already now let's get back to our vid vision transformer okay so we have this
1545:57 - vision transformer right here and you could uh here go through the the whole documentation so
1546:05 - we have the vid config vid feature extractor let's analyze this so it becomes clearer
1546:11 - there we go we have this feature extractor the vid model vid for masked image modeling vid for
1546:17 - image classification and you'll notice here we have tf vid model so the difference with this is
1546:22 - this is pytouch the model and then this is the tensorflow model which we'll be using now the
1546:29 - the tf vid model and tf vid for image classification now recall that you have your vid model which
1546:35 - starts from the patch you have the patch once you have the patch you have the transformer encoder
1546:40 - and then from here you have you take this you have this mlp head and then you have your output
1546:49 - right here now with this tf vid for image classification we have all this full year so
1546:56 - we go from this patch right up to the mlp head but with the tf vid model this part this part
1547:05 - is not included so what you get will be only this output right here so with a tf vid model
1547:11 - we just we have this whereas with the tf vid for image classification we have all this
1547:17 - now apart from tensorflow we also have the code for flex so you could check out flex vid model
1547:22 - flex vid for image classification now that said we are now going to focus on fine tuning this
1547:28 - vision transformer model we'll start by first installing this transformers library right here
1547:35 - so we have pip install transformers now that's fine we move on let's go ahead to start with
1547:44 - the fine tuning but then let's get back to documentation and here we have the overview
1547:49 - we could check out those vid config right here and you'll notice that all those different
1547:54 - configurations are basically what we've seen already so we have this hidden size 768 as default
1548:00 - value number of hidden layers 12 so here the stack in 12 different transformer encoded blocks
1548:08 - number of attention heads 12 intermediate size 3072 now for this 3072 actually is for the dense
1548:18 - layers we have here now the input you get in from this normal layer or right from here we have say
1548:23 - 1 by 256 by 768 then this gets in and gets to this point where we still have the same
1548:34 - so up to here is the same and we expect that this output here should be of the same shape
1548:40 - but then in this mlp layer we have two dense layers now the first dense layer will convert
1548:47 - this to 1 by 256 by 3072 and then the next dense layer would convert this to 1 by 256
1549:01 - by 768 so that's why they call this the intermediate size right here so this intermediate
1549:12 - size you can see dimensionality of the intermediate feed forward layer in the transformer encoder
1549:19 - we have the hidden activation JLU hidden dropout probability so you have some dropout here
1549:24 - attention probabilities dropout attention props dropout probability initializer range layer norm
1549:34 - epsilon value to better understand this we could get back to the layer norm documentation
1549:39 - in tensor flow you see epsilon here by default is one times ten to the negative three and then
1549:45 - we could see where exactly it's been used so recall that with normalization we have x minus
1549:52 - a given mean divided by a standard deviation now we we do not want a situation where this here
1550:00 - is zero and then we have an infinite output so we generally add some epsilon right here
1550:06 - now this epsilon by default as you see here is one 0.001 and in hogging phase here it is
1550:17 - one times ten to negative twelve now it's only encoder so there's no encoder decoder that's why
1550:22 - this is set to false image size 224 the patch size 16 number of channels three what are we
1550:29 - going to add a bias into the query keys and values here true encoder stride 16 now you
1550:37 - can remember the the use of the stride when we're trying to get the patches we once once we have an
1550:43 - image like this and then we have a patch size of 16 by 16 would move through 16 pixels to obtain
1550:51 - the next patch so that we have no space here so we have something like this actually okay so that's
1550:58 - it we understand this config you could check out your this usage of the config so let's copy this
1551:06 - code and get it back here there we go get back here and then we have this code paste it out here
1551:16 - okay so that's it you see clearly you could easily create a VIT model without necessarily
1551:23 - going through all this process which we had done right here so this was just for educational
1551:29 - purposes and means that if you want to build your own VIT it's simple you just have to
1551:33 - do this specify the configuration that's the config and then here you initialize the model
1551:41 - and that's it so let's let's run this and then let's also print out this configuration let's
1551:52 - look at that you see there we go we could change this we could change let's say we change this
1551:57 - hidden size to some value 144 so here let's have our hidden size so you see this how you could
1552:05 - change the hidden size to suit your needs so that's it you change this that's fine you look
1552:12 - at that and you see you now have this new configuration and that's it now for the next
1552:18 - we'll look at this feature extractor the feature extractor is similar to what we have done already
1552:24 - that is taken in the input resizing it and then clearing out some normalization so that's it for
1552:32 - the feature extractor you could check in the documentation this VIT model here is a PyTorch
1552:36 - model so let's go to tf.vit so this is for tensorflow now so here you see there we go
1552:46 - we have our tf.vit model you can expand these parameters and here we have all those different
1552:53 - arguments which we could check out now here you have this example of how we could use the tf.vit
1553:00 - model directly without going through any stressful process so here you see we have this
1553:09 - first of all we have the data data set and then we have this image which is extracted this you
1553:15 - could you could get this image from our own data set so that's it and then you have this feature
1553:20 - extractor so that's a feature extractor we have this model and now note that here the tf.vit model
1553:28 - is from pre-trained so this means that we are going to use this model which has already been
1553:33 - trained and you see this specifications here VIT base patch 16 to 24 in 21 key okay so that's it
1553:44 - the inputs now pass through the feature extractor before then pass to our model so we'll see how to
1553:51 - adapt this code so that we could fine tune our own model in tensorflow and as we have said before
1553:58 - this tf.vit model different from the image classification model in that the outputs here
1554:04 - are not the final output classes but these hidden states from the transform my encoder
1554:10 - so let's scroll here maybe let's read an example okay let's an example here you see
1554:15 - this output here that gives you directly a cat so a model produce one of 1000 imaginary classes
1554:22 - so that's it whereas here we have this hidden we have this hidden states okay we paste this
1554:30 - out here let's take this off take this one off and then we could get started with building our
1554:38 - own VIT model based off this hug and face tf.vit model so we wouldn't need this data sets here
1554:46 - we already have our own data set that's fine we wouldn't make use of this feature extractor
1554:52 - we have this model here we have our hug and face let's call this hug and face model and then
1555:01 - let's just take all this off actually so we have this we have the hug and face model we have our
1555:05 - tf.vit model from pre-trained and that's it now we're going to define some input so we have here
1555:13 - our input it's equal the input layer and then we specify the shape so here we we work with 224
1555:23 - by 224 by three and then using the tensorflow functional api we'll get an output here x which
1555:31 - we take in this hug and face model let's call this base model so taking the base model takes
1555:37 - in inputs let's call this let's change this to base model and so from here we have this
1555:43 - base model of v takes in the input and then now we have the output here x now when we run this
1555:49 - you see we download this 330 megabyte pre-trained model let's have this hug and face model here
1555:57 - so we we get the inputs from here and then we have the outputs let's call this output x
1556:05 - okay so we have that let's run this again we have a hug and face model now set
1556:12 - and we still get this error you see it's linked to the positioning of this input here so let's
1556:19 - let's change this let's say we have three by 224 by 224 we run this again you should see now that
1556:28 - everything works fine see uh it now works fine so this means that the inputs of this uh base model
1556:35 - here so hug and face model should be of this shape so it's three uh let's take this here
1556:42 - instead of being 224 by 224 by three i was inside three by 224 by 224 now this means that we need
1556:56 - an extra layer which will convert this into this before passing into our base model right here
1557:03 - so let's build that extra layer and we'll take inspiration from our resize rescale layer which
1557:09 - we had built already let's get to resize rescale right here so we have that uh we'll modify the
1557:19 - resize rescale specifically for this um hug and face model so here we have a disco resize rescale
1557:30 - for hug and face okay so we have this resize rescale we'll resize make sure it is uh 224
1557:37 - by 224 so every image which passes here will be 224 by 224 we're gonna rescale and then after
1557:48 - rescaling we are going to permitate the value so we call on this permit here permit layer
1557:56 - and the way we'll build this or the way we'll call this permit layer will be such that
1558:01 - we move this from this third position this is zero one two three batch by 224 by 224 by three
1558:09 - so move it from this third position to this first position so here we have three uh going to this
1558:15 - position and then this uh one two shifts to the right so we have uh three this one goes here three
1558:24 - one and then this two comes here so that the output now will be batch the you see the batch
1558:29 - remains intact by uh three which has been shifted by 224 by 224 so be careful not to do instead two
1558:42 - one this is this is one two and not two one because here we haven't uh height by width by channel so
1558:49 - we want to change this to channel by height by width okay so that's said uh we do this here
1558:57 - so we just do three one two and that's it so after this input layer before getting here we'll call
1559:05 - this x we'll take in our resize rescale and that's it so this resize rescale takes in the inputs
1559:14 - and then here we'll pass it will be x let's run this again we should get an error because
1559:21 - when we permute it it goes back to 224 by 224 by three so let's have this
1559:25 - see we have that 224 by 224 by three as we used to working and then we run this now
1559:32 - and everything should be okay oh we get an error resize is not defined uh let's run this
1559:40 - oh let's make sure that's how we called it let's go up resize oh we need to run this actually
1559:47 - this should be fine now um that's it you see everything is okay so now what we'll
1559:55 - do is we'll pass in some input let's let's pass in some input we have this test image right here
1560:02 - and then we have a model which takes in the test image now we need to also convert this
1560:09 - or rather add the batch dimension so let's expand dims and take in the test image right here so we
1560:17 - have that we run this and see what we get let's add this to the zero axis axis zero run that again
1560:27 - and then we told that we expected this but instead found this now let's get back up here
1560:33 - and we could change this to 256 by 256 and then knowing that this resize will convert it back to
1560:42 - 224 since our model takes 224 so let's run this again and there we go here's our output you see
1560:51 - we have the last hidden state one by 197 by 768 and as we scroll we have this puller output one
1561:01 - by 768 and then from here we we scroll down again let's see if we have another output
1561:10 - and that's it okay so this what we get is output we have the the last hidden state and this puller
1561:18 - output from the documentation where we had the parameters tfv model the parameters you see here
1561:26 - we have this last hidden state the puller output and then we told that these are the two outputs
1561:32 - we will always get and then we told that this one is optional the hidden states is optional
1561:36 - but the last hidden state is an optional get we always get the last hidden state we always
1561:41 - get this puller output and then this attentions here are also optional so if you want to get this
1561:47 - attention so all you need to do here is to specify config remember the configuration config config
1561:53 - config that output attention set that to true and so this means that by default this will be set
1561:59 - to false and then for the hidden states we also repeat the same so config that output hidden
1562:05 - states we set it to true now here in the documentation they explain a difference
1562:10 - between this puller output and this last hidden state but getting back here you should see the
1562:16 - shape so you see this is just one of this year while this is all our full hidden state our full
1562:26 - last hidden state one by 197 by 768 but this model or this hugging face with model was built
1562:36 - taken into consideration this class embedding right here and so this means that if you want to
1562:42 - uh um carry out some classification is better off or we're better off taking this final year this
1562:50 - final um or this class embeddings final hidden state now if we want just those hidden states
1562:59 - we could specify this pick out the the zero index we run this and we should get only the output or
1563:07 - the last hidden states so that's it we get this last hidden states and then since we are interested
1563:13 - only in that output corresponding to the class embedding and the class embeddings are the zero
1563:19 - position here the zero position we are going to take we are going to we are going to do this here
1563:25 - we're going to take um this we're going to take for the first dimension here we take all
1563:32 - all and then for this next one we take we select the zero index and then the next we take all
1563:41 - so let's run this again and see what we get and that's it we have this output right here
1563:49 - and now since we've converted this hugging face model into a tensorflow model we could do
1563:54 - a summary so let's run this and check out our model summary so that's it we have this model
1564:01 - summary we scroll down see 86 million parameters we have the input sequential the sequential is
1564:10 - corresponds to the resizer scale layer of it and then the slicing operator which we have here
1564:18 - which permits us get our specific output now getting back here we'll now add our final classifier
1564:28 - here so we have this and then we have we'll call this output that's fine let's just call this
1564:36 - output so we have this output takes in the dense layer has the number of classes specified here
1564:44 - and then we have activation softmax as usual so that's it we have this here output okay now
1564:52 - let's run this and see what we get we're getting an error because we didn't pass in this x here
1564:59 - so let's run that again now as the model is training just remember that the learning rate
1565:07 - we use here isn't appropriate as we can't be using this type of high learning rate when doing fine
1565:14 - tuning so we have to change this and use some lower let's say five times ten to the negative
1565:21 - say five okay so let's stop the training and then restart this process you see now that when we
1565:27 - initialize our model and then we modify this learning rate you see the loss drops now much
1565:35 - lower than what we had before with a higher learning rate and our accuracy is already at
1565:39 - 75 percent and we are still at the first epoch so be careful when you find tuning or when you're
1565:48 - updating all the parameters of an already trained model you have to make sure you use a very small
1565:55 - learning rate hello everyone and welcome to this new and exciting session in which we are going to
1566:05 - log our data to 1db in previous sessions we already saw how to do 1db logs that is logging our
1566:16 - metrics like this also logging this confusion matrices as we can see here we also looked at
1566:25 - hyper parameter tuning data set versioning and model versioning in this session we'll not just
1566:31 - only log metrics but also log tables like this just as we had done before we're going to start
1566:40 - with this weights and biases installation so we we have this pip install 1db that's fine and then
1566:49 - we import 1db and then from 1db krass we import the 1db callback so we run this and then we go
1566:55 - ahead and log in now we've we've gone through this already in some previous sessions in here
1567:01 - you just have to click on this uh authorize so you get here you should have this link or this
1567:08 - code to copy from here now if you do not have this uh this key if you don't see this key to copy
1567:15 - then you would have to sign up on the weights and biases platform so let's get back here we now paste
1567:22 - this and then we press enter so that should be it now with with that we initialize this project
1567:29 - and we call the project emotion detection entity neural learn so we have that and that should be
1567:36 - fine so you could change this to whatever name you want and then uh whatever entity you corresponds
1567:42 - to you to yours so that's it currently logged in as neural learn use this to force we log in
1567:48 - okay so we have that and then we have this configs here so we run that um train and
1567:56 - validation directory that's fine now we go ahead and train our model and we include this 1db
1568:02 - callback right here so all you needed to do here is include this 1db callback and training can start
1568:09 - now we just run this for say three ebooks just do that for three ebooks and then let's just take
1568:16 - a small part of our data set so let's take the small part so we have that and we run this
1568:23 - now as we train this model you'll notice that with just a very small part of our data set
1568:30 - the hug and face model performs quite well as you see we already have 77 of validation accuracy
1568:37 - with just this small part now the other point is we get back to 1db you see here we could check out
1568:45 - on our profile there we go you see projects you select emotion detection
1568:53 - and then while this loads okay so we have this and we click on this hopeful plans right here
1568:59 - this is what we currently haven't this is our current run so you see it shows us already this
1569:05 - charts here we have our top key accuracy validation accuracy validation loss and all of that
1569:10 - just by specifying that 1db call so this is already information which has already been locked
1569:16 - in and as we know you could always get access to this anytime since this information is locked on
1569:24 - weights and biases servers from here we see that the validation accuracy goes right up to about
1569:31 - 79.63 percent so it's about 80 validation accuracy while the accuracy itself goes to 82.6 percent
1569:43 - and you could check this here you see 82.6 and then you're 79.63 so that's it we able to lock
1569:52 - this data very easily the next callback will be adding here will be that of including confusion
1569:59 - matrices so here we'll put log confusion matrix okay so we have this log confusion matrix and
1570:09 - now we'll go ahead and put out a code for this note that for the 1db callback we didn't necessarily
1570:15 - need to pass in any information here but with this log conf matrix this is our custom callback
1570:22 - we are going to write code such that each and every time we're done with the training
1570:30 - we're going to automatically generate this confusion matrix based off what the model predicts
1570:37 - now recall that we've already treated confusion matrices so you could click on this and then
1570:42 - we would copy out this code copy out this part which permits us get those predictions and the
1570:47 - levels and then we will not make use of this any longer as 1db as a method which takes in
1570:55 - our confusion matrix matter which takes in these predictions and those levels and automatically
1571:00 - produces the confusion matrix so let's copy this here and then let's get back to our callback
1571:07 - there we go so at the end of each epoch it's here we get the epoch logs and then we go to
1571:17 - the validation data set and then we have our image the image is patched to the model to get
1571:23 - the predictions and then we have the levels obviously coming from this level right here
1571:27 - then from here we produce a simple list of the predictions and the levels so now once we have
1571:35 - this list once we're able to get this list which we've treated already in some previous sessions
1571:39 - you now just have this part so this you could check out the documentation for these plots
1571:45 - let's get to the documentation the 1db documentation you could check this out here
1571:53 - let's type in confusion matrix there we go we have confusion matrix 1db of that
1572:01 - let's check out this logplots right here okay so here we here we in this experiment tracking
1572:09 - and log data with 1db log log median objects then logplots okay so we have this we the ROC curves
1572:17 - or PR curves and our confusion matrices so we saw this already in some previous sessions
1572:24 - so you could get back to looking at how to make use of this 1db so that's it you see we copy out
1572:32 - this piece of code here where we have this confusion matrix we have all the ground truth
1572:38 - that's the levels and then we have what the model predicts then we obviously have the class names
1572:43 - so that's it and then here you see just 1db log and then you specify this string right here and
1572:50 - then you pass in the confusion matrix cm which is this and that's basically what we are doing here
1572:56 - so we just copied out that code so with this year we have our white true prets class names
1573:01 - which is specified already and then we just run this so we run this and that's fine now the next
1573:08 - thing we'll do is we're going to create tables and now we're going to see all how important working
1573:14 - with tables is as compared to just simply log in values tables will permit us visualize our data
1573:21 - in a more interactive manner and here you see under this data visualization you have those
1573:27 - log tables and we'll see how to create a simple table and log this so here we have the data you
1573:34 - notice that we have different rows this row one row zero row one two and three and there you have
1573:41 - the image and you have the predicted and the truth levels so basically what we're doing here
1573:47 - is we're taking this image then we log so here's our image then we log its predicted value let's
1573:56 - say for example class eight let's say predict class eight and then the truth level is class zero
1574:02 - then we take some other image uh let's say predicted is one and here's one so basically
1574:08 - this is what we're doing we created now this table where we have different rows and different
1574:15 - columns okay so that's it we'll we see how to create this and we also see how to log this
1574:25 - tables into our 1db runs so getting back to the code here you see we have this different columns
1574:33 - image predicted and level and then we're going to call this table our validation table we have
1574:41 - those columns which takes in our columns here obviously you could add this you could increase
1574:45 - this and put whatever values you want there and then fill that so we have this uh validation
1574:51 - table which we've just created and then now we're going to fill this table color row by row so for
1574:58 - our first row you see we have the predicted values uh our model takes in the image that's it
1575:05 - and we have the levels and then here you see for a given row we have 1db image we pass in our image
1575:13 - that's it we pass in the predicted scores then we pass in the levels scores to for
1575:19 - corresponding to this three columns right here and then uh for this valid data table that we
1575:25 - created here we're going to add this data so we're going to do this for each and every element of
1575:30 - our validation data set now here we decided to take a hundred and then we'll get him back up
1575:36 - what we'll do is we are going to uh somehow unbatch this so we're going to change this batch
1575:43 - size to one because we want to be able to play around with a single validation element and not
1575:51 - uh say for example 32 of them so because of that we'll just change this to one and we run this
1575:58 - again uh this should be fine okay so so we have this uh validation data that's fine and then here
1576:08 - uh training okay we run this again and that's it we have our validation data now let's get back
1576:14 - here as we said we've we've taken just a hundred of this so take just a hundred let's let's even
1576:21 - reduce this let's say 25 okay so but you could take all the elements actually we just want to
1576:26 - make this a bit faster so we have that and once now you feel this you see this follow
1576:32 - permission to fill each and every row so you just keep filling rows in your table and that's it you
1576:38 - could get this uh here see this we you see you have this table the columns and you have the data
1576:44 - right here and then you just log this information so this from this uh documentation you could be
1576:50 - able to uh understand exactly how we build this table although it's quite straightforward so from
1576:57 - here we do one db log and then we have our model results and we're passing the vowel table so let's
1577:03 - run this let's get back here let's run this two cells we run this two cells now and then we start
1577:10 - with the training so we have the training now over and we could check out this custom chat which is
1577:15 - that of the confusion matrix you see here we could get the number of uh times the model predicted
1577:23 - angry when we're supposed to be angry here is the prediction happy when it's supposed to be happy
1577:28 - and then here is prediction sad when it's supposed to be sad so we see this is 357 times for happy
1577:35 - 876 um for sad 682 and then here we have the times when the model predicts sad when it's
1577:43 - supposed to be angry so this is our confusion matrix uh which we generate automatically using
1577:49 - one db confusion matrix method and then we move on to this tables so here you notice that the
1577:56 - tables contain a hundred different elements so uh you could keep going on to the next so
1578:05 - move on to the next 10 and so on and so forth so let's get back or let's let's just work with this
1578:10 - now here you see that with this table you can actually like here you see if you scroll if you
1578:17 - get get to the hover on this and you have these three dots click on that and you could group so
1578:23 - check on this you see you group by image for this one what we have here is this predictions you see
1578:30 - happy the level happy uh yes sad sad angry angry and so on and so forth so this model predictions
1578:38 - here are quite great now if we increase this move you see we have this example here where the model
1578:49 - or rather the the true levels is happy and the model predicts angry but when you look at this
1578:54 - photo it's uh it shows that this person is not necessarily happy so we understand why the model
1579:02 - may be thinking that this person is angry and so with this one db tool you see that it becomes
1579:08 - very easy to detect mislevelled images like this one now another way to get this more systematically
1579:17 - is by getting here but since we do not have the group we need to start by ungrouping right here
1579:24 - so let's get back here click on this we ungroup now we've ungrouped already and then we get to
1579:31 - this level and then we group by those different levels you see that here we have happy sad angry
1579:41 - now um this is what the model predicts the model you see here the model predicts mostly happy
1579:47 - because obviously the level is happy because we grouped by happy level here sad you see mostly sad
1579:52 - angry and mostly angry now what is interesting to note here is for this angry well the level
1579:59 - is angry the model never predicts happy anyways let's uh get back here you should be able to
1580:06 - scroll through this you see you should be able to uh scroll through all the different happy images
1580:12 - and here you should be able now let's uh look at this you should be able to detect cases where
1580:20 - the model or rather the data set has been mislevelled like here this person isn't
1580:25 - necessarily happy so already you could detect uh these kinds of misclassified or mislevelled
1580:34 - data points then we also have this image which is almost completely blurred out you could always
1580:40 - could check on these kinds of images and uh take them off actually because if your model
1580:48 - isn't going to be seen this in real life then it's preferable to take this off nonetheless if
1580:54 - uh in the real world the model is going to be meeting these kind of images then you could
1580:59 - allow this uh let's keep going this way you could also check out for those sad images
1581:06 - and be sure that those are actually sad people
1581:11 - here again you see this person actually smiling and it's leveled as sad then for the angry you
1581:18 - see this uh little boy was visibly happy but instead level angry so the 1db tables permits us
1581:27 - check these kinds of errors in a very interactive manner now we will again ungroup so we ungroup
1581:35 - this and then we could check this out for a predicted group by uh predicted levels so
1581:42 - this is what a model sees so the model predicts that this is happy oh this is sad and this image
1581:50 - is angry you see what you notice here is because this image you know recall this image which we
1581:57 - just saw because this image has been had been leveled as angry the model now predicts that
1582:03 - this image is an angry image which is not true and so when you deploy this now into production
1582:09 - you would find that this kind of images or this image will be predicted as angry whereas it's
1582:16 - meant to be happy and the worst part of it is that even your evaluation wouldn't find this as an
1582:25 - error because already the level or the the the class for this was set to be angry then another
1582:35 - thing you could do here if you get here um let's have this back another thing you could do is
1582:45 - you get to this column settings here click on this you could uh get to column settings
1582:49 - just by clicking on this column settings this column settings click on that and then you're
1582:55 - you select the levels so here you're picking all the levels where the printed value is different
1583:04 - from what the level or rather what the model predicts so here we could say row predicted so
1583:11 - when row level is different from a row predicted we unless it so we have that so press enter
1583:22 - and then what we notice here is we have this true this false and uh and that so basically the
1583:31 - reason why you have this true and this false is the true is for when let's get back to this year
1583:40 - the true is for when the row level is even from the row predicted so this is this are the wrong
1583:47 - predictions so what we could do here is we could modify this to equal so that's equal uh that's
1583:53 - fine this should be reset so you see okay so you see now it's true meaning that this false is when
1584:00 - the the prediction is not the same as the level so that's it we could increase the size of this
1584:05 - table let's get to this increase the size and basically that's it so you see uh the greens
1584:14 - are for when the coincide and the reds are for when what the model predicts is different from what
1584:20 - the model ought to have predicted now we could get again to the stop and then check this column
1584:26 - settings and take this off so you see you could simply take that off okay so we have that reset
1584:35 - again and you now have your levels as you had originally you could do the same for the predicted
1584:39 - now you could also get here see this take on this and then you you check the row row we're trying
1584:47 - to get all the rows where the row level is uh different see different from the row see type row
1584:58 - predicted so that's it because if you say row level different from row level obviously you have
1585:02 - no row so let's test that and you could see if you apply you should have no element in here
1585:07 - see there's no element uh that's not actually possible now let's take this off and now we take
1585:13 - row predicted so row predicted and then you apply you apply no um some connectivity issues
1585:26 - so here we apply we check out where the the the predicted is different from the
1585:32 - level so here we have this here see happy and the predicted either sad or angry so you
1585:40 - are the wrong predictions so with this you could check out and see the kinds of images
1585:46 - which the model finds difficult or find difficulties in correctly predicting
1585:52 - so like with this one you see happy this the the level here is happy but clearly this man
1585:58 - isn't happy this one doesn't look too happy doesn't necessarily even look happy this one is happy
1586:05 - and we could look at that so generally for this our problem uh we see clearly that many many
1586:13 - problems come or many of these misclassifications come already from our data set so this means that
1586:19 - the data set we have to look at the data set and ensure that it is cleaned before making use of it
1586:26 - so you see this happy this person isn't happy this this happy this happy uh we could get to
1586:31 - sad you see this person is happy but this level that's sad so with these kinds of interactive
1586:37 - visualizations we could be able to say for example we want to see where the model was supposed to
1586:43 - predict a certain value and it didn't predict that value now we could also check out where
1586:49 - this is equals so we could check equals and say okay where the the row level is equal the predicted
1586:56 - we apply and then we check out the kinds of images we have you see from here that oh most times the
1587:03 - leveling will be correct you see the level happy level sad the level angry uh we are we still have
1587:10 - this image here this shows that we have even gone as far as teaching the model to look at this kind
1587:15 - of image as an angry image
1587:19 - Hi guys and welcome to this other exciting session in which we are going to look at the ONIX format
1587:28 - of representing machine learning models. ONIX actually stands for Open Neural Network Exchange.
1587:35 - This open standard for machine learning interoperability was co-developed by Microsoft,
1587:42 - Facebook and AWS and in this session we'll learn how to convert our already trained TensorFlow
1587:50 - model into this ONIX format and then carry out inference on this our newly created ONIX model.
1588:01 - So we've gotten to this point where we've fine-tuned our hugging face based vision transformer model.
1588:08 - Now we have this TensorFlow model which we've created and evaluated and it may happen that
1588:18 - another developer using a different framework like for example PyTorch wants to make use of this
1588:25 - model which has been trained in TensorFlow. So thanks to the ONIX format we can now convert
1588:33 - this code which was written in TensorFlow or rather convert this model which was built in TensorFlow
1588:40 - into an ONIX model and then later on convert this model from the ONIX format into PyTorch
1588:51 - such that we now have this PyTorch model which this other practitioner can use.
1588:57 - Now another possibility is the reverse that is we could go from this PyTorch model
1589:06 - to a TensorFlow model thanks to the ONIX format's interoperability. Another reason why a developer
1589:16 - will want to say take this model from PyTorch and make use of it say in Cafe for example
1589:23 - maybe because that model is more efficiently run in this other framework and the reason why we have
1589:33 - these kinds of differences for different frameworks is because for example you could have a convolutional
1589:40 - neural network or a convolutional layer which is built in PyTorch with a certain implementation
1589:47 - and the implementation in Cafe may be slightly different and maybe more efficient as compared to
1589:55 - that which was done in PyTorch. Now this is just for demonstrative purposes and we are not saying
1590:01 - that the implementations in Cafe are better than those in PyTorch. And so in summary the ONIX
1590:10 - format allows models to be represented in a common format that can be executed across different
1590:17 - hardware platforms using the ONIX runtime. So now developers can feel free to build their models with
1590:26 - just any framework say for example TensorFlow or PyTorch or Paddle or whatever framework they
1590:34 - actually want to work with knowing that they could deploy your model on whatever hardware they want
1590:40 - to since they could now convert these models to the ONIX format and then run inference on these
1590:49 - models via the ONIX runtime which is in fact an inference engine which is lightweight and modular
1590:58 - and permits us run our ONIX models on just any hardware we choose to work with. So here let's
1591:06 - look at the list of supported hardwares. You see for example the TensorRT which is very popular
1591:14 - with the NVIDIA GPUs and permits us to attain very high speeds when carrying out inference
1591:22 - on neural network models. The other great advantage of working with the ONIX models
1591:28 - is the ease with which we could convert TensorFlow models into this ONIX format. So right here you
1591:34 - see we evaluate in our model this our hugging face model which we fine-tuned previously
1591:41 - and we get in 90% validation accuracy. Now we can go ahead and save this model so yeah let's take
1591:49 - this off HF model let's call this hugging or let's call this VIT fine-tuned. Okay so that's it we
1591:58 - have this fine-tuned VIT model which we're gonna save we check this out here. There we go we see
1592:06 - a VIT fine-tuned and you could check you see you have the saved model Keras metadata variables
1592:14 - the assets folder and then here if you you could see that this is almost one gigabyte model so
1592:21 - uh you could view this here you see uh 984 just just check as I hover on this you see here
1592:29 - 984.39 megabyte model and so this means that if you were to deploy this in a real world scenario
1592:39 - then you need to always allocate this amount of space in order to run this model. Now let's move
1592:47 - on to converting this model into the onyx format. We shall have this two installs so we start by
1592:55 - installing the tensorflow to onyx tool and then we'll install uh the onyx runtime so we run this
1593:06 - and then now we'll go ahead to convert our model which is this one here VIT fine-tuned so we just
1593:13 - have to specify your VIT fine-tuned into the onyx format so here we just say VIT fine-tuned and then
1593:21 - now we have this model I'll give it this is the name of our onyx file so call this VIT onyx
1593:32 - that's it VIT onyx tf2onyx.convert and that should be it so now we'll run this
1593:41 - then while waiting for that run to be complete you could get to the onyx github repo tensorflow
1593:49 - onyx and then you'll have the documentation for how to convert from tensorflow to onyx
1593:58 - so let's get back to this uh to running because here we have a series of warnings then
1594:07 - uh specifies the tensorflow on tensorflow onyx and tensorflow to onyx versions then the upset
1594:15 - then um it's optimizing and now we have this successful conversion so onyx model is saved
1594:22 - as VIT onyx dot onyx now let's open this up see VIT onyx dot onyx you see you you we've moved from
1594:30 - this uh let's open this again we've moved from this year where we had uh 984.39 megabytes to
1594:40 - this optimized onyx version which is at 327 megabytes now another option will be to convert
1594:49 - the model from this Keras format to the onyx format so this first one we saved this as a
1594:56 - tensorflow saved model and then now we could just have this year so let's say we have the model we
1595:04 - saved this as a Keras model so we have that h5 and we run that um taking here you see we have this
1595:17 - still 984.98 megabytes close to one gigabytes and then from this Keras format we are going to now
1595:25 - convert this to onyx now here we're going to have this specification and then we'll pass in the
1595:34 - image size so here we have um the batch by 256 by 256 by 3 it's float 32 and it's our input
1595:43 - then we could specify this output path so this one was VIT onyx let's let's let's have this
1595:49 - as um let's let's call this VIT Keras dot onyx okay so this is going to be our output path here
1596:03 - and then we will have this tensorflow to onyx here which contains this from Keras method
1596:13 - which takes in our model hug and face model and then the specifications which we just
1596:21 - mentioned right here so here our specifications right here we pass this in this uh our passes
1596:28 - as our input signature we have this offset value and then we also have our output path which we
1596:34 - have already specified here then we have our output names which we shall get automatically
1596:39 - done so that's it let's run this while that's running also note that you could check out
1596:46 - this conversions on the onyx runtime.ai platform where you can get the details of all what we're
1596:53 - doing so let's get back here still still running and that's it complete let's check out our VIT
1597:01 - Keras you see 327.59 megabytes then from here we move on to the inference where we'll see whether
1597:11 - what we get from the onyx model coincides with the initial Keras or tensorflow format
1597:20 - so right here we have this provider now here we specify this provider to be this CPU execution
1597:27 - provider as we'll be running this on the CPU and then we have this onyx runtime as RT which we
1597:37 - are imported right here and we should be making up making use of here so the when we want to run
1597:44 - an onyx model you see the first thing you have to notice we do not need tensorflow anymore so even
1597:50 - if we restart this whole process all we need to do will be just to install the onyx runtime
1597:59 - and then import this this way so now we have our onyx runtime we we have this inference session
1598:08 - which we create by just specifying this path our output path here our output path is the path to
1598:17 - this model the onyx model and then the provider is this one here so we just need to specify this
1598:25 - path and the provider and we're good to go so now we have this the next thing we want to do is to
1598:33 - run the inference so the onyx prediction is this m.run and then now we'll specify the output names
1598:41 - now this output names is gotten from here so let's run this let's print out output names so you see
1598:49 - what it contains that's it you see we have that dense and if we get back to when we're creating
1598:58 - this model let's get back to this okay you see that the name here we have is dense so if you
1599:04 - specify the model name to be different from this you would have a different output name so that's
1599:09 - it let's get back to our onyx inference we've converted already and that's it so so that's it
1599:17 - we have our output names which we created all right i wish we generated automatically from here
1599:23 - does it and then obviously at least because we could have several outputs and then from here
1599:31 - we pass in our input image now this our input image is simply what we've been having already
1599:37 - so we just we could copy this copy this and then test this out here with our onyx model
1599:45 - um let's have this here add this code paste this out all we need here is just this basically
1599:52 - so we we could run this let's run this that's it we have our image and then let's run this
1599:59 - and yes let's run this and then we print out the onyx thread so we get in input must be a
1600:06 - list of dictionaries or a single non-py array so what we're going to do here is instead of tensor
1600:11 - flow we use non-py so you see already that we do not really need tensorflow like even with this
1600:17 - we could get the the test image from here let's let's get this test image directly so we could
1600:24 - have test image and that's it we run that we have our image this not defined um test image let's
1600:35 - run that again that's fine we get this other error because this input isn't a float so here
1600:42 - instead of this we're going to have test image dot as type np dot float 32 okay so we have that set
1600:54 - now let's run this again no before doing this let's make sure we pass in this image instead here
1600:59 - so we have that we run that that should be fine okay so let's rerun this again and you see we
1601:09 - have now our onyx thread so there we go you see it shows us that it's a sad image because this is
1601:17 - angry happy sad so it's sad because it has the highest probability and you see the image here
1601:22 - is sad now if we get back to this top here and simply run this the same image let's run this
1601:31 - let's get the probabilities actually so let's let's let's say we want to have
1601:36 - let's print out the model image let's get those probabilities you see this is this what this
1601:44 - model gives us is output and here's what we get from the onyx model right here now before we
1601:53 - continue another thing we would like to check out is the speed or the time or the latency of the
1602:00 - model so let's get back up here and we could add this code so let's import time we could just do
1602:09 - it below so let's yeah let's do let's just do that below so let's take this off and then here
1602:18 - we could say we want to have let's import time there we go and then we have t1 which is time
1602:28 - the time and then we do hf model taking an input image then we record the time the current time
1602:43 - minus the t1 so we could get the time which is elapsed after running this model you see 0.14
1602:51 - that's a 0.14 and you should know that here we're supposing that we're running this on a GPU so you
1602:57 - check uh manage sessions we're running this on a GPU now for the onyx model we could have here t1
1603:09 - time dot time and then we could print out the the difference in time so we have that minus t1
1603:18 - let's run this and see what we get input lists a single numpy oh let's run this again
1603:31 - okay so you see with uh we here we're making use of the cpu and we get in 0.279
1603:39 - seconds while with the gpu here we get in 0.143 seconds now we can be comparing the
1603:48 - onyx runtime results on a cpu with that of this uh hungry face model with tensorflow on a gpu
1603:58 - you could you could even see from here if you do get device you see from here that what the onyx
1604:04 - runtime is using is the cpu and so if we want to make use of the gpu we would have to install
1604:09 - onyx runtime gpu version so we could compare the the two um models so for now just note that
1604:19 - tensorflow with the gpu is 0.15 seconds and then we'll need to get tensorflow with a cpu
1604:30 - and then we'll also get we we got onyx with a cpu onyx with a cpu give uh let's say 0.3 seconds
1604:42 - give about 0.3 oh let's run this again we run it again we should get let's run this again
1604:50 - oh getting an error now that was because i had moved this uh file into the the drive so here
1604:57 - we have the vcarass here and we run this again we have about 0.38 to see that's it about okay
1605:04 - let's say 0.5 so let's let's say this is 0.5 uh here and then we need to get onyx with a gpu
1605:14 - and then we also need to get a tensorflow with the cpu so that's it uh what we'll do now is we'll go
1605:21 - ahead and install the onyx gpu so here we have pip install onyx runtime gpu and that should be
1605:29 - it we already installed this okay so we have this um onyx runtime gpu and then if you if you if you
1605:38 - do uh import onyx runtime as rt and then you as rt and then you say rt.get device we are getting
1605:53 - the cpu so what we'll do now is we are going to restart this runtime so basically restart this
1606:00 - runtime so that um this the the gpu version of onyx will be taken into consideration now this
1606:09 - time around we're going to install onyx runtime with the gpu and that's it as you can see now
1606:17 - the device we're using is a gpu okay so we have that let's now get back here uh we already have
1606:25 - our model in the onyx format so we just run this and then if you run this you'll see that
1606:34 - since we we we specify the provider to be the cpu we shouldn't have any much difference here
1606:41 - so let's run this again we import a time output names not defined okay our output names is
1606:48 - basically this so let's have this list and let's define this here so let's have output names
1606:55 - um output names there we go output names dense so let's run that again that's fine and then
1607:06 - oh we get back here but note that these output names were generated from here so you could
1607:11 - um get back to start generating this or you could just make use of the output names as
1607:16 - you can see since we restarted this runtime we do not have those files anymore apart from the onyx
1607:21 - uh which we start in the drive so if we have to do this we need to retrain our model
1607:27 - since we lost them already okay so we have our output names here which was specified and then
1607:32 - we get back here we run this again and check out on the time it takes to run the model you see 0.34
1607:40 - not bad uh 0.34 okay let's say 0.34 so here normally this should be zero point yeah let's
1607:49 - say 35 okay so the tf with gpu 0.15 onyx with cpu 0.35 now the way we're going to use this is
1607:57 - we'll get here you see um in this documentation you have the provider you will specify
1608:04 - the coder execution provider so let's copy this and then paste it out here so instead of instead
1608:12 - of here for example instead of cpu now we have the gpu so we could make use of the coder execution
1608:18 - provider so here we'll take this off and then paste this out so this is our provider now
1608:24 - note that since these providers uh can be a list you could have several providers like here when
1608:33 - we do this when we have coder execution provider before the cpu execution provider it means that
1608:41 - that the priority goes to the coder execution provider and so in the case where we are having
1608:48 - a cpu then we'll start with this one and given that this cannot work in the situation of a cpu
1608:54 - we would then move on to this but if we have a gpu then directly we will use this scooter
1609:02 - execution provider and that's it see 0.048 seconds or let's say 0.5 seconds which is three times
1609:12 - less than what we had when running our model or when running the tensorflow model so you see
1609:17 - already that the onyx uh framework permits us to optimize our initial tensorflow model
1609:24 - so here we have onyx gpu now this is 0.05 so it takes us now 50 milliseconds to run this
1609:33 - hug and face model so that's it now before we proceed it's also important to note that
1609:39 - generally the way we measure this time it takes for the model to predict or an output we usually
1609:48 - can test with several input images or we could repeat this process several times so let's say
1609:55 - for underscore in range let's say 10 we run this and then we get the time elapsed instead of just
1610:06 - testing once we could test 10 times so we could get the average so let's run that
1610:10 - and you see dividing this by 10 we have 0.035 so that's 35 milliseconds per prediction so this
1610:24 - means that for us to have 100 predictions we would take 3.5 milliseconds we could test that out
1610:35 - let's run this and there we go we take 2.3 seconds for 100 predictions now if we divide let's let's
1610:44 - say let's call this number of predictions uh we set this to be 100 and then the time taken this is
1610:52 - uh time time for a single prediction there we go time for a single prediction all this
1611:06 - divided by the number of predictions so we get the average that's fine so let's run that again
1611:13 - so we get this time okay so we see 0.023 seconds that's 23 milliseconds
1611:25 - while if we repeat the same for the hugging face model you see it takes 0.15 seconds
1611:33 - uh just as we had already now we could see that the the difference in speed here 0.15 divided by
1611:43 - 0.025 you see the onyx model is six times faster than the tensorflow model
1611:54 - now what if we get back and run the tensorflow model on the cpu so change this runtime
1612:01 - known and we save that so we'll have to rerun all this again now running our hugging face model here
1612:12 - you see this takes 0.8 seconds 0.8 so with the with the cpu we have 0.8 divided by 0.35
1612:23 - the onyx model runs about twice as fast as the tensorflow model
1612:35 - hi guys and welcome to this new section in which we are going to be looking at quantization for
1612:41 - neural networks in the previous section we looked at the open neural network exchange standard
1612:48 - which is an open standard for machine learning interoperability we saw that not only does this
1612:53 - onyx format permit us convert models from one framework to another but they also allow us
1613:01 - optimize our models for different hardwares and so in line with these optimizations we are going
1613:08 - to look at quantization which is a technique for performing computations and storing tensors at
1613:16 - lower bit widths than the usual floating points which we have been working with so far in this
1613:22 - course model quantization is a popular deep learning optimization method in which model data
1613:29 - that is both the network parameters and the activations are converted from a floating point
1613:36 - representation to a lower precision representation typically using 8-bit integers now defining
1613:44 - quantization in this manner may not seem very clear so let's try to understand first of all
1613:51 - why quantization or quantizing a neural network model is important so here let's consider this
1613:59 - very simplified model where we take in some input multiply by a weight and add the bias now
1614:06 - we have several layers so we just simply stack this up and we could say we have our model which
1614:12 - has already been trained and this model has a hundred million parameters and occupies let's say
1614:24 - one gigabyte of space so we have this model 100 million parameters one gigabyte of space
1614:31 - and if you're doubting what this space is for you should note that it's for storing
1614:38 - this weights and biases and so obviously the more parameters we are going to have
1614:44 - the heavier our final model file will be now supposing you want to use this in some setup like
1614:53 - a mobile phone so you want to use this in your mobile phone it means that you need to allocate
1615:00 - at least one gigabyte of memory space if you want to run this model and this is where the
1615:10 - techniques like quantization come in so now thanks to quantization instead of storing this weights
1615:18 - in a 32-bit space we are going to store them in 8-bit memory space so we're going from floating
1615:30 - point 32 to int 8 now if you're not familiar with the floating point arithmetic you could
1615:42 - check out this resource by Fabian Sanglar where he explains in a very intuitive manner the core
1615:50 - concept around floating point binary representation essentially if a single weight value that's your
1615:59 - model weight let's take this off if you have a model weight value which is for example 3.14
1616:06 - the way this is represented in memory is by first of all allocating this 32 spaces we have here
1616:16 - where each space takes a 0 or 1 at this first position here the 0 or the 1 is to specify
1616:26 - whether we're dealing with a positive or negative number and then for the next eight positions
1616:32 - we are going to see whether this value 3.14 lies in the range 2 to the minus 1 to 2 to the 0 or 2
1616:42 - to the 0 to 2 to the 1 or 2 to the 1 to 2 to the 2 and so on and so forth now in our case 3.14 lies
1616:50 - in this range and given that this exponent we have here is 1 we are going to apply this
1616:59 - formula where we have the exponent minus 127 should give us this power we have here
1617:09 - so we'll have that 1 so we have e is equal now 128 and if you convert 128 to binary notation
1617:19 - you would obtain this right here and now after encoding this integer position the next step will
1617:29 - be to encode this decimal value right here and that will be the role of this 23 other positions
1617:37 - remember this is only one box this is eight boxes and here we have 23 boxes for this eight we'll
1617:44 - see that it helps us locate our number in this range which we've seen already but for this other
1617:53 - 23 boxes we are going to suppose that since we have 2 to the 23 possibilities thus let's
1618:03 - try this here 2 to the 23 possibilities which is actually 8.388,608 possibilities so we have 8
1618:14 - million possibilities here this simply means that for every given range which we've seen here for
1618:21 - this range this range this range this up to the end we are going to divide it into 8 million
1618:29 - different parts and so if you see here you see you have 2 to the power of 1 this is 2 to the
1618:36 - power of 2 if you break this gap if you break this here this here this gap into 8 million different
1618:46 - parts or better still if we consider that the distance to move from 2 to 4 is 8.388 million
1618:56 - then the finding this 3.14 or encoding this 0.14 right here or let's just say encoding 3.14
1619:05 - will until calculating the distance from 2 right up to 3.14 knowing that this distance from 2
1619:13 - to 4 is 8.388 million we cannot compute this distance by simply doing 3.14 minus 2 that is 1.14
1619:25 - divided by all this distance that's 2 so we find this and then multiply by the 8 million we have
1619:37 - 4, 7, 8, 1, 5, 0, 6 so now we shall convert this to binary and we obtain this here so once we obtain
1619:47 - this we then fill up all this 23 spaces right here and that's essentially how a number like
1619:53 - this is stored in memory and so getting back here if we have to store this let's say 3.14
1620:01 - 3.14 which was previously stored in this 32 box memory and now we want to store it in an 8 box
1620:09 - memory now we move from 1 gigabyte to 256 megabytes here we have 256 and our mobile phone will now
1620:21 - need only 256 megabytes of memory to run our model now it doesn't just suffice to say we are
1620:30 - going to go from the floating point 32 to the int 8 we need to describe exactly how this is done
1620:39 - and the way it's done is actually by a simple linear mapping where we shall start by defining
1620:47 - two ranges of values the first range is for the floating point values and as you could see here
1620:55 - the defined negative a max to a max well one good thing about deep learning models is most times
1621:04 - your weight or your weight values lie between negative 1 and 1 and so getting back here we
1621:12 - could have here negative 1 so a max will be 1 and so negative a max is negative 1 and then a max is
1621:20 - 1 so we go from negative 1 to 1 and then if we want our output to be unsigned ints instead of
1621:30 - going from negative 128 to 127 we shall go from 0 to 255 now notice that the number of values we
1621:43 - have between 0 and 255 is the same as number of values we have between negative 128 and 127
1621:50 - but with the unsigned ints all our values are positives so instead of the int we have unsigned
1621:57 - int 8 and so at this point our aim is to take values ranging between negative 1 and 1
1622:05 - and map them in the range 0 to 255 and now we will use a simple linear function which has the form y
1622:15 - equals to ax plus b now our wire will be the output value so we'll have the let's call this x we'll
1622:26 - call this x quantized this equal x floating value so this is the original value of the weight let's
1622:34 - let's put this in blue we have the original value of the weight or the float value divided by
1622:41 - the certain scale plus a zero point value we'll call this z so simply one over s is equal a and
1622:54 - b is equal z then y is x q and x is x f so now our aim is to look for the value of s and z
1623:03 - such that when we have any value in this range we get its corresponding value in this other
1623:11 - range the way we'll get s let's have this the way we get s is by doing x float max x float max you
1623:24 - say that x float max is simply a max minus x float mean which is in this case negative a max divided
1623:34 - by x quantized max minus x quantized mean now if you replace all this by the corresponding values
1623:46 - we have here we would have one minus negative one divided by 255 minus zero so 255 minus zero
1623:56 - this means we have two divided by 255 that's our s which is our scale and then z our zero point is
1624:07 - x q max minus x f max divided by s this s we just had here so if we replace again we have x q max
1624:17 - which is 255 minus x f max which is one so we have 255 minus one divided by two on 255 that will give
1624:27 - us 255 divided by two so essentially 127 127.5 so that's what you get now the way you can look at
1624:39 - this zero point is it's the quantized value we get when the floating value is zero so when we
1624:49 - convert when we have zero here we have zero on s which is zero um the quantized is the quantized
1624:55 - value or the corresponding quantized values equals z so that's why we call that the zero point
1624:59 - and then s which is the scale simply scales our inputs here as we go from this range of values
1625:10 - to this other range now you could take a simple example where you could leave from um x f to x
1625:18 - q if you have negative one here you see you have let's say x q is equal negative one that's x f
1625:26 - let's suppose we have negative one then divided by s we said s was 250 two on 255 um plus z
1625:35 - z is 255 divided by two so this gives us zero which makes sense as we go from negative one
1625:43 - to negative one one two zero two fifty five this means that this here or this boundary values
1625:50 - should be almost the same now let's take another example in the middle let's say negative
1625:57 - or let's say zero point three so we could have x q which is zero point three divided by two on
1626:07 - 255 plus um 255 divided by two so in this case now we have a value of 165.75
1626:24 - which if we run up we could have 166 so essentially we're going from 0.3 to 166
1626:32 - and apart from running the output as we've just done we would also see that we could
1626:39 - clip any outliers so in case we've decided to have here a max or our x f max to be one
1626:49 - and that it happens that we have a weight or weight value which is more than this one
1626:55 - then the output um unsigned int would be 255 so any value greater than this is going to take
1627:05 - this value any value less than this is going to take this value of zero and so we've seen how
1627:13 - this um simple technique permits us reduce our memory used in storing the weights
1627:22 - now it's logical that we're going to have a drop in the accuracy because if you've trained a model
1627:30 - for example to have certain floating or certain weights which are actually floats and then you
1627:36 - convert these floats into integers where you have some extra transformations like the rounded and
1627:41 - the clipping then you'd expect to have a drop in the performance of the model nonetheless
1627:49 - this huge gains in terms of memory are enough for us to sacrifice a bit of the accuracy or more
1628:01 - generally the model's performance and that said apart from this model weight size which is dropped
1628:10 - or reduced it should be noted that arithmetic operations like multiplication and addition
1628:17 - of our quantized integers can be carried out even much faster and so we not only have a model which
1628:24 - occupies less space but a model which is even much faster we have generally three ways of
1628:32 - carrying out quantization the dynamic quantization static quantization and the quantization
1628:41 - are where training now given that during the quantization process the weights and the
1628:50 - activations are stored at lower bid weights in the case of the dynamic quantization this
1628:59 - quantization parameters does a scale under zero point which we've seen already for the activations
1629:05 - are computed dynamically or on the fly and because this year have to be computed dynamically
1629:14 - there is an increase in the cost of inference so it will take a little bit more time to produce
1629:20 - an output as compared to other methods like static quantization nonetheless year we usually
1629:27 - achieve higher accuracy compared to the static quantization methods for the static quantization
1629:34 - method we first of all compute the quantization parameters using a much smaller data set which
1629:42 - we'll call the calibration data so essentially we have our model and in here we have our different
1629:49 - quantization parameters but instead of dynamically computing these different quantization parameters
1629:58 - we are going to pass in some input and output carry out several runs such that we are able to
1630:05 - obtain the most appropriate quantization parameters based on this data we've passed in and then now
1630:15 - when we want to run or carry out inference we do not need again to compute these parameters
1630:23 - unlike with the dynamic quantization where at inference time we always have to compute these
1630:29 - quantization parameters here we compute these quantization parameters before via calibration
1630:37 - data and then now when we run an inference we just pass in inputs and we already have
1630:43 - the quantization parameters set but the problem now with this method is that if this calibration
1630:50 - is done poorly then we would have low quality values for the scale under zero point and so
1631:00 - because of this we would then have a lower accuracy as compared to the dynamic quantization
1631:07 - method that said these two methods we've just seen are post-quantization methods so dynamic
1631:14 - and static is post-quantization meaning that we train the model first in floating point 32
1631:22 - and then after training the model we convert this model to one with weights and activations which
1631:29 - are unsigned ints now sometimes the post-training quantization that is PT cure is not able to
1631:37 - achieve acceptable task accuracy this is when you might consider using the quantization aware
1631:43 - training that's QAT the idea behind the quantization aware training is simple you can improve the
1631:49 - accuracy of the quantized models if you include the quantization error in the training phase
1631:56 - so unlike post-training quantization where we train the model first before quantizing here the
1632:04 - network adapts to the quantized weights and activations during the training so as we were
1632:11 - saying we include a quantization error in the training loss by inserting fake quantization
1632:20 - operations into the training graph to simulate the quantization of data and parameters these
1632:26 - operations are called fake that's fake quantization because they quantize the data but they
1632:34 - immediately de-quantize the data so the operations compute remains in float point precision that said
1632:42 - the post-training quantization is more popular than the quantization aware training method
1632:48 - thanks to its simplicity as it doesn't involve the training pipeline the quantization aware
1632:53 - training almost always produces better accuracy and sometimes this is the only acceptable method
1632:59 - and that's it for the section in which we've looked at quantization of neural network weights
1633:05 - and activations to help reduce model size and also speed up computations
1633:16 - hello everyone and welcome to this new and exciting session in which we are going to see how
1633:23 - we'll move from a tensorflow model which occupies one gigabyte of space to an
1633:30 - unequal quantized model occupying just 83 megabytes at this point we now understand
1633:37 - the concept of quantization and we're going to see how to apply or implement quantization
1633:44 - specifically dynamic quantization to make use of our model even more efficiently
1633:51 - before we move on we should also note here that the tf size tf size is
1634:01 - one gigabyte it's about approximately one gigabyte that's 1000 megabytes while the onyx size
1634:11 - is 328 megabytes so onyx size 328 megabytes now we're going to look at the onyx quantized
1634:24 - let's just copy this so we have the onyx quantized cpu onyx quantized cpu
1634:32 - which we shall get shortly the onyx quantized GPU and then we'll also get the onyx quantized
1634:44 - size so let's take this off for now and then get back here now here as you could see we've
1634:51 - imported the onyx onyx runtime quantization we've already imported the onyx runtime so here
1634:57 - we just imported this quantized dynamic and quantized then from here you see we have
1635:06 - the two models that's the floating point and the quantized model now this year this model
1635:12 - is what we had already so we'll get back and then copy that and then we'll call this one
1635:20 - the vid quantized so that's it now all we need to do is we have this quantized dynamic which takes
1635:27 - in this model takes in the path to the quantized model it's still an onyx model and then the weight
1635:35 - type now here this weight type is an unsigned int so let's run this and see what we get
1635:44 - we get in an error this name isn't defined okay we should run this before running this one
1635:51 - so that's it this should be fine this time around and we should be able to get this
1635:57 - file right here this quantized onyx file again here you could check out documentation on quantizing
1636:05 - onyx models you see we have quantized onyx model we have an overview and all of that so let's get
1636:12 - back here and check out our model oh there we go we have our vid quantized and what we notice is
1636:19 - we have 83 megabytes so this means that we've gone from one gigabyte or 1000 megabyte to just 83
1636:32 - megabytes and now we could go ahead and check out the speed or the cpu speed so let's get back here
1636:41 - or rather let's let's get back here there we go copy this path let's copy this path
1636:50 - and then we have this year we paste that out here and then this this one here this is an cuda
1636:59 - this this is cpu cpu execution provider that's fine everything looks fine and let's run this
1637:07 - you see we get 0.39 that's practically 0.4 seconds per prediction and so here we have 0.4 seconds
1637:21 - now let's let's check this out again um let's check this out again for the original onyx model
1637:29 - so let's run this and check this out you see here we have 0.49 that's practically 0.5 so it
1637:40 - means that this isn't exactly true this should be 0.5 so we see that the quantized model
1637:47 - uh is faster and way much lighter than the onyx the original onyx and the tensorflow models
1637:59 - but we have to be careful as quantization generally comes with a drop in the accuracy
1638:08 - now we switch to a gpu and then test out our quantized model so right here we have this
1638:15 - quantized model which we're going to run and then check out its latency here is where we get 0.27
1638:24 - let's say 0.3 and this shows that the quantized model doesn't benefit as much as the onyx model
1638:35 - from the usage of the gpu now if you check out the documentation we'll see that there is quantizing
1638:41 - uh an onyx model and this is quantizing on a gpu so the quantization on the gpu
1638:47 - isn't as straightforward as that with the cpu as here we do that we will need a device that
1638:55 - supports tensor call int 8 computation like the c4 or the a100 and let's say here that
1639:03 - order hardware would not benefit from quantization though if you want to proceed with quantization
1639:09 - or quantization with a gpu you can make use of this tensor RT execution provider and here they
1639:15 - give the overall procedure to leverage this tensor RT execution provider so with that we're going
1639:22 - to get back here and the next thing we shall do is ensure that the quantization process
1639:29 - hasn't led to too much drop in accuracy as when we quantize the model generally we may have drop
1639:35 - in accuracy but our aim here is to be sure that this drop is minimal and so to do this we are
1639:42 - going to evaluate our model so here basically we'll define this accuracy which takes uh the model
1639:50 - and then for a hundred we'll take a hundred elements a hundred elements in a validation
1639:56 - data set where the evaluation data set as a byte size of one we are going to compare each time
1640:02 - the output or the unexpedition with the level so here we compare the level with the unexpedition
1640:09 - and if they are the same we increase the accuracy variable value year by one initially they are zero
1640:14 - total accuracy is zero but the total is always increased and then the the accuracy is increased
1640:21 - only when we have this two the same so with this basically we implement this accuracy
1640:26 - uh method which now we take the two models that is the onyx the original onyx and the quantized
1640:33 - onyx so here we have these providers to to to make this run faster so we run this now you see here
1640:40 - we have 90 for the original and then 89 for the quantized model the next thing we'll look at will
1640:49 - be how to visualize onyx models using Lutz Rodas Neutron app so you could get your Neutron app and
1640:57 - you you have this interface right here now we're going to open the model there we go it's loading
1641:04 - and here's what we get you see we start with this transpose you could recall we have this transpose
1641:10 - and then we moved on to this resizing then matrix multiplication and then the rest of the VIT model
1641:20 - right here so here is the VIT model in this onyx format onyx quantized format
1641:27 - screwed right to the end right here and then towards this end you see we have this
1641:35 - matrix multiplications for a linear layer and then we have this softmax you could also export
1641:43 - as png so you could open this up in this png format right here so that's our model
1641:53 - and that's it for the section where we've left from a 1 gigabyte model to an 83 megabyte model
1642:01 - with just a 0.01 drop in accuracy
1642:11 - hi there and welcome to this new and exciting session in which we are going to treat quantization
1642:17 - our world training with tensorflow now in some previous sections we started by explaining what
1642:25 - quantization is all about and the advantages of quantizing models we also looked at different
1642:33 - quantization methods and we looked at the relative advantages and disadvantages of these different
1642:41 - methods that said we are going to see in this section how to quantize a full model or just some
1642:48 - layers which make up that model in tensorflow the special model which permits us carry out
1642:55 - quantization is this tf.mod which stands for tensorflow model optimization and so here we start
1643:05 - by installing this tensorflow model optimization model and then we'll import this as tf.mod
1643:13 - then since we want to do quantization we could get in here Keras and that's it here we have
1643:19 - different methods and classes let's get into this quantize model right here as you can see
1643:25 - quantize a Keras model with a default quantization implementation and so here we have to simply pass
1643:31 - in this to quantize argument which is the model to be quantized and then we should get a quantization
1643:38 - aware model so here for example you see this model define the sequential model and then we
1643:45 - also have this functional model then to quantize this we just call this method quantize model
1643:51 - right here and then we pass in our model and we have our quantization aware model so that said
1643:59 - let's go ahead and implement this here we have our model in this case let's let's start with our
1644:06 - hugging face model so we have this our hugging face model which we've declared already and now
1644:12 - let's say we want to have our quant aware hugging face okay so we want this quantization aware
1644:19 - hugging face model and then we want to use tf.mod.quantize model so basically let's copy this
1644:29 - here and then paste it out in the code so we have that paste it out and then here we have our
1644:37 - hugging face model so from this we're running this should give us our quantization aware model
1644:43 - now we get an error quantizing the tf keras model inside another tf keras model is not supported
1644:49 - so as of now this isn't supported now um let's try out with the efficient net model though this
1644:58 - should it should be the same error because the efficient net model let's get up here
1645:05 - the definition for the efficient net model first of all you can see the hugging face model you
1645:09 - have this model in this model so that's the reason why that doesn't work and then we we get to the
1645:18 - efficient net model transfer okay so we have this model right here and we could see that we have
1645:26 - this keras model which is this backbone here in this model and so if we had to use this we have
1645:32 - to look for a way to break this backbone up into its different layers but as of now what we've been
1645:40 - doing is just making use of this backbone as is here we just have this backbone and that was it
1645:46 - we didn't actually break this model up into different layers now that said let's copy out
1645:53 - this efficient net model right here uh to have to take all this off and then now we are no longer
1646:02 - making use of this input right here so we wouldn't use this we'll use the the backbone's input
1646:08 - directly so let's look at that we have this x so from here we have the backbone's output
1646:15 - we should get into this global average pooling layer so here we have backbone output that's it
1646:22 - we have this output which gets into the global average pooling we have this x here that's it
1646:30 - which now passes through this dense layer and then to the batch norm layer and then to this
1646:38 - dense layer search that we have an output right here okay so we have that let's pass in this x
1646:45 - values there we go and finally we have this then from here now we create our model so
1646:54 - it's our pre-trained our pre-trained model is a Keras model and which takes inputs
1647:03 - the backbone input so now our backbone input is our input and then our output
1647:11 - is simply this output right here so we have that output okay so we have this set now and
1647:21 - everything should work fine so let's run this here and what do you notice you will notice that
1647:28 - the Keras model what we had previously as our Keras model has now been broken up so you could
1647:34 - see i think we should have just um set uh here pre-trained functional model let's call this
1647:44 - functional model okay so let's let's go back and run this other cell here let's get back here
1647:54 - um pre-trained model
1647:55 - uh okay so let's run this again this is our pre-trained model and then so that we could
1648:04 - we could run this um summaries down here and you could see Clara so you see here we have this
1648:11 - model is exactly the same model we're dealing with so far this is the exact same model
1648:16 - what we want to do is just to paste this so have the pre-trained so this is a pre-trained model
1648:22 - uh let's get a summary pre-trained model summary run that let's reduce this so you could
1648:30 - get into the space we have this and you see we still have this exact same total parameters
1648:37 - is here the same number of parameters number of non-trainable parameters exactly the same
1648:43 - so it's basically the same thing but the difference here is we do not have this
1648:48 - um let's open that up again we do not have pre-trained
1648:55 - summary we do not have this carat model here so we do not have this uh model right here
1649:04 - and so because we don't have this now it will be possible for us to make use of this method
1649:10 - and quantize our full model so that's it we have this pre-trained model now let's run this again
1649:20 - so we we have this pre-trained pre-trained functional model let's run that okay we have
1649:30 - our model set and now what we can do is we would run this now so let's run this again and see what
1649:38 - we get just increase the size and there we go we get another error this uh the same error actually
1649:47 - let's get back here oh this should be pre-trained functional so let's run this
1649:55 - now it's taking more time hopefully everything should work well
1650:00 - now instead we're getting this other error yeah well we told that this rescaling is not
1650:05 - or this this layer here is not supported
1650:11 - and this is normal since here in this rescaling layer we do not have any weights and so we are not
1650:18 - going to be carrying our quantization for such layers so um what we can do now is instead of
1650:24 - quantizing the whole model we'll select some layers we want to quantize so here there we go
1650:32 - what we'll do is instead select some layers so this means that if we had let's define a simple
1650:37 - model so let's let's uh get back to the top and then we define for example this learned model
1650:45 - without this resize rescale so that's it uh quite simple model we have that now let's run this
1650:54 - let's run this cell and then oops we're getting an error so that's because we we took up the
1651:01 - resize rescale i would not specify this your exact for an exact uh input size so let's run
1651:08 - this again that's fine now we have our lunette model let's do this lunette model and run that
1651:19 - we get another error let's check that out this batch norm is not supported so you cannot
1651:24 - quantize this batch norm layer so what we'll do is let's let's basically remove the batch
1651:29 - norm layers but later on we'll see how to have to to to to quantize only uh some layers so for now
1651:36 - let's just remove this batch norm layers uh batch norm off drop out let's take the drop out to
1651:44 - batch norm off and that's it uh so we have that let's run this again uh see what we get
1651:52 - okay so that's fine you see now uh we've been able to make this lunette model quantization aware
1652:00 - and we've done this for the whole or the full model now in cases like this year this model here
1652:08 - this uh efficient net model where we have this backbone which is uh our pre-trained uh backbone
1652:15 - we cannot start taking off the normalization layer for example here and taking off this
1652:21 - rescaling which comes with the backbone and so on and so forth so what we'll instead do is we'll
1652:25 - move layer by layer and select the layers which we want to actually uh make quantization aware
1652:34 - so that's basically what we'll do and so instead of proceeding as we did here with this lunette
1652:40 - that is uh quantizing the whole the full model we're going to go layer by layer so with that
1652:47 - we could comment that section there and now take this model off now in order to quantize
1652:57 - only some layers of the model we'll make use of this quantize annotate layer method right here
1653:04 - so we see again we have quantization kiraz quantize annotate layer and this takes in
1653:11 - the model to annotate with some quantization configurations so here what they explain is
1653:16 - this function does not actually quantize a layer it is mainly used to specify that the layer should
1653:23 - be quantized so you see it's there to specify that the layer should be quantized and so the
1653:31 - layer then gets quantized accordingly when we do a quantize apply so this is the quantize apply
1653:37 - method here click open that and that should be it so let's get back uh oh let's let's just get
1653:44 - let's just look at this example here where you see this layer you see we have this model but in
1653:51 - this model we want to quantize only this layer and so as you could see we have quantize annotate
1653:55 - layer and then once this is done we do a quantize apply to get our quantization aware model which
1654:04 - here is called quantized model so let's go ahead and see how to implement this with our pre-trained
1654:10 - efficient net model we'll now define this method apply quantization
1654:20 - to the conf layers which takes in a layer and then if that layer the name all right here if
1654:29 - if this conf is in the layer name we are going to carry out the quantization so we're going to
1654:40 - apply the quantization on the conf layers so here we have layer okay so in the case where we don't
1654:48 - have that we'll just return the layer itself so the layer remains unchanged whereas conf layers
1654:58 - will become quantization aware so we have this apply uh matter right here which will run
1655:05 - there we go now once we we have this method defined we'll make use of this clone model
1655:13 - method right here to create a new model but one which takes into consideration a certain
1655:21 - clone function we get back here and then we paste this out oh we wouldn't making use of this in
1655:28 - input tensors we'll just make use of the clone function and our clone function here is this apply
1655:33 - quantization to the conf layers so that's it now you you check this year if you check out here
1655:42 - you'll see that wherever we have the conf layers you see like this one we have this conf uh yeah
1655:48 - we have this count for the depth wise convolutions and so on and so forth now you could also include
1655:54 - this for the expand and reduce layers but let's just work with only those counts so that's it you
1656:03 - now understand how to pick out certain layers or how to leave out others from the quantization
1656:12 - awareness process so from here we have this apply right here and then we'll call this our
1656:19 - quant aware efficient net so that's it we have this quant aware efficient net and then we run
1656:28 - this uh no this model this model here has to be our our pre-trained model so it's our pre-trained
1656:35 - model we run that again and now this should be fine okay so we have our quantize our model
1656:42 - which is now quantization aware and you notice that when we do quant aware efficient net summary
1656:49 - we should get something slightly different from what we used to get in um we have in this
1656:58 - now oh this is no this should be func so we should have func model let's run that again
1657:05 - and run now let's run this let's get back here and you see we have this quant aware efficient net
1657:16 - and you will now notice that let's get back to the top you will notice that wherever we have
1657:23 - this uh conv layers see wherever we had the conv layers we now instead have this quantize annotate
1657:32 - so as we scroll you wouldn't see a conv layer but instead we have the quantize annotates
1657:39 - so that's it but uh yes yes because this didn't have there's no conv in this name so we could
1657:45 - as we said before we could include this as we expand and as we reduce um layers so that's it
1657:51 - we we now have this quantize annotate layers which wasn't what we had before making this model
1657:59 - quantization aware or some layers of this model quantization aware so with that now
1658:06 - we are done with the annotation and we are ready to make this actually quantization aware
1658:11 - so here we call this quant aware model um yeah this is quant quant aware let's get this exact
1658:22 - name right here quant aware efficient okay so we call that quant aware efficient there we go
1658:30 - that's it and now we have our quant aware model let's run this again and then see what we get
1658:35 - as a summary and that's it is now quantization aware so we know we no longer having the
1658:42 - the annotations but now some wrappers so here you see the the layer name but now we have this
1658:50 - quant which is added to these layers um let's scroll down check out on this you see we have
1658:56 - this other ones here and so on and so forth so that's it we we we now have our quantization
1659:02 - aware model and we're now ready to compile this model and train it like every regular model
1659:10 - we get back to our training right here and then at a level of this compile we have your quant
1659:18 - aware model that's it um this this is the same linear rate so that's it okay let's run this
1659:26 - and then here we also have our quant aware model there we go so let's run this
1659:36 - we're getting this resource exhausted error so i'm going to restart the session and hopefully
1659:42 - everything should work fine there we go we started the session and now we able to train anyways we
1659:49 - see how to implement quantization aware training with tensorflow and in the next section we'll
1659:57 - dive into post training quantization
1660:06 - hello everyone and welcome to this new and exciting session in which we are going to look
1660:12 - at post training quantization with tensorflow in the previous session we looked at quantization
1660:19 - aware training still with tensorflow and now we'll look at how to do quantization for model
1660:27 - which has already been trained now as you could see here we have this pre-trained model
1660:33 - model which obtains an accuracy of 84 percent and a top five accuracy of or rather top two accuracy
1660:41 - of 95.6 percent and then in the section we'll quantize this model check out on whether this
1660:50 - quantize model occupies less space as compared to the original model and then also verify that
1660:58 - not much model performance is lost before we start with the quantization process
1661:05 - we should note that we are going to be using this tensorflow light library now this tensorflow light
1661:11 - library is a mobile library for deploying models on mobile devices microcontrollers and other
1661:20 - edge devices so this means that in these environments where the compute resources
1661:25 - are limited it's important for us to quantize the models since we will now get smaller and lighter
1661:33 - models and also faster models here we have a general overview of how this works you will see
1661:40 - you pick a model like for example efficient net based model convert this model to tensorflow
1661:47 - light using tensorflow light converter which we are going to see shortly we then deploy this by
1661:52 - taking a compressed version or compressed tf.light file now we were already working with for example
1661:59 - Keras files which are hdf5 files now this tf.light is some sort of compressed file and this
1662:09 - compressed file will be loaded in the environment in which you'll be working in and then from here
1662:16 - we will also quantize this from 32 floats to 8 bit integers which can run on devices
1662:24 - with low compute resources so here in the documentation we have here tf.light and you
1662:30 - have tf.light converter basically this tf.light converter as the word goes is going to convert
1662:38 - your models into the tf.light format and here as you can see these examples you have
1662:43 - the tf.light converter from a saved model from a Keras model from a function from a GX model
1662:53 - so let's say for example we work with a Keras model just have the model passed here and then
1663:00 - you generate this tf.light model from this model making use of the tf.light converter
1663:07 - now apart from these arguments we also have the attributes so here you could specify the
1663:14 - optimizations the representative data set which is very important in the case where we're working
1663:20 - with static quantization remember that with static quantization we have to obtain the scale
1663:27 - and zero point values by making use of unlabeled data so basically as we had seen previously all
1663:36 - we need to do is to pass in the inputs which in this case are images and then these values
1663:43 - will be inferred from the model's interaction with the inputs then we have the target specifications
1663:50 - inference input type inference output type whether to allow custom operations or not
1663:57 - and then whether to exclude the conversion metadata or not so that's it that's our converter
1664:04 - right here now we paste out this code from the documentation and then let's let's take this
1664:10 - first here now apply some of the attributes that we have this converter dot optimizations
1664:19 - let's get back to the documentation the optimizations and here we could
1664:25 - set this optimization with this tf.light optimize so let's open this up there we go
1664:32 - you see this takes different values we have default whatever we want to optimize for size
1664:38 - this is deprecated does the same as default optimize for latency does the same as default
1664:44 - this one is experimental hence subject to change so what we're going to do here is simply take
1664:49 - this default so that said we have the tf.light tf.light.optimize and default just as we had
1665:07 - in the documentation right here now we could also specify the inference input type and the
1665:13 - inference output type so let's get back here we have there we go converter inference input type
1665:26 - is going to be unsigned int eight and then let's copy paste this we have the output type which is
1665:35 - going to be the same so that's it so we specify this the inference input type and the inference
1665:41 - output type and now let's specify the representational data so this representative data set right here
1665:52 - which in fact is a generator which permits us output the input values because recall we all we
1666:00 - need in the static quantization is just as inputs so here we have the generator which yields the
1666:08 - inputs and then here we just say converter dot representative data set equals a representative
1666:17 - data generator here we have our training data set now we could take we could take all our training
1666:25 - data set or just a few we obviously don't need to take all the data set so we could just take like
1666:31 - 20 and use that to obtain the values for the scale and the zero point now if you new to this
1666:39 - notion of scale and zero point it's important you check our previous sessions where we treat this
1666:44 - so let's run this here we also run this and now we set to convert oh this model here is pre-trained
1666:52 - our pre-trained model so let's run that and that should be fine so we now set to carry out the
1666:58 - conversion now we're done with the conversion we are now going to save this in the tf.light format
1667:05 - so we have this path and this path we're going to have this file so let's run the cell
1667:13 - and that's fine so we get that we have the the file size here and when we check this up you see
1667:20 - we have this 21 megabytes so we're going from this model which we could check out here we're
1667:29 - going from this model which is 90.7 megabytes to a 21.12 megabyte model now before moving on we
1667:40 - should note that if we want to implement dynamic quantization here then we wouldn't specify this
1667:45 - representative data generator so that's it let's get back we install this tensorflow light runtime
1667:59 - now talking about installing the tensorflow light runtime once we already have this tensorflow
1668:07 - light file right here if we want to run this in some other system say for example want to run this
1668:14 - in our raspberry pi all we'll need to do now will be to install this runtime and that'll be it
1668:21 - we wouldn't need to install tensorflow any longer so we just have this we run this
1668:26 - that gets installed we import the the tf.light runtime then we prepare our test image so we just
1668:35 - run this we've seen this already we have our pre-trained model we're going to get the
1668:40 - argmax and then the corresponding class so that's it we should get angry see it matches with what we
1668:48 - expect we could try out this other example here let's run this and there we go so this our model
1668:57 - now we're going to use this runtime to run our tensorflow light model now we've restarted the
1669:06 - session and you'll see that without tensorflow let's let's let's do this let's say tf zeros
1669:15 - and one by two for example we run that and you see this is not defined so we have no input for now
1669:22 - now let's take this off and then we get back up here we install our runtime import the runtime
1669:31 - uh i think we'll be needing numpy so what we'll do is we're going to take this numpy
1669:39 - so we try we tend to work without necessarily needing tensorflow so we have this imported as
1669:46 - numpy that's it here we have this test image open cv so import cv2 that's fine
1669:58 - uh well here we we make use of tensorflow but we will see how to get rid of this dependence
1670:06 - on tensorflow so first of all here we have to note that tensorflow was used here to convert
1670:11 - this test image here let's print out our test image to convert this test image which is uh an
1670:20 - unsigned int with it be so here let's do this you see it's an unsigned int i wanted to convert this
1670:29 - into a float and that's why we made this uh change here so here we wouldn't need this any longer
1670:39 - and here we could use numpy so we have that and everything looks fine okay so we have this
1670:47 - uh test image and that's fine so now let's run this and we have our image now image is not defined
1670:55 - let's get back here oh no this is the test image test image run that again this should be fine
1671:04 - okay so we have now our image and then here we see we have this interpreter which loads
1671:11 - our tensorflow light file which we've saved in the drive and then we allocate tensors once
1671:17 - this is done we move on to get the details the input and output details which we had from the
1671:26 - conversion process now here here we have uh the unsigned int and here we also have the unsigned
1671:32 - int and then you would see that um we have this test image which we would change the type so first
1671:41 - of all you notice that this image is tend to a numpy array which we don't need any longer because
1671:46 - it's already numpy and then even this type we do not really need to do this although um if you
1671:52 - print this let's comment this section and if you print this input details and we get the data type
1672:01 - you will see that we have an unsigned int now tf not uh defined well here we use a tf.light
1672:10 - so let's let's have this run that again we have tf.light runtime has no attribute interpreter
1672:17 - now what we'll do is we'll have this dot interpreter okay so we should have this and this should work
1672:25 - now okay that's it now we have this we run this again and that's fine so you see we have you see
1672:33 - the unsigned int which is what was expected because when doing a conversion we have specified
1672:38 - that we wanted this uh data type for input and our output so that's it let's take this off what
1672:46 - we're saying is we don't necessarily need this step right here so uh this will be useful if we
1672:53 - had this as a tensor as a tensor tensor for tensor and if we did not have this as an unsigned
1673:01 - int already so now that we have that you see we set the tensor so here we have our test image
1673:09 - that's it and then the input details index you could print this out so you see what's in here
1673:15 - input details index as you could see it's zero though now in this line we get an error
1673:25 - we got three but expected four for the input so let's get back here um oh okay here we here
1673:32 - we had this change to in let's have we actually let's get back so what we're saying is here we
1673:40 - have this expand dims to get from three dimension to four dimensions and the name was still um so
1673:50 - let's have that we run this again here we have our test image now which has uh four dimensions
1673:57 - and then now this should work so here we set the tensor and then we run the inference so that's it
1674:04 - we run the inference here and once we run the inference we should be able to get the tensor uh
1674:12 - at the level of the output so let's take this off and then run this now
1674:20 - takes a while yeah this there this is the inference uh process now it should be noted that
1674:26 - tensorflow light has been built for mobile and embedded CPUs so uh general purpose CPUs like
1674:36 - this collapse CPUs aren't the best match for tensorflow light models in terms of speed
1674:43 - now we print out the output uh i could just let's print out the output see what's what's in there
1674:50 - see there we go you show that we have this here the highest value then we could do np.arc max
1674:58 - of this output right here run that again that's it uh now let's do let's take this here so we
1675:08 - could get the class name automatically then now let's run this and we get the class happy
1675:15 - so this is what we expected and again we've done this without having to import tensorflow so we
1675:23 - did not we didn't need tensorflow once we already had our tf.light model you could see here tf not
1675:32 - defined now our next step will be to measure the accuracy of our tensorflow light quantized model
1675:40 - so here we do exact the same process as we had before we basically have the model path
1675:46 - and then this input details output details and then we go to our validation data set take
1675:53 - a hundred elements in our validation data set so this means that we'll have to import tensorflow
1675:59 - for this process since we are trying to evaluate the model's performance so here we have this
1676:06 - we'll need the validation data set so we'll need to get back and run this cells here so let's get
1676:14 - back here we will run this there we go we will be running all the cells now when we want to
1676:22 - import this when tensorflow is already imported we get in this error so what we'll do is we just
1676:28 - have this here we run that there we go and then when we get to this accuracy you will notice we
1676:35 - use that the tensorflow light so it's tf.light not tf.light so we're using this model from tensorflow
1676:44 - and not from this package which we had installed here so that said now we have that set our accuracy
1676:53 - we have input output as we're saying our validation data set test image which is going to be passed
1677:01 - here the inference we get the output we compare if they're the same we increase accuracy if not
1677:09 - we skip and then we move on but to increase the total and then from here we have accuracy divided
1677:15 - by total or let's let's say positives divided by total or let's say correct correct yeah correct
1677:24 - predictions so let's change this to correct so here we have correct predictions and that should
1677:33 - be it so we we have this accuracy here and then we specify the model path let's get back here
1677:39 - and then take this path of our tf.light model now we we have simply accuracy accuracy and then we
1677:51 - specify that path okay let's have this and we run this now there we go we're done with
1678:00 - computing the accuracy for our tf.light model and we get 0.82 that is 82 percent as compared to
1678:08 - the 84 with the original model now to get the more accurate value for this it's advisable to
1678:18 - use the whole data set so you could take this off and so now you have your model which performs at
1678:27 - 82 accuracy and which now could be deployed in some mobile device
1678:32 - hello everyone and welcome to this new and exciting session in which we are going to look at
1678:42 - apis now api stands for application programming interface and in this section we're going to
1678:50 - look at why we even need apis and also how they work now supposing you've just built this model
1678:59 - right here let's call this a model m1 and this model can take in an input and produce an output
1679:09 - the question is how do i make my web app like this one or my mobile app or even my desktop app
1679:17 - access this model which i've built such that a user of let's say this mobile app can
1679:26 - and just by pressing a button get access to this my model's predictions
1679:35 - the way we could go about this is by making use of apis that's application programming interfaces
1679:43 - as defined here on the g2.com website an api permits software development and innovation
1679:52 - to be easier by allowing programs that is for example your web apps to communicate data and
1680:00 - functions safely and quickly apis does application programming interfaces accelerate innovation
1680:08 - because more developers can build products based on existing data and functionality
1680:14 - getting back to our example this means that thousands of users now on the web can get access
1680:23 - to our model and make predictions without these developers of these web apps or mobile apps
1680:32 - necessarily mastering the art of model creation to even make these concepts clearer let's take
1680:41 - this example so here we're supposing that you get into a restaurant you give a command to the waiter
1680:48 - who then takes this command and then tells the cooks to make available the food you have in the
1680:55 - command then once this food is ready it sends it back to the waiter that's the api and then this
1681:02 - waiter now passes on this food to you or then eats it and is happy now in the case of computer
1681:10 - software where we have say this mobile app which has to communicate with this api right here
1681:18 - then this communication has to follow certain rules or protocol known as the http
1681:27 - this protocol guides the way information is being transmitted via the web now http stands for
1681:34 - hypertext transfer protocol so this is hypertext then transfer protocol now the type of data
1681:43 - exchange could be text could be images could be video could be any kind of data which is
1681:50 - understandable by both this client right here and the server also communication following this
1681:58 - http protocol is connectionless that is each time this client needs to communicate with the server
1682:07 - to know what objects are found in the particular image a connection is created between these two
1682:14 - now once the connection is created and then the client receives the output that is the
1682:20 - particular location where the objects are found in an image that connection is closed
1682:25 - and so we we now have this which is taken off here then once we want to connect again with the
1682:33 - api which is found in the server we still create or recreate another connection then also as we
1682:40 - have said the data exchange here isn't of a particular type so really this protocol doesn't
1682:50 - force you to pass or receive a certain type of data so far as the data you're passing
1682:56 - or receiving is understood by the particular entity which is sending or receiving
1683:03 - then everything works just fine then the last property of http protocol is it's actually
1683:09 - stateless that is once you make a request and then you receive a response no client information is
1683:17 - being stored in this request response cycle hence once data is being passed to the api
1683:26 - and the response received you should not expect to be able to retrieve this data which was being
1683:32 - passed to the server from this point we are going to go in depth into how http works so you get to
1683:40 - this address www.postman.com you could sign in or sign up in case you do not have an account with
1683:48 - postman now i have an account so i'm just going to sign in you get to this point here you see you
1683:53 - have workspaces my workspace and then you have this link here which comes by default you have
1684:01 - this url which is this information you pass in each time you want to gain access to a particular
1684:07 - site in this case it's postmanico.com now url actually stands for uniform resource locator
1684:16 - and so this address right here permits us get access to certain resources which are located
1684:26 - somewhere in the web and this brings us to the http methods if you click right here you'll see
1684:35 - that we have different options like if you have you have the get you have the post you have the
1684:39 - put you have the patch and so on and so forth these are known as the http method and each time
1684:48 - you want to get access to a resource which is located somewhere in the web you have to specify
1684:55 - the exact method so like with a get for example we are saying that we want to get a resource
1685:01 - for the post unlike the get method where our main interest is to retrieve some data
1685:11 - here we submit an entity to the specified resource which often causes a change in
1685:18 - state or side effects at the level of the server so this means that we could make use of the get
1685:25 - method when we want to retrieve a user's information and then we make use of the post
1685:33 - when we want to modify some data or add information so like for example if you want to get ready to
1685:39 - start on a platform then make use of this post request as we're going to add a role on the
1685:46 - database so suppose we have this database right here where we have the user's id we have the
1685:53 - user's name and for example the user's password so here we have this small database and then we
1686:02 - have id let's say id 0 we have the name fred and password let's say whatever value we have here
1686:11 - now then we have id 1 let's say sally and then we have whatever password here then we could take
1686:19 - 2 we have rita and we have whatever name we want to have here or whatever password we want to have
1686:28 - here so this is it now with the get request we could just retrieve that fred has an id of 0
1686:38 - its name is fred and the password is a given password whereas with the post request we could
1686:45 - add a new user so that's why they speak of constant change in state or side effects on the
1686:51 - server so we could actually add this new user by passing information via this post request
1686:57 - so here we could add this third and we could add say mac let's say mac and that's it so that's it
1687:06 - for the post request now with the put request we could update this data so this means that
1687:12 - here we could say we are not interested in maybe modifying the name but if we want to
1687:19 - update this password we could change this password now to some new password so now we could update
1687:27 - this row right here in the database now we could also delete so we could just simply take this off
1687:34 - with our delete request and that will be it for that row we also have other methods like the
1687:41 - head the connect options trace and patch which you could check out in this document right here
1687:49 - but most times we make use of the get post put and delete then also those responses have some
1687:57 - status codes which we have seen already so you maybe have seen 200 for okay so okay when you
1688:05 - make a request and it's okay you could receive that let's let's make this request here uh url
1688:11 - empty let's get back here and you see you see your stitches let's highlight this you see your
1688:17 - stitches okay so this uh uh as we have seen here let's get back uh this as we as we have seen here
1688:26 - http response status codes so you could again check in this documentation and have uh every
1688:33 - detail about the different status codes and you need to understand how to work with them as they
1688:38 - are very important when you're dealing with apis so here we have informational responses
1688:43 - successful responses lying between 200 and 299 and that's why you see here we have this 200
1688:50 - status code which is a success and then we have redirections we have client error and we have
1688:59 - server error so this error is coming from the person sending the api request then you'll be
1689:04 - you'll be between 400 and 499 the errors from the server will be between 500 and 599 so when the
1689:10 - server is down sometimes you'll see this 500 so you could check your internal server error the
1689:15 - server is in color situation it doesn't know how to handle so you could check out all this in this
1689:20 - documentation now let's get back here and make this error so let's let's put in whatever value
1689:26 - one year write that again and you should see here you see 404 not found so you you get here
1689:35 - you see 404 client error so this error comes from the client we are going to paste this out here
1689:43 - and then select the post request get to the body form data this is the information will be passing
1689:49 - in so here we're going to have email teams at neural learn.ai the password not a password
1690:00 - and then job neural learns and then click on send okay what do we get you see we have this
1690:07 - output right here we have our email the username the job and a token status 200 now you'll notice
1690:15 - that unlike with a get request we have now introduced this body data so if we don't pass
1690:21 - this body data you see that we wouldn't have the right output and this is because to login to just
1690:28 - any platform obviously you need to pass in your credentials which in this case are the email the
1690:33 - password and the job so this tells us that when doing an API call like this one so when trying to
1690:41 - make the client communicate with the server we need to specify the HTTP method we need to specify
1690:48 - the URL then we also need to specify the body which in this case is all this information right
1690:54 - here that's the body information and the header information now this header and body information
1691:02 - could be broken up into the request and response so this is the request body and here is a response
1691:11 - body and then here we have the request header and the response header which you can get by simply
1691:20 - clicking on this right here you can also check out a list of HTTP headers on this developer.mozilla.org
1691:27 - platform right here then at a level of this response headers take note of this content type
1691:34 - which is Gizen. Gizen actually stands for JavaScript object notation which is a very easy
1691:41 - and lightweight format for storing and transporting data from a client to a server and vice versa.
1691:48 - The Gizen format is programming language independent and so if we have this client
1691:54 - with code reading in JavaScript it can communicate Gizen data to this API via this request which is
1692:04 - reading for example in another programming language like Python and then after processing
1692:10 - data the response in Gizen format will still be understood by this JavaScript client.
1692:18 - Now to better understand the Gizen formatting let's look at this output which was generated
1692:23 - after we made the GET request on this postman echo API right here you would first notice that
1692:32 - it starts and ends with this curly braces so we start like this and we end this way and then we
1692:41 - have information stored in key value pairs so we have the key and then we have the value and then
1692:49 - each key value pair is separated by a comma so we have the first key see here key and then we have
1692:57 - the color and then we have the value we have a comma and then the next let's say key one value
1693:04 - one and then next we have key two value two and then separated also by a comma from the rest
1693:13 - and so on and so forth so here you see we have this is key one here key one value one we have
1693:21 - the comma and then we have key two this headers key two headers and then value two now value two
1693:30 - years all this year so all this data is our value two so all this so value two and then we have
1693:40 - a comma just like here we have this comma and then after we have key three and then value three
1693:49 - now for a particular value we could also have some sort of dictionary and so in this case
1693:57 - let's take all this off in this case where we had this empty dictionary this was quite simple
1694:03 - but here we have this dictionary filled with its own key value pairs so here we have this key and
1694:11 - we have this value and then this value is a dictionary made of its own key value pairs so
1694:18 - you have the commas again and just like that so we have the commas uh separating each and every
1694:24 - key value pair and then here you just have this string so basically what we have in here is we
1694:31 - have in a key which is a string and we have a value which is some variable now this variable
1694:40 - can be a dictionary like in this the first two examples or it could be a string or it could be
1694:46 - an integer or even a boolean now if you check on this other request here the push request you'd see
1694:53 - this same formatting so we have the curly braces which open and close and then we have each key
1695:00 - and its value the key is value key value and finally key value now this was for the response
1695:08 - so this was the response body let's go ahead and check at the request body request body we had this
1695:15 - form data but actually we could have raw gson data which is passed so instead of using this
1695:24 - like basically using this gui we could have this raw and then pass in gson data so you see here
1695:31 - it's actually very easy to to write this out let's have this year let's check back at the form data
1695:38 - we have email password and job um yeah we have the same so we have email there we go we specify
1695:47 - the email uh neural learn dot ai and then we have the password our password is not a password
1695:59 - so not a password there we go next on final one we have the job we have here new
1696:08 - uh learns okay so we have that so notice how here we considering this raw data which we're
1696:17 - going to pass in and we're going to see the output we get here so let's send this and what do we get
1696:24 - here's what we get as output you see here unsupported major type text planing request
1696:29 - so this means that the request we sent uh was text type and not the gson format so you'll notice
1696:37 - here we have text which was selected and we just simply have to change this to gson so changing
1696:41 - this to gson what do you notice is he changing color that is because this now knows that here
1696:48 - are the keys which are in red and the values in blue it takes back to you see turns back to all
1696:54 - black let's get back to gson and then re-run this so click on send and let's see the output gson
1697:03 - parse error check that yeah let's take that off and then send again okay so you see now that once
1697:12 - we correct that and send this gson data we have this output so basically using this form data
1697:18 - right here which we could now take off let's take this off is the same as passing this raw gson
1697:24 - data so the rule of postman is to make it easier for people to test their apis using these kinds
1697:30 - of graphical user interface but you could always actually do it by passing this raw gson data as
1697:39 - you can see here and so now you have the status okay whereas when we had this text let's send
1697:45 - so you could see the status you see 415 unsupported media type let's get back to gson and send and
1697:51 - that's what we have you also notice again that level of the headers we have this gson format so
1697:58 - the body the output we have here is gson data then instead of passing this let's add in some
1698:07 - random stuff here and click on send what do we have we have for not found but we'll notice that
1698:14 - the content type that we have now is no longer the gson but instead html and when you check out
1698:21 - the body you see we have this year preview page not found this shows us that we can have different
1698:28 - content types exchanged between the client and the server
1698:37 - hello guys and welcome back in the session we'll see how to build our own apis using
1698:45 - the fast api framework in python language previously we have seen that we could wrap our
1698:51 - deep learning model or more precisely our object detection model into an api and then let this
1698:59 - clients consume this api that is allow them send requests like this and then get responses
1699:10 - so yeah in black we have the requests and then now we have the responses in red and all this
1699:17 - is done using the http protocol and so now that we understand the concept of an api
1699:25 - and how this can interact with millions of clients everywhere in the world we are going to move
1699:32 - straight to building our own api and to build our own api we are going to make use of this python
1699:39 - framework known as fast api now other python based frameworks which can be used in building apis are
1699:50 - django and flask but in the recent years the fast api framework has gained much popularity
1699:57 - amongst python developers thanks to the fact that it comes with some key features like its speed so
1700:04 - it's a high performance framework time taken to code is reduced that's now developers could
1700:11 - now build features even faster with fast api you even have fewer bugs it's more intuitive
1700:19 - it's easy it's kind of short robust and it's standards based so because we need all these
1700:27 - features while building our object detection api we kind of have little or no choice but to turn
1700:35 - to the fast api framework and you're going to see how easy it is to create or build a fast
1700:43 - highly performant and robust api very easily using the fast api framework first things first
1700:50 - feel free to check out this documentation at fastapitiangulo.com working with the fast api
1700:56 - documentation doesn't really feel like a usual documentation as most of the concepts are well
1701:04 - explained and broken down to enable just anybody with minimal python knowledge understand and use
1701:12 - fast api efficiently so right here we have the features fast api people could get straight into
1701:20 - this tutorial here user guide you see you have this well-written user guide you also have the
1701:27 - advanced guide so once you're done with this user guide you could get to the advanced user guide you
1701:31 - have special topics like on currency deployment project generation and all this and so before we
1701:38 - move on don't forget to start the fast api github repository one of those cost purposes is having
1701:46 - used python at a very basic level and so we're supposing that you've already installed python
1701:53 - without which you could head onto python.org download and install python based on your own
1701:58 - operating system so here you could see we have python installed python version check that out 2.7
1702:05 - and then python 3 version we have 3.6 this should be 3.6 okay there's an error there we have 3.6
1702:17 - okay now the next step will be to install fast api so we head back to the documentation you
1702:23 - have here this installation all you need to do is pip install fast api all you could do pip install
1702:31 - fast api all so you get other optional dependencies and features we'll start by installing fast api
1702:38 - we have pip3 install fast api there we go api now installed we could do python 3 and then you import
1702:48 - fast api you could check out the version right here we have fast api and then we have the version
1702:58 - okay so that's it 0.78.0 so the next problem we may face will be what if we have this project
1703:08 - this object detection project in which we have the fast api version 0.73.0 for example installed
1703:17 - let's say 0.73.8 installed you have this other project on optical character recognition which
1703:27 - needs a different version of fast api that is 0.75.0 you'll find that with this there'll be a
1703:35 - conflict as what we've installed is this 0.78.0 version now the way we could solve this is by
1703:43 - making use of virtual environments the way virtual environments work is you could have this project
1703:51 - this object detection yellow x project have its own isolated python environment with its own
1704:00 - interpreter where you can install these libraries in this python environment without it affecting
1704:12 - this other project which is with another or which is built in another virtual environment where here
1704:22 - we have a different set of libraries and library versions installed so this means that in this
1704:30 - environment here we could install a version of numpy say let's say whatever version and then
1704:36 - here again we could install another version of numpy which could even be the same as this version
1704:42 - but the difference now is in this virtual environment here we have a version of fast api
1704:47 - which is 0.70 0.78.0 and in this other virtual environment we have this other version which is
1704:54 - 0.750 and now this problem of conflict at a level of the dependencies is resolved now that we want
1705:02 - to create our python virtual environments we are going to make use of this python model which is
1705:10 - the python vamp model so here we'll simply do apt install the python python3 vamp and that's it
1705:21 - so here yes okay and that should be installed now we have the vamp installed let's get into
1705:29 - our neural learn projects directory and then from here what we'll do is we are going to create
1705:35 - our virtual environment we have python3 vamp and then let's call our virtual environment vamp
1705:44 - emotion detection detection there we go we should have this created and from here you could do this
1705:54 - and you see that we have this vamp motion detection here created let's get into this vamp
1706:02 - emotional detection directory so let's do vamp emotion detection and there we go you see we have
1706:10 - bean include leap leap 64 by vamp and share now let's get into the the bean so there we go we
1706:19 - have bean and then you see we have this activate right here now let's get out of this and then
1706:26 - let's do source oh well let's get back again another step so let's do source bean activate
1706:35 - and we get in this error well let's get back into vamp emotions detection and then let's
1706:45 - redo source bean activate and there we go so one thing you could notice now is the fact that
1706:50 - previously we have this root and this but now we have this vamp before this so we see that
1706:58 - everything we're going to be doing from now henceforth will be in the context of this
1707:03 - our virtual environment now let's do python3 there we go let's import fast api you can see
1707:11 - that we have modeled not found so it's telling us that fast api is not installed now let's get
1707:17 - let's exit and then deactivate the way we deactivate by simply typing in deactivate
1707:25 - and you'll find that when you deactivate you do not have this any longer so we are no more
1707:31 - in the context of the virtual environment so now let's do again python3 and then let's import
1707:37 - fast api you will find that here we have fast api installed you see we could also get the version
1707:46 - you see we have fast api installed but in this virtual environment fast api isn't installed
1707:53 - now let's exit and get back to our virtual environment so we have source bean activate
1708:00 - there we go and now let's do keep install fast api so one thing you could notice the
1708:07 - fact that we go out of the scope of this visual environment we have this version of fast api
1708:13 - and now we could install another even different version of the fast api so that's it we install
1708:20 - fast api and now that fast api has been installed like we could do let's in this visual environment
1708:27 - we could do import let's do iton3 and then let's import fast api and there we go so we have fast
1708:35 - api now installed we could get this version and you could see clearly that this version is different
1708:41 - from the other version of fast api another thing we could do is create our virtual environment
1708:48 - with a specific version of python so here we could have python3.8 and then we have
1708:55 - venv let's say emotion or let's say emo detection okay so there we go we run that and then we create
1709:05 - this new visual environment where the default version of python is that of 3.8 now let's again
1709:14 - have this you see we have this here venv emo detection and then we could get into that emo
1709:21 - um detection and then let's activate this um there we go and see this right here
1709:31 - now when you do python you see that the default version is this 3.8.13 so again here if you try
1709:40 - to import fast api you see model not found let's exit and then keep install fast api
1709:49 - all and run that you'll get to see now with the speed freeze that we have many more dependencies
1709:56 - installed because we decided to do that so here you see we have type in extension starlets nephio
1710:03 - uh pydantic which is very useful and those others now the next step will be to install
1710:09 - uvcorn and uvcorn is an asg server so asg here signifies asynchronous server gateway interface
1710:20 - being a http server this uvcorn right here is responsible for taking requests from different
1710:29 - users so supposing we have these three users right here the one to get for example some predictions
1710:36 - from a model which is here in our fast api which we've bundled in this our fast api code so what
1710:44 - we'll go on here is this user for example will make a request see to this server and then the
1710:53 - server gets that request interacts with fast api obtains a response and then sends back this
1711:02 - response to this user now if we want to connect with this web server locally we could pass in
1711:11 - this ip address here which is that of the local host and then we specify a specific port such that
1711:21 - we now here does the client sends the request which contains the method which could be for
1711:27 - example let's say a post method and then the request body and headers such that once this
1711:36 - web server receives this it processes the information and produces the required responses
1711:43 - we are now going to dive straight away into some particles so here you get this fast api you have
1711:49 - this simple tutorial here which we are going to follow and see how to easily work with fast api
1711:58 - right here we have this which we are going to copy to open up the code with visual studio
1712:03 - we'll get back and then we'll just write code and there we go so um now let's actually
1712:11 - get out of the spy user mode and then we get back in here um neural learn there we go and then now
1712:21 - let's just do this again and you see we have our visual studio which pops up we have our neural
1712:29 - end projects folder and we could create this new file so here we have main.py create this new file
1712:36 - and then we paste out that code now if you're working with vs code is recommended to install
1712:42 - this python extension so we just go ahead and install this extensions see we have that
1712:47 - installing let's get back to the code let's just get back to this so here for example let's
1712:53 - like this so it's clear here for example you see we try to create a file main.py
1712:59 - and with this code we are going to put in there now to keep things simple we are going to comment
1713:06 - this region and then also comment this right here so you're not going to make use of this
1713:12 - we're going to comment that and that should be fine okay so we we just have this portion
1713:19 - of this part here now you see here we have this app which is an instance of fast api see we've
1713:26 - imported this from fast api and then we have this decorator here and we have this read root method
1713:38 - now this read root method we could actually change this so we could say read neural learn
1713:45 - root or whatever name you want to give it so let's call that read neural learn or let's just
1713:50 - keep it short let's just say read so we have this read method text and nothing and then it returns
1713:57 - this dictionary or this gson hello for the key and world for the value we have this decorator
1714:06 - right here where we specify the meta type recall we had different methods like the get the post
1714:13 - put the delete and so on and so forth so here for example we specify this method so this is the get
1714:22 - method and so if we are making use of this get meta right here we have this path here we have
1714:29 - this path and then this means that if our web server is hosted on an address for example
1714:37 - let's say htps whatever whatever.com then this root path will correspond to this path right here
1714:49 - and then if we want to do say a login then we'll have your login this login path will correspond
1714:56 - to having this login right here obviously we'll have a post so change this to post and that's
1715:04 - basically how it works you see how easy it is for us to create these kinds of paths with fast api
1715:10 - and also specify your methods very easily so here we have that and we could have slash or
1715:19 - let's say a whatever we want to have there and then all we need to do is we have this here
1715:24 - a now obviously we could have some variables here and we're going to look at that shortly with this
1715:29 - other example below anyways we understand that we want to get to an address here whatever.com
1715:38 - or let's let's even say google.com let's say we're trying to build google.com then we will just have
1715:45 - here this and that'll be and so with this now or specifying this is going to get into this method
1715:54 - and return this gzn now to run this we'll just use what we've been giving here in the documentation
1716:01 - let's copy this out and simply paste this out here so here we're going to open up the terminal
1716:07 - so open up this new terminal there we go clear that's it and we neural and project so make sure
1716:15 - that you're in the same folder as where your main.py is found we could do uvcorn and we run that
1716:25 - error as the app attribute app not found a model main so let's check this out let's save this
1716:35 - and then run this again there we go see now it works we have started server process waiting
1716:42 - for application startup application startup complete so it waits and is completed and now
1716:46 - uvcorn running on this address on our local host so what we could do here is we simply control
1716:53 - and we click open this so control click and it opens up this page right here there we go
1717:01 - the first thing we notice is this message where we have method not allowed but why we
1717:06 - why will we have this method not allowed message the reason why we have this is because
1717:11 - here let's open this the reason why we have this is because here in the code
1717:16 - we actually use this wrong method see we are supposed to use a get method instead
1717:22 - so we should change this and have this get so let's save that again and then we would stop this
1717:30 - and then run that again click on this open this up and there we go we have as we expected the
1717:38 - JSON output here we have the id or like the key hello and the value world now another thing we
1717:46 - could do is we could get back here let's stop this see we stop that and then let's do um let's say
1717:54 - entry point let's have that and we save this so we save this and then we run this again
1718:00 - and then we click on this and you see little not found so here because we do not have this path
1718:08 - we output not found so let's let's do this let's say entry points and run that and you should have
1718:17 - an answer um let's check this out here we have entry point and then here we have entry point
1718:27 - entry point um let's get back here and then we should have this to resolve this issue we have to
1718:37 - put the slash before the entry point so you have to be very careful with the way you write this
1718:42 - now you save that again and then you reload but aren't you tired of always having to reload
1718:48 - i guess uh as myself you would also get tired of doing this so let's do this instead of having
1718:55 - that we'll add this reload here so that once we save or do modifications and save here automatically
1719:02 - our server is reloaded so now we'll we'll run this uh application startup started let's click on that
1719:09 - we have this and then we do entry points that should be fine so that's it we have hello world
1719:16 - as we expect now let's go ahead and update this let's just say entry you see when you save now
1719:24 - you see here you see it's shutting down waiting for application shut down application shut down
1719:29 - complete finished server process started server process waiting for application startup and
1719:33 - application startup completed so now we have this uh which has been reloaded so unlike previously
1719:41 - where we would have to stop and then start again now uh once we make the modifications
1719:46 - it's automatically reloaded so let's get back here oh not here let's get back here oh not here
1719:53 - actually let's get back here and then we are going to have uh let's run this you see doesn't work
1720:01 - but when we do entry it works now and we didn't have to stop the server for that the next thing
1720:08 - we'll look at is another interesting feature of fast api which is the swagger ui so here we'll
1720:16 - save this reloaded our server and then we get into this and instead of having that we'll do
1720:23 - docs so you see we have docs so we have this page which pops up and you'll notice that we didn't
1720:29 - need to specify that in our code so let's get back here and what we see is we have this uh
1720:37 - method here and then when we open this up see that does a path and then we have the parameters
1720:45 - there no parameters so we have this simple get request and then we have the response so here
1720:52 - the the code 200 and that's a successful response and then the media type uh gizen and that's it
1721:00 - so now let's go ahead and try this out so you see when we try that we click on try that out
1721:06 - and then this comes up we click on execute and what we have is this output right here so here
1721:14 - you could see the response body and there you could have the response headers so that's it
1721:20 - you could also repeat this on the post map so we have that you see the request url
1721:27 - and then you have the curl um this you could use also to make the api calls so that's it we see how
1721:37 - to make use of these docs and now what we'll do is we are going to uncomment these two parts here
1721:45 - so here we uncomment this and then we'll uncomment this part and then we save this and then you see
1721:52 - here we have these items now we will notice that when we get back to our code and then we refresh
1721:59 - this we have this automatically put out here so this means that no matter the number of different
1722:08 - parts we create we can automatically test them without any extra effort thanks to this fast apis
1722:16 - swagger ui so like here for example you not only can test but you can also see the documentation
1722:23 - like here we told that oh this is an ism id it's an integer it's required then this is a string
1722:29 - cure query it is optional because here you see it's not required and then you could try this out
1722:35 - so let's go again try that out we uh you see well let's cancel and then you see you cannot do
1722:42 - anything here once you want to try out you could put in an id so let's let's not put anything there
1722:46 - let's just execute you see we must put out something so let's have that 20 20 uh the string
1722:54 - let's let's leave that for now and execute so what do we get as results is uh this output item id 20
1723:01 - cure null now if we modify the code right here so let's modify the code and then let's just say
1723:08 - uh item id save that we get back here and then let's re-execute actually let's re-execute
1723:21 - what do we get you see it's auto automatically we have these modifications and then now we could
1723:27 - do this let's say hello we execute that and you see here we have our optional string which has
1723:36 - been shown to us in this uh gizen so we had seen the gizen format already here we have the key we
1723:42 - have the value the key we have the value and so on and so forth and then also we have the response
1723:47 - headers getting back to the documentation you could see here let's scroll back up uh no let's
1723:55 - let's get down you could open this up you see the main the file the app the object created
1724:01 - inside of main.py with a line abacore files api reload make the server restart after code changes
1724:07 - only do this for development so uh they're saying that if you're in a production setting that if
1724:12 - you've deployed this as we'll see in the next sections where you want to deploy this you don't
1724:18 - want a situation where you get to have this reload automatically so from this now you see
1724:25 - uh this basically explains what we've said already uh receives HTTP requests in this path
1724:32 - and in this other path so we've seen this already now both paths are get requests so we have the
1724:38 - get methods the path this path has a parameter item id that should be an int this other path
1724:44 - has an optional string query kill so that's it and then here you you you they get to talk about
1724:51 - the swagger ui and then you also have this alternative doc api docs here which is redoc
1724:57 - so basically what we'll do is we just get here and then instead of having all this let's say we
1725:03 - have that we just do redoc we run that and you have this alternative uh ui here where you could
1725:10 - still again test this apis so that's it uh you could test this very easily you see we have that
1725:18 - and then you should be able to test this now you have the response samples from here so for this
1725:26 - you have this response sample uh you have the other successful response or the response is
1725:35 - unsuccessful where we have this 422 now you could check out in the Mozilla platform where you see
1725:42 - the exact meaning of this 422 so let's let's check out developer mozilla 422 so you could always
1725:52 - get back to all this so you don't have any issues understanding any code you receive as output so
1725:59 - here you see unprocessable entity and that's it that's it for this here we have different responses
1726:09 - that's fine the next feature we'll be looking at will be that of schemas what we'll do here is
1726:15 - we'll simply take this off keep it simple we'll not use this and then we'll change this into a
1726:22 - post request so we have this post request right here which takes in a body obviously we have a body
1726:30 - and then it returns the elements in this body now here we'll take this off and then we have
1726:38 - our post request so there we go we have our post request it's no more read item but now add item
1726:47 - now we here we're going to have an item which is going to be of type
1726:51 - item so this is going to be a special type we're going to create now here we are going to import
1726:57 - pydantic or from from pydantic we're going to import the base model so that's what we're going
1727:06 - to do we're going to import this base model from pydantic and then we're going to create this item
1727:12 - this class item which is going to be a subclass of this base model class right here so we have this
1727:21 - item and then we have base model so that's it and then here we're going to say okay we want that an
1727:30 - item should have a name and a price so we have name which is a string and we have a price price
1727:39 - let's let's say integer let's say we our price and integer okay so that's it so there we go well
1727:45 - what we're going to receive here is this item of type item it's no longer the usual types but now
1727:53 - this special type item which we just created so now here we have item and then what we'll do is
1728:02 - we're going to return this so let's call this item name here so we have item dot name so we get the
1728:12 - item we return its name and then we also have the item's price so item price and then we return the
1728:23 - item dot price okay so that looks fine let's save this and check our code here or check this
1728:35 - API let's refresh this and then here we see we have our post we check that out let's try this out
1728:43 - execute what do we get we get in response body item string item price zero now if we come here
1728:52 - if we get here you see a level of this name and then we put let's say back and then price let's
1729:01 - say 20 and then here we execute what do we get you see we have back 20 if there's something you
1729:09 - can notice is the fact that this request body here automatically takes this name and price so
1729:19 - get back to the code you see that if we modify this let's let's say we want to get the name
1729:23 - want to get the price or let's say we want to get the cost or let's say we want to get a discount
1729:28 - so let's say this is discount on the item discount and then let's put this int okay we save that and
1729:35 - then we get back here we refresh this try that out and you see discount automatically is here
1729:43 - so the role of this schemas is to provide a model which the request body has to always follow so
1729:53 - this means that if you get back here let's get back here and let's take one of this off you see
1730:00 - take one of that off we execute check out the response you see here expecting our property
1730:07 - name and closing double quotes all of that you see here we have this unprocessable entity error
1730:15 - now this is because here at the level of the request body we didn't specify the discount
1730:22 - which we've already said in this schema that we need to always have so doing this means that
1730:29 - each time anyone has to make a call on this api on this endpoint right here that person has to
1730:35 - specify the name the price and discount for the request body now we could also enforce this kind
1730:43 - of measure at the level of the output so we could specify a response body and then we define say
1730:53 - let's say this is input item let's call this input item and then here we would have output item
1731:01 - so we want to create another schema here let's copy this and then we're going to say that okay
1731:10 - we want this output to be the price minus the discount so here we have output output
1731:20 - and then instead of the price and the discount we just want the selling price so here we have
1731:27 - selling price and that will be an integer okay so we have that now we have the output we have the
1731:34 - input and we have the output schemas they're both subclasses of base model and then here
1731:40 - what we'll do is we'll say okay we want to have the selling price we'll define the selling price
1731:46 - to be the item price minus the item discount okay so we have that so we have this now
1731:56 - and then here we have the item name still and then we have the item we have to be careful
1732:02 - the same as this so we have to say selling price anyhow let's leave that for now let's just say we
1732:07 - have here selling price so let's say we have selling price and then let's save this and
1732:15 - check this obviously already post got an unexpected keyword argument response body
1732:21 - this is actually a response model not a response body so that's the output response model let's
1732:26 - let's let's change this to input so now this input item for the request and then this output
1732:34 - for the response now let's save this that's fine let's go ahead and check this out so here we
1732:42 - try this out there we go you see as usual that's fine and then let's let's give this a price let's
1732:49 - say 20 and no discount and then we execute so what do we get your internal server error now let's
1732:56 - get back to our code and check out what's going on here field require this missing now you see that
1733:04 - everything looks like normal but we told that there's a field that's required at the level
1733:10 - of the output and the reason why we have this is simply because this output expects to see
1733:16 - selling price you see it expects to have selling price so because we do not have selling price here
1733:22 - we get in that error now let's save this we get back here i'll refresh that and then try this out
1733:29 - again execute no let's let's put 20 anyway it should it should be the same it's just that we
1733:36 - want to have to see the difference in the or to see how the selling price is computed here again
1733:42 - we have internal server error now this looks funny let's get back and see where we get an error
1733:52 - we told field required and it happens that we even do not specify this correctly so here this
1733:58 - here's name price discount and this is name so here we should also have name so not item name
1734:05 - we save that again we go ahead and check this out here um let's refresh that try this out
1734:16 - let's just execute and that's fine so you see we have that now and this is because we've
1734:23 - put out our outputs correctly so let's have here say 200 and then 180 or let's say 20 and then we
1734:33 - execute that um and see we have this output so that's it we see how thanks to this kimas you
1734:42 - could now build apis which will meet the developer or whoever's making use of your api to have to
1734:51 - specify this without which nothing is really going to work so this means that you could have
1734:57 - a situation like here let's let's see we have this input item and then we let's let's see that again
1735:05 - and get back here we refresh this page and then let's try out while taking off this so we take
1735:14 - this one off and then we go ahead and we execute you see that the output here 422 is a client
1735:23 - error so this means that the client knows that this error is coming from his or her side now
1735:30 - we get back here and then before we move on we are going to define this structure or this code
1735:37 - structure for our first api projects we'll be working with so what we want to do is separate
1735:46 - the apis that's actually the end points from the core now when we talk about core we're talking
1735:53 - about let's get back here we're talking about carrying out computation like this so you see
1735:58 - this computation we want to have some separate file which is in charge of doing this in the
1736:04 - case of some deep learning model want to have a separate file which will be in charge of
1736:09 - say taking the inputs loading the inputs passing this input to a model doing some processing and
1736:16 - then from there now the output from that is asked on to this end points because here this one end
1736:24 - point and this another end point so this like read this add item now getting back here you see
1736:30 - this like having read here this one endpoint read is under endpoint add item this may be under
1736:36 - endpoint say update item so like root you have get items post items and so on and so forth
1736:42 - so now at the level of the core we want to have our business logic in our case for this simple
1736:48 - example our business logic was basically just taking the the price and then subtracting
1736:55 - the discount so that was our business logic but we want to have this in a separate file
1737:02 - now for the schemas we want to also have this in some separate file so we want to have this
1737:07 - kind of structure where we have the core that's the business logic schemas and then we have the
1737:11 - api in charge of all the different end points now to do that we are going to create our service so
1737:17 - from here we have we're going to add a new folder which we'll call service and then in the service
1737:24 - we're going to add a new file which is no basically let's copy this mean into the service here so we
1737:30 - could do this we could copy that move that now let's stop the server okay we've moved this into
1737:38 - our service and then in the service we have a new folder which we'll call api and then we'll create
1737:44 - another the new folder which we'll call the core now in this call we have a new folder which we'll
1737:51 - call the logic and we also have another folder which is called the schemas schemas okay so we
1737:59 - have this no this is this is this call has this new folder which is called schemas and not the
1738:09 - logic so let's let's delete this folder here let's delete this move to trash okay so we have that
1738:18 - we have the car which has logic and schemas and then we have the the api now which will have
1738:23 - different endpoints now given that we need to carry out our inference making use of this
1738:31 - quantized onyx model we are going to start by pip installing onyx runtime so here we have
1738:39 - pip install onyx runtime and there we go now we're getting this error so you have to make sure you
1738:48 - run this as a super user we now we run this as a super user we have pip install onyx runtime
1738:58 - that's now installed you see we could do python and then import onyx run time and that's fine
1739:09 - okay that said let's do pip install open cv python we could also check that out let's clear this
1739:21 - and python and import cv2 and there we go so you see we have now the onyx runtime and
1739:31 - open cv installed we will then move on to create the endpoints so in this api we have this new
1739:39 - folder which we'll call endpoints and then here we'll create new file which we'll call detect.py
1739:50 - and in here again we have another file which we'll call test.py now this detect.py here
1739:59 - will play the role of actually being that endpoints through which we are going to carry out
1740:05 - the detections and then this one will be the endpoints through which will test if our api
1740:12 - is alive or not now in our detect.py we are going to import the api router so from fast api import
1740:22 - api router there we go so previously we had imported fast api but this time around we'll
1740:28 - be working with this api router then now we'll create this object emo router of the api router
1740:39 - class where this will play the role of router information from the detect.py to the main.py
1740:50 - now we have our emo router decorator and then we have we specified i want to have this
1740:56 - get method and our path will be detects okay so here's our path and then now we could define this
1741:05 - detect method in here with this detect method given that we want to carry out detections based
1741:12 - on input images we're going to have as input this time around this image and then you'll find that
1741:19 - given that it's an image it's going to be of type upload file because the user would have to upload
1741:25 - this so that's the type upload file we we have that we could simply import this from here upload
1741:33 - file there we go and then once we once we define this we can now get into the body of our method
1741:40 - so from here we could start by just let's say we output return return say hello hello world so
1741:51 - let's just return this so now we have this method which takes in the image which is uploaded by the
1741:58 - user and then returns hello world now to dive into what this detector is going to be doing
1742:04 - we'll need to import this image from peel so from peel you're going to import image there we go
1742:15 - so we have that imported and then we are going to open up the file so let's let's say we have our
1742:21 - image we want to get our image image there we go we open up the file so we have image.open
1742:33 - the file which has been uploaded by the user so we have em which is this
1742:38 - um dot file the read but this um open method here takes in bytes so would have your bytes
1742:48 - bytes IO will convert this to the byte form and then you would simply import that so we have from
1742:56 - IO we import um bytes IO and that should be fine okay so that's it we now have our image
1743:03 - image which would then convert into a non-py array so now we have image equal um non-py array
1743:13 - which takes in this image and then now instead of returning this we could have this method
1743:20 - emotions detector emotions detector which takes in the image so it takes in this non-py
1743:27 - um array or this guy takes in our image and then is going to tell us what class that image belongs
1743:34 - to so for now we have this let's import um non-py let's import non-py so we have um
1743:44 - import non-py as np okay so that's it we now have this imported we are also going to check if
1743:53 - the image or whatever file we are uploading is actually an image file so we get the the the
1744:00 - file name we have the file name and then once we obtain the file name we try to get this extension
1744:07 - so right here let's take this off we have that um split method then we obtain the extension
1744:13 - and if this is um or it's part of this that's if it's gpg gpeg or png then we know that this is an
1744:23 - image and so we're simply going to pass um if this is not a case then we'll raise an exception
1744:30 - so here we have um race race HTTP exception and then we will make sure we import this um
1744:45 - HTTP exception then we specify the status code as 415 and our detail is not an image so that's
1744:54 - what we're going to do before calling on this emotions detector meta right here now before we
1745:01 - move on let's let's pip install on pillow because you could see here that we have this um
1745:12 - we have the pillar library which is yet to be installed so we're just going to do here
1745:16 - pip install pillow there we go while that's installing also take note of the fact that
1745:22 - this is meant to be a post request and not a get request so we replace this with post
1745:29 - now that we have pillow installed you see we do no longer have the warning we had before
1745:34 - so that's it now we could um check out on our test.py this time around we're going to create this
1745:42 - test router so here we go we have a test router and then we'll just call this test um testing
1745:53 - and here we'll take this off and this is a get request so here we have this get request
1746:00 - and we will have this test so this is our entry point and then we could take all this off
1746:09 - um there we go yeah we now have we're going to return um testing um oh let's see yeah testing
1746:23 - um testing so that's it we have testing testing ipad working so this is what we're going to output
1746:33 - when we um have this or pass this through the browser now that we have these two endpoints
1746:41 - defined as this test and this detect right here what we are going to do is we are going to create
1746:49 - in this api folder the api.py file so let's create this new file api.py notice how this
1746:57 - um two are in this endpoints so detect.py and test.py are in the endpoints folder or directory
1747:05 - and then this api.py is found in this api directory so now in this api.py what we're
1747:12 - going to do is we are going to create this main router and the role of the main router
1747:17 - will be to include this detect router and the test router which itself now will be included
1747:27 - in the main app so as usual we have to import this api router from fast api
1747:35 - and then once we have this imported we're going to import the detect router and the test router
1747:40 - so from service remember we've created this already api endpoints detect our detect.py
1747:49 - right here we are going to import detect router and then let's copy this space out here
1747:57 - now we're going to import the test router from test so there we go so we have the detect and
1748:04 - the test router is now imported the next thing to do will be to create our main router we have
1748:09 - this main router um api router which has imported already and then now the main router is going to
1748:17 - include um the detect router and it's also going to include the test router so we have main router
1748:25 - including the detect and the test router so there we go again we can access the app via the detect
1748:36 - or test or maybe some other let's say some lambda router right here and then all of this
1748:46 - get included in our main router which now is also included in the app back to our main.py
1748:55 - we have from fast api this time around we're going to import fast api take that off and then now we
1749:04 - we import the main router so we have service api.api import main router there we go so we have this
1749:17 - main router imported as usual we have the app fast api then we could specify the project name
1749:24 - uh there we go emotions detection and then we have to include now our main router so this time
1749:36 - around our app now or with our app we have the include router method which takes in the main
1749:43 - router so that's it we have this um now included and we will now turn all this into packages so
1749:52 - at this end point we're going to create a new file um init.py there we go on the api api new file
1750:04 - we have init.py then for the core we have under logic we are going to create this new file init
1750:13 - the py schemas new file init.py under core oops core new file init.py then finally for the service
1750:29 - itself we create this new file we have init.py so we have our service package api package and
1750:37 - core package now remember we have this emotions detector or meta right here which we had to define
1750:45 - we're going to get that defined in the logic so on the logic we're going to create a new file
1750:50 - we'll call this onyx inference.py there we go and then here we'll simply create this method
1750:58 - um emotions detector emotions detector which takes in the image array and then we're going
1751:08 - to follow up this simplest test we had seen already from the collab notebook we start by the import
1751:15 - then we specify the providers then we there we go we have the quantized model the image path
1751:24 - the test image we just created and then now we finally run to obtain our predictions so let's
1751:31 - just copy all this out and then put out in the code there we go let's paste it out here um back here
1751:43 - there we go and then finally we have those predictions so that's it now let's make sure
1751:51 - we import onyx runtime see onyx import onyx runtime onyx runtime and then also we need to
1752:00 - import open cv and what do we need again we need numpy so import numpy as np um there we go so yeah
1752:11 - we import onyx runtime import open cv and then numpy now this is our our method which we are
1752:18 - about to define let's send this this way there we go and then now we're going to return this output
1752:26 - actually it's practically this here so let's copy this from here um let's copy this from here
1752:36 - and then paste out here well so that's it we have this now let's modify remember what we'll
1752:43 - get in here will be the image array and so um we no longer need to specify this image path let's
1752:51 - take all this off take this off and then from here we would have this uh quantized model
1753:00 - there we go we have our quantized model we've already specified the the providers so this
1753:06 - our quantized model for our test image you see we're going to take this image array and then resize
1753:13 - it so let's take this off from here we have 256 by 256 take all this off and there we go
1753:26 - now we'll import this onyx runtime as rt so that should be fine now um that said let's get in here
1753:34 - and see what we have um we'll print this out well let's just let that for now and then we see we
1753:42 - have our predictions as we know already and then from here we have the output now one thing which
1753:48 - is left out is this um this file actually we we get to get or put this file in this right position
1753:56 - we've copied this onyx file here which we trained already and then let's update this then uh once
1754:07 - it's fine let's get back to our detect.py and make sure that we import this emotions detector
1754:14 - so here we have from service.core.logic.onyx inference we're going to import emotions detector
1754:28 - so that should be fine we have now import this imported let's check that out and then let's
1754:34 - save this uh get back to onyx inference and then now at this point we're going to say if if um np
1754:45 - argmax onyx spread which is this here let's copy this and paste out here if this is equal zero
1754:55 - um then what we'll do is we're gonna have the emotion here let's have emotion
1755:07 - um set to um angry happy or sad so here we have emotion equal angry uh yeah that's angry and then
1755:20 - um let's copy this well let's say let's have this here let's copy this here paste out here
1755:31 - now lif lif this is equal one then this is happy and else the emotion is sad okay so that's it we
1755:51 - have angry happy or sad and then what we're going to return here would be this dictionary where we
1755:58 - have emotion and then the output will be simply the emotion so that's it let's take this off now
1756:07 - there we go we have that set and when we get back to detect.py you see that this is fine
1756:13 - we're going to pass the image and we're going to get the corresponding emotion now we have all
1756:18 - this set what we could simply do is um run our server before launching the server let's add this
1756:25 - root endpoint right here and then now let's do let's launch the server uvcon service main app
1756:33 - reload so that's what we have we're getting this error cannot import name detect the detect route
1756:40 - from service api endpoints detect well what we'll do now is we're going to create this new folder
1756:47 - and we'll well this shouldn't be in here this is going to be here so in um neural end projects
1756:53 - we'll create this new folder um new folder which we'll call emotions detection
1757:02 - and then now we'll copy this service in our emotions detection folder um let's move that
1757:09 - there we go so now we have um in emotions detection we have the service we're going to
1757:16 - relaunch the server again you see we still get this error now let's get into this um detect
1757:24 - file you see here we have this you see um it's actually emo router and not detect router
1757:32 - so if we get here we should have emo router there we go emo router and here we have emo router
1757:42 - okay so uh we reload that we took another important name test router well let's get into
1757:50 - tests uh we have test router right here this should be fine now that we're getting this error
1757:58 - let's make sure we have this here so we we created this um main router which is an object of api
1758:06 - router class we save that and then um this should be fine this time around so application startup
1758:12 - complete gets into the local host you see we get what we expect we have hello world so this is
1758:20 - uh what we get when we pass this in our browser we now open up the swagger ui you see we have
1758:29 - this detect here as expected we have testing and then we have the root okay so that's it
1758:36 - let's open up this detect and you see we could try this out then we could pick out a file which
1758:42 - we could actually test so let's go ahead and test with this file let's open this up and now let's
1758:50 - execute so let's check out the output field to execute this and so that's what we get in
1758:58 - we get in um an error let's get back here one thing you could see is the fact that this you
1759:06 - could request your error right here isn't what we expect we expect to have a slash before the detect
1759:12 - so let's get back to the code and then modify this so take this out from here save this and
1759:20 - that should be fine so we should have the same with test save that and yeah okay so that's fine
1759:28 - so let's now get back and then test this out let's refresh there we go we have detects we want
1759:37 - to try this out we want to choose a file let's go ahead and pick a file this file and then let's
1759:46 - execute okay so let's check this out we get an internal server error let's get back here and
1759:56 - try to see where we have the problem so we've been told that this file doesn't exist all those
1760:03 - will make sure that we put out the right path now we have service and there we go so we have
1760:10 - this efficient quantize dot onyx model which is in our service directory you could find service
1760:17 - right here so that is it let's save this let's save this and then let's go ahead and retest this
1760:28 - so reset console try out and then let's retest this once more execute since that's going to
1760:39 - work this time around let's go ahead we still have an error now let's get back to the code and try
1760:44 - to understand why we have this error we we told invalid rank for inputs input got to be expected
1760:51 - for please fix either the inputs or the model so this what we are getting here now um let's
1760:58 - get back again to our code we told that what the model gets isn't um what is expected so let's
1761:07 - print out the shape of this image array right here print out shape image array shape and let's
1761:19 - save this and then try out run this again um reset console try out and then we test this once more
1761:30 - just execute that and obviously we have the internal server error but now we want to see
1761:38 - exactly what gets into the model so let's scroll and we should get the output okay so this the
1761:48 - output we get now you notice that here we have this um image which is of shape 90 by 90 but what
1761:56 - we would have expected here should have been 90 by 90 by 3 so it's um a grayscale image when we
1762:03 - expect an RGB image now what we'll do is we'll say here that if um the shape is grayscale or
1762:13 - rather if the image is grayscale then we are going to make sure we modify that so if the length of
1762:20 - image array um shape is equal to then what we'll do is we'll ensure that we convert this
1762:31 - from gray to RGB so let's do this uh let's go this way let's do this and then let's print
1762:40 - uh what we have from here let's copy this and then paste out here okay so uh there we go we're
1762:48 - gonna paste this again uh so we see what we get after we convert the gray to RGB so that's it
1762:58 - let's save this again let's save this this should be fine so we'll save that then we get back here
1763:07 - okay so let's test this again out um reset cancel try out pick the image execute and there we go
1763:20 - you see we have the emotion happy right here so we have the expected response so we have your
1763:27 - emotion happy now let's get back here and see see we have 90 by 90 now it's 90 by 90 by 3 after
1763:35 - the conversion and then we have um after it's resized we now pass into the model and we have
1763:40 - the expected output so that's it let's um take this off and that's it so there we go we've just
1763:50 - tested our api and we see that it works just fine now we could take off whatever prints we have here
1763:58 - take this off take this off and there we go we'll then move on to our schemas where right
1764:07 - here we'll create this input.py and this output.py file now in our output.py file we'll start by
1764:14 - importing base model then we'll create this api output class which inherits from base model and
1764:21 - then we'll specify um what we want our output to look like now given that we've seen already from
1764:27 - here our output is this string emotion so let's get back to the code and see that this is exactly
1764:36 - what we want to have so let's save this or let's let's say we put this as an int and save this
1764:42 - then let's go ahead and test so let's um well before doing any testing we have to make sure
1764:50 - we integrate this with the detect.py so let's get into this here endpoints detect.py
1765:00 - well you have this response model here but normally this isn't there so before we didn't
1765:06 - have this so we we should have this now you see response model and then specify api output so
1765:14 - um you are controlling what or how this output here should look like so now we have that make
1765:22 - sure you you import api output from here and that should be it so let's save now again and then
1765:28 - let's test this out so we execute and then we check out the response you see we get a response
1765:35 - internal server error which makes sense value is not a valid integer so you see that we are able to
1765:42 - tell our api what we want our output to look like now let's um get back to our schemas let's take
1765:50 - down your output and then modify this and say we want a string so let's save that now and then go
1765:56 - ahead and we test this so there we go we execute and we check out the response you see now we have
1766:06 - exactly what we expect so that's the role of the schemas now we're not going to um do the same
1766:12 - product inputs just um we just do only for the output and then from here we'll move on to the
1766:18 - next part we'll then go ahead to measure out the time taken by the model to produce the output so
1766:25 - we get into this onyx inference.py and then just before this well let's say at this level once we
1766:34 - obtain our image we're going to have time init time init which is going to be equal time the time
1766:44 - so we get a time this initial time let's import time right here import time there we go and then
1766:53 - once we import time and we have said this time init we want to get the time elapsed so um after
1767:01 - we've gotten the prediction we want to get a time elapsed we have time elapsed which is simply the
1767:08 - current time minus that initial time and then for the outputs we want to have this emotion
1767:17 - and also we want to get the time elapsed so let's have this well let's return emotion
1767:24 - and then we also want to return this time elapsed which is simply our time elapsed we'll convert
1767:34 - this into a string so we have that now let's save this well this should be time elapsed
1767:41 - let's save this and then now let's go ahead and retest so right here we're going to
1767:48 - uh execute and see what we get we get in um this output but we're not getting this here so let's
1767:56 - let's reduce this and save this again let's get back here let's save this again um we run this
1768:06 - execute there we go we're still getting this without the time elapsed well what we'll do now
1768:16 - is we'll get back to our schemas get back to the output and then include this time elapsed
1768:22 - and it's a string so let's save that and then kick back here um there we go we are going to execute
1768:33 - and we should get this response so you see it takes about one second to produce our output
1768:39 - now that we've seen that it takes us about a second to produce the output what if we
1768:46 - loop farm means to reduce this time so let's get back into our code you see we have the provider
1768:52 - specified we have the model from here and then we pre-process the image and then pass the model
1769:00 - pass this input into the model and then we'll obtain the output which is um this now this model
1769:08 - loading process here we don't know exactly how much time it takes so what we'll do is we are
1769:13 - going to include this time elapsed loading so we have time elapsed as a total time elapsed
1769:20 - for loading and running now want to also get the time elapsed for just loading the model like this
1769:28 - so what we'll do is now we're going to include this in the output we have um time elapsed
1769:35 - loading and then we have time elapsed loading okay so we'll include this in the output let's
1769:44 - save that um even output.py we have this already so we have time elapsed and time elapsed loading
1769:50 - let's save this and then get back here we execute and let's check out the output okay
1769:59 - so you see here we have about 1.1 second total time but the time for loading is 0.72 so this
1770:07 - tells us clearly that a huge portion of this time taken to produce the output is wasted on the
1770:15 - model loading see this about 66 percent of the time is for model loading even though this is 1
1770:21 - this is 0.66 so let's see how to load this model once we launch our API so instead of
1770:31 - loading the model each and every time we make an API call what if we take this off from here
1770:39 - so instead of having this here let's take this off we are going to put this in our main.py
1770:44 - um main.py here we will have the main.py we will have our model loading so import
1770:55 - import onyx runtime as rt there we go then um here now we we're going to load our model
1771:05 - so we have our model now loaded here we'll save that and then get back right here and then say
1771:10 - okay every time i want to make use of this we're going to have this here we're going to import
1771:18 - import um we import service.main as s so we have that service.main as s and then here we'll have s
1771:32 - dot model quantized so we've taken this from our main.py from here so that is it we save this now
1771:42 - and we had we still we took off let's take off this time elapsed loading we don't need this any
1771:50 - longer because we we no longer load the model well let's um let's cut this from here and compute
1771:58 - the time it takes to pre-process this image here so time elapsed pre-process take that off
1772:09 - then let's modify this here time elapsed pre-process well let's okay pre-process so
1772:18 - we have now this taken off pre-process okay so let's get into the schemas we have output time
1772:25 - elapsed pre-process save that then go ahead and run this right here so let's execute once more
1772:39 - there we go we execute and we check out the output you can see here time elapsed now is 0.45 seconds
1772:48 - which is um about half or more than half um of the time we were taken previously so or less than
1772:57 - half of the time were taken previously actually so here we have the time elapsed for pre-process
1773:01 - this is actually very small so all this was almost of this was for the model to compute the output
1773:09 - so let's let's rerun this again let's execute that again see 0.49 seconds now we've we understand
1773:18 - how this works let's stop this run this again and then check out the time when we just um start
1773:28 - our server um execute and let's go ahead and check the time okay so you see you have 0.41
1773:37 - second that's practically about 400 well 450 milliseconds so that's the time it takes to
1773:43 - produce the output now we went we left from one second to now 0.45 seconds one other great
1773:51 - advantage of working with the fast api and building your apis is the fact that you could build um
1773:58 - asynchronous apis very easily and it's just by simply specifying your this async keyword so here
1774:06 - we have async save that um test here async there we go and the main we have um async
1774:19 - and that should be it so that's all you need to do to um make your code asynchronous and so this
1774:27 - means that when working with fast api if we have this task for example this one which takes
1774:34 - much time to be completed what goes on is this other tasks can be run asynchronously and so while
1774:41 - waiting for this text for example let's draw here while waiting for this task we could already start
1774:47 - working on this other task and then while waiting for this other task to be completed we could also
1774:52 - start working on some other tasks this means that overall we have taken up this time to complete
1774:59 - this three tasks whereas for synchronous code we'll have to wait for each and every text to
1775:03 - be completed before taking on the next one so you see the the time difference between these two
1775:09 - different methods we have here the difference we see this difference here so this again in time
1775:16 - when working with fast apis asynchronous manner of running code nonetheless it should be noted
1775:24 - that there are some tasks which are compute bound that's the actual cpu bound meaning that even if
1775:30 - you have to run this code asynchronously this first task will still take up so much time so you
1775:39 - would end up with something like this so there will be no real gain even though you had to run
1775:43 - this other task asynchronously and this is very common in computer vision and since we're dealing
1775:50 - with computer vision more specifically object detection we have to understand that even though
1775:55 - our code is running asynchronously if at the level of this detector py we have this call on
1776:02 - this emotions detector and we're still waiting for that to be completed no matter that we've
1776:10 - already completed these two tasks right here we'll still have to wait for this to be complete
1776:17 - so for such cpu bound tasks like computer vision machine learning deep learning will instead take
1776:24 - advantage of parallelism with parallelism instead of having one worker one cpu worker to run all
1776:32 - these different tasks what we could do is we could allocate say for example let's say three workers
1776:38 - so we have now three workers where each worker can focus on a given task and now we can now take
1776:46 - advantage of the fact that our code runs asynchronously and takes or makes use of parallelism
1776:54 - now so far we've been using this uvcon http server which is an asgi web server implementation for
1777:02 - python asgi actually means asynchronous server gateway interface which now serves as a minimal
1777:09 - low level server interface for an async framework like fast api now although uvcon is coming with
1777:18 - speed it isn't mature enough to be used in the production setting hence we have to or we tend to
1777:25 - use gunicon which is a mature fully featured server and process manager for our production
1777:31 - settings so that's why uvcon includes a gunicon worker class allowing you to run asgi applications
1777:40 - with all of uvcon's performance benefits while also giving you gunicon's fully featured process
1777:47 - management so we are now going to go ahead and keep install gunicon gunicon once we have gunicon
1777:56 - installed we'll go ahead and run this command which we saw already gunicon service main app
1778:02 - and then we specify number of workers let's change number of workers to three for example
1778:06 - so let's say we want to have three um workers and then our worker class is uvcon so let's run this
1778:13 - and as you can see we have this happening uh occurring tries started several process with
1778:21 - a specific um id then we have here started several process with another id another started several
1778:29 - process with this other id indicates in our three different workers now we could go ahead here
1778:35 - refresh this page um try this out choose file then execute and see there we go so we have our output
1778:46 - exactly as we expect so that's it now you could obviously reduce the number of workers if you
1778:52 - reduce this um to two oops reduce that to two you would find that we have only this process and
1779:00 - this other process so that is it in the section we've just seen how to build our own api
1779:08 - and test it out locally in the next section we are going to deploy this on the cloud
1779:19 - hello everyone and welcome to this new and exciting session in which we are going to deploy
1779:24 - our api to the cloud now in the previous session we have looked at how to build this
1779:31 - api using fast api where we could simply pass in an image like this and then obtain the output
1779:38 - where we we get the emotion that is happy and we also get the time it takes to produce this output
1779:46 - so we could execute that and you see we have our result right here that said the platform we shall
1779:53 - be using to deploy our api is ero cool and ero cool is a platform as a service that's a pass
1780:00 - that enables developers to build run and operate applications entirely in the cloud so uh first
1780:06 - since first you could go ahead and sign up so you click here sign up and if you've already
1780:13 - signed up you could go ahead and log in so once we've signed up and log in could get back to the
1780:20 - code here we'll stop this let's clear that out and then now what we'll do is we'll do a pip freeze
1780:28 - so let's do pip freeze and you see that we have this output which is basically all the different
1780:35 - packages we have in our virtual environment so now um let's write out or put out all these
1780:44 - um packages right here in requirements.txt file so we'll do pip freeze and then we'll say want
1780:51 - to pass send this to our requirements.txt file now note that we are in this directory emotional
1781:00 - detection does this so when we do this here you'll find this requirements let's take this one off
1781:07 - we did this previously um move to trash okay so you find that now we're left with service
1781:14 - and we we have this requirements.txt that said let's open up this requirements you see all the
1781:22 - different packages which we installed already now we'll get into open cv we have open cv well that
1781:31 - should be python let's search for open cv okay so it's open cv python and now what we'll do is
1781:38 - we would modify this and the reason why we're modifying this is because this version of open
1781:43 - cv right here is one which you could use even with a desktop application but what we need for
1781:49 - the cloud is one which is headless that is one which doesn't contain all the functions it's really
1781:55 - functions like those of visualization given that what we're doing with open cv here is essentially
1782:03 - maybe resizing and that's that's basically it so we don't need this open cv python
1782:11 - we instead going to make use of this open cv python headless so that's it we're going to
1782:16 - um take this version 4.2.0.32 okay so with that done we save this uh fail to save well
1782:31 - once we save that as a machine the next thing we'll do is we are going to create this proc file so
1782:37 - in here we have well new file so call this proc file there we go see recognizes that already
1782:47 - and then what we'll do is we're going to paste out essentially what we run each time we want to
1782:53 - launch our server so we have web and that's it so here go specify number of walkers to two
1783:00 - and that's it so here we have gunicon service main app number of walker set walker class set
1783:06 - and that's fine so let's save this too and then we create another file runtime.txt now this
1783:14 - runtime.txt is essentially going to take this python version so we'll copy this and we paste
1783:23 - that out here so here we have python and this should be fine now also make sure you include
1783:30 - this hyphen before this python version with our proc file ready the requirements.txt file ready
1783:39 - and the runtime.txt file ready we are going to get into the eroku platform where we'll get right
1783:46 - here and click on create new app so we want to create a new app and then we're going to give this
1783:52 - app a name so while this loads there we go we're going to call this emotions detection
1784:02 - now we could add this to a pipeline or deploy this directly so we want to deploy this directly
1784:08 - we have three options using the eroku git using by connecting to github or using the eroku cli
1784:16 - so we want to use eroku git and so we're going to follow this simple instructions right here
1784:21 - so it's not very long now first since first eroku login so let's get back here we have eroku login
1784:31 - now you find that this will work and now but you should note that let's let's stop this you should
1784:38 - note that if you don't have eroku initially installed this wouldn't work so the first thing
1784:45 - you want to do is do a sudo snap install eroku sudo snap install eroku before doing the eroku
1784:54 - login now given i've done that already i'll just do eroku login and then that should be fine so
1785:00 - right here we have eroku login pressing the key to open up the browser pressing the key
1785:06 - and you should be able to log in very easily there we go you will click on this link right
1785:14 - here and if you click on that link you should be able to log in so you could see here we have
1785:18 - logged in so that is it we now log in the next thing we want to do is from here what we want to
1785:26 - do now is get into our project directory so once we get into our project directory let's zoom this
1785:33 - we get to our project directory and then from there we do a git init now if you're not familiar
1785:38 - with git you could take a crash cost on git so you get familiar with all these terms
1785:44 - so let's get back here and do let's clear this clear and then we do a git init there we go see
1785:55 - we in this emotions detection directory now once we've done that you see initialize mtg repository
1786:00 - in our directory now the next thing we want to do is have this here let's copy this and then
1786:09 - um paste this out here so we have this arrow code git remote and then we specify the project name
1786:18 - notice how this project name is automatically generated because this is what we put in
1786:23 - let's get back here run this and that should be fine so that's what we have now our command
1786:31 - field we told that to well we command field git remote fatal unsafe repository to add an
1786:38 - exception for this directory called git config and that's it so what we'll do is now we're going to
1786:46 - copy this and then paste out right here so there we go let's now get back and run this
1786:55 - and this should work so that is it so set git remote um arrow code to this and that's fine
1787:03 - now the next step we want to do is um deploy the application so we have git add git commit and git
1787:11 - push so let's go ahead do git add we want to add up all uh files so pick the dot and then we have
1787:22 - let's copy this git commit um git commit make it better paste that out well let's say our first
1787:31 - first um commit of uh emotions detection app oh well api so this is a string which you could
1787:45 - obviously modify now run that and then now oh you see we have all this um created and then now let's
1787:53 - go ahead and do the git push arrow code master so here we have git push arrow code master and
1788:02 - they should work fine what we get here is this push rejected to emotions detection neural learn
1788:09 - so let's scroll up and try to understand why we have this problem so we told you request that
1788:14 - um runtime is not available for this stack so open this up and then um let's try to check out on
1788:24 - those different python versions available for the stack here you could see we have this um 3.9.15
1788:33 - which is on all supported stacks so let's get back here and then um a level of the runtime let's
1788:40 - let's take all this off paste that now save that save that and then we'll do um git add
1788:52 - um git commit now what we'll say here is we have um updated runtime dot txt file with
1789:03 - uh python version 3.9.15 run that and then now we're ready to do git push arrow code master once more
1789:16 - we still get another error right here where we told that um no matching distribution found for
1789:22 - open cv python headless so now we're going to take off this version and then we do the git push
1789:30 - one more time once they get pushed down you see that we now have this release and here's our link
1789:37 - so let's click on this we open up and there we go see have hello world so this is what we have
1789:45 - online now and we could go um docs and we check out on our swagger ui so let's reduce this
1789:56 - and let's try this out so you will try this out online we've deployed this on the cloud
1790:04 - let's execute and see what we get okay so it even takes less time as compared to with uh the pc's
1790:12 - taking like uh 0.23 seconds that's about 230 milliseconds let's reduce this so you could see
1790:19 - that um okay so that's it so we see that our model is working we're able to obtain our outputs
1790:28 - and then now what we could do is we we could actually from here let's copy this here
1790:35 - and then try this out on postman so there we go we have this enter the url and then we select
1790:44 - post request um well here we have the body here we have form data then our key our key here is
1790:55 - going to be image in and then the value given that it's a file we will now select the file
1791:02 - there we go we have our file selected click on send and we should get our expected output
1791:09 - is here we have happy and time elapsed 230 milliseconds so that's what we expect you
1791:18 - could see from here we have this different timing we have the status and we have the time it took
1791:25 - for all the different events right here and so that's it for this section in which we've deployed
1791:32 - our api to the cloud and the next section we are going to carry out load testing on our deployed
1791:40 - api hello everyone and welcome to this new and exciting session in which we are going to be
1791:49 - looking at load testing the load testing tool will be using will be lockers and lockers will
1791:57 - permit us not only define user behavior with python code but also swarm our system or api
1792:07 - which we've built already with millions of simultaneous users so in this course we'll
1792:13 - see how to test or better still to load test the api we've built previously both locally
1792:22 - and on the cloud at this point you've already tested the model which has been deployed on
1792:29 - eroku and everything looks fine but then what if we tell you that we will have to still carry out
1792:38 - some load testing now what's the point of load testing the point of load testing here is to see
1792:44 - how this your api which you've built will react when you have many users so you wouldn't generally
1792:53 - want to create api to be used by yourself only but maybe to be used by tens of thousands and even
1793:00 - millions of different simultaneous users so we cannot get access to this millions of people so
1793:08 - you wouldn't start calling each and every one of these people here to test your api and what you
1793:15 - do is to have an automated system where this millions of users are simulated and this
1793:24 - tool we will be using here to simulate this millions of users is lockers which is an open
1793:29 - source load testing tool that said we are going to create this locusts.py file so here we have locusts.py
1793:39 - there we go and then in this file we are going to write out all the code we'll need to be able to
1793:47 - control or simulate all the users who will be using the platform so here we have from locusts
1793:54 - we're going to import sequential task set now one thing you should notice the fact that
1794:02 - with locusts we using or we defining different tasks and here we are going to define a task
1794:10 - whose role will be to simply open up a specific image and then pass this image via post request
1794:19 - to our api and we obtain the required outputs so let's define this emotions so emotions task
1794:32 - or let's say detector task detector task and this is sequential text set so we have this
1794:41 - and then we have this task decorator we have our task decorator right here we're going to define
1794:49 - our detection method and here this detection method is simply going to be this method where
1794:57 - we define what's actually going to go on when we want to test our application or load test our
1795:05 - application so here we have we open we open up a file which we're going to create locally so we
1795:12 - have this image.jpg and there we go so we open this file as image and then now we are going to
1795:26 - carry out our post request so we have that post request and then you see we're going to specify
1795:33 - the path this URL and that's detect actually want to do for detect meaning that you could replace
1795:40 - this with test or some other path you want so for now is detect and then let's get back here
1795:48 - for now is detect and then now we specify that we're passing a file so here we have this files
1795:55 - and we have the name em it's actually em and then here we have image so remember we've opened this
1796:03 - up as image so this is image we're passing here now from the API we've designed already its name
1796:11 - yeah what we have is em so that's why we're specifying this is em and then once we have
1796:15 - this set so this is our detector task as we said the way we build this with lockus is we define
1796:22 - different tasks and then once we define these different tasks now we could simply define our
1796:29 - load tester class so here we have class load tester and then we have HTTP user so this
1796:39 - inherits from class HTTP user and then now we specify the host our host in this case is
1796:46 - going to simply be what we've been using already so it's this which we deployed on Iroquois let's
1796:53 - copy this get back here and then paste this out so we should we should take off now this detect
1797:01 - so we'll take that off and that's it so we're just passing in this as our host and then once
1797:07 - we pass this as a host we now specify the task so this is a task in this case our task is the
1797:15 - detector task okay so that's it we've we'll specify the task remember you could obviously
1797:22 - create many more different tasks but for now we just want to work only with this detector task
1797:29 - so that's it we have that set we could save this and then now we could go ahead and launch our
1797:37 - locust so we have locust we specify that we have a file and it's locusts.py oh there we go so let's
1797:47 - run this um locust command on the phone oh we forgot to install locust so let's do pip install
1797:53 - locust there we go that should work fine so install locust and once we install locust we
1798:02 - are going to we run this command right here and now that we have that installed what we're going
1798:08 - to do is run back this so there we go we run this and we could click on this so let's click
1798:17 - there see what we get see here uh we have our lockers running so uh yeah we to start new load
1798:26 - test so now the number of users uh initially is one the spawn rate is one we're going to check
1798:31 - this on later on and then notice how we have this our url which we had specified so that's our host
1798:38 - we should specify it already now let's start the swarming by our locusts um there we go remember
1798:45 - this is hosted online so it's going to check that up um looks like none is working well check out
1798:54 - this chart statistics well let's get back here let's get back here we get in something like
1799:03 - this so we got some errors we could see here no tags define on load tester let's save this
1799:10 - and then we run this and see what we get we run that again but we're still getting this error so
1799:16 - we had to stop and let's check out why are we getting this error um yeah we have no tags define
1799:23 - our load tester but you have set a task attribute on your class maybe you meant to set tasks here
1799:29 - um here yeah this should be tasks because this is actually a list remember as we said we define
1799:34 - different tasks and then we have our load tester right here so we could have this task here we have
1799:42 - some other task oops we could have some other task and so on and so forth so let's say we have
1799:49 - two tasks let's say we have tags and then we'll call this tags too then we are going to come right
1799:56 - here and then put this in this list but given that we just have a single task let's take this
1800:02 - tags two out and then now as we saw here we should have your tasks and not just tasks so let's save
1800:10 - that and then run this again and now everything should work fine okay so we have that we click
1800:18 - open um starts forming looks like isn't working let's get back here stop this now let's check
1800:29 - out the error we're getting okay no such file as this okay so remember we don't have this
1800:36 - file in this directory so let's just go ahead and copy that file now we've copied the file
1800:41 - you can see that here we have our file here let's go ahead and rerun this and now everything should
1800:47 - work fine so let's click on this open up start the swarming and well let's get back we're still
1800:57 - getting an error um stop this and check this out here we told that no such file or this well let's
1801:08 - let's rename this rename um this should be test image okay so let's run this again
1801:19 - sign open up start the swarming and looks like another error um all users want
1801:32 - well let's check back here and you can see that we have this working as expected so you can see
1801:41 - we have certain number of requests which are sent already zero fills we have the median um latency
1801:47 - we have the 90 percentile latency 99 percentile the average the mean and the max so you can see
1801:54 - that it's taken about two seconds for every user to receive whatever output the sent in so that's
1802:03 - it so we have this mean and we have this max now we could get into the charts and you see that this
1802:09 - is increasing slowly and we have zero failures so that's very important we have zero failures with
1802:15 - just a single user and that single user is um as we saw here getting an average of now is 1.7
1802:24 - milliseconds to receive the output from the api so that's it now another thing we could do is
1802:33 - actually stop this and then carry out a new test where we would have say 10 users so we want to
1802:40 - test when we have 10 users at a time now this spawn rate here fixed at the value of two will
1802:47 - simply mean that after every second we are going to add up two users so we're going to start with
1802:52 - two users and then after one second we go to four after two seconds we go to six and so on and so
1802:57 - forth until we attend the maximum row of users which we've said already here so let's start the
1803:02 - swarming you can see here we start initially with two we're going to six and then ten and um after
1803:10 - some time you're going to have the first requests so there we go we already have two two requests
1803:15 - not that we have dealing with 10 users here and the average time it takes for each user to receive
1803:21 - the response is 10 seconds meaning that our system will take about 10 seconds to reply to
1803:28 - well whatever user whenever we have 10 simultaneous users so let's get into postman remember here
1803:39 - we have this time elapsed note that this time elapsed was for the model but what we're getting
1803:45 - now is what each and every user located in some place in the world would have as actual
1803:53 - um time or latency if we have 10 users now we start seeing that we start getting failures so
1804:02 - let's check this out when you run this um let's run arrow cool logs so yeah we let's clear this
1804:10 - so you could see that clearly so we run arrow cool logs and then we specify the app emotional
1804:17 - detection neural learn and specify tails we could get the last um locks so run that and uh this
1804:26 - should run you could see that we have successful calls and we also have this calls which field
1804:35 - see the stitches and your connection close without response so let's get back here remember
1804:42 - when we had a single user we didn't have all those failures so let's stop this run your tests
1804:50 - and reduce this maybe it's just three users and then see what we get well as you can see with
1804:58 - just three users we have a few failures that's about seven percent as you can see here and it
1805:03 - takes on average of 4.5 seconds to receive output when you send input images and so with this we've
1805:13 - just built and load tested our api they showed us clearly that if we want this to go faster or to
1805:21 - reduce our latency and reduce the failure percentage which as of now is at 2 percent
1805:27 - then we would need to increase our compute capacities so that's it for the section see you in the next section
1805:38 - welcome to the section and object detection object detection is one of the most popular
1805:44 - computer vision tasks and also a very important one object detection entails correctly classifying
1805:53 - objects which are in an image and also saying exactly where these objects are located in the
1806:02 - image so if we have this image we see clearly that we have an aeroplane an aeroplane a person
1806:10 - a person and then we could say car so this too is some sort of car now an object detector not only
1806:19 - classifies these images but also localizes exactly their positions in the image so this
1806:26 - aeroplane for example has this bounding box now a bounding box basically surrounds also square
1806:33 - a rectangular box which surrounds the object and then for this person we have this bounding box
1806:39 - for this this bounding box and this other aeroplane we have this bounding box which is in this
1806:44 - other bounding box so unlike a classification problem where when given this kind of input
1806:51 - we have as an output or a one-hot vector for example which represents the number of classes
1806:57 - we're dealing with so supposing we have five classes then when given an input we have to
1807:03 - correctly say whether that input belongs to one of the five classes now with object detection
1807:11 - we not only have this but we also have the positions in the image so like this we have
1807:18 - this position like this and we have several conventions for these positions one of the
1807:24 - most popular conventions is the center convention where we have the x center y center width and then
1807:31 - height now what does this mean this means that based on this referential right here so we could
1807:37 - define a referential right here where we have this origin now recall we used to having we're used to
1807:43 - working with this kind of referential now the referential for image data is considered to be
1807:49 - this so our origin stands from here we move in the x direction and then you're in the y direction
1807:55 - so this is our reference and then it's based from on this point on this origin that we actually
1808:02 - define positions so if we have this airplane let's consider this bigger airplane so we could have
1808:09 - this bigger airplane we could define its bounding box by its center and then its width so once we
1808:17 - have a center obviously and if we're given its width we can obviously see that it is in this
1808:25 - bounding box so this is the first convention the center that's the x center and the y center
1808:30 - the x center is basically the distance from this to the center so suppose the center is right here
1808:36 - so if our center is here then the distance from here to this is x center and the distance from
1808:43 - up to this is y center so basically that's what we have so if we want to link this up like this
1808:51 - we could see clearly how we obtain x center the distance from here to here and then y center
1808:57 - distance from this origin to this which now gives us the center so that's how we get a center now
1809:05 - we are given the width and then the height if we're given the width and the height obviously to
1809:10 - to get this points or to get because we have actually four points right here to get all these
1809:16 - four points which make up the bounding box we could start from say this point for this point to obtain
1809:23 - the x and the y coordinates right here we simply take this x coordinate and subtract from the
1809:31 - width divided by two because from this to this is width divided by two on the diagram it doesn't
1809:37 - clearly that this points at the center but normally this should be at the center so we could
1809:41 - rearrange this bounding box now so as we said to get this we have this x center minus the width
1809:49 - divided by two to obtain this and then to get the y we have the y center plus because this is
1809:56 - actually the positive direction and then this is the negative direction for the x so for y this is
1810:01 - positive this is negative for y this is positive for x and then this is negative for x now to
1810:07 - obtain the y as we said we have this y center plus the width of this or rather plus the height
1810:15 - of this divided by two that's how we obtain this now to obtain this since we're going from this
1810:21 - we're going the x center minus the width divided by two we obtain the x coordinate here to obtain
1810:28 - the y coordinate we have the y center minus because this is the negative direction so we
1810:34 - have the y center minus the height divided by two and this is very similar to obtain the x here
1810:42 - there's a x coordinate right here we have the x center here plus the width divided by two
1810:47 - to obtain the same x coordinate right here it's x center plus the width divided by two obviously
1810:51 - these two have the same x coordinates but different y coordinates to obtain the y coordinate we have
1810:57 - this y center minus the height divided by two to obtain the y coordinate here we have the y center
1811:03 - plus the height divided by two either ways once we have this two coordinates that's this point here
1811:09 - then this we could always obtain this on this automatically. Now another
1811:13 - convention is the X mean Y mean X max Y max convention where we just given this
1811:19 - coordinates. If we're given this coordinates and then this coordinates we
1811:23 - could obviously obtain all this because when once given this and then given this
1811:28 - we could just get the whole box automatically. So here are the two main
1811:35 - conventions we use to actually locate an object in the image.
1811:43 - This yellow paper published several years ago was one of the first to come
1811:49 - up with a single neural network which predicts bounding box and class
1811:54 - probabilities directly from full images in one evaluation. Now back then models
1812:03 - or to be precise object detection models follow this kind of pipeline where we
1812:09 - would have a region proposal generator, feature extractor and then a
1812:14 - classification unit. So you could look at this with a simple RCNN model where we
1812:21 - had the input image, the extract regions that's regions where the model things we
1812:29 - could have images where the model proposes the locations of objects and
1812:35 - then from here each proposed location is passed into this feature extractor which
1812:43 - obviously extracts features from this warped regions as you could see here for
1812:49 - example this region and then we have the classifier which tells us whether this is
1812:56 - a person, an airplane or say a TV monitor. Now with the yellow as we're
1813:04 - saying we have a single network so let's get here you see we do not have all that
1813:12 - different stages in our pipelines here we just have our input image a single
1813:18 - neural network and then we have the outputs see that there's also this
1813:24 - additional nonmax operation here but we'll look at this shortly. Now the
1813:31 - performance of the yellow is quite impressive or was quite impressive in
1813:35 - terms of speed is especially as now we can obtain speeds of up to 45 frames per
1813:43 - second and with a yellow the smaller version of the yellow we obtain up to 155
1813:49 - frames per second while achieving double the mean average precision of other real
1813:54 - time detectors who look at the mean average precision subsequently this is
1813:59 - the metric used generally in object detection and so a high mean average
1814:06 - precision means the object detection model is performing better. Another
1814:12 - advantage of the yellow to take note of is the fact that it reasons globally
1814:16 - about an image so unlike the sliding windows and the region proposal based
1814:22 - techniques we've looked at the regional the region proposed proposal based
1814:26 - techniques like the RCNN here for the sliding windows the way it works is we
1814:32 - have let's get this way we have this image right here and then this window you
1814:42 - see you can look at this as a window so this window is slided through the whole
1814:47 - image all I hear what we just simply pass in the image as an input with a
1814:52 - sliding window you would have to take each window and pass into our neural
1814:57 - network and you do that while you're sliding through the full image so as we're
1815:03 - saying getting back to what we're saying here as compared to the sliding window
1815:08 - and the RCNN kind of models the yellow performs better in the sense that it
1815:16 - sees the entire image during training and test time so it implicitly encodes
1815:21 - contextual information about classes as well as the appearance so you'll find
1815:27 - that models like the RCNN or mistake background patches in an image for
1815:31 - objects because it can't see the larger context okay another point is the YOLO
1815:39 - learns generalizable representation of objects so you see that here the YOLO
1815:45 - which was which wasn't trained on this kind of paintings performs quite well
1815:50 - you see the YOLO was trained on or this particular version of YOLO was
1815:56 - trained on the Pascal VOC dataset but when we test this on this image painting
1816:02 - you see that it does well so the YOLO models compared to others learns more
1816:08 - generalizable representations of objects let's now go in depth and see how the
1816:14 - YOLO algorithm works first is first remember that if you have this kind of
1816:21 - model see the YOLO model which you could have here there we go you see this YOLO
1816:28 - model has several conf layers and then completes with this connected layer so
1816:34 - we have the feature extractor and the classifier unit right here anyways we
1816:39 - are not going to get into this now let's just consider that we have this model
1816:43 - and then we have inputs like this one see this our input and then we have some
1816:50 - output right here now this output obviously is meant to be a bounding box
1816:57 - so here we will we could draw this bounding box for this woman here so this
1817:05 - shows that or this bounding box is for this this person and then let's take
1817:13 - this off at the stroke let's change that color so here we have this here this
1817:22 - this person detected and then we also have this other person right here so
1817:29 - we'll have something like this see that so we have now these two detections one
1817:35 - for this woman and and this other one for this other person and so we're
1817:42 - gonna build a model which is this one which takes in these kinds of input
1817:47 - output pairs and then learns to get this inputs and predict the outputs correctly
1817:56 - such that when given a new input it could tell us where every object is
1818:04 - located and what type of object it is precisely now that said the actual
1818:12 - outputs wouldn't be this image with this bounding box the actual output will be
1818:19 - different from what we are seeing here now the way this outputs are created is
1818:26 - using some sort of encoding system where we have this input which is breaking up
1818:32 - into some grid cells so right here let's take the pen so right here you see we
1818:39 - have let's suppose that we have this 224 by 224 input image that's this image
1818:48 - here 24 by 224 this one year and then we break this up into several grid cells so
1818:56 - this is a single grid cell another one and so on and so forth now each grid
1819:00 - cell here given that we have seven grid cells so we have seven by seven output
1819:09 - see that we have one two three four five six seven one two three four five six
1819:16 - seven so that's it and now each of this is going to be 32 by 32 so it's like
1819:25 - 32 by 32 patches so we take each patch here combine them to form 224 by 224 or
1819:32 - better still we take the 224 image and break it up into 32 by 32 grid cells see
1819:39 - that now once we have this ready or once we once we have somehow broken up
1819:45 - our image into this grid cells we are then going to encode the outputs based
1819:53 - on the locations of the center of our bounding boxes right here so you see we
1820:00 - have we're gonna redraw this bounding boxes so that you see that clearly let's
1820:06 - increase this to say five okay so here we are having this bounding box right
1820:13 - here see this bounding box and then it has a center at about this position here
1820:21 - see that position if we want to locate this year it falls about this around
1820:28 - this year so you see is about this now for this other person we have another
1820:34 - bounding box like this see that this other bounding box and the center is
1820:39 - about this about around the child's nose so it's around here see that so what
1820:46 - we're gonna do now is we're gonna have each and every one of this let's change
1820:51 - this color we're gonna have each and every one of this sorry we're gonna have
1820:57 - every one of this here having certain values now in the case where the cell
1821:05 - like this one let's this make this bit more transparent the case where cell
1821:11 - like this one let's do trainee so you could see that better so here we're
1821:17 - gonna have as we're saying in the case where a cell like this one which doesn't
1821:23 - contain an object then we'll say okay its first value will be a zero see that
1821:29 - this first value will be zero skip back and here our first value is zero so for
1821:39 - this cell the cellular office value is zero now for this other cell here where
1821:47 - the there is an object there see this value will be a one see that so each and
1821:58 - every cellular each and every cellular has or takes certain values based on
1822:04 - whether there is an object or not now as you can see this cell you have a zero
1822:10 - the zero in fact all the zero and accept this tool which we put in red which will
1822:17 - take values of one and this is simply because it happens that the centers of
1822:22 - the bounding boxes fall in those cells so that's the first step now once we
1822:30 - have this first step the next thing we want to do is we want to locate the
1822:36 - exact position of our images so the first thing is we want to know whether
1822:41 - there's first of all an image that's by encoding it like this the next thing we
1822:47 - want to know is the exact position now this exact position obviously depends on
1822:54 - the kind of the way we want to present our bounding boxes now we could represent
1823:00 - our bounding boxes by specifying X mean X mean Y mean and X max Y max with this
1823:12 - kind of representation an object like this baby right here or let's say person
1823:17 - right here will have this values or will make use of these values to locate this
1823:25 - person so we make use of this point here which is X mean Y mean and this other
1823:31 - point here which is X max Y max with respect to the origin which is at the
1823:37 - top left corner so this our origin right here you see that so we go X steps
1823:44 - and Y steps downward then here X steps to the right Y steps downward to locate
1823:50 - this person now once we once we've done this we could we could just put this out
1823:58 - here so we could say okay we are creating our outputs remember our aim
1824:02 - here is to create our outputs so we are creating our outputs we know for every
1824:07 - cell or for every grid cell where objects are located that's it then now
1824:13 - to get the bounding boxes we could make use of this but what's important to note
1824:17 - here is the notation used by the authors of the yellow v1 paper was instead X
1824:24 - center Y center then the width and the height see that of the bounding box
1824:34 - obviously none of the image so here instead of having making use of X mean Y
1824:40 - mean X max Y max we make use of X center which is the center Y center so we'll go
1824:48 - X Y and then we look for the width of this box the width and also the height
1824:59 - of the box so let's get back so that's that basically how we do that and then
1825:06 - there's another special encoding which is done and what they actually do is for
1825:12 - the width and the height of the bounding box they're gonna divide this by big W
1825:19 - where big W is the over is a width of the whole image so if our image is 224
1825:25 - by 224 will take this width let's suppose that this width is say 160 so
1825:30 - we'll take 160 divided by 224 and get our width and then for the height we'll
1825:36 - do the same thing so you know if we have the height of say 200 will take 200 by
1825:40 - 224 and we get that so it's h divided by the height of the whole image see that
1825:47 - now once we have that we here for this XC YC we're gonna do something similar but
1825:55 - not divided by the whole width and also not divided by the height of the image
1826:01 - what we are gonna do here is we are gonna have this XC with respect to its
1826:08 - specific width cell now let's explain let's pull this way so you could see
1826:14 - that clearer so we're supposing that we have this here and we've already seen
1826:19 - how to get for the width and the height we take that divided by the total width
1826:24 - and the height divided by the total height total width and total height now
1826:28 - example is 224 now for the XC and YC here for example we have this our XC YC
1826:37 - is for this other objects our XC YC let's consider the example of let's
1826:41 - consider this example here so what we're saying is we are not gonna take this
1826:45 - with respect to the full image instead we're gonna say oh we're gonna take this
1826:50 - grid cell and suppose that this distance is 1 and we take this and suppose that
1826:56 - this distance towards 1 now our origin here is this see this points our origin
1827:02 - now if we're for this cell this will be our origin and this will be our distance
1827:07 - 1 and here our distance 1 now if this are distance 1 so distance 1 then this
1827:14 - point here let's say this point here will be a fraction of 1 basically so be
1827:21 - a value between 0 and 1 now if we take this distance we could approximate this
1827:25 - to be about 0.5 6 and then this distance is about 0.7 so in this case XC
1827:33 - will be this distance and YC will be this distance in that case we'll have 0.5 6
1827:42 - and 0.7 so that's it now once we have that the next thing we'll do is we'll
1827:50 - just simply put that out here so we would have that XC with respect to the grid
1827:55 - cell right at G and then we'll have YC G that's it so here we now know how to
1828:06 - obtain those values let's change that color back so here we would have say 0.7
1828:11 - 0.5 6 0.7 so that's how we have this now once we once we get this
1828:19 - remember we already have one year so the next one we have is XC G for the
1828:26 - next part if we have a data set where we have say 20 classes like the Pascal VOC
1828:34 - data set or with a co-code data set where we have 80 classes what we would
1828:40 - have from here see we find whether the object is there or not if the object is
1828:45 - there we want to get this location now from the location want to know what
1828:50 - object exactly is found there so here we suppose now we have 20 classes see now
1828:57 - initially every one year is zero so we have 20 we just gonna align 20 zeros so
1829:03 - basically we have this this 20 zeros here now if the person if in the you know
1829:09 - in our classes we decide that the person occupies the city third position in a
1829:15 - list of classes so we could we could have a class or let's just let's just
1829:19 - check out the list of Pascal VOC classes whereas consider we have this 20
1829:25 - different classes and there if we count this we have 1 2 3 4 5 6 7 8 9 10 11 12
1829:34 - 13 14 15 notice how this person set a 15th position 15 16 17 18 19 20 ok so we
1829:44 - have the person at that 15th position and so this means that when encoding our
1829:53 - image here or when encoding our output would have to take that into
1829:58 - consideration will simply do 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 so here at this
1830:12 - position will change this and put a 1 instead of the zero which all the other
1830:19 - classes get so we take this off and then we have a 1 right here see that so we
1830:27 - have 15 16 17 18 19 20 now if you have another data set with fewer classes say
1830:33 - eight classes this will be of length eight now we've seen how for each and
1830:40 - every one of these grid cells which actually several 49 of them because we
1830:45 - have seven by seven we have this encodence that's so what our model sees
1830:52 - or what our model gets as output isn't this but instead this kind of output so
1831:01 - now we are going to prepare our data set such that we have an input image which
1831:06 - is this and then an output level which is going to be this level right here and
1831:11 - then the model is going to struggle to be able to correctly produce these kinds
1831:18 - of outputs and so as we've just discussed our output will be of shape
1831:24 - this output here will be of shape 7 by 7 by 20 see number of classes 20 plus 5
1831:36 - or could you say 5 plus 20 which is in fact 7 by 7 by 25 so this is what our
1831:50 - level will look like so when we get our input and our output I won't get an
1831:57 - input and the bounding boxes with the specific classes we're gonna create this
1832:03 - kind of outputs from the labels we get depending on our data set nonetheless
1832:09 - when we zoom into this model right here we notice that we taken some input and
1832:15 - then at a level of the output we have 7 by 7 by 30 instead of 7 by 7 by 25 now
1832:23 - the reason why we have this is simple we have the first position let's reduce
1832:30 - this we have the first position which tells us whether there is an object or
1832:36 - not so that's one and then the next position gives us the location with the
1832:43 - X C Y C W and H so here we have the position here we have the position so we
1832:50 - have X Y W H now the next ones give us the classes or tells us what class our
1833:02 - object belongs to so you would have a series of zeros some point we will have a
1833:10 - one and then we have your zeros but the length of this year is 20 now 20 plus 5
1833:16 - is 25 obviously which is less than 30 now to understand why this is actually
1833:22 - 30 oops to understand why this is actually 30 what we'll do is let's take
1833:30 - this off what we'll do is we'll suppose that we have for every cell two boxes
1833:40 - responsible for locating the object so instead of having this year only this
1833:49 - particular box so we suppose that this is a box let's take the pen we suppose
1833:55 - not this year this year is a box this one is a box instead of having only this
1834:04 - box we're gonna make two boxes responsible for locating the object
1834:10 - remember this is this permits us to locate the object because first of all
1834:13 - it tells us whether the object is found in the cell or not and here it tells us
1834:17 - it gives us the exact coordinates of the objects this one is for the classes
1834:21 - separate so the next thing we'll do is we'll take this and multiply by two so
1834:27 - let's shift this again we have this and then we paste that out see now if we
1834:35 - have the number of boxes to be three though this will multiply it twice then
1834:38 - the in the yellow view on paper they use they use B to be they considered B to be
1834:42 - equal to you could check that out your B equal to so that said we see we now have
1834:49 - two boxes responsible for locating the object in that image so when designing
1834:56 - the levels that we could design with just one because we know that this does
1835:01 - the correct answer but what the model will predict will be two values you see
1835:08 - we'll predict this to repeat this and predict this now the reason why we're
1835:12 - doing this is because we want that one of these boxes or any one of these boxes
1835:18 - should be more specialized depending on the size of the object now as you've
1835:23 - seen here we're dealing with relatively large objects with respect to the to the
1835:29 - image actually but what if we have an image of a person where the person is
1835:33 - say just a very say we have a very very small person to image ratio like this
1835:42 - where we will have a bounding box which will be small compared to the whole
1835:48 - image in that case what we want is for the model or for these boxes to be
1835:54 - specialized such that maybe this first box will will learn to detect the
1836:00 - smaller objects while this one learns to detect the larger objects so that's it
1836:05 - that's how we construct this output so we understand how to construct the
1836:10 - levels and then how to construct the model outputs remember we have to update
1836:17 - this weight such that the difference between the levels and the model output
1836:25 - let's say all is minimized getting back to the paper we see the exact structure
1836:34 - of the yellow model so right here we start with some conf layer conf layer
1836:39 - max pool layer conf layer max pool we have several conf layers then a max pool then
1836:47 - we have this other conf layers this year times for the max pool conf layers then
1836:54 - conf layers then finally we have the fully connected layers for classification
1836:59 - so this is for feature extraction this year feature extraction and then this is
1837:05 - for classification now for the training what they do is the pre-trained this
1837:11 - model on image net with 1000 classes but note that this pre-training is for the
1837:18 - problem of classification so it's a usual classification problem and then
1837:23 - the pre-trained this model for over a week year and achieve a top 5% accuracy
1837:31 - of 88% and then from this model they add four convolutional layers and two
1837:39 - fully connected layers we randomly initialized weights so while going going
1837:45 - doing the training for the object detection we have weights from the
1837:50 - previous training that's from the pre-trained weights from the image net
1837:55 - and then the add some the add the four the actually spoke of four so they should
1838:01 - be this and this one this one this last two year so they add up four conf layers
1838:10 - and two connected fully connected layers which have been randomly initialized as
1838:17 - a set year now following the example okay does it detection often requires
1838:23 - fine-grained visual information so we increase the input resolution of the
1838:26 - network from 2 to 2 24 by 2 24 to 4 48 by 4 48 so what they did was they trained
1838:32 - on 224 by 224 images and then at detection time we used 4 48 by 4 48
1838:40 - images now they also use a linear activation function for the final layer
1838:44 - and all layers use the following leaky relu so they use the relu for the final
1838:51 - and then the leaky relu for the all other layers as the activation function
1838:58 - now as a recall for that we have our relu which is simply this what the relu
1839:08 - does is all values all values it takes which are less less than 0 are sent to
1839:17 - 0 and all values greater than 0 are maintained so if you if you pass in a
1839:23 - value like 3 the other to the relu you get back 3 but if you pass say negative
1839:31 - 0.5 negative 0.5 what you get will be 0 because all negative values are sent to
1839:39 - 0 and all positive values are maintained this guy is you remain X if X is greater
1839:46 - than or equal 0 and you go to 0 if X is less than 0 now for the final layer or
1839:58 - rather for all the other this is for the final layer this is what I use for the
1840:01 - final layer now for all the other layers they use the leaky relu for the leaky
1840:07 - relu what we have is not this year not the straight horizontal line but inside
1840:13 - something like this see something a bit slanted and then here is still maintain
1840:18 - the positive still maintain so we still have it's still X it still remains X for
1840:26 - X greater than or equal 0 but for this one it goes to 0.1 X for X less than 0
1840:40 - so for all negative values would we would have 0.1 X so it means our
1840:45 - gradient here this gradient is going to be 0.1 so the bottom border too much if
1840:50 - you don't understand the national gradient anyway we have that we have
1840:55 - this it means that if we if we send a value like say negative 0.5 now what we
1841:02 - will get will be negative 0.5 times 0.1 and not more 0 so that is the
1841:07 - difference and this is activation function used everywhere in the model
1841:13 - except for the last layer so that's what is the that was defined right here now
1841:19 - the the loss function that we hear the uses the sum square error so that's the
1841:26 - simple loss function they use and that's why at the beginning the speak of we
1841:33 - frame object detection as a regression problem so that's it so it's like a
1841:40 - simple regression problem actually that is the sum square of the difference
1841:45 - between the model output and the expected output which is the levels now
1841:51 - that we understand globally how the models build and the training process
1841:57 - let's get to look in-depth into this loss function so right here we have this
1842:05 - loss function and then we're supposing that we have this levels here so here
1842:10 - are levels and then here's what the model predicts remember we have 7 by 7
1842:16 - 7 by 7 cells with cells by 30 for the models predictions whereas the levels
1842:26 - are 7 by 7 by 25 see that so we have this first 5 for the location this
1842:35 - location this location we have to add this one year oops let's change the
1842:42 - color back to red so you see that Clara so we have this location here there we
1842:51 - go we have this location here that's it we have this 5 this 5 and we have the
1843:02 - classes here we also have the classes and then we have this 5 right here now
1843:10 - the way we obtain the loss is we break it up into several parts we'll look at
1843:16 - this part first we'll start with this part now this part you see just
1843:20 - basically add in the path of the first part the next part this part this part
1843:24 - and this part for this first part here this it punishes the model when it makes
1843:30 - errors with respect to whether there is an object in a particular grid cell or
1843:35 - not so if we have for this grid cell one as a level we expect in the model to
1843:41 - predict a one year or end a one right here so this means that what will happen
1843:48 - here is we are gonna go through you see this sum from i equals 0 to s square s
1843:53 - square here in our case s is 7 so s square is 7 square which is 49 so we go
1843:59 - through each and every grid cell here which is logical we go through each and
1844:03 - every one of this so we go through each and every one of this 49 different grid
1844:08 - cells and then we'll calculate the difference see this you have c i c i
1844:15 - shuffle that's c i minus c i shuffle square so we have this here minus this
1844:22 - square and then we add all those up now also notice that there's a double sum in
1844:28 - your the reason why we have this double sum is because we actually gonna take
1844:32 - this minus this plus this minus this see that take this minus this plus this
1844:40 - minus this if we had say five or three boxes then we'll have three of this we'll
1844:45 - have one two and add another one before the classes so in that case we will go
1844:50 - three times so hopefully that's clear but what thing you notice is this this
1844:57 - notation here one one of OBJ I OBJ is actually object one of object I so you
1845:08 - notice this notation right here now or this notation let's get back and just
1845:14 - circle that out here so it's clear so this is this is it right here you'll
1845:18 - notice this and what to say in the paper is this one OBJ I notation denotes if an
1845:26 - object appears in cell I and this one OBJ IJ because this is actually IJ not
1845:33 - I IJ denotes that the jth bounding box that's here we have two bounding boxes so
1845:40 - either this bounding box or this bounding box predict predict or in cell I so I is
1845:46 - the cell clearly see that I goes to 49 and then J goes to 2 if we if we have
1845:58 - this then it should be I equal 1 to 49 and this should be J equal 1 2 2 this means
1846:03 - that there's a slight error notation right here anyways we understand that
1846:07 - we're going from 1 to 49 and then we're going from 1 to 2 because we're
1846:12 - basically going through each cell here and we're also going through each and
1846:15 - every one of these boxes now getting back to our one IJ or one OBJ IJ
1846:21 - notation we're saying that oh we've already seen that this denotes that the
1846:27 - jth bounding box predictor in cell I in the given cell is responsible for that
1846:34 - prediction you see that now what does this mean it means that if a particular
1846:40 - like here if a particular box if this box is not responsible for the
1846:46 - prediction then we are not going to include it when computing this error now
1846:53 - how do we know whether this box or this box is responsible for the prediction
1846:59 - the way we get this is simple let's pull this to the right or let's reduce that
1847:06 - so we could get more space so what happens is let's suppose we have this
1847:13 - image and then we have one object here and we have another object here then we
1847:21 - have some bounding boxes so we have this bounding box and we have this other
1847:26 - bounding box now for this for this year we have a particular cell let's suppose
1847:35 - our we're breaking this up and then we have a given grid cell like this one
1847:40 - which is responsible for predicts in this object now this the first box you
1847:44 - see this first box will predict maybe this bounding box and then the next box
1847:50 - will predict maybe this bounding box now what we say we predict this one this
1847:55 - will predict this bounding box and the other predict this is actually because
1848:00 - they have different values for X C Y C W H see this quadruplet here is different
1848:07 - from this other quadruplet and because they are different it means that
1848:11 - obviously the bounding box that you get will be different and because those
1848:16 - these two bounding boxes you get will be different it means that you you can now
1848:21 - compare which of these two is closest to the actual bounding box which is this so
1848:30 - this is the actual and this is what the model predicts so we comparing these two
1848:34 - they're competing for which of them is closest to the actual so let's suppose
1848:38 - that the actual is is something like this suppose that actually something like
1848:43 - this here so we have something like this okay so in that case it's clear that
1848:50 - this one let's look for a neutral color now it's clear that this because we
1848:57 - remember we are having this one this box competing with this black box competing
1849:06 - with this black box so this is B 1 and B 2 competing but the blue box is the
1849:14 - actual one B we just call that B so we're gonna compare the difference
1849:18 - between B 1 and B and the difference between B 2 and B now the one which is
1849:28 - which resembles B the most that's the one which has the least the smaller
1849:33 - difference will be the one responsible as I said here for that prediction you
1849:41 - see that so in our case here is clear that B 1 is responsible so for this
1849:47 - particular case because for a different grid cell you may have B 2 responsible
1849:51 - for whatever grid cell you may have again B 2 or B 1 it just depends on on
1849:55 - what on this difference between the the the bounding box by that box specific
1850:03 - box and the actual bounding box now that said another question you may ask
1850:10 - yourself is how do we compare this bounding boxes now the way we compare
1850:14 - this bounding boxes is by using the IOU score so if we have two bounding boxes
1850:19 - like this we have this two bounding boxes and then we have let's see this
1850:28 - one let's say we have this other box here and then we also have this one
1850:35 - something like this if we had to compare the how close this one this pair this
1850:44 - pair of boxes is compared to how close other pairs you see clearly that this
1850:50 - pair is closer or simply put get more closer to each other as compared to
1850:58 - this other pair right here now the way we look at this is we compute the area
1851:05 - between the two boxes so that's it you do for this area of the intersection so
1851:12 - this is their intersection and then so we have here we call this IOU IOU
1851:18 - actually let's just put it right here IOU actually stands for intersection over
1851:23 - union intersection divided which is equal the intersection we'll call it
1851:31 - intersection divided by the union so if you take for example this intersection
1851:40 - here and divide by the area this area plus this area then that's actually
1851:47 - including the this intersection is basically this is this is the
1851:51 - intersection and then let's change this so you can see Clara and this is the
1851:57 - union see this this is our union that's it so that's our union and this is our
1852:04 - intersection so we take this area divided by all this area here and we get
1852:08 - the IOU score we're gonna repeat the same process for this one where this is
1852:13 - our union and you could see clearly that this one will have a higher IOU
1852:20 - compared to this one and so this is how we compute or we know which of the boxes
1852:27 - is responsible for that prediction now getting back here let's get back to our
1852:34 - loss function as we're saying we're gonna have that if this box for example
1852:40 - if it happens that this box be one year this box here is responsible for the
1852:45 - prediction then would have this difference times one see times one now
1852:53 - if this box is not responsible then we have times zero so this this is not
1852:58 - going to be considered when we computing this loss see that that's it so it's
1853:05 - true we summing through the two boxes but actually we're gonna take only or
1853:10 - consider only one when calculating this difference for this one we're not gonna
1853:14 - we're gonna omit it now we move on to the next this other one year computes or
1853:21 - permits us to have grid cells or permits us to correctly predict when there is no
1853:29 - object notice here this is no object here is object so what we have here is
1853:34 - for the cells where we have an object like you see well where the level we
1853:40 - have the cell and the cell we're gonna use this year where there is no object
1853:46 - we're gonna use instead this one year and here basically when there is no
1853:52 - object we're just gonna take the the output or the value we have here minus
1854:00 - the value we have here then plus the value we have here minus the value we
1854:06 - have here now the next I also know that we have this lambda no object now in the
1854:12 - paper the talk a little bit more about this here we have to remedy this first
1854:19 - of all let's understand this here to say they use the sum square error because it
1854:25 - is easy to optimize however it does not perfectly align with our goal of
1854:29 - maximizing the average position it weighs localization error equally with
1854:35 - classification error which may not be ideal also in every image many grid
1854:41 - cells do not contain any object so this pushes the confidence cards of those
1854:46 - cells towards zero often overpowering the gradients from cells that do not
1854:52 - contain objects now this can lead to model instability
1854:56 - concentrating to diverge early on so to remedy this they increase the loss from
1855:03 - the bound and box coordinate predictions and decrease the loss from the
1855:07 - confidence predictions for boxes that don't contain objects we use a parameter
1855:13 - we used two parameters lambda coordinates this photo the positioning
1855:18 - and lambda no objects for when we have no objects to accomplish this so we set
1855:24 - lambda coordinates for five and lambda no objects to 0.5 so as we're saying
1855:30 - this lambda no object here 0.5 and lambda coordinate is five as they've
1855:37 - given us right here now all can deduce from this from this looking at this
1855:42 - formulas here is that the model will be punished more severely if it has if a
1855:52 - particular grid cell was meant to predict an object and it didn't predict
1855:56 - that as compared to when it doesn't have an object and it didn't predict that
1856:04 - correctly so for the object we have more punishment as compared to this one
1856:10 - because this lambda no object is 0.5 now for the coordinates is it receives
1856:16 - highest punishment year because the as we as we have lambda coordinate equal
1856:21 - five for the classes it's still equal one so here we have one zero point five
1856:26 - one and five five now getting back here we have this one for the classes
1856:34 - basically what we have here is we have this condition so this is one object or
1856:39 - OBJ of I this is the one was OB or one OBJ IJ so notice that this is now I now
1856:47 - here basically what we having is we calculating this difference only when
1856:51 - that grid cell has an object so if like we in the level year if we have a one
1857:00 - like for this two grid cells then we'll go ahead and compute this difference
1857:04 - but in the case where we have no object like in the cell in the cell the cell in
1857:11 - the cell or all other cells have this too then we wouldn't get into this so
1857:18 - we'll just keep that not also we have this oh sorry we have we actually here
1857:24 - wouldn't get into this so here we have just one now this this what we do here
1857:30 - is similar to what we're gonna do for the coordinates with the coordinates we
1857:34 - have the same process where if there is no object wouldn't go ahead to compute
1857:43 - this difference so like here if this is zero then we'll skip this now if it is
1857:48 - one then we'll go ahead and compute the difference between this and this and
1857:51 - that's what we have here now this is the X minus X of bar square plus the X Y
1857:57 - minus Y bar square is basically this X minus this square plus this minus this
1858:03 - square and then here and also notice that you know this this two squared and
1858:08 - then it's added up and then here we have the square root of the width minus the
1858:16 - square root of the order width or the predicted width all of that square plus
1858:21 - the square root of the height minus the square root of the predicted height all
1858:24 - of that square and then they add this up and multiply by lambda coordinates but
1858:31 - it should be noted that we are only going to compute this if we happen to
1858:37 - fall in a box which is responsible for that prediction so if this box is
1858:43 - responsible for the prediction it means that we are not going to make we're not
1858:47 - going to compute this for this box right here we're not going to compare this and
1858:52 - this we're going to do this and this because this box is responsible and
1858:57 - we've seen already what it means by a box being responsible for the prediction
1859:01 - getting back to the paper let's get it back up here now what you have also is
1859:08 - that the sum square error or equally waste errors in large boxes and small
1859:14 - boxes so the error metric should reflect that small deviations in large boxes
1859:19 - matter less than in small boxes to partially address this we predict the
1859:26 - square root of the bounding box width and height instead of the width and
1859:30 - height directly so what you're saying is if we have this couple here these two
1859:36 - boxes and then we have this deviation or we have the difference which we're trying
1859:45 - to compute for the loss and then we have also this smaller boxes here with this
1859:50 - similar difference so let's let's try to have something similar to that we have
1859:55 - something like this hope it's similar enough so we have something like this
1860:01 - with the initial method we will have W minus W bar or Chapeau or T over the
1860:09 - square plus the height minus the height bar square where this is W here is what
1860:19 - the level expects and the W by is what the model predicts so the same
1860:25 - for the H and the H bar now what the one is that if we have this difference here
1860:32 - or better so let's say that what is going on with this year is that if we
1860:40 - have this year these two boxes this difference because the equal difference
1860:46 - you see this difference is the same if we have this year then let's say the
1860:51 - differences is five let's suppose that this difference is five year difference
1860:56 - five that's the width and the height difference is five then we'll have five
1861:02 - square plus five squared as 50 now for this small box here this is five and
1861:08 - then here is five then we also have 50 but this is not what we want the reason
1861:16 - why we don't want this is because this kind of difference for smaller boxes
1861:21 - is more important than this difference for this bigger boxes so it's just like
1861:30 - you you suppose in that you have say say you have a loaf of bread like this
1861:37 - suppose we have a loaf of bread and then you have this part here which you cut
1861:45 - the control now compared to a case where you have this smaller loaf of bread you
1861:52 - will find that cutting off this part here is like cutting off practically a
1861:56 - third of my loaf whereas oh for this case this was like cutting out say one
1862:03 - tenth of my loaf so so clearly from here the loss is less felt as compared to
1862:09 - this other one and so to as I say in the paper to remedy the situation to add the
1862:15 - square root here now let's add the square root and see this difference now
1862:20 - the square root of let's say let's say this was 30 and this was 25 so we had
1862:28 - five and then there was 30 and 25 the square root of 30 now is let's say this
1862:34 - is 30 width 30 and for this other one so this one has width height 30 and
1862:43 - so that one has width height 25 so this gave us the difference which was 5 and
1862:49 - that's how we had this difference here now when we when we take now the square
1862:53 - root of 30 let's take the square root of 30 the square root of 30 that's 5.47 okay
1863:06 - so 5.47 now minus the square root of 25 will be 0.47 see
1863:12 - 0.47 which when you square let's compute that directly which when you are gonna
1863:20 - square it will give you approximately 0.22 see so you have 0.22 plus 0.22 now
1863:34 - that will give us 0.44 or 0.44 for this two bigger boxes now for the smaller
1863:45 - boxes let's suppose that this one was say because we want to have a difference of
1863:51 - 5 we could say 10 and 5 so here we had 10 by 10 and here we have 5 by 5 in
1864:00 - that case we will have square root of 10 minus square root of 5 and now when you
1864:07 - compute that you would have 0.84 so what I mean here is when you take the square
1864:17 - root of 10 minus square root of 5 and square it gives you 0.84 0.84 plus 0.84
1864:24 - we give you about 1.6 let's say 1.6 anyway that's already much greater than
1864:31 - 0.44 it means that the model is penalized more now for making this error
1864:37 - as compared to making this error so that now solve the problem of where the the
1864:44 - model would have penalized them the other model would have been penalized in
1864:49 - the same way for this same difference even when the size the size difference
1864:57 - between these two boxes is quite considerable now that said from here
1865:02 - this the train for over 135 epochs on training and validation data sets from
1865:07 - Pascal VOC in 2007 and 2012 we test with testing on 2012 we also include the VOC
1865:16 - 2007 testing for training test data for training then try training we use a
1865:21 - batch height of 64 momentum of 0.9 and decay that's with decay of 0.000 5
1865:27 - then you ratio was as follows for the first epochs will slowly raise the
1865:32 - learning rate from 10 to the negative 3 to 10 to the negative 2 so started with
1865:38 - a relatively lower learning rate slowly increasing it because if we start at a
1865:43 - high learning rate the model often diverges due to unstable gradients so
1865:47 - we continue training with 10 to the negative 2 that's after going from this
1865:51 - was to slowly increase to 10 to the negative 2 and then to continue to
1865:55 - training with this 10 to the negative 2 for 75 epochs then 10 to the negative 3
1866:00 - for 30 epochs so after this 75 epochs to drop to 10 to the negative 3 and then
1866:05 - finally drop against 10 to the negative 4 for 30 epochs that makes now 75 plus
1866:11 - 30 that's 105 plus 30 135 epochs okay so to avoid over fitting we use drop out
1866:21 - and extensive data augmentation drop out layer with rate 0.5 after the first
1866:26 - connected layer prevents caught up adaptation between the layers for data
1866:31 - augmentation they introduce random scaling and translations of up to 20% of
1866:35 - the original image size we also randomly adjust the exposure and saturation of
1866:40 - the image up by up to a factor of 1.5 in the HSV color space now given that
1866:47 - after the detection has been made let's get back to the top that's that's after
1866:54 - or let's say after the model has been trained we would get detections like
1866:59 - this increase this would get detections like this so we might have many more
1867:05 - detections than expected so we're gonna apply the non max suppression algorithm
1867:10 - to remove those cells or rather to remove those bounding boxes which are
1867:17 - repeated around a certain region and focus only on bounding boxes which have
1867:22 - the highest probability scores so like this one you see the thickness here
1867:27 - signifies the probability score the probability of an object being that
1867:31 - location see that so that's why we left with this after the non max suppression
1867:37 - now the way this non max suppression algorithm works is as such we have after
1867:43 - the person after the model has been trained we pass this input image and we
1867:47 - may get predictions like this now let's let's suppose that the this one this
1867:52 - this two whites have the highest probabilities and then we also have some
1867:56 - other predictions we have maybe see this other prediction here oh that's it we
1868:04 - have see another prediction around here something like this now what we're
1868:10 - gonna do is we are gonna consider that for a particular object let's say this
1868:15 - this bounding box right here for a particular bounding box we look at its
1868:20 - probability and compare with that of the bounding box around it and obviously to
1868:27 - know what a bounding box surrounds or is very close to this bounding box we look
1868:30 - at the IOU so if we fix the IOU to a threshold of 0.5 it means that if we're
1868:39 - taking this box for example considering this box then any box with an IOU that's
1868:46 - any box is close enough to this here sorry that is IOU is greater than 0.5
1868:53 - meaning that they are very close then we are gonna remove that bounding box so it
1868:59 - means I was gonna take this off because they are already very close so you see
1869:05 - that now you could you could play around with this value meaning that you could
1869:08 - take say 0.2 or even 0.7 depending on the data set you're working with now so
1869:18 - what we're saying is because these two are very close to each other we take
1869:23 - that off now obviously they must be present the same object now if we have
1869:29 - another box like this one if we had another box like this one see suppose we
1869:34 - had another box like this one then this box will not be taken off because the IOU
1869:38 - is less than 0.5 so when the IOU is greater than 0.5 we know that they are
1869:45 - very close to each other we compared your probabilities the one with the
1869:48 - highest probability is gonna win the other one is gonna be taken off and the
1869:52 - term known max suppression so if you are not in you're not a max we're gonna
1869:57 - suppress you so we suppress all of that and you see this one is left now for
1870:02 - this year we're gonna say okay this one has the highest probability we're not
1870:06 - we're not gonna compare this with this because obviously the the IOU will be
1870:11 - less than 0.5 now we're gonna take this one and this one we're gonna compare
1870:16 - this to the IOU is gonna be greater than 0.5 and so here we're gonna remove
1870:22 - this other one so this one here will be taken off you see this one is still left
1870:28 - so that will be it will be left now with this three predictions anyway generally
1870:36 - when training our YOLO model we aim to even be able to avoid the known max
1870:42 - suppression as a whole and other YOLO variants have been developed to try to
1870:50 - reduce that dependence on the known max suppression other YOLO variants are like
1870:55 - the YOLO 9000, YOLO V2, YOLO V3, YOLO V4 you also have YOLO V5, YOLO X, YOLO R
1871:03 - which perform even better than this YOLO V1 we're discussing right here so here
1871:10 - you have some tables which compare to other methods see you could always look
1871:17 - review this here fast R CNN you have the YOLO see that it performs better than
1871:25 - the than the YOLO but this one is faster than the fast R CNN we also have this
1871:34 - comparison table for different objects see for different objects we see the
1871:40 - the precision for this different objects and we compare with this different
1871:46 - methods here you have the YOLO then you could get down here see the recall see
1871:54 - that we've already had a tutorial on the precision and recall so you should be
1872:00 - able to understand this now if you're new to that you could check out our
1872:03 - previous videos here this is some quantitative results in the VOC 2007
1872:10 - because when people aren't data sets so that performs best on the on the this
1872:15 - VOC 2007 it performs best on the Picasso it runs best on the people art data set
1872:23 - let's get back here oh okay this is fast R CNN while you're there comparing with
1872:29 - the with a with a simple R CNN okay so that's it we see that with a Picasso and
1872:36 - the people art it performs in outperforms other methods like our CNN
1872:40 - mean that it's as marginalization capacities as compared to other methods
1872:47 - techniques here we have some limitations of the YOLO YOLO imposes strong spatial
1872:52 - constraints and bounding boxes so since each grid cell only predicts two boxes
1872:59 - and can only have one class so this means that let's get back here this
1873:05 - means that if we have if we had a person who was say starting just behind
1873:13 - rambien sort of person here it would have been difficult to predict this
1873:17 - person and this other person and also in the case where we have images where the
1873:23 - objects are quite small so we have some very small objects and act up like this
1873:29 - this YOLO more algorithm or this YOLO model will find it difficult in
1873:36 - detecting each and every one of them getting back to the paper our model
1873:42 - struggles with small objects that appear in groups such as flux off of birds
1873:46 - since our model learns to predict bounding boxes from data it struggles to
1873:50 - generalize to objects in the new on usual aspect ratio or configuration so
1873:55 - this has been trained on the Pascal VOC dataset where the objects have certain
1874:00 - aspect ratios I'm a turn it out you have a different data set where the aspect
1874:05 - ratio is different by aspect ratio with simply meaning the the width to height
1874:10 - ratio so it's this ratio right here this this aspect ratio can be say two by five
1874:18 - if we taking height by width then you're gonna be like say three by two so what
1874:24 - what goes on here is you have trained this on Pascal VOC dataset where you
1874:29 - have a specific as or a general kind of aspect ratio or aspect ratios but when
1874:36 - this is taken to different images where the aspect ratios aren't similar to that
1874:42 - of the Pascal VOC then past the YOLO model finds it difficult or struggles
1874:47 - to generalize in such situations finally we went wild train while we trained on
1874:54 - a loss function that opposomates detection performance our loss function
1874:58 - trees arrows the same small bounding boxes versus large bounding boxes and
1875:05 - they say that the main source of error is incorrect localizations that said
1875:11 - we're done with this review of the YOLO paper in the next section we are going
1875:15 - to build this YOLO from scratch
1875:22 - hello everyone and welcome to this new and exciting session in which we shall
1875:27 - focus on preparing our Pascal VOC dataset using the TensorFlow dataset
1875:33 - pipeline so here on Kego we have this Pascal VOC dataset which is made
1875:39 - available by Wang Hang China and it's made of this five different directories
1875:44 - that is annotations image sets JPEG images segmentation class and
1875:49 - segmentation object nonetheless we shall be making use of the JPEG images and
1875:54 - annotations for our object detection problem and now get into the code we
1875:59 - are going to start by installing Kaggle we are going to copy this Kaggle.json
1876:03 - file into this directory which we just created now note that this Kaggle.json
1876:08 - file as we've seen already is gotten from our Kaggle account so you get this
1876:13 - from your Kaggle account and you copy out here and then now after this copy you
1876:20 - change the access mode of the file and then start with the dataset downloading
1876:26 - now to download this or to have that command you just simply scroll like this
1876:31 - get here copy API command and you paste it out here so what we have here is
1876:39 - simply what we've copied so that's it that's how we download this dataset so
1876:43 - we run that and we download the dataset now once this we download it we'll go
1876:47 - ahead to unzip the content of that dataset into our dataset directory
1876:54 - right now we'll have our dataset there we go we have our dataset we pick out
1877:02 - this one here and you see we have the annotations image sets JPEGs
1877:07 - segmentation class and segmentation object open up this and this okay so
1877:12 - that's it now what we'll do is we are going to define some variables here we
1877:18 - have our train images which is simply this path to this JPEG images we have
1877:23 - our train maps which is simply the path to the annotations so that's it and then
1877:29 - we have our classes so Pascal VOC dataset takes in 20 classes from
1877:35 - airplane right up to TV monitor then we have B set to 2 now to understand the
1877:42 - significance of this B remember from the paper that we had seen that the image is
1877:49 - divided into an S by S grid and for each grid cell and each grid cell predicts B
1877:56 - bound in boxes now the other define S to be 7 and then B to be 2 so that's the
1878:04 - this number of bounding boxes which we are considering to be B and that's
1878:09 - exactly what we have here so that said we have number of classes which is
1878:14 - simply from year 20 we have the image height and width which is considered to
1878:20 - be 224 we have the split size S which is 224 divided by 32 that's equals 7 we've
1878:27 - just seen that this is actually S in the paper we have number of epochs 100
1878:33 - learning rate defined although we have some sort of learning rate scheduling so
1878:38 - we could take this off and here we could say 135 then we have a batch size of 32
1878:45 - so that's it we define all this and then we move to the pre-processing of
1878:52 - annotations now given that this annotations are essentially this XML
1878:58 - files we have here what we are going to be using is this element tree from this
1879:03 - XML package which will use to parse this XML data right here so diving into the
1879:11 - code you could see here we have the file name which is passed into this parse
1879:17 - method from which we obtain a tree then from the tree we could get its root once
1879:24 - we obtain the roots we cannot get the tree size see we have root that fine and
1879:29 - we specify size and then once we have this tree size we could get the height
1879:35 - of a specific image and its width here we have width width and height that is
1879:43 - how we obtain this tool from the size tag so we could also obtain the depth
1879:48 - from here let's copy this and paste out here and then let's say we want to get
1879:54 - the depth here we specify depth and then you see we obtain a text so let's run
1880:03 - that and then get the depth we run that and then let's have pre-process XML and
1880:12 - then the file name its train maps actually it's actually a fire path
1880:18 - train maps plus we have this file 207 207 oh oh oh 33 dot XML okay so it's
1880:31 - actually this exact file here see this if you look up here or if you look at
1880:35 - this here file name exact same file name so let's run this and then see what we
1880:40 - get there we go we should have okay so we have 366 for the height see here we
1880:48 - have 500 for the width and then we have three for the depth we've converted all
1880:53 - this into floats okay so now we've done with obtaining the images width and
1881:01 - height which are all in this size tag let's now move on to obtaining the
1881:08 - different bounding boxes of the different objects so you see here we had
1881:12 - root dot find size that's because we have a single size for that image now
1881:18 - here you see we have rooted find all objects and that's because we will have
1881:24 - many or we could have many objects for a single image here again single size but
1881:30 - you're great no or a possibility of having more than one object so that's
1881:35 - why you always specify find all so we are going to find all objects meaning
1881:40 - that we are going to get into each and every object tag we have here you see
1881:44 - this is an object this is another object if we scroll down we'll see we have
1881:50 - another object so essentially in this image we have one two three objects okay
1881:58 - so we have this three different objects and now for each and every bounding box
1882:04 - in this objects like this is our object tree here if we pick out a specific
1882:10 - object so here we pick out this object for example we have which we go through
1882:15 - each and every bounding box in this object and we take its mean that's x
1882:22 - mean y mean x max and y max so that's exactly what we do here you see we have
1882:27 - bounding box that find now x mean and then we convert that to text we have y
1882:32 - mean text x max and y max and at the end of this we now convert this into a float
1882:39 - so that's how we obtain x mean y mean x max and y max now you'll notice that we
1882:44 - have a break here and the reason why we want to have this is because for a
1882:48 - particular object we just need a single bounding box so if we have other bounding
1882:54 - boxes we are not going to take those into consideration okay so that said now
1883:01 - what we'll do is we're gonna print out x mean y mean x max y max and yeah x max y
1883:11 - max okay so let's print this out for each and every object now let's run this
1883:17 - and see what we get as we expect you see we have 9 107 499 263 that is that what
1883:26 - we have here we have 421 200 482 226 and then finally we have this so yeah
1883:32 - what three different objects now what if we try out another different image so
1883:37 - let's change this to 32 we run that and see what we get yes it's on XML 5 series
1883:44 - 32 instead of 33 you see now we have actually four different objects and so
1883:50 - we have this four different bounding boxes which you could see here we have
1883:53 - object object object and object now if you consider this image here you see we
1884:01 - put this cursor on this point we have could read from here we have 24 188 so
1884:08 - matches are with this year see this 26 189 and then here we have 46 240
1884:16 - matches are with 44 238 now in order for us to make it easier when working with
1884:24 - a yellow encodings what we are gonna do is we are gonna get the center of this
1884:30 - bounding box so the center should be around this point here now that center
1884:35 - is about 35 215 so here we have 30 oops let's get a pan so here is about 35
1884:46 - 215 and then now we've gotten the center we could also get the width the
1884:55 - width is about 18 and then the height the height is about 238 minus 189
1885:09 - that's 49 so we we now have the center which is this we have the width and then
1885:17 - we have the height and then what we'll do is we'll divide all this by the total
1885:23 - width and total height of the image so for 35 would take 35 divided by the
1885:30 - total width the total width of this is 500 so we have 35 divided by 500 and
1885:36 - then we'll have 215 divided by well this 215 is divided by the height that is 281
1885:48 - because this this year is x-coordinate and then this is our y-coordinate so
1885:56 - this is respect to the width and then this respect to the height so this is
1885:59 - divided by 281 okay so we take this divided by 500 and then this
1886:08 - divided by 281 then we have the width which is 18 divided by 500 and then we
1886:14 - have the height 49 divided by 281 and so instead of having X min Y min X max Y
1886:22 - max we have the center which is divided by or which is normalized and then we
1886:28 - have the width and the height which are also normalized now putting this in form
1886:33 - of code once we're done with getting a specific bounding box we could go ahead
1886:37 - and obtain the class name so you see we have as usual make use of our object
1886:43 - tree and then we find the name see here here we have name and here is airplane
1886:48 - here's a plane here's person here is person we're not gonna be interested in
1886:52 - the pose or what else truncated or not or what else difficult or not just
1886:56 - interested in the name and this bounding box just as we have seen already that
1887:01 - said we have our class name from this class name we could create this class
1887:08 - dictionary which will use to convert the different class names into a specific
1887:14 - integer so what this simply means is we're gonna convert airplane to zero
1887:19 - bicycle to one bird to two both to three and so on and so forth so this is
1887:25 - 0 1 2 3 4 5 6 7 8 9 10 this should be 10 yeah 11 12 13 14 okay so we have
1887:40 - airplane which is 0 and person which is 14 okay let's take note of that now
1887:44 - getting back here you see we're gonna we make use of this dictionary where we
1887:49 - simply have a class and that's converted into an integer so that's quite
1887:55 - straightforward and then now we have our bounding box which is essentially x min
1888:00 - plus x max divided by 2 that is we get the center this is the center and then
1888:06 - we divide by the width so that's it we have x min plus x max divided by 2 times
1888:13 - width is essentially the center divided by the width and then we have the y
1888:18 - center that's y min plus y max divided by 2 that's the center then divided by
1888:24 - the height and then for the width we want x max minus x min because to obtain
1888:29 - the width to obtain this width you simply take this minus this to obtain
1888:34 - the height we take this minus this and that's it so that's how we obtain the
1888:41 - width and the height so here we have x max minus x min then divided by the
1888:46 - width and then we have x y max minus y min divided by the height and then we
1888:51 - have our class which is gonna be an integer instead of say person or
1888:57 - airplane then once we are done with this bounding box we now store this in this
1889:01 - bounding boxes list so let's create your bounding boxes list there we go
1889:09 - we have the bounding boxes list and then now we will return bounding boxes so
1889:21 - that's it let's return that and then there we go so let's run this and then
1889:26 - see what we get now first thing you can notice is that we have our four bounding
1889:31 - boxes now take note of the fact that we have the classes 0 0 and 14 meaning that
1889:37 - we have airplane and person which matches exactly what we expect and then
1889:42 - when we get back to this image you see 35 divided by 500 and 215 divided by 281
1889:48 - should have 35 divided by 500 and then 215 divided by 281 okay so you see we
1889:59 - have 0.07 and 0.76 okay so does it make sense and then for the width for the
1890:05 - width we had 18 divided by 500 and 49 divided by 281 so here we have 18
1890:11 - divided by 500 and then 200 divided by 281 0.04 or 0.036 and 0.17 0.71
1890:25 - this should be 71 oh let's get back here it's actually 49 divided by 281 and not
1890:32 - 200 so this will be 49 because the height is 49 so we divide that and then
1890:38 - see we should have 0.17 okay so that makes sense so that is it we have
1890:44 - encoded bounding boxes and now we are ready to produce our outputs based on
1890:52 - what was described in the paper so in the paper we saw that our output will be
1890:58 - this 7 by 7 by 30 tensor where each and every cell we have here does each and
1891:08 - every one of this 49 different cells because we have 7 times 7.49 will take
1891:14 - values depending on whether they have an object or not now for a cell like this
1891:20 - one that's actually matching up with this one where there is no object will
1891:26 - take values like would have a value of 0 for the objectness meaning that there
1891:32 - is no object and then for the positioning we will have this year
1891:38 - that's four zeros and then for the class because there is no object we will have
1891:44 - all zeros now we have 20 classes so we'll go from 0 or we'll have 20 of this
1891:52 - zeros and there we go so you see we have 20 of this zeros now we're gonna have
1891:59 - the same for each and every grid cell where we do not have an object now you
1892:04 - should be noted that a grid cell like this one for example this one here let's
1892:10 - change your color this grid cell here stick this well let's get back this grid
1892:17 - cell here has actually no object because we consider a grid cell to have an
1892:25 - object if the center of that object is in the grid cell now although we have
1892:29 - the wing of the plane in this cell given that the center of this plane isn't in
1892:36 - this cell we do not consider that this cell has an object so here would have
1892:41 - exact same values we have here so there is no object like this is 0 1 2 3 this
1892:48 - is 0 1 so we'll go 0 1 2 3 0 1 so here would have this exact same values all
1892:56 - zeros and with this with this with this this and all this other cells now what
1893:05 - we left with will be this cell which contains the center of this object this
1893:10 - cell which contains the center of this object this cell which contains the
1893:15 - center and this cell so we have four cells which contain the center or the
1893:21 - centers of this four different objects while the other cells contain no object
1893:28 - now we shall focus only on this one here so you understand how this outputs are
1893:34 - generated based on the bounding boxes and this length this length here is
1893:40 - essentially the number of bounding boxes so it's got it could be gotten from this
1893:45 - bounding boxes so here we have bounding boxes which is in this case here for this
1893:50 - object is actually what we have here that is we've normalize this value so
1893:54 - that we have the center x center normalized y center normalized we have
1894:00 - the width normalized the height normalized and we have this class now as
1894:05 - we saw in the paper this isn't exactly what we want so what we want is a value
1894:13 - which tells us the position of the object we respect to that specific grid
1894:18 - cell so if we take off this year let's take this off and then we specify the
1894:24 - center the center here is around this position here your so center will find
1894:33 - that based now on the paper this position here has to be encoded such
1894:39 - that we have the this value based off this origin so it's based off this
1894:45 - origin because we have a grid cell here which is this one so we have this grid
1894:50 - cell it's actually the same grid cell we have here so it's based off this and not
1894:55 - based off this origin of the whole image remember the image has its origin and
1894:59 - this grid cell has its own origin now let's shift this isn't very clear let's
1895:05 - shift this so that we have this full okay so you see clearly now the origin
1895:10 - of the image which is this and the origin of this accurate this point here
1895:15 - and the origin of the grid cell which is this point and then we have the center
1895:19 - of the image which is around this year so centers around this so the idea now
1895:24 - is to obtain this distance from this year from well from this origin to this
1895:31 - point and that's it so let's get this distance and this distance so that's it
1895:41 - we need to get the distance from year to year this is from year to year as of now
1895:46 - we have our center normalized we respect to the whole image and if we want to
1895:52 - normalize this now with respect to a specific grid cell we need to take this
1895:57 - value and multiply by the number of grid cells we have so given that we here we
1896:02 - have 0.07 we'll take that and multiply by 7 so we have 0.07 times 7 which will
1896:11 - give us 0.49 and then we'll take 0.75 0.75 times 7 which will give us about
1896:21 - 5.25 now what this means is that the distance from year to this that's in
1896:29 - this horizontal direction is 0.49 that's approximately 0.5 and this makes sense
1896:34 - because the distance from year that is origin to where we have the center year
1896:40 - of this object is approximately half of the distance from year to the full cell
1896:48 - and then the distance from year this origin going the horizontal in the
1896:54 - vertical direction to this center is approximately 0.25 so we go about 0.5 and
1897:02 - then year about 0.25 nonetheless after multiplying year that's 0.07 times 7 we
1897:11 - have 0.49 that's 0.5 that makes sense we have we also multiply 0.75 times
1897:16 - 7 that gives us 5.25 but this distance actually only 0.25 so what we'll do is
1897:24 - we'll take 5.25 modulo 1 and we'll obtain 0.25 because the distance from this
1897:34 - origin to this center the center this is the center of the object is 0.5 in this
1897:43 - direction horizontal and 0.25 in the vertical direction now let's make this
1897:48 - bigger so you could see that even clearer what we're saying is we have a
1897:52 - center which is around this and then we have this origin here the distance from
1897:58 - year to year is about 0.5 of our cell and then the distance from year downward
1898:08 - up to the center this distance is about 0.25 of our cell and we've seen that to
1898:16 - compute this automatically what we need to do is get this already normalized
1898:22 - values from year we already have this normalized values we multiply them by
1898:27 - seven and then we compute the modulo of those where we find the output from
1898:33 - year modulo 1 to obtain these distances so let's do 0.49 modulo 1 it should give
1898:44 - you 0.49 and then 5.25 modulo 1 it gives you 0.25 so you see that now we have
1898:54 - this center with respect to this grid cells origin and that's exactly what we
1899:01 - had in the paper now diving into the code you see that we are going to create
1899:06 - this output level which is essentially going to be a 7 by 7 by number of
1899:13 - classes plus 5 that's 25 the number of classes 20 by 25 tensor and then we are
1899:21 - gonna go through each and every bounding box we have here and then put in the
1899:27 - values corresponding to the specific cells so again we have seen that for all
1899:33 - these different cells here we have all zeros but for this cell this cell this
1899:40 - cell and this cell we have non-zero values not all values are zeros so let's
1899:49 - concentrate on this one as we see already so we have here for being range
1899:54 - length that's for being the range of the number of bounding boxes we say that
1899:59 - length is the number of bounding boxes we have the bounding box that specific
1900:04 - bounding box this zero year is simply this so this is X center the center we
1900:14 - multiply by the split size multiply by seven so you just like taking 0.07 times
1900:18 - seven that will give you 0.49 so this is actually 0.49 if we are dealing
1900:24 - with this bounding box here and then for this next one is essentially 5.25 so this
1900:32 - is this times seven will give you 5.25 now or one other thing we need to do is
1900:39 - we need to pick that specific or the specific grid cell to pick the specific
1900:44 - grid cell out of all the 49 grid cells because we have seven by seven year this
1900:49 - one two three four five six seven one two three four five six seven so we have 49
1900:54 - options we need to pick only one option and that's what we are doing right here
1901:00 - so to do that what we need to do here is we take this 0.49 and then we convert it
1901:09 - into an integer always simply round this down so running 0.49 down will give you
1901:16 - zero and they're on in 5.25 down we give you five so it simply means that we are
1901:22 - going in the X direction our is in the direction we had the zero position but
1901:29 - in the y direction we go to the feet position so this is 0 1 2 3 4 5 and then
1901:35 - the x direction we still at zero so that's how we we get this here and then
1901:41 - so we have we have said 0 5 and then the output level 0 5 just like remember we
1901:49 - had this output which is 7 by 7 by 25 this is 1 2 3 4 5 6 so at this other one
1902:02 - 7 and then here we have 7 so we have 7 by 7 by 25 let's add this here we have
1902:13 - this this this 1 2 3 4 5 6 and then we have 7 so essentially what we're saying
1902:25 - here is 0 5 that's 0 and then 1 0 1 2 3 4 5 at this position right here for the
1902:39 - first five values for its first five values would have 1 1 signifies that we
1902:45 - have an object then for the positions you see we have 0.49 modulo 1 which is
1902:53 - going to give us 0.49 then we'll have 5.25 modulo 1 which will give us 0.25
1903:03 - so now we're finding its position with respect to this cell and then we have
1903:10 - the width we have the width here which is this bottom box 2 because it's 0 1 2
1903:16 - so this is the width but remember from the paper it's of what we need to have
1903:22 - here was a width of this bottom box with respect to the whole image so what we
1903:26 - have as value here will stay unmodified so that's it 0.036 0.036 and then for the
1903:37 - next value is 0.17 0.17 so that's how we obtain the first five values you see we
1903:46 - assign this values we have right here now for the classes we have from 5 right
1903:54 - up to 25 so we have from last 5 plus this we assign the value of 1 now to
1904:02 - understand why we what we are doing here remember that this bottom boxes or this
1904:09 - bottom box of value or index value 4 takes in this value 14 so what we're
1904:15 - saying is at the position 5 plus 14 we want to assign a value 1 now remember
1904:22 - that we have this first five values which tell us the objectness or which
1904:28 - gives us the object score which gives us a position and then the remaining 20
1904:33 - values will tell us the class of the object so after listing this we also
1904:37 - have the class but we have 20 zeros see so again we have this objectness right
1904:44 - here we have objectness we have bounding boxes and then we have the 20 zeros now
1904:54 - when we say 5 plus 14 it means that we'll get to the 19th position this is
1905:01 - 5 plus 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 so at this position here you're gonna
1905:14 - take this off and then replace that with a 1 so here we have a 1 meaning that
1905:24 - this year has an object this is bounding box and that object is of class person
1905:34 - so you see here we assign this value 1 and that's it we output or we return the
1905:40 - output level recall that we've just done this for this object but we have four
1905:46 - different objects that's for in fact four different bounding boxes so we'll
1905:50 - do for this we'll do for this we'll do for this and we do for this that's it at
1905:56 - this point we could take all this off and then carry out some testing so we
1906:02 - have this pre-process XML which outputs this year so we could do generate
1906:08 - generates output which takes in pre well let's copy this here let's copy this
1906:17 - paste out here the output of this is this bounding boxes and then scroll so
1906:24 - you can see that clearly and then we have the length of that output okay so
1906:30 - that's it we have this output and we have the length of the output let's run
1906:34 - this and see what we get this isn't defined let's run this and there we go
1906:42 - so we should have something reasonable from this you see we have this output
1906:47 - and if you check this out you see 7 by 7 by 25 and then now what we could do is
1906:53 - we could say okay want to get 0 that's 0 0 so 0 0 let's run that so we get 0 0
1907:07 - that's this cell right here you see all its values all the values of this cell
1907:13 - let's take all this off all the values of this cell here are zeros which makes
1907:19 - sense now let's do 0 5 0 5 we run that you see we have exactly what we expect
1907:29 - here we have 0.49 we have 0.31 meaning that the distance from here to the
1907:35 - center is about 0.31 here we have 0.036 we have 0.174 and then we have a 1 at
1907:42 - this position in that as a person now if we go 0 1 2 well this is 0 1 2 and that
1907:50 - was in the direction and 0 1 2 in the vertical direction so this is 2 2 let's
1907:55 - do 2 2 and see what we have we have 2 let's get back 2 2 we should have an
1908:04 - object there see we have an object so we have here 1 we have his bounding box and
1908:09 - then notice how this is his class remember the aeroplane the aeroplane as
1908:14 - we had here was the very first class so it makes sense that we have a 1 at this
1908:20 - very first position so that makes sense so that's it we see how we could
1908:25 - generate our output from this data set we've we've been given all this XML
1908:32 - files we have for each and every image before we move on we are going to do
1908:37 - some slight modifications on the code so here we no longer need this length we
1908:42 - have the bounding boxes then we're gonna create this numpy array instead of the
1908:48 - tensor flow variable we had before so it's exact same shape as before still
1908:54 - our output level and then we'll get the length directly from this bounding boxes
1909:00 - so we have for being range length of the bounding boxes then to account for the
1909:06 - fact that we are gonna have this computations bashed we are gonna add this
1909:11 - three dots right here so we have now our grid X computed similar to all of your
1909:17 - scene and then we have our grid Y computed still our i j garden and then
1909:22 - here again we add the three dots to account for the batch computations and
1909:28 - so that's it this exact same code and then now we're gonna convert this numpy
1909:33 - array into a tensor so use the convert to tensor method in tensor flow so
1909:41 - that's it we run this and we still have our same output now to ensure that our
1909:48 - validation set doesn't get mixed up with the training we are going to define this
1909:53 - set of 64 images which will be our validation set or which will make up our
1909:59 - validation set there we go we have this vowel list and then the next thing we'll
1910:04 - do is we are gonna copy as you see here we're gonna copy or rather we're gonna
1910:09 - move this files into this two directories so here if you open this up
1910:16 - now you'll see we have we've created this directory vowel JPEG images which is
1910:21 - this and we've created a directory vowel annotations which is this other one
1910:26 - which contain the validation set images and annotations and so that's it so now
1910:35 - we're gonna create this different lists here we have the image paths and the XML
1910:39 - paths we have the validation image paths and the validation XML paths let's run
1910:45 - this you see we have 17,061 files for the training and then 64 for the
1910:53 - validation and that's it again you should note that this training images
1910:58 - has already been defined here we'll define training images right here we've
1911:03 - also defined vowel images and vowel maps so that's where we getting all this from
1911:09 - that's how we obtain all those different paths now from here we're gonna create
1911:15 - the tensorflow data sets that's the train data set and the validation data
1911:20 - set which is essentially going to be made of these different paths which we've
1911:26 - just created so we make use of this from tensor slices method and we put the
1911:32 - image paths and the XML paths and the validation paths and the validation XML
1911:38 - paths so let's run this and then we could visualize our validation data so
1911:44 - here you see we have this path here that's our image path and then you see we
1911:50 - have this on the path which is our XML file so we have the image and its
1911:57 - corresponding XML file then now that we have our image paths and our XML paths
1912:06 - already making up our data set from this tool we could obtain the image and the
1912:15 - bounding boxes for the image all we need to do is pass in the image path in this
1912:22 - read file method then decode that read file and then we go ahead and resize and
1912:31 - cast the image so let's take this off we actually do the casting here so no need
1912:38 - doing that before resizing so as we say we resize and then we carry out the
1912:44 - casting and we obtain our image from this image path now for the bounding
1912:50 - boxes remember we looked at this pre-process XML method which was already
1912:55 - explained here we looked at this method which takes in our XML path our file or
1913:04 - file name and then outputs the bounding boxes so that's exactly what we're doing
1913:09 - here and so that's it now given that this method does a pre-process XML
1913:15 - method isn't made of only tensorflow operations will need to make use of this
1913:21 - numpy function method where we are going to pass in our function here we specify
1913:30 - the input which is the path that's our XML path and then we also specify the
1913:39 - data type of our output tensor which in this case is float 32 so this let's let's
1913:45 - say this is XML path and here we have XML path okay so now we have the path
1913:52 - and we have the method we can now obtain the boxes and then now we have our train
1913:59 - data set which is going to be redesigned such that we have this year we have this
1914:08 - paths the image path and the XML path which gets in here and then outputs the
1914:15 - images and the boxes so you see train data set we map and then we specify the
1914:21 - method which is a get image and bounding boxes this we get the images and the
1914:26 - bounding boxes from the image path and the XML path so now our train data set
1914:32 - is no longer going to give us the the image path and the XML path but it's
1914:37 - going to give us the image itself and the bounding boxes so let's run this and
1914:43 - then see what we get there we go you could see we have this image and then we
1914:49 - have its corresponding bounding box in this case we have just a single bounding
1914:53 - box let's go ahead and write this then we check this out here there we go we
1915:03 - have this output you could see so you see we have this output here and it's
1915:07 - showing that we have an airplane so if you check this out you see we have the
1915:13 - bounding box here and then we have the class the class is zero so if you scroll
1915:19 - to the top you would find that the class airplane was here so this is the zero
1915:24 - class which makes sense now let's try out with some others well most of those
1915:32 - ones here have only one object but there's this one at this 18th position
1915:37 - let's keep let's keep that and then a break there we run this again and then
1915:44 - see what we get okay so this this particular image has several objects so
1915:51 - we could check that out run this okay so you see here we have now many more
1915:59 - objects here we have many many people actually you have 14 and this happens to
1916:06 - be person so if you if you have here yeah we have this well okay so if you
1916:14 - do classes well the list is classes so let's just do classes 14 and we get the
1916:20 - exact class we have the classes 14 and you should see you should have person
1916:28 - okay so you see we have 1 2 3 4 5 6 7 8 persons and if you get here you see we
1916:36 - should have 1 2 3 4 5 6 7 8 exactly what we expect and then we have 10 8 8 well
1916:45 - this 8 this 8 here and then after we check out 10 it is cheer so maybe is
1916:52 - this cheer okay it is twice so we have a cheer year and we also have another
1916:57 - cheer year and then 10 is let's check that out should be dining table okay see
1917:03 - we have the dining table we have dining table we have chair chair and then 8
1917:08 - persons so now we have the image and as button boxes the next thing we want to
1917:14 - do is have the image and its output levels remember with our generate output
1917:19 - method which we had seen already here it takes in the output boxes and outputs
1917:23 - the levels so let's get right here and then you see we have this pre-process
1917:30 - method which takes in again this image and this button boxes and then right
1917:39 - here we are going to output the image here just simply output an image but for
1917:46 - the button boxes we need to convert this into output levels so we make this of
1917:50 - generate output method which itself takes in the button boxes and then we
1917:54 - specify the data type of the output tensor again we're using this numpy
1918:00 - function method right here because this generate output isn't made of only
1918:07 - tensor flow operations so that's it we'll run this and then there we go our
1918:15 - final steps now will be to batch our data set and then implement prefetching
1918:21 - so let's run that and then we have our training and validation data sets which
1918:28 - have been prepared
1918:33 - hello everyone and welcome to this new and exciting session in which we are
1918:39 - going to build our own YOLO like model so from the paper we had already seen
1918:44 - that we have this initial conf layers which are pre-trained on the image net
1918:51 - data set such that they could be used to extract very useful features from our
1918:59 - input images and then this conf layers were followed by two fully connected
1919:07 - layers which were designed in order to adapt to our problem of object detection
1919:14 - now given that we do not want to train this backbone here from scratch on the
1919:22 - image net data set we'll use an already pre-trained backbone which is the ResNet
1919:27 - 50 again we have the output dimension defined as number of classes plus 5 times
1919:35 - B from the paper B is given to be 2 we've seen this already and then 5
1919:40 - because we have the probability of obtaining an object and then for the
1919:46 - remaining 4 we have the bounding box so we have two of this bounding box
1919:55 - predictions that's why here we have 2 times 5 10 plus the number of classes
1920:01 - which in our case is equal 20 now we define this number of filters to be 512
1920:06 - so that's it from here we have our full complete model you could take this off
1920:13 - you see that we have our pre-trained ResNet which is our backbone our base model
1920:21 - and then we follow this up with several conf layers similar to what we have here
1920:27 - in the paper and then we have the global average pooling which is what is given
1920:34 - here in the paper now one thing you should note about the
1920:37 - average pooling is the fact that when we have inputs let's say we have this 7 by
1920:46 - 7 then by let's say 5 so we have 1 channel 2 3 4 and then 5 so let's
1920:58 - suppose we have 7 by 7 by 5 input now after going to the global
1921:02 - average pooling what we will have here is the averaging of each and every value
1921:09 - or let's say pixel in each and every channel so for this
1921:14 - channel for example we'll have one representative value for this channel
1921:19 - will have one representative value which is the average so for this who would
1921:23 - have the average this would have the average and this would have the average
1921:27 - average. So we average all those values here. And the problem with this is information
1921:35 - about object position is lost. And so instead of using this average pooling is preferable
1921:43 - to use the flattening. And so what we'll do is we'll take this off from here, and then
1921:49 - we'll have flatten. Okay, so once we have that, just with the paper, you see here, we have
1921:57 - the fully connected layer, which is this dense layer right here, and then this other fully
1922:03 - connected layer. So we have that, and then we reshape. Now this should be actually split.
1922:10 - Let's take all this here, copy and paste. So split by split by split, or split by split
1922:21 - by output dimension. So it's 7 by 7 by 30. So this is now our model. You can see we have
1922:28 - a total of 53 million parameters, 30 trainable and 23 non trainable. That's from our ResNet.
1922:37 - We are now going to get into the YOLO loss. So here we have this YOLO loss method, which
1922:42 - takes in Y true and Y pred, where this is our target Y, and then this is our predictions.
1922:49 - Now the YOLO loss we have here will be an implementation of what we already discussed
1922:55 - from the paper. And so it's made of four different parts. The first part is for the coordinates.
1923:03 - The next part is for the object. The next new object. And then finally we have the classification.
1923:16 - So we have this four different parts. Now we're going to start with this first part
1923:19 - here. We're going to start with the object. So in this case, we shall penalize the model
1923:25 - for not detecting an object at a particular cell. And that's why you see here we have
1923:33 - this one OBGIJ, which denotes if the object appears in cell I and this J right here. Notice
1923:43 - that here we don't have a J. Here we have this J. It denotes the fact that the J bound
1923:47 - in box predictor in cell I is responsible for that prediction. Now if we take a look
1923:53 - at this figure right here, where we have split this image, which is actually 224 by 224 into
1924:02 - seven different parts. So it's basically now seven by seven grid cell image. And each and
1924:09 - every grid cell, as we have seen, has its own predictions. Now if we specifically pick
1924:16 - this grid cell right here. So let's pick this one. As you see, we've picked this. Now we
1924:23 - have two outputs or two possible outputs. We have the Y true, Y true. And then we have
1924:32 - the Y predicted by the model. Now you'll notice that if you count this, it's going to be a
1924:38 - total of 13. And this is going to be a total of 18. As we've seen already, this first year
1924:46 - represents whether we have an object or not. The next four, which is all in green, is the
1924:51 - position of the object in the image. And then this other eight specify the class of that
1924:59 - object. Now before we move on, you should note that when we first recorded this section
1925:06 - on the yellow loss, we were working with a data set with just eight classes. But since
1925:12 - we now dealing with a Pascal VOC data set with 20 classes, our Y true will look instead
1925:18 - like this. So now we're going to have this additional 12 values here and this additional
1925:25 - 12 values here. So instead of having 13, 18 as with eight classes, now we are going to
1925:32 - have 25 and 30. So it's like 13 plus 12 and then 18 plus 12. Nonetheless, this doesn't
1925:44 - change much on our yellow loss method. So wherever you have eight classes, you should
1925:51 - consider in our specific case of the Pascal VOC data set, we're dealing with 20 classes.
1925:57 - Now we had also seen previously that although we have this Y true, which has this 13 different
1926:05 - values for each cell, this Y pred has actually 18 values because we have two possibilities
1926:14 - or two possible positions of the object. We have this first possible position. Well, let's
1926:20 - call this B1. And then we have this other possible bounding box, which we'll call B2.
1926:26 - So we had already seen previously that we said B to two because we have two possible
1926:31 - bounding boxes. And we also saw that even though our model or even though our data was
1926:37 - designed such that the outputs were 7 by 7 by 13, the model outputs 7 by 7 by 18 outputs
1926:45 - as you could see here. And so when we have a loss function like this, where we are given
1926:50 - the Y true and the Y pred, where the Y true is in fact 7 by 7 by 13 and the Y pred is
1926:59 - 7 by 7 by 18. Since we want to start with the object part of our loss function, which
1927:08 - is this part right here, our focus should only be on these two grid cells, this grid
1927:15 - cell and this other grid cell right here. So we want to focus only on this grid cell
1927:23 - and this because these are the only two grid cells where we have an object located.
1927:29 - Now the way we shall select this programmatically is by gathering all those grid cells where
1927:37 - the target is equal to one. Now our target here is simply Y true where we've selected
1927:45 - only this object score. So if you look at Y true here and if we pick this object score
1927:51 - that coincides with this one we have here. So in the case where we have an object, we
1927:56 - will obviously have a one for the Y true. And so when we say we are going to pick all
1928:03 - the different cells of Y pred where we have a one at this Y true level, that is this first
1928:12 - for this first value of Y true, it means that we'll now have Y pred, which is going to be
1928:18 - all these different cells where we have actual objects. And then we're also going to do the
1928:28 - same for Y target or Y true. So here we have Y pred extract and then we have Y target extract,
1928:35 - which is simply taking or getting all the positions where we have the objects, but this
1928:42 - time around for Y true. So now we have Y pred where there are objects and we have Y true
1928:47 - where there are objects. And with this, we can now focus on only this two cells, this
1928:54 - cell and this cell. Now let's take this off and run this so that you could see what this
1929:01 - produces. So we run that. Let's, let's print this out. So let's print out Y pred extract
1929:10 - and let's print out Y target extract. Well, we could also print out target. So well, let's
1929:19 - just print out this, this way right here. So let's print TF. Let's copy this. We'll
1929:28 - get this. So let's paste this out here. There we go. And that should be fine. So let's run
1929:36 - this and then now we'll test with some inputs. Now the inputs we are going to be using will
1929:42 - simply be the exact same coordinates we got from the dataset. So here you have this and
1929:51 - then you have this where this year corresponds to this vehicle right here. And then this
1930:00 - one corresponds to this other vehicle. Now we had seen already how we could use generate
1930:05 - output method to produce our Y true or our dataset value for Y or the output. So once
1930:14 - we have Y true, we are going to add an extra dimension. Now we have Y true ready. Then
1930:21 - for Y pred, we'll just generate this random values. And then what we'll do now is for
1930:27 - this specific values or for the specific cells, that's one four and three two, we are going
1930:34 - to put its own values. You'll notice that here we have 18 different values where here
1930:40 - we have the probability of having an object. We have the position 1234. There we go. We
1930:47 - have the position, the probability of having an object. We have its position 1234. This
1930:52 - is 1234 here. And then the photos rest here, we have the class or the different class probabilities.
1931:03 - Now we could do the same for this other one. We have this and then we have 1234. Okay,
1931:11 - so there we go. So this is what we have as our Y pred. So we're supposing that the model
1931:15 - predicts this and then this is our Y true. Remember we had already seen that this Y true
1931:21 - here would produce our datasets output. So now let's run this. Let's run this and then
1931:29 - run this and then see what we get. Okay, so there we go. You could see from here, we have
1931:37 - all the different positions where this target value is equal one. So that's what we printed
1931:43 - out here. And you could see clearly that we have one four and three two. Now the zero
1931:51 - year is simply because this is the first batch value. Now that's it. We have this 1432. For
1931:59 - the next we have Y pred extract. So want to extract only those values where we fall in
1932:06 - this grid cells. Now you'll notice one thing that you have 0.9, 0.47, 0.31, which coincides
1932:14 - exactly with this. And then we have the sort of one 0.3, 0.01, which coincides exactly
1932:20 - with this. And so clearly we are only focusing on the models outputs where we have actual
1932:29 - objects from our dataset. So we've, we've picked this year and we've picked this and
1932:37 - then now we're ready to compare what the model produces at this positions and what was expected,
1932:45 - which is this Y true now. So this is Y pred and then this is Y true. We're comparing only
1932:51 - at this two cells. You see here we have two, two. Now what if we suppose that we have only
1932:57 - one object? So if we have only one object, what we'll do is we'll take this one off.
1933:02 - Let's run this again. And you see now that we only have one, which is picked. So let's
1933:08 - get back and run that. And there we go. So you see, we've, we've picked this, we've picked
1933:15 - this cell and this other cell. And we already now as described in the paper to compare
1933:24 - the different probably discourse with one another. So we just simply have to subtract
1933:28 - this and that would be good for this part of our loss. Now, if we only had a single
1933:37 - bounding box prediction, like here, if we didn't have this, so let's suppose we had
1933:42 - only this and the Y true only this, then what we'll do is we'll take this one minus whatever
1933:48 - value we have here and then we'll subtract. So if Y pred is one and Y true is one, we'll
1933:54 - just have one minus one. We'll subtract that and then we square it exactly as we have in
1933:59 - the paper right here. You see, we have the object score minus the target object score.
1934:06 - And then that squared. And from here, we add this to all the other positions or all the
1934:12 - other objects at different grid cells. That's why we have this summation. Anyway, we had
1934:16 - seen this already. So getting back here, now that we actually have two bounding box predictions,
1934:23 - as you could see, we have B1 and B2. What we'll do is we'll take whatever value we have
1934:29 - here and then for this, let's wipe this off. Let's say this is a dash and then this is
1934:39 - a dash or some value. Let's call this lambda and we'll call this lambda. What we'll do
1934:46 - is we'll take this one minus one of this, that's either lambda one, if this is lambda
1934:55 - one and this is lambda two. So we would have one minus lambda one or we would have one
1935:04 - minus lambda two. And the way we shall pick between lambda one or lambda two is by looking
1935:11 - at this coordinates right here, this positions. So if this bounding box we have here is closer
1935:19 - to this other bounding box, that's a true bounding box, then we'll pick lambda one.
1935:26 - But if lambda two's bounding box that this is closer to this true bounding box, then
1935:31 - we'll pick lambda two. So that's how we do the peaking. And the way we would compare
1935:38 - this bounding box with this and then this other bounding box with this is by making
1935:43 - use of the IOU's course. So let's take this off and suppose that we have this input image.
1935:52 - Let's take all this off too. We have this input image and then we have this bounding
1935:57 - box. So let's say this is the true bounding box here. This is our true bounding box. And
1936:04 - then we have this bounding box B one, which is something like this. So let's say here
1936:10 - we have a lambda one and we have its coordinates. So we produce something like this. Let's change
1936:15 - this color. Let's say we have something like this. See, so we have something like this.
1936:21 - So this is what we're getting from this one year. And then we have this other bounding
1936:24 - box where we have maybe say something like this. Now, in this case, because this year
1936:33 - let's get back. So we differentiate between the two bounding boxes. Let's redraw this
1936:38 - year. And then we suppose we had this. Okay. So because this green bounding box is closer
1936:46 - to the red bounding box, we are going to pick lambda one. And simply we are, we are going
1936:52 - to take this into consideration. We're not going to take B two into consideration when
1936:56 - computing objectness part of our loss. And so getting back to the code at this point,
1937:05 - we have to subtract 0.9 with one that we have here, all would subtract. This is 1234 0.8
1937:24 - and one. So we have to, we actually have this or this and the choice between this and this
1937:31 - will depend on this coordinates and this coordinates. But then our IOU method is designed such that
1937:43 - it actually takes in pixel values. So what that means is we could design our IOU method,
1937:50 - which will take the center and the width and the height of these two boxes. Let's say we
1937:59 - have this, this box and this other box or this rectangle and so the rectangle, we take
1938:04 - the center of this, it's width, it's height. We think sort of once center is width, it's
1938:11 - height. And based on that, we compute the IOU score. But the point that we have in here
1938:18 - is that what we get in here is not the pixel values. What we have here is this pre-processed
1938:29 - values, which we had seen already. And so if we have the center here, you could read
1938:34 - that from here. If we had the center, which is at position, let's say 43, let's fix this
1938:42 - to this. We have 46, 140. So we have this center, which is at around 46, 140. Now we
1938:51 - have, well, 46, take this off. We have 46, 140. If we focus on only this first coordinate
1939:05 - here, then we would have 46, modulo 32, which will give us 12. All right, a 14. Let's get
1939:18 - back. This will give us 14. And then now this 14. Yeah, we move to the next step. 14 divided
1939:27 - by 32 would give us, 14 divided by 32 would give us 0.44. So we would have 0.44. And this
1939:38 - is the percentage occupied in this grid cell. Remember, to obtain this coordinate or to
1939:45 - obtain this value from the original center value of X would have to make sure that we
1939:54 - get this position of the center with respect to this grid cells center, which is right
1940:02 - here. So this distance from year to year is about 0.44 as we've just calculated. And that's
1940:08 - exactly what YOLO takes in. But since now we need to get back to this original values
1940:15 - from these values we used to train, because we need to be able to compute the IOU scores
1940:22 - between a given box and some other box, then what we'll do is now reverse this process.
1940:30 - So we are going to go from what we get here. Like for example, this value back to this
1940:37 - value 46. Now, if you're wondering why we picked 32 right here, you should know that
1940:44 - this image is 224 by 224. And if we divide 224 by 7, you would have 32. So each cell,
1940:51 - though this is not from year to year is 32, from year to year is 32 and so on and so forth.
1940:56 - So if from year to this point here is 44 or rather 46, then if you want to get this remaining
1941:03 - distance, you just need to do 46 modulo 32 to have this remaining distance in year. And
1941:09 - then you divide this distance by 32 to obtain the fraction of occupied by this distance
1941:18 - in this full grid cell. So with that, we are going to also repeat the same process for
1941:26 - this height coordinate. So this is a center X and center Y, we will take 140 modulo 32,
1941:34 - we obtain this value divided by 32. We now get this fraction occupied from this year
1941:40 - to the center. We've got in this already, this is 0.44. While we could also get this
1941:46 - Y and so that's how we obtain this fraction as we have here. Anyway, we've seen this already.
1941:52 - But now what we're trying to do is get back from this to a value like 46 or from this
1941:59 - to a value like 140. Now we should be noted that for the width and the height is going
1942:05 - to be quite easy because remember when designing this or when obtaining this, what we had simply
1942:10 - done was we took the distance or we took the width of this and divided by the image width.
1942:19 - So if we take this and divide by the image width, then to obtain the original width,
1942:25 - all we need to do is multiply by the image width. So if we had originally, let's say
1942:31 - this width was originally 20, for example, then to obtain what we have here, this width
1942:37 - and this height would take 20 divided by 224. We get some value. Let's say approximately,
1942:46 - this is like 111, something like that. So it's like 111. Let's say approximately 0.11.
1942:54 - Okay, so if you have 0.11, then now to obtain 20 from this, we just take 0.11 times 224.
1943:02 - So that will give us back 20. So that's how we reverse this. It's easier compared to this
1943:07 - where we need to go through all these two steps before getting back this original values.
1943:12 - And again, the reason why I want to get this original values is simply because we want
1943:16 - to be able to compute the IOU between a given bounding box and this two bounding boxes here.
1943:23 - So we make the choice of whether to say 1 minus lambda 1 or 1 minus lambda 2. Now let's
1943:29 - dive into how we move from 0.44, 0.375. It's actually 0.375 to 46 and 140. So the first
1943:46 - thing we'll do is we'll simply multiply this 1, 4 by 32. Now 1 will become 32 and 4 will
1943:55 - become 128. And this will take us from this origin right up to this position. So this
1944:02 - will be found at this point here, at this point you have from year to year 32. We've
1944:09 - already seen that each grid cell here has a size of 32. So from year to year 32, from
1944:15 - year to year is 128 because we have 32, 64, 96, and then 128. So you see we're already
1944:25 - getting close to the center of our object. And so that's why we have this rescaler right
1944:33 - here. We have this rescaler, which is gotten by multiplying those values where the target
1944:41 - equal 1 by 32. So all the positions where target equal 1 will multiply by 32. And so
1944:50 - now let's run this. And you see that we have 32, 128, and for 32 we have 96, 64. So for
1944:58 - this other object here, for this object we have 1, 2, 3, that's 96, and then 64 is 1,
1945:09 - 2. So we add this position. We're now left to get to the center. Center should be around
1945:15 - this year. Anyway, we've taken this first step. Now the next thing we want to do is
1945:21 - get rid of this batch here, the batch dimension. We don't need that. So we'll just have 32,
1945:28 - 128, 96, 64, and then we'll add two zeros for the width and the height. So here is width
1945:36 - and height. Well, this is X center and Y center. Now to do this, we are going to simply take
1945:44 - off the batch. So you see, we start from one, we take off the batch dimension, which not
1945:48 - consider the zeroth index. We take that off and then we add two zeros. So here we just
1945:54 - add this two zeros. Now the number of lines we would have would depend on the length of
1946:01 - our rescaler. If our rescaler is the length of two, as in this case, then we'll have two
1946:07 - by two metrics, which we are going to concatenate with this rescaled output. So essentially
1946:14 - we had this year, we had this 32. Let's rewrite that. When we take only this first index or
1946:21 - from the first index to the end, we have 32, 128, and then we have 96, 64, and then we
1946:31 - concatenate that with a metrics, which is two by two, where we filled all those with
1946:37 - zeros. And so now we have this year, which represents X, Y, and this height or width
1946:46 - and height. The next thing we'll do is we are going to take this target coordinates.
1946:53 - Note that we take from one to five. So it's essentially X, Y, W, and H, and they will
1946:59 - multiply by 32, 32, 224, 224. Now let's explain why we choose to multiply by all this. So
1947:09 - let's suppose that you have, for example, 0.44, you know that this is 0.44 right here,
1947:15 - and then you have 0.375 for Y for the center. Now taking 0.44 and multiplying by 32 will
1947:24 - give you this distance from year to year in pixels. So if you have 0.44 times 32, you
1947:33 - should have 14. Now remember that to have 0.44, you had 14 divided by 32. And so now
1947:40 - to get back 14, you simply have to do 0.44 times 32. And that is exactly what we do here.
1947:51 - You have this 32 times 0.44, and then you have 32 times 0.375. So once we have this
1948:03 - first two, now you have the height and the width, where you have the values multiplied
1948:09 - by 224 and 224, which makes sense because as we've seen before, to obtain the width
1948:16 - and the height, we divide it by 224. And so now to get back to the original width and
1948:21 - height, we need to multiply by 224. So we essentially have in this year multiplied by
1948:29 - the coordinates or bound in box coordinates. Now the reason why we repeat in year is simply
1948:38 - because we could have several objects. Now in case we have two objects, so we just repeat
1948:43 - this twice and carry out the same calculation for the two different objects. And this repeats
1948:49 - here where we specify the length of the rescaler. The length of rescaler is equal to 2. So this
1948:55 - permits us to repeat twice. So that is it for the target where we've taken this value,
1949:03 - because this is 1, this is 0, 1, 2, 3, 4. So we go, when we say 1 to 5, we're taking
1949:10 - 1, 2, 3, 4. So we're taking this, this, this, and this, multiplied by 32 for this, multiplied
1949:17 - by 32 for this, multiplied by 224 for this, and this by 224. And that's how we obtain
1949:23 - this distance from year to year in terms of pixels. So from year to year, we get the distance.
1949:31 - And then now, we could also repeat the same process for the other two predictions we have
1949:37 - for the prediction one, and then we have the prediction two. For prediction one, we simply
1949:41 - do the same. You see here, but the difference is why prep, and year is why prep, unlike
1949:45 - year, which is why target. We still have the same calculation. And note how year we go
1949:51 - from 6 to 10, because this is 1 to 5, and then this is 6 to 10 for bound in box two.
1949:57 - So that's the only difference we have here. And then we would get the pred one and pred
1950:03 - two. So at this point, for this object year, we note the distance from the origin right
1950:12 - up to the nearest cell, which is this distance. In this case, it's 32. We had seen that already.
1950:22 - And then we also notice distance from year to the center of the object in terms of pixels.
1950:29 - And that's what we just calculated. And that gives us about 15.21. So we have now 32, and
1950:35 - we have 15.21. So it means we have the center or the distance from year to the center in
1950:40 - terms of pixels. Now for this 128, you see, we're going to go from year to year. We know
1950:45 - that this is 128. And we also have this distance from year to year, which is 10. So now we
1950:52 - could add this up for the width and the height, we have simply taken what we had and then
1950:57 - multiplied by 224. And that's what we've had. So we don't need to do any modifications here.
1951:03 - Now you will notice now that because we have this, we could add this with this, this with
1951:08 - this, zero with this, zero with this. And then we could get our output now, which will be
1951:15 - this width and the, or rather this center, respect to this origin and the width and the
1951:24 - height respect to the full image. Now to make it very clear, let's, let's consider we had
1951:30 - only one year. So let's take off this prediction and then take off the sort of prediction.
1951:36 - Suppose that we had only one object. So you could see on that very clearly, you see here
1951:42 - we have 96, 64. Well, this should be on the order because we took this one off. We should
1951:53 - instead take this year off. Oops, this should be the one taken off. Okay. So let's run this
1951:59 - again and there we go. So you see here we have 32, 128, 0, 0. We have 15, 10, 28, 52,
1952:08 - where we will take now 32 plus 15 and then we'll take 128 plus 10. We'll take 0 plus
1952:14 - this, 0 plus that. And that gives us the, the width and the height and also the center.
1952:20 - And then for the predictions, we will take this plus this, and then we'll take this plus
1952:26 - this, take this plus this, this plus this, and that will be it. So finally we'll be able
1952:31 - to obtain the bounding boxes in terms of pixels for the target and for the two predictions.
1952:41 - So finally now we are going to take what we had here and then add with this other one
1952:46 - target this of scalar one and target of scalar two. So let's run this so you could see the
1952:52 - outputs. There we go. You see we have now 47. We've taken the 32 plus that 15 and we've
1953:00 - had 47, 128, this, this, we've had this, this, this, this, and then this. So we now have
1953:07 - those bounding boxes in terms of pixels. So the next step one we follow is actually compare
1953:15 - this box with this and then compare this box with this and then see which of these two
1953:23 - are actually closer to this. Now, one thing we did while designing this, why Prez was
1953:30 - to ensure that the first one, this one was going to be the closer one because you could
1953:35 - see here 47, 47, they are actually almost the same values so that when testing, you'll
1953:42 - see that this one is closer. So to compare this, we'll need the IOU score and to compute
1953:51 - this IOU, let's consider this simple example where we have two boxes, this B1 and B2. Now
1954:00 - the IOU, as we saw already, is simply the intersection over the union. So we have to
1954:06 - compute this, it's going to be this here intersection divided by the union. So it's essentially
1954:17 - this region, as we've seen already, divided by all this, including this intersection region.
1954:24 - So it is this divided by this. Now starting with this intersection, what we'll need would
1954:34 - be this coordinate here and also this position and this here. So the way we're going to get
1954:42 - this point is by starting by getting this point, this point here, this point, this point
1954:49 - and this point. But remember that we're actually dealing with the center and the width and
1954:55 - the height. So we are not having the X min, Y min, X max, Y max and X min, Y min, X max,
1955:02 - Y max values by pass in here. What we pass in here is, what we actually receive there
1955:08 - is the center, the width and the height. Now the first thing we have to do is to convert
1955:14 - these coordinates given to us in this form of center, width and height to one in which
1955:20 - we have this X min, Y min and this X max, Y max. And to do that, we have this piece
1955:27 - of code right here. Now first thing you have to note is the fact that we have 0, 1, 2,
1955:34 - 3, where this represents our X center, Y center, width and height. Now if I'm given this here,
1955:45 - X center, Y center, to obtain this position X min, Y min, it suffices to take for example
1955:52 - this XC, which is this distance from year to year and then subtract it or take away
1956:01 - half of the width from it because from year to year is the width and then from year to
1956:06 - year is half of the width. And if we take XC minus half of the width, then we get back
1956:10 - to this position. And then if we take YC minus half of the height, then we get this position
1956:18 - here. So at the end of the day, we get back to the origin, obviously of the specific box.
1956:26 - So if I want to get back to the origin of the specific box, which happens to be X min,
1956:29 - Y min, I need to take XC minus half of the width and YC minus half of the height. And
1956:36 - so you'll see here we have this is XC, this is X, we've seen this already, minus half
1956:42 - of the width. The width is two, see the width, half of the width, that you see width divided
1956:47 - by two and then XC, YC, which is this minus half of the height, see three year. So that's
1956:55 - what we do to have X min, Y min. And now if we want to have X max, Y max, then we have
1957:01 - to take XC plus half of the width and then YC plus half of the height. And that's what
1957:09 - we do here, just simply replace and put the plus and that's fine. So that's how this box
1957:13 - is now. This box one is converted to box one to you where we now have X min, Y min, X max,
1957:20 - Y max. And we'll do the same for this second box. Now, once we have this, the next step
1957:26 - will be to get this coordinates here. Now it happens that if we want to get this X min,
1957:33 - because remember, this is the intersection. Now this intersection forms another box. Now
1957:39 - to get this year, it suffices to compare this X min, Y min and this other X min, Y min.
1957:47 - So we take the X min, Y min of B1 and the X min, Y min of B2 and look for the maximum
1957:53 - between the two. And is this maximum or the one which is right most because our origin
1957:58 - already is at the top left corner. So increasing is this direction, increasing is this direction,
1958:05 - increasing is in this direction and this direction. So when we say maximum of this and this, then
1958:14 - we're looking for the one which is in the right most direction. So when we compare in
1958:18 - here, you see, we're taking the first two and the first two year. So we're comparing
1958:23 - the Xs and X min, Y min actually. So when we compare the X min, Y min, we get the one
1958:30 - which is maximum and that's the one which will play the role of the X min, Y min of
1958:35 - this intersection rectangle, which will form here. And then for the X max, Y max, we need
1958:42 - instead of minimum. So we need the one between these two, between this and this, we need
1958:48 - a one which is to the, which is left most. So that's why we need to take the minimum
1958:53 - and we compare the last two indices, that's X max, Y max, as you could see. And then that's
1958:58 - how we obtain this position and this position for this intersection rectangle. Now, once
1959:05 - we've gotten that, that's X min, Y min, X max, Y max for this intersection. The next
1959:10 - thing we want to do is actually compute the width and the height so that we could multiply
1959:15 - and get the area of this year. Now to get the width and the height is quite simple.
1959:19 - We just need to take, we need to subtract this simple because here you have, let's write
1959:25 - that again, we have X min, Y min, X max, Y max. So you just take X M that's X max minus
1959:36 - X and then you multiply by Y max minus Y. That's all. So this is the width and then
1959:44 - this is the height. Take this minus this times this minus this. That's all. So that's what
1959:51 - you see is, which is actually done here. You see we've, we've done the subtraction from
1959:56 - here. We've done the subtraction and then we now multiply this two and that's how we
1960:00 - get the area, the intersection area. Now, once we have the intersection area, the next
1960:06 - thing we want to do is obtain the union area. So we take the box one, you see two is the
1960:13 - width and box one again, the height, multiply the width times height. We have this area.
1960:22 - Take for the box two, width times height, we have this area. Now we add these two areas
1960:27 - up. You see here we add these two areas up. We remove the intersection because we want
1960:32 - to get only this and not, we do not want to add this twice because when you calculate
1960:37 - this area, you already have this. And when you see calculate the area, you still have
1960:40 - this. So you want to take off the intersection because it's going to be completed twice.
1960:44 - And then we now have the full union. And so now we have intersection divided by union
1960:51 - and that's it. So once we have intersection divided by union, now we will be able to compare
1960:57 - this with this and this. And so now in order to compare this different boxes, we would
1961:05 - have the target box compared with the second prediction. And we also have the target box
1961:13 - compared with the first prediction box. Well, before doing this, we could actually print
1961:19 - this two out separately. So let's print this one out. Let's do TF print. Second, second
1961:29 - box that's comparing the target with a second box. Let's have that there. And then we would
1961:36 - compare the target with the first box. So let's print now first box. There we go. We
1961:44 - have now print one. Okay. So that's it. We'll run this before having this. So you better
1961:49 - understand what we're doing here. So let's run this. Let's take this off. We could take
1961:57 - this off and then run this. You see that with the second box, we have 0.16 and with the
1962:05 - first box we have 0.93, which makes much sense because when you look at those predictions,
1962:12 - the first box looks much more similar to the target as compared to the second box, which
1962:20 - is this one. So it makes more sense that the first box has a higher IOU score. Now up
1962:27 - line the tensor flow math greater method, we'll be able to have this Boolean output,
1962:34 - which tells us whether this second prediction is greater than the first prediction or the
1962:40 - IOU between the target and the second position is greater than the IOU between the first
1962:45 - and the target. So given that in this case, for example, this IOU between the first and
1962:51 - the target is greater than the IOU between the pred two and the target, then this will
1962:57 - output false. And since this will output false, casting this to an integer will produce an
1963:03 - output of zero. And the zero will simply mean that between the two options, that's the first
1963:11 - output and the second output, this is actually the first output. And then this is the second
1963:17 - output or second bounding box between the first and the second bounding box. This is
1963:21 - the one which has, or which is closer to the target. And that is exactly what we want to
1963:26 - have here. So there we go. We have this output zero. Now let's include this other example.
1963:33 - So now we'll suppose that we have two objects, this object and this object by adding this
1963:38 - we've added this other one year, which happens to be this object. Then we'll uncomment this
1963:44 - part and then see what we have for the mask. So there we go. You see, we have zero one,
1963:50 - meaning that for the first object, that is this object here, it is this second box, which
1964:01 - is this one year, which is closer to the object or to the target. And then for the second
1964:08 - box that for the second object here, it is this first, which is closer. Now let's, let's
1964:15 - interchange this. Let's take this off here and paste out here and then take this from
1964:21 - here and then paste out here and see what we get. Take this off and add a comma right
1964:28 - here. So now because this one year will be closer, we should have this one being picked.
1964:35 - So we should have one, one. Now let's run this and see what we get. See, we have one,
1964:41 - and you could see that from here. The second boxes have higher IOU scores compared to the
1964:47 - first boxes. Okay. So now we have the bounding boxes, which are closer to the target. Like
1964:56 - in this case, we know that B1, B1 is closer to the target as compared to B2. So the next
1965:04 - thing we need to do now is get Lambda one and Lambda two. And then from this Lambda
1965:09 - one, Lambda two, choose Lambda one. Now let's start by getting Lambda one, Lambda two. It's
1965:16 - going to be quite easy. All we need to do is pick this zero value and this fit value.
1965:23 - Remember this is zero, one, two, three, four, and then five. So this is a fit. So that's
1965:31 - why you see, we picked this. So this is the probability of having an object for B2. And
1965:37 - this is that for B1. In fact, this is Lambda two and then this is Lambda one. So we concatenate
1965:42 - this and then we rearrange this by transposing. So let's run this and see what we get. There
1965:48 - we go. As you could see, we have 0.9 and 0.8. That's it. And then we have 0.3 and 0.98.
1965:58 - And so what we'll do now is based on the mask, we'll say, okay, for the first value, which
1966:05 - is zero, it means that we are going to take this one instead of this. So we're going to
1966:12 - take this probability instead and then move into the next position or move into the next
1966:17 - object. We are going to select the value number, this first index or this first or the second
1966:24 - value. So we'll skip zero and then we'll pick one. So for the first one, this is our Lambda
1966:30 - one we picked. And for this other one, Lambda two will be picked. Remember that if we change
1966:36 - these positions, then we'll pick Lambda two for this one and Lambda two for this one,
1966:41 - because this will be one one. So since this is zero one, we'll pick Lambda one for this
1966:46 - first object. And then since this is one year, we'll pick Lambda two for this other object.
1966:53 - So now to do that programmatically, we are going to gather all this Lambda values that
1967:01 - this probability is here based on the mask. So doing that, you'll see that we'll be able
1967:07 - to pick those values of Lambda corresponding to those bounding boxes with the higher IOS
1967:16 - cards with respect to the targets. So we've seen how when given this, let's take a single
1967:22 - example. So let's comment this again, comment this and this, run that. We're seeing how
1967:32 - we're able to pick these two bounding boxes, that's this bounding box and this other bounding
1967:39 - box. And then from this, pick the one with the higher IOS card, and hence pick the probability
1967:50 - which we are going to use to subtract from one. So now we know that we will have one
1967:55 - minus 0.9. Now let's reverse this. Let's reverse this again, reverse this, take this off, and
1968:06 - then we paste this out here. Then we take this off. Then we paste this here. Okay, so
1968:15 - let's run this and then see what we get. You see now we have 0.8 that we've picked this
1968:21 - other one instead because this bounding box is having a higher IOS card with respect to
1968:27 - the target as compared to this other bounding box. So this is how we are going to pick the
1968:33 - bounding box, which we are going to use for computing the objectness part of our loss.
1968:39 - Okay, so finally now we just need to compute a difference between what we have here. That's
1968:45 - in this case 0.8. For example, let's get back. So we have what we had originally, which is
1968:53 - going to be 0.9. So different between 0.9 and 1. So you see here we have 1s, which you
1969:00 - could see the length is based on the rescaler. So since we have only one object, this is
1969:04 - going to be 1. So 1 minus 0.9, and that will be it. Now this difference method we have
1969:11 - here has been defined. It's basically the square of Y minus X. So we take two subtract,
1969:19 - find the square as defined in the paper, and then we have this reduce some method to get
1969:23 - a single value. So that's it. We run all this and we could now print out our object loss.
1969:32 - So we have print object loss. There we go. Let's run that. And you see what we have
1969:40 - 0.01. You see that that's quite small because 0.9 is close to 0.1. Now let's just modify
1969:47 - this and say, okay, let's say 0.09. See that it's going to be having a higher loss. See
1969:52 - that we have higher loss. Now that's it. So that's what we do. Now you see that no matter
1969:58 - what we do here, even if we put here 1.0, we'll never have a loss of 0 here. And that's because
1970:04 - it's this one, which is used to compute that loss. We are now going to move to the new
1970:09 - objectness part of our loss, which is this year. And it's kind of similar to this calculation.
1970:17 - But what the difference is that now we focus in on those regions where we do not have objects.
1970:24 - So unlike here where we focused on this year, this cell where there's this object and this
1970:33 - other cell, what we'll do now is we'll be focusing on all the other cells except for
1970:40 - this one and this one. So as you could see, we get all the white prints. We gather all
1970:49 - these predictions where the target is 0. That is simply where there is no object. So let's
1970:57 - print this out. Let's print out these different positions where we have no object and the
1971:04 - corresponding predictions. Here we have TF prints, Y, print, extract. So this is seven
1971:13 - by seven meaning that we have 49 different cells right here. Now two cells have objects
1971:18 - and the remaining 47 do not have objects. So let's run this and see what we get. And
1971:24 - there we go. Here's what we have. So you could see clearly from here that we have all these
1971:30 - different cells and let's do print so we could get its shape. There we go. Let us run that.
1971:40 - You could see this is actually 48. Now that's because, okay, so that was because we had
1971:46 - only one object. So if we have two objects, two objects, let's get back. We have two objects.
1971:57 - So let's run this again and see what we get. Okay. So you see now we have 47 cells where
1972:03 - there is no object. So that tells you that out of the 49 we have two where there's an
1972:11 - object and then this Y, print, extract here now contains all this score and bounding boxes.
1972:23 - You could see from here that this is 47 by 10. So let's get to the bottom. You see 47
1972:29 - by 10 because for each and every one of this, we could take this off from here. We'll make
1972:35 - this connection from a cell where there is no object. So this cell, for example, here
1972:43 - we have this 10 different predictions and then now we will make use of this to compute
1972:53 - the loss. So obviously the no objectness part of our loss will be computed using this lambda
1973:00 - and this lambda. So we'll take this one or rather zero because we expect it to be zero.
1973:06 - So this time around our Y true will be zero because when there is no object like here
1973:10 - is zero. So this is going to be zero minus lambda here minus lambda one square plus zero
1973:22 - minus lambda two square. So the idea here is to ensure that this probability is equal
1973:30 - to zero when there is no object and equal one when there is an object. So getting back
1973:36 - to the code, you see that we break this up into two parts. This is like getting the lambda
1973:40 - one or competing with the lambda one and this is like competing with a lambda two. So unlike
1973:46 - with the object part of the loss where we're trying to pick which of this was responsible
1973:53 - for the prediction, here we just simply take this minus this plus this minus this. So our
1974:01 - target is obviously zeros unlike here where our target was once. So now we have zeros
1974:07 - and then we compute the difference between our Y target here and the Y pred. Now we pick
1974:13 - the zero for this one and then we pick this five for this other box right here. So that's
1974:20 - it. We simply find the difference or compute the difference using this method which we've
1974:25 - defined already right here. And then we sum those two up to obtain our no object loss.
1974:33 - So that's it. Let's print out or let's take off this here and then print out our no object
1974:41 - loss. We have no object loss. There we go. You see we have 110. Now we'll move to this
1974:52 - next part. That's for the classification where we'll focus on the objects class. You'll see
1974:58 - that we are going to only compute this or get this loss for cells where we have an object
1975:06 - here only where we have an object. And we do not care about which of the bounding boxes
1975:12 - is responsible because we actually focus in on only the classes. So instead of i j here
1975:18 - we have just i because we are not focused on choosing or we are not interested in choosing
1975:23 - any specific bounding box given that it doesn't really matter since we're focusing on classes.
1975:29 - So getting back here we have for object class loss we have the predictions and the target.
1975:38 - Now if you check from here you remember that the target will start from five because this
1975:44 - is zero one two three four and then five. But for the predictions it will start from
1975:52 - ten. So you see we have zero one two three four five six seven eight nine and then ten.
1976:03 - So we start from ten right here. So this starts from five and this starts from ten and we
1976:07 - go from ten to the last and then we go from five to the last and I will simply compute
1976:11 - the difference between this and this. So that's it. We also make sure that's where there's
1976:17 - an object. So now you have this difference and you obtain your class loss. You see that
1976:23 - we obtain a class loss of five point four seven. Now we get to the last part of our
1976:28 - loss which is that involving the coordinates which itself is broken up into two soft parts.
1976:36 - This first part is just for the center and then this other is for the width and height.
1976:43 - Now this part is more similar compared to this object as part of the loss simply because
1976:49 - here we are going to focus only on cells where we have an object and only on those bounding
1976:58 - boxes which are responsible for the prediction. So again we are going to gather all our predictions
1977:06 - here. We gather all our predictions where the target is equal to one. So simply where
1977:14 - we have objects then now we combine the centers. So we see we go from one to three and then
1977:21 - from six to eight reason being that this year because this is zero this is one this is two.
1977:28 - So this year represent our centers and then when you have here this is actually six seven
1977:39 - and this is representing the other centers. So this one is the center this two center.
1977:45 - So that's why you see we take this and this and stack them up to form our center joint.
1977:53 - And similar to what we had done already with the objectness loss we are going to only pick
1977:59 - a given center based on whether it's that bounding box which is responsible or not for
1978:05 - the prediction. And again we're going to use this mask. Remember we had seen this already
1978:11 - previously with the objectness loss. So we had this already seen. So the exact same process
1978:17 - we following year. We just want to make sure we pick in the bounding box which is responsible
1978:22 - for the prediction. And since we've completed this mask already that's what we're going
1978:25 - to do. Now let's let's print out our center joint and then let's print out the center
1978:35 - print. So we see that we actually pick out only some bounding boxes from the two choices
1978:43 - we have center right. You see here that for the first object which is this we have this
1978:52 - option and we have this option. Now because for the first object is the first or the zeroth
1978:58 - index that's responsible you see here. And then for the second object we have this option
1979:03 - and this option. But because it's this one that's responsible or the second bounding
1979:08 - box or the other first index that's responsible we actually pick this. So you see that this
1979:12 - one is discarded and this one too is discarded. So we focus only on this. Now for the target
1979:21 - we simply just pick out this one and two. So that's it. So we pick this obviously going
1979:28 - from one to three is simply taking one and then taking two. So we pick this and then
1979:32 - now we compare with whichever one of this is responsible for the prediction and comparing
1979:38 - that is simply applying our difference method. So now that we're done with the center we
1979:45 - finish with the center that's actually this part here we're now going to move to the width
1979:50 - and the height. So here is exact same thing with just the difference that we pick in the
1979:55 - width and the height. So now instead of one two we're going to pick three four. So that
1980:00 - way you see we go from three to five and then here we pick eight ten. Well okay this is
1980:05 - the prediction. So instead of picking this this we pick three four. So we have this let's
1980:11 - pick this we have this three four and then eight ten. So that's what we do here. We stack
1980:17 - them up we carry out the selection and then we also get the target. So we take this three
1980:25 - four see that and then we compute a difference. Now remember that when computing this difference
1980:31 - we have to make take the square root. So you see here we have square root and since the
1980:36 - square root takes in only positive numbers we make sure to compute the square root of
1980:41 - the absolute values. So that's it. Bread and size target and from here we've gotten the
1980:48 - center loss. We've also gotten the size loss. This now forms our box loss and that's it
1980:54 - for all those different loss functions. We're not simply going to add them up. Now before
1980:59 - adding up from the paper we had seen that lambda coordinate is going to be five and
1981:03 - lambda null object is going to be 0.5. We have seen this already from here. We have this
1981:08 - lambda coordinate and this lambda null object. So that's it. We have this. We make sure when
1981:13 - adding this up we take this into consideration. So let's run this and then well let's print
1981:18 - out the loss. So let's print out the loss. There we go and that should be fine. We are
1981:27 - then going to define our model checkpoint where our file path is this year. Then we're
1981:34 - going to save only the weights. We're going to monitor the validation loss. We're going
1981:38 - to obviously save the model which produces the minimum or the smallest validation loss
1981:46 - and that's it. We save the best weights only. So we run that and then now we move to the
1981:52 - scheduling. Here if the number of epochs is less than 40 so the first 40 epochs we use
1981:59 - a learning rate of 1 times 10 to the negative 3. Between 40 and 80 we use a learning rate
1982:05 - of 5 times 10 to the negative 4 and then after that we use a learning rate of 1 times 10
1982:09 - to the negative 4. So that's it. We compile our model and then we start with the training.
1982:18 - Now after training for epochs you'll notice that the model starts to overfeed. And so
1982:22 - in the next section we are going to use several techniques to help solve this or resolve the
1982:30 - problem of overfeeding. Now we've been training for over 20 epochs and you could see clearly
1982:36 - from the loss and the validation or the training loss and the validation loss that our model
1982:42 - starts performing well and at some point starts overfeeding. As you could see here we have
1982:48 - the training loss which keeps dropping right here and then the validation loss drops and
1982:55 - then at some point starts increasing. So clearly our model is overfeeding.
1983:06 - Hi there and welcome to this new and exciting session in which we shall be looking at different
1983:11 - strategies to reduce overfeeding. And in the yellow V1 paper some strategies were underlined.
1983:17 - To avoid overfeeding they use dropout and extensive data augmentation. Now a dropout
1983:23 - layer with rate 0.5 after the first connected layer prevents co-adaptation between layers.
1983:31 - And so here you see that after this fully connected layer we're going to have the dropout
1983:37 - and we're going to give it parameter 0.5. Then for the data augmentation the authors
1983:43 - introduce random scaling and translations of up to 20% of the original image size. Then
1983:50 - they also randomly adjust the exposure and saturation of the image by a factor of 1.5
1983:58 - in the HSV color space. So that said we are going to break up our data augmentation strategies
1984:04 - into two main categories. The very first category will entail modifying the pixel values without
1984:12 - modifying the positions of the different objects. So we could have something like this. Let's
1984:17 - click on edit right here and then let's try to say brighten up the image. You see we could
1984:26 - modify this like this. So we go from this initial image to this by playing around the
1984:32 - brightness, playing around with colorization and so on and so forth. So this first category
1984:39 - as we've said already entails just modifying the different pixel values without any changing
1984:45 - position of any object we have here. And then for the second category we could go from this
1984:52 - image to this one where you see that this flipping has made this object position to
1984:59 - go from here to this position right here. And now in the first case where we just modify
1985:09 - for example the image brightness there is little or no updates made to our existing
1985:16 - code base. But when we have to modify the image such that the bounding boxes have to
1985:24 - be changed or the positions of the bounding boxes have to be changed like this one here
1985:28 - or this one which will go from here to here, this one here which goes from here to this
1985:36 - other one. It means that we are now updating this bounding boxes and so we would have to
1985:43 - write some extra code for all these different modifications. Now nonetheless it turns out
1985:50 - that when we work with a library like albumentations, let's take this off, when we work with albumentations
1985:58 - all changes made in the positions of the bounding boxes are carried out automatically. So you
1986:05 - could see here we have this input image with this dog, this tennis ball, and this cat.
1986:11 - And then after going through some transformation like here we see we have some transformation
1986:18 - on this image where first of all the image is flipped so you see the dog moves to this
1986:22 - other position and then the image also appears zoomed in. So you see that this cat for example
1986:32 - is now not as complete or we don't have the complete cat as we had in this original image.
1986:39 - And so now we have completely different bounding boxes. This one for example becomes this,
1986:46 - tennis ball becomes this, this dog's bounding box becomes this. So you can see that it becomes
1986:52 - larger compared to the input. And so with albumentations as we're saying you have this
1987:00 - input bounding boxes like you could see for the dog at this position 23, 74, and 295,
1987:08 - 388 is automatically converted to 149, 69, I could see here 295, 381. So here you just
1987:23 - need to define your transformations and then albumentation make sure you have the right
1987:31 - bounding boxes as output. So now diving into the code as you might have seen in some previous
1987:37 - sessions we're going to import albumentations. So that's the import of albumentation. We
1987:43 - now move on to integrate our transform. You see right here we have the different transforms.
1987:51 - The first thing we'll do is to resize our images to 24 by 224 and then we'll apply a
1987:57 - random crop. Now this random crop is applied such that the output image will have a height
1988:04 - or rather a width line between 200 and 224 and a height line between 200 and 224. So
1988:12 - we could just have this height or let's say height minus 20 or we could just say 0.9%
1988:23 - of the height. So we have that we go from that to and then here we also have 90% of
1988:33 - the width. So we have 0.9 of the width. Take that off and there we go. Here we have the
1988:42 - width. Okay, so what we're seeing here is as we've had already, we want to randomly
1988:48 - crop the image. So we have the image now our new image will have a width, which will fall
1988:55 - in this range and the height which will fall in this range. And then we want to have a
1989:00 - probability of 0.5 of applying this transformation. So if you want this transformation to always
1989:07 - be applied, then you could always set always apply to true. If not, you just have P or
1989:17 - the probability of applying to set to 0.5 or maybe even 0.2 or say 0.8. It just depends
1989:24 - on you. So that's it. Now for the next we have this random scaling. Here we specify
1989:30 - the scaling limit. We have the interpolation type and then again we have this probability.
1989:36 - Then here we have the horizontal flip. So we're going to apply horizontal flip and probability
1989:42 - set to 0.5. Now finally, because after doing this random crop, we'll have an image which
1989:49 - is not 224 by 224. We actually resize this back to 224 by 224. So that's it. That's
1989:56 - all you need. Those are all our transformations here, which we pass in this compose method.
1990:02 - So it's basically a list made of this transform, this transform, this transform, this and this.
1990:08 - Now one additional term we're going to pass in this compose method is this bounding box
1990:15 - parameters. And the reason why we need to pass this is simply because unlike with the
1990:20 - image classification with the object detection, we have bounding boxes which are going to
1990:28 - be modified. So here we specify this bounding box parameters to take into consideration
1990:33 - the kinds of boxes we're dealing with. And here you notice that we specify a format YOLO.
1990:39 - Now getting back to the documentation, we actually have three formats. We have the Pascal
1990:43 - VOC format, our implementation format, COCO format, YOLO format, Pascal VOC, our implementations.
1990:52 - Now it turns out that in our specific case, we're actually dealing with the YOLO format.
1990:57 - Not just because we're building a YOLO model, but because the way we've normalized our inputs
1991:03 - or process our inputs is such that we have our X center, Y center, width and height representing
1991:13 - bounding boxes. So if we had instead X mean, Y mean, width, height would have picked this.
1991:20 - So it's not, it's not, it's not because of the name, although it actually coincides
1991:24 - with the fact that we're building a YOLO model. But then as we said, we have an X center,
1991:30 - Y center, width, height. And again, this is normalized. Notice here it's normalized. So
1991:35 - this here is divided by the width, this divided by the height, this is divided by the width,
1991:42 - and then this divided by the height. Remember, this is the width and the height of the specific
1991:47 - bounding box, which happens to be exactly what we have seen when we're doing this here.
1991:53 - Remember we took the X mean, Y mean, we obtained the X center and then we divided by the width
1991:58 - to the Y mean, Y max, obtain the Y center divided by the height to the width divided
1992:04 - by the total width to the height divided by the total height. So is the YOLO format we
1992:11 - actually using right here. Now again, gets into the code, you see, we have the format
1992:17 - YOLO specified. Now here we have this mean area set to 25 and this mean visibility set
1992:22 - to 0.1. Now to understand the concept of the mean area, consider you have this input right
1992:29 - here. And then after carrying out the transforms, what you have is say this in this output here.
1992:38 - So we have this output where the area of the, this box is 4,344 pixels. This is actually
1992:46 - the COCO format. So you will find that the bounding boxes will be different from the
1992:50 - kind of bounding boxes we have. Nonetheless, the area as we said, after transformation
1992:54 - is 4,344 pixels. So clearly it's quite small compared to the 23,892 pixels we had already
1993:05 - from this 132 times 181 computation we had here. So after transforming this, we obtain
1993:14 - this right here. But if we set the mean area to 4,500, it means that any box less than
1993:22 - 4,500 is going to be omitted. And so that's why you see that when we specify this mean
1993:27 - area to be 4,500, this box here disappears. So if you don't set anything, the box remains.
1993:34 - But if you set this, then the box is going to disappear. And then we also have the mean
1993:39 - visibility. So let's take this here. If we set the mean visibility to say 0.3, then if
1993:48 - the output box, which is this box divided by the initial box, which is this is less
1993:57 - than or gives us a ratio less than 0.3, it means that box is going to be omitted. And
1994:02 - so right here, if you take this, this area, which is 6,888 divided by 24,108 from this
1994:12 - original box here, 24,108, you would have 0.286, which is less than 0.3. And so when
1994:21 - you see, you see when you say 0.3, this box disappears. So that's it. That's the idea
1994:29 - behind the mean area and the mean visibility. Now we can actually leave those out. So let's
1994:36 - just take this off. And that should be fine. So this is it. We have our transforms. We
1994:41 - go around this. There we go. We have our org album and method, which takes in the image
1994:49 - takes in the bounding boxes. And then all it does is it passes the image and the bounding
1994:54 - boxes into our transforms here. And then we obtain the transformed image and the transformed
1995:03 - bounding boxes. So as we are seeing in the schematic here, we go from this image bounding
1995:10 - box pair to this transformed image, transformed bounding box pair. So let's get back to the
1995:17 - code. We could run this. And then we have our process data method, which makes use of
1995:25 - this TensorFlow NumPy function, because actually here, those are known TensorFlow operators,
1995:33 - which we are calling, especially here when you use make use of implementations, it's
1995:39 - made of computations in NumPy. So we make use of this method right here in order to
1995:44 - integrate that in our data pipeline. So yeah, we specify the function or album and the inputs
1995:50 - image and bounding boxes, the output tensors. Here, this one's this two year floats, actually
1995:59 - float 32. So that's it. So we run this and we create our train data set. So we could
1996:05 - visualize that. You see, for example, here, we have this image, let's write that image
1996:11 - out so we could see it, we had output one and output two, let's check that out. See
1996:18 - here, we have this, oh, no modification was made. So in order to be sure that we make
1996:23 - OC some change, what we could do is we could make sure for example, let's let's comment
1996:33 - this random scaling. And then let's carry out let's make sure the flipping is always
1996:38 - done. So we have always true set, always apply set to true. There we go. So we run that run
1996:54 - that again. And we have output one and then output two, which hasn't flipped. Now, one
1997:02 - thing you would notice is also the fact that this bounding boxes are changed. So let's
1997:07 - copy this from here. And then let's pass it out just here. So we could see that. Okay,
1997:14 - so this is what we have before. And this is what we have after. Now you see that this
1997:21 - actually makes sense because when you have this original image, when you have this original
1997:28 - image where you have something like this for the bounding box, oops, where's all of that,
1997:35 - we have something like this. And then when you flip it, you see when you flip it, what
1997:41 - actually changes here will only be this X center. So if your center was around this,
1997:47 - let's say centers around this, then flipping the X center changes position slightly. And
1997:56 - that's what we will notice here, you notice how this X center changes position just a
1998:01 - little bit. But for the Y center, it doesn't really change the width and the height remains
1998:07 - the same. Now let's play around with this year. So let's have this random crop now.
1998:14 - Let's do this. Let's set this to always apply. Let's set always apply to true. So we'll
1998:22 - have that. Oops, we would have always apply. We set it to true. Run this and check out
1998:34 - our output. So yeah, we would have, okay, we should have, let's run this year to have
1998:40 - out two. So we'll have out one and then we have out two. Okay. Now what we have is this
1998:51 - and then this, you see appears somehow zoomed in. Anyways, this example now shows us that
1999:00 - we have, let's take this off. We have this image which has this bottom box here, something
1999:07 - like this and this, and maybe the center around here. And then now it's modified and you see
1999:17 - the height gets modified and although not too much, actually the width doesn't change.
1999:24 - Well change is just a little bit. Now let's change this example so that we could see this
1999:28 - clearly. This example isn't very demonstrative of this transformation process. So we'll take
1999:36 - maybe the second. What is it? Do a skip. So here we have skip. Yeah, then we break. Hopefully
1999:48 - the second has maybe many more objects or it's a better example actually. Let's check
1999:54 - this out. Okay. So after flipping, we expect to have something which is looking different
2000:00 - from this. Um, okay. So that's it. Let's take this off for work with this example. Now let's
2000:08 - run this again. We have this year around this. There we go. We have that. We have our output
2000:21 - which we obtained from skipping and then we get back here and then we also skip. So we
2000:28 - skip and there we go too. And then from here we break and run that and then see what we
2000:39 - get. Now you could see from here we have our output one. Let's also check out our output
2000:45 - two. There we go. We have our one and then our two exactly as we expect. So that now
2000:53 - this, this example is much different from what we had before and should be a better
2001:00 - way to demonstrate what goes on in our limitations. So right here we have this input, um, boxes.
2001:09 - Let's copy that. We have the input boxes. We're going to paste it just here. Our input
2001:16 - boxes. Let's take that off. There we go. We have this output boxes. Copy that and then
2001:29 - paste right here. Now before we move on, notice the fact that those classes are the same.
2001:34 - So class 18 and here's class 18. And if we get right to the top, you, well, we use classes.
2001:41 - So let's, let's say classes. Um, let's get 18 and see what we get. It should be trained.
2001:47 - Um, from here we have, well, yeah, we have classes, classes 18. There we go. We see we
2001:58 - have trained. Okay. So that is it. Let's take this off. Now let's check this out. We have
2002:06 - our art one. You can see from here, it makes sense that the center is about, um, 27% of
2002:13 - the full width. So this distance is about 27% of all this distance. So that's it. Then
2002:20 - this distance too, it's about 36% of all this distance. And then we have the width, the
2002:28 - width, which is about 0.54 or 54% of the total image width. Let's change the color. This
2002:35 - is about 54% of the total width. And then this is about 71% of the total height. Now
2002:46 - after flipping, you see that this has to change. Let's drag this now, drag this this way. And
2002:56 - we have someone like this. Let's take this off. You would see that this now the center
2003:04 - of this distance here, this distance from year to year is about 73% of the full image. See
2003:13 - that? Excenter changes. And then the photo for the height, it doesn't really change much
2003:18 - or changes very little. See that almost the same. The width remains practically the same.
2003:24 - And then the height remains also almost the same. But at least the idea here is to show
2003:28 - that after going through this augmentations transforms, augmentations permits us to obtain
2003:36 - this output bound in boxes, which match up with a transformed image. Now don't forget
2003:44 - to make sure you change this back from always apply to probabilities of 0.5. So that's it.
2003:52 - The next set of transformations we'll make will be with TensorFlow. And we'll make use
2003:58 - of TensorFlow image. So here we have this random brightness, random contrast. We're
2004:05 - going to leave out the random crop for obvious reasons. Remember, if you had to do this random
2004:09 - crop, it means you would have to write the code which permits you also modify the bound
2004:15 - in boxes because when you carry out a random crop, the bound in boxes actually change.
2004:20 - So that's why we're making use of augmentation since it makes life, it makes life much more
2004:26 - easy. And then we use this, we use this, we carry out, no, we're not carrying this out.
2004:34 - We carry out, no, not this, we carry out random U random saturation. Okay, so this we're going
2004:42 - to make use of, you can see that here in the code, we have brightness, saturation, contrast
2004:48 - U. And then we finally carry out this clip in by value to make sure all the values lie
2004:53 - between zero and 255. Now you could always feel free to comment on comment any one of
2004:59 - this right here. So that said, we again going to carry out this pre processing. See, we
2005:07 - have that. And then remember, this is for the training, and then this is for the validation.
2005:14 - So that's it, we carry out this mapping, we batch, prefetch, and then you could check
2005:20 - out your outputs right here. So let's say out one, out two, let's check out out three.
2005:28 - Here we go, we have out three. Well, this should be let's go to skip, let's keep this
2005:35 - skip to and break. Okay, so let's run that again and see what we get out one out two
2005:44 - and out three out one out two and out three. Well, since this was already bashed, we will
2005:53 - maintain the tick. So we'll take the first we'll maintain this and then you would pick
2005:59 - out on one, run that and there we go. We instead have this so this should be two. Let's run
2006:10 - that. We have out one out two and then out three out one out two and out three. Okay,
2006:19 - so you see that this now appears much darker as compared to this one. So that's it. We
2006:25 - have seen how to carry out this different transformations or augmentations. But before
2006:29 - we go on with the training again, one slight modification will make is we'll replace this
2006:35 - ResNet 50 with the EfficientNet B1. Then we'll go ahead and compile the model and restart
2006:43 - the training. There we go, training has begun. And after training for several epochs, here's
2006:49 - what we obtained. You can see here that the training loss and the validation loss both
2006:56 - keep dropping. See that they all keep dropping up to where we have this 123 around this year.
2007:08 - So up to around this, our loss keeps dropping and then somehow increases slightly and stabilizes
2007:17 - around 128. So that's why at this point, we have to stop the training and get the weight
2007:26 - which produced the lowest validation loss. And that's it for this section. In the next
2007:32 - section, we are going to test out this model. Hi there and welcome to this new session in
2007:43 - which we are going to dive into testing out our YOLO model which we trained in the previous
2007:49 - section. And to carry out this testing, we are going to make use of the COCO dataset.
2007:56 - And so now we'll pick around some images from this dataset and test them out with our model.
2008:03 - Now the first thing we'll do is load the model. So there we go. We load the model. We're going
2008:08 - to create this outputs directory and then we'll specify the path to our test images.
2008:15 - Now let's dive into this test method. Here we're going to take the file path, test path.
2008:24 - And then we have this image on which we are going to put the bound and boxes and the classes.
2008:31 - Then given that we are not, or we did not use OpenCV to load the image previously, we
2008:37 - are going to go with the exact same process we had already. That is we read the file,
2008:44 - we decode and then we resize. So this is what we had. And once we have this image, we pass
2008:49 - this into our model. Now the output of our model will be something like this. So we'll
2008:55 - have this 7 by 7 by 30 tensor. Now remember that for a cell like this one where there
2009:08 - is no object, we suppose that this person here is an object. A cell like this one where
2009:12 - there's no object will have a zero for the other first position. And then for the next
2009:17 - position, we will have this for bound and box positions or bound and box values. And
2009:25 - then we'll have another zero and then we'll have four and then we'll have now the 20 values
2009:32 - for the class. Now, given that there is no object here, all this wouldn't really matter.
2009:37 - And so that's why we are only going to take the boxes where this two values, this value
2009:49 - and this value is going to be greater than or equal to the threshold, which will define
2009:55 - to be 0.25. So that said, a cell like this one, which is a center of our object, remember
2010:03 - we have this object here and we have this center. So you would have this cell here in
2010:09 - the center, meaning that if we take this, let's take its values. If we take this, you
2010:15 - would have our let's say 0.75. And then you have four values for presenting this position
2010:25 - or its bound and box. And then we'll have maybe another say 0.9. Then we have four values
2010:36 - and then we have different values here for the class. And so the idea here is to get
2010:44 - all the different positions where we have values greater than 0.25. Now, you should
2010:51 - note that we could pick a threshold of say 0.5 or 0.7 or 0.2 as we have done. And it
2010:59 - really depends on how this threshold affects the model performance. So we picked 0.25 because
2011:07 - it performs better than picking 0.5. As with 0.5, many objects were missed out. So that
2011:15 - is it. We move to the next. We simply just gather all these different outputs. That is
2011:22 - we have the object positions from here. And then to obtain this different outputs here,
2011:30 - we'll take the output itself, which is all this. And then based off the positions, we'll
2011:35 - get this output. So if we do here, from here we print out the object positions and then
2011:50 - below we print out the selected output. Here we have this exception. Okay, so let's run
2012:01 - this. You find that for this image, for example, we have these positions. That is, let's get
2012:10 - back to this. So it's telling us that this is under position 4, 3. We have an object.
2012:17 - So we go 0, 1, 2, 3, 4, and then 0, 1, 2, 3. So an object is found here for this image
2012:27 - we have. Now the reason why we have this duplicate is simply because it happens that for this
2012:34 - first position, that's for this first score, there's an object. And for this other score,
2012:39 - there's also an object. You could see that from here that we actually compared the 0 position
2012:46 - and this feed position, which is this 0.75 and this 0.9 respectively. And now we could
2012:53 - take a closer look at this selected output. You see that this is 0.96. So here is 0.96.
2013:04 - Well it's for this position, so we could just take all this off. So if we take all this
2013:08 - off, all this here off, this is dirty. You find that this, which is 4, 3, the object
2013:17 - for this specific image is for 4 at this cell 4, 3, and outputs this year. And you see the
2013:27 - first position 0.96. Then we have the 4 for the bounding box. And then the next 0.98.
2013:36 - So it shows clearly that the model is sure that there's an object there. And then from
2013:40 - there we have this 4 again. And then now we followed with this 20 different classes. Now
2013:47 - just by looking at this, just by looking at this here, 0.3, 0.3, try to look for the one
2013:55 - with the highest value. Okay. It shows clearly that this is the class with the highest value.
2014:01 - And so from this we know that there's an object at this position and that object belongs to
2014:09 - this class. And obviously we have the bounding box surrounding the object. So now that we've
2014:16 - had this different values right here, the next thing to do would be to convert this
2014:23 - bounding boxes as this into the X mean, Y mean, X max, Y max format, which we are then going
2014:35 - to use OpenCV to draw this bounding boxes on the image. Now we are going to go through
2014:45 - each and every object position, which we've had already from here. You can see here, we
2014:50 - have this object positions, 043, 043. Well, it's a duplicate. So let's focus on just a
2014:57 - single or this single one. So we have 043. That's essentially the position for three
2015:04 - as we've seen already. And to obtain the output box, which is this value, this value and this
2015:14 - value, what we'll do is we have the output and then we'll say output position zero position
2015:23 - is from the object positions. Remember the object position in this case is 043. So when
2015:30 - you say position zero, you're taking zero. So here you have zero, position one is four,
2015:38 - position two is three. And that's how you select this specific output here. Now, once
2015:44 - you select the specific output, the next selection you want to make is that of the bounding boxes.
2015:52 - Now when J is equal to zero, here you have zero times five is zero. So you go from one
2015:58 - right up to zero plus five, that's five. So we go from one up to five, obviously one up
2016:03 - to five minus one. So we have one. So we see this position one year, position two, position
2016:09 - three, and then position four. So that's how we select this year from our output. And then
2016:16 - notice that given that we have two different bounding box predictions, you have this year.
2016:25 - So for the first one, you have one to five. And then for the next time we get into this
2016:30 - loop, we have, since this is one, we'll have one times five, which is five, five plus one
2016:36 - is six. So we go from six to 10, which now is this year, this is six, seven, eight, nine,
2016:47 - well, six, seven, eight, nine, okay, yeah, six, seven, eight, nine, go again from six
2016:52 - to 10 minus one. So that's it. So this is how we obtain the output boxes. That's how
2016:59 - we obtain this year, this bounding boxes. And then given that, as we said already, we need
2017:04 - to convert this into this X min Y min X max Y max format. The first thing we'll do is
2017:09 - convert it into the X center Y center format or X center Y center with height format, which
2017:17 - is what we do here. Now to obtain the X center from this year, from this 0.53, for example,
2017:26 - and so let's suppose for example, that this 0.53 is at the position 043 as this, well
2017:34 - it's 43. So we go 0, 1, 2, 3, 4, 0, 1, 2, 3. So we have this year around the center or
2017:44 - 0.53, 0.17, so it's around here, we have this. And the idea is to obtain its value with respect
2017:53 - to this full image height and image width. So first things first, we know that the distance
2018:01 - from year to this position year is simply four divided by seven times 224 times 224.
2018:15 - And that's simply because all this is 1, 2, 3, 4, 5, 6, 7. So because the full image width
2018:22 - is 224, it means getting right up to this position year is 4 and 7 times 224, which
2018:32 - is in fact 4 times 32 because 224 by 7 is 32. So you take this year and multiply by
2018:42 - 32 and you get this distance from year right up to this year. Now to account for the fact
2018:48 - that we have this 0.53 year, 0.53, we'll take 0.53 times 32 because this full cell is 32.
2018:59 - So 0.53 times 32 plus 4 times 32. So to obtain this distance, we have 4 times 32 plus 0.53
2019:14 - times 32. Now for the height, because for the Y center, because this is X center, we'll
2019:23 - still go 0, 1, this is 1, 2, 3, so we still have this year. Again divided by 7 times 224,
2019:32 - this is going to be 3 times 32 plus 0.17 times 32. So that is it. To obtain the Y center,
2019:47 - we have this position, that's 3 times 32 plus 0.17 times 32 to find this distance year.
2019:55 - So that's exactly what we do right here. You will notice we have this post one. This post
2020:01 - one is actually from year. This is post. So post one is 4, which is multiplied by 32 because
2020:10 - this is this post one plus this output zero times all of this times 32. So you have this
2020:19 - times 32 plus this output box zero. Output box zero is 0.53, 0.53 times 32. So all this
2020:27 - is just like saying we want to have 4 plus 0.53 and then all of this times 32. So that's
2020:36 - what we do here. For the Y center, it's the same. We have post two. Now this is year three
2020:42 - times 32 plus output box one. Output box one is this 0.17 times 32. Remember, we got output
2020:55 - box from year and it coincides with 0.17. So that's it. We obtain X center and we obtain
2021:05 - Y center. And the next thing we want to do is obtain the width and the height. For the
2021:11 - width and the height, it's going to be easier because when encoding this, we simply divided
2021:15 - by the complete width and the complete height. So now we'll simply multiply by the height
2021:21 - and then multiply by the width to obtain the width and the height. So that's how we obtain
2021:26 - this. From here, we could now leave from X center, Y center, X width, Y width to X min,
2021:32 - Y min, X max, Y max. Now if we have a bounding box like this, let's say we have the center
2021:40 - and we know the width and the height. To obtain the X min, we could simply take this center
2021:45 - minus half of the width because this distance, let's say this is the origin, this distance
2021:52 - here, here is X center. If we subtract half of this width, then we would have this distance
2021:59 - which will take us to the X min. And then we'll do the same for the Y. That is we take
2022:05 - this distance right up to the center and then we subtract the Y, the Y height, that's the
2022:12 - height. We subtract half of the height, not the height, but half of the height. Then we'll
2022:18 - get to this position. That's why I mean, that's what we do here. We have X center minus half
2022:22 - of the width. And then we have Y center minus half of the height. And then for the max,
2022:27 - we have X center plus half of the width. So if we want to get this position here, we'll
2022:32 - take this plus half of this width, which would take us to this point here. And then if we
2022:38 - want to have this for the Y, then we'll take this distance, this distance plus half of
2022:47 - the height, which will add up to this point right here. So we have X min, Y min, X max,
2022:53 - Y max. Now be careful. In case where the X min happens to be less than zero, we want
2022:58 - to fix this to zero. If the Y min is less than zero, we fix that to zero. So we don't
2023:03 - have negative values. If this is greater than the width, we fix that to the width. If the
2023:10 - Y max is greater than the height, we fix that to the height. And so once we have this now,
2023:14 - we obtain our final boxes. That's X min, Y min, X max, Y max. And then not to forget
2023:19 - the fact that we have some classes. So we are going to simply get the class with the
2023:26 - highest probability score. So we just do this arc max and we make use of the selected output.
2023:34 - Remember the selected output is what we had already seen here. So based on this, we are
2023:42 - going to take the last 20 values. This year we get the arc max, which happens to be this
2023:49 - year. And then we get a class which corresponds to this position. So that is essentially what
2023:53 - we do right here. We have that position and then we have its corresponding class. And
2024:00 - we make sure that this is string. And then we add that to our final box. So that's it
2024:06 - for our final box. We also need our final scores. We will understand why we need this
2024:10 - final scores later. For now, just take all the final scores. We make sure that we have
2024:16 - again, you see, if J is equal to zero, then here we have zero. So we have the selected
2024:24 - output, I, and we pick zero, meaning that we pick in this. And then if J is equal, if
2024:31 - J is equal to one, then here we would have one times five, that's five. So that's a fit
2024:36 - position, which is going to be this probability score. So essentially we're getting the probability
2024:41 - scores for the two, um, predictions. Remember we had actually, we actually have two predictions.
2024:47 - So we get the, uh, probability scores and then we, um, in them out. So we see what this
2024:55 - looked like. There we go. As you could see, we have 0.965, which makes sense. This is
2025:03 - 0.98. Uh, yeah, we have 0.965 and we have 0.985. Well, this is because there are some
2025:12 - duplicates here. So that's it. Then we also see the final boxes. You see the class person,
2025:17 - see person, person, person. Now, uh, we're going to see how to eliminate this duplicate
2025:23 - shortly. And, uh, that's it. So for now we have understood how we could get from this
2025:32 - models outputs to then be able to obtain this final boxes and the final scores. And now
2025:42 - the next step will be to get into this norm max suppression. So we have, uh, maybe seen
2025:48 - already the, we looked at the norm max suppression already in theory. Now we'll see that with
2025:53 - tensor flows actually very easy to implement this, but before implementing, let's, um,
2025:58 - take a look at what it's all about. Let's suppose we have an image like this and then
2026:03 - we have, um, this object. Let's say we have this object here and then we have some bound
2026:10 - in box. We have this bound in box. Remember for each, um, cell, we have two predictions.
2026:16 - So let's suppose that our cell predicts this and that same cell predicts again, another
2026:20 - bound in box like this, all this for this same object. Now what we'll do is with a norm
2026:26 - max suppression algorithm, we are going to compare these two probabilities and say, okay,
2026:32 - which one has the highest probability? If it turns out that is this one with the highest
2026:37 - probability. So let's say if this is 0.98, oops, is let's say if this is 0.98 and then
2026:46 - this one year is 0.96 and then these two are predicting the same object, then we are going
2026:54 - to discard this box. So hence the term known max suppression. So we'll suppress this box
2027:02 - and we'll be left only with this. So that's how we are going to also, um, discard those
2027:07 - duplications. Now, um, going back to the implementation, all we need here is just this, um, nonmax
2027:14 - suppression method we have here. So we have this no max suppression from tensorflow image.
2027:19 - We specify the boxes. So we have this boxes here. Now note that our boxes from year included
2027:27 - the classes, but we do not need that year. So we just, as you see, we pick the first
2027:31 - four elements as essentially X min, Y min, X max, Y max. We pick this first four boxes.
2027:36 - Then we also make sure we send in the scores. Remember in the normal suppression algorithm,
2027:42 - we need the scores to be able to discard certain boxes which have, uh, which are not the max,
2027:48 - uh, scores, which we do not have the max scores and which are predicting, uh, an object, which
2027:54 - has already been predicted by another box of higher score. So that's why we need to pass
2027:59 - in the score year. So, um, essentially we pass in the boxes passing the scores. We want
2028:05 - to specify the total, the maximum, um, output size. Yeah. We just pick a hundred. We, we
2028:12 - don't expect to have more than a hundred, but depending on your, on your task, like you
2028:16 - could have a task where you generally have maybe say 150 objects to be detected at once.
2028:23 - In that case, then you will need to increase this max output size to maybe say a thousand.
2028:27 - Now we have this IOU threshold right here to understand this concept of the IOU threshold.
2028:33 - Let's take back our example we had here. If we have this year, if you have this example,
2028:41 - in order for those algorithm to know that these two boxes are trying to predict the
2028:45 - same object and we want to actually discard this one, what we'll make use of is this IOU,
2028:53 - um, threshold. So remember we have seen the IOU already. So if you have two boxes like
2028:59 - this, these two boxes will compute the IOU score. That's essentially we'll look for
2029:04 - the intersection between these two boxes, which is this area, and then divided by this
2029:10 - total area occupied by these two boxes. So in this case, it's all this area right here.
2029:16 - So let's, let's, let's have it back. We have this year is the intersection and then
2029:22 - this year totally is the union. So we take that intersection divided by the union to
2029:30 - obtain the IOU score. Now, if that IOU score is greater than the IOU threshold, like in
2029:39 - this case, let's make, let's specify an IOU threshold of 0.5. It is greater than 0.5.
2029:45 - Then we are going to discard this box. So we're going to discard this if that IOU score
2029:51 - happens to be greater than 0.5. Now, if it is less than 0.5, many of that, if we have
2029:56 - a box like this, um, let's say we have a box like this where this area, this area here
2030:04 - is divided by all this area is less than 0.5. Then we are not going to discard this box.
2030:13 - So we consider that this box is trying to is, is for a different object and not this
2030:19 - other object. So this IOU threshold year permits also determine whether two boxes are trying
2030:25 - to predict, um, the same object or not essentially. So that's it. And then here we have this call
2030:32 - threshold, which is set to negative infinity. Now the documentation is said that the score
2030:37 - threshold actually is already float tensor, representing the threshold for deciding when
2030:44 - to remove boxes based on score. So if you have a score threshold of 0.4, for example,
2030:52 - one, then what you're seeing is all boxes, which are, uh, which have a score of less
2030:58 - than 0.1 are going to be discarded straightaway, mindless of, or regardless of whether, um,
2031:06 - they overlap with a box of higher score or not. So that's it. We get back here and then
2031:12 - now we could print out, let's print out, um, our norm max suppression output output. Let's
2031:22 - run that. Now, one thing you'll notice here in this output is the fact that we have a
2031:27 - single element. Now what the single element actually means is between all this four options
2031:34 - we have, that is here, we have this person, we have this person, we have this person and
2031:39 - this person only this one year at this position one is going to be left. All the rest will
2031:45 - be discarded. And to understand why they're discarded, you could look at this course.
2031:49 - This is 0.96. This is 0.85. This is 0.96. This is 0.98. Oh, this is 0.985, not 85. So
2032:01 - what we're saying here is because this one has the highest probability and because it
2032:07 - overlaps with the others, like you see here, uh, this one and this one will overlap because
2032:12 - it's actually represent the same person. Then, um, this others will be discarded. Now in
2032:19 - the case like this, where we have this exact same box with exact same probability, one
2032:23 - is going to be left out and the other one left. So, um, that's it. We have our output
2032:29 - now. We know that we only have a single box instead of all this four boxes. So we have
2032:34 - our normal expression output. The next step will be to, um, show visually what our, um,
2032:43 - predictions look like. Now, yeah, you know, the fact that we are going to write this in
2032:49 - this image. So we will draw the bounding boxes and put the text on our image only for I in,
2032:59 - uh, no max operation output. So in this example, we're going to do that only once, unlike the
2033:05 - case where if we do not have no max operation, I would have had to do that four times. So
2033:10 - now we're doing this only once because after no max operation, we're left with only a single
2033:14 - box. Now take a look at what we have here. We have the X mean, we have the X max. Remember
2033:20 - from the final boxes here, what we had here, X mean, um, Y mean, like you see, we have
2033:27 - the X mean, we have, um, the Y mean, sorry, not the X max. We have the X max, we have
2033:34 - the Y max, and then we have the color for the box and that's it. So that's it. We now
2033:41 - put the text. Now we're putting this text based on, um, certain position. So, um, the
2033:49 - text itself is going to contain the class from the final box. You see, we take this
2033:53 - last element. Remember from here, we had the class and then, uh, once we obtain that class,
2034:00 - we write that as the text we're going to put to the text and then the position of that
2034:05 - text will be based on the X mean Y mean values. So you see, we go, we go to X mean, but for
2034:12 - Y mean we step 15 pixels, um, downwards. So that's it. We define the font and the color
2034:20 - and that's it. So we've put out this text and then now we're ready to write this out
2034:26 - in our, um, new image. So we create this new image and then we resize it. Obviously the
2034:34 - content of this image is this year, this image, which on which we have written the, or we
2034:39 - on which we've drawn the rectangle or that's the bounding boxes and the texts. So let's
2034:45 - run this now completely. And then you see again, we have one open this up. Okay. So
2034:52 - we should be able to have our output and there we go. You see, we have a person notice from
2035:00 - your, um, from your, we had decided to go 15 steps. Oops. Let's take this off. We had
2035:10 - decided to go 15 steps. Let's go downward here. We decided to go 15 step. That way you
2035:16 - see the text comes slightly down. So if you, if you, if you don't have this and then you
2035:22 - do this, see it goes up and it's not very visible. So let's have that back. You could
2035:30 - obviously change the color. So let's say the 225 and you could play around with all those
2035:37 - different parameters. So that's it. Well, let's get back to the color because it's actually
2035:42 - better. Let's say we want to have one. So now we could run this for all the different
2035:48 - files and see what we get. Well, before you've been checking on that, let's suppose that
2035:53 - we do not have this normal expression output. So let's, um, leave out the normal expression
2035:59 - algorithm and see what our outputs will look like. Let's have your, uh, for I in range,
2036:05 - the length of the final boxes, the final boxes here had a length of four. We had four outputs.
2036:11 - Um, let's take this off. There we go. And then run this again and see what we have.
2036:19 - So we have outputs. Oops. Well, we already had several different predictions. Um, like,
2036:26 - okay, let's, let's take this one. For example, you see here, we have this one. Uh, no, let's
2036:31 - take, let's, let's not have that. Let's say we want to have your 40, let's get back to
2036:35 - 40, um, 40. Okay. So one thing you can notice here is the fact that this, you see, we have
2036:43 - this two predictions for this person and, uh, that's not what we want. So you see that
2036:49 - the fact that we add this in a, uh, not my suppression year permits us to remove some
2036:55 - of the boxes. So this threshold is what you play around with to ensure that you have
2037:01 - a single box for a single object. So yeah, you can see that this model does well and
2037:08 - predicts in the location of this train and knowing that it is actually a train. We have
2037:13 - this year predicts that this is a person. We have this produce this person, but unfortunately
2037:19 - doesn't get this other people year. Um, guess they're playing this person, this person well
2037:27 - doesn't get this person, um, does quite well here. See, this is the dining table. Um, this
2037:34 - person, this person and this person. So that was great. Um, yeah, we have the TV monitor,
2037:40 - um, though it doesn't get the other monitors. Okay. So from here, we also have this bus.
2037:46 - Unfortunately, yeah, we, we, it predicts two buses, um, a car also predicts two cars,
2037:53 - first, maybe due to this auto car being here. Then we have this dining table and this person.
2038:00 - We have this person and a dog. We have this person, this person, well, stills a dog here,
2038:07 - but that's not right. Um, person, person, but doesn't see this other person, um, sees this
2038:14 - two people here. Um, here says this person's though the bottom box isn't, um, quite well put out.
2038:25 - Then we have the cut. We have this person. Um, we have this car
2038:32 - and then we have this person doesn't see the dog. We have here sees this cows. Well, this,
2038:39 - this particular image was gotten from the paper. So just basically crop this from the paper to test
2038:44 - it out. So it sees a person, but doesn't see that this to a dog store actually locates them quite
2038:50 - well. So that's it. Um, yeah, it sees a cat, but doesn't sit as dog. Um, she's a person. She's a
2038:58 - cow. She's the person person. She's two people sees this person. And this person though the
2039:04 - bottom boxes aren't quite well put out. Unfortunately I see cars, um, not quite correct.
2039:12 - Then here we have motorbike and person, but this should be two motorbikes actually.
2039:18 - Okay. So that's it. We've just tested out our model. We'll see how, um, it does or how it works
2039:25 - with our images on which it has never actually seen. Hello everyone and welcome to this new and
2039:37 - exciting session in which we shall dive into image generation. So as you could see right here,
2039:43 - this image was AI generated, but back in 2014, 2015, 2016, images like this
2039:50 - weren't yet to be generated using AI with advances in AI, like the original auto encoders,
2039:58 - the GANS and even more sophisticated GANS like the W GANS, pro GANS, SR GANS, and cycle GANS.
2040:06 - AI algorithms have been trained to produce high quality images. And today we even get much better
2040:14 - results with a diffusion models. That said, in this section, we shall treat the variational
2040:22 - auto encoders and the DC GANS. And so at the end of the section, you should be able to produce
2040:29 - images like this. Back in 2014, one of the best performing image generation models was this model
2040:37 - you have right in front of you. That is the variational auto encoder. And the way this model
2040:42 - worked was quite simple. We had an encoder block, which took in some input image and then generated
2040:53 - this embeddings. And now this embeddings having encoded information about the inputs could be
2041:02 - used by the decoder to generate output images. Nonetheless, by 2014, Ian Goodfellow came up with
2041:13 - this idea of the GANS. And the GANS signifies generative adversarial neural networks. So here
2041:22 - we have two neural networks here, the generator G and the discriminator, where this G and this D
2041:33 - that's the discriminator are both in some context where the generator is learning how to produce
2041:43 - images which look like those from the real data set or the training set. And on the other hand,
2041:53 - the discriminator is learning how to differentiate between real data like this one and fake data
2042:02 - produced by the generator. If we consider this simple example here, you can see that we pass in
2042:09 - some input noise, we get this output. And because this output doesn't look like the real data,
2042:19 - the discriminator considers this as fake. Whereas now for this other example, the
2042:27 - output from the generator looks like the real data. And so the discriminator sees this or the
2042:34 - discriminator is tricked by the generator to think that this is real data. So after updating
2042:41 - the parameters of the generator and discriminator, such that we get to that point where the
2042:45 - discriminator no longer knows the difference between what is coming from the generator and
2042:50 - what's coming from the training set, we now have this generator block, which is able to take in
2042:57 - random noise and generate outputs, which are similar to those from our training set. And
2043:06 - although this architecture was groundbreaking in 2014, 2015, today we have more advanced
2043:15 - better models like the StyleGAN. Hi there and welcome to this new session in which
2043:27 - we shall be trading the variational encoder. And we shall see how it could be used in image
2043:34 - generation. In this first part, we shall dive deep into understanding the theory behind the
2043:40 - variational encoder. And then the subsequent sections, we shall practically implement a
2043:47 - working variational autoencoder. That said, we shall start with explaining or understanding
2043:54 - this autoencoder and we'll make use of this blog post by Jeremy Jordan. Now to understand the
2044:02 - autoencoder, we can break this word into two parts. That's auto and we have encode.
2044:13 - So essentially we have a system which self encodes itself.
2044:18 - Now, supposing you have an image like this one right here. When we pass this into some
2044:28 - encoder block, let's have something like this. We have some encoder block and then we could
2044:34 - obtain this output vector. Now this output vector is six dimensional. So yes, six different positions
2044:42 - and each of the positions represent a specific characteristic of this image. You could see your
2044:49 - smile 0.99, skin tone 0.85, gender negative 0.73, beard 0.85, glasses 0.002, hair color 0.68.
2045:00 - So all these six values here, the six values we have here are characterizing our image.
2045:09 - So they encode information or information about this image is encoded in this vector,
2045:19 - or this vector right here. And then on the other hand, when we want to retrieve this encoded
2045:27 - information, what we could now do is we get a decoder which takes this encoded information
2045:35 - and then reproduces this original image. And so that's globally how we produce this kind of system
2045:45 - which could be used in image compression where we could take this image, encode it so that we have
2045:52 - just this vector. Then we could pass this vector via some network. And then on the other side of
2046:01 - the network, we decode this vector such that we have the original image. Apart from compression,
2046:09 - another field where we could apply this kind of auto encoder network is in image search.
2046:18 - So let's suppose that we have this image right here and we have this vector. Now if we have
2046:26 - another image of this same person here, so we have another image of the same person.
2046:32 - We call this image B and here we have image A which produces a vector which we'll call VA.
2046:41 - Then it means that in this 6D vector space, six because we have six different values here,
2046:49 - it could be 128D or whatever dimension we make it to be. So as we're saying, we have this image B
2046:58 - which is the same image here, the image of this person but not necessarily the same image,
2047:04 - maybe some other image of the same person. Then after encoding, after passing through an encoder,
2047:10 - you have our 6D vector VB. But because it's a similar person or because it's the same person,
2047:20 - we would expect these values to be similar. And so VA will be close to VB.
2047:29 - And if we have another person, let's say another person C, this is here and we generate that
2047:40 - person's VC, that's this encoded vector, then we would expect VA to be much different from VC.
2047:52 - And so this means that in an image search scenario, we'll just pass this input, we obtain this vector
2048:02 - and then we'll compare the two vectors to see whether it belongs to the same person or not.
2048:09 - Now it should also be noted that when training an autoencoder model where we have an image A
2048:15 - and we have a reconstructed image A', then our aim here would be to minimize the difference
2048:25 - between A and A'. So we could minimize A-A'. Now it turns out that in image generation
2048:37 - to get better results, instead of dealing with discrete values like what we had here,
2048:43 - let's get back to the top. You see here we had the given value, let's take this off, a given
2048:50 - value for smile, for skin tone and so on and so forth. So as we're saying, instead of having a
2048:56 - fixed value for each and every one of these features, what we'll do is we'll make use of
2049:03 - a probability distribution. So here, instead of having a value, let's say this is negative 0.6,
2049:14 - we would have a probability distribution whose mean is at negative 0.6. Well, this looks more
2049:23 - like negative 0.5. But here we suppose now we're going from 0.6 to this probability distribution
2049:29 - which means a negative 0.6 with a given variance. Now for those of you who don't have a map
2049:36 - background, what this essential means is instead of picking a value or picking the value negative
2049:44 - 0.6, what we'll do is we'll pick some random value within this range. So instead of having negative
2049:54 - 0.6, as we said, we're going to have a random value in this range and values closest to negative
2050:02 - 0.6 have a higher probability of being picked. So instead of having this, we could now have
2050:10 - negative 0.3 or negative 0.55 or negative 0.75 and so on and so forth. So we have values which
2050:21 - we can pick in this range. Now if we see this example here where we have 0 now turned to this
2050:28 - probability distribution, we pick values in this range, negative 1 to 1. So here the variance
2050:36 - or the range of values which are in which we are allowed to pick a value is larger than this other
2050:47 - one. But still values around this zero, that's the mean, around this zero have a higher probability
2050:57 - of being picked. So here you would have a higher chance of picking 0.1 instead of picking 0.9.
2051:05 - To see that clearly here, let's say this is 0.1 at this map and then this is 0.9 around here.
2051:12 - You'll find that if you link this up here, this is 0.1, you see that this has a higher score
2051:19 - and a higher chance of being picked as compared to this one which has much lower chance of being
2051:25 - picked. So in a nutshell, instead of having this 0.5, we now have a mean value which is 0.5
2051:38 - and a variance which shows us or better still gives us the range of values for which we can
2051:51 - pick the specific value for a given feature. And so as you could see, this one here has a smaller
2052:01 - variance as compared to this and as compared to this one. And from this point, we'll define this
2052:08 - mean as mu and the variance which is some distance from here, this distance as sigma square.
2052:21 - This probabilistic approach to generating a latent vector which previously was this vector
2052:29 - we had here. Scroll back up. Previously, it was this vector. It's now what leads us to the
2052:36 - variational autoencoder. So you see that here we have our input image. It gets into the encoder
2052:48 - which produces mu and sigma square or let's just say sigma. Sigma is a standard deviation,
2052:55 - sigma square is a variance. So it produces mu and sigma. And then using mu and sigma with our
2053:02 - decoder, we are able to obtain our reconstructed output image. Now note that in this case,
2053:11 - we would have 1, 2, 3, 4, 5, 6 positions. So mu would be this 6D vector, sigma would be another
2053:19 - 6D vector where mu1, this first position here, mu1 and sigma1 represent the mean and the standard
2053:28 - deviation for this distribution. Now it should be noted that the main benefit of a variational
2053:36 - autoencoder is that they are capable of learning smooth latent state representations of the input
2053:43 - data. Now to better understand that statement, let's consider this output generated by an
2053:50 - autoencoder and this other output generated by a variational autoencoder. You will notice that
2053:59 - as we go from one digit to another, like let's say we're going from six to eight year, you see
2054:06 - here we have six. Well, it looks very well like six. Let's take this one which looks already
2054:11 - very well like six. This is six but here it's really confusing because we don't know exactly
2054:17 - what this is. Now this looks more like eight but not really very clear and then here we start getting
2054:24 - eight and eight and well this too doesn't look very clear. But when you look at the output
2054:33 - generated by the variational encoder or the variational autoencoder, as we go from one digit
2054:39 - to another, we can see here that we have an even much smoother transition. And this is thanks to
2054:48 - the fact that instead of working with discrete values at the level of our latent vectors,
2054:55 - we're going for a probabilistic approach with the variational autoencoder. Because we're going
2055:01 - in for this probabilistic approach, the training of our variational autoencoder is no longer
2055:08 - evident. And this is simply because during the training, we need to compute partial derivatives
2055:15 - with respect to z here with respect to mu and partial derivative with respect to mu and with
2055:29 - respect to sigma. But because the z we have here is drawn from a normal distribution,
2055:37 - with mean mu and standard deviation sigma, we won't be able to compute this partial derivative.
2055:46 - And so the idea now will be to convert this node that's here to one that is deterministic. You
2055:56 - could see here we have this key random node and then deterministic nodes. So this one is deterministic,
2056:03 - that's fine, this one fine. Now, well, this is fine. Now the idea will be to convert this into
2056:11 - one which is deterministic such that we could compute this partial derivatives and hence
2056:17 - train the model such that we could update the encoder and the decoder parameters. And so now
2056:26 - this idea of converting this node from one which is random to one which is deterministic
2056:35 - is known as the reparameterization trick. So instead of having this where we have, well,
2056:44 - let's take this off, let's make it simple. So we have this where we have the mean mu
2056:51 - and the standard deviation. Well, we'll pick any value at random in this range. We are instead
2057:00 - going to define this epsilon which is drawn from a normal distribution with mean zero. So our mean
2057:11 - now will always be zero and then the standard deviation will be one. So we have negative one,
2057:18 - one. So this epsilon here as we've said is drawn from this probability distribution
2057:23 - and then to obtain z, unlike here where we obtain z randomly from values surrounding the mean,
2057:35 - here we'll do the mean plus the standard deviation times a random value which lies between or which
2057:45 - surrounds zero. And so now we could compute this partial derivative here, this respect to z,
2057:57 - and hence train our variational autoencoder model. The next and final point we'll look at in the
2058:04 - section is the variational autoencoder slots. Now from the autoencoder or the variational
2058:12 - autoencoder paper, the authors break up this loss into two main parts. The first part,
2058:20 - let's take this off, the first part is the reconstruction loss and this other part acts
2058:28 - as a regularizer. For the reconstruction loss, we try to minimize the difference between x and x
2058:37 - prime or x-shapo. So we want that the input and the reconstructed input or the reconstructed output
2058:48 - should be similar. Here is denoted as this, we're trying to minimize this and then for the
2058:55 - reconstruction loss, we're computing the KL divergence between this distribution and this
2059:03 - other distribution. Now to understand what these distributions actually signify, we can take a look
2059:11 - at this figure. So here we have this KL or this distribution KL of z given x which happens to be
2059:24 - a learned distribution meaning that when we'll be training this encoder model right here, this is
2059:31 - our encoder model, we'll be training this encoder model, we shall in fact be getting this distribution
2059:39 - which as we said already is a learned distribution. Nonetheless, we do not want this learned
2059:45 - distribution to be very much different from the distribution P of z and so that's why we are going
2059:55 - to minimize the distance between this distribution and this all this learned distribution
2060:06 - and the true prior distribution P of z. Now it should be noted that this KL divergence here
2060:12 - is a tool which permits us measure the distance between two distributions and so if we could
2060:18 - minimize this, if we could minimize this, then we'll reduce the distance between this distribution
2060:25 - and this distribution P of z and getting back to the original paper, it should be noted that
2060:32 - the reconstruction loss can be taken as the mean square error while this here will be our
2060:40 - regularizer and so this is what we obtain after computing the KL divergence between those two
2060:47 - distributions. Hello everyone and welcome to this new section in which we are going to be
2061:01 - building our own variational autoencoder models from scratch. Previously, we saw how variational
2061:09 - autoencoders could be used in helping to generate new images where we build out this encoder decoder
2061:19 - structure such that we could produce outputs which are similar to the inputs while being entirely new
2061:28 - images. In this session, what we'll be doing will be to build our own variational autoencoders and
2061:36 - generate our own images. The data we'll be using and training our variational autoencoder
2061:44 - will be the MNIST dataset which you could get from TensorFlow datasets. So right here we load
2061:51 - this dataset and then we concatenate both the training and the test datasets. So generally,
2061:59 - we usually have a dataset made of say x-train and y-train, x-test, y-test but since here we are not
2062:07 - going to be making use of those outputs as the y-train and y-test we just get this tool and then
2062:13 - we concatenate both since we would not be having a test set. So basically, we have that and then
2062:20 - one other modification we make is we or one other reprocess instead we take is we divide these
2062:26 - values by 255 so we normalize our dataset. So let's run this and then once our dataset has
2062:37 - been downloaded what we do now is we convert this dataset into the TensorFlow data format.
2062:45 - So we have our dataset which is tf.dataset or rather tf.dataset from TensorSlices. So you see
2062:59 - that we take that and then we pass in our MNIST digits which we've already downloaded. So we run
2063:07 - that and that should be fine. Now we could check out the length of this dataset and you see you
2063:14 - should have 70,000. So we have 70,000 different data points which make up our dataset. From here
2063:22 - we're going to define the batch size. So we would have a batch size of 128. That's it and then we'll
2063:31 - go to the usual steps of shuffling our dataset. We have our dataset, we shuffle, we batch and then we
2063:39 - prefetch. Now if you're new to this you could check out the previous sections in this course. Anyways
2063:44 - we have this three as we've said and then now we could run this. So that's it. You could see train
2063:55 - dataset. There we go. You see that we have this train dataset here and we could see its shape.
2064:03 - So it's all 28 by 28 by 1 images we have in our dataset and there are 70,000 of them. So that's
2064:11 - fine. Now getting to the modeling we're going to start with the encoder. You can recall that what we
2064:18 - had seen so far was this model or this encoder model which takes in an input image right here
2064:29 - and then outputs the mean and the variance. So we have the mean and the variance and then this two
2064:41 - have been combined via the reparameterization technique where we have mu plus sigma
2064:52 - times a random value drawn from a normal distribution and then this z is passed
2064:59 - into a decoder here. So it passes into a decoder and then we get an output image such that the
2065:08 - difference between this two is minimized. Now that said let's get back to the code and we
2065:15 - design our encoder. So our encoder here is going to be a very simple conf net. We'll start by
2065:21 - defining the latent dimension. There we go. Let's just put this right here. So latent
2065:29 - dimension will be two. We have that and then we get back here. For encoder we're going to start
2065:37 - as we said with this encoder input. So we have an encoder input and then this has a
2065:45 - shape which we're going to give to be 28 by 28 by 1 just as we just the same as that of our images
2065:54 - now data set. So that's the input. We've seen this already and then from here we'll define a conf 2D
2066:01 - a conf 2D which has 32 filters 3 by 3 activation relu. So supposing that you already have some
2066:16 - background knowledge and conf nets activation relu number of strides equal to the pattern
2066:24 - same. So we're going to build this very basic conf net. Now this takes in the encoder inputs.
2066:32 - Remember or recall we're using the Keras functional API right here. So we have encoder inputs.
2066:40 - We then create another conf layer. We just basically copy this and paste it out and then
2066:46 - what we would have here is an increased number of channels. So we have 64 here and from here
2066:54 - we'll go ahead and flatten our outputs. Now note that here we have X so we should change this to X
2067:01 - from here we move on to flatten. So we have flattened and this takes in X. So now the
2067:10 - output is flattened. We are now going to output both the mean and the standard deviation which
2067:17 - we are going to use in sampling. But before that we'll pass this into another dense layer. So here
2067:24 - we have this dense layer let's say 16 outputs activation, activation relu and then we take in
2067:35 - X. Okay so we have that and now we're ready to let's just copy from here. We're ready to get the
2067:42 - mean and the standard deviation. Copy that paste it out here and this. Okay so here we have the mean
2067:51 - and we'll have the standard deviation. So we have dense activation relu standard deviation and then
2067:58 - here since remember we have this output to be the latent dimension. So here we have instead of 16
2068:06 - we now have latent dimension which we've already fixed right here to be 2. Now one very important
2068:15 - reason why we we have those activations to be relu here would be simply the fact that the the
2068:22 - mean and the standard deviation are all positive numbers. So because we may output or we may get
2068:30 - negative numbers here we want to always make sure that the values we get are positive. Now the
2068:38 - problem with the standard deviation particularly is the fact that it's usually a very small number
2068:45 - between 0 and 1 where the number is very far away from 1 meaning that the number is very
2068:54 - instead closer to 0. So we have a number very close to 0 like this but then the problem with
2069:02 - working with the relu is that having to find derivatives around this 0 here will lead to
2069:12 - numerical instability during training and so what we want to do instead is to map this range of
2069:19 - values or this possible range of values that the standard deviation can take to a larger range.
2069:26 - Now to carry out this mapping we have to use a function which is both continuous in this range
2069:35 - and monotonous that is either increasing or decreasing. Now one great function for this
2069:42 - task will be the log function as this log function will map values of x in the range 0 to 1
2069:53 - two values in the range let's open it up here in the range negative infinity log or the limit as
2070:02 - we go to a 0 the log is negative infinity if you plot out a log you would have something like this
2070:06 - let's have something like this so you find that as you go towards 0 log goes towards negative
2070:14 - infinity so that's why we have negative infinity here and the log of 1 is 0 so we go from this
2070:24 - range to this larger range hence we can have a much more stable training process and so what
2070:33 - we'll do now is instead of relying on this ReLU activation to ensure that our standard deviation
2070:40 - is always positive what we'll do is we'll instead compute the log of the standard deviation square
2070:48 - which happens to be the log of the variance so we're going to take this off for both the standard
2070:57 - deviation and the mean stick that off and then right here we have log bar that's a log of the
2071:07 - standard deviation square log bar and then we'll move on to the sampling process where we're going
2071:13 - to obtain z remember z is equal to mu which is the mean plus the standard deviation times epsilon
2071:22 - where epsilon here is a random number drawn from a normal distribution we are now going to create
2071:29 - the sampling layer which takes in the mean and the log of the variance and then outputs the z
2071:36 - so here we have z equals sampling there we go we now taking as inputs the mean and the log of the
2071:47 - variance and that's it so from now we're going to create the sampling layer so let's go ahead and
2071:56 - create the sampling layer so we have your sampling sampling there we go and this is a layer
2072:05 - okay so we have that then we just we just have this call method which takes in our inputs
2072:13 - let's let's just have our inputs and then from those inputs what we would have is the mean
2072:21 - and the variance so basically we have we extract the mean of the variance from this input remember
2072:28 - we have the mean and a log bar let's say mean and the log bar actually so we have that mean
2072:37 - and log bar which we extract from the inputs and then if you remember what we have here
2072:47 - so the way we obtain z is mu plus sigma times this random number right here now let's let's see how
2072:55 - when given mu when we when we get mu and we get the log of the variance we are able to obtain
2073:04 - this sigma because we already have mu but now we need to get sigma now note that sigma is a standard
2073:10 - deviation and the standard deviation or let's say let's just write sigma and sigma itself
2073:18 - is equal the square root of the variance you see that it's equal to square root of the variance
2073:25 - so it means that and then we know that the variance written like this can be written as
2073:33 - e that's exponential to the power of log of the variance so generally we know that x equal
2073:43 - e to the log of x you see that so here we have e to the log of the variance and then we also
2073:51 - have the square root so let's have this here now this is equal e to the log of the variance let's
2073:59 - just leave that as v and all of this to the power of a half we also know that e to the power of
2074:06 - e to the power of say x to the power of all of this to the power of a is equal e to the power of
2074:13 - a x see that so here this is going to be equal e to the power of half times log v so it's going
2074:21 - to be half log variance so now we now that we have the log of the variance to obtain the sigma
2074:31 - this basically what we need to do so let's get back and then we have the mean plus
2074:41 - sigma which is exponential and then we have half so 0.5 times the log of the variance
2074:56 - and then we need to multiply this here the sigma by a random value so we have tf the random
2075:05 - normal and then we specify a shape which is simply the batch size and the latent dimension
2075:16 - now that we have this we can now go ahead and define our encoder model call this encoder model
2075:24 - which is a tensorflow model and we have encoder inputs for the input there we go that's it here
2075:32 - and then for the outputs we have this list made of z the mean and the log variance so we have
2075:42 - mean and log var okay so let's give it a name we'll call this encoder now once we have this
2075:50 - we could get a summary of this so we have encoder model summary which we could visualize right here
2076:00 - see that from here now we move on to defining our decoder the decoder as we've seen already
2076:10 - we'll take in the z here and then output the images so we'll go ahead here and create the inputs
2076:23 - call this latent inputs and then there we have a tensorflow input and shape is gonna be the same
2076:31 - as that of z so right here we have latent zim and that's fine okay so now we have this we're gonna
2076:40 - take our z let's get back here so now we have to up sample this z here which has a shape to now
2076:49 - an image of shape 28 by 28 by 1 but generally what we've been doing is we've been used to
2076:59 - down sampling so in down sampling we have an input we have some conf net layers we stack them
2077:08 - up and then when we get towards the end we could flatten and then we have some dense layers with
2077:15 - a specific output which matches the type of output one again but in our case we're now doing
2077:21 - some sort of the opposite of this so what we'll do is we'll go through some dense layer
2077:27 - here we'll go through some dense layer and then from this dense layer we would pass this into a
2077:35 - transpose convolution layer so we've been used to the convolution layer but here we use the conf
2077:41 - to the transpose layer essentially what we have is this conf to the layer with its weights which
2077:54 - up samples inputs now getting back to the code we have this input which has shape batch size
2078:02 - by the latent dimension let's say two and then from here we want to make use of a conf to the
2078:09 - transpose layer which takes inputs batch size by some x by some y by some z that's similar to
2078:19 - the conf layer so what we will have to do is we're gonna uh reshape this actually so we have
2078:26 - to reshape this such that we have something like this now if we want to have uh x y z
2078:34 - such that x is say for example seven y is seven and z is say 64 let's take that example
2078:43 - then we have to ensure that what we're getting after this has shape b by um this seven by seven
2078:52 - by 64 seven times seven times 64 you see getting back here we have this let's let's maybe redraw
2079:05 - this again so it's clear so what we're saying is we have this input and then what we intend to have
2079:12 - is something like batch size by seven by seven by 64 now the reason why we pick in seven is because
2079:23 - the output is 28 by 28 so one to be able to up sample sorry that we could say seven by seven
2079:31 - up sampled to 14 by 14 and then the 14 by 14 of sample to 28 by 28 so that's why we pick in seven
2079:41 - year now uh this uh year you see it doesn't match with this there's no way we could reshape two to
2079:48 - become this so what we'll do is i will pass this through a dense layer which has outputs batch by
2079:55 - seven times seven times 64 and now after reshaping we could obtain this so let's
2080:04 - get into the code and what we'll have is uh dense layer dense oops we have uh dense layer
2080:13 - um seven times seven times 64 and then we have the activation
2080:24 - activation relu and this simply takes in the latent inputs so we have here latent inputs
2080:32 - there we go now from from here now we we do the reshaping so we have x equal reshape
2080:38 - and then we specify the shape so we have seven by seven by 64 so that's it now we have that we
2080:48 - have x there we go we now start with our conf to the transpose so you see we'll reshape this into
2080:54 - this now could make it so far come to the transpose come to the transpose which takes in uh number of
2081:03 - filters is very similar to the conf layer so we have a number of filters let's say 64 filters
2081:08 - and then the the the kennel size 3 the activation relu so we're going to use the relu activation
2081:17 - number of strides um two the padding is going to be the same see so it's quite similar to the
2081:24 - conf layer but the difference is that now we up sampling instead of down sampling so we have that
2081:30 - then from here we are going to change this to 32 so for the encoder what we did was we increased
2081:41 - this year this number of channels and then here we reduce the number of channels now in our final
2081:50 - output layer we're going to have this decoder output which is going to have just one channel
2081:59 - so here we have an output which is 28 by 28 by 1 where the values lie between zero and one so
2082:09 - what we're going to do now is we're going to have channel numbers equal one um the activation
2082:17 - instead of relu will be sigmoid so here we have sigmoid and then we're not going to use any strides
2082:25 - since we're not up sampling so that's it now the reason why we're using sigmoid here is quite
2082:30 - simple since one of our values fall between zero and one we want that each and every time we have
2082:36 - an input we have something like this so no matter the input we have the sigmoid will always put the
2082:44 - the value or make that input turn into a value which lies between zero and one and that's
2082:52 - basically what we want to do in this last layer right here so that's why you see we're making use
2082:57 - of the sigmoid now once that's done we create our decoder model which is tensorflow model and then
2083:05 - we have latent inputs there we go we have our decoder output the name is decoder and then we
2083:18 - could get a summary of this model so that's basically it let's run that and there we go
2083:26 - see we have a decoder model now for the training we are going to make use of the atom optimizer
2083:32 - with a learning rate of 0.001 and we're going to train for over 20 epochs now as we've seen already
2083:41 - our loss we made of two parts that's the reconstruction and the regularization part
2083:47 - now for the reconstruction part aim is to minimize the difference between the output image and the
2083:55 - input image so we'll go ahead and start with the reconstruction loss we have our custom loss
2084:04 - there we go it takes in y pred takes in y let's start with y true takes in y true y pred
2084:12 - and then the reconstruction loss itself is defined such that we have recounts let's just say loss
2084:18 - reconstruction is equal tf keras losses binary cross entropy loss okay so our outputs remember
2084:34 - outputs range between zero and one so we could use the binary cross entropy loss here feel free to
2084:41 - test all our different losses so we have that and then we pass in y true and y pred now once we
2084:49 - make use of this we are going to now sum all the values because we have in let's say we have
2084:56 - something like this let's suppose that it was our five by five outputs we are having so we would
2085:01 - have something like this one two three four five that's fine five there we go you see we have this
2085:11 - five by five here so with this binary cross entropy we'll be able to get the the difference
2085:18 - for each and every position here now we need to sum all this here so what we would have is
2085:24 - um let's take this off we'll take this off and we'll have the reduced sum so this reduced sum
2085:36 - now we'll sum all this different um difference all these different loss values we get here
2085:43 - so now we we we sum for each and every position obviously we have 28 so it's 28 by 28 positions
2085:49 - and we also need to specify the axis here so the axis we're going to work with is one and two
2085:57 - one and two now to understand why we have in this let's take a look at the shape of the output
2086:03 - is b by 28 by 28 by one but where we're actually carrying out this where we're actually computing
2086:12 - the loss is in this axis here so uh we specify one two because this is zero one two three so
2086:23 - this is where we want to compute our loss so it's on this two axis now said we have that and then
2086:32 - we now look for the mean so we could after summing up we could look for the mean let's
2086:38 - average the values and that should be it so that's it for our loss reconstruction the next step
2086:46 - let's take all this off the next step will be this loss regularization getting back here you
2086:53 - see we have this sum see the sum of the the the the variance plus the mean square of the mean minus
2087:02 - one minus the log of the variance so let's get back here and we have this negative half here
2087:09 - if you take this negative and multiply by each and every one of this would have
2087:14 - log var plus one minus the mean square minus the var before we continue remember again that
2087:24 - we could get sigma j as e to the log of uh sigma j see that so oh so yeah yeah no let's
2087:35 - let's let's let's write this better so we have sigma j equal e to the log of sigma j okay so
2087:46 - that's it um let's get back here we have uh loss uh we'll still have this mean and sum so we'll sit
2087:56 - uh average sum and then find the average uh let's have that and then we'll have
2088:03 - negative 0.5 times the log var let's get here we'll need a log var
2088:18 - oh let's say log mean and then let's let's get exactly what we had here remember in this
2088:25 - um encoder model we outputted a mean and a log var so let's have mean there we go we have mean
2088:36 - and log var
2088:41 - okay so we have this set now we could make use of it right here we have as we've said already
2088:46 - log var plus one so we just have plus one minus the mean square so tf mat mean uh rather the square
2089:01 - of the mean and then minus tf mat e to the power of log var see that and now we return
2089:12 - the loss reconstruction and the loss regularization so that's it so this is our custom loss we run
2089:23 - this uh we get in this error let's add this and run that again okay so that's fine and we now
2089:33 - set to start with the training but before going on we have to um also specify the axis for this sum
2089:41 - so right here we have this axis which is equal one and we explain why we we specify this axis to be
2089:49 - one now the shape of the log var and the mean is this matched by two and so the shape of all
2089:58 - this sum will still be this and so if you're comparing this all right if you're computing
2090:06 - this here you'll get this kind of output and so since for the loss we need a single value
2090:12 - we need to sum all the um values we get in this axis so that's why you see we specify the axis
2090:20 - to be one we have this uh input which we define then we have the encoder model which outputs
2090:28 - z the mean and the variance which you're not going to make use of and then we have the decoder
2090:34 - which takes in z and then produces this output so here we have this uh vae model which contains
2090:42 - both the encoder and the decoder now we're going to go ahead and build our custom training block
2090:47 - we suppose that you already have an idea of how this works so we have that training block which
2090:55 - takes in our x batch and then we're going to make use of tensorflow's gradient tip so we will have
2091:03 - this width tf gradient tape gradient tape there we go as a recorder
2091:12 - we're going to pass this batch into the encoder so we have our encoder model which takes in
2091:19 - x batch and then what it outputs is z the mean and the log variance here we have this okay
2091:29 - so we have that set and then now once we get this we we we get the z from here and pass this
2091:38 - into our decoder so we have our decoder model which takes in z and then what it outputs is
2091:45 - our y predicted see that and now from here we could obtain our loss by simply calling on our
2091:54 - custom loss method which we'll define it takes in the y true it takes in the y pred y pred it takes
2092:04 - in um as we'll define here the mean and it takes in the log variance now the this y true
2092:15 - y true happens to be the x batch remember we are having this here this encoder takes in that
2092:27 - so we we have our input image here let's let's take this off android a bit here a bit clearer
2092:33 - so here we have this and we have this so we have our encoder and we have our decoder we have our
2092:39 - input image which is what we expect to have here so our y true is what we pass as input here which
2092:46 - is this x batch so that's why you see we specify y trials x batch then y pred is what a decoder
2092:52 - produces so we're going to compare the y pred and then the y true which is as we've said already
2092:59 - the x batch that's fine we get back to the code we have that said and then now we have our partial
2093:10 - derivatives our partial derivatives which we'll get by making use of the recorder
2093:17 - so we have recorder gradient it takes in the loss it takes in the overall models trainable weights
2093:26 - so we have trainable weights okay so talking about the overall model which we yet to define
2093:34 - we have it right here it takes in this input 28 by 28 by 1 it takes the input
2093:42 - pass into the encoder model gets the output from the encoder model and then into the decoder model
2093:49 - and then from there we create our model which takes in v ae input and this output right here
2093:56 - so from here let's get the summary and we see that we have exactly what we expect so we have
2094:04 - this model which takes as we've said already this input the encoder outputs z the mean and log bar
2094:13 - and then the decoder outputs this image right here so let's get back to our training as we're
2094:21 - saying where we we have this partial derivatives from the loss and the trainable weights
2094:30 - so that's it we then go ahead with the gradient descent step we apply the optimizer
2094:37 - so we have optimizer that apply gradients there we go which takes in our partial derivatives
2094:47 - and our trainable weights so there we go we have z partial derivatives which we just calculated
2094:55 - right here derivatives and then the trainable weights trainable weights there we go okay so
2095:06 - that's it we could from here just simply return the loss now we're gonna run this
2095:15 - and let's we haven't run this yet okay so we have that already now we could define our neural
2095:22 - learn method so we have your neural learn which will take in the number of epochs and then
2095:30 - from here for the epoch in range epochs there we go we're going to start by printing out
2095:44 - the training starts for epoch number whatever epoch we add so we format that to take in the
2095:55 - epoch plus one since we're going to step from zero or we could just take this one from here
2096:02 - and then there we go now we have to add plus one here okay so we have that and then now
2096:12 - we're going to do for step x batch we're going to enumerate in enumerate
2096:21 - um the train data set which we've defined already let's get back to the top and we see we have our
2096:29 - train data set so we're going to go through this train data set let's get back here there we go
2096:37 - so we go through our train data set we take a specific batch and then we compute the loss
2096:45 - so we have that training block which we've defined here our training block has been defined here
2096:53 - so not only we compute the loss but we also apply the gradient descent step for that specific batch
2096:59 - so we're doing this for each and every batch of our training data set now what this takes in is
2097:05 - our x batch simple as that now once we have this the next thing we'll do is we'll print out our
2097:12 - loss so the training loss um is um there we go we have loss and then once the training is complete
2097:24 - we could simply print out training complete okay so we have that set now let's run this
2097:35 - and then we have neural learn let's train for ebooks um here we get in this error let's take
2097:44 - that off that's fine let's run this again so training is now complete and here's what we get
2097:52 - you see that our loss drops and then we can now get straight into testing out our VAE model
2097:59 - now before testing let's recall that our VAE is comprised of two units that's our encoder
2098:06 - and then the decoder right here now we've trained this VAE model end to end to make sure that the
2098:16 - inputs look very similar to this outputs produced now if we want to generate new outputs what we'll
2098:26 - do is we are going to cut off or we are going to take off this region here and focus only
2098:33 - on the decoder now to generate an image at random or to generate a digit in our case at random
2098:41 - we'll just have to pass in a z in here that's a random value of z remember z is mu plus sigma
2098:52 - epsilon so we have to pass this value of z in here and then get a random output now remember
2099:02 - z is two-dimensional so z is this vector made of two values and so that said we will define
2099:11 - the first values here which we will call greed x and we use the lean space method to get values
2099:22 - from a scale let's add a cell above this so we would have a scale which will take a value of one
2099:32 - and then we will have n equals to say 16 different values so here we'll go from negative one to one
2099:39 - and then we'll have 16 values in between we'll pick this for grid y now so this is the first
2099:48 - element and this is the next element so here we would have different elements so that we could
2099:54 - generate different images there we go let's run this and then print out greed x and greed y now
2100:08 - that we have this done the next thing we'll do is we'll plot out our different images which we
2100:14 - shall generate using our decoder so here we have this figure we define the fixed size let's say
2100:24 - five anyway five by five and then for i and greed x and for j in greed y we define the different
2100:42 - subplots subplot um five by five by k plus one let's define k right here so k equals zero okay
2100:53 - so that's it this is greed not grad we have greed that way and then now we're ready to use or to
2101:02 - use this greed x values that's the values of i and j to generate new images okay so what we'll do
2101:10 - now is this is plt the subplot so what we'll do now is we'll have our input which is tensorflow
2101:19 - constant and then we have i j so these two values and then from here we have the output which is
2101:29 - our va e but notice how we pick out uh layers too so uh to better understand this you should get
2101:38 - back here where we defined our va e model so here you see this is uh the va e our original
2101:46 - auto encoder which is made of different layers so uh this is layer zero layer one and then layer
2101:56 - two um if you do va e dot layers um let's say zero see you have this input layer right here
2102:10 - now if you change this and say anyway let's let's say for i in uh range three there we go we're
2102:21 - going to print this out so we're going to print our layers i let's run that and we see that we
2102:29 - have this input layer we have this uh functional model this other functional model but you should
2102:36 - note that's basically our encoder and our decoder okay so that said we're going to make use of our
2102:44 - va e layers here so we have the layers two this is to say that we're using the decoder actually
2102:51 - so our decoder is going to take in the input see that it takes in the input there we go and then
2102:59 - we'll have to select um our first axis here so that's why we have this and then we have to
2103:10 - um select this other axis right here so sorry that our output now you see if we if we do this
2103:18 - if we have zero here we do this see this is not going to be transformed to 28 by 28 so basically
2103:24 - that's why we do this now once we have that we do the aim show and then we have our output our map
2103:34 - is uh we there we go plot axis um off and then k plus equal one okay so we basically increase
2103:52 - the value of one of k sorry sorry that uh we could have this as different subplots so let's run this
2103:59 - now and then see what we get getting this error here so this is this is actually because we have
2104:07 - in many more values as compared to what we're defining here in this subplots so what we should
2104:14 - have here should be n and here should be n let's set let's run this again and see what we get
2104:24 - now as you can see we are able to generate these digits making use of just this z vector right
2104:33 - here which is composed of two numbers now one thing you can notice here in this latent space
2104:40 - is that as we go from values of negative one to one in this two-dimensional latent space
2104:49 - the outputs are created or generated such that in each line as you could see we have
2104:59 - one digit which is being slowly morphed into another see here we start with a nine but as we
2105:11 - change values in this latent space see slowly we get to eight and then here you see eights
2105:20 - and at this point you start with a nine and then you slowly get into fives and so on and so forth
2105:25 - you can look at this horizontally as well as vertically see that we get to six and that and
2105:32 - then you could also look at this diagonal you see nine eight three two six and then you get to zeros
2105:39 - so for this first line or this first lines you can look at this first lines as going from nine
2105:49 - to eight then here it's like from nine to five passing through three see and so on and so forth
2105:58 - so basically this is how we generate images using variational auto encoders
2106:05 - at this point one question you may ask yourself is we are having this
2106:18 - outputs which have been generated from the making use of this z vector right here
2106:25 - but how does the z vector this z vector here of the different images vary with the particular
2106:38 - digits in those images so how does the z vector in say let's take this for example in six
2106:45 - vary with that of zero with two and all other digits to experiment with this we'll copy out
2106:55 - this code we had already and then get back here and now try to pass in the different digits into
2107:05 - our encoder and then plot out the positions of our z vectors and then see whether there is some
2107:16 - correlation with the z vector position and the input we have your right your other
2107:24 - specific input digit that said we're gonna not make use of this so we have that and then here
2107:33 - we're going to have our y train so this time around we'll be needing the y train because we
2107:39 - need to know exactly what digit we're dealing with unlike here where we just needed to have
2107:45 - the input images okay so that's it we have x train y train we're going to reload this and then
2107:53 - we do the usual pre-processing from here we run this cell and then we move straight away
2108:00 - to pass in those images in our encoder now similar to what we had here now we're going
2108:07 - to be making use of our VAE layers and then picking out the first index so here we have
2108:16 - output we could call this or basically we have z then we have the mean and the variance and then
2108:24 - this is equal let's copy this here um there we go we have that then this is now one see that
2108:34 - and then from here we pass in this x train so that's it we pass in the x train and then we could
2108:42 - go ahead to start with a plot in uh plot figure specify the figure size figure size equals 12 by
2108:53 - 12 there we go and then we do a scatter plot now the scatter plot will show us the different
2109:00 - positions of the values of z that's the z points in our two-dimensional space let's take this off
2109:13 - this different z points and then also we're going to color this such that digits belonging to the
2109:22 - same level have the same color there we go we're going to plot this as our x and then our y so we
2109:34 - have that and then i will specify this y train as our levels so that's it and then we can now
2109:44 - go ahead to plot out this color bar and show let's run that and here's what we get as you could see
2109:56 - the levels here go from zero right up to nine and then you could notice these different clusters
2110:05 - we have in here see these different clusters you see that this shows that the encoder path of our
2110:15 - variational auto encoder has been trained such that now it's able to generate values of z
2110:23 - where this point is outputted such that two inputs belonging to the same digit will be
2110:38 - closer to each other as compared to two inputs belonging to two different digits
2110:44 - so what we've been doing so far is we've been training the variational auto encoder model with
2110:52 - our own custom training block you see right here we made use of this gradient tape and
2111:00 - we carry out the gradient descent manually as compared to when you just have say model
2111:07 - feet and then you pass in your training data set and basically this takes care of
2111:14 - doing the training now the advantage of working with this feet method is that it's quite simple
2111:20 - to use but compared to this custom training loops it doesn't give you that much freedom and that's
2111:30 - why we're making use of this gradient tape here to train our model now with tensorflow it's possible
2111:41 - to actually get the best of both worlds now this means that we can be able to train or get this
2111:51 - custom training right here and then still make use of the feet method the way this is done
2112:00 - is by overriding the train step method of our model which we shall define so that's
2112:09 - set let's get right here and define this VAE model then we go ahead and define this model
2112:17 - as we would do with a usual Keras model but just as we had done right here that's in this VAE model
2112:27 - we had built here we need to take into consideration or take this argument here the encoder model
2112:35 - and the decoder model so that's it now once we have these two models we have here our encoder
2112:45 - which is our encoder model and we do the same for the decoder now once we're done with this decoder
2112:53 - let's type this out we have your decoder as we're saying once we're done with this decoder
2113:00 - the next thing we want to do is define some loss so here we have our loss
2113:07 - it's actually a loss tracker now we'll get to understand better why we call it this a loss
2113:12 - tracker so here we have the average or the mean of the different losses we shall get
2113:19 - and then from here we'll go ahead to the main section where we shall override the train step
2113:25 - method so just like let's add this here just like we used to working with methods like model
2113:33 - like the compile method like the feet method here we have this train step method which is
2113:40 - called each time we call on this feet method right here but since we are not going to make use of the
2113:47 - default feet or the default train step method we'll have to override this and write out our own
2113:56 - training block which is basically this block we have here so what we could do here is copy this
2114:03 - out so we could simply copy this out and then paste it out here now we let's just copy all this
2114:10 - there we go and that's it we paste this out so here we have our training block we'll call this
2114:19 - train step so we override this train step method which is a Keras model method so here we have
2114:28 - our train step we're taking in our batch there we go and then it's going to be similar to what
2114:37 - we've had already in the training block so you see here we have z mean log bar which is gotten
2114:44 - from the encoder model and from here we pass z and then we obtain y print we have our y true
2114:52 - we obtain our loss we've already defined our custom loss method and that's it then from here
2114:59 - we're going to update our loss metric so here we have loss tracker which we defined already here
2115:07 - we're going to update its state so we update our loss state with the value of this loss
2115:15 - so that's it now we are going to return the result so we have this dictionary we have loss
2115:23 - and then we have our loss tracker dot result see that we have that so there we go we have updated
2115:34 - our loss we output a loss but one important method we need to call again is the metrics
2115:43 - method so here we have metrics you could define several metrics but here we just have this loss
2115:51 - and then we return our loss tracker so we have loss tracker returned now notice how this is
2116:03 - actually a list so you see this is a list because we could have say different losses which we could
2116:10 - return and then we have the property decorator which we place right here so once we have this
2116:19 - set you see we do not need any more to make use of this or to write this as as it was written
2116:26 - and so this means that we we are not going to be writing out for example training start for epoch
2116:33 - this the training loss is this and all of that now all we need to do is just call on our model
2116:40 - dot fit method so right here let's do let's run this first let's run this right here there we go
2116:49 - and then we go ahead we're getting an error now from here we we define our VAE so it's quite
2117:00 - similar to to what we had already seen here see that now we have this VAE which is now this VAE
2117:09 - we should create a year so let's have that VAE equal VAE and then it takes in the encoder model
2117:18 - and the decoder model that's it so once we have this now we could do just as we were used to
2117:25 - doing let's let's even say model let's just say model and then here we have model compile as
2117:31 - usual and then here we're going to pass in our optimizer so here we have optimizer i guess we
2117:39 - defined this optimizer already so let's check here yeah we have defined optimizer so here we
2117:45 - just have optimizer equal optimizer we should define already and then we do model dot fit see
2117:56 - that so that's all you just need to now make or call this fit method and then we have our train
2118:04 - data set which we pass in the number of epochs we'll define this to be 20 and then the batch size
2118:15 - equal say 128 so that's it so here unlike before where we needed for example to specify the
2118:23 - optimizer in this way and then actually get in depth to understand that this gradient descent
2118:32 - process needs for example the partial derivatives and the trainable weights now you'll see that
2118:39 - unlike before where if you were for example needing to make use of some callbacks you would
2118:45 - have to write that or include that in here with some custom code but now all you need to do is
2118:54 - just have callbacks so we we we both will now to write our own custom training loops while still
2119:06 - taking advantage of everything that comes with the fit method so right here you could have
2119:14 - some callbacks and i'll be it anyway let's take this off and then let's get back here
2119:20 - where there's this slight error you see here we have this VAE VAE dot trainable weights and that
2119:26 - now this was in the case where we had defined our VAE this way we had this model and then get into
2119:33 - our custom training loop right here let's get down here we had to actually uh we the way we
2119:41 - got the trainable weights was from our VAE model which we had defined now given that we we have
2119:48 - this VAE model right here the way we access the trainable weights is by using self so we have
2119:56 - self dot trainable weights and then here we have self dot trainable weights and this is simply
2120:03 - possible because we are inheriting from a Keras model which already has these different attributes
2120:13 - which have been defined now that said let's run this that should be fine trainer complete you
2120:20 - could see that we get in our usual outputs when we making use of the fit method and then we could
2120:30 - go ahead and start with a testing so we run the cells and then in here we have some modifications
2120:36 - to make so right here or previously we had this layers and predict and we took in the inputs and
2120:44 - we got some output now here we have our model model that predicts you see that but this model
2120:52 - is the is a whole VAE model so since we want to make use of our decoder we're going to say model
2120:58 - dot decoder model there we go model dot decoder actually the decoder because here you define this
2121:08 - decoder attribute so we have model dot decoder there we go dot predict and that should be fine
2121:16 - okay so let's run this and then see what we get there we go you see we have exact same kind of
2121:25 - outputs as when we were having our own custom training loop right here and that's it for the
2121:33 - section hope you enjoyed it see you in the next section
2121:40 - hi there and welcome to this new session in which we are going to delve deep into generative
2121:46 - adversarial neural networks in the previous session we have seen how we made use of the
2121:52 - variational auto encoders to generate these kinds of outputs which are in fact MNIST digits
2122:01 - from just an input noise so what we had was this kind of encoder decoder structure where we had
2122:10 - some input images here like this one for example some inputs and then we train the model such that
2122:19 - the outputs look like this input and then once training is done we could take off this encoder
2122:28 - and then just pass in a noise signal in here and then generate new outputs like what we have here
2122:39 - which look like those from the original data set and in this section we'll be seeing how to build
2122:48 - a new set of or a new category of generative models known as the GANs and we'll use them to
2122:56 - generate images like this one which we have here you should note that all those images are images
2123:03 - of people who do not actually exist but before we dive into practice and see how to build models
2123:12 - which can build this kind of realistic looking images we should start by understanding how this
2123:20 - generative adversarial neural networks that's GANs work now this GANs were first introduced
2123:27 - in this paper by Goodfellow et al where the GAN architecture was first proposed
2123:34 - and to understand how the GANs work let's make use of this figure from Louise Bouchard's post
2123:41 - now let's suppose we have the bank who produces real money you see here we have this real $100
2123:48 - bill and then on this other end we have the thief who produces fake money you see here
2123:57 - this $100 bill has this man with a mustache which is in the case of the real dollar bill
2124:04 - and because differences like this and say for example this compared to this are very clear
2124:14 - or can be easily seen this police officer is able to detect that this money is fake
2124:22 - but when this police officer detects that the the the bank notice fake he or she says that
2124:30 - it's fake because we have a mustache for example because we have this word fake written on it
2124:37 - because this $100 has a fake around it and stuff like that so it tells the the forger or the thief
2124:48 - what needs to be ameliorated in order to make sure that the next time this thief presents this
2124:56 - fake money to the police officer the officer thinks it is maybe this real money
2125:05 - and so if we suppose that real money takes a value of one and fake money takes a value of zero
2125:11 - let's change the color fake money takes a value of zero then let's say in the first years of
2125:17 - production of this fake dollar bills the police officer correctly says okay this is a zero and
2125:24 - this is a one that's it's very evident at the beginning but then with time this thief gains
2125:32 - experience and now produces fake money which looks just like the real money and this now
2125:42 - pushes the police officer not to be able to distinguish between the real and the fake any
2125:49 - longer now although we do not advocate for these kinds of malpractices it turns out that this is
2125:58 - the way the gains actually work and in our case we'll replace this generator or we replace this
2126:06 - thief by a generator model which is a neural network so this is going to be a neural network
2126:13 - and we replace this police officer by a discriminator which also is a neural network
2126:20 - specifically a binary classifier which takes in some input and says whether it is real or not
2126:31 - whereas our generator here takes in some random inputs and then learns to output
2126:40 - this bank notes such that the discriminator thinks that it is real
2126:50 - now we shall head on to the GAN lab which is a project by Minsukang et al and we'll consider
2126:58 - some very simple example so here we'll suppose all that data distribution is this year see that
2127:05 - and then notice how we have a generator we have a discriminator and then the generator takes in
2127:12 - some random noise and then outputs the sample and then the discriminator takes in the sample
2127:22 - and says whether it is real or it's fake now apart from the fake samples it also takes in
2127:29 - the real samples and then also says whether it's real or it's fake now now the weights of the
2127:36 - generator and the discriminator have been updated such that after some time this samples which are
2127:44 - going to be produced will look very much like this right here so let's go ahead and click on run
2127:53 - and we see what we get you see how we start with the training let's take all this off see we start
2128:01 - with the training for now the fix let's let's pause and start over let's restart that okay
2128:07 - you see initially we're getting these kinds of outputs you see this output you could look at
2128:12 - this here so this is this kind of sample is generated and this is the data of our real
2128:19 - data right here and one thing we notice is that at times as time goes on you have here
2128:29 - both the green and the purples which are considered to be real and then here's only the purples
2128:36 - considered to be fake so this discriminator now starts making errors when it comes to
2128:42 - saying whether a given sample is real or not whereas on the other hand the generator
2128:50 - is now producing samples which look much like the real samples and it's because of this competition
2128:59 - between the generator and the discriminator that they actually called the GANS generative
2129:07 - adversarial this adversary comes from this competition between the generator and the
2129:13 - discriminator and it now leads to the generator producing samples which look very much like the
2129:20 - real samples one other point you should notice that as we carrying out this training overall
2129:28 - we have two main parts we have this block right here oh let's take this block we have this block
2129:38 - which consists of wearing when the discriminator takes in this real and then outputs some value
2129:46 - the the output from this is used to update the parameters of the discriminator and then
2129:54 - when the discriminator takes in this and then when the discriminator takes in this fake samples
2130:02 - this output here is used now to update the generator so we have this block
2130:11 - wherein we update the discriminator and we have this other block wherein we update our generator
2130:20 - so with that now we could pass this and you could change the data distribution
2130:28 - and so again in comparison to with the VAEs where we had this encoder and then the decoder well if
2130:37 - we have a distribution like this one so we have some some input we want to have or we want to be
2130:44 - able to output or get outputs which will look similar to this input distribution and then after
2130:52 - training this encoder decoder we now break this up and then make use of only this decoder to now
2131:04 - generate output which are similar our distribution is similar to that of this real inputs right here
2131:18 - again another important point to note is the fact that after training after a certain number
2131:24 - of epochs at some point where when the reals and the fakes look very similar the discriminator
2131:31 - now becomes somehow confused as if you notice here you find some green patches you see you
2131:36 - have some green patches purple patches here we have some green patches and purple patches so
2131:42 - it's no longer able to distinguish between the reals and the fakes and so instead of as before
2131:50 - or as at the beginning where I was able to say this is a one and this was a zero now it sees this
2131:57 - as a 0.5 and this as a 0.5 it becomes confused and so in fact the aim of our GAN training process
2132:07 - will be to ensure that the generator wins the fight it should be noted that most of very cool
2132:19 - applications of GANs are in the domain of image generation and so we'll look at some of these
2132:27 - applications in this article by Junotano Hoi GANs can be used in creating anime characters
2132:34 - you see here we have this anime characters which have been generated automatically using GANs
2132:42 - and so this means that similar to what we had here we are going to have a real data set which
2132:51 - produces images similar to what we have right here let's get back here similar to what we have right
2132:58 - here and then we're going to have a generator which will learn over time to generate images
2133:05 - which will look like the real images and that's how we get to have images like this one now from
2133:13 - here we also have pose guided person image generation here for example you'll notice that
2133:21 - we have this input image and then if we want to have this same person but with a different pose
2133:28 - then we could pass in this pose and then get this output right here so you see that we have
2133:36 - this input that's this image with this different pose and then this is being generated another
2133:43 - application is in cross-domain translations so you see here we have this input where we have this
2133:50 - two or say three zebras and then we are able to transform this input image automatically into
2133:57 - this other domain where we instead have horses and so this image or this input or this output
2134:05 - rather has been generated from this input and this could be done in the reverse direction as
2134:12 - we would see here we see you could go get from zebra to horse as we had here and then from horse
2134:20 - to zebra then we have another example this is a star gun which permits us carry out translations
2134:28 - or transformations where we modify specific high level features of an image so right here you see
2134:35 - we have this input and then we add for example blonde hair so it changes the gender modified
2134:42 - so that this this male becomes a female aged and talking about aged you could build an application
2134:51 - which tells you or which shows you what you would look like after say 2050 years then here we have
2135:00 - pale skin another awesome other modifications we could have here is he angry happy fearful and that
2135:08 - so that's it you you see making use of this star again you could carry out these kinds of
2135:14 - transformations on an input image the next we have this pixel dt again which creates clothing
2135:22 - images and styles from an input image so you see we have the source image and we have this
2135:27 - different images which have been generated from this source we have other examples right here
2135:34 - it's good see and then we have super resolution now for super resolution
2135:43 - gains have been used to increase the resolution of an input image while making this higher
2135:52 - resolution images more realistic so you see this image for example this by cubic this one right
2136:01 - here which has been gotten using the by cubic method and then we have this other image which is
2136:08 - using res net and then now you'll notice that there is some difference with the kind of
2136:16 - output we get using again notice how this image here looks more realistic or looks much more
2136:26 - like the original as compared to what this other two known gain methods produce to even
2136:35 - get a much clearer difference notice this part here where we have this water pouring you'll
2136:42 - notice how this looks much more realistic as compared to using a classical neural network
2136:50 - like the res net then from here move on to the next application which is that of generating
2136:58 - faces by this time around very high definition faces so here you see you have 1024 by 1024 images
2137:06 - generated you see this images of people who do not actually exist which have been gotten using
2137:13 - a pro gain that's a progressive gain now we move to the next we have style gain which even comes
2137:23 - with much better resolution and with some styling so from here we go on to high resolution image
2137:32 - synthesis where we could get you see this semantic map and then from here we generate this output we
2137:40 - have right here then the next go gains which as we've seen already takes these kinds of semantic
2137:51 - maps and then produces this output you see this is the the ground truth this is the exact output
2137:59 - and this is what the gain produces see that see from here we were to produce this from this
2138:05 - we able to produce this now this kind of technology could be applied in video compression
2138:12 - in a sense that during a video call where we have this input so supposing that we have this
2138:19 - this is a sender and then this is a receiver we separate it by this dotted lines right here so
2138:25 - we have this input right here and then we carry out key point extraction where we get this key
2138:32 - points as you could see here and then this is what's been transmitted via the network so instead of
2138:37 - transmitting this input we transmit this key points and then making use of some key frame which has
2138:45 - been passed initially we combine those key points with the key frame to produce now this output
2138:54 - right here which looks like this original input which we wanted to pass and so now at the receiving
2139:03 - end we are able to get this here at a much lower bandwidth since we are taking in only the key
2139:12 - points and not this whole input image then we also have applications in text to image where
2139:19 - we could pass in a text like this flower has long thin yellow petals and a lot of yellow enters
2139:27 - in the center and this generates this kind of output now talking about this kind of applications
2139:34 - we could check out rayon.com which is in fact a del E mini model where we'll be able to create
2139:43 - much more realistic output so let's click on this and while this is loading we could check out on
2139:50 - the other applications the next application text to image we've seen this already face synthesis
2139:59 - you see right here we get with a single input image we create faces in different viewing angles
2140:06 - so for example we can use this to transform images that will be easier for face recognition
2140:11 - so if we suppose that we get in this kind of input let's say we get this kind of input
2140:18 - we could generate or we could convert this for example into this one here such that
2140:26 - it will be easier for a face recognition model to do its job now from here we go to the next one
2140:35 - image in painting okay so right here we have this inputs let's increase this take this off
2140:43 - so as we're saying we have let's let's pick this one for example we have this input right here but
2140:48 - we have this patch which has been taken off and now making use of again we are able to generate
2140:54 - an output which will be like this input without the patch so just like the gain takes this patch off
2141:05 - and as you can see the gain does its job quite well let's reduce this there we go before we move on let's get
2141:15 - back to this output create it or i create on the com and you can see that this dial e mini model
2141:23 - produces even much more realistic images then we move on to the disco gains where we could create
2141:32 - outputs which match the style of a given input so supposing you want to go out on a little trip
2141:41 - you want to say take this back and you wanting to get some ideas of the kind of shoe you could put
2141:48 - on then you could make a call on the disco gain and you'll get this kind of output based on your
2141:56 - input so this is a model which is similar to the cycle gain which we had already seen right here
2142:05 - and where we were able to leave from one domain to another one other fun project will be to generate
2142:14 - emojis from input images see here we have this input image for example and we have this emoji
2142:21 - which is generated from the input then another very interesting application will be in deblurring
2142:29 - so right here we suppose that we have this input image which is clearly blurred and then we want
2142:36 - to de blur this image you see that we're able to produce this kind of images making use of gains
2142:41 - and you can see from those images here that gains do this job quite well another application is in
2142:49 - photo editing and so now you do not need to be some expert in photo editing to carry out some
2142:57 - of this photo edits all you need now is some gain and you're good to go apart from image generation
2143:05 - gains too can be used in music generation though in this course we shall focus on image generation
2143:13 - then the medical domain gains could be used in anomaly detection and that's it for this
2143:19 - section in the next section we are going to look at how these gains are actually trained in practice
2143:25 - and the type of loss functions we use when training those gains
2143:36 - hi there and welcome to this session in this session we're going to look at the gain loss
2143:41 - function and also how gains have been trained then finally we'll look at common gain training
2143:50 - problems so for a brief summary of what we've seen already we'll consider that we have this
2143:57 - two distributions right here this distribution or this one this black dotted lines represents
2144:05 - the real data that's similar to what we had here so similar to this real data right here
2144:13 - and then this other distribution in green represents the fake data which is going to
2144:18 - be generated by our generator so getting back here we have this you see this here which is
2144:30 - our discriminator so right here we have the discriminator d then we have as we've said
2144:37 - already the real and the fake distributions now once we start training we have this discriminator
2144:47 - which sees that most most of the real data is getting a score of one or it classifies with
2144:55 - the probability of one that this data is real and then as we approach this generated data
2145:02 - as we get samples from generated distribution we find that the discriminator now outputs
2145:11 - zero now as we keep on training the generation gets better and then the generated samples now
2145:21 - look a bit more like the real data you see they come they get closer this distance here
2145:28 - becomes smaller as compared to what we had before so we've trained and we've gotten to this point
2145:35 - where now this discriminator still sees the real data to be one but now confuses or sometimes
2145:44 - classifies this fake data to to be one so you see there around here see this samples we consider to
2145:52 - be ones are classified as one though we'll still have some samples classified as zero
2145:57 - and then once we get to convergence we have this half here so we have this classifier now which
2146:08 - is unable to differentiate between the real and the generated of fake data because the distributions
2146:16 - look very much alike and so that's it for the recap of the previous session now we're going to dive
2146:21 - into the gain loss function so right here we have this training algorithm which we could see here
2146:29 - expand this we have this training algorithm and if you notice we have this two loss functions here
2146:36 - one for the discriminator and the other for the generator but this could be combined into one
2146:42 - equation right here let's take this year and now we have our two player min max game with this value
2146:51 - function v now when we talk about this min max game right here it makes allusion to what goes on
2147:00 - between the generator and the discriminator so our two players in this case are our generator
2147:06 - and the discriminator now one thing you could also do with this gain lab is you could put out your own
2147:13 - distribution so let's say for example this distribution we apply and we start to train
2147:20 - and the the generator now together with the discriminator start to play this game where
2147:26 - at the end or at convergence we expect the generator to produce outputs which are similar
2147:32 - to that of the discriminator now coming back to this equations you'll notice that we have
2147:37 - min and g and max and d so to understand this notation you can consider that we are
2147:46 - minimizing this expression right here by updating the parameters of g and then we
2147:55 - maximizing this expression by updating the parameters of d which is our discriminator
2148:01 - where g is our generator and then if we try to separate this two that's to to get for the to
2148:09 - to start with minimizing for example for the generator you'd find that given that in this
2148:15 - first expression you see this is this expression right here let's have that so in this expression
2148:22 - there is actually no g so we could make use of only this when we're trying to minimize this
2148:28 - whole expression with respect to parameters of g so that's why if you if you take in the
2148:35 - algorithm given to us right here you find that for loss for g you have only that second expression
2148:45 - now for the d let's get back here for the d d is in the both sides or this both this expression
2148:53 - and this sort of expression so that's why we have the combination of the two in this our algorithm
2148:59 - right here now to better explain or better understand this in depth let's consider this
2149:06 - here so we've extracted for the d and for the g you should also note that let's get back here
2149:12 - while we talk about updating the discriminator or updating the generator is basically our gradient
2149:18 - descent step remember if we have for example tethered all right if we have let's take this off
2149:25 - if we have tethered at a particular step let's say step i to get tethered at i i plus one step
2149:35 - then we would have tethered i that's a previous step or let's say tethered i minus one
2149:40 - let's take this off so we want to get tethered i which equal to the i minus one minus the learning
2149:47 - rate times the partial derivative of the loss with respect to our tethered i minus one so
2149:58 - this is basically what we have in here and this expression here where we have this reversed
2150:07 - triangle that's this right here is actually this section of our gradient descent
2150:15 - and so this means that we're finding the partial derivative of our loss this is the loss with
2150:21 - respect to the parameters tethered then we have the same for the generator now let's get back here
2150:29 - where we're going to understand in depth what all these different expressions actually mean
2150:36 - right here we have this real sample and this fake sample so this two here we have real and
2150:43 - fake and then with this real sample we pass it into the discriminator which we hope or which
2150:53 - we will train to see that this is a one and then for the fake sample because this is a person who
2151:00 - doesn't exist whereas this person actually exists and we'll train again which we'll see
2151:05 - later on in this course to produce this type of output so let's get back here we have our
2151:13 - discriminator then we from here let's take let's instead shift this let's move this this way
2151:21 - so let's take this right here yeah okay so we have that and then here we have this right here
2151:30 - now this is our fake sample and then we have our generator let's change the color for the generator
2151:36 - so our generator produces this and then we have some random noise which we send in here because
2151:41 - we just we just want to be able to produce this from some random noise so we we have this random
2151:46 - noise right here which produces this our g which produces that fake sample and then we have our
2151:53 - discriminator who is able to classify or say whether an input is fake or not okay so getting
2152:02 - back here let's take this off getting back here when we want to train our discriminator we're
2152:10 - gonna have an input x right here see this input x now this input x is this real sample right here
2152:18 - so those are x i which is which is this here and these are discriminator which takes in this input
2152:26 - then once the discriminator takes this input it is expected to output a one and if you get back
2152:34 - here let's get back here you find you're told update the discriminator by ascending its stochastic
2152:42 - gradient and then you update the generator by descending the stochastic gradient now generally
2152:46 - when we're trying to ascend or ascending or descending actually is is different like what
2152:52 - we've seen already where we had theta equal theta minus linear rate partial derivative of the loss
2152:59 - with respect to theta is gradient descent which is actually what we're doing for the generator
2153:07 - but when we talk about gradient ascent it's actually this instead we have a plus instead
2153:13 - of a minus see that so if we have for let's plot our loss function here respect to theta for our
2153:24 - gradient descent that's a classical gradient descent what we want to do is to minimize
2153:29 - this loss but for our gradient ascent we want to instead maximize this loss
2153:34 - and then getting back here before we move on we'll consider this plot of the log function
2153:44 - so here if we have x and we have log of x then we have a plot which looks like this
2153:52 - let's have this there we go and this is one so when x equal one the log is zero and then well
2154:01 - as x takes smaller values approaching zero the log goes to us negative infinity so as we as x
2154:09 - goes to us zero obviously from the right in this direction going in this direction then
2154:17 - the log of x goes towards negative infinity so that's it now let's get back here for the
2154:24 - discriminator as we've seen we're having a gradient ascent so we're trying to maximize
2154:29 - this remember from our min and max loss function we we we're trying to maximize our discriminator
2154:36 - so when we when the discriminator takes in a real sample it gives us an output of one and
2154:46 - with an output of one the log of one so see here it's going to give us zero so the log of one is
2154:52 - zero so this will give us a zero now you should note that all the outputs of our discriminator
2155:00 - range between zero and one so our discriminator is our usual classifier so it's going to output
2155:07 - values between zero and one and so getting a value of zero is the highest um you could get as an
2155:14 - output after passing through the log so let me re-explain we we have this discriminator
2155:20 - which outputs value between zero and one oh let's plot it out let's reply it again here so the
2155:25 - discriminator outputs values between zero and one see that so this is what the discriminator outputs
2155:32 - and so when you when you have the log of this values between zero and one then the maximum value
2155:40 - for the log will be a zero see that the maximum value for all these values between zero and one
2155:49 - will be a zero so when you have a zero it means you've maximized this and that falls in line with
2155:54 - what you expect because we want to maximize this expression we have right here okay so we get back
2156:04 - here for the reals we want to output a one and the log of one will give us the maximum possible
2156:09 - value which is in this case zero now uh for this year we have a z remember this x is this real image
2156:17 - here the zer is our random noise which we have seen already here so when this random noise passes
2156:25 - through our generator we output a fake sample see that the generator takes in the random noise
2156:33 - and then this generator let's have this here the generator outputs this fake sample and once the
2156:40 - outputs this are fake sample we now take this fake sample and pass it into our discriminator
2156:47 - and we expect our discriminator to produce instead of a one this time around a zero see that
2156:56 - and in our case uh the log the log of one minus a zero is a log of one and the log of one is zero
2157:06 - that's the highest possible value we could get when we're dealing with logs in the range zero
2157:11 - to one so we maximizing this you see that now it's less understood we could move on to the generator
2157:18 - for the generator we want to instead minimize this expression so since we're trying to minimize
2157:23 - this expression we would expect the output from this to be negative infinity as the lowest
2157:29 - possible value but uh let's get let's put in these values and see how we'll obtain this negative
2157:36 - infinity now we have z which is our random noise when the random noise passes through g it outputs
2157:43 - this fake sample again and now this time around we expect z to consider this fake sample this
2157:52 - time around to be like a real sample so what we're saying here is at one instance we want
2157:59 - our discriminator to see this as real another instance we want the discriminator to see this
2158:05 - as fake and depending on the instance we're going to get or we're going to update parameters of
2158:13 - the corresponding network so in the case where we want to see this as a fake we want to update the
2158:21 - parameters of the discriminator such that it sees this as fake and then the case where we want this
2158:28 - to be seen as a real by the discriminator want to be updating the parameters of the generator so in
2158:34 - fact what's going to be happening here is let's change the scholar when training the discriminator
2158:42 - we're going to freeze the generator so we're not going to update these parameters we're going to
2158:45 - update the parameters of the discriminator now when training when training let's get back when
2158:51 - training our generator we're going to freeze this year oops when training the generator we're going
2158:58 - to freeze this and then update its parameters solita is able to fool the discriminator to think
2159:03 - that this is a real sample and when he thinks it's a real sample he's going to output a one
2159:10 - now log of one minus one is log of zero and log of zero is negative infinity see that and this
2159:18 - is the minimum possible value so we're minimizing this expression so getting back here we have for
2159:25 - a number of training iterations that basically for the number of ebooks uh we're going to do k steps
2159:32 - we're going to update the discriminator we are k steps see four k steps do this we take
2159:39 - we sample a mini batch of noise we get the noise uh mini batch of real samples then from here we
2159:46 - obtain uh we get the generator outputs and then we obtain the output loss which we use to update
2159:55 - the discriminator's parameters and then uh for these k steps i think in here they say we use k
2160:03 - equals one the number of steps are applied say use k equal one so you could modify this although in
2160:08 - practice k equals one is fine so um getting back here after going through k steps for this we're
2160:15 - going to update now the generator so sample mini batch of m noise again so just like this
2160:22 - then uh we obtain this output from the generator this time i only expect it to fold the discriminator
2160:28 - so we're going to update the generator by descending its stochastic gradient such that it
2160:33 - folds the discriminator and that's it for this section in the next section we're going to
2160:38 - uh get into some practice and see how to get this kind of results we had here so these are the real
2160:46 - samples and then uh what we'll obtain will be something like this see we're able to generate
2160:52 - this um fake outputs of these people who do not actually exist and they look pretty realistic
2160:59 - then one point we have to note before we move on is that the type of neural network used in this
2161:04 - original GAN paper is a classic artificial neural network and these were kinds of outputs they got
2161:10 - but in the practice we're going to be making use of the DC GAN so instead of the the simple GAN
2161:16 - we'll make use of the DC GAN the DC actually stands for deep convolutional um and that's it
2161:23 - DC deep convolutional so DC GAN means deep convolutional generative adversarial neural networks
2161:30 - okay so here we're going to be looking at this um neural networks
2161:35 - which are convolution based and so with that see you in the next section
2161:44 - hi guys and welcome to the session in which we are going to write the code for generating images
2161:51 - like this in TensorFlow from the previous session we had looked at the GAN loss function and now
2161:59 - we'll see how to adapt this loss such that we could instead use the binary cross entropy loss
2162:09 - and then from there we'll go ahead to look at different methods to make our GAN training
2162:16 - much more stable you can see from here that this GAN training is in a very stable process since
2162:24 - we have two adversaries where one is trying to maximize the loss and the other is trying to
2162:29 - minimize the loss so by nature GANs are much more difficult to train as compared to other
2162:37 - classical neural networks then after looking at how or looking at different methods to make
2162:43 - this GAN training process more stable we'll go ahead to train our GAN from our previous session
2162:50 - we had the discriminator loss and the generator loss now we want to introduce the binary cross
2162:58 - entropy loss you'll notice that it looks quite similar to what we have here now let's start first
2163:04 - with the discriminator for the discriminator it takes in this real image and then we expect it to
2163:10 - output one see that we expect to have a wonder now for the binary cross if we have to compare
2163:19 - this with binary cross entropy then this y-shuffle this y here is going to be equal d of xi which is
2163:33 - practically what we have here so this is our y-shuffle and then y this yi-shuffle is going to be
2163:41 - this here now when oh since we want our y to be equal one it means that one minus y will be zero
2163:49 - so we will not consider this but we consider only this part right here now if we consider only this
2163:57 - part we are left with this expression since y is equal one and then when y equals zero the case
2164:04 - where we have this sorry this y-shuffle equals zero then we'll have log of zero which is log of negative
2164:11 - infinity minus that with this minus which we didn't have here is now in this year because we're
2164:17 - dealing with a binary cross entropy the negative of the negative infinity gives us positive infinity
2164:23 - so it means that for y-shuffle equals zero we instead have positive infinity so here since we
2164:32 - want to have y-shuffle to be equal one then our aim will be to minimize the binary cross entropy
2164:41 - loss since the values we could get will range between zero and positive infinity unlike here
2164:48 - where the values were ranging from negative infinity to zero so here we was maximization
2164:54 - problem we had gradient ascent but here we're trying to minimize this instead so we minimize
2165:02 - this because we want to obtain a zero since it's when our output is zero that or is when our y
2165:10 - shuffle is equal one that we get this output of zero so that said we have it for this first part
2165:16 - for the next part when we're dealing with our data our fake data which has been generated by
2165:21 - our generator we expect d to be equal to zero and since our y in this case is equal to zero
2165:28 - since we have y to be equal to zero then this term is taken off and we're left with only this term
2165:34 - now for this term we would have one minus the expected value of d which is zero that's one
2165:41 - log one minus y-shuffle where y-shuffle is what the model predicts so in the case where we have
2165:50 - this y-shuffle to be equal zero as expected then we would have log of zero and that will give us
2165:57 - zero so we would have a zero here now in another case or in the other case where our model instead
2166:05 - predicts one year if the model instead predicts one then we'll have log of one minus one which
2166:11 - is going to be zero log of zero is negative infinity this negative here turns out to positive
2166:17 - infinity and so now we have positive infinity and since our aim is to obtain this y-shuffle
2166:23 - to be zero it means that we'll minimize this expression here such that it takes the value of
2166:32 - zero over the with the aim of having a value of zero and again unlike this original expression
2166:40 - where we went from negative infinity to zero now we're going from zero to positive infinity
2166:44 - and so this means that when working with the discriminator and making use of the binary cross
2166:52 - entropy we would go through a simple gradient descent where we will minimize the loss from here
2167:00 - we will move on to the generator for the generator well we expect d to produce a one right here so
2167:07 - we expect them to have one year and this means that this expression is taken off since we have
2167:13 - one minus one which is zero so here we left only with this now we left with one bit is one so it's
2167:20 - just log log of y-shuffle which is what the model predict or what this d the discriminator would
2167:26 - predict and so we have negative anyway let's just say negative let's take up meet the one on n and
2167:32 - the sum so we have this expression right here and then in the case where the model actually predicts
2167:39 - a one so the model is predicted to predict a one and it actually predicts a one in that case we
2167:43 - would have a zero in the case where the model predicts a zero when when it's supposed to predict
2167:50 - a one in that case we will have a log of zero that is negative infinity negative negative
2167:54 - infinity is positive infinity and again here our aim is to obtain uh this zero right here so we
2168:02 - will again be minimizing our bce loss now given that we'll be implementing the model or the
2168:11 - architecture in this paper that is this again paper we should note some of these guidelines
2168:18 - here we told replace any pulling layers with strided convolutions so instead of using pulling
2168:23 - we use strided convolutions and fractional strided convolutions for the generator
2168:27 - um the next use batch norm in both the generator and the discriminator then remove fully connected
2168:34 - hidden layers for deeper uh architectures basically here we're using the convolutional
2168:40 - layers instead of the fully connected hidden layers uh use relu activations in generator for
2168:47 - all layers except for the output which uses the tanh activation then use leaky relu activation
2168:54 - in the discriminator for all layers but before we move on to look at some of these details
2168:59 - we should note that there is this github repo here by one of the authors authors of the paper
2169:06 - that's um there we go let's get up here by Sumit Shintala and what he proposes here is this um
2169:18 - list of tips and tricks used to make GANs work now as we said already it's it's not that evident
2169:25 - to make GANs work so uh taking advantage of the experience from one of the authors of the original
2169:34 - DC or of the DC GAN paper not the original GAN paper will be very interesting for us since we
2169:40 - will not uh get to make the same mistakes which maybe he had made uh before discovering all those
2169:47 - different tricks now that said we have here the very first one you should note that this
2169:53 - list is no longer maintained i'm not sure how relevant it is in 2020 so this is this has been
2169:58 - for a while uh six years six years anyways we have uh normalized the first one normalized the input
2170:05 - so normalized the images between negative one and one then use tanh as the last layer of the generator
2170:10 - again this was already in the paper the next tip will be to modify the loss function now it should
2170:16 - be noted that we've already explained this but maybe the transition from this previous loss
2170:21 - function to this other loss function wasn't made very clear now let's get back to that call for
2170:27 - our loss function we had um we had a log of one minus d of g of z this for the generator remember
2170:39 - so we had only this year only this expression in the original GAN paper they expected the
2170:45 - discriminator to produce a zero although previously we had mentioned that we expect the discriminator
2170:50 - to produce a one right from the beginning we've been speaking of this but in the original paper
2170:56 - what they actually wanted was the discriminator to produce a zero for the generator before we go on
2171:03 - to look at how to compute this BCE loss we are going to take into consideration this modification
2171:12 - on this original loss right here and this modification comes in because of the following
2171:18 - problem when the training just starts it's difficult for this discriminator to output
2171:25 - a one unlike here where it's easier for it to output a zero when it sees fake data because
2171:31 - remember let's get back to this and uh let's restart here let's restart this and let's pick
2171:40 - another distribution oh let's let's pick this one which is slightly more complicated so when you
2171:46 - just start with the training the the the generated outputs you see uh the generated outputs do not
2171:52 - look very much like the real data you see this year it's very different from this and so because
2171:59 - of this great difference at the start of the training it's difficult for us to make the
2172:04 - generator fool the discriminator to output a one right here and also because of the fact that
2172:11 - classifying whether an input is real or not is easier than generating new inputs the generator
2172:18 - here will experience vanishing gradients and so instead of as we've seen already trying to minimize
2172:25 - this we can instead maximize the sum of log dg of z so instead of log one minus dg of z we're going
2172:33 - to have log dg of z the reason why is preferable for us to use this expression where we maximizing
2172:42 - this year instead of minimizing this other expression is simply because when we make use
2172:48 - of this expression we are being more lenient on the generator at the beginning of the training
2172:56 - especially so when we were minimizing the log of one minus dg of z we had levels or if we had to
2173:05 - make use of the binary cross entropy loss we would have levels which are equal zero and so if our
2173:14 - level equals zero obviously this expression is left out and we left only with this part now we
2173:19 - left with y equals zero and then we have log of one minus y just similar to what we have here
2173:24 - but then the fact that at the very beginning we are expecting our discriminator to output
2173:32 - y equals zero when it sees fake data from the generator is a problem because it's a very easy
2173:40 - task for the generator for the discriminator especially because right here or at the very
2173:46 - beginning this year is obviously not going to look like the real data and so the discriminator will
2173:53 - find it very easy in predicting that this is fake data and because of this ease the generator's
2174:02 - weights wouldn't be updated any further to ensure that we could produce even more realistic looking
2174:10 - fake images and so what we do is we flip the levels and flipping the levels matches with this
2174:17 - expression that is let's take this off we are having now y equal one so instead of y equals zero
2174:28 - we now have y equal one so we're expecting the discriminator to output one when it sees
2174:34 - fake data from the generator now doing this we have y equal one so obviously this is left out
2174:40 - you see that this now matches with maximizing this expression and so when y equal one now
2174:46 - we are telling the discriminator to output a one year when it sees fake data and now this will
2174:53 - permit the generator to be able to update its weights and so make the training much more stable
2175:00 - as compared to when we're dealing with or working with labels yi which are equal to zero you should
2175:08 - also note that when we're talking about levels here we're talking about this yi's and when we're
2175:12 - talking about what the model predicts we're talking about this y hats or this y shappels
2175:17 - our next tip is one which we've discussed already so we told in the GAN paper is the last function
2175:23 - to optimize g is the minimization of log one minus d but in practice folks practically use
2175:30 - max of log d this uh what we have seen already where we had the minimization of
2175:37 - one minus log d g of z so we had um log sorry log of one minus d g of z right here so we minimize
2175:53 - this expression and then now instead of this we maximizing um the log of d g of z and the reason
2176:05 - why we prefer to work this way as we've said already is because if you are expecting the
2176:11 - discriminator to take in some fake data from here at the beginning and say that it is fake then
2176:22 - this is going to be a very easy task especially at the beginning since the fake data here is
2176:29 - going to look very much different from the real data and so because we our aim here is to make
2176:36 - this discriminator output zero is going to be difficult for the generator to update its
2176:42 - parameters such that the discriminator can start uh getting fooled and so instead of this as we've
2176:49 - said already we flip the levels and uh instead aim for the discriminator to output once
2176:57 - for the next tip we said uh they say don't sample from a uniform distribution so for the noise
2177:06 - here for a generator noise we're going to sample from a Gaussian distribution or normal distribution
2177:13 - okay so from there we're encouraged to use batch normalization avoid sparse gradients
2177:20 - and unlike in the paper if we get back to the DCGAN paper and we check let's get to this we check
2177:27 - here we we told use leaky relu activation in the discriminator for all layers and then use
2177:33 - relu activation in the generator for all layers but what the say is uh the leaky relu is good
2177:40 - in both the g and the d that's both the generator and the discriminator then one point you should
2177:47 - notice the fact that the stability of the GAN game suffers if you have space sparse gradients
2177:52 - now what does it mean by this uh if you have a relu activation if you have some input
2177:58 - all negative inputs are sent to zero and all positives remain the same then for the leaky
2178:06 - relu we have all negatives we should take up some value depending on what we pick the value to be
2178:14 - if the value is 0.2 for example then uh an input of negative one will give us an output of negative
2178:21 - 0.2 uh and an input of negative two will give us for example uh negative 0.4 an input of say
2178:31 - whatever value stems negative 0.2 to get output now that said the this the positive section
2178:38 - remains the same but now when we talk about those sparse gradients here it comes due to the fact
2178:44 - that if we have relus in our network then we'll tend to have many zeros and the zeros uh or will
2178:54 - cause this sparsity in the gradient and so because we do not want to have this and because we want to
2179:01 - train the GANs in the most stable manner then we'll make use of the leaky relu
2179:07 - then for down sampling we're told to use average pooling or conv2d and strides
2179:15 - then for up sampling you speak the shuffle conv transpose convolutional transpose 2d
2179:21 - with strides okay then from here we're told to use soft and noisy levels so what does this mean
2179:30 - this means that if we have a discriminator and let's say we're training our discriminator
2179:35 - where we have an input from a generator we train now we update the parameters of the generator
2179:44 - where we pass in our fake data from this output of the generator into our discriminator and the
2179:51 - discriminator has to compare the the output from the model let's let's put the output from the
2179:57 - model in this color let's say the output from the model is 0.4 for example so what we'll be
2180:02 - comparing will be this correct level with the model's prediction you see that now what the
2180:10 - sayers apply level smoothing so instead of taking one we could take a random value around one so
2180:18 - instead of this we could take for example say 1.2 or 0.8 or 0.9 or whatever value just around one so
2180:29 - instead of having hard levels just like some strict levels either we have zero or we have one
2180:35 - would take values around so around exchange the color will take values around zero
2180:43 - and then values around one you see that so we use smooth leveling instead of some hard
2180:49 - leveling then we also total make the level make the levels the noisy make the levels noisy
2181:01 - for the discriminator that's occasionally flip the levels when training the discriminator
2181:06 - okay so that's fine now next use this again when you can it works so if you
2181:11 - if you can use this again and the model table use hybrid model
2181:14 - like for example the VAE and GAN now here we have been told that if we are to generate images
2181:23 - we should not use the original GANs that we should not use the simple neural networks
2181:30 - that are fully connected neural networks we should go in for convolutional neural networks
2181:36 - okay the next is to use stability tricks from reinforcement learning now we we get to treat
2181:42 - your reinforcement learning so we're going to skip out this now from here use the Adam optimizer
2181:47 - optimum Adam rules so the after Adam optimizer rules and then track failures early so if you
2181:54 - want to be able to train your GANs without maybe upgrading at the end that you you haven't
2182:03 - or you you can get the kind of results we expected you should try to ensure that you
2182:09 - make sure your GAN isn't doing any one of this right here so if you're training and you and your
2182:14 - loss the loss of the discriminator goes to zero then it's failure mode because your discriminator
2182:22 - is proving to be too good at doing its job and then if the norms of the gradients are over
2182:28 - a hundred things are screwing up when things are working the loss has low variance and goes down
2182:36 - over time versus having huge variance and spiking so what the same year is if we have this the loss
2182:44 - for the discriminator remember we have the discriminator on the generator what we expect
2182:49 - to have is something like this which should go down slowly over time instead of having this kind of
2182:58 - high variance and spiking now if the loss of the generator is steadily decreases then it's
2183:04 - fooling the D that's a discriminator with garbage and so this means that we do not expect the
2183:10 - generator to be so good that during training its loss just drops steadily and so as we said already
2183:19 - you should track all those failures early on now the next we have don't balance loss via statistics
2183:26 - unless you have a good reason to now oh they say they've tried it's hard and they've tried it all
2183:32 - um let's take this off so what they're saying is when try to balance the training of the generator
2183:39 - and the discriminator based on some loss value so if you are to try this you should have a
2183:48 - principled approach to it rather than just intuition now if you have levels use them now
2183:53 - talking about levels this means that if you have say we have this our discriminator and then we
2184:00 - have our real data and then you have maybe some data set of fake data then you could train your
2184:08 - discriminator like the usual classifier in supervised learning then the next point is to
2184:15 - add noise to the inputs and then decay over time from here we have this tips where they're actually
2184:22 - not sure so well we may just keep them actually uh this is for conditional gains we'll not take
2184:29 - this into consideration use dropouts and g in both training test phase so provide noise in the
2184:34 - form of dropout as this generally leads to better results much thanks to the authors smith emily
2184:42 - martin and michelle then apart from the vanishing gradient uh problem another very common problem
2184:48 - would be that of mode collapse where the generator produces output or produces the same outputs even
2184:56 - after training for several epochs and so now we're going to start with building all this again
2185:03 - while taking into consideration the tips and tricks which we've just seen
2185:10 - hi there and welcome to the session in which we shall practically train again to produce images
2185:17 - like this one here we'll start with the imports and then we'll move on to prepare our data the
2185:22 - data set we shall be using will be a celeb a data set which signifies celeb faces attributes data
2185:28 - set now this is over 200 000 images of celebrities with 40 binary attribute annotations let's uh
2185:36 - open up some of this here you could have some of these images from this file up and there we go
2185:43 - you see we have this faces right here and so what we'll be doing will be to train our discriminator
2185:51 - alongside with our generator such that our generator can generate image of faces which can
2185:59 - be able or which can be realistic enough to be able to fool the discriminator to think that
2186:04 - they are actually real faces this notebook is provided by Jessica Lee on Kegel and can be
2186:11 - downloaded so let's uh go straight away to download this data set and then start with uh
2186:17 - uh DCGAN modeling in order to download a data set from Kegel we'll be needing this Kegel.json file
2186:24 - right here now this Kegel.json file can be gotten from Kegel by getting to your account and then
2186:31 - creating a new API token so once you have that you'll get right here and then click on copy API
2186:38 - command which when you paste out here you see you have Kegel data set download and you have the
2186:44 - the the user name of the person who uploaded this data set to Kegel platform and then you have the
2186:50 - data set name right here but before carrying out this data set download we'll start by installing
2186:54 - Kegel we'll make this directory we'll copy this Kegel.json into this directory then we can now go
2187:02 - ahead and download the data set from this command API command which we downloaded or which we copied
2187:09 - rather and then we can now unzip this into some data set folder or directory which we specify so
2187:15 - that said let's simply run the cell and everything should move on well let's take this off as you can
2187:22 - see the data set has been downloaded and now we're extracting the files into this data set folder
2187:29 - right here now that we have this successfully extracted into our data set folder as we could
2187:33 - see we specify the batch size the image shape and the learning rate now from here we let's run the
2187:40 - cell and then we move on to create our tensorflow data data set so here we have let's call this data
2187:49 - set and then we specify this path now to get this path you could click open right here and then you
2187:56 - see if you click open this it's going to take a while since we have 200 000 of different images
2188:01 - here so let's just let that and then we copy this path there's a path that you specify in here
2188:07 - and once you specify that you have you have a labeling mode which is known we have the image
2188:14 - size which was specified already we have the batch size anyway let's let we could have your
2188:24 - batch size but it doesn't matter as we'll see shortly anyway we have that and then from here
2188:30 - we run this cell you see you could have data set we're getting an error tensorflow not defined
2188:37 - let's run this oops um that's fine um next this we've run this already and then we run this
2188:47 - now this should be fine let's check out our data set it should use that so as we can see we have
2188:52 - 202 599 files belonging to one class and our data set has been bashed so we have a batch data set
2189:00 - you can see the shape right here we have 64 by 64 by 3 images now the default is um 256 by 256 so
2189:10 - if we if we do not specify this let's see what we get let's take this off take that off and run
2189:18 - this again and check out on the image size you see here when we don't specify anything you have
2189:25 - 256 by 256 okay let's get back and run this again then we're moving to process our data
2189:34 - so right here what we're going to do is we're going to make sure this data lies between negative
2189:39 - one and one and so that's why we have in here the image divided by 127.5 minus one and so this
2189:47 - means that any value we get between zero and 255 let's say for example we have the value 255
2189:54 - we'll take 255 divided by 127.5 which is two then minus one which gives us one so that's how we
2190:05 - we pre-process these images and then after pre-processing we're going to un-batch because
2190:11 - we need to reshuffle or because we need to drop the remainder so we un-batch and then we use the
2190:18 - batching of our tensorflow data i then from here we carry out some prefetching for a more efficient
2190:25 - way of loading the data now from here you can visualize a single element in our data set there
2190:31 - we go we have 4d in our train data set let's take a single element we could print out its shape
2190:39 - the shape and there we go now we have this shape which is 128 by 64 by 64 by 3 as expected
2190:48 - and we could go ahead and visualize some elements here so let's visualize four elements
2190:56 - we could increase this definitely so we should visualize four elements of this for now now here
2191:01 - we have the subplot um plot image and we could take off the axis so let's run that and then see
2191:11 - what we get see here we have this um four different images let's reduce this a little
2191:19 - now uh one thing we could do too is modify this here this um value of our array now the reason
2191:27 - we want to modify this is because this value ranges between negative one and one whereas this
2191:34 - plot that image takes in values of range zero to one so we're going to modify this so we move
2191:40 - from negative one one to the range zero one and to do that we need to take whatever value we have
2191:47 - in this range add one to it and then divide by two so let's take this off let me get back here
2191:55 - we just have plus one then divided by two there we go let's run that again and there we go you see
2192:04 - you have now the images are much clearer and you you do not get the messages which we're getting
2192:11 - previously now we'll go ahead with the modeling and we're going to use this same architecture
2192:17 - presented in the dis again paper so right here we have this 100 dimensional latent vector
2192:25 - and then this is projected and reshaped into this four by four by 1024 tensor and then from here
2192:34 - we apply the up sampling that's actually the conf 2d transpose to then get this other vector
2192:42 - right here notice how we're getting from four by four to eight by eight and then from here again
2192:48 - repeat the same process 16 by 16 32 by 32 and then finally 64 by 64 also notice that while the
2192:57 - size of the outputs keep increasing from eight six from eight to 16 32 64 the depth is reduced and
2193:04 - so we'll go from 1024 to 512 to 256 to 128 and finally we have three so we get back to the code
2193:13 - and we specify our latent dimension which is equal to 100 let's rerun this cell right here
2193:21 - there we go and then that should be fine and then we go ahead and build our generator so here we
2193:30 - have our generator or which we'll build with a sequential model we have tf keras sequential
2193:37 - model and then we start to pass in our different layers so here we have our input layer input
2193:47 - which has a shape of the latent dime or latent dimension so here we have latent dimension
2193:57 - there we go and that's fine so that's our first layer and then the next is our dense layer so
2194:02 - this is our projection so we project this such that the output is having four times four times
2194:13 - the latent deem number of out units so just as we had seen in the paper now once we have this
2194:22 - we move to the next layer which is going to be the reshape layer so we go ahead and reshape
2194:28 - such that because at this point we're having latent dim is 100 so we're having 16 times
2194:34 - 100 at 16 100 outputs now we reshape this such that it is a three-dimensional tensor so here
2194:43 - we have four by four by latent latent deem and that's it there we go so this is our next layer
2194:53 - reshape from the reshape we go ahead to do the conv2d or the up sampling with the conv2d transpose
2195:02 - as is the paper we have conv2d transpose and then we have 512
2195:11 - number of filters represent the number of output channels then the kernel size
2195:17 - kernel size equal four now if we get back to the paper here let's get back to the paper you'll see
2195:26 - that the kernel size isn't necessarily exactly equal four but one very important rule to follow
2195:34 - when picking out the kernel size is that the kernel size has to be divisible by the number of
2195:40 - strides so when we pick kernel size equal four we could have the strides to be equal to and the
2195:49 - reason why we generally want that the kernel size is divisible by the number of strides is simply
2195:56 - because of the quality of outputs will get generated by the generator when this isn't the case
2196:04 - so always ensure that we have the kernel size divisible by number of strides now from here on
2196:12 - we go on to apply batch normalization as suggested in the paper and also in the
2196:20 - tips and tricks github repo so here we have batch norm and then from after the batch norm
2196:28 - we have our leaky relu now for the leaky relu we have it takes in value of 0.2 so here we have
2196:37 - 0.2 and that's it for this first part so we have this first block here which you could see in the
2196:44 - paper this very first conv layer now once we have this here it could be repeated again so we just
2196:52 - um copy this and then paste it out but modifying this depth so here we have 256 and then again
2197:00 - oops here get back um and then here we paste this out and then here finally we have 128
2197:13 - okay so we have that uh for now we're not going to apply any dropout you could always feel free
2197:18 - to apply that and see um the kind of results you will get so here we have this and then uh now we
2197:25 - have that we we already set that's we have the let's get to the paper we have the first the second
2197:31 - and the third conv layer now this final conv layer is to get an output which is like an image so we
2197:37 - have 64 by 64 by three and so let's get back here um uh we will have no leaky relu or whatever like
2197:46 - that just copy this out and this is out here okay so we have that and now we just come to the
2197:54 - transpose we have um activation which is a tang so after the strikes here we specify the activation
2198:04 - activation and this activation is uh um tang activation so just have tang and that's it
2198:13 - pattern um equal same we'll also copy this out here so here we have pattern same and right here
2198:22 - we have pattern same okay so that's it uh that should be for the generator i guess we've
2198:29 - respected what we had in the tip and tricks and also um right here let's see we're told use
2198:37 - relu activation and generator for all layers except for the output which is the tang we'll
2198:41 - call this model the generator so we have our generator model and then let's run that and
2198:49 - the next thing we want to do is summarize this so let's get a summary we check this out here
2198:57 - and you see how we get in this year instead of three so let's go ahead and modify this right here
2199:03 - uh this should be three let's run that again and get the summary okay so that's what we have
2199:13 - then now we can move ahead to our discriminator so instead of generator right here we have
2199:20 - this screaminator and the input we're gonna have here is gonna be 64 by 64 by three so
2199:29 - it's actually in shape so in shape um the index in shape there we go by three and then
2199:41 - instead of the conf 2d uh transpose layers we'll be using the conf 2d layer so here we have conf 2d
2199:49 - and then the the depth increases instead here instead of decreasing as we had with the
2199:54 - generator so here we go from 64 so we start with 64 and then we move on to 128 and so on and so
2200:02 - forth for now let's take this off since we're dealing with a conf 2d and then again we have
2200:08 - the kernel size which is divisible by the number of strides we have the leaky relu as we've seen
2200:13 - already in the tips and tricks um take this off here but we'll make use of the batch num still
2200:18 - so let's let's place this out and then we have batch batch normalization we have the batch
2200:26 - normalization there we go um here is conf 2d but we increase the depth so we have 128
2200:36 - now that we have this depth increased we just simply copy this out and paste it over the next
2200:43 - layers so uh for the next blocks because we consider this would be a block and this a block
2200:48 - and this a block um now we move on to 256 batch norm leaky relu still and that's it now for the
2200:58 - final or for the last conf layer let's just paste this out here let's take this off uh we have this
2201:06 - last conf layer right here we'll give it a depth of one and then given that our discriminator
2201:13 - call that our discriminator is a usual classifier which takes in the 64 by 64 by 3 input and then
2201:22 - outputs a single value whether one or zero or a value between zero and one actually so you
2201:28 - output a single value here so at this point we should be thinking of using some dense layer and
2201:34 - then specifying that its output is going to have only one unit okay so let's take this up and then
2201:42 - now from here we could um flatten so we flatten um what we get is output from the conf 2d layer
2201:51 - and then after flattening we could uh have our dense layer one see here we have just one output
2201:59 - and then uh the activation activation is sigmoid so that's it recall with a sigmoid
2202:10 - with a sigmoid we have values or our inputs from negative infinity to positive infinity
2202:16 - which have been mapped in the range zero zero to one actually and that's exactly what we need
2202:23 - right here so we have that um that's fine let's take this off now now before we move on it should
2202:31 - be noted that unlike the sigmoid which maps values between zero and one the tense function
2202:38 - maps values between negative one and zero so this is from zero to one and then the tense
2202:46 - maps values between negative one and one and that's what we use in the final layer for the
2202:54 - for the discriminate for the generator and now for the discriminator we're using the sigmoid
2203:00 - okay so we have that understood now we have here our discriminator okay so we have that
2203:08 - let's run this cell and then finally we're gonna have our summary discriminator summary let's run
2203:17 - this and then see what we get there we go we have our summary um everything looks fine and now we
2203:24 - could go ahead start with our training and just like we had done previously we're gonna overwrite
2203:31 - the training step so here we have a VAE model which we had built previously where we override
2203:38 - at this train step right here and with this we're able to make use of methods like the model.fit
2203:47 - so here instead of the VAE we have again let's get back we have again model this again model
2203:57 - is made of a discriminator discriminator and a generator so let's replace the encoder and
2204:04 - decoder by the discriminator and the generator respectively then here we have our discriminator
2204:11 - discriminator and our generator self generator and self discriminator we can modify the compile
2204:26 - method let's modify this compile method the compile method actually will take in the optimizer
2204:35 - for the discriminator the optimizer for the generator and then the loss function so we have
2204:42 - uh the optimizer let's say the optimizer g optimizer and then the loss function so that's our
2204:54 - compiler method and then we also go ahead and define our discriminator loss metric and our
2205:02 - generator loss metric we've taken all these three and then we've put this out here okay so that's
2205:10 - it now let's have our d loss metric and our g loss metric and then from here we move on now to
2205:22 - the training step for the training let's recall that we have a discriminator we have a generator
2205:30 - and then this generator takes in a fake data here takes in a vector fake this uh this noise here
2205:38 - and then generates fake data so takes noise our g takes noise generates fake data and then this
2205:46 - fake data is then passed onto the discriminator which says whether it's a one or zero or gives
2205:51 - the value between zero and one okay uh we also have our real data right here which is also going
2206:00 - to be passed into our discriminator and it's also going to give a value between zero and one now we
2206:05 - should be noted that we'll start with training the discriminator and when you're training the
2206:10 - discriminator we're going to freeze the generator that's we do not update its parameters so we're
2206:16 - going to just update the weights of the discriminator just like we had seen previously
2206:23 - so the first thing we want to do here is to get our noise the noise is tf random normal
2206:32 - so we have normal and then we specify since we have a normal distribution we want to specify
2206:39 - its shape now the shape of this here would be um the shape will be our latent deem now let's
2206:48 - let's have our latent deem we should have defined already so here we'll have latent dimension now
2206:55 - given that we'll be working in batches we need to add the batch dimension so here we have batch
2207:02 - size by latent deem now to obtain the batch size all we need to do here batch size is equal
2207:10 - tf dot shape of our x batch x batch and then we get the zero value now from here we have the
2207:22 - batch size we have the noise and then we're ready to fit this into a generator and then obtain the
2207:29 - fake data then also make use of the real data which is basically this year because this is our
2207:34 - real data remember our data set is made of 200 000 different images or faces of celebrities and
2207:42 - what we've done is we've broken this up into batches of 128 so for every batch we're going
2207:50 - to take this x batch here which is basically the real data and then we're also going to
2207:56 - use make use of this noise generated fake data and then train our discriminator
2208:02 - and so with that we have our fake data or let's say fake images fake images equals self
2208:10 - generator which takes in the noise or let's just say some random noise here we have random noise
2208:23 - random noise vector actually okay so we have this now we have our fake images that we have this year
2208:30 - now the next and we also have the real so we can now dive into training a discriminator
2208:35 - now as you may know that the discriminator's last function will take in the output from here
2208:44 - that's output from the real data and compare it with one and then taking the output from those
2208:51 - fakes and compare it with zero so we'll take the output from the real let's call this r
2208:58 - and compare with one and then we take the output from the fakes let's call it f
2209:02 - and then compare it with zero so getting back into the code right here let's change this let's call
2209:09 - this real images and here we have real images real images there we go so here we want to have
2209:21 - the predicted output or better still let's say real predictions real predictions so you have
2209:29 - our real predictions here now the real predictions are gotten from taking in our discriminator
2209:36 - discriminator and then this discriminator actually takes in real images so we're presenting what's
2209:45 - going on right here so we have this real which gets into our discriminator and then we output
2209:52 - the real predictions which would then compare with the value one so that's it we can now go ahead and
2209:59 - compute the loss so we'll have discriminator loss for real is equal our last function which we're
2210:08 - going to pass in and then this last function takes in the real predictions and uh once so
2210:17 - oh yeah we will have both the real predictions and the real levels uh the real levels as we've
2210:26 - said already are the ones so it's basically this one year since we we haven't batches of
2210:33 - images we we have several ones of size the batch size so here we have real
2210:39 - um we there we go we have real um levels see tf ones and then we specify its size
2210:52 - or better still its shape so here we have the the shape which is batch size by one see that
2211:01 - batch size by one and the reason why we have batch size by one is simply because we have
2211:06 - an output which takes in just one single value while this output will all for the real level
2211:14 - will be equal one and we have the batch size okay so we have that now the next thing we want to have
2211:21 - here are the fake levels now this fake levels is going to be this zeros right here so we have
2211:30 - uh sorry we have zeros and then we have batch size and one okay so that's it now we we have
2211:40 - our real levels and we have our fake levels and we will take the real levels we'll take
2211:45 - the real predictions that is we take we we get what the model thinks about a particular input
2211:52 - that's all the classification and then compare it with the real levels the real levels is
2211:57 - once because we expect that the model should take in a real input and then know that it
2212:05 - is a real input and that and that means that it should output a one when it takes in a real data
2212:10 - and if it outputs a value different from one then the loss is going to be greater than zero
2212:16 - whereas if it's exactly equal one the loss is going to be zero and our m years minimizes loss
2212:21 - now we have that and the next thing we want to do is repeat this but this time around for
2212:28 - the fake predictions so here we have fake predictions now the the first step is
2212:36 - we have real data getting into the discriminator and the other we have fake data getting into
2212:41 - the discriminator so here we have fake predictions and this time around it doesn't just take the real
2212:47 - images but it takes the generated images so we have self generator and it takes in
2212:56 - noise actually so we should take in a random noise which is this one right here so it takes
2213:04 - some random noise and then it outputs fake images but since we'll define this already here
2213:12 - which we could just make use of it here so let here we have fake images and there we go
2213:19 - so here we have the discriminator which takes in a fake image and then gives us a fake prediction
2213:25 - and we're going to compare this fake prediction with zero so we expect that the the fake
2213:30 - predictions should be zero if not the loss isn't going to be equal to zero so here instead of real
2213:36 - levels we have fake levels so we comparing zero with what the model is going to predict
2213:44 - or the output of the discriminator oh yeah we have discriminator fake we have here fake
2213:52 - that's it i think that's okay we the same loss function actually is a binary cross entropy loss
2213:58 - and once we have this now we could have could define the loss to be equal the loss real plus
2214:07 - the loss fake since that's basically a combination of this two losses right here
2214:16 - now before you move on you could recall in the tips and tricks we saw the level smoothing now
2214:23 - here we have our levels our real and our fake levels let's separate this and now what we're
2214:29 - going to do is instead of taking a one we'll take values around one so we take we add plus
2214:37 - 0.25 times some random value between negative one and one so basically what we're saying here is
2214:47 - we want to take this one and then add it plus a value in the range of negative 0.25 and 25
2214:58 - on 0.25 so this means that now instead of having the level to be fixed at one we would have the
2215:05 - level to be between because one minus 0.25 is 0.75 so we between 0.75 and 1.25 instead of just one
2215:15 - so that's how what our level will be now and then for the zeros since we we don't want to have
2215:20 - negative values we'll take the zero plus some random value between zero and 0.25 so instead
2215:29 - of zero we'll have some random value between zero and 0.25 okay so that said what we'll do now is
2215:38 - we'll get right here we have tf random uniform and then we specify the mean
2215:45 - vowel which is negative one and then the max vowel which is one now specifying this means
2215:52 - we're going from negative one to one and then multiplying by 0.25 means we're going from
2215:56 - negative 0.25 to 0.25 so that's basically it and then also we specify its shape so we have
2216:05 - um the batch size by one there we go now we're just going to copy this out and then paste this
2216:14 - right here so we don't want to have negative numbers so we start from zero instead so here
2216:19 - we have zero to one but by default the values are already from zero to one so we could take
2216:25 - this off okay so that's it everything looks fine i think everything is done for our discriminator
2216:35 - now we have in this let's take this off we have our loss and that's it okay so that's
2216:43 - basically it we now move on to our partial derivatives here we take in our d loss
2216:49 - and then we are going to update um the discriminator so here we have this screen
2216:57 - this screen meter dot trainable weights then for the optimizer is the optimizer which we have
2217:07 - specified already right here so here we have instead of this optimizer we have our d optimizer
2217:15 - which we're going to specify so we have the optimizer uh takes in the partial derivatives
2217:22 - and our trainable weights again here we are training only the discriminator so we have
2217:28 - discrete meter that's it so here we go we have our model our again the discriminator the trainable
2217:36 - weights and we repeat the same process here now uh this is self okay so that's it uh this should
2217:45 - be fine now and then next step we do is we're going to do this same but for the generator
2217:52 - so what we'll do now is we're going to again sample some noise random noise here we copy
2217:59 - this code out and then paste it out after this okay there we go so we have this random noise right here
2218:08 - random noise and then from this random noise we have our fake images um cell generator
2218:23 - and it takes in the random noise okay now we're gonna have the same again so we're gonna
2218:31 - uh make use of the gradient tape as we've done already with the discriminator so paste that out
2218:39 - here and then here we have we'll be working instead with a generator um this is discriminator
2218:46 - discriminate let's go back discriminator okay so as we've said this is let's let's just comment
2218:53 - this let's find out generator and then right here we have the discrete discriminator okay
2219:08 - so that's a discriminator and now for the generator so as we were saying we have those
2219:14 - fake images we have our random noise and we make use of the random noise to generate the fake image
2219:20 - but this time around we want to follow the discriminator so instead of expecting our fake
2219:26 - levels to be zeros as we had here this time around our fake levels will be ones and we'll
2219:33 - obviously not have anything to do with the real data so let's take this off and then get back
2219:38 - here now we have the fake levels which which is equal one and it happens that they have actually
2219:46 - been flipped so let's let's um get down here and then paste this out here so here we have
2219:55 - flipped flipped fake levels which are actually ones instead of zeros remember we've seen this
2220:02 - already so we have that and we're not going to do any level smoothing right here so we have that
2220:09 - and the next thing we want to do is start with our recording of the gradients now yeah let's take
2220:19 - this off we have our fake predictions we have um yeah we go discriminator takes in the fake images
2220:27 - and that's it now here we have the flipped flipped fake levels and then we have our fake predictions
2220:35 - fake predictions i guess if we we should have had we should have made an error here this is actually
2220:41 - fake fake predictions we're comparing the fake predictions with the fake levels and then here
2220:46 - we're comparing the fake predictions with the flipped fake levels so that's it um that should
2220:54 - be fine here we have g so this is our g loss g loss that's not like g loss fake or g loss real
2221:02 - we just have g loss and that should be it partial derivatives self generator that's it
2221:11 - radar and then here we have updated our generator so we're not making we're not updating the
2221:17 - parameters of the discriminator here now here we have g optimizer that's it okay so um if that's
2221:25 - okay we have now to update the different states so we have d loss metric we update the state and
2221:35 - we pass in the d loss and then we repeat the same for the g loss that's the generator loss
2221:42 - so here we have g and then here we have g then for loss we have g loss g loss metric result metric
2221:56 - that's it and then here we have d loss self d loss metric and then result okay
2222:15 - let's now run the cell and normally everything should work fine now we move on to define the
2222:22 - number of epochs so we're gonna work for 20 epochs and then let's get back here
2222:29 - we define our gain which is those gain we've just defined and then takes in the discriminator
2222:38 - and the generator which we defined already uh from here we go ahead and compile the model
2222:45 - so we have again compile and then we specify the optimizer the d optimizer optimizer
2222:54 - which is uh the atom optimizer optimizers and then atom now this atom optimizer will be
2223:04 - or with a learning rate learning rate which was specified already at the beginning learning rate
2223:10 - equal two times standard negative four which is specified at the beginning so we have your
2223:17 - learning rate and then beta one beta one equal zero point five now we'll put the same for the
2223:25 - generator let's get back here copy that and then paste it out here so this is for our generator
2223:33 - now now you notice that there's no there's no major difference actually there's no difference
2223:37 - we just use the same optimizer now we have that the next thing we want to do is pass in our loss
2223:44 - function so we have your loss function which is equal our binary cross entropy loss so we have
2223:54 - losses dot or binary cross entropy and that's it so we have this set we could run this now and then
2224:06 - go ahead to train our model by calling on gain dot fit method or just model of fit method
2224:14 - so we have history equal gain dot fit and then we pass in our train data set we'll start with say
2224:25 - 10 or let's let's take just 100 elements first and then here the number of epochs equal epochs
2224:33 - we should define already and then we have some callback so will you make use of this callback
2224:39 - and you already see the advantage of overriding the train step method as now we could just
2224:46 - define our callback and then pass it in here and the job is done so we will have this callback
2224:53 - which is going to show us the generated images at the end of an epoch so let's call it show image
2225:01 - and it's going to take in latent deem okay so that's it everything looks fine now the next
2225:10 - thing we want to do is define this show image callback right here now here let's let's define
2225:17 - this callback just above here so we have our show image callback and then we get the latent dimension
2225:26 - and then we specify on epoch and so at the end of every epoch we are going to run this code right
2225:33 - here now what's going on here is simple we have our model and then we have the generator we take
2225:39 - in some random noise we pass it into our generator and then we try to see what the model is generating
2225:48 - so this means that when we're training or initially we have some output or some fake data generated
2225:56 - by the generator and then after an epoch we want to see what the model is generating as we keep on
2226:03 - training our whole complete model so this that this is very important as we could already be
2226:10 - able to debug and understand what's going on so this means that in a case where the model is say
2226:18 - for example generating the same kinds of output or generating some outputs which are clearly not the
2226:28 - type of output which we expect to get or whose distribution is very far away from that of the
2226:35 - real data then we will have to take some measures so it's very important to work with these kinds
2226:40 - of callbacks as they already permit us to debug our whole model training process so that said we
2226:49 - just have to specify the figure size here and then what we're doing is we we have in this different
2226:54 - subplots because we see we have 64 we could you could reduce this or it could increase just
2226:59 - depends on you you could just pick whatever you want to pick here so you could generate a certain
2227:03 - number of images here n is equal to 6 so we're generating 6 by 6 that's 36 images so we could
2227:11 - change this to 36 and then now for each and every subplot we're going to show the image you see this
2227:19 - out this outcome from here so it's from the generator and that's it now we're going to save
2227:24 - this figure in some in some directory which we could visualize so let's modify this let's take
2227:33 - here to be 36 um that's fine let's run this we have that show image that's fine um everything
2227:41 - looks fine now let's go ahead and uh start with the train so that we could even reduce this to
2227:48 - just stand so that we could be able to notice any errors quickly and then now train on the full
2227:56 - data set we're getting this error unexpected keyword argument mean vowel this actually mean
2228:04 - let's get back here without the underscore so this is here you have this main vowel and here
2228:12 - we have max vowel now you could feel free to check out the documentation and you should
2228:16 - find the exact syntax so let's run this again and then start with the training
2228:20 - oh getting these arrows well we are told that no no gradients are provided for any variable
2228:31 - and when you look at this you will notice that we have come to the transposes
2228:36 - this means that most probably this arrow is coming from the generator so let's get back here
2228:42 - and make sure everything is okay uh one thing we notice here already is that this should be g
2228:47 - so here should be g and what do we have again here um yeah it looks fine so let's run this again
2228:55 - and then see what we get we still get this arrow which again shows that it's coming from the
2229:02 - generator we get back here and what do we notice we notice here that we we get these fake images
2229:12 - out of the scope of our gradient tape so what we have to do is instead directly call this in here
2229:22 - so we we we want to update the parameters of the generator so it has to be in this
2229:30 - gradient tape scope and not outside so let's take this off take that off and one question you may
2229:38 - be asking yourself is why is it possible to just do this here let's have this out but not in the
2229:44 - generator and the simple answer is for the discriminator this generator isn't updated so
2229:50 - it doesn't matter if it's in here or not whereas for the generator it actually matters so we we
2229:56 - have to make sure it's in here so that's it let's run this again and then see what we get so that's
2230:03 - it training started we told no such file directory generated and all of that anyway let's let's make
2230:09 - this directory generated and then we run this again
2230:18 - there we go didn't seem to be working fine you see that struggling to generate some
2230:23 - images let's open up our generated year you see we have the different files let's check out on
2230:29 - say the 18th output you see this already so you see it's struggling already to produce you see
2230:37 - this image here this one year looks already like a human face though it's still struggling a lot
2230:45 - so we have that let's let's check out on say the 19th there we go okay so that's it let's close
2230:54 - this hopefully there's no issue with connection let's get back up and then if those images aren't
2231:02 - very visible you can retrain and go for or take many more samples so here we're dealing with
2231:10 - 100 we take 100 and then after let's check this out after six epochs you see the kind of results
2231:18 - we get it's the images we get already look like humans now let's modify this code let's stop the
2231:25 - training and then let's modify the code such that we do not flip the levels there we go we could
2231:34 - get back to the flip thick levels and then instead of ones we just take the thick levels so let's
2231:40 - let's take zeros so if we have this we're going to compare it with when we actually do the flip
2231:48 - leveling so let's run this um let's uh run the cells and then we start with the train
2231:58 - after nine epochs we can now check out those generated images let's let's open up the zero
2232:04 - open up the third the feet and then say the eight um let's see what we get here see see this
2232:13 - our generators experiencing vanishing ingredients and that's why we're getting these kinds of
2232:20 - horrible outputs so let's um again stop the training let's stop the training and then let's
2232:27 - get back to our models what we'll do now is instead of using the leaky relu we just use the relu
2232:36 - so let's let's change this activation and set it to relu there we go let's um simply places out
2232:45 - everywhere and then take off the leaky relu and then see the kind of output we would get in case
2232:51 - we we we used the the the relu itself instead of the leaky relu um let's take that off everything
2232:58 - looks fine now let's get back into the generator and then repeat the same so space is out here
2233:04 - relu activation take off leaky relu take off leaky relu and then rerun this again um this
2233:15 - should be fine now let's run that and then see what we get okay now that train has been going on
2233:21 - for a while now we could open up the zero let's say open up the second the feet the seventh um
2233:30 - and then the tent so we could uh look at this now what do you notice you see we're getting
2233:35 - practically nothing as output right here and that's simply because we using the relu instead
2233:42 - of the leaky relu and as you can see sparsity isn't great for generating images so you have
2233:49 - to be very careful with that now let's stop the straining um stop the training and then
2233:55 - uh get back to what we had before um we have this yeah this should be fine uh no discriminator
2234:06 - then right here get back again and oops let's get back then we restart the train again and
2234:16 - then see what we get now that train has been going on for a while let's go ahead and open this up
2234:22 - one there's zero take out two for example five um seven and say eight so let's check out what
2234:32 - the model is outputting see um there we go we can see that with the normal relu it isn't doing
2234:39 - that bad either so in this specific example using the leaky relu isn't maybe oh that necessary
2234:47 - okay so now we've looked at the effect of not using this leaky relu um you could also
2234:54 - or take off the batch norm and see how that affects the kinds of outputs you get from the
2234:59 - generator um here we get an error let's run this again okay that's fine now let's go ahead and uh
2235:08 - restart the training by this time around with a full data set so let's take this off and then
2235:14 - start with the training after training for over 20 epochs here are the kinds of outputs we shall
2235:20 - be getting in our deep learning for image generation course we delve deep into how to
2235:26 - create even much higher quality outputs like this one for example which was created with a diffusion
2235:33 - model or this other one which we created with a program that's it for this section on image
2235:40 - generation with the variational auto encoders and the generative adversarial neural networks
2235:46 - so we've come to the end of this course and if there's one thing we suggest you're doing
2235:52 - is working on as many projects as possible on your own so you could make use of tools like tensorflow
2235:58 - hugging face 1b onyx just to name a few to build and deploy image segmentation models object
2236:07 - content models text detection text recognition depth estimation image search engines pose
2236:14 - estimation face recognition drawsness detection license plate recognition object tracking and
2236:22 - video classification and if you want to take a deep dive into natural language processing
2236:27 - image generation or object detection you could check out those different courses on the neural
2236:34 - learn.ai platform with that said we should very best as you move forward in your career

Cleaned transcript:

Welcome to this course on deep learning for computer vision with TensorFlow. In this course, you'll master deep learning concepts and their applications in computer vision tasks, such as image classification, object detection, and image generation. You'll learn about tensors, variables, and neural networks, including convolutional neural networks through practical projects like predicting car prices and diagnosing malaria. You'll also learn advanced techniques for model performance, data augmentation, and deployment, as well as learn about modern convolutional neural networks, transfer learning, and transformers in vision. This course comes from NeuralLearn, which offers a variety of machine learning courses. Hi, everyone, and welcome to this course on deep learning for computer vision by neurallearn.ai. In this course, we shall make use of tools like Hug and Face, TensorFlow, Onyx, and 1DB to build and deploy different computer vision solutions. Applications of computer vision are everywhere today, going from Tesla selfdriving cars to livestock farmers who can now automatically count the number of animals they have, to mobile facial recognition apps, and even in the hospitals where the gastroenterologists could make use of computer vision for much faster diagnostics. Behind those different computer vision solutions is deep learning models like the convolutional neural networks and the vision transformer. Throughout this course, we shall explain in detail how the confidence of the vision transformers work. Given that these are all deep learningbased models, let's look at a high level how deep learning works. Let's suppose we want to build a system where we get an input like this one, and we are able to say that this is a damaged car, or get an input like this one, and we say that this car is still intact. Then in that case, we could build and train a deep learning model, which takes in this image, for example, and then for its output here, we'll be able to say that this car is damaged. Or in the case where this model takes in this other image, then we'll be able to say that this car is intact. Nonetheless, for this deep learning model to be intelligent enough to make these kinds of decisions, we need to train it. And the way this training is carried out is we have thousands, hundreds of thousands, or even millions of damaged cars like this. And so our output level is obviously damaged. On the other hand, we have thousands of cars which are still intact, and the output level is intact. And so now we have what we'll call our data set, that is this input and your corresponding levels. And then in this deep learning model right here, we stack several neural network layers. So we'll have this, we have some other layer, some other layer, right up to the output. Now it should be noted that this neural network layers here are essentially mathematical functions. If you do not have a background in mathematics, you shouldn't be worried as in this course, we shall focus on implementing all this practically. So that said, as we're saying, this we have here, all these layers, these neural network layers we have here are essentially mathematical functions. And if we have a linear layer, then this means that we could take an input x, multiply this by a weight which we'll call w, and then add some bias which we'll call b. And then this gives us the output which is that output of a given layer. So here for example, let's suppose this input is x, we have x here, we'll take x, multiply it by the weight, and then add up the bias to have the output at this level y. Let's call this x1, then this is y1, and then this y1 will be used as input to this next layer, and then we'll have y2, right up to say yn. And so during the training process, given that we know the input and the output, our aim will be to obtain the values of this w and this b which ensure that when we pass in images like this or this, the model is able to know whether it's a damaged or an intact car. Now we see that at every layer we have our weights and biases which are all going to be updated during the training process, making use obviously of our training data, that is these inputs and their corresponding levels, such that when shown an image which this model has never seen, our deep learning model is able to take the right decision. So in essence, these kinds of models are called deep learning models because we have several layers stacked between the inputs and the outputs. And the number of layers we stack represent the depth of our model. On the other hand, deep learning algorithms fall under a category of artificial intelligence algorithms known as machine learning, where the model learns from the training data. A prerequisite to this course is knowledge of Python programming. So you could head over to the Neural Learn YouTube channel and check out our free essential Python programming playlist which will help you master the basics of Python programming. Let's now take a look at what we'll be learning in this course. The way we've designed this course is such that we'll start from the very basics. So we'll suppose that you have no prior experience in deep learning and so we'll start with basic topics in TensorFlow like the tensors and the variables. And from here, we'll dive into solving our first practical problem which is that of call price prediction. Now it should be noted that although this isn't necessarily a computer vision problem, this will permit us to learn how to prepare our data with TensorFlow, build simple models like the linear regression model, train these models, evaluate them, and test out this model such that by the time we move into our next project which happens to be now a computer vision based project which is that of Malaria diagnosis, we already have what it takes to start building more complex models like the convolutional neural network. But before building them, we'll first of all understand how and why they work. By the end of the section, we should have learned how to build a simple solution for our Malaria diagnosis problem and so now we're ready to dive into building more advanced models with TensorFlow. After looking at these more advanced models, we'll then dive into evaluating classification models. So we'll look at different metrics like the precision, recall, accuracy, and then we'll learn how to come up with the confusion metrics and the ROC plots. Once we're done with this, we'll dive into model performance. Here we shall look at TensorFlow callbacks, learning rate schedulers, model checkpointing, and then how to solve the problems of overfeeding and underfeeding. That said, one main way in which we could mitigate the problem of overfeeding is by using data augmentation and so we have the section reserved for data augmentation using TensorFlow and albumentations. Now once we're done with data augmentation, we'll look at more advanced concepts in TensorFlow like custom losses and metrics, the eager and graph modes, then custom training loops. So we'll learn how to train our model without necessarily using the fit method. We'll look at TensorBoard integration where we'll learn how to carry out data logging, viewing model graphs, hyperparameter tuning, then profiling and visualizations. From here, we'll get into machine learning operations with weights and biases where we shall look at how to carry out experiment tracking, hyperparameter tuning, data set, and model versioning with 1 dB. And now we shall move to our next project which is that of human emotions detection. Again here, we shall prepare our data set, build our model, carry out data augmentation, and then we shall look at TensorFlow records. Now once we're done with this section, we shall go ahead to look at modern convolutional neural networks like the AlexNets, VGGNets, ResNets, MobileNets, and the EfficientNet. With all this in place, we'll learn about transfer learning. We should help us now train our models much more efficiently using already pretrained convolutional neural networks. Then we'll look at how the convolutional neural networks take decisions by visualizing intermediate layers. And so up to this point so far, we've been looking at the convolutional neural networks and now we'll be set to dive into the vision transformers. We'll understand how to work and even get to fine tune our own vision transformer using the hugging face transformers library. And so now that we have a working solution, the next logical step will be to deploy this such that anyone around the world could make use of our model which we've just built. And so we'll convert our trained model to the onyx format, we'll quantize this, build out a simple API, and then go ahead to deploy this API to the cloud. Now that we've learned how to deploy our computer vision models to the cloud, we could dive into other computer vision problems like object detection. And in this section, we'll look at the basics of object detection and also build and train our own object detection YOLO model from scratch with TensorFlow. Then finally, we'll dive into the domain of image generation where we'll look at the variational auto encoders and the generative adversarial neural networks which we shall use for digit generation and face generation. In this course, the coding platform we'll use to build and train our models will be Google collab. We shall make use of Google's free GPUs to train our models. We've just explained that in order to train a model like this one, we make use of a training data set which has its inputs and its corresponding outputs. These inputs and outputs are multidimensional arrays which are commonly known as tensors. In this section, we shall start with tensor basics. Then we'll move on to casting and tensor flow. We'll look at initialization, indexing, broadcasting, algebraic operations, matrix operations, commonly used functions in machine learning. We'll look at the different types of tensors like the ranked tensors, sparse tensors, and even the string tensors. In the context of deep learning, tensors can be defined as multidimensional arrays. An array itself is an ordered arrangement of numbers. It's important we take note of these keywords as the data we shall be dealing with like for example this image right here can be represented using these numbers which have been arranged clearly in an ordered manner and can be represented in multiple dimensions. In the specific case, we have this array which is represented in one, two dimensions. So this is a two z or two dimensional array or what we generally call a matrix. Now we'll explore different types of arrays based on their dimensionality. So here for example we have what we'll call a zero dimensional array which is simply because this array or this tensor contains a single element. So let's say we have 21. This is a zero dimensional array. Let's say we will have one. This is zero dimensional so on and so forth. So essentially once we have a single element then it's a zero d array and then now for our next example we have this 1d tensor right here which in fact is a combination of several zero d tensors. So if you look at this you see that this is a zero d tensor. This is another zero d tensor. This is another zero d tensor. So this vector is made of three of these kinds of elements. Now we could have other examples like this. Let's say we have five, eight and then three. Let's change the length. So let's say we have one of length five for example. We could have ten, two, eleven, four and seven. So this is a 1d tensor we have right here. So so far we've looked at the zero d tensor. We've looked at the 1d tensor and now we could dive into the 2d tensor which essentially is made of a combination of several 1d tensors. You could see that right here. This is a 1d tensor here. This is another 1d tensor. This is another 1d tensor and finally here we have a 1d tensor. So when you bring together this 1d tensors you form this 2d tensor. For a threedimensional tensor you might have guessed this right. You would simply combine several twodimensional tensors. So right here we have this 2d tensor with this other 2d with this other 2d and this other 2d forming a 2vd tensor. We could visualize this differently here. So we take this this this and this. Now you see we have one two and the third dimension. Now that we understand this we're going to take a look at the concept of tensor shapes. Starting with this given that it's zero dimensional then there is no shape. With this one here it's one dimensional and it's made of three elements and so we could say this is of shape three. So we have three and that's it. Now for this other one it's made of one two three and four 1d tensors and each and every one of this is made of three elements. So this here for example is made of three elements. This made of three, this three, this three and given that there are four of this made of three elements its shape is four by three. So that's it. That's how we obtain the shape for this. Now for the 3d tensor we have here it's made of one two three four 2d tensors. So this here we have four two or two d tensors and each and every one of this is made of two 1d tensors where each and every one is made of three elements. So what we'll see here is this is two by three this year because we have one two is two by three this is two by three and this year is two by three and now given that we have four of this different two by three um two d tensors then we will see the shape of this 3d tensor is four by two by three. So similar to this we have four by three this is four by two by three. Now notice that the number of elements we have here tells us or gives us information about the number of dimensions our tensor is. So here because we have a 1d tensor or one dimensional tensor we have just one here because it's 2d or two dimensional we have two and here because it's 3d we have three. Also in matrix notation we would say that this year or this matrix right here is made of one two three and four rows. So number of rows we have here is four and then here we have one two three columns number of columns is three. In the case of this 3d tensor we have right here we have one two rows and one two three columns for each and every one of this 2d tensor we have right here. Now we have some basic knowledge on tensors let's go ahead and create them with tensor flow. So first things first we'll import tensor flow as tf and then you should note that because we're using colab we do not need to install tensor flow before making use of it all we need to do here is just import and we're good to go. Now first thing we're going to do is we're going to have our tensor which we'll call tensor 0d and the way we're going to create this tensor is by calling on the constant method. So we have tensor flow's constant method and then we specify for example that we want this to be let's say four so that's it that's how we create a zerodimensional tensor. You could see here from the documentation that this constant method takes in a value takes in a data type takes in a shape and a name but for now we pass in only this value right here and then we could go ahead and print this so let's have tensor 0d run that you could see right here we've had this tensor which has value four it has no shape and it's of type int 32. With this we could go ahead and build out our tensor 1d, tensor 1d we have as usual our constant method but this time around because it's 1d we're going to have a list. We're going to take this list we had here 2 0 negative 3 and then putting this now right here we have 2 0 and negative 3. Now let's print this out we have tensor 1d and there we go you see we have the value which is this 1d or this list we could see shape right here and then the data type now let's modify this and say we have 8 and let's say 90 let's run this now and check out its shape you see here this is our new list or this our new tensor and then you see now it's five because we have five elements in here but again this is still an integer so let's add this here changes to a float and then see what we have see here now this is a float so it's still our inputs by this time around floats same shape but the data type changes from int to float let's get back to integer and then we move now to the 2d tensor so we've looked at 0d 1d to create a 2d tensor it's going to be quite as simple as we've done with the 0d and 1d so here we have 2d and then we have as usual our constant and then let's open up the square brackets so we have the square brackets right here open up and then we'll have this 2d tensor we have seen already so let's have this right here we have 1 2 0 we put a comma to move to the next row or to the next line we have 3 5 minus 1 then we have 1 5 6 and then we have 2 3 8 okay so as you could see here we have this 1d tensors which we stuck and which will form our 2d tensor now as usual we have our constant method which takes in this value and you should also take note of the fact that we have this outside of this stacked 1d tensors now we have this let's go ahead and print out tensor 2d we have tensor 2d and there we go as you could see we have our 2d tensor that's it we have the shape 4 by 3 which is what's expected and then the data type is in 32 from here we move on to the 3d tensor which is essentially made of this 2d tensors which have been stacked together so let's go ahead and see how to create this with tensor flow we'll start by putting together those 2d tensors we had 1 2 0 3 5 minus 1 so that's it 3 5 minus 1 and then we have all these remaining 2d tensors so let's get here we have tensor 3 tensor 3d it's a constant we have our square bracket here then we simply copy this copy this and then paste this in here now let's print this out and see what we get so here we have tensor 3d as you can see we get an error and this comes from the fact that we omitted commas now if you look at this you'll see that uh between one 1d and another 1d to form this 2d there is a comma and so this means that between this 2d and this other 2d there should be a comma so let's have this comma here let's have this comma let's have this comma and this comma okay so let's run that again and see what we get as you could see we have this um our 3d output of shape 4 by 2 by 3 and that's exactly what we expect to get data type int 32 now before we move on we could we should note that we could also do let's let's have this here we could also do um tensor 3d shape to obtain this tensor shape so you see that we have that um right there we could replace this zero we could have one see that's zero that's one that's two um there we go let's let's have one that's one um that's it okay and also we could also have here let's say three we could do ending and this gives us a value of three showing us that our tensor is a 3d tensor at this point we've looked at zero d 1d 2d and even the 3d tensor let's go ahead and check out a 4d tensor but to construct a 4d tensor we need several 3d tensors so let's have this here we have this 3d well we saw this already this is this 3d and then we move to this other one right here or this two others we we have this other 3d and this other 3d and then now if we stack this three 3ds up what would obtain would be a 4d tensor see we have this one we add this other and we have this other so now we have here a four dimensional tensor which is made of three three dimensional tensors now you should know that this could be two this could be four this could be whatever number so if we consider only this two then we're talking about two three dimensional tensors but uh what's so important to note here is the fact that once you stack several 3d tensors you create a 4d tensor now at this point you could take it as an exercise to create your own 4d tensor so you could pause the video but um we are going to go ahead and show you how to create this first things first we're going to take up our 3d here we have this 3d tensor let's just copy this there we go we paste this out here let's repeat that operation copy and paste again but now let's modify some values so let's say we have 13 26 let's just randomly modify this um 23 so it's now this this other 3d is different from the other one say 30 okay now let's copy this again and then paste out here so we have the three 3d tensors um let's say this is one oh let's just add zeros we have here um 23 um two four and six okay so now we have our three 3d tensors we have this one we have this other one and then we have this other one now to create our 4d tensor as usual we call on our constant method so we have 4d um we have the constant and as usual we have our square brackets which will open up and then now we just simply take this copy all this to the end cut this from here and then paste this out here okay so we see we have three of these um 3d tensors now remember last time when you were separating two 3d tensors or between the three d tensors we need to have a comma so here we're gonna have this comma and then here we're gonna have this comma and here we have a comma okay so that's it so let's run this oh let's print this out and then see what we get so here we have print um tensor 4d there we go you can see from here we have our 4d tensor you see it's cheap three by four by two by three and this is because our 4d tensor is made of one stick this off it's made of one two three three d tensors so that's why we have three and then for each and every one of this we have four that's one two three four two d tensors so here we have four and for every two d tensor we have one two that's two rows and one two three that's three columns and that's now exactly what we have here so we still have the same data type we have uh values which we could have here and that's it if we get back here we'll see that we have different data types which we could use so instead of um does this default int uh we could have um tf float say 32 and you'll notice that the outputs as we have seen already should have this decimal since we're now dealing with floats now the float we have in here is of type um float 32 or specifically the float 32 meaning that the position value here is 32 now if you reduce this if you reduce this you'll see that you actually have the same output but the difference is that less memories are located for storing this tensor as compared to maybe even let's say the 64 that's float 64 and so in certain contexts where we have the memory constraint we would want to use the lower precision tensors in the documentation you could have um all the different data types um which are supported in tensorflow so here we have quantized data types and talking about quantization we're going to treat this in subsequent sections and so don't bother for now if you don't really master this um here we have the brain floating point here we have the boolean here we have complex and it's 128 and 64 bit versions here we have the double which in fact means the double precision floating point now this is double because the float 32 is a single precision so here we have single the float 16 is half prediction or precision and then the double is double precision here we have the ints under different versions or the different uh positions we have the quantized ints um we have resource we have string we have unsigned int and then we have variant getting back to the code if for example here we have this float let's just add a decimal so we suppose that this is a float and then now we do int let's say int 64 we run that you'll see that would obtain an error so let's have this here what do we see we told that we cannot convert this um tensor to or to eager tensor of the data type int 64 but now if we use the cast method if we do tensorflow cast um and then here all right let's take this back let's let's have our tensorflow or our tensor 1d and then let's take this off or let's yeah let's just have this float um 64 there we go or let's say float 32 and then let's say we define some casted tensor casted tensor 1d and then this casted tensor 1d is simply going to be the casted version of this or 1d tensor so here we have tensor 1d and then we specify the data type and now we say we want this to be in 16 for example so here we're going to print out the tensor 1d i'm also going to print out the casted tensor 1d we get in this error we should have had tf dot so let's have this here there we go and then let's look at what we have as output as you can see thanks to the casting operation we are able to get from this float to this integer tensor and so this tells us that instead of coming right here and saying for example int 16 or let's say int 32 it's preferable to make use of the cast method that said let's say for example instead of having this that's instead of casting this into an integer we want to cast this into a boolean so let's run this you see here that we get all true except for this one year which is false and that's simply because this is a zero so it looks like the what a casting method does is for all the values which are different from zero they're true but values equal zero equal false we could also go ahead and create our own boolean so let's say we have tensor tensor bool we call this tensor bool and then we have all this lists made of this tensor so we could have true true false true true false okay so let's print out tensor bool and see what we get as you can see here we have our output tensor of shape three and here is values you see the data type bool right here now another data type which you could look at is um the string so let's let's say we want to have tensor string um there we go let's say we want to have um halo world halo world um let's print this out tensor string there we go we have our tensor we could also put this in the list because right now this is a zero dimensional tensor so let's put this in the list and um hello world hi and we'll close that and now we have this 1d tensor of shape two which is a string from here we would also look at how to create or how to convert a nonpy array into a tensor so let's call this um np array first of all we'll start by importing nonpy as np we could just do that right here import nonpy as np there we go and then we have um np array and let's say we have np array and then let's take in this here let's say one two three or one two four there we go print this out we have np array right here now we could make use of this tensor flow convert to tensor method so let's say we have your converted converted tensor and we have um convert to tensor which essentially takes in a nonpy array now let's have this and then print out the converted um tensor with that getting back to the documentation we will look at a couple of other um methods like the i method here we have this i method right here as usual we have the well described method in the documentation which comes with this um short phrase explaining how it works it comes with different arguments and also even some examples so getting back here we told that this i method permits us construct an identity matrix or batch of matrices so now we have this definition we could simply copy this out and then get back to the code right here paste this out and let's say we call this um i tensor i tensor and for now we have this number of columns which is known the batch shape known data type float 32 name known now let's say we want to have three rows so we have number of rows which is equal three now in this case uh we'll print this out so we see the kind of output we get here we have i tensor and there we go as you could see we have this identity matrix where all the values of the matrix are zero except for those of the leading diagonal let's um show clearly this leading diagonal right here here we have our matrix which is this and then we have this element of the leading diagonal which is equal one now you should note that you could you could say for example um let's say three times this and you should have all the elements of the leading diagonal to be equal to three and the others zero so in this case we now have someone like this so you see all this three while the rest zero now let's get back let's take this off here we have our i tensor and then we see that we could specify the number of columns now here or previously where this was set to known we had a square matrix that is when you define the number of rows to be three the number of columns automatically takes up the value of three so we have a three by three matrix and obviously the type here is float 32 could modify this and say for example 16 from that let's try out bold after this there we go we have float 16 let's try out bold you see everywhere is false false false false false false except for the leading diagonal which has true now getting back let's say here we have five and then now number of columns let's set this to three you see right here this is the output we get now it's it acts like we have a five by five matrix so we suppose we had a five by five matrix where this would be zero zero this would have been zero zero this would have been zero zero this would have been one if this was five by five zero and then here zero one so this is what we would have had if we do not if we let this to to be known that is number of rows equal number of columns but now that we've set number of columns to be three this part has been cut off and so this is what we're left with now with that said let's go ahead and take this back to known and then let's set a bad shape so let's say this is three around that as you could see because we set this bad shape to be three here we have this output which is three by five by five well let's let's change this to two so you could see that this actually is responsible for deciding on what number of batches we have right here so here we have two by five by five so essentially saying that you want to have this bad shape to be two like this means you want two five by five matrices which are identity matrices that is which have their values or zeros except for those of the leading diagonal which are equal one and so we see how we could create this 3d matrix or this 3d tensor from the i method now we could go ahead and say for example one foyer let's run this and see what we get we should get two by four two by four by five by five here we have two by four by five by five and that's it for the i method we move on to the next method the field method here we have a method as defined here in the documentation which creates a tensor filled with a scalar value so here we have this field method which we're going to copy but before testing out the code you could see here from this example that with this field method we have this tensor which takes all which has shape two by three and which has value nine on all different positions so let's get back here paste this out let's call this field tensor and let's say this dimensions let's say we have three by four and then let's still want to have the value um let's say five okay so let's print this out and see what we get field tensor and there we go that's the output we get now let's have this one for example run that see we're able to create this 3d tensor where all the values all the elements in each position takes the value five from here we'll go on to the ones method and it should be noted that this ones method is quite similar to the field method in the sense that this creates a tensor just like with a field method but the only difference is that here all the elements are set to one so you notice that here in this definition we do not have this value argument right here we had a value which we passed but with the ones there is no value because the value by default is one so let's have this here and then paste out right here see we have this ones so we'll call this ones tensor um there we go we specify a shape let's say five by three and then we print that out so here we have ones tensor and there we go we have this output matrix which um is made of five rows one two three four five and three columns one two three where all the elements of this tensor take up the value one now you could obviously do this to have or to obtain a 3d tensor so that's the output we get you see it's five by three by two from here we'll look at the ones like and what this one does is it creates a tensor of all ones that has the same shape as the input so what this means is if you have an input like this one let's say we have twelve let's not make that one twelve one three and then five um seven two this actually a two by three matrix let's change this so you see that the this is two so we have the shape the shape is um two by three so its shape is two by three now if this is the input here if this is the input into our ones like method what we'll get as output will be um a matrix or tensor with this same shape that is we shape two by three so our output is going to be having a shape two by three meaning that we're going to have um two rows and then one two three columns but the output or rather the values we're going to insert here will be all ones so now we're going to have one one one one one one so hence the name ones like so this like is actually for the shape of this so you imitate in the input with respect to its shape now getting back here you see we had this field tensor which is a one by three by four tensor so let's um let's have this right here let's say we want to have uh ones like tensor there we go tf ones like and then we have field tensor field tensor there we go okay so let's print out those ones like tensor now before printing let's um try to obtain the output so here we know that this is field tensor the shape is one by three by four so it means that we should have an output of shape one by three by four where all these values are ones so let's run that and and make sure that that's the output we have there we go c is one by three by four and we have all ones from here we move to the zeros method so it's quite similar to the ones here uh just like with the ones we have um all elements set to a given value with the ones all the elements will set to one with the zeros all elements are set to zero so that's essentially it now let's just modify the code a little right here so this was once tensor um let's change the shape this three by two and then instead of ones here let's say zeros here we have zeros and um let's call this well let's say this is let's just copy this out and put out separately so here we have this you paste that out here we have zeros and here we have zeros fine and here we have zeros okay so let's run that and there we go we should have this here so you see three by two three rows two columns that's fine and all values are zeros if you were once then you have all values once and for the ones or the zeros like is similar to the ones like so you could take that as a simple exercise we then move forward we have this shape method right here where we thought that this returns a tensor take note of that it returns a tensor containing the shape of the input tensor so um as we had seen before we could obtain the shape of a tensor by simply um making use of that tensor's name dot shape now if we want to have an output like in our case now we want to have this output um tensor which contains the shape of our tensor then we could make use of this shape method from tensorflow so you see here we have the same four by two by three but this is now in this list um and then we have its shape and we also have its data type obviously it's made of integer so we shouldn't expect to have a float data type right here another method which we could make use of is a rank method which simply returns the rank of a tensor so it takes an input and then returns its rank you have a simple example right here where we have this 3d tensor t and then when you call on the rank method you see that we obtain the output which is three so let's um put that out here let's just paste it out here and test that quickly there we go as you could see because the output um is a zerodimensional tensor the shape it actually has no shape but you could see its value right here that's three now if you take out this and take out this and then take out this other part here there we go we run this again we should have a rank of two this time around that's fine it's here rank is two we now move to the size here the size method which returns the size of a tensor so we put an input um in the size method and it gives you the size of the tensor we have as usual in the example here which we could copy but before running that we could read this um note here what we told this returns a 0d tensor representing the number of elements in the input of type output type this is the output type here we by default is int 32 so let's get back here and paste this out and check out the size as you could see here we have this size of 12 and that's because we have 12 different positions or 12 different elements in our tensor now let's take this off again take this off take this off there we go let's run that we should have six so that's it you see we have six and here we could also specify this d type um let's say float 32 we get an error here uh got an unexpected keyword argument d type now getting back here you see this is actually out type not d type so let's get back here and then we have um out type so we have out type there we go run that and as you could see right now we have a float instead of an int the next step we'll take is creating our own random tensors and that is essentially creating tensors which take up random values now in the case of tensorflow random normal our output values are going to be from or are going to be drawn from a normal distribution and we are going to explain at least at a high level what this really means for now let's just copy this out and get back to the code and run this and see what we get so right here let's say we have random tensor there we go we need to specify the shape so we'll say let's take let's make um a matrix or 2d tensor so let's say it's three by two now you see we have this mean value standard deviation values which have been given right here by default and we're going to look um at what this actually means shortly the data type is float 32 cdc is known name noon now let's run this or let's print this out and see what we get so here we have random tensor and there we go as you can see we have the set of values which are all negatives and if you notice they are very close to zero so here we have practically negative one about negative two negative zero point three very small number and negative zero point five negative zero point eight seven so this number is very close to seven let's run this again so you see that we're able to randomly generate a matrix with this shape with random values it takes up different numbers this time around we have a mixture of positive and negative numbers but again these numbers are very close to zero now what if we modify this mean so we modify this mean run this again let's see what we obtain you see that we have uh these numbers now which are instead close to a hundred and so what this tells us is that we could make use of this mean and the standard deviation to decide on the kinds of random values we want to have here now to better understand what is going on let's consider this figure from this probability playground by Adam Kerningam from the university of Buffalo as you had noticed in the code when we did tensorflow random normal and the mean when we specified the mean yeah the mean is this mu you have here when we specify the mean to be equal zero most of the values we had were surrounding zero and when we set this mean to a hundred when we set it to a hundred most of the values were surrounding a hundred now this curve you have here actually explains why so you have this curve right here uh which is bell shaped and the idea here is that it permits us randomly pick values around the mean that's around zero so that's why you'd notice that at zero we have the highest probability score here our probability score is f of x so um for values around our values surrounding zero that's this value surrounding zero so there is much higher probability or chance of them being picked as compared to values far away from zero so let's pick out these two values let's say we have um let's say we want to pick out the two values 0.5 let's say 0.5 and we want to pick out negative five you'll see from here that the probability of of us having negative five is particularly zero that's almost zero but the probability of us having 0.5 as you could see from here if you're taking this middle the probability of us having 0.5 is about 0.35 now if we change this value and take a hundred as we didn't with the code let's take a hundred we have that um hundred well it looks like they fixed this to six so we cannot go above this we could play around this like this you go from negative six to six okay let's say the mean is six so we have a mean of six one thing you can notice is that now this um 0.5 as you could see 0.5 let's take this off um 0.5 no longer has a probability of about 0.35 of being picked 0.5 has a probability now of about zero of being picked while negative five also still has a probability of about zero being picked but um on the other hand as you could see here let's take this off you see that values like five have much higher probability have been picked seven eight six they now have much higher probabilities have been picked now this means that if this was a hundred or if we're able to get to a hundred then uh will be values like 97 uh 98 99 100 101 102 and this explains why when we run this code with this mean we had values surrounding a hundred and that's simply because those values surrounding the mean have um higher probabilities of being picked now let's take this back to zero and then talk about the standard deviation so we have the zero there we go see get back to zero and now let's talk about a standard deviation but before um get into any um explanations let's modify this let's say we take 10 here okay it looks like the max is fixed at 2.5 okay so let's let's say we have that 2.5 one thing you can notice is we still have this our mean at zero we still have our mean at zero does it make sense because we haven't changed this but our bell curved um shape now appears wider so an increase in sigma sigma here is what we call the standard deviation so it's what we called std dev in the code so we had a mean and the standard deviation so sigma standard deviation the sigma square is the variance so sigma square is the variance anyway uh what we're trying to say here is an increase in sigma will make this um curve wider and a decrease let's decrease sigma you see decrease in sigma makes the curve thinner and narrower now what this implies is that if we have let's say let's get back to this if we have sigma square as 2.5 there are much higher chances now that we could um randomly select negative five as compared to before so if this is negative five here you see there's a slightly higher probability of negative five and peaked as compared to when we reduce this you see when we reduce this um you see negative five here is practically zero but when we increase see negative five is at least this time around having some negligible value and so this is essentially the row that the standard deviation plays now if we reduce this again we find that even 0.5 because this is 0.5 around here even 0.5 let's take this off even 0.5 which used to have a probability of about 0.35 of occurring um now has almost no chance of being picked let's increase it back and set it back to zero okay and so that's it at this point we could modify this we could take this to six and then take this to 2.5 and you see that we could take values now between zero see go from zero to about um 11 with that said we'll then look at the uniform distribution or how to generate random numbers or random values drawn from a uniform distribution so again we'll just copy this out and then get back to the code we'll paste this here there we go we have the shape let's pick um this uh one okay and then minimum value zero and now we have maximum value so you see that unlike here where we have the minimum the the mean and the standard deviation now we have instead of a minimum value and a maximum value so let's let's let's leave the leave it at this and then let's say we have random tensor random tensor okay so let's print this out let's print out our random tensor there we go we have random tensor let's increase this so we have many more values five from that okay you see we have these values now let's let's say we change this max value and say it's um eight eighty or let's say eight okay let's say eight let's run that and what do you notice now is you have much larger values well values between zero and eight but um before the values we're getting where values ranging between zero and one now this tells us that most probably this maximum value by default is one so here uh you see it's default so one you could always make use of the documentation whenever you're in doubt so let's get back here you see we'll take this to 100 and then see what we get you see we have this values now between zero and 100 now the question you may be asking yourself is what then is the difference between this uniform and this um normal distribution so let's get back again here um in our probability playground and then let's pick out our uniform so there as you could see here many other properties distributions but we're going to focus on the normal distribution and the uniform distribution so we get into the uniform distribution and here's what we get so we have a and b now this is like our mean vowel which we saw in the code and this is like our max vowel so when we say mean vowel negative one max vowel one obviously our values will fall in this range now by default this is zero and this is one on tensorflow so um that's why you saw the values ranging between zero and one now as the term goes it's actually uniform so this means that all this um values here have equal chances or probabilities of being picked and so unlike the normal distribution where we would have something like this something like this now we have this square instead or this rectangle and so simply here all we're doing is picking the range of values which we want to have or which we want to be outputted so we could get here increase this um where you cannot you cannot the mean cannot pass the cannot be greater than the max so that makes sense let's reduce this reduce that see from negative four to four see we take all the values well that's it see there we go we could go from negative one to two that's it now one great thing is also the fact that we could modify this and have ints so let's run this and see what we get you see we have only integers now now let's say we had norm let's run that you see we have an arrow most specify a max vowel in the case where the data type is integer so this actually um reading out here in the documentation so you can see here now that you need to have this max vowel so let's change that to let's say 1000 so you get values ranging between zero and 1000 so we have five values here from the shape we could change this let's say five by five and then all this values range between zero and 1000 another argument which we have been spoken of so far is this seed argument and here we're told that this seed argument when used in combination with tensor flow random set seed we'll be able to create a reproducible sequence of tensors across multiple calls and so in cases where we want to produce reproducible experiments we want to set we want to set this global seed value and also the seed argument value in this uniform or let's say normal function so let's go ahead and copy this out we get back to the code there we go let's just run this right here let's take off some parts and print this out so we print this out then we modify the shape and the max values so we have exactly the same value for the seed year which is 10 and then here we set this global seed to five let's run this you see it outputs 4 3 1 4 3 2 1 1 1 and 1 3 3 now let's create another cell um let's copy this there we go paste this out here and you see we have the same global seed um and then we have the seed of 10 now let's run this again and as you could see we have exactly the same output as this one see 4 3 1 4 3 1 4 3 2 4 3 2 1 1 1 1 1 and 1 3 3 now obviously modifying this and taking for example one gives us something different but if you take 10 again you see you should have exact same output now you also see that here when we take this off if we take this off and we run this cell you see we have different outputs but when we get back here let's let's run this again see we have another set of different outputs but when we take this run that you see we have exact same output we had already and now if you want to have more details about certain seeds you could always check out this um the documentation now we're going to move on to tensor indexing so let's take this example we have this tensor to be indexed and we declare this we have the state of the constant and this 1d tensor we pin this out this is what we get now supposing we want to get this first four elements to get this four four elements we need to consider these are the indexes so here we have the 0th index then here we have the first index here the second here the third and the fourth so we want the first four elements all we need to do is take out this first four elements by doing this so we have indexed and then we have the square brackets we start with the minimum index and then we have minimum index that's zero that's the 0th index which is this we go one two three four now here we specify four there we go and that's what we get we have this first four elements right here now you might have noticed that the fourth index is actually this year but then since we started from zero our third index happens to be the fourth value so if we're taking this from zero we have one two three four so y'all first four values now note that if you want to grow from the zeroth or from the first let's say we want to go from this index here six to 66 if you want to go from six to 66 what you will have to do is this we have that we take out zero we go start from the first zero one so we have this one right here the minimum and then want to go up to 66 then we'll have to go one two three four five now if you put this five right here you'll notice that it's not going to give you exactly what you want so let's run this and then we get that so see that we have here six two four six we actually have this instead of all this so how do we do to get all this in order to go from one index to say a maximum index what you need to do is to add this plus one so you want to go from the first index to the fifth index right here that's one two three four five want to go up to this index i need to include i need to put this plus one it's similar to with a tf.range read scene previously where in order to put out an tensor from let's say a range two to five what we needed to do was specify this from two to six so that it would be like two three four five so yeah what we did was add two five plus one so it's a similar kind of pattern that we're following right here so to go from this up to six to six we need to add plus one so let's run this now run that and there we go is we have exactly what we expected so let's take this off now and we have six so with this we see how we're able to slice out some parts of this array of this tensor from here we could also include steps so if we put out say two steps of two you will notice that we go from the first that's the first index this is the zero index so we'll go zero we start with this one we skip by two so we skip this element and then we go straight to four so notice how we go four and then from this four we skip this element and then we go to six to six so that's it so we could include the steps right here now by default we have a step of one so when we do this you see we have exactly what we do what we get when we don't put the steps at all so that's it we've seen that in general we have a minimum value right here and then we have a max value or rather a max index so yeah yeah we have so that's it in general we have a min index up to the max index plus one so generally this what we get now in the case where we don't specify this min index that's if we have this for example let's suppose we have zero this we don't specify any min index and then we go up to four then this is considered to be zero so you notice that this tool come up with the same answer we have exactly the same answer right here when you specify the minimum value say three and you don't specify the maximum we're just gonna go right up to the end so let's run this we'll see that we'll get zero one two three so we're gonna start from this and then go right up to the end so we'll have four six six six seven so that's it we have exactly as expected four six six six seven because we're starting from three and we're going right up to the end if we want to go from this first off from this minimum index right up to the last but one value that is want to go from this four right up to 66 what we could do is have our maximum value to be negative one so doing this you'll see that we have four six six six so we're going from this index this minimum index to the last but one value now let's take this to two see we have four six take this to three we just have four take this to four this should be an error from this point we'll see how indexing is done with tensors of dimension greater than two so we will start with a twodimensional tensor which we had declared previously now for indexing to be done we'll start with a comma right here so we have this comma and then if we want to get the first the first three rows so suppose we want to get this first three rows want to get this first three rows and then in this first three rows we want to get just the first two columns so we want to get this colon and this colon in fact fact what we want to get is this one two three five one five this happens to be the first three rows and the first two columns so in order to do this we note as we said here we have this comma and then to this left side we have the rows and then to the right side we have the columns so what we do here is since we want to get the first three rows we're going to go zero one two so what we're gonna have here zero right up to three so we're gonna get zero one two obviously two plus one three so we have this so it's kind of like similar to what we've seen already here but with the difference that all this let's say all this gets to this left side and then for the columns are right here for this yeah for the columns want to get the first two columns so what we're gonna have here is zero two so we'll have that there we go and then let's run it so running that we say we have one two three five one five now let's suppose we want to get this first three rows like this and then want to get all the columns so basically here what we can do is this so just like you're getting all the elements we just had to do this that is with this column yeah we just have to put this column right here and then we're gonna go so let's run that and we should have one two zero three five negative one one five six so we've got in the first three rows and then all the columns now if you want to get a particular row so suppose I want to get just the third row that's want to get only this row right here we're gonna specify its index this is zero one two so here we have two so for the rows recall we have the left side representing the rows and the right side representing the columns so yeah what we're saying is we're getting the second index or this row the second index or the third row which is which happens to be this and therefore the columns to get in each and every column so let's run this we should have one five six now if we want to specify or if we want to get just some of these columns that is if we want to get maybe the this only this column right here we're gonna have two and then we specify the zeroth column so run that and we should have one now if we want to go ahead and take say one right up to the end you see we have five six because here we'll specify want to get a second row that's zero one two second row and then for the columns we're going from one this one because this is zero one two one right up to the end so that's it just like with the rows we could also specify or we could also get just a particular column so suppose we want to get this zeroth column right here which made of one three one two so to have that done we are taking all the rows so we're gonna have all the rows and then we will pick out just the zeroth column so let's run this and we have one three one two which is this column right here now let's take this from here and we'll see we have an error because we haven't specified how this rows are gonna be managed so here again let's say we want to do one two three so right here we have three one three one because we're making for the columns we're taking the zeroth color so we're taking all this and then for the rows we're going for one two three so zero one this our first this one two and then three meaning that we're stopping at two because stopping at three minus one so we're going we're taking this and then this and then since we're taking the zeroth column we just have three and one so that's how we get this so let's come back we have this and then if we want to pick out the first the column the first index we have two five five three now this year means we're picking up all the index indices so if I'm picking up all the indices what we could do is specify just this three dots three dots simply means picking up everything so we're picking up all the indexes or we're picking up all the rows and then we'd specify in the first column so running this should give exactly the same upwards we could see right here we now move to the 3d tensor so we have this 3d tensor right here let's take this off now we kind of like this kind of like very similar to what we've seen already with the 2d tensors but with the difference that we have the two commas now we're asking why we have this two commas recall that with three dimensions we have the first dimension the second dimension and the third dimension so first thing let's put this this way let's have this we have this each of this is a two by three shaped tensor so here we have two by three that's what's not a shape actually and then yeah we have one we have one two three four elements so we have four elements so this happens to be the shape of our 3d tensor right here now if I want to pick out just this first elements right here all I need to do is specify a zero and then I could take all this all this simply means that I've picked out this first index and then taking all the rows and all the columns so let's run this and you should get just this right here so we see we are 1 2 0 3 5 negative 1 so that's fine now if in this index that if after selecting this index I want to say get only this first row what I'll do is I'm gonna take 0 and then right here I'm gonna pick everything so I'll run it and I'll have 1 2 0 I have only this right here now if I want to get this last column what I'm gonna do is I'm gonna specify I want to get all the rows and then for the column I want to get the last column so there we go I have the last column 0 negative 1 because I'm turning from the end actually here now another way we could do this is by saying okay I have 0 1 2 so I'll take the second index run that and I should have the same answer so that's it now if I want to go from 0 to 2 so I'm gonna pick 0 to 2 means I'm picking 0 and 1 so I'm picking this 2 actually picking this 2 around that see I have 0 0 negative 1 2 all right I have 0 negative 1 0 2 the reason why I have this is because right here I have the 0 negative 1 and then yeah I have 0 2 so that's it the 0 negative 1 is because I have picked I've picked this 2 indices so 0 negative 1 is the last right here the last index or the last column pick out the last column and therefore this I also pick out its own last column so that's how this works we just have this commerce we separates this now another way I could do this is I could take all so doing all I should have exactly the same response so instead of having that I could just put out this three dots right here then here I could pick out also yeah I'm supposing I'm taking all this since we're picking up all this indices that's we're picking a 1 2 3 4 all this four elements we're picking up for each of these elements all the rows picking this all the rows and then for each of these we also pick up just the second column so if we yell picking up all those rows and just a second columns that will be left with 0 for this and negative 1 then for this we left with 0 2 this 0 0 there's 932 we now move on to tensorflow.math which we could see clearly here we now move on to tensorflow.math which we could see clearly here we'll be able to use all these math functions which are made available with tensorflow so as you could see we go right up to this zeter function so starting from the apps function this apps here clicking on it will have tf.math.apps and note that for each and every function you'll have the function definition and you will have an explanation so like you have this kind of example where the function has been applied and you'll be able to understand exactly how those functions are used so in case you want to understand how the A10 to function works just click on that you have this we click on A sign actually let's all let's just work with A sign in case we don't work with the A sign you would have this you have this example and then you get this explanation on all these arguments which get into this function like the X you have the name and then what it actually returns so you don't need to always figure out by yourself how this functions work you just basically need to make good use of this documentation now that said let's look at a couple of functions we have the apps function it's actually the absolute value so it gives absolute value of a tensor now to get an absolute value we simply pass in the tensor so like in this case where we define this X so X apps we have the X which is this tensor right here and then we could get an absolute value very easily so we have that we run it and you see we have the absolute value now what's an absolute value up to the values are defined as such if the number or the input is a negative so if the input is a negative it sends it to positive so it turns it into positive if the input is a positive it turns it into a positive so it remains the same or basically it remains the same so that said we could say for example TF that apps of say TF that constant 0.2 if you run that you'll see you still have the same now if we tend this to negative 0.2 you would have still 0.2 so it takes it from a negative number to a positive number or basically we can say that the absolute value of X is negative X when X is less than 0 and it's equals X when X is greater than 0 so if we have a positive number it remains the same and if we have a negative number you attach a negative sign and obviously a negative with a negative would turn this to a positive so that's why when you have this negative 0.2 here you have negative of negative 0.2 it turns into 0.2 so that's it for the absolute value function you could also check out when we have the complex number that is if a tensor if we have a complex tensor you could check out this absolute value function right here the inputs of this absolute value function can also be complex numbers now if we have a complex number say a plus bg then its absolute value is computed as a squared plus b squared all of that square root so if we have negative two point two five plus four point seven five j as see it looks exactly same as a plus bj right here then we'll have the absolute value or get absolute value by doing negative two point two five square plus or rather the square root of negative two point two five square plus four point seven five square that's that let's take this off let's just have one so if we have just this one right here okay now we run this xops xops complex complex and then yeah we have xops complex to complex okay so let's run this and you get this output five point two five now let's do a squared as let's take this if we have the square root we'll be using the square root method so we have to have the square root of negative two point two five square notice how the this pops up right here with the definition nice with the documentation of the square root method so we have a square root text in this input and then name so basically we just find the square root of all the elements we have in our tensor so that's it we have negative two point two five plus four point seven five square so four point seven five square so that's it we completed square root and we see that we do not have the same answer that's because we didn't put this in a bracket so there we go we run this again and we see we have exactly the same answer so we now take on the addition so yeah we look at this addition function this can be seen as element wise addition so supposing we have x1 and then x2 tf.constant we have c7 6 to 6 7 and 11 the type a specified a d type tf.int 32 we could simply do tf.add x1 and x2 so let's run this we'll get our output it's basically the addition of all this element so it's an element wise addition now you could check out other methods like multiply so you could have multiply multiply we run that c5 times 735 and so on and so forth you could check out subtract so we could have you subtract and you have your answers let's check out divide divide there we go run that there we go five divided by seven to rewrite by six and so on and so forth then we also have other interesting methods like this divide no none so in the case where you divide into values and you expect it to have a man like when you divide him by zero expected to have a man that's not a number this method takes care of that exception for you so let's come in here and instead of zero run that again you see we have this now let's of this infinite infinity so let's now say no man run that we have this error because this is actually a math so you should have that we have here that tensors of type interior two are not allowed so let's take for example a float so let's change this we have float float there we go run that again and it's fine so you see that when you have this like if you have it a problem where you just want to have this NAND or this output where it's meant to have infinity give you a zero then you could use this method right here now note that it's not every time that while we're doing this element wise operations that we have the two tensors being of the same shape so that said we could modify this so let's take this to be just seven right here let's take back the add so we have add and around this again you will notice that we were able to do this addition but with a difference from the others where the two tensors had the same shape in that yeah we have this seven which adds up to each and every value we have right here so seven plus five twelve seven plus three ten and so on and so forth now this is what we call broadcasting in broadcasting we have this smaller tensor which is stretched out match the shape of this bigger tensor so that the patient could be carried out so in fact what's going on here is you have this X2 right here which has been stretched out so we can see X2 stretched stretched and then what we actually have is this so finally when we carry out the operation we have this seven let's make sure we have this values right seven and then we have this last seven so basically this is what we have now let's in this out let's copy this and then we have X2 stretched that's that's it now we run it and we see that we have exactly the same output so if I why you put this tensor flow does broadcasting and this tensor right here is being stretched out like this and then the operation is carried out so we could test this out with multiplication let's modify this supposing we have this right here we have that say five forty five so on and so forth okay so you have X2 and then we have X1 let us put out let's do run with the addition let's take off the shape run that see give the same answer we go to multiplication multiply multiply there we go run it again we see we have this element wise multiplication now another slightly more complicated way of looking at this is we have a shape or when we have a tensor where one of the indices has a length of one so if we take this off let's suppose it would take let's take this what I want off so if you take this one off right here you see we have a shape of 1 by 6 now let's define X2 differently let's take off the X2 stretch from here so let's define X2 differently we have in this case instead let's define a 3 by 1 tensor so would have this we have 5 we have 3 so there we go run that let's take this one off let's print out the shapes actually let's print out X1 that shape print out X2 that shape there we go we have this is X1 1 by 6 X2 3 by 1 and then the output of the element wise multiplication is this matrix we have here so now let's explain how we get this matrix yeah if we write this tensor right here would have this it has just one row so we have one row and then six columns as you can see here and then this other where we have three rows and one column just as one column three rows one two three now that said when we want to carry out our operation here we're taking the incarceration the fact that we have three rows right here so what goes on is this one row is stretched out so it's stretched out so that we now have three rows to match up with this three rows we have in this X2 so that said we have now five three and finally six right here so there we go we now have this new tensor that's it and then for this you should guess it right we are gonna stretch this out such that the number of columns match with the number of columns of this so basically we're just gonna rewrite this six times there we go three right here we're reading this six times or stretch this out and now we could carry out our multiplication operation so let's take for example this element we have this six times five right here we should have 30 so that's why we have 30 here and if we take 4 times 5 20 that's why you have 20 here if you take this 4 by 3 that's why we have 12 right here so we have that as a rule of thumb both should have one dimension of length 1 as you could see here and then the other dimensions are stretched so as to match one another our next method will be the TF the maximum method so there we go we have TF the maximum which gives us the max of two tensors or better set the element wise maximum of two tensors now this should be the same with a minimum which returns the element wise mean of two tensors so that said if we have this two tensors X and Y you see that we're gonna have here negative five so negative five zero negative two the mean is negative two negative two zero zero means zero zero three means zero so that's it as you could see here this suppose the broadcast semantics that the broadcasting we've just seen so that's it for the maximum and minimum we also have the act max and act mean which we're gonna see shortly so let's go we have the act max okay that's it now what I max is a slightly different syntax yeah we have an input and then we specify the axis so let's copy this B right here let's copy this tensor is this out there we go okay we have this B oh we have this egg egg arc X arc max for example so we have this tensor let's rewrite this we have this tensor here and then we let's print out X arc max that shape bring out a shape okay now we want to get the arc max but before looking at the arc max of this tensor let's look at a simpler tensor so let's take something simpler let's have this let's say we have yeah let's close this okay so we have this let's print out X max we have TF TM the mat that arc max this and then we have that we have it okay so let's run that we see that we have a value of 2 now why do we have this value of 2 why do we like with this matrix given to us have this so here now what if we modify some of these values say let's take this to be 200 let's take this to be 120 stay 130 and 0 3 run it again oh we have this run it again we see that now this value changes to 0 why does it change to 0 now if you notice previously like let's copy this again this is on here let's take now this or let's take this to that okay you notice that with this the position of the maximum value is the zeroth index now if we have here this is zeroth index this is our first index this is our second index your third and then your fourth so where is the maximum value here you guessed it right is 200 where does it fall zeroth index and so the output is zero let's take some example where does the maximum value fall it falls here whereas the index 0 1 2 3 and that's why we have this so actually the arc max is different from a max and it says that yeah we're actually looking for the index or the position of the maximum value so that's it if one I have the mean we just have to change this to mean so we could have mean run that you will see we should get this year so we have 0 1 2 3 that's it there's a minimum value this position is the third position now if it changes to mean 2 we have that we should have this last position 0 1 2 3 4 so that's it now when we did it with multidimensional tensors like this one right here we can specify and exist so let's do this we have we print out TF the math the arc max of X marks or X arc max and that will specify the zeroth axis now we'll look at the output and we explain why we will have the output so here we have 2 2 0 2 2 now let's understand why we have this output given to us notice that right here we have fixed the axis to zero and so that said we are actually fixing the rows so we have a row we have this three rows or let's take this we have this one row we have this row and then we have this row and then we're doing comparisons on the each element or the corresponding elements on each row so it turns out now to be the column so yeah we compare this two that's on this row we compare this two with this three with this 14 and then yeah we compare this yeah we compare this yeah we compare this let's take that off yeah we compare this 3 1 5 yeah we compare 6 8 27 so that's it so when you fix the axis to zero it simply means comparison is done on the other axis so in this case that other axis is a column so we're doing comparisons on each and every column now what is the maximum value here the maximum value is 14 where does it fall in this in the tensor if we have this tensor to 3 14 we see clearly its position if the axis we have the zero position for its position second position so we see that it falls on the second position that's why here you have this too now if we look at this maximum is this falls in the second position that's why you have to we'll continue to this our maximum year is 30 falls on the 0th position if you want to look at it clearly we have 30 extract that we have 30 16 and 23 our maximum false year the 0th index 0 1 2 so that's why you have 0 and then for the next you have 5 and then yeah we have 27 that's why we have this 2 2 0 2 2 now let's take this off and then change this to mean so change that to mean so we have a different angle we have here 0 our minimum is this next is 1 our minimum is this next our minimum is this one minimum is this one next we have this okay so that's it now let's modify this axis so we change axis now we will be working on each row so we'll be doing comparisons on this row comparisons on this and then comparison this so if understood what we've seen previously when axis was equals 0 and we had axis equals 0 then you should be able to pause at this point and then try to solve that for when axis equals 1 so that said let's explain how this works we're gonna have comparisons with this so the maximum years or the minimum since we'll change this to mean our minimum year will be 2 year our minimum is 1 your minimum is 5 so that said we're gonna have 0 since axis position is 0 or we are the 0 index and then yeah we add 0 1 2 3 3 and then yeah we add 0 1 2 3 so we're gonna have 0 3 3 so let's take that off run it and we have 0 3 3 as expected you have other functions right here we have the equal function which could be used to compare tensors so we could compare to tensors and in the case where each and every element is the same we're gonna have this output boolean tensor so you see d type bool and then here we have true true the reason why we have true true year is because this element 2 is the same as this and here 4 it's almost 4 now we see again broadcasting year where we have 2 4 compared with 2 2 now if you have a tool to broadcast it to match the shape so this 2 turns into 2 2 now the first 2 compares with this 2 it is true and the next 2 compares with false with 4 is not the same so it turns into false so that's how we look at this we have the documentation here we could look at other methods we have this power method right here we have yeah we have the power method yeah we basically to get the power or to raise all these elements we're given power let's copy this so we better understand that this is our right here scroll down okay so we have this X we have the Y and then let's understand this power now basically what you understand here is let's reduce this to 0 and then here 1 okay so as we're saying stick this of force we should have 81 there so while we have it you guys are taking 2 to the power of 3 2 to the power of 0 3 to the power of 1 3 to the power of 4 so if we run this this off run this you see clearly we have 2 to the power of 3 is 8 2 to the power of 0 is 1 3 to the power of 1 3 3 to the power of 4 81 so that's how it works now we could have TF the power sorry power and then TF that constant of C2 and then TF that constant constant of say 3 so we have that run it there we go we have 8 on next we will be just reduces we have here the reduce all reduce any reduce Euclidean reduce max mean prod STD some variance let's start with a reduced sum which is kind of like the most popular of all this so we have the reduced sum what goes in here is it competes it takes in the input tensor exists keep that keep dimension set to false by default name known so it computes the sum of elements across dimensions of a tensor so again here we're specifying the axis and we're going to sum through that all the elements in that axis so let's look at some examples this is our right here so we have this input tensor let's copy let's say we have let's work with one of the tensors we had declared previously so let's have this tensor to the right here we have tensor 2d we have it here okay tensor 2d there we go now we want to print this out print of this we have that and then in here we have our tensor 2d tensor 2d there we go we specify the axis let's let it let it let's first run this and see what it gives us its output so we told we have an error oh we didn't close this run that again okay so we have this year first of all this is this 2d you created in the hour let's run this okay so now we have this value of 35 how do we have this value of 35 when you don't specify the axis what goes on yours we just add up all elements so we reduce our elements I will reduce the tensor and then the reduction is done by summing all the elements which make up this tensor 1 plus 2 plus 0 is 3 3 6 11 10 11 16 plus 6 22 24 27 and 35 so that's how we get all this already that's how we get this by summing up all this so this will reduce sum that said we could do a reduce max so we just max there we go we have 8 so what goes on here is we're reducing all these values into just its maximum value so we see with the sum we adding up everything with a max we're just looking at a maximum value so if it changes to let's say we take this to 100 you see you get now a hundred now if you look at reduce mean you run that it should have zero so that's it let's take negative here run it okay we have negative too so that's it now let's go back to our reduce sum and then let's specify the axis let's say the axis is zero let's print back the shape of this tensor to the tensor to the that shape let's run that so we get a shape okay it's 4 by 3 now let's okay we specify the axis zero now we have 7 11 4 or 140 now why do we have this specifying this axis zero means we fixing the axis our way like taking a fixed position on this axis zero and then we paint around with the other axis which is the axis one and which happens with a column because we have the rows axis zero and then we have the columns axis one so what goes on here is we now doing this we comparing already we summing up we reducing this into one we reduce this and then we reduce this so one plus three plus one plus five plus two seven so that's how we have this negative two plus all of that eleven if you add all of this you have 140 and so that's how we get this by when we fix this axis to zero now let's fix the axis to one you'll see that we'll fix the columns and we paint around with the rows so yeah we compare this all right I will sum this up summing this up we have negative one this should be a hundred and eight this should be 12 and so on and so forth so let's run it and we see that we have negative one hundred eight twelve thirteen and that's it so that's how we look at this now we could go back to max reduce max that's it we could go to reduce mean see zero thirty six four fourteen now the mean is like the average value so let's take this axis oh let's let's do that zero run it and we have that now what's the main of this when comparing this we have one plus three four plus one five plus two seven seven divided by four is one point seven five but since we didn't with integers we just have one so let's change this d type let's say we want to have a d type of TF the float 16 so we run that and there we go we have one point seven five so that's it we just calculate in the moon of each on each of these columns because we've fixed we've said to fix the rows and then we paint around with each and every column now we could change this to standard deviation standard deviation we have that zero point eight to nine two point eight six forty one point three eight so that said we now look at the keep deems now take a note look at this shape right here we just have this one dimension tensor here now when we do this to won't send this to true we run that we see that distance or two dimensional tensor with the same elements but we've added this extra axis of extra dimension right here and you could actually get to see this explanation from this so that's it we've looked at the reduces or you reduce max mean STD pro variance none of that now we continue our journey through our mat functions we have this sigmoid which is one of the very popular method the sigmoid we're gonna see this is basically this formula is this guy this function so y equals it takes in X and then what it does it takes the exponential of negative X 1 plus exponential and then 1 divided by 1 plus exponential so basically this is this function and then when we run the sigma we just take each and every element and then pass into this function our next method will be this top k mat function with a top key what we're doing here is we're taking as input our tensor and then we're taking the top or the top say if K equals 2 we're taking the top two values just like in the class of a hundred students and then you want to get the top 10 students oh it's kind of like the same function so that's it we have that let's see how that works in the notebook so yeah we have top K right at TF the mat that top K and let's just take in this tensor 2d right here so we have the stands are 2d we run that you get this output note that this output comes out or with two values so we have first of all this first part of the couple and this second part of the couple the second part has to do with it indices that is the positions of this top K values so by default as we saw the K is equals 1 so by default we have hopes by default we have K equals 1 so that's what we have by default so run that we should have the same answer so that said let's come slide this up okay so what we're seeing here is we going through each in every row so we're going to each in every row we look what's the top value right here is one what's its position zero so we have one year position zero so our value has a hundred position two top value is six position two top value is eight position two so that's how we get these values now let's say we want to take top two top two we see we have the top two values one and zero we have their positions 105 positions six and five that's it eight and three as you can see that's it and by default this is sorted so you don't need to bother about sorting now you could change this to false in case you don't want to do or you don't want to have it necessarily sorted let's now go ahead and look at linear algebra of patients so here we have TF dot linoc good right here we have TF dot linoc and we have all those different linear algebra of patients which come with tensor flow let's look at the matrix multiplication giving yours matmul so TF dot linoc dot matmul what does it do multiplies a matrix a by matrix B producing a times B now note that this is different from the multiply the TF dot mat the multiplier we had seen previously as the TF dot mat the multiplier was an element wise multiplication whereas here we're working with a matrix multiplication so right here we pisses out we have TF dot linoc dot matmul and then let's go ahead and define this two matrices X1 TF dot constant and then X2 so we have X2 just we copy this and then we have that now we have our X1 X2 and then we want to find a matrix multiplication of the two matrices so it will piss here X1 and then we have X2 for now we have all this default values so that's fine now let's run this and then see what we get now what do we have we have an arrow which is normal we've been told that there is a mismatch now why is there this mismatch if we check out the shapes like you know the shapes of X1 and X2 X1 X2 shape you would see that both have shapes 2 by 3 2 by 3 now if you want to do matrix multiplication then this isn't possible since in order for matrix multiplication to be valid the total number of columns we have here must match the number of rows we have here so since there's number of columns which is equals 3 zero from this number of rows there is not it's not possible for us to carry out the matrix multiplication operation so that said let's modify this we have let's now have a 3 by 3 the shape matrix so we would have this for that and then we run it see that works so this works now because this 3 that the number of columns here equals number of rows right here now let's even change this that's let's increase the number of columns if increases number of columns we have this oh we're increasing the number of rows by doing this so let's go back okay let's increase the number of columns to do that we need to do this to let's take two let's take zero so let's run that we see it's still valid because yeah we still have this equal this that is the number of columns here equals number of rows here so since it matches we could still work that out now once you take off so if we take this off you see running this now we still have the mismatch because here we have this to be driven for one let's take one step back we run that and that's what we have now also note that the output of this matrix multiplication has a shape which is dependent on inputs so if you notice the output has a shape 2 by 4 and this 2 here is going from this number of rows and then the foyer is going from this number of columns and that said if now we instead have a 1 by 3 that is if we have 1 by 3 times 3 by 4 our output will be 1 by 4 so let's run this you see you have 1 by 3 3 by 4 our output is 1 by 4 and here's what we get now if you know the voice with this linear algebra terminologies you could check on our linear algebra course available on our platform neural learn AI so you could check on this yeah you will have this course on all this section on matrix algebra you will have matrix multiplication right here and you get to understand all this in depth so you could check on this preview you see how yeah we explained stepbystep with some class exercises how all this works another way you could compute the matrix multiplication of the x1 and x2 is by having this so we could say TF or rather we could simply have x1 right here at x2 so this is matrix multiplication while this is element wise multiplication so we have that add run it we should have exactly the same answer we got here so there we go we have exactly this same answer another very common matrix operation is that of the matrix constables so yeah we'll look at how to do the transpose of the matrix just you just put this dot capital T you run that another way you could compute the matrix multiplication of the x1 and x2 is by simply doing x1 at x2 so we run this you see we have exactly the same response we got from here now we move to another linear algebra operation or matrix operation which is that of the matrix transpose so let's compare the matrix transpose we just have TF the transpose TF the transpose and then we pass in our matrix so we have passed in x1 we run that you will notice that the rows becomes the columns and and you'll notice how rows become columns and columns become rows so let's go back to our x1 let's print out x1 side here x1 there we go we have this x1 right here we see how the shape is 1 by 3 and that of the transpose is 3 by 1 it still contains exactly the same numbers but now this matrix that's the initial x1 has just one row you could see one row whereas here we have three rows so this becomes this this two becomes this this zero becomes this now let's take this now with x2 so we run x2 or rather let's let's bring out x2 first so we have x2 right here now notice how this one two zero that's this first row 1 2 0 2 becomes our first column this second row becomes our second column and this third row 5 4 4 5 6 0 becomes our third column so we have a year 4 5 6 0 so this is basically the transpose operation now how is that related to this matrix multiplication you may have noticed here we have transpose a false and transpose B false so in the case you were trying to multiply x1 by say x2 transpose you just have to put B true recall that x2 is B so because initially we had air I am B so if we multiply this or rather change this to true now you be multiplying x1 by x2 transpose let's run this and we should have an error this is normal why because we're multiplying x1 by x2 transpose now what is extra transpose let us let us say we have TF dot transpose transpose x2 so there's a transpose and then we get a ship so we run that and we see that we have 1 by 3 times 4 by 3 we see that this 3 is not equal to 4 so the number of columns here is not equal to number of rows right here and so we cannot do the matrix multiplication so that set that's not valid so let's modify this let's say we take let's create x3 so we could have x3 which has four columns so we could have x3 with four columns let's change this x3 and then yeah let's say we have this just two okay so we have that we have that we'll print out x3 so we print out x3 shape x3 that shape there we go we run that we have x1 x2 x3 x4 now notice how so it is x2 so this is x2 transpose now because this is three four three four tens of four three so it's normal so we have x3 what we're trying to find here is x3 at x3 matrix multiplied by x2 transpose so matrix multiplied by the transpose of x2 that's it so let's in this out in the out see if that's valid so we have in this error TF TF the transpose so there we go we see that we are having an output right here which is two by three that's because this far is the same as this four and then our up but will take number of rows two and then this number of columns here to read so that's it so it kind of like takes the this output this outer axis that we have here so that said we have in our transpose of x2 and then we multiply x3 by 8 now how do we have this exact same response without necessarily having to say for example TF the transpose x2 all we need to do is just specify that our x2 is going to be transposed and here we have x3 so we have x3 and then x2 so it's just like we multiply x3 by x2 but we're saying that our x2 is going to be transposed so that's it we run that and we should have exactly the same response so that's it we have exactly the same response is doing this is the same as saying x3 times or actually matrix multiplied by x2 transpose so we'll send transpose B is true now if you send this to true this should let's check out what is gonna be shown we have an x3 x3 shape is 2 4 so if we have the transpose of x3 is gonna give us 4 2 times the transpose of x2 x2 is actually let's check actually x is actually 3 4 the transpose is 4 3 so we have 4 2 by 4 3 it's not gonna work so when this they shouldn't work we're having 4 2 by 4 3 now if we change this because we have an x2 transpose let's say x2 transpose is 4 2 and then x3 transpose is 4 3 so for this to match we must modify this so if we change x3 to a 3 by 2 matrix let's have that rather yes actually x3 so this x3 and this x2 so let's modify x2 so that we have a 3 by 2 matrix and if we modify x2 to be a 3 by 2 matrix then x2 transpose will be a 2 by 3 matrix and if we have 2 right here this matrix multiplication will match so let's go ahead and change this into a 3 by 2 matrix 3 color 3 rows and then 2 so let's take this off take this off take this off that's fine okay so x2 now is 3 by 2 and x2 transpose is 2 by 3 so they should match let's comment this out that's fine and run it so yeah we have in an error okay the error is coming from the fact that yeah we need to put a transpose so initially have TF the transpose because now we're doing x3 transpose times x2 transpose so we're gonna have this TF the transpose here that's fine so let's run it now and everything should work fine see we have exactly the same response because we actually do an x3 transpose by x2 transpose this shapes here now match now the way we look at this next arguments here does the adjoints is exactly the same as for the transpose and I join it I joined in a matrix is another operation which will not get into the details but just note that specifying this or saying that I join of a is true for example is the same as just saying we're taking the adjoint of x3 and multiplying by x2 if we say if this is false and then here is true so if we have this then what we're saying is with multiplying x3 by the adjoint of x2 so it's the same as or similar to the way we treat the transpose arguments right here let's now look at how to multiply matrices with greater than two dimensions so let's go ahead and take this tensor 3d we had defined previously we have tensor 3d right here we just put it out down right here we have tensor 3d we've now defined this to three dimension tenses x1 and then x2 so there we go we run this and this is what we get so we have this two three dimension tensors and we want to do matrix multiplication so we could simply have TF the mat model or TF the lean log in your algebra the mat model we have that and then we specify x1 and specify x2 running this should produce an error this is normal again because there is no matching the shapes now how do we look at this now the way we look at this is quite straightforward what we do is when having these kinds of matrix multiplication where matrix multiplication is done in batches what we do is look at each and every batch and look at its corresponding batch in the other matrix and then do the matrix multiplication so what we have here is we're multiplying one this matrix here this 2d matrix by this matrix now if you have to multiply this matrix here by this matrix and then later on this matrix by this matrix and then this matrix by this matrix now you must make sure that there is a matching in the shapes now this matrix right here is 2 by 3 and this is 2 by 3 we've seen already that 2 by 3 multiplied with 2 by 3 wouldn't work because the number of columns here is 3 and number of rows here is 2 which don't match so what we need it do now is we could modify this tensor such that each of this matrices right here have a shape which will match with this so that we could do the matrix multiplication so let's change this now into 2 by 2 matrices so instead of this take this off take that off 2 by 2 comma and finally here we have this comma and that's it so now we have X1 which is 2 by 2 and then X2 which is 2 by 3 2 by 2 2 by 3 will output as 2 by 2 and then 2 by 3 will give us an output which is 2 by 3 so it's going to be 2 by 2v output now let's take that off and then run this while we run that we see this output let's take this off let's take X1 X2 off so we take that off run it again so we could see clearly our output now notice how what goes on here is we have this matrix multiply with this this with this so on and so forth now if you want to check this let's go ahead and recreate this right here so what we're gonna do now is we're gonna just take let's take for some this middle values here off take this off take this off there we go we have this okay so that's fine so now we have this okay we have this to we have this matrix does this to the tensor and then let's take this middle value because we chose the middle so let's take this take this off off and then yeah we take this off okay so we have X1 and X2 and then we're trying to print this out run this we have 10 20 20 11 8 6 so taking the middle values even look at this carefully you see we have exactly the same response so this confirms the fact that while we're doing what we call batch multiplications where a batch is considered to be one of this indexes in the zeroth axis so we have this zeroth axis right here we have three indexes so each and every one of those indexes is considered to be a batch so when we're doing this kind of batch multiplications we just take each and every one and multiply with its corresponding value in the other metrics from here we explain why we are how we use this sparse or this be sparse on this a's bars arguments that we have right here now the way this works is sometimes we have matrices which are full of zeros so we may have a matrix like this let's modify this and then here we have zero here we have a zero yeah we have zero and then yeah we could have a zero zero and that's it so we could have matrices like this which are made of mostly zeros and so what happens is tensorflow has a way of optimizing competitions involving tensors which are mostly made of zeros this type of tensors are known as sparse tensors so when you specify that tensor a particular tensor is sparse tensorflow takes that into consideration when carrying out a computation and this helped us carry out computations even faster since tensorflow now knows that this matrix or the particular matrix or particular tensor is made of mostly zeros so that's it we could check on other methods like the adjoint method which we spoke of previously this is adjoint method right here I mentioned to get the adjoint of a matrix very easily let's now look at the band part method and this band part method we are actually rewriting the tensor but setting some values to zero based on certain conditions so here we have an input we have a num lower and a num upper we have the name as usual so right here we have an input which could be of key dimensions as is defined right here and then we have these conditions so we have this indicator function which is say in band and with those conditions and then we take this in band and multiply by the input matrix right here to finally get the output so let's look at this example right here we see that we have this input we have this which is passed into the band part method and then we have this output you'll notice that this output looks similar to the input what a difference that at some positions you have zeros so this negative 3 is 10 to 0 negative 2 10 to 0 and this negative 2 you're 10 to 0 so that's how we look at it before looking at the special cases let's take an example so we understand exactly how this works note that here this indicator function is defined such that for each and every element of this new matrix in band because we're creating a new matrix in band such that each and every element is defined such that if this num lower num lower is passed in here so if for example like this num lower if the num lower is less than 0 or n minus n n minus n happens to be the position of each element in the matrix on the tensor so if n minus n is less than num lower less than 1 and if num upper is less than 0 or n minus m is less than or equals num upper then this in band will be such that when multiplied with this corresponding element in the input then that input remains the same but if this condition is not verified then that input turns to a 0 here we have this tensor this 2d tensor which was defined we have that condition which was given to us in the documentation and then what we do is we're gonna define this two matrices one is m minus n and the other one is n minus m notice how this tool have exactly the same shape as the input so what's going on right here what happens here is we have m first of all you have to understand that m is for the rows and then n for the columns so what goes on here is we have at this position we have the zeroth row zeroth column so m minus n that's 0 minus 0 is equal to 0 at this position we are at the zeroth row and the first we are not allowed to 0 minus 1 times so this position 0 row minus second column 0 minus n and we have our position so we have this row 0 negative 0 let's call it 0 1 2 0 0 1 row here finally 9 second row alright we add all first row in this column so we have one row and here we add first just row second column 1 minus 2 negative 1 and then we continue with this and we'll get all these values so this is how we get m minus n we're simply taking the index of the row index minus the column index now we look at n minus m for n minus m we're looking at the column index minus the row index so yeah we are the zeroth row zeroth column 0 minus 0 0 here we add the first column and the zeroth row so 1 minus 0 is 1 here we add the second column and the zeroth row so 2 minus 0 is 2 so that's how we get this n minus m matrix right here now once we've got an m minus n matrix and n minus m matrix we can now see how to get the output very easily so we have this output for now we replace it by this axis and we are gonna get for each and every element its exact value after going through this band part method right here since we have as input our lower to be 0 and our upper to be 0 we could take this condition off because our lower will never be less than 0 so we could take that off so we could have this out there we go and now we'll be focusing on n minus n so at this position n minus n which is 0 is actually less than or equal is actually equal the lower so our lower is 0 and our upper is 0 so we have n minus n here is it it's actually less than or equals our lower so we have that to be true and then n minus m which is 0 is actually less than or equals our upper since our upper is 0 so that's true so since all this is true we have these value at this position that's the zeroth row zeroth column position that's when m equals 0 this is where m equals 0 and n equals 0 so at this position where m equals 0 and n equals 0 this is gonna be maintained so this one is gonna be maintained so that's how it gets output maintained now we move to this next one year so in this next we are having m to be equals 0 and then n moves to 1 while the first column we come again and check here m minus n we have a negative 1 negative 1 is it less than or equals our lower yes that's true it's less than or equals 0 and then n minus m n is 1 m is 0 that is 1 is 1 less than or equals upper no that's not true 1 is not less than or equals 0 so this condition is not fulfilled since this is not true so both must be true so since this is not true this value this negative 2 is 10 to a 0 so here we have a 0 now we'll move to the next we do the same here we have a 0 now what choice we're always gonna have a 0 because if the 0 is maintained we have a 0 if we turn to a 0 we still have a 0 so no need chicken on this now we'll go to the next we have to read for this we are at m equals that's m equals 1 and then n equals 2 no n equals yeah we at m equals 1 and then n equals 0 sorry so we have n equals 0 now we have n equals 0 we could compute this m minus n 1 minus 0 is 1 but 1 is not less than 0 or it's not less than or equals 0 so this is not true so this 3 turns to a 0 so yeah we will have a 0 that's how we get the 0 right here now we go to this 5 we have m equals 1 and n equals 1 in this case m minus n is equal to 0 which is less than or equals 0 so that's this is true this here is true and then n minus m is 0 this is also true so we maintain this value so this turns to a 5 when we get this we have m equals 1 in this case and then n equals 2 so we have 1 right here and then we have 2 okay n minus n we have 1 minus 2 negative 2 negative 2 is actually less than 0 so this is valid and then yeah we have n minus m 2 minus 1 1 1 is not less than 0 so this is not valid that's sad since this is not valid this 100 turns to a 0 so that's it you could just repeat this and what you should have will be something like this so yeah you should have 0 and then yeah you should maintain this value and yeah you have 8 here you have 3 and then here you have 2 so that's how you get this oh yeah you should have 0 actually this is 0 here 0 now everywhere if you notice everywhere m is different from n you will never you will never have a situation where m is different from n and then m is less than equal to 0 and at the same time n minus m is less than equal to 0 so that's why all the other values of 0 except for this diagonal values so you see here 1 5 6 I maintained now let's test this so with this command that and then we run this so there we go as a response we get this comes to confirm what was said here where the give this useful special cases and we told that whenever we have this 0 0 that's got an input with 0 0 the output is going to be a diagonal matrix so diagonal tensor so right here we understand why we should have that diagonal now if you repeat the same process you should be able to see that 0 negative 1 we give upper triangular and then negative 1 0 would give the lower triangular now let's go ahead and see what this upper and lower triangular tensors mean so yeah let's take this to negative 1 we run that and what do we notice we notice that this matrix of the input that stands out to D does it here is actually maintained so 1 3 1 2 see everything's maintained except for the upper part now if you have a matrix like this and that you have this diagonal and that all this this is what we call the lower triangular part of the matrix and this is the upper triangular part of the matrix so as you could see here this upper triangular part of this matrix is all zeros and then if we take this to 0 so we have 0 negative 1 if you have 0 negative 1 and you run that you'll notice now that is instead the lower triangular part so here we have this lower triangular part this lower triangular part right here which is not made of all zeros so that said in case you want to get this lower triangular you just have to specify 0 negative 1 upper triangular you have negative 1 0 and then when I want to have a diagonal matrix you just need to specify 0 on both sides that's it for this band part method we look at other methods we have a color scheme decomposition so here we have a way of decomposing matrices we wouldn't get into that here we have the cross product this is completed pairwise cross product cross product of A and B so here you could complete a cross product of two input tensors here you could get a determinant of a tensor so we have an input you could get this determinant we have many other interesting methods here we have the inverse linac dot inf we have your tf dot linac dot inf tensor 2d we run this we should have an error so that's normal so this is normal because to obtain the inverse of a matrix that matrix must be a square matrix that is a number of rows must be equal the number of columns so that said let's recopy this and then put it here we have we really find this answer 2d take this off that's it and then we now run this again we told you I cannot find device for node and we given a set of data types which registered for the operation matrix inverse so this means most probably our data type isn't registered in this matrix inverse operation now we could search on stack overflow we click on this what we get here I think your value should be float as error says I have encountered the same problem while finding the determinant of a matrix and change the detail to float 32 and the polynomial solved so here we have a great way of taking for this kind of errors but nonetheless we understood already that this should most probably come from the data type we're working with so let's change this to float 32 as it was set by the stack overflow user and we have now this other error which corresponds with the error we expected so here we are getting this error and we told that let's take this off we told that oh for doing from three now this is normal because as we said the number of rows must be equal the number of columns for us to find or calculate the inverse of that matrix or the tensor so that said what we're gonna do here is if we take this off and create a 3 by 3 matrix what we have now will be the right answer so we see we found an inverse of this matrix and this inverse so let's let's put this in a variable as a tensor to the inverse so this inverse is such that when you multiply so let's run this when you multiply tensor or when you do a matrix multiplication not actually not an element multiplication element wise multiplication when you do a matrix multiplication of this tool you should obtain the identity matrix so here you have tensor 2d inverse around that and you see that you have the identity matrix so here you have 1 0 0 this is 0 very small number 1 0 this is 0 0 1 so here we have the identity matrix taking even in the documentation you see that we're given a list of accepted data types and the float 16 which we were using previously was in a month so you have to ensure the input data types are in this list we have this matrix transpose here similar to the T of the transpose we have seen already so that's our transpose we'll see the matrix small that's it we have the trace we have the singular value decomposition of a matrix so here you could obtain the matrices singular value decomposition this returns three outputs s u and v where s is a tensor of singular values u tends of left singular vectors v tends of right singular vectors so the SVD is a way of breaking up a matrix in a way that less important information contained in a matrix is eliminated so that said we could check this out right here we have SVD taking the tensor so we could simply just pass in SVD and then get the output so here we have SVD of tensor to the SVD of tensor to D there we go as we did not defined TF the Linux the SVD we run that and then we get this output so we have this three outputs now let's define this s u v equals that and then let's print out s so print out s first to see the singular values there we go we have the singular values we print out u we have tensor of left singular vectors and then we print out v to give us our tensor of right singular vectors so this is what we get right here so feel free to always look at this documentation and the more you use these methods the less time you would even need to always come back to documentation so just make sure you keep practicing so you get to master all those methods we now go ahead and look at a tf.ensom method in terms of flow you could have it as tf.ensom you could look at this documentation right here with some examples before diving deep into understanding how this operator works it's important to know that this operator uses or takes in arrays of all sorts of dimensions that is one the arrays to the arrays up to n the arrays so we'll start straight up with this example with two dimensional arrays I will see how the handsome operator can be used to replace the usual matrix multiplication by matrix here will simply meaning a 2d array and this example will suppose that we have this array A and then this other array B A is of dimension of shape 3 by 4 B of shape 4 by 5 in other for the matrix multiplication to be valid we have to ensure that the number of columns of A have to be exactly the same as the number of rows of B in that case we have to ensure that in generality this J has to be the same as this right here and that's the case for this example and so the matrix multiplication of A and B is valid and what plan this with this would give us C but also note that the shape of C depends on that of A and B as you could see here if we kind of merge these two shapes and then take off this same shapes does this column for the first matrix and then this row for the second matrix would have 3 by 5 so look at this outer values right here this 3 and this 5 and that's what gives us the shape of the output matrix C in general if you have ij jk then we should have an output ik so that's how we have this right here it's very important for you to take note of the shapes whenever you're working with an handsome operator okay that's understood let's now dive into the code we started by importing on pi and B we have that and then we have this two matrices A and B which we've defined already so let's command this for now we have the matrix A which is on the slides and we have the matrix B as you could see printing out this would give us A that shape now we'll print out B shape so we have A that shape B that shape gives us three four and then four five so three by four matrix and then a four by five matrix right here as B and that's it now let's look for the or let's find the matrix multiplication of a and B know that and non pi just suffice to have m beta mod so this matrix multiplication would as an A and then B so there we go we run that now we'll find this is the answer we get which is correct this is our C so we've got an RC now how can we replace this matrix multiplication with the any some operation in order to do that just take note of this syntax right here notice how we have first of all this screen right here so we have the string and what do we put in the string first we have this IG and then next we will follow this IG after the idea we have a comma and then we have the G key and then we have this arrow to I K now this can be read as a interacting with B to produce seed and the way we chosen this I J J K such that it matches up with the shapes of A and B and with a kind of operation we are dealing with given that with a matrix multiplication we have a so supposing it's I J and then if we have a B then we must ensure that the J we had here has to be the same as a J we have here so this is just like the row so either row J the column and yet the row and then you're the color so we have to make sure that the number of columns equal number of rows that's why you have the J and J right here and then since the output is such that it takes the the shape of the output is I K that's what we specify here so we've taken a B and then transform into a C now whatever you have before this transformation has to be passed obviously so that's why we have this I J so the comma here does that we have this I J match now with a and then this J K matches our would be so we've kind of like separated the two different array is to be passed and then we have this I see which is output we get that's why when we print this out now we're gonna have an output so this I K is output we see clearly that this gives the same answer as just doing the matrix multiplication well at this point you will be down like why do I need to do all this when I could just simply put in the matrix multiplication or NP dot mall a B and then I have my answer well that's a very common and normal question and if you follow to the end you'll see that in some examples are in many applications you find that working with the ASAM operator is gonna be easy than working with the usual nonpi functions or methods you used to working with so let's go ahead we've had that I will now understand the ASAM syntax let's go ahead and look at many other examples for next example will be using the ASAM operator to do an element wise multiplication let's take this example right here we have a that's the symmetrics a we have the symmetrics B obviously for LNY element wise multiplication we have to ensure that the two matrices A and B have the same shape so as you can see from this we know a shape and B shape we have 3 by 4 and then 3 by 4 let's comment this for now so we have this tool right here 3 by 4 3 by 4 let's go ahead the computer element wise multiplication the Hadamard multiplication we have that there we go we see that we have 2 times 2 gives us 4, 6 times 9, 54 and so on and so forth so just simply each element on this position multiplying with this corresponding or the corresponding element on the other matrix like if you just pick up this random we have 4 times 5 20 so that's the output here now in this case you see that if we have this matrix A right here of shape ij then B has to be of shape ij and then the output has to be of shape ij and so that's why you would see let's recommend this part right here you'll see that we have this input ij or the first input ij which is a the second input ij and then the output ij so the ASAM operator automatically sees this as a Hadamard operation so here we're gonna have the right output on that oh let's take this off and that's it so there we go we see that we have the same output as with the usual Hadamard operation or the usual element wise multiplication for next we go on to matrix transpose so at this point I'll urge you to like take a pause try to come up with a corresponding matrix or the corresponding ASAM operation for the matrix transpose before continuing with a video now hopefully you got it right you have this matrix A right here which is to be transpose to see we have 2 2 1 6 negative 2 5 and so on and so forth so basically we have a matrix which is of shape ij and has an output ji in this case would have say print the ASAM transpose of A is equals the prints of mp.ansam we have that mp.ansam of ij which is being transformed to ji and then we pass in the matrix A so let's take this off right here and then run that there we go we have exactly the same answer so yeah we pass in A we have this ij no need for any commerce that's if you have to multiply let's say we have a b c g so on and so forth then you have say ij jk k l l m yeah we have four right here and does it so now we have just one so we have that let's take this off again that's fine so hopefully you had this correct we now move on to working with three dimensional arrays in machine learning is very common to how to deal with this type of arrays let's suppose we have this array A which is a 3d array of shape 3 2 by 3 by 4 now let's see the way we can read this we have this old array A and then we have this two boxes in this two arrays this one and then this one so that's where we have this two dimension right here and now for each of these that's for each of these arrays we have 2d array in it that's for the first one that's for this first box right here we have this 2d array which is of shape can be that's 1 2 3 by 4 so that's why we have 2 and then by 3 by 4 so all those a threedimensional array of shape 2 by 3 by 4 we could generalize this as B by I by J and then for B we have a 2 by 4 by 5 dimensional array now note that we have here that those B's could be taken as a batch size and this is because in general or many times in machine learning computations are done on batches of data so in this case we could have this as a single batch that's this box right here as a single batch and then we have this other one as another batch giving us two batches so we have the batch size equal to and then we may want to do matrix multiplication or batch matrix multiplication where want to multiply this matrix right here of this 2d array with this one and then get this corresponding output and then multiply this one with this one and then get this other corresponding output and we may not want to say use a for loop to say for each of this we multiply this down as they used to get output and so on and so forth we may just want to use a single operation which understands that this multiplication is done for every position in the batch also note that in this kinds of computations the batch dimension remains exactly the same we have here 2 2 2 while 3 4 multiply with 4 5 gives 3 5 so we still have this exact same value of B which is maintained everywhere now that's that let's go straight away into the coding we have this 3d array a right here and of this to 2d arrays the sort of one this one and then this other 2d array we could come in this part and then now we have to print out a that shape and then B that shape there we go Oh print that's fine so 2 by 3 by 4 2 by 4 by 5 now note that the matrix multiplication can permit us do this without any problems that is we could get the we could do the batch multiplications with the nonpise matrix multiplication as nonpise matrix multiplication understands that when data is placed in batches like this all we need to do is to ensure that each element in the batch multiplies the corresponding element in the batch in the other array now if we want to use this we could also use the N sum operator now the way we use the N sum operator here is by again specifying the shapes so we have B I J for A and then we have B J K for B and then for C we still have B I K notice how we have I J J K and I K while B remains the same so with this around this and we'll see that we have exactly the same answer for the two outputs that's when we will use the matrix multiplication and when we use the N sum operator and still up to this point we see that we could always we may not necessarily use the N sum operator now what if we want to sum up all the elements in a given array so yeah we'll look at another different way of applying or making use of this N sum operator so yeah we have this matrix or rather we have this 3d array a right here and then we want to sum up all the values in the array yeah we have the sum gives us a value of 72 and then if one of you is the N sum operator what we'll be doing is we have the shapes that's B I J put an arrow and then when you don't have any output while you're simply doing is you're summing up all the different possible values so that's what this signifies right here so it's just like saying we're summing up all the possible values then we put in or we make sure we have the right shape of a put right here notice I will do this you see we have an arrow so we'll correct that we have G on that and that's fine we could also sum up all elements in a given dimension or given row given column say we're working with a matrix here we're having this in sum let's just this okay so yeah we'll see that we have this matrix a shape IJ and now when we specify only one of this axis right here what we're doing is we're summing up all the elements in each column because the J is like the column so there's a row and then there's a column so we're summing up all the elements in each column and that's how we have this we see we have two for the first column we have two plus two plus one five six plus minus two plus five nine five plus two plus four eleven two plus three zero five so that's how we have that so the way we understand this is we just summing up all the elements in each column and then if we change this to I will summing up all the elements in each row around that you see that for this row we sum this sum this now we sum that now moving to a more practical example and this attention is all you need paper you will have yeah this formula right here where we have the query multiplied with the transpose of a key if this key has a shape of by size by sequence length of the key by a model size and the query shaped by size by sequence length of the query by a model size then we could define this nonpy array query of shape 32 by 64 by 512 and key 32 by 128 by 512 and then what we will do now is define the query by transpose operation where we would have MP the in sum of this is considered to be say be the by size by cure by M and then this is considered to be say B by K by M so yeah we would have B cure M comma B K M which outputs B now at this point we have to be very careful we have cure M times K M transpose so this becomes B cure M times B MK and now when we multiply this cure M and MK what you have is cure K so yeah we have B cure K let's take it away again we have B cure M and B K M by dimension doesn't matter take that off so we have this now and then what we transpose is we have MK and when we multiply this we left with cure K so that's how we have that and then here we have as input cure and then key so let's run this and there we go we have this output which is of shape with verified shape which is of shape cure key we have your cure which is 64 by key which is 128 so we have 32 by 64 by 128 output right here our next and final example inspired from the reformer paper in this paper the others explain how the data could be breaking up into chunks and then the attempts of each other again you don't need to understand what's gone on that paper but it's just for you to get an idea of where this problem were to present comes from now supposing we have this matrix A of initial shape 2 by 4 by 8 all to keep them simple let's say 1 by 4 by 8 so we have a batch size of 1 we have a sequence length of 4 and a model size of 8 here you could see this one that's the only single batch we have here then we have four rows and then eight columns B2 has a shape 1 by 4 by 4 so we have four rows and then four columns and the paper we just saw they further break up this data into buckets so and so right here we have four different buckets now we see that this eight columns are broken up into separate four by two matrices here we have the four by two matrices we have four rows two columns four rows two columns and so on so forth now the same is done for B with a difference that would break this far up that would break the four columns up until four by one matrices where we have four columns and just all right at four rows and just one column in the paper there is a task which involves taking the transpose of this B and multiplying by a and in doing that we are in fact multiplying the corresponding batches and the corresponding buckets so you see that again here we have some sort of two fixed dimensions and then we have this dimension or this matrix here which is actually gonna be involved in the multiplication process and so if we have a defined as B C I J or the shape B C I J and B B C I K multiplying the transpose of B or getting the transpose of B would give us B C C B and C fixed and then I K turns to K I and A is still B C I J now we have K I times I J giving us K J so we have B C K J so if we have this 1 4 4 2 by 1 4 4 1 we have 1 4 1 2 and now with a Einstein operator if we have the a given as this here we have B C I J and this B C I K so let's just put in common so we have this B C I J B C I K we had seen already on the slides on that now we're trying to find B transpose by a so what we're gonna have here is nonpiler and sum of B C I K this year we have B transpose times a so we have B C I K comma B C I J which is gonna output B C now at this point we recall that we're gonna have the I K which is gonna be interchange to give us K I K I times I J gives us K J so when I have K J just as we had in the slides and then we pass in B and then pass in a so run that and then look at the ship that's it so we see that we're able to compute this very easily with an answer operator as compared to the usual map mall where we have to ensure that the matrix B is transposed correctly so yeah we're gonna have MP that transpose of B and then we have to ensure that we have zero one so zero one fixed and then we have three two because this is why we do the transposing and then we now passing a from this so we have that and then there we go we still have the same shape we could take this off you see that we have the same answer for both methods but this method looks clearly is cleaner than this one as if we have to go to say 5 or 5d array we'll just let's say for example we have six year and then we have six all we need to do here is to say for example D so put that D that way D and that's fine so let's run this again see the ship and that's fine but with this we have to say 0 1 2 and then we would have to ensure that yeah we have four and then yeah we have three for that to work and so this is an example showing how the answer operator comes to make our work clearer and easier to understand let's now look at the expand deems method with this method we get to add an extra axis to an input tensor and that extra axis has a length of one if we suppose we have this tensor for example tensor 3d then let's print out the shape 4 by 2 by 3 now what we could do is we could add an extra axis such that this tensor 3d now takes a shape of 1 by 4 by 2 by 3 that's that what we do is tf.expand deems and then we pass in tensor 3d tensor 3d that's it and we specify the axis so axis specified to be 0 we're gonna have this output shape so let's run that and then put out a shape here so that's it now let's take a simple example suppose we have X equals tf.constant and then we have this right here two three four five for example we could print out X the shape and then we could print out tf.expand deems of X and then specify the axis to be zero you will see that we're gonna have this shape 4 and then we have this output let's put this shape right here we have this output 1 by 4 notice how this tensor leaves from a 1d tensor into a 2d tensor still we have the same elements but we've added one extra axis right here this extra axis would have been added manually by doing this so if we had done this would have had that extra axis we run that you see we have one by four and now doing the expand deems we have one by one by four so now we'll have from 2d to 3d and then if we keep doing this you see how we'll leave again from 3d now to 4d so we have one by one by one by four so that's how we look at the expand deems method if we want to do this expansion or this addition of this extra dimension in a different axis we could specify that axis let's say the first axis axis equals one notice how we'll leave from 4 to 3 now to 4 1 to 3 so we kind of like fit in this year let's take this off kind of like fit in this year so when you say axis equals 0 is fitted right here x equals 1 is fitted here as this is called so be fitted here as equal to 3 fitted here as as equals 4 invalid so that's it let's take as equals 4 for example you see it's not valid you have 3 so now you have 4 by 2 by 3 by 1 from here we look at another method which is like the opposite of this expand deems method here we have the squeeze method with a squeeze method what you have is you taking this input specify the axis just like similar to the expand deems method but here we instead remove the dimension of size 1 from the shape of the tensor so let's get back to this we had around this and we had let's take this of X squeezed let's define X squeezed sorry X expanded X expanded is this that's from that we have X expanded 1 by 1 by 1 by 4 now let's do X squeeze X squeezed is equals TF the squeeze and then we pass in X expanded so firstly we're gonna specify the axis to be equals 0 and then we print out our X please so we have X squeezed right here X squeezed print it out and we should have this 1 by 1 by 4 so 1 axis has been taken off let's do this several times so here we have 4i range see let's do this twice copy this pieces out here there we go X squeezed X squeezed so we start by squeezing the first time and then we continue squeezing let's print out X squeeze here so here we have X squeezed that's it we run that and we should get this so there we go we've left from this 4d tensor to this 1d tensor coming back to this other example where we had the shape that is we have expanded such that this extra axis was added in the last position so yeah what we're gonna do is do a TF the squeeze with X which is an expanded and then specify the axis so now we're gonna specify the third axis because we want to squeeze out this third or last axis right here now before squeezing out that last axis let's take let's suppose we're trying to squeeze out this zero axis you see we're gonna squeeze out this because we expect the dimension or the length of this to be equals 1 before we could squeeze it so that said if we specify now 3 you see that way it's fine so now we have 4 by 2 by 3 record that this was X exp that shape was actually 4 by 2 by 3 by 1 so squeezing out now wait this works now because we specify the right axis which is of length 1 so that said we've looked at a squeeze and the expandings methods which have to do with modifying or reshaping tensors let's look at the reshape method so here we have our reshape method as usual we have the definition reshape the tensor and then we have some examples so right here we'll see how we're modifying or reshaping this 2 by 2 tensor into a 1d tensor before looking at that example let's take a look at how we could use the TF the reshape TF the reshape to modify the shape of this X expanded that is this X which produces the shape such that we have an output of 4 by 2 by 3 so there we go we have this we pass an X expand it and then we specify the shape we want to get us output or the new shape of our tensor so yeah we specify 4 by 2 by 3 we don't want to get a tensor so let's put that shape even here let's take this shape we run that and this is what we get we see that we have exactly the same output shape right here now let's change this remove this and then we compare these two tensors and we see we have exactly the same outputs so this tells us that what we do we did with a squeezing could be possibly done with a reshaping now let's take another example supposing we have this tensor X reshape we could actually reshape this as we could have TF the reshape and then pass in X reshape such that we modified our changes into 1d array so or 1d dancer so if you put this to save 6 we run that we should have an error so invalid syntax well this is different from the error we expected okay so this is the exact error we get to input to reshape the tensor with eight values but the requested shape has six so that said the fact that you could reshape doesn't mean you just be able to modify any tensor and then change that to just any type of shape you have to make sure that when we shaping the number of values that you are getting the initial tensor actually fit in this new shape of this new form you understand so to take so you have eight values and you don't expect to reshape this into six values so you have to put this eight so running that now you see how we've reshaped this and we have three five six six three five six six and then four six negative one two so it's kind of like we'll flatten out this matrix now let's do another taking a look at another reshaping we could change this to four so here we have four by two now we're changing all these two by four these already two by four so we could put it four by two four by two we run that so yeah we have two by four now we could change it to four by two notice that this actually isn't exactly the same as the transpose of this matrix so we're shaping it from two four to four two doesn't find get the transpose of the initial tensor extra shape we could change this again let's say we have four to four to one we run that see that that works just fine and this is because with this we expect eight values and here we have this eight values now it may happen that you will be dealing with tensors wearing you don't exactly know whether this is a four by two tensor you want to get as the output of this reshaping so you can just put a negative one and tensor for automatically knows that this is two so if you run this you would get this output right here let's bring this out let's have this bring it out so we print this out copy this print it out we have to run it and we see that we have exactly the same output now if you just put in negative one yeah you would expect to get the same output as this so putting this you see it automatically understands that the value should have here should be eight we now look at the concatenate function that's T of the concat which permits us concatenate tensors along one dimension so right here we have this values as input we have a specified axis and then we have the name so at the bottom yeah you can have this value you could be a list of tensors or just a single tensor now let's look at this example so let's just copy this example from here so we're going to copy this example we paste this out right here we have T1 T2 and then let's look at the output I'll get the output of this before getting to the output we're gonna have print T this is actually a list so let's say print tf.constant T1.shape and then we print tf.constant T2.shape so we have that so we've been opening out this to T1 and T2 and then we're concatenating them let's run this we have the shape 2 by 3 2 by 3 and we have this concatenation which is 4 by 3 now what do we notice we notice that this first part here this first two rows first two rows actually T1 and the next last two rows T2 now the reason why we have this is because we'll specify this axis this axis right here to be equals 0 now saying that axis equals 0 simply means we're doing concatenation across the rows doing row concatenation means we're taking this we're taking this first tensor taking this other tensor and then we are completing the rows of this T1 right here so it's just like we were taking let's copy this so this concatenation operation is just like this we have in this year completing like this and there we go so this is exactly what we're doing we actually just completing this and then having this so that's why yeah when you look at this you see it's exactly the same as what we have here so that's how the concatenation operation is done to go back we have T2 T1 now we could modify this and take to one now if you specify the axis to be equals one it means we're doing concatenation across the rows or rather across the columns so that said instead of adding extra rows as we just did previously when the axis was zero now we add in extra columns so we come here and we add extra columns so here we have seven we have eight we have nine and then here we have ten we have eleven and then we have twelve so that's what we're gonna get let's go back take that back that's fine and then let's run this and see what we get we see that we have one two three seven eight nine so we add the extra columns and then ten eleven twelve so with the axis equals one that's when we were specifying that we're working on the columns we add in extra columns when axis equals zero that's what the rows we add in extra rows now what if we are having 3d tensors or let's say we're having a 3d list right here let's take this and that and then this you look at you'll know the shape will get from here so this is going to print out the shapes and then let's specify this is zero so we run that and then we have one two three so shape one by two by three one by two by three and then our output here is two by two by three so in fact we're doing the concatenation on the zero axis which is this axis right here so our output is one plus one now we'll go back slightly so let's get back to what we had previously there's a point we didn't mention so let's run this again we have this now what we forgot to mention was that the output that is when we specify axis equals one our output will be we go to the axis one and then we add this to up so up will be two so we fix the other axis and then we add this to up so we have two by six now when is zero we're gonna fix this other axis and then we're gonna add this one up to make four by three so let's run this I'm gonna get four by three let's go forward we will run this again and this is what we have so as we're saying simply specify axis zero we fix the remaining axis and then we add this up so here we have two by two by three now what if we specify the axis to be equals one if our axis equals one our output should be fixing we fix the one and then we add this to up one by four by three so same way concatenating on the first axis here means we'll be stacking up the rows and we'll get this kind of output so let's run this there we go we have one by four by three and then we have this year which is this and then this other one which is the t2 if we had to do this for axis 2 we would have this inspected see we just stack this up on the columns added extra columns and then here we added other extra columns right here to obtain this new tensor we have now another very commonly used method in terms of flow is the stack method as TF the stack TF the stack and then yeah we just go straight away and then you see the kind of output it produces so yeah we have specific we specify axis equals zero it's kind of like similar syntax with the concatenate so we specify as equal to zero we run that and then we should have this so there we go we have instead of like with the concatenation where we just join this two tensors across the rows here when we specify axis equals zero it simply means we are creating a new axis so if both tensors are all 2 by 3 2 by 3 like this we create a new axis and now and then you axis depending on the number of tensors we have so if we had for example if we have to add C1 again let's add this C1 depending on the number of tensors we have we are going to have this shape or rather we're gonna have this axis having a length corresponding to the number of tensors we have so like here we have this three so that's why here we have three we're taking this back to two we're gonna have two so basically we actually stacking up this tensors and you should be able to clearly see the difference between the stack and the concatenate and then using this new T1 T2 for the stack we have that when we have the stack axis equals zero there's an output get shape 2 by 4 by 3 so from here we could understand that we have T1 4 3 T2 4 3 and then we create this extra axis at the zeroth position so we create this extra axis right here so 4 3 4 3 now we add this extra axis and we have now 2 4 3 where basically this T1 and T2 stay intact as you could see here we have this is T1 and then this is T2 all the values are intact forming this big tensor of shape 2 by 4 by 3 so we leave from this two 2d tensors to this 3d tensor by just stacking up this two tensors now when we change the axis or when we working with axis equal one the stacking up is done such that we having this 4 3 4 3 but the outputs we get is such that this additional axis is at the position 1 so unlike here where this additional axis at the position 2 now this additional axis at the position 1 so just like we have in let's say we have in 4 and then we have in 3 so if axis equals or 0 we add it at this position if axis equals 1 we added at this position if axis equals 2 we added at this position we also understand that if we have 3 of this so if we have let's say T1 and then yeah we take T1 let's run this too if we run this too this is what we now obtain we have here for when axis equals 0 we have 3 by 4 by 3 so let's see why we have this we have in this tool we have in 4 by 3 stack with 4 by 3 and then stacked again with 4 by 3 so we have this 3 tensors been stacked here T1 T2 and then T1 again so here we're gonna add an extra axis but then since we have three of this we have we just extra axis 3 length 3 so we have 3 by 4 by 3 that's why we have this right here and therefore when axis equals 1 we add an extra axis so we add the extra axis here we have 4 by 3 we add the extra axis in the middle so here we have 3 so that's why we have we have 4 by 3 by 3 let's check this out we have 4 by 3 by 3 when the axis equals 1 the stacking is done at this position one year and so what you have here is this 1 2 3 stacked with 7 8 9 and again stacked with 1 2 3 this is because we have T1 T2 T1 so if I take this off if I run this I have 1 2 3 stacked with 7 8 9 so I have this 1 2 3 here stacked with 7 8 9 and then to obtain the next I have 4 5 6 stacked with 10 11 12 I have this 4 5 6 stacked with 10 11 12 and then the next is 5 6 2 stacked with 0 0 2 5 6 2 stacked with 0 0 2 1 2 1 stacked with negative 1 5 2 that's it 1 2 1 stacked with negative 1 5 2 so let's go back to when we had C1 right here C1 we run that and that's it so we understand exactly why we have this responses here we understand exactly why we have this up right here to get this for example we have 5 6 2 stacked with 0 0 2 and re stacked with 5 6 2 since we have T1 T2 T1 now at this point you could pause the video and try to find the output when we have axis equal to when axis equal to we are meant to have this so let's just recopy this is that when axis equals to we have 4 3 4 3 4 3 that's T1 T2 T1 and then 4 3 and then we add this axis at the end so here we have still 4 3 3 now to make it clearer let's just take T1 T2 so if we have T1 T2 yeah we just have this too so in this case we just have 2 now we have 2 we're gonna have 4 3 2 so 4 3 2 and that's it so we have 4 3 2 unlike here where we have 4 2 3 and here where we have 2 4 3 that's if we constrain that we're doing we just T1 and T2 so that's it and that's fine so as you could see our up now before by 3 by 2 and the stacking will be done on the columns so let's look at what we'll get let's have this run right here let's run this then we compare with our values here we see we have 4 3 2 and then this output can be seen as you taking this row that's this first row stacking up with this one and then transposing it so it's kind of like similar to what we had here 1 2 3 7 8 9 but yeah we now transpose it then yeah we have 4 5 6 10 11 12 transpose to obtain this we transpose this to obtain 1 negative 1 2 5 1 2 we transpose this other you look at it that way you can see directly that you have 1 7 there we go next 2 8 there we go next 3 9 here is it and then we move to the next where we have 4 10 so we have 4 10 5 11 6 12 and so on and so forth so that's how we get this output right here with the communication we'll see how the stack method can be written as the concatenation here that's I'm reading as a combination of the concatenation method and the expand deems method so yeah we have this stack of t1 t2 on 0 axis which produce this and then following the exact same method we've been given in the documentation we have this concatenation and then we run it so we run this and we see that we have exactly the same answer the reason why we have exactly the same answer here is simple you see that we have this expand deems so if we have let's suppose that we have this two tensor or we have this least made of t1 and t2 which is a which are the tensors one stack so here we have 4 by 3 and then we have 4 by 3 obviously when we do expand deems of each and every one of this that's basically what we're doing here for all this when we do this expand deems 0 axis will be left with one so either this extra axis we're left with this now once we do this we now concatenate on the 0 axis and we have the output 2 for 3 that's because we just basically add this to up so 1 plus 1 gives us 2 as we saw with concatenation so here we have 2 for 3 and this coincides with just doing a stacking of this two tensors t1 t2 then we get to the method path as TF dot path right here we have the definition and then we have this example so we just look at this example here we have this tensor T and then we define this patterns which we've seen up here so we have to pass in the tensor and the patterns copy with the mode and its constant values okay by default this is 0 and here is constant but we must put in the tensor and this patterns now let's take a look at that as we said we have the tensor we have this patterns which itself is another tensor and the way it works is we are gonna add this tensor right here with a constant value by default this constant value is 0 so we have gonna we're gonna do the pattern with a value of 0 that's why if you notice you have this 1 2 3 here is it here 1 2 3 4 5 6 this is it and then we do this pattern with all values surrounding it being zeros let's copy this and check this out in a notebook we run this here okay so that's what we get now let's modify this and say we want to have constant values to be equal say three you run that and what do we notice we notice that we still have 1 2 3 4 5 6 but all the remaining values surrounding it actually all to me now we are having this output here but how is output generated now if you look at this carefully let's take this let's send this back to a 0 if you look at this carefully the way this tensor is defined that this pattern stands as defined as such that here we have the number of rows above the initial tensor that's if we have this initial tensor here we have a set of number of rows above this initial tensor which we are going to pad with this value 0 so yeah we have one row above one lower below so that's why you notice here let's run this see we have this that's why you notice here we have this one row above one row below and then we have two columns to the left that's this two columns and then two columns to the right so that's how we generate this this all padded tensor right here now let's modify this let's say we want to have three to the right one I have three to the right you see here we have now three to the right still two to the left one row above and one row below so that's it now let's change this let's say we want to have five rows below you see we have one two three four five six and then all this five rows below now we could change this method to reflect our symmetric as we've seen here another way of doing tensor indexing is by using this gather method which comes as tf.gather yeah as usual we have the definition and some examples to permit us understand exactly how this works so that said let's look at this first example where we see how this tf.gather does a similar thing as the usual indexing which we've seen already so right here we have this tensor which we see here and then we have the tensor that's params params three simply means we're selecting the value or the element at the third position third index so if we count here we have zero one two three this is our third index and the output should be p3 so right here you see we have p3 now if we want to do this same thing with the gather method we just have to pass in our params and then specify the index so you see a specified three and then we have this output now let's this is out here unlike with the indexing where we had for example suppose we want to do params or want to take in these values what we're gonna do is just pass in say zero because yeah this is actually zero one so one two three so if we want to take all these numbers we'll have one two three and then we add a foot so here we have one two three plus one to give us those values so you're gonna have p1 p2 p3 that's it p1 always set it from zero so stuff on one so we have p1 p2 p3 which gives us this as expected I want to redo this gather we have this we pass in the params and then we specify this indices so right here we have one two and three we run that and that's fine we have p1 p2 p3 we could also do TF the range one up to four so we have that that's fine we run that there we go we have exactly the same response so let's get one step back we have this so let's pair on with a syntax now supposing we do not want to get maybe this three consecutive elements or any key consecutive elements supposing we want to get this zero element want to get the last element I want to get this element right here so this is 0 1 2 3 4 5 I want to get 0 that's 0 1 2 3 so I want to get 0 I want to get this last and I want to get it second to the last. So that said I'll just have to put your zero. I want to get the last element so negative one and so that one negative three so I run that I expect to have those elements. You see here we have an error so this gather method doesn't understand the syntax so we just have we just have to use the usual syntax yeah we have zero one two three four five so we'll put in here five and then here we have three we run that again there we go we have p0 p5 and p2v you see that this permits us to do this kind of even more complex tensor or slicing. Let's take another example we have supposing we have this tensor right here params and then we want to have tf.gather we pass in params and then we have three one for example now note that this params is to is rather is a four by two V tensor so it's of shape four by two V so we want to get want to gather this three one so look at this again let's start from something simple so let's start from the zero you see that doing this will get a zero one two so yeah we're seeing that we want to get just the first or rather just the zero element and it happens to be this one right here now this is this other this other params here we have the shape we had a shape of params that shape we had a shape of six and then here we have in a different shape so this is 1d and this other params here is 2d so the way we approach the index in here is slightly different now let's look at this when we do this gather we have the index zero selected and the default axis is equal to zero even here our default axis was equal to zero that is with this one right here the default axis here is equal to zero if you do this you should have exactly the same response oh yeah we should run this around that so you see that we have exactly the same response now let's get back to this our axis is zero means that we are dealing with this axis right here which happens to be the columns or rather which happens to be the rows so putting out zero here simply means we working with the zeroth row so we're working with this and that's why the output is as such now if we do this as if we do zero for example let's say three and we run this what we get is zero one two and thirty thirty one thirty two so we get in this and we get in this this is because we're dealing with a zero row and the third row so zero one two three you have this and we have this if instead we want to do this selections based on the columns then we will have to deal with the first axis so x is equal to one running this this is what we obtained yeah we're saying we're dealing with a zeroth column so we have zero ten twenty thirty so that's it right here and then the third column but yeah we don't have any third column so that's why it just by default just places all those zeros right here now let's change it and put for example the second column we have two twelve twenty two thirty two two twelve twenty two thirty two we could change this to zero we run that and we have two twelve twenty two thirty two zero ten twenty thirty now let's modify this let's modify our params so let's have this yeah this and we run it again before running that let's let's go back to zero so let's suppose the axis is zero our ship is one by four by three now our axis is zero so we are on this zero axis right here and if you considering the zero axis here we have only this so basically have only this now we say we want a second so one second element of this 3d tensor but there is no second element since we only have just this one so what happens is we have all the zeros right here now the next is we say we want a zeroth element since this element actually exists we see what we get now is exactly this so it's basically giving us back this year now we turn this to one what do we what are we supposed to get since here oh we are on this axis we focus in on this rose focus on this rose we pick out the second row so zero one two we pick out the second row and the zeroth row so let's run this and we expect to get 20 21 22 and 0 1 2 so let's run that there we go 20 21 22 0 1 2 which is what we expected to get now let's double this so let's have this supposing we have this step of 3d tensor let's modify say this one let's find that 0 0 2 and that's it so let's run this now and what do we get we see that since we are on this we've picked out the first axis let's let's even go back to 0 let's run this yeah we see again that we still have the 0 0 because there is no second element but if we had one right here if you had one yeah we don't have the zeros anymore we have this year here is it and then we have the zeroth and here is it yeah now let's put this back to two and then send this axis to one we run that and here's what we get so as we are saying we are working with this axis this one fixed we're working with this axis we just function with a row so for this element right here we check out the rows what is the second row this is the second row and this one so that's what we have here and then for the next element where the second row we have 0 5 55 and 3 1 21 so that's how we obtain this now we have this other method gather nd so it's slightly different from the gather what it does is it gathers slices from params into a tensor which is specified by the indices so this indices is gonna influence the output shape so you have to say whereas in tf.gather that when working with a gather method indices defines the slices into the first dimension of params and the tf.gather nd indices define slices into the first n dimensions of params that said let's look at some examples so right here we have some examples here we have this indices the params gather nd takes in the params and the indices note that unlike with a gather yeah we don't have this axis argument so the gather nd doesn't take in the axis argument and we just basically have this params and then the indices so let's see how this works we have a param this tensor right here or this list which subsequently become a tensor after getting to this method and then as it was said in the documentation we're gonna have a tensor which is going to get the shape of the indices right here but the way is gonna get this is such that we're gonna have this zero element of params and the zero element of params happens to be this here's a zero element of params and then you're gonna have the first element of params so this happens to be this so let's run this and we see what that gives us we have here a b c d so that's fine we have exactly the same as params now if we take this off so let's run that we would have a b we have this one by two shaped output yeah we have this a b right here notice how since we've picked out the zeroth element we have this this is actually the zeroth element so we take this and then we replace right here and we get this output let's modify this to take e and then f so here we have e f that's it and let's do two for example we run this and we should get e f so that's it we have e f now let's do two with one so two one what do we have we have f now why do we have f we have f here because we're picking up the second row that's 0 1 2 this row and the first column so that's why we have just this f right here this is different from the gather where when we do this that's when we have like this index in this is specified to one and then we let's run this and then we do tf.gather params indices here yeah we should have this year ef that's two and then cd1 so we should note that with those gather nd our output is our output ship is defined by this indices right here so what we have is two one we pick this up and then there we go here's what we have whereas with a gather first of all we have an axis which is zero so we're working on the zeroth axis that is we're working with all the rows and then we can out the second row and zero all right on the first row so that's why we have ef cd stick this off we take this other example here now we have this indices so obviously we know that our output is going to take the shape so we expect to have this kind of output but then what values are we going to put in this output so that's what we're going to look now we have this year zero one since there's a 3d shape it's a 3d tensor we have zero when we say zero we're picking up this first element here the zero element and when we say one we're picking up this element right here now zero one means we're picking up this element and then one means we're picking up this because first of all when you pick up this element you have two elements in this element so we have this year and we have this year so when we pick up this element that's the zero element right here we now have to pick up this first element or the element of the first position so here we have the zero position and then we have the first position so that's how we have c0 d0 now for this other one right here so we already know that we would have an output which will look like this so we'll have c0 d0 they're actually strings so understand that yeah we have one zero now one year means we after this two elements we have out of this one on this element right here we're picking up this one so we pick this one year and then after picking this one we're gonna pick out the zeroth element so we have in two elements this zeroth and this first element we pick out this one so we have a1 b1 so right here we should have a1 and b1 now let's run this and we confirm that we have exactly the values we expected now if after picking that's if after picking for example here if we pick this zeroth element and then we pick the first element so we've picked the zeroth and then we've picked this first element right here now what if we want to pick as if we pick this first element in this element we have two elements that's c0 and d0 what if we want to pick this zeroth element so the zeroth element we should have c0 and now yeah we'll pick this first element with the zeroth that's a1 b1 what if we pick the first element that's b1 so this one right here we run that which we have c0 and b1 so that's it let's take this example if we have this indices given to us and then we have the primes and we're trying to gather gather and the we're trying to get it up right here now what would have us output as usual would be of similar shape with the indices now we have 0 1 so notice how we have in here a three dimensional indices so we should have something like this 3d there we go we have this and then we have this okay so we have this 3d indices now we pick out 0 1 picking out 0 1 simply means we're picking out the zeroth element and then after picking out the zeroth element we pick out this first position here so we have c0 d0 and then from here so we first of all start by picking out c0 d0 c0 d0 and then we have then we have the next 1 0 1 0 we're picking out this first element and we're picking out a1 b1 so we have this a1 b1 that's it we download this first part so we close that so we have this here we have this 2d tensor right here which is like this one and then we move on so for the next we have 0 0 1 1 now 0 0 is simply this so we have the zero and then we have this 0 so we have a let's just copy this copy this we have a right here we have a1 b1 a1 b1 and then 1 1 that's picking out this one and then this one so we have c1 d1 c1 d1 that's it okay so we're done with that we could now take this off so that's fine let's now go ahead and run this and compare answers here we have I suspected this 2 by 2 by 2 that's because we have this two elements and then for each of these elements we have 2 by 2 tensor so that's normal for the shape then here we have c0 d0 a1 b1 a0 b0 c1 d1 we actually having something different here here we have a0 b0 and here we have a1 b1 let's understand why we should have a0 b0 right here we get back to this at this position we have this 0 0 now 0 0 we have the zeroed element and after picking up the zeroed element we have this zeroed position 0 a0 b0 so that should be a0 b0 so that's normal we made an error here we have had a0 b0 we now look at this gather nd method while taking into consideration this argument that's the batch dims argument right here by default the batch dims equals 0 so by default we have this output right here where to obtain this 0 1 when you say 0 1 you're simply saying you're taking the zeroed element and then with the zeroed element you're taking the first index so you have this c0 d0 and then 1 0 means you're taking this first position and 0 so first position and this first position you have this zero position so a1 b1 and that's how we get this now let's turn this by dimension to 1 you see that we have a different output now let's understand how this output is gotten you'll have to note that this batch dims here is kind of like making this method to be batch aware so that said once we have this indices passed in that's this indices here we understand that this corresponds to a given batch that's this batch and this corresponds to this other batch so that said oh we haven't 0 1 and then we have already picked this batch so when we select the bad things equal one yeah we've already picked this zero batch and so doing zero year means we're picking this zero element here and then doing a one means after picking the zero element we're picking this one year and position one so that's why we have a b0 now for the next as I said is batch aware so this matches with this second element here with this element at position 1 now 1 here is 1 and 0 here is 0 c1 so that's how we obtain this right here unlike with a format where we had batch dims equals 0 let's look at this other example we have this indices we have the params now when the batch dims equals 0 saying that you want to have 0 1 is just like picking this so we pick this and then we have 1 which is c0 d0 so that's why we have this c0 d0 1 0 we're picking this we're picking this element at position 1 and then 0 is a1 b1 so c0 d0 a1 b1 and then a0 b0 c1 d1 we've seen this already now when you say batch dims equal 1 while you're saying here is all the indexing you will do here will match with this zeroed element so this zeroed element here will match with this so it's not batch aware and then this first element will match with this that said 0 1 year means first of all we've already selected this so 0 means we're picking out this zeroed element and then 1 means we're picking out this one so we're gonna have b0 so we have b0 1 0 we've already picked this one so we have c we have this 1 0 with this and this so we have b0 c0 now for the next we have is batch aware so we've already picked this and then 0 is this 0 0 is only picking only this one so we have a1 now for the next 1 1 means we're picking this one and then we're picking this d1 so we have b0 c0 a1 d1 now let's run this and we hope to get an exact answer so here we have c0 b0 c0 as expected and then a1 d1 you could continue exploring other methods we'll now move on to racked tensors so we have your tf. racked yes overview we try to understand first of all what racked tensors are about to understand the racked tensors let's take this example we have this tensor 2d good print out this shape tensor 2d shape we have that which is 4 by 3 now let's copy this and then paste it out here and modify this one so we'll modify this so we have this 3 this like this 2 3 and that's it now let's print out tensor 2d let's print out tensor 2d there we go print out the shape what we get here is an expected error can't convert non rectangular Python sequence to a tensor so as you could see here tensors are meant to be rectangular that is if we decide to have four rows like in this case we have four rows that's this this this and this then each of this four rows must have the same number of columns so here we have three columns this one one column there's three columns there's two columns hands this arrow is thrown here but then it happens that sometimes we may be working with data which comes in this form that is we may have data where we could have for example this rose or we have this row or having three columns and this other row having one column this row other row even having say five columns go we have this kind of data and the way tensor flow deals with this is by using racked tensors now let's see how to create a rack tensor to create a rack tensor is quite easy we have tensor let's say tensor racked is equals TF dot racked dot constant so instead of just TF dot constant we have TF dot racked dot constant and there we go so we could also have this is like a simple list so could have this as a list let's put it this way we could have a list and then just pass this in here so we have tensor 2d now let's go ahead and print out a shape we have tensor racked that's it let's run that we have an invalid syntax here okay it's because of this so let's run this again and there we go we see we have for by noon now for because we have four rows that's fixed but then we don't have an exact number of columns so we just have unknown here we don't know exactly the number of columns we're dealing with but nonetheless we have racked tensor which we've just created so we see here we have this TF the racked tensor unlike with this let's print out tensor 2d if you print out tensor 2d you see you have TF dot constant now your this actually a list so let's say TF dot constant TF dot constant and then we have that oh we have an error here let's just come let's use this errors because it's not rectangular so that's normal so okay there we go we see we have TF dot tensor here and then here we have let's run this again here we have TF dot racked tensor so this is an example of a racked tensor wherein we have known rectangular data like in this case here apart from this constant method which we just saw we also have methods like the Boolean mask now with a Boolean mask we pass in our tensor or our list and then we pass in the mask let's check out on some examples so we see here we have this 1 2 3 4 5 6 7 8 9 we're passing the mask true false true false false false and true false false notice how wherever we have the false this data or this element in the data is taken off so this 2 is taken off and we're left with 1 3 so that's why we have 1 3 right here because we have true true and then here we have all false so this is an empty list here we have true false false so you have with just seven so that's it and from this we found this racked tensor right here now we could also do this for rows so instead of just specifying for each and every element we just say true false true and then this understands automatically that this means our first row is going to be left out so it's all right our first row is going to be taken so we have 1 2 3 maintained and then this next is false so it's going to be taken off and then this next is true you could check on this other methods we are going to look at the tf.ragged tensor this ragged tensor class contains several methods as you could see here let's check on this from the row lens method right here to see how we could create ragged tensors it takes a potentially ragged tensor the row lens is specified a name and then validate so let's see how this works we have this input right here and then we have this row lens specified so we run this what do we get we have 3141 empty list 5 9 2 6 empty list now how has this gotten you see that with this row lens we're saying okay we are going to start from this first position or from the 0th position and then the first four or the next four we have will form a row so we have 3141 from in this row and then from here we say the next we're gonna have is gonna be an empty list or the next row we're gonna have is gonna be empty so that's why we have this and then from here we continue from our position where we go to the next three so we've gone to the next four at this point we created this empty and then now we're going to the next three 5 9 2 that's how we have this and then from here we have the next one six and then we have an empty so if we take this empty off of take that zero obviously you don't have the empty list anymore so that's it that's how we can create this rack tensors from the row lens from that we now look at the from row limits method here again we have this input values we have the row limits let's see how that works here since we're given the row limits we have this first value four so we come and put ourselves at this position right here we simply go one two three four elements we fix ourselves about this position we create this first element of our rack tensor there we go and then for the next four we are still at that same position so we move we still add that same position since we are at the same position and that we've already used up this first four positions we have this empty list again we move to the seventh position so we go let's go this way we move to the seventh position here we are at this point we pick this up so here we have 5 9 2 and then from the seventh position we move to the eighth position so we go to this position right here we pick this up we have the six again we have this eight position we still at the same position since we've already taken off all of this we just have an empty list so that's how this works let's now take one last you could always come back to the documentation to explore all those other methods with the from row splits method what we do is we actually checking out in this row split we've been giving here each element and the next element so we have 0 to 4 see right here 0 to 4 we pick out this 0 to 4 and yeah we constrain that this position this position here is 0 this position here is 1 this position here is 2 this position here is 3 and then this position here is 4 so when we say 0 to 4 we actually taken from this 0 1 2 3 4 so it does about this position and we get this first four elements that's how we get 3 1 4 1 and then the next we still at 4 so we still at this position so we're getting this 3 1 4 1 so we just have an empty list here now we move to we have 4 to 7 so we go again from here this is 4 this is 5 this is 6 this is 7 so we have all this 5 9 2 and then we have 7 to 8 we have the 7 and then here we have 8 so we get 6 and then 8 to 8 since we are the same position we have this empty list right here we could also convert tensors directly into sparse tensors using the from tensor method so here we have this tensor defined this is rectangular and then we could automatically convert this into a sparse or rather into a racked tensor this from tensor method takes in this lens parameter where with this lens given to us we're able to take only certain parts of our racked tensor so yeah since we have one it means for the first row or for this row that we have here we're gonna take only this one or the first element the next we're gonna take no element so we have an empty list the next we're going to take all three elements so we have six zero zero so that's it for the racked tensor you can always explore all the sort of methods the documentation it should be noted that many times we will have to deal with data which contains many zeros it should be noted that many times we will have to deal with data which contains many zeros and a more efficient way of treating and storing these kinds of data is by using sparse tensors so we get to TF dot sparse right here chicken sparse tensor there we go we see how to create the sparse tensors right here we specify the indices we're gonna understand this soon so let's say we have 1 1 it's actually 2d so yeah we have 1 1 yeah we could have say 3 4 there we go and then we specify the values this is gonna take let's say we add 11 and then 56 specify the shape let's say it's 5 by 6 there we go we have indices values and dense shape so yeah we've defined this tensor sparse run it and then let's bring this out so yeah we have tensor sparse there we go we see that we have exactly the same values we put out here but then let's understand how this relates to a usual tensor or how we could map this to a tensor now to do this let's do TF dot sparse dot to dense so note that we are not doing TF dot sparse dot sparse tensor we're just doing TF dot sparse dot to dense directly so we take this and we pass in tensor sparse we run that and what do we get we have this 5 by 6 tensor right here which happens to be what we had defined here so we had defined the shape to be 5 by 6 we said that at the position 1 1 that's at this this is the zero row the first row so we have this position first row first columns 1 1 we want to fit in this value 11 and we also said that at a position 34 want to fit in the value 56 so we go 1 2 3 that's sorry 0 1 2 3 and then 0 1 2 3 4 so this is a position 3 4 right here and then put in this value 56 so that's how we're able to map this back to a usual tensor then you'll notice that all the remaining values are zeros so this parse tensors permit us to work with these kinds of tensors more efficiently this last tensor type we'll be looking at is the string tensors so yeah we have TF dot strings and then we have this different methods which we could use before looking at this methods let's see how to create a simple string tensor so right here we have tensor string call it tensor string equals TF dot constant so basically we just have it a list made of string elements so hello I a string so that's it let's bring this out we have tensor string we run that and there we go we have this string tensor right here which hasn't defined the type string and then we have the different elements which make up our string you have as we have seen previously all this string methods right here let's look at this joint method with this joint method we are able to combine several elements of this list together and with a specific separator by default our separators is empty string right here so that said let's do this let's say we want to have all those elements joined together so we just have TF dot strings dot join and then we pass in tensor string so we pass in tensor string our separator our separator is equals let's say for now let's keep it at its default value we run that and what do we get we have hello I am a string now let's suppose that we want to have a separator which is the space so let's have this separator you see hello now there's some space between now let's modify the separator we put a plus and we run that and there we go hello I am a string with this separator right we have all the methods like the length the length basically tells us how long our string is so here in the case where we have this you see of hello tensor flow and you have this Unicode string you see how we're able to get that this length is five one two three four five this is 10 and then this is four we have the lower method which converts all uppercase characters into your respective lowercase replacements we have the n grams method and all these other methods which you could explore so that's it for tensors we now move on to variables to better understand tensor variables let's come back to this model which we had defined at the beginning of this course this values a1 a2 b1 b2 which are given initial values get updated as we train our model and so we need to use variables which can be updated as we do model training that said instead of using the TF dot constant we are gonna have X var X var which is equals TF dot variable so this how define this and a variable must always be initialized so yeah we pass an X and then we print out X var and there we go we have this TF the variable its name shape data type and its content we could include this so let's say we want to have var 1 let's specify it as name so we have that and now we've specified a variables name var 1 here's how we create a tensorful variable notice also that we have this trainable argument right here which says whether during the training we update that variable or not and so if this is false then during training this variable wouldn't be updated if it's true then this variable can be updated during training and then we have this other arguments which you could explore now well this TF the variable has other methods so we have the assign method for example and we'll see an example of how this is used we have this variable defined so we pass in this number 1 and then we assign its value to 2 so now when you print out this V what you're gonna get is this new value for our variable now you see we have the assign add this other method right here with the assign add we're simply doing addition on the elements now variable so let's check on assign sub this assign subtract yeah we're just doing subtraction so yeah we have xvar and we could do some subtraction so we say xvar dot assign sub of this three four so it takes in this three four and then what goes on here is we're gonna have one that's the actual value of xvar no xvar is actually this so okay as one so xvar we have this one minus this three and then this two minus is four so let's run this what do we get we have xvar negative two negative two if we modify this to six we have negative five negative eight reason being that the first time we did the update we went to negative two we had negative two negative two now we're doing the subtraction so we have a negative two minus three negative five and then negative two minus six negative eight so now we have negative five negative eight let's go ahead and add eight so let's just do this we have add we run that we should have zero zero so that's fine we have zero zero because negative five negative eight plus five eight would give us zeros so as usual you could check up on all this other methods in the documentation then another important part which we need to mention is the fact that you could decide on what device you want your variable to be so generally we have CPUs GPUs CPUs let's check on this runtime let's change runtime type you see that personally we're using a GPU so if I say none then that's the CPU TPU and then let's do that so yeah we could have with TF device and then we specify that device in this case we have a GPU so we have that GPU with this we could define our X bar so you define your X bar in this scope and you're actually saying that you want your X bar so X bar that's variable one or X bar to be in this GPU now if you wanted to be in a CPU you just simply specify your CPU and that's it now you could also let's print this out first so we have X bar let's run that and then you could check out a device which is found so check out a device see it's not CPU if we modify this to GPU you run that it's not GPU you should also note that this could be done with just simple tensor so we could have X tensor tf.constant and then 0.2 there we go print out X tensor the device so let's run that and we see we have this now we could let's do this copy that there we go we have now let's put your CPU so we have the CPU and then let's take this off there we go we run that and we compare so here we have a GPU and then we have a CPU for our tensor that said it's possible to initialize so you could initialize this right here like this let's say we want to have 1 2 3 4 so we want to have 1 3 4 that's our X let's say this is X 1 and then yeah we want to have X 2 which is tf.constant and then we want to have here 1 so let's say we have this year let's put this list so we want to have this X 1 and X 2 which is in a CPU initialize in the CPU and then we want to carry out the computations in the GPU so that these computations could run faster so right here what we're gonna do is we're gonna have X 3 which is equals X 1 plus X 2 so as simple as that now obviously broadcasting will happen here and then we're gonna have 1 1 1 which when we add with all this it gives us 2 4 and then 5 so that's it let's now print out our X 1 let's take this off let's print out X 1 print out let's print out X 1 and where it's found so X 1 the device copy that take this off that's it we have now X 1 X 2 X 2 X 2 X 3 here X 3 so let's run this and see what we get we see we have this CPU CPU and then your GPU now we have 2 4 5 so output which is expected so that's it with just looked at tensor for variables and this marks the end of this part on tensors and variables don't forget to like and subscribe so you never miss amazing content like this and in case you want to gain some solid foundation on linear algebra you could check out our cards see you next time what's up everyone and welcome to the session where we build a linear regression model and a deep neural network to predict the current price of a secondhand car based on the number of years that cars been used its number of kilometers traveled its rating condition economy the present state of the economy top speed horsepower and torque and so we're gonna build models which when given this inputs permit us predict this current price as we could see in this results right here where we have in blue the models predictions and in orange the actual price so before moving on it's important to note that we are gonna follow this machine learning development lifecycle where we start with defining the task we'll look at the data source we're gonna prepare the data build machine learning models which permit us learn from this data to the right arrow functions for this learning process and then get into the training and optimization from here we're gonna measure the performance of the model on this data we're gonna validate and test our model and then finally we are going to take up some corrective measures to improve the performance of this model our task here is to predict the price of the used car using some input features in this case we've selected just one feature that is say the horsepower so supposing we have this and then we have the price in thousand dollars and then we want to make use of this input data right here that is this horsepower and the corresponding prices to train a model such that when given the horsepower we'll be able to predict the corresponding price of that used car or when given this 150 we'll be able to predict the price of that used car based on a model which has been trained on this data right here plotting out this data we could have this in the xaxis and then the price on the yaxis as you could see on this plot based on a given horsepower we are able to predict that the price of the secondhand car now if we draw points like this to draw lines like this from those points you see that we have a continuous range of real valued outputs right here and since our outputs here can take up continuous values we term our task as a regression task now let's modify this problem so that you better understand regression problem here we have this house power and then if we modify this such that we are going to insert predict whether the car is going to have the car is going to be noted as expensive or not so instead of predicting the price we may want to predict whether that car is expensive or not so let's say for all cars below say eight let's say 8.5 they are cheap and then so we'll call that cheap see and then greater than 8.5 we have the expensive cars so yeah we have going equals to here we have cheap here is expensive and so from here we may have a different kind of task that is one in which we want to say if based on some input or some inputs because you may have many inputs right here based on the inputs the car falls under one of these categories right here so you could see that this kind of problem is one in which our outputs are discrete see you have just two options and then with this problem where we're trying to predict the price we don't just have some two or three or four options which want to pick from we have an infinite number of options since the prices can go from let's say a thousand dollars we suppose that we'll fix a minimum to a thousand dollars and then the maximum to a hundred thousand dollars so we could fix this to a hundred K from K from one thousand to a hundred thousand but the values fall under this range which is actually an infinite number of possibilities unlike here where we have a finite number of possibilities we now move on to the data and before diving into that let's look at a big picture so here we are having a model which is gonna fit in this inputs and the outputs such that later on we could have this input fed and then we obtain the outputs notice a difference the change in direction of the arrows so initially we have this inputs so we could have this inputs right here let's take this off we could have this inputs that is this X and then this Y so here we go we have this inputs and then after the model trains we will now pass in just this and then we get this one automatically so let's get straight away into how we're gonna prepare this data such that it could be passed into this model right here you already have the secondhand cars data set made available on the cargo platform by Mayang Patel you already have a description of our data set so we have these different features which will be using to predict the price of the secondhand car so that said you could see here you could get more details so basically here's our data set as you could see and then by clicking on this so you could start and then you could get to this compact view column view here you see that we have the ID which is we're not gonna use this because this is actually not part of our data set but just an ID then we have this unrolled old feature right here though isn't actually well explained because we don't clearly see exactly what it signifies nonetheless we have let's take this off we have this mean and the standard deviation so as we had seen the previous video we could have this and most of our values around the mean that is most values we get in this data set for the unrolled old feature lie between the mean this is new the mean and signal standard deviation lie between the mean minus the standard deviation and the mean plus the standard deviation so in this range we have most of the values so this simply means most of our values lie between 602 K as we could see there plus or rather 602 K minus 558.4 K so that should be about 543.6 so if you subtract this 58.4 we have 543.6 so this means our values fall in this range now most of our values fall in the range 543.6 and 602.2 plus 58.4 so that's 660.4 so most of our values fall in this range right here so that's what we mean by having this mean and the standard deviation so we're gonna have this same right here now we have other features let's take this off we have other features as you could see we have unrolled old and unrolled now which wasn't very well explained we wouldn't use this and then we have the number of years that's obviously number of years of usage of the car you see we have missing number of missing values we have known number of mismatches known and valid a thousand so this kind of like cleaned data we have the mean that's 4.56 years and the standard deviation 1.72 years so most of our cars in this data set have been used for most of them will fall under the range for the 4.56 minus 1.72 and 4.56 plus 1.72 years so number of kilometers covered there we go we have the rating that's the car rating at the moment you want to buy it the condition and moment you want to buy it you see there's a mean right here the current state of the economy the top speed of the car the car's horsepower and that's it so your other different features we have and which we will use in predicting a car's current price so that said we could download this for free so we have this downloaded we now get into our collab notebook where we're gonna do the preparation of this data so here we have this three inputs we have tensor flow for our models we have pandas for reading and processing data we have C bond this is going to be for visualization so that's it now you don't need to have any prior knowledge on this tool as we're gonna explain every single step of our data preparation and visualization processes we've now uploaded our data set right here the strain dot CSV file where you download it from the cargo platform you could see it right here so double clicking on this gives us this file we have here now click on a hundred yeah we have 10 per page okay let's say I want to get a hundred per page so that's it we have this different data points right here now that said as you could see we have the ID on road old on road now the years number of kilometers radiant conditions economy top speed HP torque and then the current price so that's it that's for each and every data point and now let's do this so let's break this up so we could do this we could break this up so if we break this up like this you see we have this first section our inputs and we have this other sections right here all this this other section actually we have this this section our outputs so here is now X and here we have the Y yeah we suppose that we have totally n data points that is if we have if you count all these data points and we have n of them then we could have this shape all could represent this inputs in a tensor of shape and by yeah we're not taking this we're not gonna take this column we're not gonna take less we're not gonna take this column we're not gonna use this column because we don't understand it very well we're not gonna use this we're gonna use this so we have one two three four five six seven and eight so we have n by eight that's it that's our input X that's a shape of our input X and here we have n by one so this is a shape of our output tensor if you're able to put this data we should be given the CSV file in the form of a tensor with the shapes then you could pass this now into a model like this let's call this M a model has this in and then train this model such that when you come now with new data the model is able to predict the current price now to read CSV files we are gonna make use of this pandas rivalry right here is here we will define I was imported this as PD so that's why we have in this year so this particularly pandas and then we have read CSV so this method is used in reading CSV files like the train the CSV file we have here and then we specify the separator let's open up this file with Excel there we go yes what we get is very similar to what we've seen already closes up and now let's open this up with a notepad so okay that's it so you'll notice now that this is our same data but the way is put out is quite different as you could see all our column heads are separated by this commerce so we separate each column by a comma you can see that clearly from this and then when you go to the next row we also have the separation by commerce let's do this let's put this link so we see this one see linked right here we have this number right here linked to this we have this next number linked to this and so on and so forth so that's it and that's why we are having comma separated values format so this is the meaning of the CSV format commerce appeared values so we see each and every value is separated by a comma that's each and every value which makes up this row is separated by a comma and that's how CSV's are formed and you should now understand why in this code when we read the CSV was specified at our values are separated by commerce now in a case where you have data and then you have something like this so we may have this semicolons everywhere so let's say we have the semicolons we're gonna save it as a different file so we could have the semicolons there we go you know all the separators common columns are separated with the semicolons we save this and we've now uploaded it right here so that's what we have see we have this right here now let's go ahead and read this so we've created this data and we could say data.head you see data.head we run that and see we get the first five rows of our data and straight away you could already do stuff like this data.shape you see we already get the shape of our data we have a thousand by twelve now let's get it as semi so let's just copy this so we'll copy that and paste it out here and then let's run it with supposing our values are separated by commerce so let's run that and this is what we get you see we have this head which is not actually well formatted because by default this formatting is meant for comma separated values now what if we modify this we run that again and you notice that there is a difference actually now so let's do this side by side from that let's take this back to comma so we have that and you notice that there's a difference now here as well formatted whereas here is not very well formatted so that said if you want to work with these kinds of data you could always make use of the pandas library let's print out data that's shaped here we run that and then yeah we print out let's say data.shape so let's have that see yeah we have one by one which is not exactly what we expect to get we've now been able to read our data and then put this data in some sort of data structure let's now visualize how each and every feature right here is related with each other and to do this we're gonna use a seaborn library right here so seaborn has been imported as SNS so we have SNS.pairplot which we have here SNS.pairplot we pass in our data and then we have all these features with the output you can check out the seaborn.pairplot documentation on the seaborn website so right here you have the different parameters you have the data you have the hue which has to do with coloring and then we have this data kind which we use and the data kind is a kind of plot for the diagonal sub plots which we're gonna see shortly so yes we see we've selected a KDE we run this the KDE actually stands for canal density estimate after running this this is what we get we see this KDE plots in the diagonal now let's look at each and every one of this so let's take off this IDs because we're going to be using this ID unrode old unrode now that's because we don't understand exactly what the signify so we will run this again here is what we have now scroll up a bit and now we'll say we have the number of years and this plot here shows us how the number of years is related to the number of years you see how this related to a number of years in the diagonal we have how each feature is related to itself so let's reduce this okay so yeah you could see how this okay so you see out here we have this years related to years here we have kilometers related to kilometers radius to radians and so on and so forth that so that was diagonal that was a KDE plot now here we have how the years is related to the kilometers so we see how okay let's take it from this so we have here this kilometers and then how the years later to the kilometers here we have years related to the radians and you repeat the same process to get all those relationships between these different features but one very interesting point to note here is we could already see the relationship between these different features and the current price so let's take for example yeah we have current price so this is a plot showing how the years number of years is related to the current price and this is based on the data we have we see clearly from here that for each number of years spent the current price could go from very low to very high now if we move to the kilometers you see a certain pattern you notice that as we increase in the number of kilometers covered by the car the overall current price of the car drops so this is one very interesting feature to note we see a similar pattern to this years with the rating the condition the economy the top speed HP and tort so here is how each feature is related to the current price at this point we convert our data into a tensor so we have tensor data equals t of a constant and we pick in this data so we do this and we run that and there we go we have our tensor data so that's it let's print out this without a shape so we have that there we go we see we have our tensor of type float 64 obviously you could do some casting so we could say tensor data equals TF the cast of tensor data and then we specify we want to have an eat say all right I want to have a float say 16 so let's run that again a problem which arises now is we have some values which are converted into infinity the reason why we have this is because some of these values are too large to be stored as foot 16 data types so we're gonna put this 32 you see we run that and everything is intact another important step to take is to randomly shuffle our data so to avoid any bias based on the way in which the data was gathered with it is random shuffling such that this other right here is no longer respected so let's do that what we need to do we have our tensor data is now equals TF the random that shuffle so we have that and we pass in tensor data that's it let's print out tensor data let's say we print out the first five elements so let's let's print that out here is what we get but then let's copy this so you could see the difference before and after the shuffling so let's say we have this let's take this off let's have that and then we have our tensor data let's run this again we print out tensor data you see we have this and then we print out this shuffled tensor data you see that here this ID order was respected 1 2 3 and 2 1 2 4 whereas here we have 551 567 229 557 402 so we've actually shuffled our data and then let's take this off now so we've shuffled our data and then we're ready to break this data up such that we have both the inputs that's X and outputs Y or the output Y but continuing let's add this little text here so we have this text we add a text and we say we're doing data preparation that's it put it in bold and we're fine so oh we have that data preparation now right here as we're saying we need to get this X and the Y let's start by getting the X so here we have X tensor data that's it and then we pick out all the rows so obviously we're interested in getting all the rows interested in getting all thousand or 1000 rows but then we're not interested in some columns we're not interested in this column this column this column and this column so we're interested in just this or rather this so that said we have here 0 1 2 3 so we're gonna take from 3 right up to this position right here so that's it so we do that we just have here we pick out everything and then in this next we have 3 right up to negative 1 so let's print out a shape before printing out a shape let's look at this and get that shape before printing out so normally we should have a thousand by 12 but since we're picking out just the inputs now we have a thousand by 1 2 3 4 5 6 7 8 so we have now thousand by 8 so running this should give us just that we run that and there we go we have a thousand by 8 if you bring this out let's print out the first 5 we have that there we go we have just this now notice how here this first 3 values have been taken off and we left with this right up to this value so that's it and then the current price was been taken off we repeat the same process to get the output so here we have Y that's it okay but now we get in all the rows that's true but we get in just that last column so that's it yeah we know why and that's fine so that's what we get for Y not notice that here we have the shape which is actually just 1D so what we could do is after getting this we could expand themes so we have TF that expand themes to add that extra dimension so we have to have the expand themes and then we put that we specify the axis here we have negative 1 so we run that again and what do we get this what we get we have 5 by 1 just like here we have shape 5 by 8 now we have 5 by 1 all like previously where we had just this 1D shape let's take this 5 here so previously we had just this 1D shape now we've added this extra dimension so that it matches that of the inputs we have here from this point another very common transformation which we could do on our data to enable our model train faster is by normalizing this data in fact we actually normalizing the inputs that is we take for every input we subtract the mean and we divide by the standard deviation in this example here the mean of this eight values is around 138 so that means if one normalizes data we're gonna have 109 minus 138 divided by the standard deviation let's say the standard deviation is 150 for example so this means this point is going to be converted into negative 0.193 if we now take a value like 206 we're gonna have 206 minus 138 divided by 150 which will now give us around 0.45 so if you notice our input features have been rescaled before passing into the model and to carry out those features killing TensorFlow has this normalization layer right here which is part of the tf.keras.layers now we haven't spoken much about tf.keras because we have not gotten into the modeling but for now just note that in this tf.keras.layers we have this normalization class which can be defined by specifying an axis the moon and the variance that's that we have from tensorflow.keras.layers we are going to import normalization so that's it that's from that and then we get to use this normalization layer before looking at our data let's see how we could use this normalization layer we define a normalizer then we have normalization we pass in nothing inside there by default the axis is negative 1 we'll look at that shortly and then here we are gonna have the X to be normalized so we have X to be normalized which is this tensorflow tensor right here let's say we want to have 3 4 5 6 7 there we go and then we do normalizer and we pass in X to be normalized so that's it we have as this and we see we have exactly the same output that's because yeah we haven't specified the mean and the variance so we just have this the input pass as an output now let's go ahead and specify the mean so if we say we want to have a mean of let's say let's if we look at this let's say 5 and then the variance of C4 we run that and there we go what do we notice we notice that this inputs are being rescaled into this new inputs now let's add another wheel right here so we want to have four say five six seven let's keep it simple so we have this okay that's eight so we have this next row that's it and what do we do we run this let's correct that there we go we run that and we see we have this normalized properly though it's worth noting that seems by default the axis is equals negative 1 so by default we have this axis equal negative 1 it means this normalization here is done with respect to the columns so if you could recall from our previous section we have our shape 2 by 5 and picking the axis negative 1 means we're picking this axis here and this actually corresponds to the columns that's 1 2 3 4 5 so we have all five columns and then for each column we are doing the normalization so practically what we're doing here we're doing 3 minus 5 divided by the variance we should speak we normalize that 4 minus 5 divided by the variance 4 we have the value and so on and so forth we've made a slight error in this definition is actually X minus the mean divided by the standard deviation so it's not a variance not divided by the variance but if a standard deviation now we also know that the standard deviation squared is equals the variance so basically when we have this variance to obtain the standard deviation we simply find the square root of our variance so in this case our variance is 4 so standard deviation is 2 if we replace here by 2 our mean is 5 that's it and now let's pick X let's take X for example to be this to be right here so if we have 3 see when we run 3 you should have negative 1 and if you pick 4 you should have negative 0.5 so that's it so that's how we get this now let's take another example but in this example we don't specify the mean and the variance now you should know that it's not an every situation where you could get the mean and the variance upfront so there are some cases or in fact in most cases you just have the data so you wouldn't always go and calculate this mean and variance upfront so what TensorFlow allows us to do is to obtain this mean and variance automatically so the TensorFlow permits us to adapt to the data we're given so if we're given this data for example what we're gonna do is get the mean and variance for this column that's this column right here middle 34 and then get the mean and variance for this column for this column this column this column and be able to normalize our data so just like here where we suppose the mean and variance for each and every column was 5 4 yeah we're gonna get the mean and variance for each and every column got an automatically so let's look at that we don't need to put a mean and variance now so we just let that go could take this off too since by default the axis is already negative 1 so we have that and then what we do is we do normalizer that adapt so we're gonna adapt to this our data we adapt to the normalized data and then we pass the normalizer pass X normalizing this to obtain this now so this is what we obtain when we adapt automatically so our data now let's understand what goes on we have 3 and 4 here the mean is 3.5 so that's clear so we have X minus 3.5 and divided by the standard deviation if we have two values 3 and 4 it's clear the mean is gonna be 3.5 that's gonna be in the middle and then if we have a mean of 3.5 so we have something like this our standard deviation should be 3.5 minus 0.5 gives us 3 so here we have our mean minus the standard deviation to give our least possible value so here we have standard deviation of 0.5 such that 3.5 minus the standard deviation gives us 3 and still 0.5 such that 3.5 plus 0.5 gives us 4 so that's it so coming back here if we take this 0.5 and we put in the value 3 we run that you see we should let's correct this you see we have negative 1 so that's how we have negative 1 here and when we put 4 you see we have 1 now if we move to this next you have 4 and 5 and we maintain the same mean and standard deviation you see we wouldn't get those answers right so let's take this with 5 you see here we have 3 which is not what we get here see when we have 5 5 is transformed to 1 and this is made possible because we get the mean and the variance for each and every column so that said when it's 4 and 5 we have a mean of 4.5 standard deviation 0.5 so if we change this now 4.5 0.5 put this to 4 you see we have negative 1 again and then 5 oh we have 1 so if we modify this now let's take this to let's say 10 let's run that what do you see we have negative 1 1 why do we have this is because the mean of this is 7 so yeah we're gonna have a mean of 7 we're gonna have a standard deviation of 3 when we have 10 we run that we see we have 1 still and then with 4 we have negative 1 still we could go ahead and add now one extra row so let's say 32 1 that and this let's run this again and we see that we have our normalized data right here and which has been done automatically unlike where we needed to specify the mean for each and every column here our normalizer adapts to our input data so how is all this related to what we've been doing here all we need to do is we'll be given X so we have X recording in normalization on the Y obviously we're doing this on the inputs so we are having our inputs X we've gotten already so we could print out the shape let's take this off you know the shape of X X shape see we have oh correct that we have a thousand by eight so we're gonna normalize for each of these eight columns and instead of struggling to get a mean and variance for each of those columns tensor for premise also adapts to this our data set so that said we have that given to us we could just we copy this oops we have that is it out we have our normalization normalized this is actually X so we don't need to specify this anymore so here we just adapt to X adapt to X and that's it let's bring out this X so we see kind of the kind of outputs we get let's say we want to get the first five elements again so that's it so what do we see we have that and then let's bring out X first five elements our first five rows there we go so yeah we see we have this five which is converted 0.25 we have this 4 to negative 0.2 3 2 and so on and so forth so this is with an automatically and we are now ready to clean our model using this normalized beater we've gotten to the point where we're gonna trade the machine learning model in our case our model is simply this straight line right here of the form of equation Y equals MX plus C as you could see here we have this inputs X so there we go inputs X outputs Y and we have our weights M and C which happens to be this constants that said we have an X which gets in here it gets multiplied by M so we have a multiplication by M and then we obtain M X so now we have M X and then it gets added to C so here we have multiplication and here we have addition and this gives us Y which actually equals MX plus C so let's say it gives us MX plus C let's take this off we have MX and then we put all this into a box which is what we call our model now notice here let's have this we have our model we have X get into a model and we have our output right here and then what we're trying to do is to get the most appropriate values for M and C such that we have an output which best represents our data set so that said our model could be represented by this line so we could have a model like this we could have this we could have this you see we have so many different possibilities and so when we talking about a model essentially what we're having is a function which tries to be representative of our data set now this means if let's take this off this means if we have a model like this clearly this isn't a very good model this is a very poor model since it doesn't actually represent it doesn't embody the data set we have in here and so a model looking like this will be a better one for now we'll just stick to the model creation but we should note that this model has to be chosen such that it represents best our data set so we're gonna look at this in the error management and training and optimization sections and before we get there that is before we see how to get the optimal values for M and C we are going to create this model using TensorFlow the good news is TensorFlow makes it very easy for us to create deep learning models so here we can define our model TensorFlow.keras.sequential so we have that and then we pass in our normalizer and we have a dense layer so here we have a dense layer which takes in an output of one now this dense layer is actually a Keras layer so let's get in here so from TensorFlow Keras layers we imported normalization previously now we import the dense layer so we have dense we run that again and right here we have a model model summary there we go we could visualize this model we've just created our first ever model with TensorFlow so let's break all this down now we have this sequential API which we use here in creating this model this is just one out of two different ways and ways models are generally created with TensorFlow we have the sequential API we have the functional API and then we have the subclassing method so that said for now we'll work with a sequential API we could look at the documentation for the sequential API in the TensorFlow.keras model so right here we have this sequential click on that and there we go so basically what this takes in is just layers and here we have some examples on how this is used so that said you could see we define a model and then we add layers to this model so basically we're just stacking up layers to this model now this means instead of having this syntax right here we could instead have this so we could take this off we take this off we have that and then we have model dot add tf.keras our legacy normalizer so we add the normalizer and then model dot add the dense layer so we have our dense layer output one and that's it let's run this and we get exactly the same output we had right here now let's take this off get back to our sequential API so as the word goes is the way we build models when layers all form a sequence that is if you're building deep learning models where the way they are constructed is such that we have the input we have the model we have the output and then all the layers which make up this model simply just stacked up one layer after another so we could have this layers which are from the model we'll look at more complex layers later but for now let's suppose we have this kind of model made of different layers layer one layer two up to layer n where we just simply stack up this layers in this way then working with a sequential API is a good option now let's take this off let's take this and then make use of the exact model we're actually currently dealing with right here we have this model and we have this other model so we have all rather we have this layer and we have this other layer right here we have the normalization layer which we've seen already we understand that our inputs need to be normalized before being passed into our dense layer right here so here we actually have this dense layer but then this should be the first time or maybe it's the first time you get in of this dense layer so we try to explain what goes on in this dense layer but you should note that this model is simply made of the normalization layer and the dense layer so without the normalization layer let's look at how the dense layer works with a dense layer as you could see here let's suppose we have an input so we have the horsepower input getting in here we have our input then in this dense layer we take this input the horsepower which we call X multiply by M and then add a value C this M happens to be the weight and then the C is what we call the bias so that said we have this and then here we have M X plus C so here's our output which is actually equals Y predicted so let's call this Y predicted and that's it so what do we do now well we have many variables once we have many variables we still have our same dense layer but there's a difference so here we have this variable let's suppose we have eight variables so we have one two three there we go we have this eight variables now all these get into our dense layer so we pass this into our dense layer then what goes on in this dense layer is that for each of these inputs we have this M1 M2 M3 M4 M5 up to M8 and we have this M1 times X so we have M1 X similar to what we had here here we just had one M since we had one input but now we have M1 times X that's the input here let's say X1 plus up to M8 X8 then we add that bias so we add this bias C right here and then we have plus C so yes our dense layer right here this is exactly what goes on in our dense layer and then we have obviously our output Y predicted so that's it we now understand exactly how this dense layer works now you'll notice that we have one two three four five six seven eight weights plus this bias making us nine different parameters and that's why right here you actually have this nine params given to you you have this nontrainable params 17 this comes from the normalization layer now it's nontrainable simply because we've already fitted this normalization layer to our data or we've adapted this normalization layer to our data so we don't need to modify the parameters mu and variance anymore we don't need to modify the mean and the variance anymore actually so that's it we understand exactly what's going on and why we have nine trainable parameters here so from here we should note that the way we constructed dense layer is quite simple all you need to do is to see how many outputs do I need to have now in our case we just want to output a current price and since we want to output this current price which is just one value our number of outputs here equal one so that's why we pick this number of outputs to be equal one now if we needed to predict say two current prices or maybe you needed to predict the current price now and the current price in the year say 2030 then we would have output 2 so that's it we just have one output that's why we do that from here we have this model the summary which you can call very easily and see this kind of interesting summaries of our model so that's it we now look at how to plot this model out that's quite easy we have tensorflow.carazadutils.plot model and now we pass in our model we specify to file want to plot this model out and then generate a file so we just say model.png show shapes show shapes there we go true so we want to show shapes we run that we see clearly here we have an input layer we have our normalization and our dense layer notice how the inputs and the output has been specified so this known here is actually the back dimension and since we could treat data of any batch size we just have this as known now what's our batch size you see our full data set we had right here generally we are not going to load all this data set at once into our model as we may be limited by memory requirements so that said what we pass into our model is actually passing batches so we could work in batches of two so we could just pass out just two values or two of these data points into our model we could pass just this two and so on and so forth if our batch size is say eight we just pass all this into our data into our model at once and then move to the next batch and so on and so forth but now note that generally you will not want to have a very large batch size as this will lead to a problem known as over treating which will treat subsequently in this course but for now just note that working with batch sizes of 32 and below is actually a good idea so let's come back to our model which we had here we now understand why we have this nodes here this actually our batch dimension now we could also in addition to this normalizer or before the normalizer would define an input layer so let's let's just declare this here let's just add this here let's import this we have input layer there we go so we have our input layer which we're gonna add from this Keras layers if you notice here we just had this node so our inputs are not specified the input shape is a specified output shape not specified so what if we come and specify this here we have input layer input layer sorry we have this and then we have the input shape which we could specify so yeah we have a shape of the batch by 8 so we have a batch dimension so if it's 32 we have 32 by 8 so this is what's getting to our model at once and not maybe the whole thousand samples we have so instead of sending all this we send just a batch of 32 now when creating this input layer since obviously upfront we don't know the exact batch size we'll take this off and just have that so we just have this shape we pass that we have this error let's add a comma and we also have to know that this is a list we're passing in so we have this first element of the list this next element on this other element and I would do this or we use the model add option which we saw already so we run this let's have this here and there we go so we see we have exactly the same kind of response as previously and then let's run this again so we see this difference now we notice that we don't have known again here now we know exactly the shape of our inputs so that said we could now get straight away to the next section which is that of error sanctioning so at this point with how to build a model as you could see y equals mx plus c and we'll now see how to know how well or how representative this model is of our data set so to check this out you'll notice that for each and every output we have so supposing we have this points right here we are going to compare the actual output with what the model gives us so our model tells us that at this point so we add this X this is X axis at this points right here we should have what our model tells us is we should have this output this so this is our selling price are cut into the model but then what we actually have is this so what comes from our data set or the actual selling price is at this point if we look at this other points right here you see that our model does quite well as our model or the actual price is this and if we extrapolate we see our model tells us our price is this so the actual price and the models price is quite similar now if you take another point like this you see this performs poorly here our model tells us we should be around this selling price and the actual selling price is around this so this tells us that if we want to choose the best possible values for m and c then we have to choose them such that we minimize these differences so these differences we have here that's this is what the model gives so we have to try to minimize these differences by minimizing the differences we actually sanction in this error so with sanction in the model every time it makes these kinds of errors so with these kinds of large errors we have here we're trying to sanction the model and then when we kind of like work perfectly like this our model receives less sanction or in the case where it's actually perfect our model is if no sanction that said this sanction is actually quite simple like oh what we'll do is when you have an error or simply let's say we have an output so we have this error but the way we calculate this error was we have an output and then we have the already we have the actual output and we have the models predicted output so we have the actual output here let's call it YA and then we have Y spread so this is what our model produced right here now what we could do is we subtract this two and then we square this and you see clearly that if the Y actual on the Y predicted both the same then in that case we will have zero because YA Y spread is the same so YA minus Y spread is zero and if we have zero it means we haven't to give that model a zero sanction for that particular prediction like in this case right here now if there's a very great difference between this two then we are subtracting it and also squaring so you see clearly that if you have maybe two and four so if our actual is four and what the model produces two then two minus four would have already four minus two anyway actually because we are going to square this so we have four minus two we're gonna give us two and if we just Y spread minus YA we would have two but now we're squaring this so we have two squares so we now have four so we're squaring this error so any time we make an error we amplify that error now if we want to get an overall error we could use what we call the mean square error function with a mean square error function what we're basically doing is this but we're repeating this for each and every point and then we're finding the average of all those errors and as usual tensorflow already has this built in so all we need to do is just make use of this function which has already been built here we have our loss function you notice that it's under the tensorflow.keras, tf.keras let's roll up here we have tf.keras we've seen the sequential now we're looking at the losses so here we have this losses we could also check in this layers here and you'll see that we have our dense layer so we start to understand how this tf.keras section here is structured so here we have our dense layer which we've seen already the number of output units we're going to look at all this subsequently for now let's close this and get back to our mean square error so here we have losses mean squared error that's it there we go we have here some examples so we can look at this y true that's the y actual under y predicted by our model now doing the mean square error here we have 0 minus 1 that's 1 1 square is 1 1 minus 1 0 we have 1 plus 0 plus 1 obviously 0 minus 1 is 1 is negative 1 negative 1 square is 1 so for now we have 1 plus 1 because 0 minus 0 is 0 so when you add up all this we have 1 plus 1 that gives us 2 now how many different elements we have we have 4 elements so 2 divided by 4 it gives us 0.5 so we've summed up all these square errors and then we divide by the total number of elements we have which in this case is 4 so that's exactly how we obtained a 0.5 now we have that we just simply come right here and then we have from tensor flow that keras the losses we're going to import our mean squared error so that's it from that that's fine and we could make use of this mean square error now the way we make use of this is actually when we compile in our model so we have model compile and we define our loss so our loss is a mean square error which we just defined mean square error that's it and what this does is as we compile in our model we can take into consideration the fact that our error sanctioning function is going to be the mean squared error now for regression tasks apart from the mean square error we also have the mean absolute error and here every time we do the subtraction of the y true that's why actual and the y predicted instead of squaring this what we do is we calculate the absolute value so it's quite similar to what we've seen already with a mean square error just the only difference here is we have the absolute of the subtraction of the difference between the actual and the predicted value of y to better understand when to use the mean square error or the mean average error or rather the mean absolute error let's take this example you see with this example we have the horsepower and the current price most times the horsepower is positively correlated to the current price that is if we increase the horsepower generally overall we have an increase in the current price and for reduce the horsepower we have an overall reduce in the current price now we may have this point here which is actually what we call an outlier we see that we have a very high horsepower but the price is very low and in the case where we're using the mean squared error we'll be having y minus y is y a or let's say y yeah y actual and then y pred so we have y actual minus y pred and then we're gonna square all this you see that this is just one or is kind of like a minority in our data set and when modifying the weights of our model we don't want these to weigh too much or to have so much priority in the way we choose the values for M and C and so that said using a loss function like this which actually squares this errors is not a good idea because since this error here is going to be very large so if you continue with this this let's suppose around this so our model others are this X our model pick something like this will be around this and then the actual beyond this so we have this large error squaring this large error again gives us a very large value and so this will have very much to say when we're picking out this values for M and C as basically we're modifying these values based on this error so based on the error we're trying to modify the values such that we pick the values which minimize this error actually and as we've said in the case of the mean square error for these kinds of outliers we making them too important when we should focus more on these other data points now with a mean average error or with a mean absolute error we have YA minus YP and we computed absolute value now here clearly we understand that our overall loss or the loss we get with this kinds of outliers is reduced compared to this since here we just simply finding the absolute value unlike here where we're squaring and so in working with data sets where we have outliers like this is preferable to use the mean absolute error now there is a loss function known as a Yuba loss which actually permits us make use of this mean square error and the mean absolute error in a more intelligent manner that is when we have an outlier we are going to use the mean absolute error but when we have a normal data point we use the mean square error by normal data point we mean a point where the Y true minus the Y prepped which here is this X isn't or is less than a given threshold and by an outlier we're talking about points where the Y2 minus Y prepped is greater than a certain threshold so here we have the definition for the Yuba loss we see how this X X actually Y2 minus Y prepped so when coming the Yuba loss if we have this condition we use this variance of the mean square error where we have 0.5 times this Y2 minus Y prepped square and then when is greater than D so here we're dealing with outliers as these are threshold when we dealing with outliers we have 0.5 times D squared that's times this different squared plus this D times the absolute value of Y true minus Y prepped minus D so there's a formula the Yuba loss formula again we don't need to write all this from scratch since we have this with tensor flow so that said all we need to define here is our delta which is that threshold that defines whether a data point is an outlier or not so here again we have our losses we have the Yuba loss and then we have the mean average or rather the mean absolute error this actually Yuba simply so we run that there we go so yeah we yeah we're gonna compile our model let's take this we have the mean square we have the mean absolute and we could also have the Yuba so if you run this you see your model compiles you could just simply specify Yuba but we need to pass in our Delta so we're running this we use the default Delta which was given to us if we want to specify our own Delta we just specify that way so we could take 0.2 and stuff like that so yeah we have all the default data is actually is one so that's it let's take this off we're gonna be using the mean the mean absolute error now we do a lot of experiments so after we're gonna train our model we're gonna measure its performance and so if you measure the performance and you see that it isn't good enough you could come and modify the error function using so you could change the mean absolute error we about to use to mean square error or maybe to the Yuba loss and from there you try to see which loss functions permits you get the best possible performance for your model from here we'll move on to training optimization recall our model was linear function which we were trying to eat such that this year this M and this X are rather this M and the C right here peaked so that this errors here are minimized so just looking at this we could have another line drawn here we have this line we could have this other line so we could have the line and so many on infinite possibility of an infinite number of possible lines we could generate and so in order to get this M and C what we use the method commonly used today is stochastic gradient descent now let's understand how this works we'll just use this or write this out in this one formula we have a weight which has to be updated we call that initially our weights random so we randomly initialize our weights so this means here that if we randomly initialize M and C and then let's suppose we we have M to be 0 so we have M zero and then let's say C is one then in that case we would have something like this we would have a line like this and it's clear that this line isn't very representative of this data we have here and so what we could do now is to update this M and C such that the take up values which permit us adapt to this data set which will be given and the way that's done as we've said already is with the SGD algorithm and this is how it goes so we have a weight so we have a weight previous so we have the previous weight or in the case where we are the first step we have the initialized weights minus a learning rate so we have a learning rate the rate at which we're learning this data which we've been given times the derivative now in case you have no background and calculus you shouldn't have any worries as TensorFlow would take care of this so yeah we have learning rate times this derivative of the loss function with respect to that weight so yeah we have the weight previous so that said if initially we have zero and then here we have C let's say C as we said already is two now what happens is for each and every weight we're gonna take this so we want to get the new values for the weights or for M so we have M it's gonna be equals its initial value is zero so the previous value is zero minus now let's pick a learning rate generally learning rates are picked in the other 0.001 0.1 0.01 0.0001 and you could continue to 1 times 10 to the say negative 6 so yeah we're supposing we've picked 0.1 and now we have 0.1 so that's our learning rate times the derivative of the loss now what's the loss we call we have already seen three loss functions in our error management section so here we have in 0.1 times the rate at which the loss changes with respect to that particular weight so we have 0.1 times this we respect to M and the same is done for C so we're gonna do 2 minus our same learning rate times DL on DC we've now updated M and C and then now if we compute the error we would have Y actual minus this new values of M and C are going to be used here so we're gonna have our new M times X plus our new C and we compute a new error so we'll have a new and so from here we expect that as we keep on training so as we keep on training we want our loss to keep dropping up to say a value of zero since we want to have zero loss though in this case we see clearly that it's not possible for us to use just a straight line and obtain a loss of zero since we cannot pass through each and every point with a straight line so if we're able to build this kind of model then would have a loss of zero since for each and every point the actual output is the same as our predicted output and so we'll obviously have zero loss but with a straight line that's not possible so as a recap we see that we have our inputs and we have our outputs we have M and C that's our model parameters we pass our inputs using our initialized M and C we get outputs we compute a loss that's we compute the difference between what we actually supposed to get and what our model predicts get this difference we could use a square or just absolute value function we get this loss and based on this loss we modify these values and then we repeat the same process to our training converges our training converges simply means we've attained a point where our loss doesn't increase anymore so if we have training like this we could get to this point to see at this certain point we keep on training that is we keep on repeating the gradient descent step which we've seen already where we updates by doing W equal W minus a learning rate times DL over DW so we repeat the step which we've seen already and if our loss doesn't change much or doesn't even change at all then our model has converged and we could stop training at that point as usual TensorFlow does all the hard work for us and so here we have model of feeds we pass in our X that's our inputs XY we specify the number of epochs let's say 100 verbose equals 1 now we understand what this X and Y means that basically our data set now the epochs or the number of epochs here is specified the number of times we are going to update our weights so the number of times we are going to go to the gradient descent step and for verbose which it has to do with the outputs from our training step so yeah we've run we've compiled let's correct this we have okay so we have that we compile that's fine and we run this you see because our variables equal one we are able to get these kinds of outputs so here we're going to see in real time the values of our loss let's stop this so we stop that and then we send variables to zero you will notice that as the training goes on we don't get to see those loss values anymore so let's take this back stop this you see variables equal one now we're able to see a lot so as we go from one epoch to another we are able to get the mean absolute errors that the total mean absolute error which is an absolute value of our model predictions or the absolute value of the difference between our model predictions and the actual current prices when we get into this tier the Kerasone model documentation and we go to compile you see we could have this right here we have the definition of compile we see how by default our optimizer is RMS prop now note that this optimizer or those different optimizers are essentially variants of the stochastic gradient descent algorithm so yeah we get into optimizer as you could see we have added Delta out of grad item and you see you have SGD so this is the SGD we've seen already right here so we can have this SGD will specify the learning rate this point we understand what learning means we have the momentum we could increase this parameter so that we could speed up training and we could also specify whether we're having a nester of type momentum right here so when we see nester of true then we're having a nester of type momentum so that's it of all these optimizers the most commonly used is the Adam optimizer so you'll see that many sand is in practitioners generally use this optimizer by default the learning rate is 0.001 beta 1 0.9 beta 2 0.999 epsilon or 1 times 10 to the negative 7 and the AMS grab time is set to false to better understand the learning rate let's take this example so here we have loss versus a weight like the M or the C and now we have the derivative of the loss with respect to the weight so let's consider that this derivative is positive so let's take a point like this we have the weight here is this particular weight let's say W I would have picked out this point and we have this derivative of the loss with respect to the weight or the partial derivative actually and then we have let's say we have the slope so the slope is positive to update this weight from W I to a new weight we apply this formula so we have W I minus a learning rate times this positive slope now if this learning rate is too small let's say 10 to the power of negative 10 so we have a very small learning rate it's clear that we are not going to have a great difference between the W we obtain after this update since this is going to be multiplied by this positive value and W minus a very small number will give us value very close to W so there's not going to be a very great change after this update we're gonna have a very small change actually smaller than this now if L R does a learning rate is too large let's say we take a learning rate of 10 to the power of 3 in that case we have W minus a thousand times this positive value then the change is going to be too brutal and instead of say going slowly towards this point we are going to actually skip that point and get to this other point right here and if we keep doing this we actually go away from this point so we're picking out points which are far away from this point now why is this point important this point is important because at this point as we could see here the loss is minimal so recall that as an aim of this error sanctioning section where we're trying to minimize the loss that said many times by default or from many experiments starting out with 0.001 is going to be a good idea but note that you could always change this so we could feel free to change this you could take something bigger recall that if you take a bigger value your training is going to be faster but risk divergent that is risk not leaving you towards that minimal loss whereas if you take it too small then you would converge but your training is going to take too much time so this value is kind of a great one that is very commonly used now particularly for this item optimizer we have the parameters beta 1 and beta 2 now your beta 1 max and beta 2 max are all 1 so taking a value like 0.9 and 0.99 here means we're taking higher values of beta 1 and beta 2 you want to speed up training you want to increase the values for beta 1 and beta 2 by default this value is set to 0.9 0.99 now when doing or carrying out computations respect to the item optimizer we're trying to avoid dividing by zeros so we have this epsilon parameter here which is by default 1 times 10 to the negative 7 now if one of the AMS grant variants of the item optimizer you could set this to true so that's it we now have from tensor flow that harass the optimizers we're gonna import Adam so we've imported that that's fine now we're gonna make use of that so actually in this compile we have optimizer equals Adam and that's it so we'll define our optimizer and we'll define the loss so let's run this again we actually had an arrow so let's take this Adam run that that's fine and there we go now we've been getting these values for loss what if we start this in a variable so we could have history equal model of feet so this time around after doing the training we'll be able to recall all this loss values we got during training there we go we have history the history we run that and here we got all our lost values let's climb up and you see here we have this dictionary we have lost and then we have this list of all the lost values we now import matplotlib so we now paste this out here we specify this plot we have history the history which we've seen already I was specified a loss because this actually a dictionary will pick the loss we specify the title ylevel xlevel the legend and then we show this lot so this is what we get right here we see how we drop our loss value is dropping but the way that which is dropping is actually quite small so what if we speed up the training by modifying the learning rate so right here we have learning rate equal 0.1 or let's just pick a learning rate of 1 and run that we run this and we see that the loss actually drops faster this time around from here we move on to performance measurement we actually measuring how well this model performs and one common function used in regression problems like this is the root mean square error you should note that is not in every case where you would have the performance measurement function similar to that of the loss as this two actually quite different concepts for the performance measurement are making use of the performance measurement we are able to see whether these two models so supposing we have two models model one and model two have the same performance or do have different performance and which of them outperforms the other so if we run performance measurement we'll be able to see whether model one out performs model two or model one under performs model two that said we could include this metric in here so we have the optimizer loss and the metric so we have this root mean square error metric which has been added and which has already been imported so imported this year note that this is a metric not loss so we have this metrics with important means square error and if that's fine we could go ahead compile and start our training again now you'll notice that both the loss and the root mean square errors are being printed out so this what we could see now after the training will be done we'll be able to plot this to out so here we have the loss to see we run the loss and we have the root mean square error so this is what we are putting to get this exact string right here you could do history dot history so here we have the history and you would see that okay here you have the loss so we have the first key loss then we have this list of all losses and then we have the next root mean square error and then we have this list made of all the root mean square error values so that's what we that's how we get that we've plotted it out and this is what we have another method which comes with tensor flow models is the evaluate method so here we could have model dot evaluate and we simply pass an X and Y so we evaluate our model this way so you see we've evaluated our model and we have the models loss and the root mean squared error from here we move to validation and testing to better understand validation and testing let's take a simple example imagine you get into class that is you get a class on the first day and you are being taught some course materials so you've been taught this course materials throughout the whole term or say the whole semester and then at the end of the semester the teacher who is feeling lazy tells each and every student to feel free to produce their own exam and after producing this exam they should sit for this exam produced by themselves so they could feel free to come up with any kind of question sit for this exam and after sitting for this exam the Mac describes by themselves it's clear that most students will have max greater than say 18 and 20 and this is simply because this exam has been set by the students themselves now it may happen that a student has followed through the course work and mastered everything and then set real questions or tough questions and got this Mac of 18 same as other students may not have terribly gone through this course work and produce very easy questions which will get them a Mac of greater than 18 without any much effort or without mastering the course work and so that's why the strategy is very dangerous as we need that external validation from our teacher and so far what we've been doing here is kind of like the first method where each and every student produces the exams writes sits for the exams and then marks the exams by themselves and that's simply because here we're using our full data set training on this full data set and evaluating this performance without taking into consideration data that a model has never seen so the idea is to be able to create a model that when it sees new data is able to come up with a reasonable current price which is as close as possible to the actual current price and so when dealing with machine learning models it's important to say break data set into two parts so if you have like the example we're working on 1000 examples you could break this up such that you have eight other examples which you train your model on and then you could test that model on another 200 examples so here we have never other model has never seen this part of your data set right here and this is one very important use of the shuffling because with the shuffling we are sure that there is no bias in the way the data set is is constituted as now if we just break this up we have one part for testing another part for training which has been randomly built and so as we're saying if a model for example has a performance of say root mean square value equals let's take say five on the training data so on this training data and then on the test data it has a root mean square value of 50,000 then it's clear that this is a very poor performing model as it does well on only on data it has seen but on data it hasn't yet seen it doesn't perform well and so recall machine learning is all about empowering the machine to do stuff humans and them to do and obviously humans use some intelligence and doing all those tasks so if you want to build a model you have to ensure that it performs well on data it has never seen and so we always have to split our data set so firstly we shuffle our data set very important we split this data set before proceeding with modeling and training now sometimes we don't want to wait until a model are strained before testing it out and discovering that it performs very poorly on data is never seen so what we want to do is while doing the training we want to be able to see how it performs on data it has not seen and that's why we have a validation set so that's why I need a validation set we'll see now we need a testing set as you can see here and then now we need a validation set and that said yeah we could reduce our training 600 and then we increase this so we have 200 so we have our data set of a thousand data points which is broken up now into our training validation and testing now note that if you have a data set of say let's increase this of this how many this is a hundred if you have a hundred million data points then taken for example like previously we chose this to this is actually 60% 20% 20% but if we have a hundred million data points we could always use 90% year year we could use 5% and year we could use 5% reason being that since the total data set is very large getting 5% of a very large number is already quite a huge number of samples so that is good enough for us to build our validation and testing sets we get back to our data set preparation let's include a cell here we specify our training and validation our training validation and test ratios so we're gonna use 80% of our total data for training or 10% for validation and 10% for testing the data set size we have a year the length of X is a thousand and we run this we added a code cell actually right here at this then we have this X train Y train made of elements in the top 80% position as here we have data set size so if you have 1000 times this ratio would give us 800 and we have the first 800 elements same with this we print out the X train shape and we could do same for Y train so that's it you have this shape right here so that's it we have booking this up thousand now we have a hundred let's do same so let's add this and do same now for the validation so instead of training we have valve and down here we have valve so we have valve and now we want the next 10% so what we're gonna do here is we simply copy this that's it here we have the validation ratio alright we have we're going from the train ratio actually so we have the train ratio up to the train ratio plus the validation ratio because we're going from 0.8 now to 0.9 that said we would have this plus the validation ratio so that's fine next we let's just copy this so we've copied this there we go take this off we get the next values so that's it we've got in this next values oh we have X right here we have X valve X valve and here X valve so we run that let's close this right here so we have this we run it that's fine okay so now we have the next in our next hundred and that we now get the last hundred so since we have no other section we just all we need to do is just specify this specify this to validation actually but right up to the end so we have that we run it and there we go we have the data set which has been split into the training validation and testing we have X train Y train X from Y valve yeah we have X test this should be X test so let's change this X test X test X test okay so we run that and that's fine now one important point to notice you have to avoid information leaking from the training into the validation and the testing so this means even when doing normalization and you're trying to adapt your normalizer to your data you have to not use the validation and testing you just have to use the training so here we're gonna use only the training set to adapt our normalizer to our data from here we have to modify the way we do our training so here we just have to specify that we have X train we have Y train and then our validation data is equal we have this X valve and Y valve so that's it so here we specify X valve Y valve as your validation data and then you have X train Y train as your training data now note that when doing this so with a validation data you actually specify X valve Y valve but there is another argument which is the validation split where you just specify the fraction of the training data to be used as validation data so again here as usual you could go through this documentation you see you have shuffling which you could do but we should have done already you have all these other arguments which you could check out in this documentation getting back to the code we run this we run our training now training is done you could notice how we have this extra outputs here the validation would mean squared error and the validation loss so you could see you notice this right here let's get straight away and to put in the spots again let's do history let's add this so here history history here we see we have the validation loss go up we have the room in square error we should we should get the validation room in square error so let's scroll down there we go we have the validation would mean square error just right here so now we have the training and validation which has been outputted during training process we call the use of the validation is for us to be able to see during training how well our model performs on data it is never ever seen before so let's go ahead and do some plotting here we have our loss we have this right here we specify the validation loss there we go let's run that okay so we see our validation and training loss let's this year we have our loss so we specify the legend okay there we go we have our training data and we have our validation data we see that our validation data our model does better on our validation data actually as we have lower loss values for the validation data we could repeat the same process with the root mean square error so here we have that there we go we now put in your validation so here we have validation and then we have validation we run that and it's kind of like similar here the validation does better or the model does better on our validation set from this point let's take this off we get back to model evaluate so let's evaluate our model by just specifying this wouldn't help so you could actually put this X valve Y valve so we're not evaluating anymore on our training data but not a validation data and we could also now evaluate on our test data recall our validation was used during training we saw how it performed during training now we're gonna evaluate our model on data it has never seen so we evaluate on our test data and this is what we get you see we have this loss and this root mean square error value at this point we'll train our data or we've trained our model on our data and we can get to that long awaited part that is testing our model so here we are not just evaluating our model or doing some validation but we are passing in data and then we are allowing our model to predict the car price for us so that said let's get straight away to that we have X test which we know already well which we built already there is it extend our shape there we go let's pick out a value from this X test let's just say X test 0 so what we do is model that predicts so now we've trained our model we will now do model predict how we pass in X test so here we don't need to pass Y test we just pass X test so here we pass X test and our model should predict the car price for us now passing X test we have all those predictions so because we have a hundred you see shape there we go we have a hundred different elements so for each and every data point of for each and every input in our test and set we have all these outputs let's just pick out a few let's say we want just the first so there's it there is one which is normal because the input shape here has to be of type batch size by 8 but yeah we send in this shape 8 so we should do expand games in case you don't understand this well let me repeat again or let me just do this test zero that shape that's a shape you see that's the zero shape now X test the shape that's the shape now this model as we defined previously you know input right here you know input takes in batch size by 8 so here we have to ensure that if we're passing in any data it has to be of the sheep so now we have put in this and what we know to do is to expand them so that will leave from the shape 8 to the shape 1 8 so that it takes this form and when is like this it means our batch size is 1 so that said let's do expand teams we've seen this in the previous section we have expand teams there we go we add this axis equals zero that's fine and we run that so that's it everything works fine now let's take this off we run that again and we have our response so this tells us that for our first point in our data set or first input our car price prediction is this now oh this is very interesting to view but what if we compare this with what the model or what the data already presented to us because we it's true we level this as test but we know the actual values so we know the actual car price and what if we compare it with this predicted car price so that said let's look at why test 0 and check out this value so here we have a hundred and thirty thousand dollars and here we're predicting eleven thousand nine hundred and forty dollars so clearly our model is performing very poorly so visually understand this form model performance we are gonna plot a bar chart showing the predicted and true values shown sidebyside let's take this to say 20 and there we go as you could see right here scroll okay that's fine as you could see we have this blue which is for predicted and then the orange for the actual values of y so we have the predicted and the actual prices which is shown to us now the world is out is quite simple we have we define we have the figure size we have this bar so yeah we put now some bar charts and we specifying the position of each element respect to the bar chart now there's in here if you plot this out so we're not we also have a non pie to get this so let's run that and you see that this is a list comparison a value from 0 to 99 so this permits us get the position for each and every element so we have a hundred different positions and then the width obviously this Claire let's reduce the width so you understand that you see that visually let's take this let's increase it to one so we've increased the width you see this becomes very large now let's take that back let's see 0.1 you have this you see comes very team and that's it so we could play around with a width now once we fix a position say we'll fix this position we now move with steps further so that's what we do and we have that passed in here we have the level X level Y level and then we put that out so that's how we come up with this plot we shows clearly that our model is performing very poorly this takes us to our next section on corrective measures this one model of performance actually has a name it's called under PT so when you're training loss looks like this and your validation loss normally validation loss should be above the training loss but in some cases like as we just saw it was below the training loss so we have a validation so let's say we have our training and then we have our validation losses which are like this and even after training for a long period of time or for so many epochs we are not able to go below a certain threshold this is known as under feeding and the idea here is to bring this loss values like this so we want to modify our model so that we could have better loss values that's we could reduce our loss values as much as we can that said in order to do this it suffices to make our model more complex for now the model we're having is like this so here we're having this simple regression model where we have our inputs right here and one m2 m2 m8 our weights and then our bias C we add this up and then we have our output so what if we don't we take this off and then we add up more neurons right here so instead of just having one we're gonna stack up more neurons but they are doing basically the same thing so kind of like repeating the same process here so we're gonna have this link to this this one link to this the same way is linked to this one we had previously this link to this and so on and so forth and then we'll do the same process for this link then from here so here we have this dense layer from here again we could add more dense layers so this is what we call a hidden layer here we have a hidden layer we add more so here we could have one and you see here all these are all linked up this way and it's actually the same operation mx plus C so basically we're taking this weight times this plus bias and then we get in this take this times its weight and we get this plus bias we have this and then we continue this same operation so we have this stacked again we have first hidden layer in layer one in layer two and we could even go to another one again so let's add this to here let's add this other hidden layer let's do this there we go we've created this turret hidden layer so here we have our inputs and we have three hidden layers and obviously since we just producing a single price from there we could have this so we now have this our output layer here so input output and then hidden layers here we have this so we have this dense layer and we have also this dense layer let's try to write out or sketch out some tensorflow code here so basically we're gonna have dense so we have a dense layer right here and what does it output that is this first dense layer has key outputs basically comma and then we move to the next dense layer so if I have an at least we have this dense layer right here the outputs unlike the previous one we have just one output now we have this dense layer with two outputs we stack the next dense layer so you see the way its easy to carry out all these operations with tensorflow we stack the next layer with four so we have four outputs then the next we have two outputs and then finally we have one output it's very important to ensure that we have this one output because that is matching up with our data we have a hint where we have one output and we put this in bracket and then have this sequential and all that so that's it we have seen how we could do the real returns of flow and there is one point we need to mention that's the activation functions activation functions since we put nonlinear functions which add even more complexity to the model you saw how previously we had these inputs and then we had this one output and we linked them up like this now we've made this model more complicated we've added more hidden layers so that we could learn more complex information stored in our data set now another thing we could do is add in the activation functions as we have said already and make this model even more complex common activation functions are the sigmoid activation function, the tensh activation function, the relu activation function rectified linear unit and the leaky relu so we could have this relu that is when x is greater than 0 we maintain x that is x remains the same or the output equal the input but when x is less than 0 the output becomes 0 now here when x is greater than 0 the output is the same when x is less than 0 the output is negative of certain alpha times x hence the term leaky as this alpha is generally a very small number so if we have 0.1 we could have negative 0.1x which is kind of like giving us very small outputs which are kind of close to 0 now we have the sigmoid activation function 1 divided by 1 plus e to the negative x see looks like this the tensh e to the x minus e to the negative x divided by e to the x plus e to the negative x for now we are going to use this relu activation function and you will see subsequently that all these activation functions could be gotten from tensor flow parast activations so note that if we are to apply for example the sigmoid activation function right here what we will do is just after summing up or just after multiplying the weights by the inputs and getting the outputs once we have this we are going to do sigmoid of this so we do sigmoid so actually for each and every neuron right here we have the sigmoid of this so here we have sigmoid of our computations or the output of the computations here we have sigmoid here we have sigmoid and in the case in this layer we have the sigmoid then for each and every neuron we have sigmoid if here we have for example relu then for each and every neuron we have relu activation we now have all that is needed to make our model perform better and stop under feeding and so we come right up here we have a dense layer now let's say we have 32 so our op will be 32 neurons then from this we have 32 again and then we stack up another one and finally we have this layer right here recall that we've added up this other dense layers but at the output we need to have just one neuron so it's important to ensure that our output here is one we now specify the activation so we have activation equal relu and here we do the same for our output here we are not going to specify any activation because we don't want to interfere in the way our model comes up with its outputs that said we could run this and to make our model even more complicated we could increase this value so we could take this to 128 let's run that again you see that we have many more parameters we are training on again we are still having the 17 nontrainable parameters so when we had this when we had this you see we had 17 and 9 trainable now that we have this you see the number of trainable parameters increases while the nontrainable parameters remain on the same spot we have the 17 nontrainable parameters which is logical since our normalizer remains the same now with that said we've done that we could put our model so we see this plot right here that's a clear plot to see how we live from the input to this dense layer see here known by 8 known by 128 this next dense layer this other dense layer and finally this dense layer from there we compile our model let's take this so here we have 0.1 let's run this and we are ready to fit our model so let's run that we now notice how this loss right here is a smaller order of magnitude as compared to what we had previously notice how this drops from 100,000 about 144,000 and goes down to like 30,000 so that's it for this training we now go ahead and view our plots so there we go we see we have totally different plot now we live from this and then we actually drop to about 30,000 also notice how this time around we have a validation loss which is higher than that of the training and this is normal because obviously the model was trained on the training data so it would turn to perform better than data it wasn't trained on now if a model performs very very well on the training data and doesn't perform well on the validation then we have a problem which is known as over feeding and we are going to trade this in subsequent sections so for now we just continue we cut out the root mean squared error notice recall how we used to have values of this order 234,000 but now we are around 30,000 okay let's run this what do we have okay so we have something very similar to the mean average error that's our loss that's fine we evaluate our model so that's it we have this loss for our model on our test data so here is our loss on the test data here is our root mean squared error from there that's fine okay we do this white bread and here is what we now obtain we notice how this model performs way better than that previous smaller model and as you could see here for this particular test data point you see our what our model predicts is exactly the actual current price our model isn't perfect but it's doing quite well at this point we've taken up some creative measures and our model now performs better what if we look at how to load our model even faster and more efficiently this can be done thanks to the TensorFlow data API so right here you have tf.data you can go to the overview and then for now we check on this data set right here this class is made of many methods so you have those different methods here well now we'll start with the fromTensorSlices method so from here we're going to adapt our code to use TensorFlow's data API in order for us to gain from all the advantages that come with it now note that when you're working with a data set of say a thousand elements you wouldn't see or you wouldn't clearly notice the advantage of using this data API but as your data set gets larger it becomes very important to master how to use this API now that set we're going to redefine our XTrain so yeah we're going to say train data set train data set and we're going to have tf.data.data set .fromTensorSlices so that's it now we're going to have this tuple which we create and then we pass in XTrain and YTrain so that's it now we've gotten this the next thing we're going to do is we're going to shuffle our data set now we've already done shuffling but in the case where shuffling wasn't done previously you could do the shuffling very easily now so here we have this.shuffle and we specify a buffer size buffer size let's say 8 and let's see exactly what this buffer size actually means so here we have a seed and you have reshuffle each after each iteration now let's look at how this buffer size or what it means exactly so here we have been told if you did a set contains 10,000 elements by buffer sizes set to a thousand then shuffle will initially select a random element from only this first 1000 elements so it's just like saying I'm picking this first 1000 elements and I want to carry out my shuffling only from this first 1000 elements such that when I'm picking a random value obviously when we're doing shuffling we're picking out random values in a random order so when I'm picking up my random value I'm going to pick it up from this first 1000 values then as I said here this once this element is selected its space in this buffer is replaced by the next 1000 first elements so if we have a data set like this with one, two, three, four, five, six elements and then we want to shuffle with a buffer size of three in this case we'll initially select or pick up a random value from this here from this buffer from this first three elements and then if suppose we pick this random value right here what we'll do let's take this let's annotate this one, two, three, four, five and six so if we've picked up two we have picked up two now we're going to have our next buffer to be like this we're going to now have one, two, three and four so this next element is going to be added to our buffer and then we'll have this five, six, five, six right here and once we're done with this so once we pick up another random number let's say we pick up four, we pick up four here our next buffer will be one, three, five so we have one, three, the next five one, three, five and we repeat this and so on and so forth so that's how we work with this that said yeah we could also pick reshuffle each iteration so after each epoch we're going to reshuffle again so that's it we've had our trained data set shuffled and then we have again trained data set bashed so we could just add this right here actually we could say we've shuffled then we batch this by size 32 and then from here we could do prefetching so let's do some prefetching prefetch we're taking a documentation to see what it takes this argument to see again here we have a buffer size now we are told that this allows later elements to be prepared why the current element has been preprocessed or been processed and so if we have a data set like this so let's suppose that we have this three elements element one, elements two and elements three so currently we are actually training on this data or on this batch here we're actually training on this and generally when we train on this we have to train after training we have to load this data process it in case we need to process it and then so let's let's suppose that this is processing time so for our key this is training time after we process then after we train after we process this here so after we process that and then we train initially we even process this so initially we will have this so we process this one we train we process we train we process with train now what if instead of going to these steps we load, so yeah, we start by loading, we load. And then while we're training, we actually loading up our data. So we could be loading up our data while we're training. So this could be done. And then after training, we have this loaded up data already. So we just continue with the training. And then from here, again, while we're training here, we load it up. So loading up, suppose we load this up like this. And then from here, from this point, we train. So recall we have one, two, three train steps or train blocks. So at this point, we add our last block, so that's it. So from here, we see that we take less time to load and train our data as compared to this first method. So working with prefetching is very important. Now, this prefetching takes up a buffer size, which can be autotuned. So here you have tf.data.autotuned. In case you don't want to specify the buffer size, you could just simply allow tensorflow dynamically tune this buffer size for you. Now that's set, yeah, we have this. So we have prefetch and then tf.data.autotuned, that's fine. So that's our data, let's take this off. So we now have train data or train data set. Let's add this and let's run this actually. And then for T in train data set, we print out C. Now with, okay, we'll batch that. So let's print out C and then let's print out just the first element actually. Let's say for x, y, so we could print out x and y separately. So we have x, y, there's a first, that's it. You see, we're going to have this, like this, let's go up. Okay, so yeah, we see we have this right here. What's its shape? So we have 32 by eight, which is normal batch size by eight. And then we scroll down, scroll down. Yeah, we have 32 by one, which is normal. So that's very normal. We've bashed our data and we're ready to train this data. So let's just recopy this and do the same for the validation right here. We have a validation, val data set, and we have val data set. Don't forget to change this. So you have val, that's it. That's fine. We run this, our val data set. And now we go to the test. So right here, you could also do the same for the test. We have that, test, test, there we go, test, test. So that's what we need to do to convert this so that we could use it as the, or we could work with a TensorFlow data API. We don't do any modifications in the model. All we need to do here is come and specify, train data set. And then for validation, we have our val data set. So here we have val data set. So that's fine. You see training has started and everything seems okay. So we'll now go ahead and do this right here. So that's what we get. So it still continues dropping. Actually we could, from the previous training, we would have continued training. So we could have increased the number of epochs. That's fine. Let's look at this as well. And we could evaluate our model. Now note that this actually isn't very different from what we had previously as you may feel like this is producing a different kind of plot. But the model is in its current state already had a loss or was around, had a loss of about 30,000. So if we have to recompile our model, so right here, if we have to run this, recompile our model and feet, you get to see that it isn't much different from what we had previously. So this doesn't actually come to better our model or model performance actually, it comes to speed up the training. So you wouldn't expect to have better loss values with a tf.data, but instead you could attain this better performance faster. So let's run this again. And you get to see that it's kind of like similar to what we had already. So that's fine. We run this as well. You could always increase the number of epochs. You'll see that here, that's what we'll get. And so you could predict with this, fine. And now we could go ahead and view this on our bar chart. So we look at this similar to what we had already. So that's fine. We now master the basics of working with a data API. Though in subsequent sections, we'll look at even more interesting ways of working with this API. That's it. Hope you enjoyed this. Thank you for following up to this point. Don't forget to like, subscribe, and share. See you next time. Hello everyone, and welcome to this session. According to the World Health Organization, the estimated number of malaria deaths stood at 409,000 in the year 2019. In this section, we are going to build a machine learning model based on convolutional neural networks to diagnose malaria in a person based on cell information gotten from a microscope. In this section, we'll start with loading this data. After loading our data, we are going to visualize this data, process this data, build a model suited for this data, then train this model, and finally, evaluate and test our model. As usual, we'll start by defining the task, which in this case entails correctly classifying whether an input cell contains the malaria parasite or not. Then we'll go ahead to prepare our data. This data is going to be made available to us from the TensorFlow data sets. Then we'll build a model. The model or the particular model we'll be working with in this section will be the convolutional neural network. Then from here, we'll define the error function. We'll go ahead and train our convolutional neural network. We'll check out on some performance measurement metrics like accuracy, F1's core, precision, recall, and many others. Then we'll do validation and testing. And finally, we'll take as many corrective measures as we can. In essence, what we want to do is to build a model like this, which takes as input this segmented cell from a team blood smear, and say whether this segmented cell right here is parasitized or unparsitized. We're supposing you have no medical background, so we'll briefly look at how malaria diagnosis is related to those segmented blood cells. To start, we get infected by malaria once we are beaten by a mosquito. These mosquito bites usually lead to the passing of Plasmodium pacifarum parasite into our blood system. And so to diagnose whether a particular person has got the malaria or not, it's important to get that person's blood. Here you see how the medical practitioner has to select the finger to puncture, usually the third or fourth finger. Then to obtain this blood, you puncture the side of the ball of the finger. In the case the blood doesn't well up, you would have to generally squeeze the finger so you can obtain the blood. Then always grab the slide by its edge. So to control the size of the blood, drop on the slide, touch the finger to the slide from below. From that cdc.gov website, we'll get to this microbe notes website where we'll get more colored images. Now we've obtained the patient's blood or the person's blood and there are two possibilities. One is getting a thin smear like this and one is getting a thick smear. In our case, our data set is gotten from a thin smear like this. So here we have this thin smear, which when passed on a microscope, produces images like this. So here we have thin blood film and we have thick blood film. From this point, we now segment the cells. The cells are now segmented and that's how we obtain an image like this or a segmented cell image like this, which now could be used by a model to predict whether that patient has got the malaria parasite or not. It should also be noted that we're dealing with a classification problem since our output can only take two discrete values. This type of classification problem is known as binary classification. Since throughout the section, we'll be dealing with image data, it's important we understand how image data is represented. So here we have this image of this bird and then when you zoom in, this is what you get. If you zoom in again, you should have this. So this tells us that this image right here is formed by combining all these little boxes we have right here. And these boxes are known as pixels. That said, here we could localize this pixel. We have this pixel, this one, this one, and so on and so forth. So basically, this image is made of all these little boxes which we call pixels. If now we take this image from our dataset, you could see that, notice from here, we have 86 by 82 PX, 86 by 82 pixel image, meaning that we have 86. Notice how here we go up to 86. So at this point, we have 86 and the width. So we have 86 of these boxes. If you have to go from this point to this other point right here, we would go through 86 of these pixels. And then if you have to go from this point to this one right here, you have 82. Now check out what is displayed at this position. So as we move, you should be able to see what's displayed. You see that at this point, for example, we had position 85, 34. So we've gone 85 steps horizontally and then 34 steps vertically. We're considering our origin to be at this top left corner right here. So we have 85 steps and then 34 steps in this direction. And then you should be able to localize these pixels right here. So you could notice how all those pixels now, when combined, will be able to form this image. To reduce this, you see that it becomes less evident to notice those pixels. So that's it. It should also be noted that each of those pixels contain values ranging from zero to 255. So here we have values ranging between zero and 255. And for each and every pixel, we have three different components, the red, the green, and the blue. So if we break this image here into these three different components, we would have something like this. Now note that these values are actually normalized. So basically what they've done here is they've taken all these values and then they've divided by 255. So if you want to get the unnormalized values, you should be able to take this and multiply by 255 to obtain the original values. That said, notice how at this point here, this position, we have this. At this position, we have this and this. And so we could represent image data in terms of the height, the width, and the number of channels, which is in this case equal to three. So we have height, that is the shape of our image tensor, height by width by three. And then another common format is the grayscale format. With the grayscale format, we, this number of channels equal one. We have just one channel, and it could be represented as a 2D tensor. Another interesting point to notice, though we've said for each position or for each pixel, we have a given value per each per component. We have to note that all these values fall between black and white. That is for black, if you want to get a black value, that the pixel value will be zero. And then for white, the pixel value will be 255. Obviously, when we normalize these values, here we're going from zero up to one. So that's it. In this data section, we'll make use of a dataset contained in the TensorFlow datasets module right here. So here, you just have to pick your problem and you will have a whole lot of datasets available to you. In our case, we're dealing with image classification. So we select this, we scroll down and we should get malaria. So here we have malaria. So we double click on that and here we go. We have our malaria dataset. Now you could visualize your dataset and you could get some information of all description about your dataset. The malaria dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells from the teen blood smears like images of segmented cells. So here we are going to explore data. You could come right here, draw the border. So that's why you have this border and then group by level. Notice how we have the parasitized level and the uninfected level and both have equal number of images. So in fact, what the scientists have done to get at this dataset is they have gotten this segmented blood cells from tested malaria patients and this cells right here from tested nonmalaria patients. Let's click on this. You see, we have our cells. There we go. You could always visualize all the cells here and then let's get more information about the image content metadata aspect ratios. You see, there we go. We have the format, this is PNG, mega pixel resolution, mode, RGB mode. We're going to look at this shortly. Pixel height, 112, pixel width, 91. Also note how not all the cells have exactly the same pixel height and pixel width. So they all have different pixel heights and pixel width. We have other information about a dataset like the homepage, source code, versions, download size, dataset size, autocached, splits. In this case, we just have a train split. So it's left on us to split the dataset into train, validation and testing sets. We have the features here. We see we have an input image and we have the output level. Note how the number of classes here equal two. Now we could look at some examples. And finally, we have the authors. In order to load this data so we could make use of it, we're going to make use of tensorflow.dataset.load method right here. You see that this method takes several arguments and this here have default values, but we need to specify the string. So we need to specify the name, which in this case is malaria. This also has some outputs. So if you get this or if you succeed to load the dataset, then you should be able to get this output. Notice now how this integrates with a tensorflow data API, which we talked of in the previous section and which is an efficient way of dealing with tensorflow datasets. So here, all you need to do is to load the data and you should have your tensorflow dataset with the dataset information. Let's now dive into the code. We start by importing tensorflow, numpy, map.lib, and we shall import tensorflow datasets as TFDS. So we import out tensorflow datasets, that's fine. And we could go straight away into loading this dataset. Here we have tensorflow.load and we specify the name malaria. So once you specify this, you should obtain your dataset. Recall we have our dataset, let's say dataset, and we have dataset info. So that's it. Let's run this. You see, we downloading this dataset. After loading the dataset, we obtain an error. So let's go ahead and check in the documentation. By default, this with info information right here is false. So trying to get this dataset information right here, we'll try an error. Let's now add this with info, right here with info. We set that to true and we run again. So you see that now all is well, we have our dataset. We have our dataset information. Wish we could check out right here. So we have dataset and then we have dataset info. That's it. Clearly we see prefetched dataset. We have the image and we have the level. We have for data dataset, we are going to print out our data. And then after this, we just take a break. We could also do for data in dataset.take. So just to take one element of our dataset instead of doing this, let's take this off. So let's run that now. We have an error, we told the object has no attribute take and that's because let's run the dataset again. We run that and you see, we actually have this dictionary made up the train, which is our dataset and then we have the types. So let's specify we want to get just the train right here. We have dataset train and we will run that again. So now we have our data. You see, we have this 103 by 103 by three. That's our image. And then if we scroll down, you see we have the level. So we see it's one. Let's scroll back and let's say we take two values. So let's take even four values and see. So we have those four values. You see, let's scroll down. You see here we have one. Scrolling, we have this other one, zero, different levels, different images. And here we have one and here we have one. So that's it. We see how we obtain the images and the corresponding levels. We now pass in more arguments like the shuffle files as supervised and the split. Here we have more elaborate explanation of how we can work with a split. You see, we want to split our dataset and so say train and test. You could do this, you could simply pass this and all you could specify the percentage of the training set on the whole dataset and could specify the percentage of the test set. So there we go. We simply add as supervised. There we go. True shuffle files. There we go. True. And then finally we specify the split. So we have our split. That's it. We are going to pass this out. We have the train test. So we could just simply do train test. Let's take this off. So for now we have the split, train test and let's run this. We obtain this error telling us that there's an unknown split test and that the splits we have here should be part of this list train. Now, actually in other datasets available in TensorFlow datasets, we already have the train test split. And in those cases, we could make use of this kind of split. Now we only have the train. So here it's needless even putting the split or you could just, let's just take this off. Let's take this off now. And you'll see this should work fine. So that's it. So we have our training set. We'll split this separately. And since we've shuffled the files, now if we run this, we'll notice how the shape is going to change because our files are now shuffled. So let's go up, scroll this up. There we go. And you see, we have different files. Whereas when we don't shuffle or when we set the shuffling to false, you will always have the same image at the same position. So that's it. Info specified and that's fine. In order for us to create this year, that's to create the train validation and test sets. We are going to make use of the take method and the skip method. So let's take this example. We have this little dataset, we'll create it. And we want to print out the dataset after skipping seven values. So here, you see, if we skip seven values, we have this. Now, if we take six values, you see we have those first, rather your take. So let's put your take. We have those first values. So let's do this. At every level, we print out dataset.s nonpy iterator. So we print out a dataset. We have dataset take, we take the first values. Let's define this percentage. All the ratio, we have the train ratio is equals say 0.8. Val ratio equals 0.2, no 0.1. And the test ratio equals 0.1. There we go. We have the train ratio, validation ratio, and test ratio. Now we are going to pick this up. So we have the length of our dataset. We could, let's comment this part and then print out this length. There we go. We have length of dataset. And as you could see, we have 10 elements. So when we run this, you have this element here and our length equals 10. Now that's set. To obtain the train dataset, all we need to do is simply take the first 80% of our dataset. So here we're going to have this, let's say train dataset. And then we take 0.8 times the length of, let's define this. Let's say we have length, all dataset size. Dataset size. There we go. So we have defined our dataset size. We have that, dataset size. That's fine. Now that we have this, we print out the string dataset. So you could see here how we are not able to convert this. So let's, cannot convert 8 to eager tensor of type int64. Right here, we're supposed to pass in an int. So let's have that. There we go. Okay, so we see that we have the first eight elements right here. We have this, this, this, eight. Now we have gotten this 80%. Let's take this, let's say we take this to 60. So let's reduce that. And you have this 60% right here. Let's modify this here. So let's now have the train ratio. So here we have train ratio. We run that. And there we go. We have the top 60% and if we get back to 80, we have top 80% or rather the first 80%. Now we have the first 60%. So we have this first six elements right here. And we're left with this other four elements. Now let's go ahead and see how we could get this out of four elements. To obtain this, all we need to do is to use this keep. So we're going to have that and define our validation. So we have your vowel. Now, instead of taking the first six elements, we're going to skip this first six elements. So we could start getting this other elements right here. So here we have skip. And then we specify that we're skipping this first six elements. So here we're going to have six. So that's it. And we print this out. Let's have this vowel right here. So that's it. We have our validation. You see, we get this first six elements. But recall that our validation is in fact this two elements right here. So what we're going to do again is after skipping and getting this last elements, which essentially is made of the validation and training set, we are now going to take out, take the first two elements which correspond to the validation set. So right here we have vowel data set is now equal. Notice how instead of data set as previously, now we're dealing with a vowel data set. So we have vowel data set dot take. So here we have a vowel data set of take. What are we taking? We're taking this first two elements, which could be obtained by having this int of vowel. So where we have vowel ratio times our data set size. So times our data set size, we have the vowel data set. And now let's run this. There we go. You see, we have now six, seven. Now in order for us to get this next, all we need to do is to skip. So we've gotten this as validation data set. Let's call this vowel test, actually. Let's say this is our vowel test and here we have our vowel test. So we get the vowel test, which is this. So up in the validation, we take this first two and then to obtain the test, we skip this first two. So right here, we are gonna copy this again and then paste this out. We have the test, which takes the vowel test and then takes the vowel test. But the difference is, yeah, we have the skip. So we run this and there we go. It's not exactly what we expect. We should change this. Yeah, we have test. So that's it. Now we've gotten all the, we've gotten the train set. We've gotten the validation set and we've gotten the test set. We could play around with these values. Let's suppose we have no validation. So we have that, you see that we have this empty list and then all this is used for the test. Now we could create a function from there. We'll call this function the splits function. So we have our splits function. What it takes in is the ratios. So we have the train, the train ratio, the validation ratio. We have the test ratio. That's it. And what does it do? Simply does exactly what we've just done right here. So let's copy this out. Let's cut down and then have it here. Yeah, we're taking the data set too. So we have our data set and that's it. Now we need to define our data set again. So we just have this data set and let's take this off. So we cut that off. Let's put this here. And we have our data set size from the data set. We define a train data set, no quinting again. We have the validation. We have that, take this off. We have the test and then we go ahead and return. So we return our train data set, our validation data set and our test data set. So that's it. So we've returned all this and let's send this up. So we have this method right here we've built. And then from this, we have our data set. We're going to pass this in here. So let's say we want to have vow. So we have vow data set. All right, a train data set. We have vow data set. We have test data set. And this equals split of data set and train, train ratio, vowel ratio and test ratio. So that's it. So we pass all those in and then now we could from here print out. So here we could print out our train data set, for example. So we have this train data set. We could print out our vowel data set. And then we could print out our test data set. So we get this right. That's it. We run it and we told split is not defined. This actually splits. So let's have that S run again. And that's fine. There we go. We have our three data sets. So we have our train, we have our validation and then we have our test data set. In here, we could always have this. So we could see a list as NumPy iterator. So yeah, we have this. There's a list. So we have that and that's it. So there we go. We have the train. We could do the same for the validation and the testing. This is where we now obtain. We can now test out the splitting with our own data set. So we'll comment this part and go with a data set. We will load this, here we go. We've loaded that. And then we shall pass this data set now in here. So we should have this. Let's rerun this, run this and that's fine. So now we have this error list object has no attribute take. So better understand this error. Let's run the cell right here. You see this data set is actually made up of a list. This list is made up of the data set and the types. So when we doing this, we actually taking the data, we're taking all this list.take. So what we do is we should pick out just the data set. And to do that, all we need to do is specify here that we are picking out just the data set. So that said, we have picked out a data set. We could now run this. Before running this, let's make sure we take out just an element because running that full data set is going to be very time consuming. So let's take this one and pick this out right here. So that's fine. Now we will run this and wait a little. Now, as you could see, you have this right here. That's the image and the label given to us. Notice how we have this empty list because we've set here that we want our validation ratio to be equal to zero. So since we have this at zero, we have the empty list for the validation. So our validation is actually empty. Now you could modify this, let's take 0.1, 0.1 and right here is 0.8, we run that. And here's the response we get. So that's fine. We have all the, we have the validation training test set, which are known empty lists. We'll now get into visualizing our data set, visualizing some elements in our data set. So we have for I image. So here we're picking out the image and the level and image rates. And we have the train data set. You could as well pick out the validation or the testing data set. So we have that. Let's take say 16 elements. And then for all this, we'll come up with subplots. And here we pick out four by four because we are having to plot out 16 different images. At this point, we do plots.imshow of our image. So that's our image we're plotting out and then we'll run this. Here's what we get. This is actually what we expect. And now what we could do is by the side of every image, we could put out a level. For each and every image, we'll put out a title. We have the title and what we'll do is we have data set info because you're interested in getting the corresponding level name. So we have say a level zero, want to get his name and we could have a level one, want to get his particular name. Here we have this and we have the features. To understand this better, you could check out this data set info right here. You notice that we have this features and then we'll pick out the level. That said, we are just gonna go straight away into this. We have level and then we convert this into a string. So we have the string and then we pass on the level. Now we pass it on this level we've got right here for the corresponding image. And then once we have this right, we run it and we take this off. So that should be fine now. We have this images and the corresponding levels. You can see how this is pasteurized. This, for example, is uninfected, but this isn't very clear. So let's take off this, put that axis. We set this to off, run that. And that looks better, but we could make this better. If you wanna know which of these is the level zero and which is the level one, let's take this from here. And then we print out to zero. You see that this level zero, it's given to us as pasteurized and one is uninfected. So that's it. At this point, we're gonna dive into data processing. Our data processing unit from now will be made of two parts. The first part will be the resizing part. So if we have an input image of say 102 by 102 as width and height, we could have this. We're gonna transform this into an image of a fixed width and fixed height. Now that said, all our images, irrespective of their width and heights, will now have just one width and one height. In this case, we'll consider an image size of 224. So our width and height will be converted to 224. And subsequently, we'll see why we are picking this image size right here. After this resizing, our next part will be on normalization. So we'll have an input image which will normalize such that all the data falls in a given range. Here we have the standardization process and we have the normalization process. In this case, we are gonna use a normalization process and we're gonna explain why. In the previous example on the car price prediction, we actually work with standardization. In standardization, each value is subtracted from the mean and then divided by the standard deviation. That's actually each value on each and every column of X right here is being standardized based on its mean and the standard deviation. As you'll notice, these values, for example, are normally distributed. That is, we have a mean value on average value and we have a standard deviation or range of values where most of times our values will fall in. So it's gonna look bell shaped like this as we've seen before. We have the mean and then we have a certain standard deviation right here. And that's why if you look at X2, you wouldn't have say 5,600, 7,100 and then here, for example, come and have a value like say 12, this isn't very typical since most of the times these values fall under a given range and there's a certain average value for which most other values just fall around. So that's why it's typical to have these kinds of values. Let's take this off. For the next, you'll notice how these values fall under a given kind of range and with this too. Now, this is for when we deal with standardization and that's why previously we used this actually. Now, in the case of image data, the choice of whether to standardize or to normalize will depend on the kind of data we're dealing with. So if we have images where most of its pixels revolve around a particular mean value, then we'll wanna standardize and if this image is made of pixels where their values are mostly different from one another, then we would want to normalize. Now, in our case, as we continue in this section, we are gonna go with normalization, that is we will have X minus X mean, which is zero, divided by X max, which is 255, minus X mean, which is zero. So simply do X divided by 255. So we'll normalize the inputs before passing them into our model. Now, it should be noted that for some other datasets like, for example, ImageNet or ImageNet, we have ImageNet dataset, or the RDE20K image segmentation dataset, they have their known mean and standard deviation values. Nonetheless, for whatever problem you have to deal with, you may work with standardization or normalization, experiment for yourself and see which one works better. Now that said, since we're dealing with a TensorFlow data API, we are gonna use this map method to help us in this preprocessing. So there we go, we have trained dataset. We'll start first with the resizing. Trained dataset equals trained dataset.map, and then we'll call this resizing method, which we shall define. Let's add this and put this up here. So we have our resizing method. This method takes in the image and the level. So we have our image and the level. And the level right here. But note that we're doing processing on only the image. So we're gonna pass this in, and then we're gonna resize this using the resize method, which comes with TensorFlow images right here. So we have TensorFlow image, and then we have this resize method. Here you could see the arguments which are passed. For now we'll focus on the image and the size. We will also note that you could change the method using resizing. But for now we use this default values given to us. So that's it. We are gonna return the image. So let's have our TensorFlow image dot resize, and it takes in our image. And then we have the image size. So we'll have in size by in size. Note that we are gonna define the in size here. So we'll have in size equal to 124. So we have this resizing. Our image is returned to this, 224 by 24, 24 of a shaped image. Now we have resized this. We just put out a level. So here again, we just basically taking the image, resizing it, and then the level remains the same. So we could run this, and then we have our trained data set, which now has been resized. That's it. We could say for data in train data set, we're gonna print out, oh, let's take one value. So yeah, we take just one value, and we have our, let's say image, actually image and the level. We print out, we print out the image and the level. Let's run that, and there we go. You'll notice that the shape of this image is gonna be 224 by 224, irrespective of whichever, or which image we pick, and then we have its level, which remains unchanged. Notice how this is a plot 32. Unlike previously where we had an unsigned int. Now you could always do casting right here to modify this depending on what you're working on. So that said, we're done with the resizing. We could now look at reskilling. So let's put this as resizing and reskilling. So we have resizing, resize, let's just say resize, reskill. So our resizing reskill function is such that after resizing, we reskill by dividing by 255. So we divide all our values by 255, our level remains the same. We run this, we run that, here is resize, reskill. There we go, we run that, and then we have this. So that's fine. Just as we did before, that was in the previous section, we're gonna shuffle our data, we're gonna put this in batches, and then we'll do prefetching. So all this was explained in the previous section. We are now ready to build our model, and up to now neural networks have been performing quite well. We had seen previously that if we have three neurons, that's one, two, three in the input, and in the output we have three neurons too, then there'll be nine different connections here, and hence nine different weights plus the bias. But for this example, let's consider only the weights. So we have nine different parameters for three inputs, three outputs. And if here we have five, considering only the weights, we would have three times five, that's 15 different parameters. Now, if we're dealing with an image like this one, so let's consider this image, which is 224 by 224 by three image, three for the three different red, green, and blue channels. So we have this input image now. So unlike previous examples, where we would have features, or specifically in this case input features, we do not contain as many elements as this. We now have this case where if we want to count the number of features or the number of pixels, we would have 150,528 different input features to take into consideration. Hence, instead of having a total number of three right here, we are going to have 150,528 different values. And if we want to do the same competition which got us this number of parameters, we would have 150,000 times three, that's approximately 450,000 different parameters. Now, what if we modify this number of neurons right here and take say 1,000 neurons in this output right here? We would see that we'll move from 150,000 or rather from 450,000 to 150 million different parameters where each and every parameter has to be trained and optimized. This becomes clear that deep neural networks like this are better still than layers of fully connected layers aren't scalable. Since when we increase the number of features, the total number of parameters also increase considerably. Hence, we need to build a type of layer unlike this one where each neuron isn't connected to all the previous neurons. And this layer happens to be the convolutional layer. In order to better visualize this, we'll use this demo platform from Wrightson University. So here we'll put in a figure, let's say four, and then we'll see exactly how this confnet work. So we have this and then we have this input right here. So we get this input, we have some weights and then this output features. Note how to obtain this particular pixel right here, only a few of these inputs are used. And so yeah, we call this the receptive field. As you could see, we have the receptive field right here. And if we take this other example, you'll see it's on receptive field. You see if to get this value, only these four values you see below, actually this one, this, this and this, as you could see here, let's scroll back. Okay, so only those four values have a role to play when it comes to giving us the corresponding value we have here. And so unlike with a dense layer, where to obtain this value, we needed to link this to each and every previous neuron. Now only just some neurons in this neurons receptive field play a role in getting this value. Another great tool for better understanding the convolutional layer is this CNN explainer. Actually CNN stands for convolutional neural networks. This is created by J. Wang, Robert, Omar, Park, Das, Frank, Kang and Polo. We are really grateful for this tool. Before getting back to the explainer, let's take this example, right? Here we have a four by four image. So we have 16 different pixels. If we flatten out those pixels, that's if we put those pixels out like this, such that we have those inputs. And then in the output, we have four different neurons, then here would have 16 by four connections that will have 64 different parameters, excluding the bias, but with the conv layer, we could leave from this four by four to just a two by two. So we could leave from this four by four to this two by two with just nine parameters. So we would have what we call a kernel right here or filter. So we have this filter, which is three by three, which actually corresponds to our weights, which we've seen already. This kernel here, kernel size three, will produce this output of two by two, which when we flatten out can give us this output right here. And so instead of working with 64 parameters, we're working with nine parameters. If we want to replicate that same example in the CNN explainer, here's what we get. We have this input right here, which produces this output. Now, notice how we specify that a kernel size equal to three. And because a kernel size equal to three, we are able to get this output. But how is this output gotten? You would see that at this top left corner, we are going to feed our kernel, which is of size three by three. So we put in our kernel right here, and then we take each and every value of our kernel and multiply it with a corresponding value in the input. To obtain the first value, you see how we pass this kernel on the input. So at this top left position, we pass this kernel. Notice how we have a three by three kernel, which is passed on this input. And we have the output right here. Then to get the next position, to get this next output, you see how this kernel is passed on this next part of the input. And the way we get this next part is by simply sliding through the image. You see how we're left from this, we slide that through the image and we got this, we got this next output. And since we've gotten to the end, we move to the next position, which is this, we get this next value right here. And then from here, we slide again to this end, and we finally get this value. So that's how we get all this outputs from the inputs and the kernel or the filter. We can now go ahead and increase this input size. Let's take, for example, seven. You see, we have a input size of seven, seven by seven input image. And then we have this output five by five. The reason why we have this five by five is because we have this kernel size of three. If we get to increase the kernel size of five, you see our output reduces. You see, when the kernel size gets to six, our output reduces. Let's take this back to three. So you take back to three, and this input of seven by seven gives an output of five by five. We see how when we get to this, we have that output, move, slide, slide, slide, slide, move to this next here, slide, slide, slide, and so on and so forth, right up to the end. So from here, we see how the size of the receptive field of each of these outputs equal to three, which is actually our kernel size. The next thing to notice is reducing the kernel size permits us extract more features from the inputs. You would see that since we have this input seven by seven, with kernel size of three, we have the output five by five. So we've extracted much more features from this input as compared to when we push a kernel size of six. Here we extract less features from this inputs. Now, though using a smaller kernel size permits us extract much more complex information or complex features from the inputs, working with larger kernels permit us extract larger input features. One logical question which may come to your mind is, how do I get the size of this output feature map right here? To get this, we'll use this formula where this output width is equals the input width minus the filter size plus one. If we take this example, we have an input width of seven minus a filter size of three plus one, this gives us five. So that's how we obtain this right here. Now, if we take, say, kernel size, let's take the kernel size to be four, for example. In that case, we have seven minus the kernel size four, which is three plus one, which gives us an output of four. And that's how we obtain this output right here. Now, in a case where you're designing a convolutional layer and you want to get a particular feature map size, say, for example, in this case, if you want to get an output size of three, then all you need to do is specify this kernel size to be equal to five and you should get this output. Now, in some cases, you may want to have a particular feature map shape that using just the kernel size, you wouldn't be able to get that. And so to match up with a particular output shape would include a pattern. Let's look at this pattern. You see, we have a seven by seven. And when we take this to one, you see that we go from seven by seven to now nine by nine. Notice how it's written here. After pattern, we have nine by nine. So we'll leave from this seven by seven. Let's take this in this box right here, this internal box. And then the pattern is also nine by nine. Notice how after doing the pattern, and if we pair around with our kernel size, we go from one, one to eight, eight. So we go from one, one to eight, eight. But with zero pattern and the input size of seven, you see, we go two, three, four, and six. So we cannot go up to eight, eight. We're gonna have an output of eight, eight when the pattern is zero. Whereas when we increase the pattern by one here, increase the pattern by one, that's one. You see that we could go from, we could get an output of say, let's put a count to three. We could get an output of seven, of eight, you see, up to one. So that's it. Now, or we understand how this pattern works. Basically, we just have our input and then we add this surrounding elements. Right here, we just add our input image. Now, the pattern generally we use is the zero pattern, though there are other pattern methods, but the zero pattern is one of the most common since it's easy to use and it's computationally less expensive to work with. So that said, here's our pattern. And then we also have another advantage of working with a pattern, which is that of ensuring that the corner pixels have an influence on our output features which are generated. Now let's take this back to zero pattern. We have zero pattern and we have this input image right here. If we're doing an image in which most of the information or most of the relevant information is centered, then there is actually no issue since this photo right here will go through each and every pixel we have where we have this person and the image. Now, if we modify this image such that we have a person's, let's suppose we have a person's face here just at the corner of the image. You'll see that unlike with this image where this person was centered and that our kennel was able, our filter was able to pass through each and every part of this image. With this one, we have a different scenario. We've just modified this so it looks similar to what we had here. Now, what we could do is if we monitor the number of times this filter goes through the person's head, we would see that we'll have here one time because our filter passes here once. The next time after the sliding, we'll have this. So second time, the next time this other sliding will lead to this four times. So at least it passes through the head and then here we have five times. Now we have one, two, three, four times actually. But in this case, since we are on the borders, you'll see we have this one time and that's practically all. So we see that in this example or in this case where we are on the borders, this influences the outputs in a smaller way or exerts less influence on the values we get in this feature maps which are generated. So in an example where this, let's suppose an example where all this wasn't there and that where our information lies most is this, you will find that it would have been better to at least pass through this head region just as we did with this year where we pass through the head region four times and we're able to extract very useful information from this image because our filter goes through this many more times. Now to remedy this situation, we have the pattern. You see that when we increase the pattern, so let's take this to one, we've increased the pattern, let's increase the pattern to, oh, okay, yeah, let's fix to just one. Anyway, we just increase the pattern to one and then we'll retake that example. Now we're taking the example, we'll see that if we maintain our filter size of two, this filter here will at least touch the head although slightly and then the second one does this twice. So in this case, our filter touches the head twice as compared to previously where it touches the head only once and hence this useful information has more influence on the output features which are being generated right here and which is very important as practically that's what we're trying to do. We're trying to extract information from this input and pass to the output. Another hyper parameter which we could look at is a stride. Now note that for now we've looked at this one this and this. So here's what we call the hyper parameters. We have the stride and we'll understand surely how this works. For now we'll deal with a stride of one. Why one? Simply because you just slide in through one step we're going to the next. So you just slide one, one, one and so on and so forth. Now if we move this to two, we start from this position right here, fix that. You start from this position. There's a problem. You get back to one and back to two. You notice that as we go to one this turns blue and then to two it turns red. The reason why it turns red is because there is no valid output or there is no whole number which with a stride of two can produce an output. So in fact what we're going to do is we modify this input right here. So modify this input, this works now. So it's possible for us to leave from this input of six to this output, but with a stride of two. So let's now understand how strides work. We start with this. Notice now how we're going to skip two steps instead of just one step. So as we go, you see we skip two steps. Notice two, two and that's it. So we move to the next. And now even moving downward, you see, it's not like with a stride one where we just had one step, like with a stride one we just did this one step, but with a stride two we're going two steps below. So that's it. Now that said, increasing the size of our stride actually reduces the size of our output and hence reduces the amount of information we extract from the inputs. And so in general, we get better results by working with smaller kernels and smaller stride values since we're able to extract more information from our inputs. Generally the kernel size is used to read the kernel size of three is generally used in practice and a stride of one. So we may decide to use pattern or not. And the new formula when we include the pattern and the stride is given as such. So we have the output equal to input size minus the field of size plus two times the pattern divided by the stride plus one. In this case, if we have an input size of six, so here we have six, we have six minus field of size three plus two times the pattern, also our pattern one so it's plus two, divide that by the stride, stride is one and plus one. There we go, we have an answer of three plus two, which is five and then plus one, which is six. So that's how we obtain this output size right. Also note that one good thing when working with a library like TensorFlow is when you don't know the exact pattern to use such that you have particular output size, you could specify the pattern to be valid. Once you specify the pattern to be valid, TensorFlow automatically calculates the pattern for you such that the output you want matches up. Up to this point, we've been supposing that our input image is two dimensional, that is we have a height and a width. Now what if we use the kinds of images we have in real life, that is 3D images where we have the red channel, the green channel and the blue channel. So if we have this RGB image right here, RGB image, we'll see how we get the output. Now the way this is done is quite straightforward. So what we have is in this case, we include this pattern. So you see the zero pattern, we've included this pattern and then we have our kennel right here. Now notice how we have this kennel and then it goes through this first part or this top left corner. And what we do is we multiply each value right here. We have negative one times zero plus negative one times zero plus negative one times zero and so on and so forth up to plus negative one times zero right here, you have negative one times four. So basically what we're doing here is kind of like a dot product. So we're taking all these elements, multiplying by the corresponding elements and then adding all this up. Once all this added up, we should have 41. So you could take this as a simple exercise and then you should be able to get 41. And then we have this other kennel right here where we repeat the same process. We have this and then we have this kennel, we have this. So here what we do is we have, we take this, we obtain this value, take this obtain this value, this obtain this value and then we add all this up to get this answer 41. Now note that we also have the bias which we've already included. So for now we have this 41. Now we move to the next step. So we slide through next step. Yeah, the stride is equal one actually. From here we do the same process, negative one times zero, this negative one times zero, add it up and all of that. So all that added up, all this added up, all this added up, we have 12. We repeat the same process right up to the end. So that's how we obtain our output right here. You can also check out on this other more visually appealing example in the CNN Explorer website. So here we double click on this and then we see exactly what's going on. We have here, you see how we're forming this outputs by sliding through the kennels over the inputs. So here I click on this, you see a slide through this and you see how all these values are multiplied and then added together to form the output. So that's it. From this point, we move to this explained visually project by Setoza, filtering is of course part of image processing. And since we're dealing with image day to year, it's important for us to better understand how this works. So right here, we'll choose a kernel or filter, or let's pick this sharpen filter right here or this sharpen kennel. Notice how once we pick a kernel, those values change, that's the values of the filters actually change. Let's take this outline, you notice the change. Now let's keep this outline and then we'll notice that every time we, let's hover over this input image, we have the values right here, that's to the right. So for now we have this and then let's notice that here we have this negative one, that's negative one, times at this position, notice at this position, we have the value 255. So 255 times negative one plus 255 times negative one plus unknown value, unknown year, because we are the borders, so there's no value. And then we have 249 times negative one, 255 times eight, 233 times negative one, 255 times negative one, and then all this sums up to give us a given value. Since we have this unknown values, year is unknown, so we have no value there. But if we change this position and then we fit around the I region, you see we have a value of negative 533. So we notice that no matter the image we put in year, we'll always get an output where the outlines are being highlighted. So if you check out this image, which we've just imported of Elon Musk, you see that the output is this image right here, which highlights the edges. And so as it's explained here, the highlight large differences in pixel values, which generally occur at the edges. So around those regions, you see these large differences are being outlined as compared to this zone year where there's no difference. And since there is no difference, we just have this black region. So here again, you see that we have the filter, that's it. This is the exact same value we have right here. We have the filter and then this example shows us how this output is gotten. Now the major difference between what we're doing right here and the convolutional neural networks is that with this, we know this can of values. So we know that we have this metrics negative one, negative one, negative one, negative one, eight, negative one, negative one, negative one, negative one, which is an edge detector. So we know this and then we know the obvious output. But with the convolutional neural network or with convolution layer to be specific, what we do is initially we just initialize these values and we let the model doing training to learn these values automatically. So these values are learned by the model during training automatically. One of the very first convolutional neural networks or convnets was built by Yann LeCun in 1989. And right here we have the structure of this convnet known as the LeNet. This LeNet takes in an image. Here we have a 28 by 28 by one image. Here we just have one channel, black and white image. And then we pass this to a convolutional layer, which we've just seen after passing through convolutional layer, we have the sigmoid. But also note that here we have a five by five kernel. We have a plus two pattern. So if it's zero pattern, we'll add zeros around our input and then add another zero or another group of zeros around our features. Then from here, we have this output 28 by 28 by six. So shortly we'll see exactly how this output is gotten. So we've seen we have the sigmoid activation. From here we have the pooling layer. Now this is a subsampling layer and we'll understand how it works. So we have two by two average pooling, kernel and with a stride of two. From this we have another convolutional layer with five by five kernel. Here is no pattern and we have this output. Activation, pooling, flattening, with flattening, all the features have been modified. So we'll leave from this 3D tensor to a 1D tensor. And then we have this dense fully connected layer followed by sigmoid followed by another dense fully connected layer followed by sigmoid. And finally, we have this dense layer with output 10 neurons. And the exact reason why we have this output of 10 classes or 10 neurons since we have 10 classes is because we were predicting whether an input is either a one. So those inputs are images of handwritten digits. So we want to predict whether the handwriting digit is a one or a two or three up to nine. And so here we have this and then we also have a zero so that makes 10. So zero to nine gives us 10 possibilities and that's why we have 10 different classes right here. Now for the AlexNet, it was built to correctly classify whether an input image belongs to one of a thousand classes in the imageNet data set. So here we have this AlexNet with a different architecture as we could see right here. And now we'll go ahead and understand how all those outputs are gotten. And so we'll be rebuilding the Lynette architecture but this time around considering an input 64 by 64 by three. So right here we have the RGB channels. We have R, G and B. If we pass this input now through this convolutional layer we are going to have this output. And how do we get this output? To get this output, we have to take into consideration this parameters which have been given to us for a convolutional layer. That said, the photo size is equal to five. So we have here photo size of five, there is no pattern, stride equal one and then the number of filters equals six. We'll calculate the number of parameters shortly. Anyway, these are the four most important parameters. So here we have this and this is what we call our filter. We have this filter. You see that we have the dot products which are completed to get the outputs as usual. And so we take this, like this R, we have this. Then we follow through with this and then we follow with this. So we take this and then complete the dot products and with the other two. And then from this, we add all this up to obtain each and every value for this very first feature map right here. So to obtain this feature map, we are using this filter. Now, this shows us clearly that when we specify that the number of filters equals six, it doesn't mean we actually have just six of this as you may feel like number of filters equals six means we have six filters stack like this. Here we have five and then six. So we feel like we have just six stack like this which is not actually the case. What happens is we have three, there's these three channels and then each of our filters also have the three channels as you could see. And so what we call a filter here is this three or rather is this five by five by two B kernel right here. Now we've done a competition and we've had for this, we repeat the same process. So we take this for the R, we repeat this, we repeat this and then we obtain this next. We do all this the same way up to this position and then we have this right here. And that's why when we have six filters, we always gonna have six channels. Then if you do five by five by three and all that times six. So if you have five by five times three, that's 75. 75 times six should give you a total of 450. And since for each of those filters, we add the bias. That is we have an extra bias for each of these filters. We have one, two, three, four, five, six biases. So adding this plus six, we have 456 parameters to be trained. So here we have 450 weights and six biases. That said, we understood why we have the six channels. Now, how do we obtain the 60 by 60? The way we obtain the 60 by 60 is by applying this formula, which we've seen already. So here we just have the output equal 64 photosize five. So we have minus five, add in no pattern. So pattern is zero, stride one. So we have 64 minus five and then plus one. This gives us 64 minus four, which is equal to 60. And that's how we have this output right here. From here, we move to the subsampling pooling layer. For the pooling layer, we have these two parameters. That is we have the number of filters and then already we have the filter size, not the number of filters, the filter size. And then we have the number of strides. To obtain the dimension for the pooling, the formula is slightly different, obviously, because here we don't have the pattern. So we have X minus F divided by S plus one. And then we'll leave from this to this feature map right here. Notice how we still maintain the number of channels, but our input feature map has been subsampled. For the particular case of max pooling, if you want to understand how this works, let's say we are at this position. Let's take this position. Let's take a position where we have some values. Notice how as we pick this value, we see the max is 0.02. So you should look at this. So we see here the max is 0.08 and so on and so forth. So basically what we're doing is we're just simply sliding through the whole image. And then for every, since our kennel size is equal to two, we have a two by two kennel size here. So for every four values, we are gonna pick out one to represent them. And in this case, this one is the max. In some cases, we will instead take the average of this. That is known as average pooling. But for this max pooling, which is the most commonly used, we take the max of all these different values. That said, to obtain this, we have X. That is in this case, we have X equals 60. So let's take this off. X equals 60 minus F. F equals two divided by the stride. Our stride is two and then plus one. So 60 minus two, 58 divided by two, 29 plus one gives us 30. That's how we get this. We still maintain a number of channels. Here's another example showing even more clearly how this max pooling works or pooling in general works. So if we have this input, so we're picking out just one of this, suppose we're picking out just one of these channels and we have this, we are gonna have our kennel two by two, that's it. And then what we get is output is zero since the max of zero, zero, zero is zero. We do, we have a stride of two. So notice how we've shifted by two positions, stride two. And then the max here is two. We move again, stride two, max now is two. We move, you see max is one, move, max is zero. And then you go to the next, max three and so on and so forth. So that's how we obtain this new feature map right here. Notice how we have this is five, 10, five, 10 and then here we have five. So we leave from 10 by 10 to five by five. After this subsampling layer, we have the activation. We've already looked at activation in the previous section. We've seen the sigmoids and we've seen the relu, we've seen the tangs and we've seen the leaky relu. So that said, we understand that and now we move to the next convolutional layer or filter size equal to five, add in no pattern, stride one, number of filters 16, number of parameters 2,416. So you could take this as an exercise to be able to show that the total number of parameters we have is 2,416. That said, we're going to have this output right here. We have the relu added and then we have this output. So we have the output 26 by 26 by 16. Recall number of filters dictates the number of channels right here and not the filter sizes, the number of filters. And then this 26 by 26 is gotten by using the same formula we've seen already. From here, we have another subsampling. We've understood that already and then we flatten all this out. So after doing the subsampling, we have this 13 by 13 by 16. When you multiply all this, it should give you 2,704. And this is what we call the flatten layer. So we pass this to a flatten layer to obtain this. Now this takes each and every value we have here in our feature map and then just simply places it in this one dimensional output right here. Then from here, we have the dense layer, which we're used to working with already. And then we have a thousand neurons in the output. And finally, we have 200 neurons. In our case, we should have two since we're actually predicting whether it's a parasite or it's a parasite, so we should just have two. Anyways, we understand all this now and we should be ready to dive into the code. Before moving on to the code, let's get to see what the feature maps of a trained convolutional neural network looks like. As you could see right here from this example from the Stanford website, we have this car which is passed into a convolutional neural network and then its output, we have it predicting a car. As you can notice, this first layers actually serve as filters for low level features like the edges. So you'll notice that this input or this feature maps from the first layers produce visually interpretable outputs. Now you will notice also that as we go farther or deeper, we have this outputs which are less visually interpretable. And the focus more on high level features like the car parts, which per meters correctly classified that this image contains a car. And so the first layers are for feature extraction and then the last layers are for classification. Both subsections actually need each other as if you do feature extraction and you don't have layers which are able to permit us correctly, classify this image, then we will not achieve our goal. And in the same sense, if we have a good classifier, but we get this kind of inputs directly, where we've not extracted useful features from them yet, then our classifier is not gonna perform well. You could also check this ConvNetAmnes demo by Andres Caparte, where he trains a model on the Amnes dataset. Here we have for model, the input, conv, pull, conv, pull, and softmax. So here we have this on Amnes dataset. And then we have in this passed in, as you could see, notice how in this first layers, the feature maps actually contain much more visual content as compared to this final layers right here. See, if you look at this final layers, you see that we have this feature maps, which instead contain content, which permits us see whether a particular input belongs to a particular class. If you want to build convolutional layers with TensorFlow, you could make use of this TensorFlow Keras model right here. So that said, we're just gonna come to the layers, see other Keras, and then we pick layers, and then we'll find the conv2D layer. So here is the layer we're searching for, double click on that, and then here we go. We have the arguments, that's what we pass in this layer. The filters right here corresponds to the number of filters. So let's take this off. Here is the number of filters, we've seen this already, and F, and then the kernel size corresponds to the filter size. From there, we move to the stripes, which take a tuple, let's take this off and check this out. So here we have this tuple actually, and then what's important to note here is the fact that if you want to do a striding of say one two, that is one two, then in the height dimension, your striding is gonna be equal one, and then the width dimension is trying to be equal two. So if we have this feature map right here, and we pass a kernel, we are gonna be sliding and skipping two steps in this horizontal direction, while when we come in, when we're going downwards, we are gonna be skipping only one step. So that's what this means. Now in the case where we are having exactly the same number of steps sliding horizontally and vertically, then the striding, like in this case, can just be given as equal one. From there we have the pattern, by default the pattern is valid. Here we tell that for the pattern, if the pattern is valid, it means there's actually no pattern, so we have zero pattern. And then when the pattern is same, the result of the pattern are the, we've padded with zeros to ensure that the input dimension equal the output dimension. Also note that this is only possible when the stride is equal one. So that said, oh, if we have a feature map like this, let's say we have 60 by 60. Then we wanna pass this to a conv layer to get an output feature map. And if we want this output feature map to be the same dimension as the input that is 60 by 60, then all we need to do is specify that the pattern is same. So once we have this, the output will be the same as that of the input. From here we have the data format. Now note that there are generally two kinds of data formats. That is we have height. So for an image, we have the height, width, and then the channel. So if we have a 224 by 224 by two V image, then we actually making use of this data format. Now we could also use this format where we start with a channel. So we have the channel by height and by width. And so here we have three by 224 by 224. Note that by default we have the channel last. So by default we have this first convention last right here. But if you wanna take this convention, all you need to do is to specify that you're working with the channels first. Then from here we have the dilation rate. So we'll look at this GitHub repository by Vido Mulung where he uses animations to explain how convolutions work. So you could scroll down and you would find the dilated convolution animation, which when we click on, we have this. So there we go. We have this example in which the dilation rate is equal to. So the way this dilation works is we have this kernel which initially had no spaces between its values. So we had something like this. We had this three by three kernel. And now what we do is we have this spaces or this holds between these different values. And so now what we get is some sort of five by five kernel right here. If we're working with a dilation rate r equals three, then instead of a single whole year, we are gonna fit in two other holes. So then I have this, we just have one, two, and then we fit this. Then we have one, two, and then we fit this. So that's how we build this out. We have this and then that. Now to get the shape, we have one, two, three, four, five, six, seven. So we have a seven by seven filter now. It should be noted that the dilated convolutions are used in problems where we wanna keep increasing the receptive field as we keep going deeper in the network while maintaining the number of parameters. The next argument is the groups. And according to the documentation, this is a positive integer specifying the number of groups in which the input is split along the channel axis. This means that in this case, for example, we could break this up into three different groups. So we have one, two, and then three groups. Then each group has its own group filters. And then the output is a concatenation of all the group results across the channel axis. From here, we have the activation we've seen already. And then we'll see whether we're gonna use the bias or not. We have this kernel that is the weight initialization. And then we have the bias initialization. We have this regularizes, which we'll see subsequently. And then we have the kernel and bias constraints. In order to create our convolutional neural network, we're just gonna make use of this sequential API which we had built previously. And then we'll define the input layer. So here we have this input shape 224. Let's, we had defined this as in size actually. So we have in size, that's it, by in size by three. We could also define a number of channels, but I just let it to be equal to three. Then from here, we have no normalizer. And then we have this conv2d layer right here. So we have this arguments which are default. So we'll then bother to check on this. We'll take this off, take this off, and that's fine. So we'll have this, there we go. We have our conv2d, our conv2d which we copy. Or rather we cut that off and then we put this after the input layer right here. So there we go, we have our input layer. We want six filters and then with a kernel size of five. So that's it, our stride. We could take this to be equal to one. There we go, padding is valid. So that's what we expect. And then just here we have our activation. So we have activation, we could say sigmoid. We have the sigmoid activation. Since we're replicating the ReLU or rather the LeNet architecture. So there we go, we have the sigmoid and that's our conv2d. From the conv2d, we have a max pooling layer. Let's take this off, let's take the dense layers off. From this conv2d, we have the max pooling layer. Now what we could do is we could just simply import all this. So let's go ahead and say, import the conv2d, max pool2d and the dense layer. Coming back to our documentation, we could check out on the max pool2d. So there we go, we have max pool2d. We'll specify the pool size, which is equal to and then the number of strides we call two. Padding is valid and data format, we're not going to specify that. So we have our max pool2d and then the pooling size with the strides. Copy that. We actually don't know the stride of two. So we replace that, we have a stride of two. The next step is this one right here, similar to what we had seen previously, with a difference that now we have 16 number of filters. And so we could simply copy this and then paste this out right here. We have conv2d, max pool2d, now conv2d, max pool2d. And then here we have 16 number of filters. The padding is valid, activation sigmoid. Then this next max pool2d is still the same as what we had previously. And so we move on to the flatten layer. We add this here, we have flatten, there we go. And we run that. So while that's running, we just simply come right here and then add that flatten layer. So we have this flatten layer, which is in charge of flattening this or converting this into this 1D output. From this flatten, we now have a dense layer, which is what we've seen already. So we just have to put this dense layer here. Let's take this, this, and there we go. So we have our dense layer and then the activation is sigmoid. So we're respecting the activation using the Lynette paper. Then we have this here, we'll take a thousand. And then we have that. We add another dense layer, add this other dense layer. Now we'll make sure that as we create this final dense layer right here, it has an output of two neurons since we are dealing with a binary classification problem. So that's fine. Take this here and then run our model. We have it at input layers now defined. So let's simply add that here. We have the input layer. So run that and then check that out again. There we go. We have this input layer, which we run, which we add and then we run this now, everything is fine. So that's it. We have 45 million parameters and no nontrinnable parameter. Notice how this dense layer here is responsible for a huge percentage of our parameters. So we could reduce this. Let's take this to a hundred and then this to like 10. Let's run that again. And there we go. We have a smaller model this time around. A point to note is this number of parameters which we precalculated here. Here we have 456 and here we have 2460. I will see how we get exactly this number of values here. For the dense layer, it's quite obvious as we just have a hundred times 10 plus the 10 biases giving us a hundred times 10 is 1000 plus 10 giving us a thousand and 10. And then here we have 10 times two 20, 20 plus two bias is giving us 22. It should also be noted that there was a slight error here as we don't have like with this, we have five by five by three but here is five by five by six since we have the input number of channels equals six. So this means this is five by five by six and here is five by five by three. All this five by five by three and all this five by five by six. And if you take five by five by six and multiply it 16 times, you should have 2416. It should be noted that here we're trying to replicate the lunette architecture and this is in no way the state of the art kinds of models we use today. So in the section on the corrective measures we are going to use even better models. Now let's just work with this. We're now building the model. Let's now move on to the error sanctioning section. For error sanctioning binary classification problems we generally use the binary cross entropy loss. Let's look at those formula of the binary cross entropy loss from those ML cheat sheet website. Just as most error sanctioning functions with a binary cross entropy, we're trying to penalize the model when the actual prediction is different from the predicted value. And so in this case of a binary classification problem, if our actual prediction is meant to be a one and our model predicts a zero, putting this in here would have one log zero. So we have one log zero. And then here we have one minus one, one minus one is zero. So we have here zero, one minus one, log of one since P is zero. So we have one minus one log of one. If we plot out the curve for the log we would have something like this, we have this plot right here. Here is one. And then we'll notice that as we are approaching zero, that's X. And then here we have Y equals log of X. And as we are approaching zero, that's as X is tending to a zero, we would have this log which is going to us negative infinity. And so the log of zero is a very large number. Now with this case, we have zero, so this is taken off. And so this means when the actual prediction is one and the predicted value is zero, our final output becomes a large number. Now let's modify this. Let's say we have zero here and then here we have one. In this case, we would have a similar scenario because here we're going to have for the Y, Y is zero. So you have zero, zero times whatever we would have here will be zero. We would have zero here and then here we would have one because one minus Y, let's erase this one minus Y. Take this off. One minus Y in this case will give us one since we'll have zero here. And then we'll have log of one minus P, P is one. So we have log of zero. And again, we have a large value because log of zero is taking us to us negative infinity. And then we have one times that big number, giving us a big number. And so our model is sanctioned because it hasn't correctly predicted the expected output. And then supposing the actual prediction is one and then our model actually predicts one. In that case, we would have year one. So we have one, log of one, and then we have one minus one. So here we have one minus one, and then log of one minus one, that's log of zero. But here we have one minus one, which is zero. So zero times this will cancel out. And then what will be left with will be this. And as you could see here, when X equals one, log of X equals zero. So log one is zero. And then we have a final answer of zero. So our final output here is zero, telling us that the model has done its job correctly. If you do the same for when actually zero and predicted zero, you'll see that you always have this year zero. Now note that this zero, log zero is actually a standard limit, but we wouldn't get into that. And you could check on our calculus cause to better understand that. For now, just note that this will give you zero. And then yeah, we have also zero. So our final output is a zero. And our model now makes use of the binary cross entropy loss to update its weights. If you have a zero, and then let's say we have a 0.8 right here, we compute this BCE as a binary cross entropy loss, Y true, Y red. There we go. You see, we have this value of 1.6. Now let's modify this. So let's have say 0.02. I run that, you see we leave from 1.6 to 0.02. And then if we take say 0.2, you see we have this. Now you could always stack up outputs as we had in the beginning. So let's have back this outputs. And then we have BCE, Y true, Y red, and run that again. You see, we have this value here for our loss. And then if we take this to one, you see now we have your zero one, you see the loss is increased. Another argument we could pass in here is a from logits argument. You could check on the other arguments in documentation. For the from logits argument, by default, it's actually false. But then this default value of false is supposing that the output of our model, the Y red will always produce values in the range zero one. Now, the way our model has been constructed ensures that all our values will be produced in that range because we have the sigmoid right here. And if you could recall, we had the sigmoid, which was like this. So with our sigmoid, we have this function which looked like this. And that as we increased the value of X, the output was going towards one. So as we increase the value of X, output goes towards one. As X becomes very small or take a very large negative number, X goes towards zero. So in fact, it's gonna always ensure that the output lies between zero and one. That's why it's very important to have the sigmoid here. Now, in the case where this, we don't have the sigmoid, that is our output doesn't necessarily lie between zero and one. What we're gonna do is say that we're gonna use from logits equal true. So specifying from logits equal true simply means you are trying to say, you are not sure that your output or your model output will always fall under range zero, one. Now that said, if you run this, you see we have a totally different response. So you have to be very careful when working with this. If your values range between zero and one, make sure you use a default from logits equal false. From here, let's go ahead and compile our model. We have our optimizer, we have the loss, our losses, the binary cross entropy loss. So here we have binary, binary cross entropy. There we go. And then the metrics for now, let's take this off. So we just come in this part for now. We're not gonna take that into consideration. So that's it, we've compiled our model. So we could run this, compile our model, item is not defined. Let's go ahead and define all this. And I'm now defined, let's take this metrics off. And then we have the binary cross entropy, binary cross entropy, there we go, we run that. We're running this now, everything works fine. And so we go ahead and train our model in a similar way we had done previously, right? Here we have our train data and our validation data. Let's also reduce this learning rate, let's run that. And then we start with the training. We have those arrow which reads logits and labels must have the same shape. This shape is given versus this one. Now we'll try to understand together why we're having this error. So let's go right up to the model creation. Now, when creating the dataset or when processing this dataset, we had these kinds of inputs and outputs. For the inputs, we had 224 by 224 by three and for the output, we had just one. And this was because our output could take either a zero or a one. But the way we'll define our model is slightly different. With this model definition, we actually have a shape year of two. So let's take this, we actually have a shape of two, meaning that we could have two outputs. Whereas the delay our dataset was constructed was such that we could only have one output. When our output is zero, we suppose that it's parasitized and when our output is one, it is uninfected. And so because of this, we are going to modify our code such that this output year is one. Let's run that again and there we go. So we've modified our model, we recompile our model and then we get to train our model. We still have the same error, but this comes from the fact that we changed this name from model to lunet model so that when working with other models, we could always find ourselves. So we have lunet model and then here is lunet model. Lunet model. So that's it, we run that again and that should be fine. So we've started out with our training process. You see, we're training, but this looks very slow. Let's check on the runtime. So let's click on change runtime type, hardware accelerator and we see known. So we actually using a CPU year of which we should be using a GPU. So that's it, let's check on this and then let's select GPU. So that's fine now and we save. We will run that again. We told this model is not defined and this is because every time it changes runtime, you have to restart all this. So let's rerun this and then get to the training. The training process is now much faster than what we had previously as you could notice. And this is because we're now using a GPU. After training, we have this error. Now the fact that the model trains and then gets right up to the end of the epoch before trying this error should give us a hint that most probably this error is coming from the validation set because for the training, everything went well. And then at the end where some validation has to be computed give us a validation loss, we have an error. This means that that validation data set has a problem. So here we have validation train data set, valid data set. And then what we notice is that these two are slightly different. So here we've not done the resizing. We've not yet done the preprocessing of the validation set as we did with the train data set. So let's get back and check on that. So right here, we have this. We have this, let's add this code and repeat this for the validation and the test set. So that's what we get. And here is it. Let's ensure that we had no preprocessing before this. So here we have this train. So we have to repeat the same for the validation. Everything we do with the train, we do the same with the validation. And coming back up here, we have, okay, everything is okay. So yeah, we need to make sure that this train and validation have the same preprocessing. We have the train and then the validation. So we have validation and your validation. We repeat the same two for the test. So we have test data set, equal test data set, the map, resize and rescale. That's fine. So we run that and then we go ahead and check now on this next one. Here we have the batch size defined, the train shuffling and batching and prefetching. So yeah, let's take this off since it's already defined. We have the validation. Here is validation. There we go, validation and that should be fine. So we have the validation which has been processed. So we've done this, we run this and then we have the test. We do the same for the test. But when we check this out, we see that we are trying to shuffle this test which is actually a very useless operation since we do not want to shuffle our test set. And then we do not also want to do this batching. So this tool already useless and then the prefetching useless. So practically we don't need to do this for the test set. So that's fine. And then we have train and then validation. Train, validation data set, we run that and here we go. Now we have this known and known here because we have completed this batching the first time for the training and then we've redone this again. So it's like a batch on a batch. That's why we have this. So we have to get back and make sure that we work in from this. So we have to, we run this to make sure we have this train data set calculated again. So now we have our train data set which has been completed from the original data set and everything should be fine. We've done that and we'll visualize, resize and rescale. Yeah, this could be done too for the testing. So that's why we allow this because we need to resize and rescale for our test set. So we have that, fine. And then here we go. We run this right here, run this validation and train. You see, everything is now okay. So we have that, that's fine. Our model, we have a model. It's needless actually, reinitializing our model since we've already started with training. So we just keep it from here. But for demonstration purposes, let's just rerun this again. So we've been training for a while and we're getting this very poor results. We notice how the loss isn't changing at all for the validation. It's kind of a similar situation. We're having these changes within five, four, four, five and that's it. Another thing we could do to make this debugging faster is we'll for now take off the validation. So let's take this off for now and then let's stop the stringing. We now make some changes to our model right here where we, instead of having this activation, we have relu. And then here we have this relu. Right here we have relu. There we go. We have relu and this is a sigmoid. We would also want to reduce the size of this receptive field. So we take the current size to three and your current size three. So that's it. Let's rerun this and see what we get. There we go. We run that and now we don't have the validation. So the training should go faster. We see again here that nothing has really changed with the training. So let's interrupt this and get back to our model. There we go. We have our model. We're now going to include batch normalization. In batch normalization, values of the same, all values belonging to the same batch are standardized. So we have X becoming X minus the mu, mu, divided by the standard deviation. So that's it. Let's add this batch normalization layer right here. Batch normalization. There we go. We have the batch normalization. After this conf 2D again, we have batch normalization. And then with the dense, we have batch normalization. We'll do the same for this. Let's not forget a commerce. So we should have this here and have this here. Let's copy this and then paste it out here. We have batch normalization. So that's fine. We have included batch normalization. We will run the model, batch normalization not defined. Let's have this here. We have batch normalization. There we go. So we take this and then we will run our model. We're running this. That should be fine. Our model is now recompiled. So we recompile our model and then we feed the model. What do we notice? We have this loss which now drops normally. And so we'll see how important it is to work with a batch normalization layer. Apart from this, the batch normalization layer serves as a regularizer. I will see that subsequently. Given that our model is now trained properly, we could halt this training process and then include performance measurement. So here we have the metrics and we have accuracy. So our performance metric here is accuracy and we run this. So when training we'll be able to see how the loss and the accuracy will evolve. There we go. We recompile the model and we go back to training. A model's accuracy is equal the total number of times that model predicted an output correctly divided by the total number of predictions. This means if we have a model right here, let's say we have model A right here, and then we have another model say model B, and that we allow these two models to carry out say 1000 predictions. So we have totally 1000 predictions. Now, if model A does 800 correct predictions, like correct predictions, then its accuracy is 800 divided by 1000. Now you could put this as a fraction or in percentage. So here we have 80% accuracy. Now, if we have model B, which does 980 correct predictions, so we have this accuracy of 98% for B. And in this case, we'll see that model B outperforms model A. It should be noted that the accuracy as a performance metric isn't always the best choice of a performance metric when it comes to classification problems as others like the precision, the recall, the F1 score, and many others exist. For now, we'll use the accuracy, and later on we will look at the other metrics which we could use when we're dealing with classification problems. After training through 20 eBooks, we have this resource here. Now let's plot this out. So let's plot our loss and then plot the accuracy. We have these two plots, there we go. We see that the training and validation losses both keep dropping, and then the accuracy tool keeps increasing, though the training accuracy is slightly greater than that of the validation accuracy. Our next move will be to evaluate our model. So let's go ahead, we have learnnetmodel.evaluate. We evaluate this model, and here's what we get. We receive this error telling us that there is an incompatibility when they expect that shape and the shape of this test dataset right here. So recall that when building the test dataset that was on this position, when building the test dataset, we didn't include this batching. So let's just do that straight away. We have, let's add a code cell and that's it. So here we have test dataset. Now before doing that, let's print this out first. Let's print out the test dataset and you see the train dataset. So we have that. You'll notice that with the train, we have this batch dimension, whereas here we don't have that. So to include that, we have test dataset across test dataset, and then we include a batch of one since we just tested on single elements. So we have that, we run it, and there we go. So here, if we run test dataset this time around, see we should have this batch dimension. Now let's go ahead and evaluate our model. There we go. On data, this model has never seen. It has 94.16% accuracy and a loss of 0.2. This sounds interesting. And note that we could continue with this training. So you could train for more epochs as compared to this. And many scientists have made this remark where sometimes the forget to stop the training and then the comeback and notice that they've gotten an even better performing model because they allow that model to train for longer time. After evaluating our model, let's look at how to do model predictions. Now the whole idea of model predictions makes sense since we have trained our model inputs and outputs, and then now we want to pass in an input and let our model automatically come out or come up with the output. That is to say whether the image contains a parasitized cell or an uninfected cell. That said, all we need to do right here is our model.predict. So we have this predict method right here, and then we pass in our data. This case we have test data set, and then we take one value, that's it. Then we pick this up. Now we run that. Model not defined, we have the net and run that. So that's it. We're told that this is an uninfected cell. Now we'll define this method parasite or not, which is defined such that if we have an input X, then that if that X is less than 0.5, consider that we have a parasitized cell, and if it's greater than or equal to 0.5, then it's an uninfected cell. Recall that the way the data was created was such that parasitized was zero, and then uninfected was one. So we are having a threshold value of 0.5. This threshold value is defined now such that every value less than it is considered parasitized and everybody greater than it is considered uninfected. So that's it. If you now replace here with parasite or not, parasite or not, and we run this again, we are told that this is uninfected. We are going to do a test on nine different elements. So right here, we take nine of this, and then we do the subplots. First, we do the initial, and we specify this because we don't want to batch dimension. And then we have the title. On the title, we have the actual output, and we have the model's predicted output. Now that said, we run this. Here's what we get. We see that for this year, we have UU. So the actual uninfected is both uninfected. UU, UU, here we have UP, meaning that the actual is uninfected but it predicts parasitized. Here we have PP, correct, UU, PP, UU, and PP. Thank you for getting to this point. In our next section, we'll look at corrective measures. So we'll look at how to slow and save our model, how to build other types of models using different APIs, how to use different kinds of metrics, visualizing what our model sees, using callbacks, data augmentation, dropout regularization, early stopping, batch normalization, instance normalization, layer normalization, weight initialization, learning rate scheduling, custom losses and metrics, and sampling methods, custom training, tensor bot and hyperparameter tuning, weights and biases logs, weights and biases artifacts, and finally weights and biases sweeps. That said, see you next time. In our previous section, we've built a deep learning model based on convolutional neural networks to help detect the presence of malaria in blood cells. Nonetheless, in the real world, we are not always going to be using our models on a collab notebook like this. Hence, we need to be able to save this model so it could be used externally. In this section, we'll learn how to save and load a model and also do the same process with Google Drive. That is, we'll be able to save our model in our Google Drive and then later on when we want to use this model, we'll just load it from our Google Drive. That said, don't forget to subscribe and hit that notification button so you don't miss amazing content like this. We've built this very performance model though we could improve on it. But then once we close this, we do not save this model's current state. And so if we have to come next time, the model will have randomly initialized weights which will be different from the weights we've got now after training on this data set. Another issue is in case we want to use this model in another scenario or in another environment like say on a browser or on a mobile phone, we'll need to find a way to export this model from here. And so TensorFlow allows us to save our model. Now we'll have to differentiate between a model's configuration and a model's weights. So a model like this, let's suppose we have a model which is defined as such. We have the input, which we pass into a count layer then we have batch normalization, we have pooling for subsampling and then we flatten and after flattening, we pass through a dense layer and we have our output. So suppose we have this small model. Now, all the parameters for the creation of this model are known as the model's configuration. In the model's configuration, we may have it that the model for example, like in this case here, the model starts with a count layer with six filters, kind of size three, batch norm and all this. So these are our model's configurations, but this model's configurations are different from the model's weights. The model's weights are those filters we have, for example, in the case of the conf 2D. So we have the model weights and the model's configuration. And upon summarizing the model, we see clearly here that we have this conf 2D and then we have this number of parameters. And so whenever we want to save a model, we have to take into consideration this configuration and the weights because for this same configuration, we could have different weights. And so there are actually two main options. The first option is to save the full model. That is to save the model configuration and the model weights. Another option will be to save only the model weights. So we could save only the model weights. Now, this option is used when, for example, we don't want to, or we don't even know this model configuration upfront. So we have used this year, we've defined the model's configuration, we've trained it, we've got a new weights and this is the current model state. But if we take this to another environment where we don't have this configuration, then if we've saved this model's configuration and weights, all we need to do is just to load this configuration and weights which have been packaged as the full model. Now, in another case where we are able to get the configuration and all we need is just the weights then we'll just save the weights and then reload this weights since we already have the configuration. Either ways, we'll always need the configuration and the weights. Nonetheless, it's important to note that the most important part of this is actually the weights since working with a randomized or randomly initialized weights after we've trained our model isn't very useful. And sometimes we may take many days to train this model. So imagine you've trained your model for like 10 days and then you wanna reuse that model and the weights have been randomly initialized. You find that those 10 days have been wasted both timewise and monetarywise. So you have to ensure that you save your weights properly such that you could reuse them. And then the great thing with TensorFlow is you could also continue training from the state. So this means that at this point where we've gotten this model's performance year where we have 94%, we could keep training from year so that we could get even to say 99%. So you have to ensure that your saving is done properly. Now let's get into that. But before getting to that, one last point. Also note that with the first method here, with this first method, apart from this model configurations, we also have information like the metrics. So the metrics you use like the accuracy, the loss you use, the optimizer. So the optimizer information you use and all that. So this kind of hyper parameter information has been saved here. So next time all you need to do is just to load your model and then make use of it. Whereas here all you're saving is just the weights. That's it. Let's save our model. This case we have lenetmodel.save. Actually, lenetmodel.save and we give it a name. So we say lenet save model, for example, that's it. Now we have this lenet save model. Oh, we run that. So, and we check this out here. So we check out these files. And what do you see? You have this lenet save model folder. In this folder, you have the assets, which in this case is empty. You have the variables, which actually contains the weights. So you could download this from here. You could download this and then upload it next time. So from here, you see click on download. That's fine. We have the variables, which contains the weights. That's it. And then we have this saved model that put about file here, file here, which actually contains our configuration. We've had our configuration saved and our weights saved. Now let's load this. So we've saved this and now we can now load it. Note that you could always download this. So you could download the weights right here, download this. So let's click on download, download all this. And then next time, all you need to do is just to load it. Now let's go ahead and load this. The loading is quite simple. Here we will define a new model, lenet load that will have loaded model equals tf.keras.models.load model. There we go. And now we specify this exact same name. We have lenet saved model. Now what we'll do is we're going to do a lenet loaded model model and then summary. So that's it. We're going to run that and we're getting this error here, which is unusual. Changing this name actually makes this work. So lenet and then here. So we have this lenet and let's run that again. We save that and then we load this and we have our model right here. So this means if you have to come back to this notebook, all you need is to load this model, which has been saved right here in this lenet folder. And so just like with this, we'll replace this lenet model by lenet loaded model. So let's load this. Let's use this loaded model and do some predictions. There we go. Yes, we'll get UU, UPP, PP, PP. Here we have one error, UU, UU and PP. So it's kind of similar to what we have with the original model. From here, we could also evaluate this model. We have the net loaded model. We evaluate that and let's see what we get. Recall previously we had 94.16%. So now we expect to have something around that value. There we go. Exactly the same output as previously. Now we are going to look at how to load and save with the HDF5 format. Now this HDF5 format is a lightweight version of this TensorFlow model saving method. Here there's only this slight difference. All we need to do is to say, include this file extension. So we have your HDF5 and then we save that. Now you check this out. You should have the HDF5 appearance. So here we have the net HDF5 and then you could see its width by 53 megabytes. Now let's load this model. To load it, what we have here is the same code we had previously. And then here we specify HDF5. So there we go. We run that and we have exactly the same summary. So that's it. Now we're yet to work with custom layers but you have to note that in the case where you built custom layers, then those configurations aren't stored when you're dealing with this HDF5 format. And so that's why generally it's preferable for you to use this first formatting which we presented. That said, we're done with this first method where we save the configurations and the weights. Now let's look at this next method where we save only the weights. So in this case, for example, where we're having this notebook, what we could do is simply just save the weights given that we already have the model's configuration defined in here. So let's get straight away into looking at the save weights method which comes with TensorFlow. So we'll take that off. And then right here we have the net model which we defined already. And then we save this weights as the net weight. So here we've saved this weights. Let's put it in a folder. So we have the weights folder. We run that again. So we could see clearly our weights. Click on that and there we go. We have our weights. And this weights, we have the checkpointing. We have the weights. Now notice how this weights here does in this variables. Click on this variables. Okay, so notice how there is some similarity between what we had here and this. Notice how this is the same as this. And then this index here is the same as this here. Because we said that these variables contain the weights. And then we have the checkpoints. Subsequently, we're gonna look at checkpoints in with TensorFlow. So for now, just know that this is how we save the weights. And then upon defining your model, so you've defined your model, you can now load just this weights and not the whole model. But loading the weights, you're saying that you don't want the optimization or the optimizer configuration. You don't want the metrics and you don't want the last configurations. So that said, let's look at how to load this weights. Now here's all we need to load the weights. We have the learned model, the load weights, and then we load this weights. Let's do this so you see clearly that this loading actually works. So the first thing we'll do is we are gonna reinitialize our model. So we'll rerun this. So we rerun this. We'll compile our model. And then we run this evaluation right here. So we evaluate the model. And so you see that when the models weights are run on the initialized, we have very poor results. So there we go. After random initialization, we have this. Now what we'll do is we'll take the net model and then we load the weights. So we're gonna load the weights and then pass in our weights slash the net weights. That's fine. Let's run this again and then get our new models performance. There we go. Let's see that our model now gets back to the 94.16% accuracy we had initially after doing the training. At this point, we've been able to load and save our model right here on Google Collab. But as we know, at the end of the session, or after closing my notebook, all this information will be lost. So let's see how to save this information on Google Drive. We'll start by imparting this drive. So we'll have from google.drive, from google.collab, we're gonna impart the drive, let's run that. And then the next thing we'll do is to mount this drive. So we have drive.mount, and then we specify the location. So we have your drive and running that, you will be asked to put in an authorization code right here to get this authorization code. So if I just click on this right, this link given to us here, so we click on that link. And then once this pops up, we have this, you select your account. Once you select that account, you now go to connection. So you've connected and then you copy your code. So your code is copied. Now you put this in here and then you press enter. Once that's fine, you see we have here mounted at content slash drive. So mounted in this location, you could see clearly from here. And this tells us we are in this directory content and in this directory content, we've created this other directory drive. Now we click this open and then from this, I can get access to my own Google Drive. If now I want to copy this Lynette folder into my Google Drive so that next time I could just load it from my Google Drive, I'll make use of this CP command right here. So what we'll have is CP, some option, and then we have the source and the destination. So here we are going to specify, or we are going to use this R, so recursive. So we're going to use this to copy directories recursively. And that said, we run the command. We have this here and then we'll specify this folder's directory here. So we have content and then Lynette. So we copy in this Lynette and then to what destination to my drive. So we specify my drive. We have my drive and then in here I have Lynette. I'll let you say Lynette collab, so that's it. From here, I'm going to run this, my drive, and then I will search for Lynette collab. So that's what I have now. I have this Lynette collab right here. And then our next step will be to copy from the my drive to the Google collab such that next time in case where we have not, for example, saved this year, we'll be able to just quickly get that information from the drive onto the Google collab. Also note that this is really used through a data sets. So what we could have here is a data set in our collab and then we could transfer that data set to our drive and vice versa. Now let's do the same thing. So here we are going to copy this back, but this time around we are going to copy this into Lynette collab. So we're going to create a new photo here, Lynette collab and then take this information. So this time around we're copying from our drive into Lynette collab. So that's it. And then Lynette collab, we run that. And let's click on this, click again. And guess what we see? We have our Lynette collab right here. Thank you very much for filling up to this point and see you next time. Hello everyone and welcome to this new section in which we'll look at different ways of creating models other than the sequential API which we've seen so far. In this section, we'll look at the functional API. We'll look at building collable models. We'll look at building model via subclassing. We'll also look at building our own custom layers. Previously in those cars, we said that there are three ways in which models are built in TensorFlow. That is the sequential API using the functional API and then finally model subclassing. As of this point, we have been using the sequential API as you could see right here. Now you may ask yourself, why do we need to use a different method in creating TensorFlow models when so far we've achieved close to 99% train accuracy and around 95% test accuracy. Now, as you may have noticed, so far all the models we've been building have taken up this kind of structure where we have an input, we have the first layer, the next layer, which has been stacked in this sequential manner right up to this very last layer here and then we have the output. So the question we could ask ourselves is, what if we have a model which takes in say two inputs and has three outputs? These kinds of models are very popular in deep learning and we shall look at them subsequently. But before getting there, you could just imagine a problem where instead of classifying whether we have a nonparasitic or a parasitic cell, we wanna know the exact position of that parasitic cell or in general that cell in the image. You would find that you would have one output which classifies whether it's a parasitic or not. So we have this first output, parasitic or uninfected and then this other output which gives us the position of the cell or the exact position of the cell in the image. So here we see already how we could get two outputs from this, let's take this third output out. So here we see we could have, let's even take this one. So here we have this one output, two outputs model and with a sequential API, we can't really do this. So that's why working with a functional API is very important. The next point is we'll be able to create more complex models with the functional API. So there is this model known as the ResNet which is very popular in deep learning computer or deep learning for computer vision. Now, a ResNet like structure will look like this. We have this model, this layer of outputs I've been passing to this next layer and then we have the outputs of this which are gonna be concatenated with this outputs and then after this concatenation, we are gonna pass this to the next layer right here. So if we wanna add this layer, we could have a layer here and have that. So as we're saying, we take this output and then concatenate it with this next output before passing to this next layer. And so those kinds of structures or those kinds of models could not be built with the sequential API and hence the need for the functional API. And then the last reason why we are gonna be using the functional API is the fact that we could use shared layers. With shared layers, we could have a layer or a particular layer in our model which has already a predefined way of encoding information. So when we pass information, let's say we have this input, let's say input one, when we pass this input one, this layer right here or this encoder produces an output which is gonna be different from when we pass in another input I2, but the way it produces these outputs is in a very thoughtful manner. So we could have I1, I2, I3, which all share this layer and then we have other layers of the model which follow on. That said, we'll look at how to create the functional API. So here we have the sequential and then just below we are gonna create this functional API. Before starting all the creation, we are gonna impart some classes. So start by imparting the input class right here which is a layer, we import input and then we have from tensorflow Keras layer, rather models, we're gonna import model. So we import the model right here and we import the input. We run this, that should be fine. We now have this func input since we're using the functional API, this way of calling that, we have the func input and then we have input which we just called and this takes in the shape. So here we are gonna copy this exact shape we use in the sequential API. Have the shape right here, there we go. We copy that shape and then we reuse it here and create in this input layer. So here we have that and then we have your shape. So we've had the shape, those points, you could start stacking up all these different layers we had stacked up in the sequential or with this when we're using the sequential API. We started with this, yeah, this com2D right up to this dense layer. So there we go, we is gonna make use of this. So we copy that and then we are gonna paste this out right here. Now, first things first, we have an output. So first things we have this layer that we have this com2D which we've defined and then we pass in the output from this input layer. So here we have this func input, we copy that and then we pass this into this comf layer right here. Now, once it passes into this comf layer, we have an output and that output is this x and then you should guess that right, we pass this x into this back norm layer. So here we have x as you could see and then we have an output of x, there we go. From here, we pass in the x into the max pull 2D layer. So we have this, we cut that and then we have this x right here. So we'll just repeat this same process right up to the end and there we go. You see that we haven't done much changes as compared with the sequential API. So that's it, we pass in this input right here, we have x, we pass it in, we have this, we pass in and right up to this end. Now, once we get to the end, we are now going to create the lunet model from this. So we have lunet model equal model, which will import it and then we have the func input. Now, yeah, let's say we have func output. So we pass this last and then our last output is func output. So we have the input and then we have the output. So there we go. We can now give it a name. We have name lunet model. If you look up, we have, let's take a look at this right here. So this would be the input image. Here we have our input image and then we've created a model lunet model. And then from here you could simply do lunet model summary. Now you'll notice that we should have exactly the same summary as we had with a sequential API. So let's run that and see what we get. Yeah, we have how many parameters? We have 4,668,297 parameters. There we go. You see, we have exactly the same number of parameters, the same number of trainable and nontrainable parameters. So basically what we've done here is we've created this model created with a sequential API. Now we've gotten this. We'll see that we have to change absolutely nothing from our code. So yeah, we was going to compile our model without changing anything. We have the same lunet model. Now we could also change this, let's say lunet func. So you see clearly that we are actually using this functional model right here. So we have this func, there we go, func and that's it. We could run that. And then we recompile right here. So we are not changing any parts of this. We recompile that and then we train the model. We are getting this arrow because of the way we named this model right here. So let's have this lunet model. That's fine. We recompile and then we run. So that's it. We train our model and here is what we get as results. Now coming back to our model, we'll see that we have this feature extraction unit right here. So this conf layers are responsible for extracting useful features from the images. And then this last layers are responsible for correctly classifying whether the image is parasitic or not. That said, we could build a model known as feature extractor. And so here we're at this. We have our model feature extractor, which is going to be like similar in construction as what we've done so far. So we just have that copied and then we have this. But the difference is we are not going to include this other, this final layers right here. We're only at this point. And then we'll have as output this year. So this is our functional, here we'll call this extractor. Let's just say we have this as output. So we have this output and there we go. So here we have our functional input and then we have this output. And then here is the feature extractor. So we have our feature extractor model right here. We could do this feature extractor. And then we summarize this. So let's run this and see what we get. So that's it. We have our input and then we have this output right here. At this point, instead of writing all this here, we're just going to call or let's take from this point. So we have our feature. Let's look at the name. We gave it, we gave it the name feature extractor model. So here we have feature extractor model. So here's our feature extractor model. So we take all this off. And then in here we pass in our input. So notice how we are making this model look like a function. So TensorFlow models are callable, just like the layers. And as you could see here, this feature extractor model could be seen as a layer, just like the dense layer, the batch norm layer and all other layers. So we've gotten this X from this input, which has been passed in our model. And then from here, you see, we pass this X into this flatten and we have the rest. So that's it. Let's now rerun this again. So you could see what we get as output. And as you could see, we get exactly what we expected. We have the same number of parameters and there is this difference here where we have this feature extractor. So unlike before where we had the conf nets, like the conf 2D batch norm max pooling and the same, like let's go up here. There's actually a feature extractor. So unlike before where we had this and then this, now it has been replaced with this feature extractor, like right here. That said, we've just built this model using the functional API. And in subsequent sections, we'll build even more complex models using this functional API models, where we're going to use shared layers. We're going to have multiple inputs, multiple outputs and models where we're going to have even more complicated model configurations. It's important to note that you could mix up the functional API model creation style with that of the sequential API. So you're, instead of creating this, so instead of having this of feature extractor created like this, we are going to create it using the sequential API. Let's add that. And then we copy out this from your, copy out this full model with a sequential API, this is out and we take all of the feature extraction part. Here, you see, we take this off and then we're left only with this feature extraction part. Now let's call this feature extractor. So feature extractor, sequential model. There we go, this is out right here and we're fine. So we have our feature extractor model. We run that, that's okay. Let's take this off and then we'll just make sure we put exactly the same here. So there we go, we paste it out and we rerun this. You see, we should be able to get exactly the same output. See, we have exactly the same output and here, instead of our feature extractor model, we have your sequential layer. So that's it. This shows us that we could mix up these different ways of creating models. From this point, we'll look at the model subclassant. So right here we have our model subclassant. There we go. It's important to note that model subclassant permits us to create recursively composable layers and models. Now, what does that mean? This means I could create a layer where its attributes are other layers and this layer tracks the weights and biases of the sub layers. Before taking an example, let's make this import. So we're going to import layer from layers. We have tensorflow.keras.layers. We're going to import layer. We run that and then we move on to create our model using the model subclassant. Now that's set, we have this feature, our feature extractor. So we have feature extractor right here and then this inherits from layer. So inherits from tensorflow layer and then we have an init method and followed by a call method. So that's it. Now let's use the right syntax. Here's a class, here's a method, init method. There we go. You could always check on our free cars on Python programming in case you're not versed with all the syntax. So that's it. We've had that. And now just like the way we did when we were creating this feature extractor, let's go back to our feature extractor with a functional API actually. Here you'll see, let's copy this out and then get back to our model subclassant. So that's it. Here, let's just put this down here. And then we have the super feature extractor. There we go. That init, so that's it. Now that we have defined this, we could now go ahead and use this layers as the attribute for this feature extractor layer. So there we go. We have this here. We have our conf, the self.self.conf one, which is this conf2d right here. So we take all this off and just place it here. And then from here we have the batch batch one, which is this batch norm layer right here. Take this and then place it right here. We have the max pull2d, take that off. We have self.pull one, there we go. So that's it. And then we just repeat this process. So we could have the self.conf one, conf two rather, so conf two. And then we just take up this parameters. Let's take this from here and place it right here. So here instead of having this, we will just get this. And then the batch norm export2d remain the same. So that's it for our init method. We now go ahead to build our call method. We have our call method here, which takes this input x. And then here, what it does is it permits us call each and every layer defined here in this init method. So here we have x equals self. So the value of x is going to change. We're going to pass it through self.conf one and self.conf one, we're not taking x. This looks similar to the functional API. We have x equals self.batch one, there we go. Batch one and then we have x equals self.pool one, x equals self.conf two, x and then self.batch two. And finally we have self.pool two, that's it. That's it. So now from here, we just return x. Also note that we could pass in a parameter, an argument like the training, which can tell us whether to use a given layer or not during the training process. Nonetheless, for now, all this is going to be used during training. So we have that. Now let's take all this off. We run that. There we go. That's run correctly. Now we should be able to build our model. So here we have this Lumenet model, which took the feature extractor model from here. Let's just copy this. And then, but before copying that, we have to ensure that we create this here. So we have feature subclassed. So we have our feature subclassed, which is this feature extractor viewed right here. So yeah, we have feature extractor built. So that's it. Now you could always pass this parameters, like the number of filters, the kernel size via this. So we could pass this here. You could specify filters and the kernel size. So let's just do that. So let's say filters takes kernel size, stripes and say padding and activation. So if we could pass all this here, such that when we get to this point, we just, we don't need to specify all this. So we'll simply take this off. We don't need to specify all this anymore. Now we have activation, all that's specified. Okay, we could also include the pool size. Let's include the pool size and all the strides here. Okay, so let you say we're gonna have two times the strides because we always specify the strides to be one. So let's take this off. Yeah, we're gonna have two times the strides and we've defined, here we have the pool size. So let's take this off and let's take this off rather. Let's get back. So we take this off and take this off. So that's it. We've defined all that. And now we are ready to pass all these values. So just simply copy all those values. And then in here, we specify the number of filters. So yeah, let's take eight, the kernel size. Let's take three, the number of strides. We have one, the padding. Here is valid, valid activation. ReLU pool size is two. So that is it. So we've defined all this. Now at this point, let's ensure that we have this times two. So let's run this now again. And then we have our feature extractor that we're getting this error. Let's try to understand why and how to solve this error. So there we go. Scrolling, we have this. Now, the reason why we have this error is because of the order in which this come to the Texas arguments. So if you look at this, trying to get this to come up. Anyway, we just look at the documentation. You see, we have filters, kernel size, strides, padding, data format. So you see, we have the data format, the duration rate groups before the activation. So it's important that we specify that this is filters equal filters. Then you will specify kernel size equal kernel size. If not, it's gonna take this to be the data format. So that's it, strides, padding equal padding, and activation equal activation. So this should be fine now. We'll run that again. We have this error, but this time around is for the second conf layer. So let's just redo what we had done here and take this off. Oh, notice filter size times two. So here we have filters times two. We try to do the same for the max pool 2D. So here we have pool size equal pool size. That's it, strides, that's fine. And then yeah, we have pool size equal pool size. Okay, so now everything should work fine. We run that and there we go. Everything works fine. So now we've created our layer, our feature subclass layer. We will now be able to use it in this model right here. So let's copy that and then get back to this. Oh, this is out here. And now here we have our feature subclassed. Let's take all this off subclassed. And then we could comfortably run this and we get in this error. Now let's get back to check. We see batch two, there's an error level of batch two. Now we see here, we have this two, there should be two and there should be two. So that's fine. Let's run that again and everything works well. So you see, it gives us exact same output we expect to get with this feature extractor right here. Then one last thing we could do is instead of doing this way, let's insert some code. Instead of doing this way, we are going to create a model using this model subclassing method. So yeah, we just copy this out. So we copy that out. And then in here, instead of having layers, so yeah, we're not having a layer, we're having a model. So instead of having layer, we now have model and then we're going to define a feature extractor. So our feature extractor now is going to be the feature extractor we've just defined here. So let's go up and there we go. So we're going to get this feature extractor right here. And then what we'll do is we put it in here. So let's take all this off. We have all this off. That's now our feature extractor. So we've got in this feature extractor. That's it. And then once we get X, we're going to pass this to our feature extractor. There we go. Let's take all this off and we'll find. Now we're done with the feature extraction. We could get the other parts which make up the model like this pattern, the dance and the back norm. So let's take this here. There we go. We are in this model and let's just copy it. Let's paste it out here. So here we're going to have this. Also note that we're going to have feature, or let's say, LearnNet. This is our LearnNet model. Modify this here. We have the LearnNet model. LearnNet model. So that's fine. Now we've gotten this. Everything is understood. We now check out this self.flatten, equal flatten. And then next we have self.dance1 equal, let's copy this out from here simply. So we have this dance right here, which is our dance one. There we go. We have our dance. And then we have self.batch1, which is our batch normalization. There we go. Copy this paste. Move to dance2 and batch2. So here we have dance2, we have batch2. This is 10 actually. So we have that. And then finally we have this dance layer. So let's take this off. We have the dance layer. Output one, activation sigmoid. So that's it. Oh, we call this dance3. So that's fine. Everything seems okay. Let's take this off now. And then there we go. We get into our call method. So yeah, we get into this call method. And this call method will basically call all these different layers. So after the feature extraction, notice how we've created this class. And this class makes use of this feature extractor, which was also created using the same model subclassing method. So there we go. We're using this here and we're actually using it here. So that's it. Now we just make this calls. So we have x equal self.flatten. And then we pass x. x equal self.dance1 as in x. x x equals self.batch1 as in x. And then finally we have this. So there we go. We now return x just as we did previously, and we have our model. So here we have our lunette model. Lunette subclass model. And we have this lunette here, lunette model. So let's take this off and there we go. We've just built our model, which when we try to find a summary, so we try to do lunette subclass.summary. What do we obtain? See, this model has not yet been built. Do the model first by calling build or by calling the model on a batch of data. So we're going to call this model on a batch of data. Right here, we're going to have lunette subclass. And then we have tf.zeros, tf.zeros, and one, two, two, four, two, two, four, and three. So let's run this and see what we get. That's fine. So yeah, we have our summary and then we are ready to compile this model. Yeah, we have lunette, lunette subclass. So we compile lunette subclass. And then we're going to fit lunette subclass, lunette subclass. So we feed that and everything should work fine. Let's take this to just five ebooks. You can see that we're getting similar results when we compare this with what we have with a functional API and the sequential API. So we now move on to creating custom layers. If you could recall from the previous sections, the way the dense layer is built is such that if we have this as our dense layer and then we have this input right here, let's call this I, or let's say it's X, we have this input X, then we have a certain M times X plus C. So this M is actually the weights. So the weights times X plus the bias, let's call this B and then this is now equal our output. So here we have an output of Y, Y equals MX plus C. So the symptom means if we want to recreate the dense layer, then we have to take this into consideration or the definition of the layer from scratch into consideration. That said, we could define a neural learn dense. So it's like our custom dense, neural and dense. It's gonna inherit from layer and then right here, this class. So we have this class inherits from layer. Then from here, we have our init method, init. init and then we have the super with pass and neural learn dense. There we go, we have that, that init. So that's it. Now from here, if you had noticed, whenever we're creating a dense layer, like let's call up here, whenever we're creating a dense layer, we generally had to specify at least because this is by default. This is not by default, but. So here we need to pass in this year to specify the number of output units. Now that said, we have to take that into consideration when building our neural and dense layer. So here we have output units. There we go, we have our output units. And then here we'll define self dot output units to be equal output units. So that's it. Now from this point, we're gonna build this layer. In order to build this layer, we have to take into consideration this definition right here. But this definition put out the way this isn't very clear. Now let's make this, or let's break this up. So you suppose now we have this input of shape batch size by let's say number of features. So suppose we have this input. Now what happens here is this input is gonna be multiplied by this weights. So it's gonna multiply by this weight, which happens to be a matrix. Now we take this and multiply by that matrix. And for the multiplication to be valid, we need to ensure that the number of columns we have here matches with the number of rows of this matrix. But then what are the dimensions of this matrix? We have to note that this matrix has to be defined such that we have a shape of F. This F here must match by the number of output units. So if you want a number of output units, for example, to be one, then here you should have one. And that's why when defining the dense layer, we don't need to specify this as this value is gotten automatically from the number of columns in the inputs since if we don't take this, we are gonna have an error. So TensorFlow test takes this automatically piece in here and then collects the input you pass in the dense. So when you specify, when you say you have this dense like this, and then we say, for example, one and maybe some activation, then you have that. So let's say we have some activation here. Now, once this one gets here, this weights matrix is now defined such that the input you pass into this dense is gonna affect the number of rows we have. But then what you pass in as argument here is gonna tell us or give us a number of columns we're gonna have for this weight matrix right here. So that said, we have F by one. And then when we multiply this, we're gonna have B by one. So we see, we now understand how we get this output. Then we have plus B by one. So that's it. Where this one comes from the bias. Now, once we add this up, we have an output of B by one. And that's our Y. That's the shape of our Y. If that's understood, we'll move on to building. So here we have our build method. We have self and for now let's keep it that way. So there we go, we have this build method. And then we're gonna define our weights. So we have self.weights. Let's specify weights, we call that. And then here we're gonna have the self.addWeights method. So this actually comes with this layer class right here. So we're able to call this because we are inheriting from the layer class. So here we have self.addWeight. And now we specify the shape. So guess what? We are gonna have our number of rows. So n rows, which is gonna come from the inputs. And then we're gonna have our number of columns, which is gonna come from this output units we specify. We should pass in when calling the neural and dense layer. So here we're gonna have self.outputUnits. So we get a number of output units. Now, how do we obtain this number of input units? We're gonna look at that shortly. For now we have the weights. And then we have that. And then let's go to bias. So we have self.biasis. And self.this is also a weight. Add weights. And then we specify just the number of output units since it's one dimensional. So we have output units. There we go. And so at this point we'll define our weights and our bias matrix. This actually weights. Just similar. So that's it. Now let's go into call. So we have our call method. We should actually get a job done. And then we have our input. So let's call this, yeah, input. Let's add an S. Or let's say input data. Or input features. So there we go. So we have that. And then what we're gonna do here is we're gonna return simply the matrix modification as we've seen already of the weights. So we have self.weights. And this input features. It's actually the input features times the weights. Let's get back to this. Here if we take the input, yeah, and it takes input times the weights. Because if we have the weights times the input we will have F1 times the F. We call our weights this, our weight shape and then this our input shape. More than this, you see that this is always the same. So it's not gonna be, you're gonna try an error. And so that's set. What we're gonna do is we're gonna just simply have input features right here. So that's it. And then we add up the bias. So here we have self. biases. Which is from this one right here. Now the way we're gonna get this number of rows is gonna be easy. Yeah, all we need to do is to specify here that we have the input feature, features, shape. We have our input features shape. Which is gonna come automatically from this. And then to get the number of rows, all we need to do here is have input, features, shape. And then we get that last dimension. So let's get back to this. We call, here we have an E by F plus E, F. And so here we need the weights, it needs to be F by the output. So to get this F, we just need to take the input and then get this last element right here. And there is the number of columns we have here. So that's set. We will just have that input features. Specify this, take this index. And that will be good. So that's how we get, we obtain this value automatically. The number of rows we've been looking for. So now we've gotten this, everything seems fine. The next thing to do is to specify that it's trainable. So we have to specify that these weights are trainable because in some cases we may want that the weights shouldn't be trainable. So let's have the trainable equal true. Now there's no self here, there is an argument. Like it's one of the arguments which I've been passing to this add weight method right here. So we would have that. And then there again, we have this trainable equal true. Now apart from that, we could randomly initialize our weights in biases. So here we have random, normal, random normal initialization, it's fine. Then it is same here. So we have our initializer equals random normal. That's good. We now run this. There's our neural learn dense layer. Then after running this, let's get to integrate this. Let's make it quite simple. We just use our sequential API. So let's get back to this sequential API we have built initially, copy that. And then we are going to integrate this new dense layer, this neural learn custom dense layer. So there we go, we have that. And then instead of dense layer, here we have neural learn custom dense layer. So you see that you can be able to create your own layers with TensorFlow, that's it, neural learn. So neural and dense, neural and dense. Now you see, this should try an error because we don't take into consideration the activation. And so what we could do is, we could get back right here and say, if the activation is equal relu, if activation is equal relu, we'll return this. And then else, let's say elif activation is equal sigmoid, return that, let's get back. We have this. So we're going to return this, let's copy this and then paste out right here. And then we have els, we will do the same. So now let's get into this and see the modifications we're going to make. There we go. So here we have our relu. In case it's relu, what we want to have here is tf.nn.relu of that. So we're going to pass this in to our relu. And then if the activation is sigmoid, we should have tf.mat.sigmoid. So that's it. There we go, we have that. And then this one we just maintain. So we have modified our code such that we now integrate the activations. There we go. That seems fine. So let's run this and then take this error. So here we have this double equals, find some syntax. And then we come down here, return seems fine. Let's take this again. Here we're supposed to have, we're supposed to specify activation. So we're supposed to have activation. And then salve.activation for activation. And then here we have salve.activation, salve.activation. We run that again, it's fine. And then now what we do is we pass, we just simply run this. So run that and we get in this error right here. To solve this problem, what we're gonna do is we're gonna include the shape here. So we're gonna have shape equal that and then shape equal this. Let's rerun that and we should have no error again. Oh yeah, we have in cancer attributes weights, likely because it conflicts with an existing read only property of the object. So right here, instead of using weights, we're just gonna say W. So we have the W and then, right here, we should have W. Now, since we don't want to repeat this over, what would you say we have a pre output. So we have our pre output, which is this here. And is that out? And then here we have W. Right here, we have B. That's it. And now we have, if this, then we pass the pre output. And we do the same right here. So we have just a pre output before the activation. Now, yeah, we just have pre output actually. So we just have pre output to take that out. Now running this should be fine. So let's run that and see what we get. We run this, we run that, and there we go. We have our model, the same exact model we have been building right from the start. But this time around, we're having, we're using a custom dense layer, which is our neural and dense layer. Now let's go ahead and compile this model and then train it. Yeah, we've actually maintained this year. So we have to change this name. Here's Lynette model. Let's say Lynette custom. Yeah, custom model. Yeah. Okay, we copy that. We have all the net custom model. Let's have it here. There we go. We run it. That's fine. Right here, we have Lynette custom. We run that. Yeah, we have Lynette custom. We run that. And then let's check out. As you can see with the train, there's a slight difference in the last values. And accuracy we're getting for this first epoch. And most probably, this is coming from this random normal initialization we've chosen here as with the standard dense layer, which you could see here. This kernel initialization, or the weight initialization is using this blurrow uniform. And the bias initialization is the zeros. So here we have all zeros for the biases. And then here we use the blurrow uniform method. So yeah, as you could see, this is in performing as well as what we had previously. But scrolling up, there is this error. So yeah, it was meant to be sigmoid. So let's stop this. Let's interrupt this training. And then get back and run this. So we will run this and compile and then start with the training. Again, this time around, it looks better. It looks more like what we should expect. So even though we're not using the blurrow initialization method, this training process looks quite similar to what we would have had in the case of the blurrow initialization method, which is used with the standard dense layer. And here is what we get after training for over five epochs. Thank you for getting up to this point and see you next time. Hello, everyone. And welcome to this new section in which we will look at other methods of evaluating our model other than the binary accuracy, which will be seen so far. So in this section, we'll look at how to compare the true positives, false positives, true negatives, false negatives, the precision, the recall, the area under the curve, how to come up with confusion metrics like this, and finally, how to plot out an ROC curve like this one, which permits us select the threshold more efficiently. Don't forget to subscribe and hit that notification button so you never miss amazing content like this. Let's now look at other ways of evaluating our model other than the accuracy, which we've seen so far. To better understand why working with other accuracy isn't always a great idea, we have to take into consideration the fact that our model on a test set has 94% accuracy. Now, this means that we have six out of 100 predictions which are actually false. Now, what if I get to the hospital and I'm told that I don't have malaria when in fact I actually have this disease? So that said, the model predicts uninfected and I actually have the parasite in my bloodstream. Now, this particular situation becomes very dangerous because the patient gets by home thinking he or she doesn't need any treatment, whereas that patient actually has this parasite. You see that even with a 94% accuracy, we wouldn't be able to save ourselves from such chaotic model predictions. Now, in another example, you have a situation where the actual is, let's put it out here. So in another example, we can have a situation where the actual is unparasitized. So actually you do not have this parasite, but the model predicts that you have the parasite. Now, in this case, although we have a wrong prediction, we have actually a less chaotic situation as compared to this previous case here, since at least actually you are uninfected. If we consider negative, that's this negative to be uninfected and positive to contain the parasite, that's P. So here we consider we have positive and then we have negative. So here we have negative, uninfected and positive parasitized. Now, if we let this, you will find that this first situation where actually we have the parasite and the model predicts unparasitized is known as a false negative. So here we have this false negative. And this is because the model predicts negative when it isn't actually negative. So since we have this wrong prediction for negative, we call it a false negative. And in this case where we have the model predicting parasitized, that is positive, when it's actually negative, we call this a false positive. So here we have FP and here we have FN. Now, there are two other scenarios that is with TN and TP. For the TN, we have the true negatives and the TP, the true positives. For the TN, we have the model predicting negative when actually we are negative. That is, we have the model saying this is uninfected when actually it's uninfected. So that's a true negative. And then for the true positive, the model predicts a positive that's parasitized when actually it's parasitized. So that's a true positive. Now, hopefully you've understood the concepts of true negatives, true positives, false negatives, and false positives. We can then summarize all this information in this matrix known as the confusion matrix. This confusion matrix, we have the true negatives, the number of true negatives here, the number of true positives, the number of false negatives, and the number of false positives. This means that if we have a test set of say, 2,750 different data points, and then we run this or we evaluate this with our model, we'll be able to get this number of true negatives, get this number of false negatives, number of true positives, number of false positives, and hence better evaluate this model. So if we take this example where we've evaluated our model on the test set, and then this model A produces this confusion matrix, and this model B produces other confusion matrix, where here we see we have for the true negatives and true positives, we have 1,000, 1,000, that's 2,000 correctly predicted data points, and then here we have 1,000, 1,000, 2,000 correctly predicted data points. And then for this model A, we have 700 false positives. And 50 false negatives. Whereas for model B, we have 50 false positives and 700 false negatives. Recall we had defined negative to be uninfected and positive to be parasitized. And hence, if we had to choose a model between A and B, we'll try to choose that model which minimizes the number of false negatives. So we are not saying that we shouldn't minimize the number of false positives because we have to try to minimize all the false predictions. But then, since with the false negatives, we are telling a sick person that he or she isn't sick, this at least is worse than telling a healthy person that he or she is sick. And so we'll try to prioritize the number of false negatives and based on this prioritization, we are gonna prefer model A since we have the smaller number of false negatives. And so here, we'll choose model A over model B. Now, as a quick note, you may decide to say no negative is for parasitized and positive is for uninfected. It isn't a must that this must be tied together like this, but for clarity purposes, it's better to look at it this way since saying you are tested negative means you're uninfected and tested positive means you're parasitized. Now, you should also note that depending on the kind of problem you want to solve, in some cases, you will want to prioritize minimizing the number of false positive over the number of false negatives. So this actually depends on the problem you're trying to solve. But in this case, we are prioritizing the number of false negatives. Now, based on what we've seen so far, we're gonna introduce several new performance metrics. And others take up this different formulas we could see right here. We have the precision, which is the number of true positives divided by the number of true positives, plus the number of false positives. Recall, true positives divided by a number of true positives plus a number of false negatives. The accuracy, the number of two negatives, plus number of true positives divided by number of true negatives plus true positives plus false negatives, plus false positives. So we'll stop in this first three for now. Now, what do you notice? You'll notice that in this position and recall, we have this true positive, true positive, and here true positive, true positive. What differentiates them is the fact that in the position we have false positive in the denominator, and in the recall we have false negative in the denominator. This means that if the number of false negatives is high, that is we have, let's say we have a constant, a constant K divided by a high value. So constant divided by a high value. So constant divided by high. Here we're going to have a low output. And so if we want to have a low recall, then we need to have a high number of false negatives. And if we want to have a low position, then we need to have a high number of false positives. Now in our case, we're trying to minimize the number of false negatives. And since we're trying to minimize the number of false negatives, it means that we're trying to maximize the recall. Since minimizing this denominator, we're until maximizing this overall TP on TP plus FN. And so here we're trying to prioritize the recall over the position. Now, if you look at the accuracy, you'll notice that we have TN plus TP and TN plus TP right here. TN plus TP, TN plus TP, and we have FN plus FP. If you are keen enough, you should see that this accuracy doesn't give any parity for whether the false negatives are the false positives. It treats these two as the same. But as we've seen previously in the real world, in solving real world problems, many times we'll have to prioritize. Hence, the accuracy may not always be the best metrics for our problem. In our case, we find that using the recall is even better than using the accuracy. As with the recall, we get to reduce or, with the recall, we get to see whether our model does well at minimizing the number of false negatives. Now, we also have this F1 score, two times the precision times recall, this precision is recall, divided by the precision plus recall, the specificity, the number of true negatives divided by number of true negatives plus number of false positives. And then we also have this ROC plot right here, ROC stands for receiver operating characteristics. Here we have the true positive rate and the false positive rate. The true positive rate is the number of true positives divided by number of true positives plus number of false negatives, which happens to be the recall. And then the false positive rate is the number of false positives divided by number of false positives plus the number of true negatives, which if you look carefully, you'll find that is equal. One minus the specificity, which has been defined right here. Before getting to understand this ROC plot, which we've put out here, let's recall this two models, which we had described previously, that's model A and model B, where model A had a smaller number of false negatives as compared to model B. Now, if we pick out just this model A and then we are interested in reducing this, or let's say we pick our model B. Suppose we pick our model B, I will interested in reducing this number of false negatives right here. Then one solution could be that of modifying the threshold. So if we have a threshold of 0.5, meaning that above, like we have the 0.5, below 0.5 we consider negative, above 0.5 we consider positive, that is pasteurized and then below uninfected. Then what I could do here is reduce the threshold. So if I take the threshold to say value of 0.2, let's take this here. So I've reduced the threshold now to 0.2. You see that for most of the predictions, our model is going to say that this is a parasitized output since now the threshold has been reduced. This means that if we have a model prediction of 0.3, which initially would have been uninfected, now this model sees this as parasitized. And so this makes it now more difficult for our model to have false negatives since our model now has this tendency of predicting that a given input image is parasitized. That said, we now need to look for a way that we could automate this process. That is we want to be able to choose this threshold correctly or rightly. Because if let's say we take a threshold of say 0.001, it means that anytime our model predicts less than 0.01, it's uninfected and then greater than 0.01 is parasitized, then this will be very dangerous for the overall model performance as now most times would have the model predicting that the input is parasitized. And so our aim here is to pick this threshold such that this number of true positives and true negatives we've had right here don't get reduced. Now, the way we could look at this is now by using this ROC plot. With this ROC plot, what you actually have here is the different true positive rates and false positive rates you'd have at a given threshold. So this means that a point picked here is just a given threshold. Now let's suppose that the threshold 0.5 is about here. So let's pick this. Let's suppose that this is 0.5. We could pick another threshold. Let's say this one is say 0.2 and so on and so forth. Let's say this one is 0.1. Now we could have another model with this different ROC plot, another one with this kind of ROC plot, but note that overall our aim is to ensure that this false positive rates is minimized and the true positive rate is maximized. So if we have an ROC plot, which is like this, so let's redraw this. If we have this kind of ROC plot, that is one that goes up straight like this and then comes this way, we have this right here. So if we have this kind of ROC plot, then we will be able to pick out this threshold right here, 0.0, say X. We'll be able to pick this threshold because at this threshold, for this threshold, at this point, the true positive rate is at its highest value, that is of one. And then the false positive rate is at its lowest possible value, that is of zero. So here we have one and then zero. So here we go, we have this and then we pick out the threshold. Now this value of X could be five, could be four, whatever. So we have a zero point, whatever value will lead us to this. Nonetheless, many times we wouldn't have this kind of plots. So we will do it plus, which will look like this, this and so on and so forth. Now, once given a plot like this one, let's suppose we have a plot like this one. The aim here is to ask yourself the right question. If I want to make sure that my recall is always maximized, which is our case, we will try to ensure that we pick out these points around this. So we'll try to pick out these points towards the top right here. Let's take some of this off. So as we're saying, if we want to maximize our recall, is that normal that will pick out threshold values, which will take us around this region because it's around this region that our recall is maximized. Let's do this so you could see that clearer. Okay, now the problem with picking a point around this is that when you pick a point around this, the false positive rates is increased. So you need to find that balance between this false positive rate and true positive rate. So it will be much logical to pick a point around this right here. So we could pick around this region instead of this previous region right here. Now you could see that in this region, or let's say we pick this point. If you pick this point, your false positive rate now is smaller while your true positive rate is, or your recall is maximized, though it isn't the best possible recall we could have. But trying to focus on getting that recall of one will lead you into trouble since getting a recall of one in this case will increase our false positive rate. And then if we're dealing with a problem where we're trying to maximize the precision, then in that case, we want to ensure that this false positive rate is minimized. And so in this kind of problems, we'll want to pick a point around this. So you see, we want to pick this kind of value since at least our false positive rate is minimized. But then if you want to go and pick a point around this year, you would have a false positive rate of zero, but doing this will get you this into trouble because here you're going to have a true positive rate, which is very small. And so you need to get that balance. Now, if you're having a problem where it doesn't really matter that is you aren't trying to prioritize the precision or the recall and working with the accuracy is just fine, then you could pick out this point right here. So this is when you are having, or you don't have to prioritize on any, here is when you're trying to prioritize on the recall and here is when you're trying to prioritize on the precision. But as is one great thing is with this tool, that is the ROC plot, we are able to pick out this point and then automatically get that threshold we need to work with. And so when doing predictions, we will not, or we may not use 0.5, but we're going to use a certain threshold, which will suit this objectives we've set initially. Now we'll now move to the area under the curve. For the area under the curve, we generally use this when we comparing two models. Here we have this model, let's call it model A. It's not actually this model A, it's a different model A. Let's call this model alpha. And then we have this other model, which we shall call model beta. So we have model alpha and model beta. It's clear that model beta is better because it gives us better options. Now you'll see that if I find myself here, I get it better through positive rate, false positive rate balance as compared to when I find myself at this position. And so if we are comparing these two models, we could make use of the area under the curve by calculating this area covered here. So let's bound this and then for alpha, we have this area under the curve, popularly known as AUC, A small U and then C, which will give us this. And then for beta, we're going to have this area under the curve now, which covers this area plus this extra area right here. And so in general, if we have two models and then we want to compare them, then we could use this area under the curve. Since it shows us how much freedom we have in playing around with the thresholds. We now get back to the code and see how we're going to implement this new metrics we've just talked about. So right here, we have metrics and then we have the false positives. We have the false negatives. We have the true positives. We have the true negatives. We have the precision. We have the recall. We have the AUC, AUC. And then since we're done with a binary classification problem here, we have binary accuracy. We run that, that's fine. And then right here, instead of having this metrics, we'll define this metrics list, which will contain those different metrics, which we've just talked of. So we have the true positive, false positive, right up to the AUC. Let's run this, compile, and then feed our model. You'll see that as we train, we have the true positives, false positives, true negatives, false negatives, accuracy, precision, recall, and AUC scores, which have been given to us. As you could see, the other results we obtained, we're going to go ahead to evaluate our model. So let's get to the model evaluation. We run this, test data, and then we have our model evaluated. There we go, our model has been evaluated. As you could see, we have this last 0.35, number of true positives, 1,323, number of false positives, number of true negatives, false negatives, accuracy, precision, recall, and AUC. So that's what we have for this model. For now, we have been showing you this binary accuracy, false positives, up to AUC. What if we plot out the confusion metrics and also the ROC plot? So we'll impart scikitlearn and seaborne. Right here, we have scikitlearn and from scikitlearn.metrics, we're going to impart confusion metrics. And then for seaborne, we need to import seaborne and as SNS, so that's it. So here we are importing scikitlearn and seaborne, which we'll both use in plotting out this confusion metrics. So let's run this and then we'll visualize our confusion metrics. So we'll have to get the level, that's the true values of the outputs. And then we'll also get the predicted values. So yeah, we want to get the levels and the predicted values. Now let's start with the levels. So yeah, we're going to have levels. We have this list, there we go. And then for x, y, or let's say for x, y in the test data set, we're going to take this as numpy iterator. So for x, y in this, we have the x and the y, we have that. And then we append every output to these levels. So yeah, we have levels.append y and we run that. So that's fine. We can now print out levels. What do we get? There we go. You see, we have this levels. Now let's convert it into a simpler form where we just have only this values in here and not this arrays, which we have here. So in order to get that, we'll have levels will be equal levels or rather will be equal array. And in here, we're going to create this list. So in this list here, for every element of this levels list. So for here, for i in levels, we're going to do i the zeroth index. So we're going to take always this elements right here. So that said, we have that and now we have levels. Let's print out the levels after doing this transformation. There we go. Here's our levels now. So we have that fine, take that off. We have this error actually, because we tried, we've converted these levels already and then we're trying to reconvert them. So let's comment this right here. We comment that and run this and we have all levels. Okay, now we have this levels, we could move ahead and get the predicted values. So to get this predicted values, we have our lunette model, lunette model. There we go, dot predict. What we're going to pass in this predict method is our input. So we have the input and then right here, we add this input list and then we do same as we did with the levels. So we have input, dot append X, which is this input here. Now we run this and then let's add a cell. So we add this code cell and then we're going to preprocess our input. So from this, we're just going to print out the input to see what it gives us or what kind of shape we're having right here. Let's put nonpy array while that's loading, shape. Then we run this other cell right here. Here's what we get as shape. So we have to take this off, this one off here. And to do that, you see we would have the input. We'll select this first index. So we take that and then for this next, we take only one. So we're going to chop this off and then we have the rest. So that's it. Now we run this again and we should have put a shape here. So we run that again and there we go. So this is what we expect. Now we've gotten the levels and then we are now ready to get the predictions. So here we have this input, take this and then we pass it in here. We pass our input and we get our predictions. Let's print out the predicted here. Oh yeah, let's print out a predicted and see what we get. So what we get right here, we could print out a shape to better understand what this is. And as you could see, we have this. So how do we take this over? How do we make it look like the levels? We could simply take this off. So we have, we select the first and then for this one, we take it off. So we run that again and there we go. So now we could do this, take out the shape and then it should look like the levels we've had here and that's it. So now we've gotten this. Our next step is to use our confusion metrics from scikit and metrics and use our confusion metrics. Now let's comment this sections first. So here we've passed in the levels, we've passed in the predicted and then we're specifying the threshold. So what we're saying here is all values greater than the threshold considered uninfected and all values less than or equal to threshold is gonna be considered as contained in the parasite. And then we're gonna have this in this confusion metrics here. So we run that and then as you could see, we have our confusion metrics, which shows us the number of true positives, true negatives, false positives and false negatives. Getting back to our model evaluation, we'll see exactly which of those values are the true positives, false positives, true negatives or false negatives. So let's just copy this out here. We take this up and we shall see that below. So here we have this confusion metrics, which we've just seen. Here is it right here, confusion metrics right here. And then let's place this out here. So we have the number of true positives. This one matches. And then number of true negatives, one, two, nine, eight. Yeah, this is closer. So what Psychelearn gives us is quite similar to what we had with TensorFlow, but they're not exactly the same. So this tells us that this is a number of true negatives. This is the number of, no, this is a number of true negatives right here. So here we have true negatives. Here we have true positives. Then here we have false positives. And then here we have false negatives. We'll then see how modifying the threshold will either reduce the false negatives or reduce the false positives. So let's have say 0.25 right here. We run that again and there we go. Let's print this. So we see that we now have a number of false negatives reduced when we reduce the threshold. We now go back and then take this to 0.75. We run that again. You see that this is now reduced. So here we have an increased number of false negatives and a reduced number of false positives when the threshold is increased to 0.75. So that's it. Let's now print out or rather let's now plot out this more elegantly looking figure with the CBUN library. So here we go. We run that and that's it. So here we have same confusion metrics but more elegantly plotted. Let's take back to 0.5 here and have that. Okay, so that's it. Now you notice that when we're trying to say reduce the more false negatives or the more false positives, what we're doing is we're just picking up some values here like we could pick up 0.2 and then we see its effect and we could do the same for 0.25. You see how this takes us to 58 and this to 130. We could continue doing this until we get our best results. But then this isn't very efficient since we're just trying out different values. Now the way we could do this more efficiently is by working with ROC plots where we'll be able to choose a threshold in a more efficient manner using the plots. And so we'll be able to reduce the number of false negatives or number of false positives without having to try out all these threshold values manually. The very first thing we'll start with will be to import ROC curve. So we have ROC curve and this is part of scikitlearn metrics. So we'll run this again and then just right here we'll make use of our method. We're going to output a number of false positives through positives and then thresholds which we'll use in coming up with the ROC plot. So we have that, we have ROC curve, ROC curve and then this takes in the levels and then the predicted. So we have this predicted value. Now if you print this length out, the length of FB, length of TP and length of thresholds, so we have exactly the same number. Thresholds here, we should have exactly the same number. Here we have 330. Okay, so now we have this. Now note that the reason why we need this is because when coming up with our ROC plot, like say we have this ROC plot, what I have for each and every point, the corresponding TP, FP and then the threshold which will lead to that TP, FP pair. So that said, we are going to make use of this data now and then plot out the ROC curve. So let's get straight into that. We have our plot and then we do plotting. We pass in the false positives through positives just like XY. So here we have X and then here is Y. Let's get back, that's fine. And then from here we have the levels. So we have X level, which is our false positive. And then the Y level, our true positive rate. So we've seen this already. Now we have this, we could include the grid. So we have this grid and then we show. So that's it. We run that and here is what we get. So this is our ROC plot right here based on this FP and TP we got from here. Now, how do we include the thresholds? In order to include the thresholds, we are going to make use of my plot leaps test method. So we have plot the text and then here we're going to have the TP or rather the FP TP. And then what if the actual text will be put in here will be the thresholds. So we'll be passing in the thresholds. But now note that we actually have to do this for each and every point, which isn't possible since there will be too many texts put out here and it's going to be chucked up. So what we could do is we could skip some values. So we'll say for I in range, we're going to start from zero right up to the length. That's actually going to be 330, length of thresholds. And then we're going to be skipping some values. So skip and then we'll define the skip. So let's start with a skip of 20. So initially we're going to skip 20 values. Then once we skip this value, let's pass this in here. Once we skip this value, we pick in a given I, pick in that given I, same here. And then we do the same with the thresholds. So we get the corresponding false positive rate, corresponding true positive rate and then the corresponding threshold. Now that's done, we could run this. So here's what we get. We see this plot. Now we could see the ROC plot with the different thresholds. I think we could let it like this. It's fine. Let's create a size. And then we try to focus just on this portion, which actually matters the most because we wouldn't want to get into these regions because in these regions our false positive rate is going to be too high. And these regions below this would have a very small true positive rate. So generally we'll try to focus on this zone right here. Now, depending on the problem you are trying to solve, if your false positive rate is what matters the most, that is if you cannot afford to have a high false positive rate, then you would tend to pick values. Let's say, let's break it out like this. So here is like the meat point. We have some sort of meat here. Let's draw this line. Okay. So we have some sort of meat point here. So this is like 0.5, 0.46, 0.62 and all of that. So we're breaking it up like this. And then if you want to ensure that you try to minimize as much as possible your false positive rate without reducing your true positive rates too much, then you tend to take values around this. But if you want to make sure a true positive rate remains quite high, even at the detriment of the false positive rate, then you would tend to pick out values in this zone. So you have these two zones to pick your threshold from this is the main zone. And then you have this zone right here, this other zone and this other zone. So you have zone one and zone two from which you have to pick from. One other quick note is that if you have a problem like the one we're trying to solve where parasite is zero and then uninfected is one. So this is how the data set was created. And based on this we build our model. Then this means that this will be considered as negative samples while this will be considered as positive samples. Whereas in the real world, we'll tend to look at uninfected as negative and parasitic as positive. So you have to be very careful with these terms and know exactly how your data and models have been built. And that's why in our case where we're trying to avoid situations where our model predicts a fake uninfected output, that is a patient who actually has a parasite but the model predicts that's uninfected. There's actually a fake uninfected. This is a fake or false positive in our case. So we'll tend to minimize this number of false positives. Now, if your model was built such that parasitic is one and then uninfected is zeros, then it's clear that you try to instead minimize the number of false negatives since uninfected is considered as negative. That said, coming back to our problem, since our data set was constructed in this way, we try to minimize the number of false positives at our cost. But while doing this, we have to ensure that the true positive rate remains at the reasonable position. And so we could pick out a threshold of like 0.6265 given right here. Getting back to this, let's take 0.6265, we run that. We have here a number of false positives to be 87, which is gonna be smaller than when we're having a threshold of say 0.5. Run that again. You see, 87 is gonna be smaller than this 99, the value of 99 we're getting now. And so that's it for this section on metrics. Thank you for following up up to this point and see you next time. What's up everyone and welcome to this new section in which we'll build callbacks with TensorFlow. In this section, we'll look at how to build a callback from scratch by inheriting from the callback class. And then we'll build other callbacks made available by TensorFlow like the CSV logger, early stopping, learning rate scheduler, model checkpointing, and finally reduce learning rate on plateau callback. Don't forget to subscribe and hit that notification button so you never miss amazing content like this. Callbacks are methods we call during training, evaluating our prediction. This callbacks can permit us extract useful information from those processes we just listed or even carry out changes on those processes. Here in this TensorFlow documentation, we have TF Keras and then we have the callbacks. So this is what we have for the documentation. We could go through each and every one of this, but for the sake of this course, we are gonna look at the key ones. So right here, we have this callback, but before getting to this, we are gonna look at the history. The reason why we're looking at history first is actually because we've used it already. So we use this callback without really knowing that we're using callbacks. So yeah, we told this is a callback that records events into a history object. So remember the times we were having this year. So after training, that's when we're training, we start some information in this history. And then after training, we're able to come up with plots like this because we had this information start in this history right here. So that said, as you could see in this example, which is kind of like similar to what we've been seeing so far, we have this history and it collects all values we've been starting during training. And then we could print out this params, history.params. We get the params, and then we could also get the keys. So that's how this works. We've seen this already. And then we could look at this callback class right here. Now, the reason why this is like the most important of all this different callback classes is because this is kind of like the modern class, all that abstract based class, which can be used in building new callbacks. So if you want to build a callback, which isn't listed here, you could always get back to this and then build that callback from scratch. So here we have this callback class, which has attributes as you could see, params and model. And then it also has this methods, which we'll see how to implement very easily. If you look at this on batch begin and this on batch end, you'll see that the take similar arguments like here we have the batch, and then we have this logs. Then for the next, the epoch begin and the epoch end, they're actually taking the epoch and the logs. That said, we'll import the callback. So here we have from tensorflow, Keras, callbacks, we're going to import callback. This class, which we're going to be using and creating our callbacks, we run that. Then we define this callback class, which we'll note as call last callback. We have this last callback class, which inherits from the callback, which we've just imported. And then we will make use of the different methods which have been given to us in the documentation. Let's start with the on epoch end. So on epoch end, we're taking the epoch. And then what we're going to do is we are going to print out the loss values at the end of an epoch. So what we'll be doing is kind of similar to what we have already here. So let's do that and have print epoch number, let's say epoch number this, and then has a loss of for epoch number this, for epoch number this, our method, the model has a loss of this, and then we just format. So there we go, we pass in the epoch. Let's have this here and the locks. So just as in the documentation, where we add in the epoch and the locks. Now we are going to pass in the epoch and then the locks, but since we want to get just the loss, we could get the loss from this. So we have this dictionary here and we'll pick out just the loss. Now let's run this and then we'll see how to include a callback in the training process. So just right here, we're going to have callbacks. So the callbacks argument and we have this list. So in this list, we're going to insert this callback we've just created here. So we have the last callback, we should just pass in here and that's fine. So now let's rerun this, let's take this for say three epochs and then see what we get. We get in this error, we click search stack overflow and then click on this. You'd see that we have a solution just here. And what it said here is, what you're going to pass is the object and not the class itself. So that said, instead of passing the loss callback, as we just did, we're going to pass on this object. So we're going to put in the brackets. Let's modify those prints here and pass in a space. So we're going to have that to the next line. And then we rerun that again. So let's run that and that's fine. Then after training for all our three epochs, here's what we get. We see that unlike before where we just have this output, now we have this message output that is for epoch number zero, the model has a loss of this for epoch number one and then for epoch number two. Now what we could do is we could add this. So we have this formatted normally. So we don't start from zero, but it ends up from one. So we could say plus one right here. So that's it. Now another thing we could do is we could have on batch end. So on batch end, we have that and then we have the batch. We pass in the locks. This time around what we want to do is kind of similar to what we've seen already here. So we just print this out. That's fine. Take that off. So for the batch number, for this batch number, the model has a loss of this. So there we take in the batch. So that's it. And then we log out the loss. Now, yeah, let's just pick up this locks totally. So we have the locks and then that's fine. So yeah, we've put in the batch unlike here we'll pass in the epochs. We run this again and run this on this. You'll notice that this time around we have much more information, which has been locked out. Let's take this year and get right to the top. What we have here is for batch number one, the model has lost. So you see that this is after each and every batch. So here the first batch, we have this log and then the next batch we have this log and so on and so forth. And since our batch size equal to 32, this simply means after working on 32 different data points, we're going to have this locked out. And then the next 32, we have this locked out right up to the end. Now for the epochs, you'll notice that as we go on, so we're moving on to, let's get right up to 689. Oh, we have that and finally here we have 689. Okay, so for this last year, that's at the end of the epoch, we now have this locked out. So here we have for epoch, whereas previously we had for batch for all the different batches. In case you want to understand how we got this 689 right here, you could take the total data set size and then divide by the current batch size, which is 32. You should get this 689, which is given right here. So from this, we can look at the CSV logger. Now with a CSV logger, what we're actually doing is we are logging out this information in a CSV file or in some file, which we're going to define. So what we'll do is just simply copy this. Now this time around, we wouldn't actually have to recreate a class as we just did with the callbacks because there's kind of like a general way of creating these callbacks. Now with the CSV logger, we'll actually create the callback more easily. So here we have this and then we're going to specify the file name. So yeah, let's have the CSV callback equal that. Let's take this off and we specify our file name. So yeah, we just have file locks.csv and that should be fine. So we take that off. Take that off and then up right here, we have to import CSV logger. That's fine. We run that, we think it's okay. We get back to our callback and then we run this cell right here. Now note that this append is, as described in the documentation, a Boolean which tells us whether the locks we are currently putting in the CSV file or in the file in general are going to be appended on previously locked content or not. So when we have this as false, we're supposing that this is empty and so we are going to be putting information in this file for the very first time. So we've run this and then now all we need to do to take this new callback into consideration is to have your CSV callback. So let's make sure, let's take this batch locks out from here. So we are not going to take this last callback into consideration any longer. We run this. After three epochs, we're going to open this up and then we have this locks of CSV file which has been created. So let's have that. And as you could see, we have this CSV file which we could now download and then view it later. So here we have the accuracy, AUC and all the other metrics and lost values which we want to store. So that's fine. Next thing we could do is we could get back again here and then I'll select append true. So if we've done training the first time I want to redo the training process, we don't want to erase all the values we had previously. So here we set this append true around this again. After three epochs, we open this lock.csv file and what do you have here? You see, we have this new information which has just been appended on the previous information. From this CSV logger, we could now move on to this early stopping callback right here. To better understand early stopping, let's get back to the plots which we had previously. So let's take out this plot of the model's accuracy where we see how the train accuracy keeps increasing while after a certain point, let's see this point here. Let's take this off. After say this point, our models, even this point here, our model's validation accuracy doesn't increase any further. So what we have in here is something like this. We have this train accuracy which increases and goes towards one and then the validation accuracy which is something like this. Now in some other cases, you would even have situations where this starts to drop. Nonetheless, in this case, we have this plot where it just kind of like stabilizes and doesn't increase any further. Now note that this kind of situation is known as overfeeding. In overfeeding, the model starts to overfeed the training data. So because the model has been trained on the training data and not on the validation data, at certain points, the model stops or ceases to generalize because the aim of this training process is not to come up with a model which only performs well on the training data. We're trying to come up with a model which performs well on any type of data, be it a train, be it a validation or the test data. So if we're able to have a model which does the same or which has the same performance with a train and with a validation, with a test, then that model is an ideal one. But in this case, we see that as we keep on training, the model's parameters have been modified to suit only the training data. And this is very dangerous because at a certain point, you may feel like because you're having high training accuracy, your model's performing well. Whereas this isn't the case because when your model will be shown new data, like the validation in this case, and the test later on, this model wouldn't perform as well as it will do, or as well as it's doing with the training data. So to avoid this kind of false measurements, we tend to stop the training once this overfeeding status will occur. So this means that if you're training and then your validation, let's take, let's suppose we're stopping at this here. So we're training and then your validation data or your validation accuracy seems to be constant, whereas that of the training seems to kind of increase, then it's better for you to stop training at this point. Because after this point, the model parameters are just being modified to suit the training data and it doesn't really generalize, which is the case here because we're trying to extract some information from this data and make the model intelligent. So the model doesn't become intelligent by only modifying its weights or parameters based on the data it's been trained upon. It's intelligent because after being trained, it can perform well on data it has never, ever seen. So here we have this early stopping where after we notice that the validation accuracy doesn't seem to increase any further, we just kind of like stop the training and then use the model parameters from this number of epochs. So we could see that after say 12 epochs, we just stop the training. Now let's take this off and then do replicate something similar for the loss. So if we're having a loss, we could have something like this and then that. So yeah, we have the number of epochs and then we have the loss. So you could have a situation where you're having your training data, your training loss which keeps reducing whereas for the validation you see you would have something like this. So this is a typical plot for over feeding. Nonetheless in our case, the model over feeds but not that much. In some cases you will have a situation where this even starts to drop and where the loss starts to increase after a certain point. Here is the validation loss and then here is the training loss. Obviously the training loss will always keep reducing because we're training on this training data. So what we're saying is at this point where the validation stops reducing, it's important to just stop this training and this is known as early stopping. Now recall that the aim of callbacks is to be able to modify the training process, the evaluation process or the test process as a prediction process in an automatic manner. So that said, we shall make use of this early stopping callback right here which will permit us stop training automatically once we notice that a given parameter like say the loss, the validation loss doesn't drop any longer. So here we're just gonna copy this and then we just apply it similarly to while we are done with the CSV callback. So here we add this text and then we add that code, we just paste this out. Now we define this AS callback, that's early stopping callback and then we look at the significance of each and every one of these arguments. Now coming back to documentation, we have this monitor, quantity to be monitored. So by default, here we have this valid loss. This means that this callback will simply check on this validation loss right here and then once it stops reducing like see at this point, we're gonna stop the training. Whereas if we change this to say validation precision or validation accuracy, then what we'll be monitoring will be that accuracy or the precision value. So if you have them like this, we'll see we'll stop on this right here to ensure that we don't go and over feed on the training data. The next argument is this mean delta argument right here. So with the mean delta argument, we are defining a minimum change below which any change is considered as no improvement. So if we have a loss like this and that our mean delta is say a value of 0.1, then even if this loss reduces by a value of 0.5, then this callback will consider that there has been no decrease in the loss because the mean delta is 0.1. Now by default, the mean delta is set to zero. This means that any slight change is considered as a drop. So if we have even 0.0005, then we consider this as a drop in the loss. Now this is important because this has a patience, this callback makes use of this patience. With this patience, we are defining the number of epochs above which if we don't have a decrease in the validation loss, like in this case of the validation loss, we consider that we could stop that training process. And for the accuracy is the number of epochs above which if we don't have an increase in the validation accuracy, if we've picked validation accuracy for the monitor, then we'll have to stop the training. So we define this, well, we predefined this so that this could run automatically. Now for the mode, by default, we have the auto mode, but we could specify mean or max. Notice that when speaking about the loss, we spoke of a value or the number of epochs above which if the loss doesn't decrease. So here we're supposing that the loss is meant to be decreasing. And in that case, we are having a mode of mean. Now for the accuracy, we spoke of, for the patience, number of epochs above which if the accuracy doesn't increase. So in this case, we're having the max. Now, what TensorFlow permits us to do is to use an auto. And with this auto, TensorFlow automatically infers whether it's dealing with a mean or max. So this means that if you place in, for example, a valve position, this auto should be able to understand that a position should be increasing. And so it's going to use a max. Then we move to the baseline where the training stops if the model doesn't show improvement over the baseline. And finally, we have this restore best weights. With the restore best weights, which by default is false, we are simply saying that the model is going to take up its final state. So this means that if we start monitoring the model, say at this point here, or at this point where we have the lowest possible valve loss, and then the model, let's say we have a patience of five. So we are going to train for four to five epochs before stopping. So if after five epochs, we are on this, let's suppose that the feed epoch, we are on this. So we've added from here plus five epochs, we are on this. And then we are having this loss. It's clear that this model with this loss is less performant than this one. Now, if this restore best weights is set to false, then we'll just take the model's weights here, or the model weights which give this loss value here. Whereas if it's set to true, then it means we're going to take the best weights we've had throughout the training process and which happens to be the weights which provide this loss right here. That said, here we go. We have this, we have, let's say the patience to three, verbosity one, mode auto, baseline known, restore best weights false. So let's run that, and then all we need to do is to just include this year. So here we're going to have ES callback. Now we could take off the CSV callback, but you could always put all this together. Let's just let it, so we could see how all that works. So we just have this list right here, and we have this ES callback with the CSV callback. Now we run the training. We didn't train this for long enough to be able to observe any callback changes. So let's take this to 10 epochs, and then we reduce this to one or two. Let's take this to two. We run that again, so that's fine. And then we fit our model. After training for eight epochs, we see clearly here how the early stop in callback stops the training process. Now let's understand why this is, all this training process has been stopped. If you take a look at this validation loss right here, you would find that there was a drop here, drop increase, but after this increase, there was a drop. And since the patience is equal to, that's the patience we had defined here equal to, we have to get two successive increases or two successive same loss values before the training process could be stopped. So since we, after this, we have a drop, the training process continues, then we have this increase, and then we have this drop, so it continues. Then here we have this increase, and then here again, we have this other increase. So because now we have had this two successive increases, as we could see in the plot right here, you see right here, where we have this increase, drop, increase, and then increase. We now have the training process, which has been stopped, as we could see here. That said, we now move on to the learning rate scheduling. Up to this point, we've been used to training our models with one fixed learning rate throughout the process. So we could fix our learning rate as we did to say 0.01, and we use this same learning rate throughout our whole training process. But it happens that if this learning rate is too large, then we reach diverging, and if the learning rate is too small, it would take too long for our model to converge. So let's consider this plot right here. It's actually a very simplified plot, as actually what really happens is way more complex than this. So let's consider this plot, and we have this, and here we have the loss. Recall our aim and the weights. So here we have loss and then weights or parameters. And our aim is actually to modify this weights such that the loss is minimized. So our aim is to get to this position right here. Now we start with a case where we have a high learning rate. So if we were dealing with a high learning rate, then it would be easier for our model to find its way to this minimum position right here, to some position close to this minimum position, let's say at this point here. But the problem here is once it gets to this position, it could also very easily diverge from it. So it could very easily get back to another point around here, and then repeat this kind of process again where the model just kind of diverges. So you could have something like this, it could come down here and then get back to some point around this and so on and so forth. So we may have this case where the model doesn't really converge because the learning rate is too high. And then for the small learning rates, we may start training and then if say the model or if we find ourselves at this point here, that is we've modified the weights so that the loss value happens to be at this point here, it becomes difficult for us to get to this ultimate or global minima. Since the learning rate is too small and it changes, we're making very small changes. So we may find ourselves just staying around this local minima here instead of going towards this global minima, which is this. And so to bring in a balance, what we could do is when we start the training process, we could use a relatively high learning rate so that the model kind of approaches this global minima faster. And then after a certain number of epochs, we start or we modify the learning rate so that it becomes or it takes in very small values. And since it now takes in small values, we now start taking up very small changes such that we can get towards this global minima now without risking divergence. Now, one way of doing this is by say you could fix, let's say you could suppose that for the first 10 epochs, you train your model at a learning rate of say 0.1. And then from 10 to 20, you're gonna train the model a learner of say 0.01. Let's say we're dividing by a factor of 10. So we now move to 0.01 and the next 10 again, let's say we're training all this for 30 epochs and then the next we go to 0.001. So you could train your model and then after 10 epochs, you restart the training by modifying the learning rate in your optimizer and then again, you do this same year. But now the problem with this is you always have to be there to ensure that after the training, as after the 10 epochs, you modify this manually. Now, what if we're able to do this automatically? As usual, this is made possible by TensorFlow callbacks. With this callback, that's the learning rate shadow log callback, we could define a function which takes in the number of epochs and then modifies the learning rate based on the current epoch, based on a mixture of the current epoch and some predefined function. So as you could see here, we have the learning rate shadow log, it takes in a meta shadow and then we could specify the verbosity. Now, this is an example of this shadow log method which has been defined here. Here what it do is if the number of epochs is less than 10, then you're gonna use this predefined learning rate. And then in the case where the number of epochs is greater than or equal 10, then we start to reduce this learning rate in an exponential manner. So there we go, let's take this off. Now, what we're saying here is we're modifying the learning rate such that after that, we have in before 10 epochs, we have in this fixed learning rate. That's it. And then after this, we start to reduce this. So the learning rate starts dropping as we continue with the training. So now we don't really need to be monitoring the training manually because this callback will automatically modify the learning rate for you. We could simply copy out this example which has been given to us right here and then make use of it in our training process. So here we have to include this text and then our code. So here we paste out the scheduler. Let's do this. And then for this one, we have our learning rate scheduling callback. So learning rate scheduler. And then we do this import. So here again, we just have learning rate scheduler. We run that and then get back to this position right here. Let's do this. One, two, three. So that's fine. So notice that here we have this order. So we have the callbacks, learning rate scheduler. And we have CSV logger, LSTopping, learning rate scheduler under the callbacks. So that's fine. Let's get back to that. And then we are going to define. So here we've had the learning rate scheduler or this scheduler method, but we're yet to define our learning rate scheduler callback. So we have our, let's say scheduler. Scheduler callback equals learning rate scheduler. And then it takes in this scheduler method right here. So that's fine. Notice how the scheduler method takes in the current epoch and the current epoch number and the learning rate. So here we have this given to us already. And so we'll modify this to take, say for example, three. So after three epochs, we're going to modify the learning rate and then we could bring out the learning rate. So we could say, for example, the current learning, actually what we could do is let's take this off. Let's not have that. Let's specify the verbosity to be equal to one. So here we have the verbosity specified as one. And then we'll take off this. So you could also check out the CSV logger, the locked CSV file. So you'll notice how through our login these values since we just adding up or stacking up the values on the previous values we've had already. So for now, what we could do is take this off. Now let's take this off all this and then focus on just the scheduler. So we have scheduler callback and that's fine. Let's ensure that the cells have been around already. So that should be okay. Now we have in this arrow, verbosity, unexpected keyword arguments. Let's come back, get back to this year. And then we have always verbose. So let's modify that as verbose equal one. We run that again. That should be fine now. We could now get back to our training. So we're expecting that below three epochs, let's say equal three epochs. We have a given learning rate and then above that we have a learning rate which decreases exponentially. So that's it. You see here that as the training process starts, we did not really need to print out any value because when we set our verbose to one, it outputs this. So here we have learning rate, scheduler setting the learning rate to 0.00999. That's practically 0.01, which is what we had given here in this learning rate. So it's practically this learning rate setting. And then as time goes on, it's going to decrease its value and then always output the current learning rate. So as we carry on the straining process, recall that the aim of having to work with these kinds of learning rate schedulers is that we want to actually get the best of both worlds. So what we want is speed because a very slow or a very small learning rate doesn't actually speed. And so we want speed. And then we also want stability when training. So because we want this tool, we are going to use or modify our learning rates such that we always get this through our training process. And so that's why whenever we started our training, we have high learning rates, which could ensure speed. And then as soon as we have trained for a given period of time or for a given number of epochs and that we're trying to approach this global minima right here, we now seek for stability by reducing the learning rate such that we don't get to this point and have to diverge. So if you want a more stable kind of training process, what we'll do is reduce this learning rate. After the training is complete, here's what we get. You could see that we have this learning rate, which is now modified after a given number of epochs. So here you see how the learning rate starts decreasing as we go on with the training process. And so that's how we implement learning rate scheduling. But before moving on, let's check out this tutorial provided by the MXNet project developers, which talks of other different learning rate scheduling techniques. So here we have this learning rate scheduling with warmup as a slanted triangular. And as you could see, the learning rate actually falls between these two values here. So we have a learning rate between one and two. So you could always define your max learning rate and then your mean learning rate. So you could always have this. And then with the warmup, what we'll do is we are going to increase this learning rate linearly up to the max for a given number of epochs. And then once we get to this number of epochs, we now start decreasing this learning rate. So here the decrease is linear. So it's a linear function we're using. And then once it gets to the minimum learning rate, we just maintain that constant value right up to the end. So another learning rate scheduling technique we could use here is we have this linear increase. That's the warmup. And then we decrease this exponentially right up to this minimum value. Now this technique of warmup was developed in this paper by Priya Goyal et al. And they found that having a smooth linear warmup in the learning rate at the start of the training improved the stability of the optimizer and led to better solutions. Then from here, we move on to the cosine aligning scheduling method, where there is this smooth decrease in the linear rate, which is kind of resembling the cosine function. Now this is what a cosine function actually looks like. So here we have this and that. So here is our cosine function. And then if you notice, you'll find that this portion actually looks similar to what we have here. And now after we get to a given number of epochs, we just maintain that. So we have defined the mean and then the max. And then once we get to the mean, we just maintain that mean throughout. Then we also have this stepwise decay scheduling. Let's take this off. Here you see how we start with a warmup. So this warmup is kind of like a method used very much in practice. And then from here we have this stepwise reductions. So we go in for this fixed learning rate after a given number of epochs, we now drop the learning rate and then we drop it and so on and so forth. So here's our stepwise scheduling with warmup. Then from here, we have this cool down where we follow the stepwise method. And then after a given number of epochs, we now reduce the learning rate linearly and the term cool down. We also have this one cycle scheduling technique proposed by Leslie and Smith and Nicolet Topin. And here's what it looks like. You see that we increase that we have this warmup and then this linear decrease. Once we get to this initial position, we also have again this linear decrease with a different slope, with a smaller slope. And then once we get to this minimum or this final minimum, we now just actually just maintain the learning rate. Now finally, we have the cyclical scheduling methods originally proposed by Leslie and Smith. The idea of cyclical increasing or cyclically increasing and decreasing the learning rate has been shown to give faster convergence and more optimal solutions. So as you can see here, we are having the learning rate which is going to be bouncing from the minimum to the maximum, as you can see here. So we have this linear increase to the max and then linear drop, linear increase and your drop and so on and so forth. And then finally, we have this cyclical cosine aniline scheduling right here. We see how we have the cosine aniline and then we have this full cyclical process as we go from the highest to the lowest. And then this next cycle from the meat between the mean and the max, that's this right to the mean learning rate. And then we take up this final cycle from this quarter of the total. So we had 0.5, which is a quarter of two. So we start from this and then right to the mean. Now notice also that as we go into the cycle, the cycle lengths keep increasing. So we move from this to this length and finally this length. Based on the scheduler you want to implement, all you need to do is to modify this scheduler method right here. The next callback we'll be looking at is that of model checkpointing. In this model checkpoint callback, we are able to save the model's weights at some frequency. So unlike previously where after training, like with this after training our model, we get to load or rather we get to save the model like here. So here we save the model, we save our weights. After we're done with the training here, we could be doing this weight saving during the training process. And thanks to this model checkpoint callback, now as usual, we just copy this and then we could check out on this documentation for the significance of each and every one of those arguments. Here we have the file path. So this is the file where we are going to be saving our model. So here we have our checkpoint callback and that's it. We have model checkpoint here. We're going to define this file path. And then what are we going to be monitoring? We're going to be monitoring the validation loss, which is what's given to us by default. Just like with the early stopin, where we had this monitor right here, we are going to look at our validation loss. And supposing we have this validation loss and then our training loss. So we're going to look at our validation loss, which is this, and then save the model weights when we have this mini model or the smallest validation loss right here. And then if you set the save best only to true, then we'll save only this weights, whereas when it's false, we'll save this and also save the latest weights. And then for this argument save weights only, if it's set to true, then we're going to save only the weights, whereas if it's set to false, then we're going to be saving a model. That's a full model with its parameters. Then we have the mode, which is automatic, like since it's validation loss, since it's a loss automatically, this would be a mean. Whereas if this is an accuracy or precision, we're going to have a max right here. So let's take this back to auto and then we have validation loss. Now the save frequency when set to epoch simply means we're going to do the saving after each and every epoch, but we could modify this to say three. So after three epochs, we're going to verify if our current model is the best of all the previous models, that's if we've set this to true. So if we've set this to true, we're going to verify if this model, the current model is the best, if it's the best, then we override this file path, that's override the file, the weight file we've saved. And if it's not the best, we continue training. So that said, let's modify our file path and then let's call this check points. That's it. So we have the checkpoints and then we run this. So we have that. And then we add this callback right here. So we have the checkpoint, checkpoint callback. That's fine. We run the training now and see what we get. After training for over 10 epochs, we have this two logs, which you could notice here, this one where we've been told assets reaching to checkpoints assets. So you could open up this checkpoints here. And then you'll see that we actually saving this model. And then we also have this log here, that's after the seed epoch. Now let's understand why this login is done after the first and also after the seed epoch. Just when training starts, we have a validation loss of 0.47. And so we're supposing that since this is the first then, the model sets its best state. Then it moves to one, which is greater than this. So the model state or the best state is this one after the first epoch. This, the model, the first epoch or the model state of the first epoch is to the best. This doesn't beat the record. This doesn't. And then you'd see that here we have this validation loss of 0.24. That is why the model is now saved. From this model checkpoint and section, we'll look at the reduced learning rate on plateau. And then later on, we'll have a dedicated section for tensor board. The way this learning rate or rather reduced learning rate on plateau works is that, if you start training with a fixed learning rate, and then say you've trained for over 100 epochs, and that the model performance after 110 epochs, so let's say we have 110 right here. So after this 110 epochs, our model performance doesn't improve. So we still fix our learning rate. But since for the past 10 epochs, we haven't had an improvement in the model's performance, what we'll do is we'll reduce the model's learning rate by a given factor. So here is the factor. The patience, our patience here is 10 million that we're gonna wait for 10 epochs before deciding whether we're gonna reduce the learning rate or not. The verbosity as usual, the quantity to be monitored, in this case, the default is the validation loss, the mode automatic. So if you want it to be mean, you could specify mean or max. But setting it to automatic allows TensorFlow to automatically decide whether it's a mean or max. In the case of validation loss, obviously it's gonna be a mean since our aim is to reduce the validation loss as much as we can. Then we have the mean delta, which takes in a default value of 0.0001. And this means that if after say 10 epochs, we have a change in the validation loss, but this change is less than this mean delta, then we'll consider that that isn't a change. And hence we are gonna still reduce the learning rate by this factor right here. We also have the cool down and then the minimum learning rate below which we wouldn't wanna drop our learning rate. As described here, this cool down is a number of epochs to wait before you resuming normal operation after the learning rate has been reduced. We are gonna create this callback very easily. And this time around, we're gonna monitor the validation accuracy. So if the validation accuracy doesn't increase after two epochs, we are gonna reduce the learning rate by a factor of 0.1. Meaning that if we had a learning rate of say 0.01 and that this validation accuracy hasn't increased after the two epochs, we are gonna reduce this such that this becomes 0.01 times 0.1, which is this factor right here. Just as before, we are gonna add this plateau callback and then train our model. After training for five epochs, we can observe that we have this learning rate which has been reduced after the third epoch. And this is because after these two epochs, there is no improvement in this initial validation accuracy. And so that's why the accuracy is gonna drop from 0.01 to 0.001. Now that's it for this section. Thank you for getting up to this point and see you next time. Hello everyone and welcome to this new section in which we're gonna look at different strategies of combating overfitting and underfitting. This strategies will include data augmentation as you could see here, dropout, regularization, early stopping, smaller network usage, hyperparameter tuning and normalization. So far in this class, we've mentioned the terms overfitting and underfitting without really getting in depth to what this two actually mean. Now for overfitting, we'll consider this plot of the loss versus the number of epochs and then precision versus number of epochs. With a loss versus number of epochs, what we'll generally have when a model overfitting something like this. So here we have the training, the validation loss and that here we have the training loss. So this is just like a general way of looking at this though this may take different forms. Now that said, as you could see, we have this two sets and the loss versus number of epochs plots. Clearly we could see that the validation and the training set initially start up with a similar pattern. Sometimes you may even have the validation which comes up like this. So you can even have a situation like this where the validation performs even better than the training set initially. But generally when a model overfeits, what we'll have will be this kind of model where at some point the model keeps doing well at the level of the training and starts doing very poorly in the validation set. We could replicate this with a precision epoch. This could be precision accuracy recall or some other metric which we've chosen. And what we could have will be something like this. Obviously you saw in the practice, it isn't always like this, but something in this sense. So you would have this here, you see you have the training and then you have the validation right here. So what goes on is your model keeps on doing very well for the training and then with a validation at some point it starts doing even a very, very poorly. And so the danger with this is if you are considering working only with a training set, you may feel like you're more epochs or training over more epoch is a good idea because you keep having this great results. Like supposing we have fixed this year and that your training is like say 99%. So you have 99% precision score on your training data and you're like, wow, this model will do very well in the real world. But this isn't generally the case because this model actually has been overfitted on your training data. Your model has learned instead to modify its weights based on the training data instead of being able to extract useful information or some intelligence from the data which has been used in training this model. The main cause of overfitting is having a small data set and a large and complex model which contains so many parameters. Now, if a model like say deep neural network has many parameters and you're giving it a small data set, then obviously it's gonna adjust its parameters such that on this small data set, it performs exceptionally well. So you may come across a model which has say 99.9% precision or accuracy just because the data you've used to train the model has or was very small. Now, in another case, you may have say a moderate sized data set and then not just this kind of model but a very large model. So at the end of the day, we notice that there has to always be a balance between this data set and the model size. So this means that even if you increase the data size and then you also increase the model size, your model may still risk overfitting. To better understand this concept of overfitting, let's take this simple example. Supposing that you have three subjects to master a school, let's say math, English and sports. But when kids get to school, they are only taught mathematics or they only taught English or only taught sports. Let's choose for example, mathematics. So these kids get to school and from the first year to the last year, they taught only mathematics. It's clear that when you evaluate this kid or when you pick a kid at random and evaluate that kid in mathematics, the kid would tend to have a better or an above average result in mathematics compared to kids from other schools. But when tested on subjects like English and sports, that may not be the case. And this is because the English and sports weren't taught at school. And so if you are evaluated on only what you were taught, you would tend to have very high scores like this. And then what if we started to evaluate on stuff the kids were not taught? You'll notice that those kids will then start having poorer results because those kids haven't taken out some time to master English and sports. And so you may end up with a kid who was able to show for example, that sine squared x plus cos squared x equal one, but can't tell you the past tense of it. Now it's clear that to readjust this situation, the kids have to be taught all the subjects such that there is a balance that those kids need. And obviously this balance will come in a way that the kids now perform better in these two subjects. And because they have had part of the time they used to study maths allocated for English and sports, they may perform slightly less than before in maths, but at least what's important to notice this time around, they can now express themselves better in English or practice some sports. Let's now take this other example. Supposing we're training a model which predicts the presence of a car in an image, and then you feed your model with this kind of data. Then after training your model, where you get to test that model is on this kind of real world data. It's clear that even if you had very great results with a previous data, in this working with this or testing on this new data set right here, you wouldn't have those amazing results you had previously. And so it's important that not only should your data be large enough or get as much useful data as you can, but ensure that your training data represents or looks like your test data or looks like what you're going to be having in production, as that's really what matters. What matters is the fact that you have a model in production which works well, or which has a high accuracy and not a model in your notebook, which has say 100% accuracy on your training data, whereas on production, it doesn't perform very well. Now moving on to under feeding, it turns out that here our model becomes way too simple for it to even be able to extract information from our data. So we may have our validation data and then our training data like this. And then there is this huge gap between our current loss and the minimum possible loss. We could also have this at a level of say, let's say accuracy. So yeah, we could have accuracy and then we have our validation and then training and we have say accuracy 100%. Let's say we have one right here and then our model is still too simple that we just end up say at 0.6 or 60% accuracy. In this kind of situation, our data or the relative size of our data as compared to the model may be too large. So you could even have a situation where this data is smaller than what we had here. So your data could be small like this, but if your model is way too simple, say we have just this very small model, then you may face this problem of under feeding. It also turns out that sometimes you may have a situation where you have even a very complex model, but that model still under feeds. And that's because that model hasn't been built in a way that it could extract useful information from this data. Now, if you could recall in the section where we're predicting the car price, we had a situation where a model, we use a simple dense layer. We use a simple single dense layer with just two parameters and we have this fixed data set. But once we increased our stacked up more dense layers, we found out that we're able to get better training and validation mean average error values. There are several ways in which we could mitigate this problem of over feeding. The very first one we'll look at is that of collecting more data. So it's important to lay hands on as much data as you can. This data has to be representative of what the model would see in real life. And this data should be as diverse as possible. Even after collecting more data, to solve this problem of over feeding, we could use data augmentation. Now, what is data augmentation all about? Supposing we have this image cell right here, the cell image we have here, which happens to be pasteurized. Now, instead of having just this in our data set, we could have this image right here modified such that we now have more data to train on. So this means that in the case where initially we had say 20,000 images, so we have 20,000 images. Now, after doing data augmentation, after modifying each and every image, we now have a data set of 80,000. Now we're considering 80,000 because we're supposing that each image is gonna be flipped as we've just done right here. So we take this image, we rotate it, we have this other image, the same level obviously still parasitized. And then we flip again and get this image. We flip, get this image, flip, get this image. We see that we now have, instead of just this one, we have four others. This actually means we're multiplying this by five. So this is 100,000 since we have it now, one, two, three, four, five examples for this single example we had initially. It should also be noted that there are many other data augmentation strategies for this kind of image data. So apart from flipping as we've just done, we could crop just a portion, we could add some noise to this data, we could modify the contrast, we could modify the brightness and carry out so many other operations. And there is no particular data augmentation strategy which works for all problems. This means that when you have a particular problem, you will have to try different augmentation strategies and then be able to select the one which works for your data. Now, that said, we have dropout. To better understand this notion of dropout, we'll consider this simple neural network right here. Now, if you could recall, the reason why we have models which overfeed is because we are working with very complex models with many parameters. Now, in order to reduce the complexity of this neural network, what we could do is take off, for example, this interaction between this neuron and all those previous neurons right here. So this means that when training our model, we are only going to consider that we have in this hidden layer just this two neurons right here. So all those connections here become useless. Now, this has the effect of simplifying our network as what we have as output. Now, that's after carrying out the dropout operation, looks like this. So we have now a connection which look like this. Now, this particular case is an example of a dropout. We drop our ratio equal 0.3, or let's say 0.333. As we're dropping out exactly one third of all the connections, or rather one third of all our neurons right here. And if r equal two third, what we'll be left with will be this. So we'll take this off and we'll be left just with this neural network to train right here. We see that we could leave from this very complex model to a simplified model via this dropout operation. And this has an overall effect of mitigating overfeeding. The next step we could take is that of regularization. To better understand regularization, suppose we have this model with weights WJ. So we have say n weights, and this weights are free to take up any value. As we've seen previously, the fact that this weights can take up just any value may lead to overfeeding. As now, this weights can be adjusted to fit on the training data in a very perfect manner. So this means that we could have a model which picks out each and every point like this. So we have this kind of model and we have this. So we have this model which picks up every point like that. Whereas if we restrain this weights to stay in a given range, then we may end up with something like this. So we may end up with a model which looks simplified like this because it doesn't have as much freedom as this other model. Now the problem with this model is if now you're putting this new data, you'll find that this will try to pull out like this and then your prediction will be somewhere around here. And so if, for example, we're having horsepower in the xaxis and then year, want to predict the price of a car, let's do same year. We have the price and then the horsepower. Then it happens that we have this car with this very high horsepower. This model, because it is overfitted on this training data, will predict this very low price. Whereas this model which is generalized on this training data will tend to predict a more reasonable price. Now it should be noted here that because this model was able to go through each and every point, we would have a training loss of almost zero. Whereas now when we give it this new data, it doesn't perform well. Whereas with this, we wouldn't have a training loss of zero. But when given new data, at least we're going to have reasonable predictions. So coming back to regularization, our aim is to ensure that this weights, because this model can be represented as this function and this function is made of this weights. And so since when we're doing training, our aim is to ensure that we minimize the loss, then we could include this weights in the computation of the loss. This means that we have our loss, which is now equal. The loss would have normally plus the regularization constant times the sum of this weights of each and every weight square. Now this is known as L2 regularization. Whereas here we have L1 regularization where we're summing up the absolute value of each and every weight. For now, we're just going to explain how regularization helps in mitigating that problem of overfitting by restraining this weights in a given range. So let's have your loss equal L does initial loss plus let's call this R. Now, if our aim is to minimize this loss, then obviously this L will be minimized and R will be minimized. And so if we're trying to minimize this year or this sum in general, then it would have that overall effect of restraining these weights in a given range. Especially as we know that when we square very large values, these values become even larger. And so to avoid this, our weights will tend to take up smaller values which fall on the smaller range. This L2 regularization is also known as weight decay. It should be noted that the main difference between this L2 regularization and L1 regularization is that in trying to restrain the range of values which this weights can take up, the L1 regularization has that negative effect of making many of those weights to take up values around zero. That's values very, very small or take up many zero values. So this will lead to sparse models as compared to the L2 regularization. And that's why in practice we generally use the L2 regularization. That said, we move on to L stopping, which we've seen already. In L stopping, as we have seen, if we have our model, let's say we have precision, validation precision and then the training precision which keeps on increasing. And then we have this limit year of one or 100%. So we have the precision and we have our epochs. And then after a while this starts dropping and this starts dropping simply because our model is now trying to over feed on this data it's been trained on. And so we have to stop training once we notice that the validation performance isn't improving any longer. So this means that after a certain number of epochs we are going to stop the training. And we've seen this already in the previous section, we've seen it both to really go like this and then we had seen it practically. Then another thing to do is to reduce the size of the network or use a less complex network. Our next step will be to properly tune our hyper parameters. Hyper parameters like batch size, drop our rate, regularization rate and the learning rate can affect our model and dictate whether this model will overfeed or not. Now, if we look at the batch size, training with a larger batch size may speed up our training process, but working with smaller batch sizes have a regularization effect which help reduce overfeeding. And so according to Yann LeCun, frames don't let frames use many batches larger than 32. For the drop our rate, we'll see this already. Increasing the drop our rate means we are making the model simpler and the regularization rate, increasing the regularization rate means we're reducing the effect of overfeeding. And then finally, picking too small of a learning rate may lead to overfeeding. So in general, we have some hyper parameters to tune and they are not only limited to this as you may have many other hyper parameters depending on your problem. Now the fact that normalization introduces extra parameters, mu sigma, which bring in some noise in the model has that regularization effect which help reduce overfeeding. So if you're including batch norm in your model, then you could feel free to reduce the drop our rate. Since this normalization layer already brings in that regularization effect. From here, we look at ways of mitigating the problem of over underfeeding. So with underfeeding, we could use more complex models. We could also collect more data. We see that this solution falls in the tool that's both overfitting and underfitting. Here is always a good thing to collect more data or more clean and representative data. So from here we have, you could also improve the training time. Now note that you could have this model, like let's take this, and then we have here you've trained and then you have the validation. So here you have the valve and then you have the train and then you've been training for over say a thousand epochs. And then you feel like the model may not perform any better. Now, several scientists have reported that many times they've given up on a model and then come back later after forgetting to stop the training process and notice that this model kept on performing were much better. So sometimes you don't have to give up on your model. So you could train or you could increase this training time. Then again, we have hyperparameter tuning, which could help in making your model more performant. And we have normalization, which stabilizes the training process and leads to better performance in the model. We now see practically how the dropout could be implemented with tensorflow. So here we have this dropout layer, which takes as argument the rate, the dropout rate, noise shape, and the seeding. To better understand how and why we need to use seeding, is simply in the case where one reproducible experiment. So if wanna apply dropout in this layer with a dropout rate of 0.2, then we'll be taking off one neuron out of this five neurons to make our model simpler and avoid overfeeding. Now in doing so, we may take this one, or this, or this one, or this, or this other one. So it's a random choice. Now, if we wanna fix this choice so that this experiment can be reproducible, then we can set this seed so that each time we run the experiment, it's going to be exactly the same neuron, which is going to be taken off. Getting back to the code, the way we could use this is by importing the dropout layer. So here we just have this dropout, we run that and it's fine. And then we could include this dropout right here. So we could have here dropout, let's say dropout rate, dropout rate equal 0.2. Now you could always increase this rate. So let's have that. And then we have the dropout and then dropout or add a rate equal dropout rate. That's fine. We could piece this out here, but we wouldn't wanna add dropout here. Anyway, you could always include the dropout at this level, depending on how the model responds to this dropout, which has been added here. So let's add this dropout and then add the dropout right here. Bearing in mind that we could always add more dropout layers and increase the dropout rate or even reduce this rate. So that's fine. We run our model. And as you could see right here, this dropout has no parameters. From this, we look at regularizers. We'll see how to implement the L2 regularizer and the L1 regularizer. So yeah, we have TF Keras and then we have this regularizers right here. So if you select L2, you should follow on this page. Now, once you get this, you see you have this TF Keras, the regularizers, L2, and then you specify the regularization rate. So let's copy this out and then get back to our model. At the level of our model, let's come back to the definition of the Conf2D. So here we have this kernel regularizer and this kernel regularizer that we use in carrying out regularization. So here we have this Conf2D and then we simply specify the kernel regularizer. We have kernel regularizer, which is equal this regularizer right here. Now let's take this off. And there we go. We have this kernel regularizer, which is now our L2 regularizer. You could always take this off and then from here, and you have from tensorflow.keras.regularizers import L2. So that's it. You could also import L1. So run that. Let's correct that. That's regularizer, regularizer. Okay, so that's fine. We get back to our model right here and then we have just L2. Now you could have this taken off from here and then you add it up here. So pattern valid, activation relu and that. Okay, so we have this and then we add our regularizer and that's fine. Now you could also do this for the dense layers. So just right here, you could have kernel regularizer and that should be fine. So this is how we implement the weight decay with tensorflow. Now you could always modify this parameters. So let's have decay or regularization rate 0.01. So that's it. And then we run our model, that should be fine. And we could go ahead and retrain this model. That's it for this section. Thank you for getting up to this point and see you next. Hello everyone and welcome to this new section in which we'll delve deep into implementing data augmentation with TensorFlow 2. The first method we'll use in implementing data augmentation will be using this TensorFlow image model, which is made of these different functions. Now this is just a few of those functions and you could check out the rest of the documentation. So we'll be able to do stuff like adjusting brightness, contrast, gamma adjustments, saturation adjustment, flipping left, right, flipping up, down and rotation. Using this TF image is known to be a more flexible way of implementing data augmentation as we could alter an input image with all these different functions given to us. The next method we'll use or we'll implement in this course is by working with the Keras layers. And although we're limited by the number of data augmentation layers made available to us, this method permits us to carry out data augmentation more efficiently and hence speeds up the training process. Now to find a balance between these two methods, we could implement our own custom Keras layers. And that's exactly what we're going to do in this section. We're now looking at how to implement data augmentation with TensorFlow. So we're going to get into TensorFlow images. So here we have TF, here is layers, we actually in Keras closes up and TF.image actually. So we have the TF.image with all these methods which we could use in data augmentation. We call that in order to do data augmentation, we're basically modifying the images while keeping the labels fixed. So that said, as you could see here, we have this several methods. Let's check out on this overview and we should be able to get this list of methods. So here we have the list of methods and we have the categories. So here we have image adjustments working with bounding boxes. Here is for object detection where you have to get into this. Here is cropping, flipping, and decoding and encoding. So this is what we have. Now if we get back to the top, we'll have these adjustments. OK, so we have this resize, which we've seen already. And then we have these adjustments. We could adjust the brightness, contrast, gamma, hue, JPEG quality, saturation, brightness, contrast, hue, saturation, and per image standardization. Now notice that these ones here are random. So we are adjusting our image randomly. As you could see, we have random brightness. Whereas here we're actually adjusting this brightness with some fixed parameters. So if you click on this, I just brighten this right here. You see, we have this image and we have this fixed delta, which we choose. Whereas if it's random, let's get back. If it's random, that will be fixed randomly. So click on this random brightness. You see, we are just given a max delta because all we need to pass in here is a range. And then we are going to randomly pick a delta in that range. So that's it for this image adjustments. We also have this cropping. So we could crop out some parts of the image. We could crop or do a central crop. We could crop and resize. So this means if we have this image, so we have this image. And it's 224 by 224. And then we want to do a crop. We could do a center crop like this. We could take this center crop. Sorry for that. We have this center crop. And we have this new image right here, which is this center. Now after getting this image, let's suppose we have now this smaller image of shape, say, 150 by 150. And then what we could do now is to resize this so that we get this shape, which we need to pass into the model. So we could resize this. Hence the reason why we're using the crop and resize right here. And you could as well just crop and then do the resize manually. So that's it. We have flipping, rotating. So flipping, we have an image. We flip it left, right, up, down, random, randomly. So let's get the same thing. But this time around, we're doing it randomly. Rotation and then the transposition of the image. So as you can see, TensorFlow gives us all these methods which we could use in modifying our images and hence augmenting our data. So here we will define this method, visualize. So you could see this clearly. We have the original image. And then we have the augmented image. Now in here, we have subplot. We want just one line, two columns. And this will occupy the first position in that twocolon space. So here now we have this image show, which takes in the original. And then we repeat this. So here we have this second position. And then we have the augmented. Augmented. OK, so that's our visualized method. Now what we'll do is we are going to get the original image. So let's say we have this original image, which is equal an element which we take from our data set. So we have train data set. And then let's just pick one element. So we have this. And then we take just one element from our data set. Now that's it. And this should output a level. So we should have the level. And there we go. So we have the original image here. And then we now work with augmented image. We get this augmented image. And to obtain this only augmented image, we are going to modify this original image. To do this modification, we are going to make use of these methods, which we've seen already. So yeah, let's pick out this flip left right. Click on this flip left right and see what we get. So all we need to do is just pass the image. So let's copy this. And then while we have here is our flip left right and our image, which has been passed. That's fine. We have original image. So that's it. Now we have this augmented image. We run that. And then we could go ahead and visualize this augmentation. So here we have original image and augmented image. OK. So we run this and see what we get. So that's it. You see that now in your data or in your data set, you will not only get this image right here. You will not only get this one, but you would get this and this. So this means our data set now will be multiplied by two. Let's check on other augmentation strategies. Let's get back. Here we go. We have flip up down, this similar to what we've seen already. OK, let's check out. Let's do this random flip up down. And then let's click on this. And then rotation by 90 degrees. So here we have this image, similar. And then they could specify the seed. So here we have the random flip up down from that. And then here we have random flip up down. OK, so we run that again. We have augmented image. And then we visualize. You see that it happens that it's exactly the same image, which is outputted. Let's run this again. And now what we get is instead this. So notice how there's a difference between these two images. So that's it. The next will be this rot 90 degrees. Click on that. You could always feel free to click and then understand what it actually means. Like, yeah, we're told to rotate image counterclockwise by 90 degrees. Now let's take this off. We know clockwise is this direction. So rotating counterclockwise means we'll be rotating it in this direction by 90 degrees. So that said, let's take this off. And then we have rot 90. We run that. And there we go. OK, so as you can see, we've rotated this by 90 degrees in the anticlockwise direction. We can also try out this adjust brightness and random saturation. So let's get back to this notebook. We have your adjust brightness. And we run this visualize. We have this error missing positional argument. So let's get back to this and understand how it's used. We check on adjust brightness. And there we go. We need to pass in a delta. So we have to pass in this delta. Now this delta should be in the range negative 1, 1. And as you can see, when you add this delta, it basically just adds up to each and every pixel we have. So this 1 turns to 1.1. This 11 turns to 11.1, and so on and so forth. And we told you that this delta is a scalar. And it's an amount to add to the pixel values. So let's go ahead and add this delta. Let's have that. We add the delta. And we have 0.1. So we run that again. And this should be fine. OK, so that's fine. We now visualize. And that's what we get. You'll notice that there's some difference with this, like this one appears brighter than this. And let's go ahead and increase this. Let's take 0.8 and then have that. OK, so you see, we are able to modify this brightness. We could also include the random saturation. Random saturation. And we have the original image. Let's get back to documentation for random saturation and see what we need to pass in. So there we go. We have this image. We pass in the lower and the upper limit. So here, let's find this lower upper limit. We are told here, we're going to get an error if the upper is less than the lower, which is logical. The lower should be less than the upper. And then if the lower is less than 0, so we have to ensure that we're dealing with values greater or equal to 0. So getting back here, we have saturation, that. And then we have lower. Let's say 2. And then upper, let's say 12. Take this off. Run again and visualize. OK, so that's fine. So you see, we've added some random saturation. And this is what we get. We could obviously reduce this. We could make this fall in the range 0, 1. And here is what we get. OK, we now check out on cropping. Let's check out the central crop. Here we have central crop, central crop, and the original image. So that's what we have. Let's make sure we understand exactly how it's meant to be used. So here we have this. And then there's a central fraction. So here they explain how this works. Now, where x is the central 50% of the image, it is a float which lies between 0 and 1. As you can see here, 0.5 has been picked. Meaning that we're going to pick 50% of the image. But this portion of the image we're going to pick has to be centralized, has to be surrounded in the center. So that said, we've understood how that works. We can now put that, let's say, 0.3. Run that and see what we get. OK, so you see, we get this portion right here. Now, what if we expand this to, like, say, 0.8? We should. We expect to have some black regions now. Run that, and you see, we have some black regions. So it's actually centralized. Let's take this off. You see, it's actually centralized. And the reason why we have something like this, it's actually cutting out like this and something like this. So this is what we get in here, something like this. OK, so this is what we get when we do this central crop. Now, the way we're going to integrate this augmentation in our data pipeline is going to be similar to the way we did with the resize rescale. So just like we used this map method, we are going to reuse this same method for augmentation. Here, let's suppose we are going to add this code and then define our augment method. So we have this. Let's put this down. And OK, so let's just have your augment. We have this augment method, image, level. And then what we'll do is we'll take the image. And then for that image, we're going to have the rotate 90 degrees of that image. Then next, we are going to have adjust saturation. And then pass in the image and specify the saturation factor. So here we have our saturation factor equal, say, 0.3. Now you could always feel free to visualize this so you don't get to poorly augment your data. That said, here we could have this saturate. Always don't copy that. Anyway, this flip left, right, let's come back to this. We have this adjust saturation. We could check that out. And let's have it here. OK, we have that. So we run this and visualize to see exactly what it's going to look like. We modify that image to original image and then run it again. OK, so this is what we get. Now that's fine. We could also include the flip left, right. So here we have image, tf.image, flip left, right. And then what we do is we could return the image and the level. So that's fine. Then we could also just simply include this resize with scale in this augment. So here we have image and the level equal resize with scale, which takes in the image and the level. So we resize and rescale before doing the augmentation. So that's it. Now we have this augments meta defined. We could now, instead of doing this here, so here we have augments. Now another thing we want to do before doing the map is actually shuffling. So we'll modify the order in which we're doing things. Now here we have this train data set, could have that. And then we have the train data set right here. .shuffle,.match, and.refresh. So yeah, that's what we have. And then we just include the mapping right here. So instead of doing the mapping before the shuffling, we'll just do it after the shuffling. And there we go. Let's now have this, it's basically this with augments. So we just have that. And then there we go. We have this map. OK, so we now have that for the training. And then we could close this up. So that's fine. We repeat the same process for the validation and the testing. So yeah, we're going to modify just as we did with the training. We have here our validation. Then we include this map right here, map, resize, rescale. Now notice how we are not augmenting the validation and test data sets. So here we have the validation. And then we now do the same for the testing. Anyway, for the testing, we only did the map. That is, we only did the resize, rescale. So that's fine. We will now take this off, train validation. That's off. That's fine. OK, so here we have our test data. We have our train data. And then we have our validation data. At this point, everything is set up. We could now rerun ourselves so we start out with training. So that's fine. We will come back, get back to this. We have our data. We run all the cells and our model created. So yeah, we're going to work only with this sequential API and keep out all this other methods of creating models. Once we're done with that, we get right up to this level. OK, so now we get into training. That's fine. Compile the model. And we wouldn't use any callbacks for now. Take that off and start training. We're now done with training. And we'll see the results right here. So as you can see, the model performs very poorly. And we'll try to understand why the model doesn't perform as well as it performed without the data augmentation. Now, if you can recall, what we did for the data augmentation was that we rotated the image by 90 degrees anticlockwise. We adjusted the image, the image's saturation. And then we flipped the image left right. But taking a look at the kind of data set we're dealing with, this saturation shouldn't be a great idea. And this is simply because if we look at each and every one of this, what differentiates a parasitized cell like this one and this uninfected cell is this patches we have right here. So you'll notice that with a parasitized cell, we generally have these kinds of patches. Whereas with an uninfected cell, we particularly have no patch. And with this, whereas here you could see these patches. And so rotating or flipping left right wouldn't change much about that. So that is acceptable. Now, the problem with modifying the contrast is when you modify the contrast, you tend to make the parasitized cells and uninfected cells less differentiable. And so it isn't a good idea in this case to modify the contrast or saturation of the image. Let's take this example so we could clearly understand why this saturation data augmentation strategy isn't a good idea. So yeah, right here, what we do is we just copy out this and then paste here. We take out just two elements from our dataset. And then here we have this 1, 4, and 2i plus 1. And we have the image show. From here, we create another subplot. There we go. We have that. And this one now is the saturation. So we have image.adjustsaturation. That's it. And then we'll pass in the image. And that's what we have. So here we have this first subplot and this other subplot. We have that, too. Now we could run this and see what we get. I'm using the positional argument. That is the 0.3, the saturation rate. So here we have 0.3. We're using exactly what we used previously. So here we have that. And this is what we have. In this example, we have both an uninfected cell. So we wouldn't actually see much difference. It's true this was modified. This was modified. But there isn't much difference. Let's take a parasitized cell. Let's rerun this again. Hopefully we get a parasitized cell. OK, as you could see here, let's take this off. As you could see, in this parasitized cell, what could clearly show the model that this is a parasitized cell is this patch we have right here. But when we adjust the saturation, it turns into this. And you'll notice that this parasitized cell now looks more like an uninfected cell. So the model isn't able to differentiate between these two clearly. So we see clearly that using the adjust saturation dedevelopmentation strategy isn't a great idea. Let's comment this part. And that's fine. Now we're going to modify our strategy. So just right here, what we had previously, we're going to take this off. So we comment this. And then we'll retrain our model. And so here we retrain our model after modifying our dedevelopmentation strategy. We run this again. And after training, as you could see, we get the results. Now we check out this loss. This is what we get for the loss. And if we look at the accuracy, we see that it looks more like what we had previously. Though we still have that maximum validation accuracy of about 94% and the trained accuracy of about 99% right here. And though the dedevelopmentation strategy hasn't closed this gap we had between the training and validation accuracy, we're seeing other sections how dedevelopmentation will be very instrumental in helping reduce this gap between the training and the validation scores. So far, we've been applying dedevelopmentation by using the image methods. Now the other way we could apply dedevelopmentation is by directly making use of Keras layers. So you come right here, tf.keraslayers, as you could see. And then you would notice that among those layers, we have those layers which permit us do modifications on images, like with this random contrast, which permits us randomly adjust contrast during training. Here we have the definition of this layer, this random contrast layer. And we have those attributes right here. Now, apart from this random contrast, we have the random crop, we have the random flip, and you could always get all these definitions, get back to, yeah, let's have this flip. Now the mode horizontal and vertical, so we're going to flip it both horizontally and vertically. We could also have other modes like horizontal, that's kind of a leftright flip. We have vertical, kind of updown flip. We have horizontal and vertical, which is a mixture of the two. And the default is horizontalvertical, as we've just seen right here. We also have this random rotation, which you could see, random translation, random zoom. So you could randomly zoom the data you're training on. And you also have this resizing. So we had looked at resizing previously, but not actually under the documentation. So we did the resize using TF image. Now you could also do this resizing, yeah, this resizing layer. Now, here you have the height and the width, as we had, similar to what we had in the image. We could scroll down and check on others. We don't really find anyone here. We could scroll back up, and that's it. Anyway, what's important to note is that you could create, you could implement data augmentation via two main ways, using the TF image and using the Keras layers. Now, we are going to look at how to reimplement this data augmentation strategy we had previously via the Keras layers. So here we had this rotation. So we had rotation and we had the flip left, right. So we'll get back to the documentation. And then what do we have here? We have this random flip right here, and then we're going to select the mode to horizontal, since we're doing left, right flip. So we'll copy this out. We have that layer, random flip. Let's put this just below. So this is the augmentation. This is augmentation using TF image. So TF image, augments, and then this is augmentation using Keras layers. So TF.keras.layers, augments. Okay, so here we have our augments. It takes the image and the level. But since we actually built in Keras layers, we are not going to take this this way. We're going to have augment layers. It's just a way of differentiating between these two augment and augment layers, which is a sequential model. So we have the sequential. We're going to make use of the sequential API, TF.keras.sequential. And then just right in here, we are going to specify all these different steps. And so we copied this out from the documentation. They just space it out here. We have this random flip. So we're going to use this random flip right here. Let's take this. Let's import random flip from the layers. So we have your Keras layers. There we go. We'll import random flip. Now we could have this. We have random flip. Okay, so we have the random flip imported. And then the other one is that rotation. Let's get back and we have your rotation. Let's check on that. We have scroll up and let's just do a quick search. So here we have rotation. Okay, so here we have random rotation. And don't forget, we will rotate in 90 degrees in the anticlockwise direction. So here we have to be careful to make sure we reimplement that same kind of rotation we had seen before. Now, here we have this definition. We have a factor, fill mode, reflect interpolation by linear, the seed for disability, fill value. And that's it. Now let's check down here and understand better this factor. Now this factor we have here is actually this tuple. And this tuple is going to represent our range. To better understand this range, you should note that it's like each and every one of this represents a fraction of two pi. Now what is pi? Pi equals 180 degrees. So two pi is 360 degrees. So we're looking for a fraction of 360 degrees. Now recall that we're going anticlockwise by 90 degrees. Like we're just trying to redo the exact same thing we have with a TF image. So here we're gonna look for a way to get 90 degrees from this 360. And to do that, we need to divide this by four. In other words, it means we're gonna get 25% of this 360 and 25% of 360 is 90 degrees. So 90 degrees, not percent. So that said, we wanna get 25% and that's fine. Now from what is given to us here, this is, we represent as a single flow. This value is used for both the upper and the lower bound. For instance, this results in an upper rotation by a random amount in this range. A positive value means rotating counterclockwise while a negative means rotating clockwise. Now we're interested in rotating counterclockwise as anticlockwise that we're actually interested in rotating this way. Recall, it was this, this is clockwise. So we're not interested in this, we're interested in this anticlockwise 90 degrees rotation. So we take positive values and then we'll try to ensure that it lies in that given range. Now, given that we are actually picking just random numbers and unlike the ROT90 method we have seen, which was somehow fixed. Yeah, what we'll do is we could just pick a range between say 90 degrees and say 90.1 degrees or any value very close to 90. So that said, recall it's gonna be positive, unlike if it was clockwise, it will be negative. So yeah, we're gonna pick the 0.25 because we wanna get 25% of this 360. So we have 0.25, that's positive 0.25 up to say 0.2, let's say five, zero, one. So let's pick out this range. So we'll work in this range so that we'll be around this 90 degree that we're actually looking for. Now that said, since we've taken 0.25, 0.205, two zero one, what we'll have here is 25% of two pi, which is 90 degrees, so it's gonna be translating it to 90 degrees and a value very close to 90 degrees. So it'll be like 90 point, a value very close or 0.00, whatever, degrees. So it's gonna be around this. So we pick this up and that's fine. Now we're sure it's anticlockwise because as we've picked it to be positive. Now the fill mode and the fill values will just be the default values, even the interpolation tool. So that said, we get back to the code. Let's copy this out right here, random rotation, copy this out, get back to the code. Let's include random rotation here. So we have random rotation. Okay, random rotation, that's fine. We could now run this, let's run this cell and then get back to our method or this layer we're trying to construct right here. So here we have the random flip and then the random rotation. Now we started in this other method with the rotation. So yeah, we're gonna redo that. Now we have this random rotation, random rotation. Okay, and then we have this factor. So we just do exactly what we just agreed to put. So we have the factor, a triple take 0.25 and 0.62501. Okay, now we have that, which is, for the rest we take the default value. So we don't need to put those values. So that's fine. We've done with this first layer. The next layer will be this random flip. So we just put out this random flip. Let's copy this, copy that and then paste it out here. Now we have our random flip, we've decided on the mode, take that off and that's fine. Now this is horizontal. So let's have this as horizontal. Okay, so we have this random flip and that and there we go. We have our layer, sequential layer. Okay, now let's take this off and that's fine. So we have this augmented layers or this augmented layers which we run this time around. Horizontal is not defined. Now let's use this small case, horizontal and run again. Horizontal still not defined as pass as a string. Horizontal, run again. Then we could redefine an augment method. Let's call this method augment layer and then this takes in the image, image layer, sorry, levels and then what we'll do is we're going to return this augment layers which we just created and then pass in the image, specify that training is equal true. That's okay and then we send out the levels. So here we're going to take this augment layers as we're going to take the image and pass into this layer right here. It's going to rotate the image and then into this other layer which is going to flip that image. So we get our output which has been augmented and we have our levels. We run this, run this and then we get to this year where we have our augment and we have augment layer. So we call the augment layer method right here. We call this method, that's fine. We run that and everything is okay. Now after running this training, we have this error. So it shows that we have an error at a level of the shapes. Now let's get back. The problem here is we forgot to do this resizing before the data augmentation. So we could copy this out and then put out this resizing here. We have the image and levels, level, let's say image and level, we have that. And then we resize and rescale. After resizing and rescaling, we now do the augmentation. So we run this again and then we run the stringing. As you could see all as well, we will now look at another way of creating of doing data augmentation. Before doing that, we should also note that instead of doing this resize rescale using the TF image as we had done here, that is using this TF image resize and then by dividing by 255 as we just did, we could also use the resizing and rescaling layers. So right here, we're gonna include those layers. Here we have resizing and then rescale. We run that and then just here, we're gonna include that resize rescale. So here we say resize rescale layers. Okay, so now we have this sequential API and then we always start with resizing. So we have the resizing, take this off and have this. Yeah, we're gonna put in size. We have in size, in size. Okay, and then we do the rescaling. So we have rescaling, take this off and we do the rescaling. So here we have this taken off, one divided by 255. Okay, so now we have those layers which are responsible for resizing and rescaling. Resizing and rescaling. So we'll see how we'll replace this method in doing the resize rescale. So that said, let's get back to this augment. We had augment layer right here. After getting this image, we do resize rescale and layer. So we do the resize rescale on the image before doing the augmentation. Let's take this off now and this should be fine. Now, since we've built this resize rescale layers, we could replace this here. Resize rescale layers, even for the test set. And then we do same for this. This is already fine. Augment layer is what we've seen already. For the validation, we have resize rescale layers. Okay, so that's fine. We run this, run the validation, run the train and that's fine. We then train our model and see what we get. So that's it. Our model is training. Everything is working well. Now let's pause this and then check out this other interesting way of doing this data augmentation. So as of now, we've seen how to do data augmentation by using TF image and then by building these layers. Now we'll see the importance of building these layers. But before looking at this importance, it's important to note this and that is a fact that when you're working with a TF image, you have more flexibility as with these layers, you are kind of like restrained to what you could do with data augmentation. Though these layers already have many common data augmentation strategies like the rotation, the zoom, the crop, the flipping and the others. Nonetheless, when you're working with a TF image, you'll see that you have many more operations you could do. Like if you open up this TF image right here, you see you have many more methods and you could do many more stuff with this. Anyway, getting back to our code, we have to note that the advantage of these layers is that you could embed this into the model itself. Now let's explain. Supposing you have, let's take this off. Supposing you have your training pipeline here. So you have that TF data, you do the shuffling, you do the mapping, you do prefetching, caching and everything you need to do with your training data. So yeah, you're doing your data. And then next, after doing all this resizing, reskilling and everything, you now pass this into your model. So you pass this into your model and then you train your model and have all your results. Now, when you want to test this model, you will actually still need to do resizing and reskilling. So you still need to ensure that you resize your image and then you also ensure that you reskill your image before passing to the model. Now, if you recall, like, let's go down, let's get down here. You would see under the testing section, let's look at this, under the testing section, that we made use of this test data. And this test data, as we have seen from here, was already resized and rescaled. So if you check here, you see, we had done some resizing and reskilling right here. Now, what if you have to take this, this your model, which you've trained into another setup? And obviously in that new setup, you will still need to resize and reskill because just like here, before testing your model, you had to resize and reskill. So what if we put this resizing and reskilling in the model? So if we embed this resizing and reskilling in the model, it means that no matter where we go with this model, we don't need to resize and reskill. All we need to do now, let's, we'll take this off, right, I will take this and put in the model. So all we need to do now is just to pass in your data without the resizing and reskilling and the job is done. So now we're going to see how to do this with this augment or rather with the resize, rescaled layers, which we've just seen. We also do with the augment, but the augment isn't very useful since in testing time, we don't make use of the augment. What's more important here to notice the resizing and rescaling, although if you can include this augment layers into your model and make the training faster. Anyways, let's go ahead and see how this is done. So firstly, you have the layers, which you've built up, you've had the augment layers, and then you've had the resize, rescaled layers, which we have had here. So all we need to do is to get to this model right here. And then after the input layer, you just have the resize, rescaled layers, and that's it. And then you have augment layers. So here you have resize, rescaled layers, and augment layers, and that's all. So with this, you don't need to do any resizing or rescaling on your input because this is going to be done. All these layers, which are now part of your model. And then here too, you don't really need to specify this image size. You could now have this as known. And then yeah, you have that. So you could put in any image and then the resizing and rescaling will be done in that model such that you could put this model or use this model in any environment. And all you need to do is to pass in the image and you have your corresponding output. Now we've modified this model. We need to also modify the way we're working with our testing and training validation datasets. So let's get back to this right here. We have the down here, we have this, augment, augment, and then we have this test dataset. So with the test dataset, now we do not need to do this again. So we just comment this because we don't need to resize and rescale anymore. Now with the validation, you see with the validation, we do not need to do this mapping anymore. You see, we take this off. We don't need to do this resize and rescaling. And then with the training, get back to the training, this training, we do not need to do this augment because we've included this already. So all we need to do now is just to shuffle, put in some matches and do the profession. So that's it. So we have this, okay, we will now rerun this and see how the training goes. We run this training. As you can see, we have this very long trace and with all this, with this message right here, which isn't very readable. So what we could do is generally when you're trying to get or understand an error and you don't seem to have an idea of what's going on, you could run the model eagerly. So we use an eager mode. So run eagerly equal true. And here you see when we run this again, you would have a shorter error message or more readable error message. So you check this out and you see you just have this shorter error message. It should be noted that TensorFlow operates into main modes. That is the graph mode and the eager mode. So as you could see here, when you don't specify this, you run the training in the graph mode. The graph mode is actually faster and more efficient way of training our model, whereas the eager mode is slower but more easily debuggable. So when you have errors like this, you advise to run in this eager mode and when everything is okay, you could take this off or set this simply to false. But by default, it's not true. So by default, we are not in eager mode while doing the screening. Now that said, let's scroll down and check on our error. And what it reads is we cannot batch tensors with different shapes in component zero. First element had the shape and element one had this other shape. So we try to, we understand from this that since we're doing the rescaling in the model that is in here, we're actually doing the rescaling, the resizing and rescaling in here. We have this batching, that's what we had here. We had doing this batching on elements which do not have exactly the same shape. And so to avoid this, we'll just fix this batch size to be equal one. So we work with batches of one and yeah, work with batches of one, we run that. And we run our model again. We run this and that's fine. So you'll notice that training starts and everything now works well. Those gonna be slower since we're no more working with batches of 32, but now batches of one. And also to ensure that this goes even faster, let's take this off. So let's pause this and then let's restart training with the graph mode. So we run this and run this and this should now be faster than what we had previously. Now training is going on, we could pause with the training and see how it's gonna be very easy now with this model to run inference. To test this, we've had this cell here, the cell image which we'll run inference on. So we have the cell image and then we'll make use of OpenCV to read this image. So yeah, we have our image equal CV2 in read, in read this image. So, and yeah, you could start by printing out image. So you would have that not defined. Let's import CV2 right here. Okay, so we go ahead and import CV2. We can close this and we could import that just here. So we import CV2, that's OpenCV, we run that. And then we now rerun this cell. OpenCV actually stands for Open Computer Vision. So this is a very popular image processing library which normally you cannot do image processing or computer vision today without really working with this. So that's how with OpenCV you could read an image. So image read, and then here we go. You have the sprint of the shape. So you see that it matches up with what we expect. We have that. Okay, so here's the shape of our image. And now we are ready to, let's add a batch. So here we're gonna have image equal tf.expanddems and then we have the image and then we specify the zero axis. So we wanna add the batch dimension and then we print out this image. So here we have our image. Okay, that's fine. Let's print out the shape. So we should see that. Okay, we have this here. And now you see that we're gonna pass this into our model directly. So here we're just gonna do a lunette model that predicts image and that's it. So all you need to do is just do this. We don't need to divide by 255. We don't need to resize and the model is gonna understand. Now note that we are passing this image of this input, which is 265 by 262 by three. And our model is gonna automatically resize this because we've included resizing in the model. So let's run this now and see what we get. And that's fine. So it tells us that zero, that is, it's parasitic, but which is actually wrong, but this is normal because we have not really trained the model for a long period of time. And clearly from here, our results show that we should expect very poor results. Why is this disturbing? Let's scroll down. Okay, so let's scroll this way and then scroll back up. Okay, so you see that here we have this poor accuracy, 48%, if we train for longer, we can get to the 99% wish we had already. So this is actually done to show how easy it's gonna be for you to run inference on this model anytime you wanna in any environment you find yourself. So far, we've seen two methods of creating, of implementing data augmentation with TensorFlow. We've seen the TF image and then we've seen the TF Keras. Now with the TF Keras, it's actually interesting when you want your preprocessing that is, say, resizing, rescaling, and the augmentation to be in the form of layers. And with the TF image, it's more flexible to work with. Now, we're gonna look at a way in which we could build a custom Keras layer based on the TF image operations. So recall that when we're working with a TF image, we had this rod 90 degrees, which was right here. And when we wanted to work with this layers, we had to play around with this to give us that same rod 90 degrees effect. Now, what we'll do is in order to create a layer, which takes, to create this rod 90 degrees Keras layer and not just use this random rotation because it's not exactly this, we are gonna define a class which inherits from Keras layers. So right here, we have this class, which we'll call rod 90. Let's call it rod 90. And then it inherits from layer. We have all in it. There we go. Self. Super. There we go. dot init. And that's fine. So yeah, we have this rod 90, which inherits from layer. And then we'll define the call method. So we'll define the operation, which is gonna be carried out in this rod 90 layer, which we're about to create. Yeah, we have self and then we have the input image. So what we simply wanna do here is we wanna have, we wanna return the tf.image, the rod 90. So we're carrying out the same operation, but now we're converting this into a layer. So yeah, we take that and then we have image. So that's what we output now. That sounds fine. So yeah, we've had this rod 90, which we've created. We run this. And then instead of this random rotation, we are gonna use this rod 90. So yeah, we have rod 90. Take this off, not in its past, and that's it. So that's it. We have our rod 90 now, and we are no longer using the random rotation layer. So that's it. We get back and then run the cells and get to training. And as you can see, everything just works fine. And we'll see how to get the best of both worlds. As now we have the flexibility of tf.image and the speed and portability of Keras layers. Before we move ahead, there's this point which needs to be clarified. So we've already seen that when we have a single sample like this one, with data augmentation, we're able to produce different samples. In this case, we have the rotated version of this. In this case, we have another version in which we've modified the saturation. In this, we have this version where we've flipped this horizontally. That said, in practice, we are not gonna feed or we're not gonna create, let's say, this three other extra samples. What we do is, if we suppose we wanna implement three augmentation strategies, that is rotation, saturation, and flipping, for example, then for one epoch, let's have this year. For one epoch, we may decide that we do not wanna carry out rotation. And then from here, we may decide to adjust its saturation. So let's have from this, we have its saturation adjusted. And then in the last step, we'll decide that we do not wanna flip. So at the end, we have this input and then we have this as our output. Now in another epoch, you would have this instead. You may decide that you wanna rotate. So you may decide that you wanna have something like this. Let's rotate this by 90 degrees. Again, here, this is random. So we randomly rotate in this. We decide a random whether we wanna rotate this at 90 degrees or not. Then once we have this, we may decide that we do not want to adjust the saturation. And then finally year, we have flipping. So let's flip this. There we go. We flip this and we have this as output. So in this case, this is what we pass in the model. In other case, we may decide that we still wanna rotate. So we may decide I wanna have this rotation. Then still no saturation, but year we may want to maintain this. So we may still have this, you see. Now we do not flip this. And this is instead what this example or this model sees. Now in another case, we may have the input itself. That's this input, it gets past this. No rotation, gets past this. No saturation, gets past this, no flipping. And this is what the model sees. So we can see that the model has now seen four different examples of this same input without us necessarily having to create this four different examples separately before starting to train. And so this helps us carry out the data augmentation much more efficiently. That said, when we get back to the code, you see that that's what we actually did. Yeah, we decide to, or we may rotate or not. We may flip this or not. And if we had to flip, it's gonna be horizontally. And then year we may modify the contrast or not. So these are all augmentations which are randomly carried out during training. Now for the other alternative, which is the TensorFlow image, we need to make sure that this rotation year is carried out in a way that sometimes we have the image rotated and some other times the image remains intact. Now get into documentation, you'll see here we have this route 90, which takes in the image and this K right here. Now this K is simply a scalar integer tensor. And the number, and it's actually the number of times the images, the image of the images are rotated by 90 degrees. And so here we're gonna define this K, which is a random number, which could either be a zero or a one. And that's it for this rotation. Now getting back to documentation, you could check the same for the adjust saturation. Here is actually stateless random saturation. So you click on this. As usual, we pass in the image and then we have this low and upper bounds, which are those of the random saturation factor. So let's take this here, it's basically this. We copy this out here. So instead of this, we would have this. Well, let's just put this out here. We have image, which is equal this. Okay, so we taken the image and then we could have, let's say 0.3 and then 0.5. Okay, so that's it. We are gonna have this. And then now for the flipping left, right, we get back again here. Here you have random flip left, right. Then we copy this out. See, so we're gonna randomly decide whether we're gonna flip the image or not. Let's paste this out here. Instead of this, we have this now. There we go. Take this off. And that's it. So now we have this augment method, which randomly selects which augmentation strategies we are gonna use for a given instance. And this permits us carry out augmentation much more efficiently. Now let's do the same here. Let's copy this and paste out right here. So here we have this. Hello everyone and welcome to this new session in which we'll treat mixed sample data augmentation. Previously, we saw how to do data augmentation on a single image like this or this one. And now we'll learn how to create new samples based on a combination or mixture of different images or different samples from our dataset. And more specifically, we'll treat the mixed up data augmentation strategy where we'll pick two samples from the dataset. We're gonna mix up the samples and then we're gonna include the strategy in our TF data pipeline. Up to this point, we've implemented data augmentation strategies which involve modifying input samples like this one. So we could take this input sample, we could do a zoom, we could do a center crop, we could rotate, we could translate and do many other stuff with this in order to augment our already existing data. Now, in this section, we'll look at another data augmentation strategy which is known as mixed up. Now, mixed up doesn't only involve just one sample as we had seen previously. With mixed up, we are gonna make use of these two samples instead of just one, mix them up and then use this output sample. And if you look very carefully here, you'll notice that this image contains this one, you could see here, you could see it carved out this dog, carved out like this, you see here and then this other dog right here. So we also have this one which is also carved out like this. So we could see the mixture of this two to form one input. So we have this output image which we could define as X prime which is a mixture or a combination of this image X1 and X2. So yeah, let's have X1 and X2. But this actually happens to be a weighted addition. That is we have a certain factor lambda which is a value between zero and one and drawn from the beta distribution such that we have X prime equal lambda times X1 plus one minus lambda times X2. This means if lambda equals 0.5, then we'll have 0.5 X1 plus 0.5 X2. Lambda equals 0.3 for example, we have 0.3 X1 plus 0.7 X2. So some sort of weighted addition. Now we've created this new input, it's logical that we need to modify the labels because this doesn't belong to either or you cannot really say that this belongs to this class or this class. It actually is a mixture of both classes. So all like previously, when we do data augmentation on this image, we are gonna maintain the label because it still remains this particular class. But when we mix classes up together or images from two different classes together like this, we need to modify the level. And so that said, we have a new level Y prime equal lambda and you guessed that right, Y1 plus one minus lambda Y2. And from here, we could now dive into the code and implement this mix up data augmentation strategy. Recall we said that lambda was to be drawn from a beta distribution. But if you come to the documentation we've been using so far, where we have this tensorflow.org API docs, you wouldn't really find this beta distribution. So you're advised to look out in this one year. So go to tensorflow.org slash probability instead. It's here you would find this distribution. So here we have this tensorflow probabilities and then you have tensorflow distributions. When you click here, you could find many probability distributions, including this beta distribution, which we want to work on now. Now, just right here, you could see the beta distribution, you have the definition. And then notice how we have this two parameters, which we must pass. So the beta distribution is defined over 01, this interval 01 is in parameters concentration one, aka alpha and concentration zero, aka beta. So we have the parameters alpha and beta to pass right here. Now, if we look up in this mix up paper, you'd see that the parameters alpha, they use like here from this experiments is 0.2. And then sometimes they use 0.42, but most times they tested on 0.2. So it's this parameter we'll be using. And then getting back here, all we need to do now is copy this out. So we copy this, get back to our code. And then we have the mix up down here. So here we have this mix up. Okay, mix out the documentation, let's reduce this. And then we reduce this part. So you wouldn't run the cells because we're not going to make use of this now. So here we have this mix up, let's test out this. We run it and we have this error, name concentration is not defined. Anyway, we could get back here and then import TensorFlow probability. So let's import TensorFlow probability and then put that as TFP, run that and then get back to our mix up. Okay, so here we have this mix up. Let's take this off. And let's take all this off actually, 0.2, 0.2. Okay, so we got that from the paper. That's understood. And then we have Lambda. So here we have Lambda. If we spell it this way, we'll have the keyword, Python keyword. So let's just keep that simple and just have this Lambda spelled, although spelled wrongly. And now we'll do Lambda, let's print out Lambda, Lambda.sample. So we take out one sample from that beta distribution and we run that and here's what we get. So you see, if you run this again, you would obviously get different outputs and all this drawn from the beta distribution and the range 01. So here I'm just going to do this, pick out this zero item and then we have this output. We could also put this NumPy to get it. See, you have this now. So you're not having the tensor. But anyway, we prefer to use it as a tensor and we'll explain why it's preferable to work with tensors in the function we're trying to build. From here, we just simply apply the formula. So we'll have the output image, which is equal Lambda times the image one plus one minus Lambda times the image two. So that's it for the image. We'll repeat the same process for the labeling. Then using OpenCV, we'll test this on this two images, which we've added here. We have this dog and this cat image right here. So we have this two, you're going to test this on it. Let's take this off right here. We're going to read the images. So we have image one. We're going to do the reading in read and then we'll do the same for in two. So right here, we could print out image that shape and level that shape. All right, let's print out the level. So we have that. We get in this error. Let's get back up and we correct this. So this actually Lambda, let's modify Lambda. Lambda equal this. So let's have that out. Okay, so we have that right. We run this again and require broadcastable shapes, which happens because we haven't yet resized this because here we have image. If we print out the shape of image one and image two before doing this operation, you'll see there are two different shapes. So we have to ensure that they are both the same. See, they're two different. Now let's resize this with CV2 resize, resize and that will specify the shape. So let's have your image size, image size, image size. Okay, we have that done. We just copy this out, paste out here. We have image size and then CV2 resize. Okay, so we've read, we've resized and that should be fine now. We run that again, image size is not defined. Let's have that to be defined here. We actually defined this previously, but we haven't run those previous cells since we restarted the notebook. So that's why it isn't recognizing the image size. Okay, now level one are defined. This looks great already. For this levels, let's say we have level one. So here we have level one equals zero and then level two equal one. So we have just these two levels. Okay, we run that again and this is what we get. You see, we have this output image and then we have this final level and which happens to be neither zero nor one. From here, we could plot this out, PLT in show and then we pass in the image and normalize this. We run that and this is what we get. So you see, we have a mix up of these two images. Now that we have succeeded to do this, let's make this part of our TensorFlow pipeline. So we could take all this out now and then define this mix up method. We have this level. Okay, we'll take this off. Okay, we're gonna define this method. Let's call it mix up and the way this method works is we're gonna take in our data. So train data set, here we have one and then train data set two. So we have this two data sets which contain the same elements but which have been shuffled so that we could have this kind of mix up. Then we can now make this data sets available which we'll do the mix up on. So let's add some code, take this up and then here we have our first train data set. We have train data set one which is actually our train data set we have built already. And so we're getting our train data set right from here. We've run this already. We get back. Be careful we are not running this but we may make use of one or two methods from this. So we have that and then we get back to this. So here we have train data set. We do some shuffling. We specify the buffer size and then we also specify that we're gonna reshuffle after each iteration. We just copy this and paste out here to have our train data set two. Now, once we have this train data set two we now have our train. Let's call this mix data set or mixed data set. Okay, we have our mixed data set that will make use of the zip method. And with the zip we're gonna pass in the train data set one and then the train data set two. Now, if you could remember we had an arrow when we passed into images we had two different shapes. So we have to ensure that we do some preprocessing before doing the mix up. So that said, after doing the shuffling we could do the preprocessing. So let's say preprocess. Let's get back up while we define this preprocessing. And we had here. Okay, so we had at this level of data augmentation we had preprocessing. Although we had inserted this in our augment but it's practically this resize, rescale method here. So we could run this, that'll be fine. And then we just do resize, rescale. So we wouldn't call that preprocessing again we just have resize, rescale as we've done already. We do the same mapping. Yeah, we have resize and rescale. Okay, so now we shuffle, we resize and rescale and then we have our data which is now a combination of data set one and data set two. This GIF was gotten from gifi.com. Now we have this mixed data set formed. We run the cell, that's fine. And then in here, let's take this off. We've run this already. So we have that. And then yeah, we're gonna take in image one. So image one and level one. Level one, we have that. Yeah, this suppose we have image two and then level two. This closes up. Okay, so we have that. And then we get this from the train data set one, data set one and the train data set two. So that's how we get this. We have image one, level one, image two, level two. The image one that we had here, we don't need this again. We have Lambda, we get Lambda, we get the image, we have the level and then we have our output. So we return image and level. So this is all about the mix up. Now we run this cell and then we create this other new cell. Yeah, we have this error, should have this. And then we create this new cell right here. Then we pass this out from what we had done already. And then here we have our mixed data set. So we now have this mixed data set. We shuffle again, we do the mapping with the augment layer, we do batching and prefetching. But since our augment is no more the previous augment layer we had, here we have now the mix up. So yeah, we replace that with mix up and that should be fine. We have the train, we could do the same for the validation. We get an error or spell an error, train data set. Let's get back to this, train data set. Okay, so here we have the train data set. We run that, run this again. We have your input Y of mall operation has type in 64 that doesn't match the type float 32 of argument X. So this is where we multiply in the Lambda by the levels. So clearly we have the Lambda which is a float and levels which are ints. So yeah, we're just going to cast this. So we have this casting, we specify the D type, float 32. That's fine. And then right here, we do a same casting, specify the D type again. And that's it. Let's run this again and see what we get. See, it works fine. We have this warnings. Also note that here we were trying to experiment and change this. So let's get back to point two, run that again. Okay, so now we have our training data as you can see here, we have the batch and then the image and then the batch and then the level. So that's it. We've now created this data set which happens to be a mixed data set. Then from here, we are going to prepare our validation data set is actually the same as what we had already. So you could just simply run this previous cell right here. This cell is data loading. Just simply run this, run this and you should be fine. Instead of doing this augment layer, we meant to just do resize with scale. We just have to resize and rescale our validation data. We don't really need to shuffle. We could take that off. We could take the profession off and that's fine. So we run our validation and then check it out here. See, you have a validation data. Now the reason why we have this is because we run this twice. So let's reinitialize this right here. Let's get back to the splits. We created train data and validation data. Okay, let's run this again here. Now everything should be fine. Okay, so you have the batch dimension and that's good. So we now get back to training and make sure everything is okay. We rerun this and everything is now fine. Okay, so we're now set to train our model. So we run our sequential API right here. We run this. We then compile our model and then we could get ready to train the model. Have this poor results. Reason being that the mixup data augmentation strategy isn't adapted to the data set we're working with. Even if the mixup data augmentation strategy we've just applied wasn't very helpful for this particular problem, it's important to note that this mixup strategy could be used in many other problems. And with that, we've come to the end of the section. Thank you for getting up to this point and see you next time. Hello everyone and welcome to this new session in which we're going to implement the cutmix data augmentation strategy with TensorFlow 2. The cutmix data augmentation strategy, though based on the fact that we are combining two different samples, is different from that of the mixup. Instead, with a cutmix data augmentation, we are going to take a random patch from one of the samples and attach to the other sample while modifying the levels accordingly. We've looked at how to implement data augmentation with TensorFlow and also how to implement more advanced data augmentation strategies like the mixup. In this section, we'll look at this cutmix data augmentation strategy. Here, if we suppose that we have these two images, image one and image two, we are going to randomly crop a part of this image. So just like you can see in this output here, they randomly crop this section from this image and then attach that to this other image such that what you have in the output is this one with this patch. So this is how this cutmix data augmentation is implemented. If we take this example where we have this cut and this dog right here, we'll try to randomly crop a part from this dog and then attach that part at the same position on this cat image here. To do this cropping operation, we get to TensorFlow image. Let's have this. We have this TensorFlow image and then we have the crop to bound in box. So we'll click on this and then we have this definition. We see the arguments, we're passing the image and then we are going to see, or we're going to specify the offset height, offset width, target height and target width. Now let's explain what all this means. If you have an image like this one, just as it's giving you the offset height is a vertical coordinate of the top left corner of the bounding box in the image. And so this means that if we randomly select, for example, this box right here, let's suppose we've randomly selected this box and our offset height will be this distance that is, our reference is this top left corner and our offset height will be this distance here. This distance and our offset width will be this other distance. So that's how we have this offset height and offset width. And then the target height is a height of the bounding box. So here we have this bounding box's height and target width, the width of the bounding box. So once you provide this, it will be able to automatically crop out this zone from the image. So coming back to the code, we are gonna add this other extra subplot. Let's have this other subplot. We have, let's paste this code first. Could just copy this out. We have this. And then we have the subplot, third position. What we're doing here is we're not gonna be having this. We're gonna have image, let's call that image three. And then what we'll be doing here is we'll be making use of this method. So we have this method and then we'll specify this offset height, offset width, target height, target width. Let's suppose that our offset height is, let's say 20. So here we have 20, 20, say 15. So we have 20, 15. And then let's suppose this target height is 100. And here, say 98. Okay, so we have this specified and then we now pass in the image. The image we'll be using here is image two, this image here. And that will be it. So here we take this off and then we could simply show this passes here and that's fine. Now we could take this off. Okay, so we have this. We can now plot this out and see what we get. Okay, as you could see, what we obtain here is the correct, it's a cropping of this zone, as you could see. You see that we have something around this, we crop out this zone. So actually it takes more of this. So it's something like this. Now we could modify this, like we could shift the height and the width. So we could actually maintain the height but shift the width so that we could get more of the dog. So let's do just that. We shift the width. Let's say we take 100 too. We run that and here is what we get. You see that we get the dog's face this time around. And basically this is how we crop out a region from this image. Now, once we've done this cropping, we want to create another image which is made of only this crop while the remaining zones are actually left out. Now to do that, we'll make use of this other method which is the path to bound in box. So here we have this image path to bound in box. We'll copy it out and you see how it works. So here we have this image path to bound in box. We're going to create another plot. So let's increase this number of plots. We have four and year four and year four and let's place this out first. So we have that, copy this, paste this out and then create this foot plot. So we have this foot plot. And now what we'll do is just copy this here and paste it out. So here we paste this out and then let's get back. And then what we pass in as image now is this cropped image. So let's actually copy this. Let's say we have this crop. Let's call it a crop. And then we have, let's take this off. We have the crop, paste it out. Okay, so here we have the crop and now we're going to take in the crop now. So after, let's look at this. So after we've, oh, we've had an error before. So let's do this so you could see it better. Okay, so actually what we're seeing here is we have this padding to be done and then we're taking this image and then we want to pad it or like stamp it on another image which contains only zero pixels. So let's look at that. We have that. We have this pad. We pass in the crop. We have the offset height and width and then the target height has to be given. So yeah, we're going to put in the image size because we want this to be padded on an image with this dimensions. So we have the image size there. Now we could run this and see what we get. We have this error. Let's modify that quickly. Okay, so yeah, we have this taken off and then we have 20, 100, 100. Okay, so let's run that and we should have something unreasonable. Offset width again. Oh, okay, we should take this off. Okay, so let's look at this and there we go. As you could see, we have exactly what we expect. You see that we have this somehow the same image but we've taken out only this crop. So that's what we actually want. We want to be able to take only this crop and then take this like this crop and then add it with this image here. So what we want to do is we want to take this image now and then add it with this image so that we could be able to create our data augmentation pipeline. Now we could call this image four. So let's have this as image four equal this here. Stick this off. Image four, paste it out. And then once we paste it out, we could now let's also do another subplot. So we keep on doing the subplots. Let's copy this and paste it here. That's fine. We take the crop as we have the, or rather we have this image four plus our initial image, image one. So let's have that and then we put it out. Okay, we get in this plot but it isn't very clear. So let's increase that figure size. Let's add the figure size here and that's it. So we run that again and then let's look at this now, Clara. Okay, here's what we get. So as you could see here, we have this patch, like it looks like someone is working but there is a problem as we have some sort of mixture of this and the initial image. Now this is logical since when you do this addition for this black region, you just have only the cut but for this region, you still have the path of this cat image. So what we need to do is we need to remove this path such that when you take this and add to this, it just fits in like a puzzle. Then to crop out just this part from this image, we are going to use the same process we've used for this dark image right here. We'll simply copy out what we had already here. So we had this cropping, could copy this out and paste it out here. We run that and here's what we get. So you see, we have this crop again. Now we will take this crop and then add to bound in box as we had done here previously. So let's copy this and then what do we have? We have the seventh and then we're going to pass in this crop cat. So pass in the crop cat and that's it. But we'll print out this image. So let's call this image five and here we have image five, we run that and this is exactly what we get. Now the aim is for us to be able to take this and subtract like take this and subtract this image from it such that we'll be left with the full cat. We doubt this portion, without this portion, this portion right here. So that said, if just here you do image one minus this image five, you would get this answer. So as you could see, you have this whole cat without this portion and this is exactly what we want. Now that we have this part, we could add it up with what we wanted to add initially here. So we added up with this image four. So here we have plus image four and we get the response and there we go. So we've completed this process of cutting out this portion from here and then fitting on this image one. We now take off this other part, take this off and there we go, we now have the crop, the image four, crop cat, image five and then our final image. We copy out this mix up code we had done previously. So we get back to this. You see that now we could easily integrate this since we have image one and image two, we just left with the levels. Let's just copy this out, cut that and then here we take this off, paste it out. There we go and we now have the output image. We now modify this variable name such that crop is crop one. So here we have crop one instead of crop and then here we have crop two. So everywhere we meet crop cat, we have crop two, here we have crop one and then this image two is maintained from here, we have image two. Then the image four, we could tend this to pad one. So let's call this pad one. Here we have pad one, but this is actually pad two since we're working with the image two. So let's call this pad two and crop two. We have the crop two, then from this we have crop one and here image one, here we have crop one and here we have this image five, we'll call it pad one. Then coming right here, image five, image five is pad one and then image four, pad two. Okay, so we have that done. Now we could focus on how to get this bound in boxes. So yeah, we just pick this bound in box, but how do we get this bound in box? To get an answer to this question, we are gonna make reference to this formulas which were given in the paper. Right here, we are told that rx is drawn from the uniform distribution, which takes parameters zero and the width. And then ry is drawn from the uniform distribution, which takes parameters zero and the height. Now if we consider this image and say this box randomly picked, then our rx is a center that is this distance from this as based on this origin actually. So we have this vertical distance to this center, which is our ry and then this horizontal distance, which is our rx. So that's how we obtain ry and rx, which we draw from the uniform distribution. Now to obtain rw and rh, we have this formula right here. rw equal w, w is the width of the image. So we have w times the square root of one minus lambda. Recall how we obtained lambda with a mix up data augmentation strategy. So this is exactly the same way we get lambda. So here we have w times square root of one minus lambda, rh equal the height of the image times square root of one minus lambda two. Now note that when you multiply this two, that is if you take rw times rh, you would have the numerator right here. Let's have this. rw times rh gives you this numerator. And then if you multiply this two, you would have wh times the square root of one minus lambda. So if you multiply this times this, it gives you wh, this times this, square root of one minus lambda times the square root of one minus lambda. And you have the square root of one minus lambda. Okay, so this is equal rw rh. Now we'll see how they obtain this formula. You see here that since you have rw rh, you could divide here by wh and divide here by wh. This goes away and we left with this. Obviously, if I have the square root of x times the square root of x, then it is equal to x. Since I'm having the square root of x squared, now the square root gives you x. So in this case, our x is one minus lambda. So this is equal one minus lambda. And that's how they obtain this relationship right here. Now that said, we have rh. Oh, we've taken this off. Let's go one step back. We have rh. Let's take this. We have, sorry, rw and then we have rh. So this is rw, the width, and then here's rh, the height, r height. But recall that what we have to pass into the method, which permits us crop out the zone, for example, is actually this point right here, the top left corner of this bounding box. So we are not going to use the center, but instead it's top left corner. Now, how do we get a top left corner? To leave from the center to the top left corner, what we need to take into consideration is the fact that we notice this width, for example. Now, if we know this distance, and we have this distance from this to this point here, we're supposed to use the center, then we could obviously get this distance here. We could get this distance. To obtain this distance, we need to take all this minus just this portion to obtain this portion right here. Now, to obtain this portion, it's easy because we already know the width, r, w. So all we need to do is divide this by two. If we divide rw by two, then we have this distance. And since we already have this distance, rx, then we can obtain the x coordinate for this point right here. Now, the next thing to do is to find rh. And we're going to use, sorry, we're going to find ry. And we're going to use the same method we did to find rx from base in the top left corner. For ry, we know this distance. Now, do we know this distance? Yes, we know this distance because this is half of the total height since we are found at the center. Now, if we have this distance, then we could find this distance because this distance plus this distance gives us this distance. And so to get this distance, we need to take this distance minus this distance to obtain this distance. And if we have this distance, then we have the y coordinate of this point right here. Recall that the reason why we're going through all this is simply because the methods offered by TensorFlow consider that the bound and box coordinates are given based on this top left corner right here. So now we define this function box, which takes in the lambda. It takes in lambda. And then what this does is, it makes use of this uniform distribution to obtain rx, ry, and then makes use of lambda to obtain rwrh. Now, getting back to our uniform distribution, we could copy this right here. And then put in our code. For now, let's keep the function aside. Let's just have this. So here we have our uniform distribution. Recall that our low is zero. Let's take this back. Our low is zero and our high is the width. So we have your aim size and we have that. Okay, so we've defined this and then we have rw, or rather rx. That's rx. Now we want to have ry. Let's have it rx, ry. And then we could simply copy this out. There we go. We have this. Let's print this out so we see what we get. Print out rx. For example, there we go. We have rx. Aim size is not defined. We restarted the notebook. So let's get back to redefining this aim size. We run it again. And this time around, everything looks fine. Now we could draw a sample from this distribution. So let's have sample. We draw a single sample and see what we get. You see, we have that. We could take this zero element and there we go. So now we're able to draw a sample from our distribution. We could do the same for ry. So yeah, basically we have this. And then what we want to do is to ensure that all this coordinates are integers. So we could cast this. We cast that. We have the dtype equal int 32. That's fine. Copy this out and paste it here. Okay, so we have that and then we cast this too. Now we have rx, ry. Let's just do a sampling directly. So we've taken this and then we do the sampling right here. Sample, take a single sample, take zero element. And that's fine. We do the same here, paste it out and everything looks okay. So yeah, we could now print out, we could now print out rx and ry. Okay, it looks great. We have now our random rx and ry. Run again so you could see the response. Now we have to obtain m size times the square root of one minus lambda. So we suppose that we have lambda. We actually going to use the same method we had used previously. So suppose here we have our lambda and then right here to obtain rw, we have the m size times tf mat square root of one minus lambda. Okay, one minus lambda. Okay, that's fine. Now we have rh, r height, rh same in size, one minus lambda, that's okay. So now we have rh, rw and rx and ry. What we'll have to do is to modify this rx. So we will have rx equal rx minus the width divided by two. Now we want to have a whole number. So we have that minus the width divided by two, but the width is the m size. So we want to have this m size, there we go. Now, if you don't get why we're using this, just get back to this image to obtain this distance, that's to obtain this coordinate, this top left corner, we simply take this distance to the center minus the width divided by two and we do the same for the height. So that's it. Now we're doing that. We have rx equal rx minus m size divided by two. We'll repeat the same and then we have it for y. Now this is actually this width divided by to the width of the box, not the width of the whole image. So we're making an error here. Let's get back to this. We're making use of this width actually, not the width of the whole image. So let's get back and we'll put this code after this. So because we're going to be making use of this rh and rw. Now we have here, this is rw, rw, that's fine. Okay, now yeah, we're going to make use of rh. So we have the rh, ry, ry. Okay, so now we have rx and ry. We could now print this again and then also print out rw and rh. We get in this error. We told that in this competition right here, the rw is meant to be an int that was passed as a float. Since after this competition here, we would have that as a float. Now we could always print out rwd type. Let's run that and see what we get. You see, we have a float. So we'll modify that and cast it to ensure that we have an int. There we go. We have dtype equal, let's just copy this, tf ints 32. So we paste this out here and paste it out this way. tf.cast and that's fine. So now we expect to have the expected response and there we go. You see, we have rx, we have ry, we have rw and we have rh. When we run the cell several times, you'll notice that you have some negative values popping up from time to time. And this actually happens when, for example, we could have our center at this level, but our weight is so large that it doesn't fit in the image. So it goes in a negative direction. And this kind of situations are, say in this kind of situation, you could have our box going out of the image. And so we have to ensure that each time we create in this box, we limit it to the image. So this box now, let's take this off. This box now will be this one. And this other box right here will be this box. So we redraw this box, but it wouldn't go out of the image. Our aim is to ensure that we take this part off. Okay, so that's it. We're now going to clip this values programmatically by using the clip by value method. So here we have tf.clipByValue. There we go, we have that. And then once we pass this in, we're going to specify the range. So we're making sure that the values always fall in this range zero to the image size. So that's fine. Place it out and we have this clip by value right here. So that's it for our X, our Y. And now after running, you should have only values fall in that range. After clipping, what we have is, if we had say initially this box right here, we now have this box. So this is what we get after clipping. Of course, we get the coordinates of the top left corner, but the width in this particular case has changed. In another situation, we could have this, for example, if we have this, in this case, the height changes because we no longer have this height, but now this new height. In a situation where we had a box, which was like this, for example, here, but the height and the width will have to change. And so based on these new modifications, we have to ensure that we actually pass the right width and the right height. And so what we could do is make sure that once we have the center and we subtract the width, or we subtract half of the width and half of the height to obtain this, because we had the center, we had X minus the width divided by two to obtain the X coordinate of this point. And then Y minus the height divided by two to obtain the Y coordinate of this point. Now, what we'll do is we'll obtain this coordinate right here, but while implementing the clip function. And so this means that if we had a box, like normally centered in the image like this, and then we have this, we'll obtain this point, top left corner, and then we'll obtain this point, bottom right corner, just by using X plus W plus W divided by two, and Y plus W divided by two. So this point is of has coordinate X plus W divided by two and Y plus W divided by two. And that said, after clipping, we'll have just this now. But what's interesting about the method of clipping after getting this is that now we know exactly where this bottom left is found or rather this bottom right is found. And if we know we have this coordinate and we have this coordinate, then we could recalculate the width and the height. To recalculate the width and the height, it suffices to take, for example, for the width, we take this point here, the X axis, minus this X axis right here, or minus the X axis at this point, because this point and this for the X remain constant. Whereas for the Y axis, this point and this also remain constant. So that said, all we need to do now is to take this Y minus this Y to obtain the height, and then this X minus this X to obtain the width. So based on that, we'll recopy this out and paste. So here we have X bottom right, X bottom right, and Y bottom right. All we need to do is to change this and add a plus, and here we add a plus. We keep it by value, so always ensure that it's found in the image. Then also now to obtain this final R, W. So our R, W now is equal, our Y bottom right, minus our R, Y, and then the R, H, we equal our Y instead. This is actually X, and here is X, because that's H, that's W, sorry. So you have a Y, B, R, minus R, Y. Okay, so now we've modified the way we calculate this based on the fact that the box may be out of the image, and that now it has some modifications to be done on the width and the height. Now the next step we have to take is, if this R, W is equal to zero, then we have to make sure that R, W becomes one. And then here we repeat the same process. If R, H equals zero, then R, H becomes one. So that's it. Now we have our R, X, R, Y, R, W, and R, H. That said, let's now create our box method. We have our box method right here. Let's have that. And this is what we return. So we return in this. But note that here we're taking the height, the offset height, offset width, target height, target width. And so this means that what we have to output in this method right here has to be R, Y, R, X, and then R, H, R, W. So there we go. We return that and normally everything should be fine. So we have this box method defined. We run that. We get in this arrow. We should pass in our Lambda. Let's go down. We have here Lambda. Okay. So we've passed in Lambda and that's fine. Now next thing to do is get back to our mix up. And then we like we did before with this Lambda, we actually could get this. We pass in the Lambda so we could take this off from here. So we run this again. And then we'll define this Lambda creation just right here. We have Lambda. Let's go back. We have Lambda. And then now we have the box. So this box is going to produce this output we need here. Let's call it R, Y and that other in the documentation, R, Y, R, H, R, W equal box of Lambda. So now we have box of Lambda and then we have this outputs here. Now instead of passing this, we have R, Y, R, X, R, H, and R, W. Let's take this off. Those boxes we fixed initially. We have this. Yeah, we have R, Y and R, X. Scroll down, that's R, X. Okay, now we repeat the same process here. We just simply copy this out and paste here, paste it out. Then also we have R, Y, R, X again. So here R, Y, R, X, let's take that off. Okay, so that's what we have now. And the mix up seems to be fine. So we could get back, run this method and then run our mix up method again. Now as usual, we are going to mix up these two data sets. So it suffices to run this. And then for this one, we are going to, instead of using the mix up, so yeah, we command this part and then we have the map and cut mix. So yeah, we have the cut mix method. We run this and everything should be fine. We get in this arrow. Let's check on how we call this. Oh, we call this mix up still. This should be cut mix, cut mix. We run that, fine. Let's get back to this year and run it again. Okay, that looks fine. Train data set, everything looks fine. Now we'll try to plot out some values so you see clearly what this looks like. Let's create this new code cell down here. This is out and we are going to show this image from our training data. And there we go. We could notice this patch, we could run this again. And this time around, we even have a bigger patch. So that's it, we've seen how to come up with this data augmentation strategy. But yeah, we get to do with the level. So I think if we print out this level, let's print out the level. Yeah, see, we print out the level. We just get in all zeros. Anyways, let's go ahead and implement this section for the level. And to do that, if you recall this formula, you have one minus lambda equal this. So this means that lambda is equal one minus RWRH divided by WH. And the reason why we need to do this again is simply because the RH, like at this level where we had this RH and RW modifications, all those clippings, we have to ensure that when creating the level, that condition is verified. So let's get back. Let's check on this formula. Lambda equal one minus RH, RW. So here again, we have lambda equal RH times RWRH, anyway, we have this RWRH, then divided by WH. So here we have in size, in size times in size. Okay, now this is one minus all this. Then we apply the same formula for the mix up. We have this lambda. Okay, we've got no lambda. We now pass this. We have level one and level two. Okay, so there we go. We have our cut mix. And then let's get back. Actually, we will have to rerun this, run our cut mix. And then for our training data, we would have to rerun again. So we have to run this cells again, run this, and then recreate our training data set. Again, this error, meaning that lambda is of type float 64. We get back to this and then we do some casting to send this, make this float 32. Okay, that said, we run our cut mix, cut mix run. We run this, there we go. Your cut mix, we run this again and this is fine now. So we have our training data set and then we'll go ahead and visualize it. We have this double knowns right here. So it's preferable for us to actually rerun this from this point. Okay, so we run that from that point and then we get back to this, we run this. Well, that's fine. We can run this and this should be okay now. Okay, that's fine now. We have everything intact. So that's fine. We can now visualize our data. So you can see we have our patch and then unlike before where we had add a zero or one, now we have values between zero and one as our levels. So let's now run this model, compiling and training. After the training is completed, we obtain those results. We see here how this accuracy, let's scroll down here. We have this accuracy, which doesn't really change much. So we just around this 46, 48 and high is this 50%. So it's around this 45 to 50% range and the last two doesn't really change much. Now, this is due to the fact that the model is getting confused. And the reason why this model gets confused is because if you have say this uninfected cell right here and this parasitized cell, it's actually this portion which permits the model know that the cell is parasitized. And so when you come and crop this part right here and then attach it to this cell here at this position, the actual part of this parasitized cell image which makes this parasitized isn't taken into consideration. And so the model gets confused as now it doesn't really know how to differentiate between an uninfected and a parasitized cell. Again, this cut mix data augmentation strategy isn't adapted for our model though it could be applied in many other problems. We thank you for getting around to this point and see you next time. Hello everyone. And welcome to this new section in which we'll see how to implement data augmentation using a specialized data augmentation library called albumentations. We'll see how to use albumentations with TensorFlow and also PyTorch which will permit us to see how easy it is for us to integrate albumentation with just any library. Note that this session was inspired by a question posed by one of us. Feel free to always ask questions as this will permit us better discourse. We shall be looking at the albumentation tool which is a specialized data augmentation library. Albumentation is a Python library for fast and flexible image augmentations. It efficiently implements a rich variety of image transform operations that is optimized for performance and those so while providing a concise yet powerful image augmentation interface for different computer vision tasks including object classification, segmentation and detection. Now we will look at why we need a dedicated library like albumentations. Here in the documentation they argue that if you do this kind of usual data augmentation there are a host of libraries like TensorFlow which we've seen already which do this kind of data augmentation. But then when carrying out data augmentation with different tasks like say object detection like in this case or let's take this example here which is more illustrative. You have this image as input and this bounding boxes which will permit you build a model for object detection. But then if you were to do data augmentation and then you want to augment this data by applying cropping like as you could see here notice how this image right here has been cropped like what we have now is something like this part or rather than this part actually. Now notice you have cropped out this whole image and you're left only with this. I think it's it goes right up so it should be something like this. Yeah, something like this. Okay, so you've cropped this image and it's part of your data augmentation pipeline. So let's take this off and this off. As we were saying, you crop this image and here's where you get. Now with the usual data augmentation methods or working with the usual libraries what you have to do is after doing this cropping you would have to manually modify each and every bounding box you find here. You see this bounding boxes right here or this one's not taken into consideration because it's not part of this crop. So you have to modify each and every bounding box you find here, this red, brown and boxes right here. And this is because this bounding boxes they get the positions with respect to this origin right here or in this case with respect to this origin. And so when you do this cropping obviously those positions change. Now, instead of doing this manually augmentation permits you to get this restructuring of this bounding boxes automatically without having to manually modify them. And apart from object detection another very common use case is in image segmentation. We have this original image which has been transformed into this one and then the image has a mask. So you are trying to segment the different parts of this image. And so after you've applied this rotation you now get augmentation to apply the same rotation in the outputs. Another reason why using augmentations is advantageous is the fact that it has this declarative definition of the augmentation pipeline and provides a unified interface. So basically this is what it takes to build augmentation pipeline. Now notice how with this we can also include this probabilities. So you see a probability 0.3 and this probability 0.5. Now stating that this brightness contrast augmentation will get a probability of 0.3 symptom means that you're gonna apply random brightness contrast three times for every 10 images you process. And this means that if you have a data set of let's say let's say we have a data set of 10,000 images we have this data set of 10,000 images then what our model will see are the probability of our model getting the original images is in this case of P equals 0.3, 0.7. So we have a probability it is most probable that we will get 7,000 out of this 10,000 from the original data set which our model sees and then we have 3,000 which is gonna be augmented. Then you also have this horizontal flip so you have the 0.5. So this means that you could pass in an image and then there is no brightness contrast and there is no horizontal flip. Same as you could pass in an image and you have the random brightness contrast and you don't have this or you may not have this and you have this or you may not, we've seen this already or you may have this and have this. But with this random crop you always have it because here there's no probability specified. Then also it's advantageous to make use of augmentation because it has been rigorously tested. As you can see, it has been battle tested, used widely in the industry, deep learning research, machine learning competitions and open source projects, high performance, diverse set of supported augmentation, extensibility and rigorous testing. We also have here this list of transforms and their supported targets. As you could see, this list is broken up into two parts. We have the pixel level transforms and we have the spatial level transform. So if you want to get more information for each and every one of these transforms, it suffices just to click on this. So you could say, for example, let's pick this sharpen right here. Click on the sharpen and you have here the arguments and the description. Let's take another example from the spatial level transforms. Here we could take the vertical flip. So click on this vertical flip, you see, we have this float probability of applying the transform default to 0.5. Now note that if you're dealing with a very large dataset, that is, if your initial dataset is very large, then you could use probabilities between 0.1 and 0.3. Reason being that since your dataset is already large, it doesn't need data augmentation that much. Now, if you're dealing with a small, medium sized dataset, you could use probabilities between 0.4 to 0.5. Nonetheless, you could always pick whatever value depending on how it affects your model performance. Getting back to the code, we're going to make use of this example, TensorFlow data augmentation pipeline built with augmentations. So here we have the transforms and then we have the augmentation function, then data preprocessing, and finally integration with TensorFlow datasets. Now, based on the kind of dataset we're dealing with, we have to be very careful in the dataset augmentation strategies we're going to be using. So like, yeah, we could use this random rotates because rotation doesn't wipe off those sections which contain information that permits us to differentiate between a parasitized and an unaffected cell. Getting back to our random rotate 90, you have here this argument, a float, which is a probability of applying this transform default 0.5. So we could make use of this random rotate right here. Then just below, we have other different rotations. We have the random rotate 90 apply, which rotates the image a certain number of times. We have the geometric rotates where we can select the angle and which will do the rotation. Then we have this safe rotate would avoid this kind of data augmentation strategies like the cropping because here you could crop out information which permits us differentiate between the parasitized and uninfected cells. You also have this resize here, which we were using preprocessing our images. We have the vertical flip and the horizontal flip, which we are gonna use. So here we have horizontal and vertical flip. Let's add that here. We can also do just a flip. We also have this random grid shuffle, which we could make use of. This randomly shuffles grid cells in an image, meaning that if we have this kind of image and then we've picked a grid size of three by three, we could break this up this way. You break this up and then you have this three by three grid cell and then you simply randomly shuffle this position. So this one can end up here. This ends up here. This ends up here and so on and so forth. Could also have this random brightness contrast. So with this one, we'll take this all and then paste it right here. Random brightness contrast. The next one, let's take this sharpen. So that's it. You could use other data augmentation strategies you want. Always make sure that you visualize the outputs to better understand exactly what you're using. Now we define our transforms. We have our transforms, augmentation, compose, and then we have this list which is made of the different augmentations. Yeah, we just copy this out. Let's have this copied. But before this, let's do a resize. So yeah, we could have a dot resize and then we specify the image size and that's it. So we specify the image size. We're going for the horizontal and vertical flip. So let's copy this out. There we go. You have that horizontal vertical flip. For those random brightness contrast, we're gonna use the default parameters. Sharpen, we're gonna use the default parameters and that's it. We now have our transforms. So we could run the cell. We get in this error and this is because right here we have to have this. So it's augmentation. This is going from augmentation. We run this again. We get in some errors. Augmentation has no attribute sharpen. Even when we comment this one, we will also get this error. Augmentation has no attribute random grid shuffle. So we'll just comment this tool and then run that again. Okay, that's fine. That looks fine. We can also implement this augmentation one off. With this one off, either the vertical flip or the horizontal flip. So let's take this out of this year and put it right here. We have one off horizontal flip or the vertical flip. Now we're gonna have this year and then, so we have this list actually. We're gonna create this list and it's gonna be made out of this two transformations, which is the vertical and the horizontal flip. Let's have this year and close this. Okay, so we have defined that one off and then we could also specify a probability. So let's take P equals 0.3, for example. And there we go. This probability year actually defines whether the one off will be applied or not. And so we have that. We could run this again and that's fine. Drawing inspiration from this method given the documentation, we are gonna create this own method, augalbumant, which takes in an image, creates this dictionary, feeds this information in the transforms, which we've created here, and then normalizes the image. So this is what we do with this augalbumant method. From here, we could have our train data set similar to what we've been doing already. But what the difference is that instead of this, we have now process data, getting back to the documentation. In this process data year, we have, let's copy out this process data. In this process data, what we're actually doing, let's add this cell here. In this process data, we're taking the image, we're taking the level, we're taking the image size, and then actually modify this image and have this and then take this level and pass it to the output since the level remains unchanged. Now for this image size, we wouldn't need this so we could take that off. We're just taking the image and the level size, and then as this input, we pass in the image. The tensor out is gonna be updated type flow32. The function is gonna be arc albument. So arc albument is our function. And then we are gonna make use of this tensorflow NumPy function. Now getting to documentation, we see it wraps a Python function and uses it as a tensorflow operation. That said, we could still work in the graph mode even though we are having this Python code right here. And this is because tensorflow is gonna convert everything that goes on here as a tensorflow operation. So we could run this. We have the process data that looks fine. We could run this too. And then finally, we have our train data set. So we run our train data set, batch size not defined. We should have run this here. Okay, we run this cell. And then getting back, we run this again. It should be fine now. Okay, so here we have train data set. We could look at that, and that looks fine. We can quickly visualize our data set. We have the image, the level, and then we have an element pick from our data set. So now let's im show this, im show im. Around that, we get this error because we are dealing with batches of 32. So let's just pick one of these elements. Let's run that now, and that should be fine. Okay, so here's what we obtain. Now we could have many more plots. So we have this figure. We define the figure size for i in range one to 32, one and a half, plot dot subplot, subplot, eight, four, and then i. And then plot dot im show our image, i. Okay, we run that. And here's what we get. So we have this, and now what we could do is, let's get this cut out, which we have here in the documentation. Specify a number of holes, maximum height size, maximum width size, the field value. Always apply false and the probability. So when we apply this, you're gonna see exactly or better understand all these different arguments right here. So let's go ahead and apply cut out, which should be more visible as compared to the other transformation like the rotations that we did. So yeah, let's have this year, let's, you see how easy it is now to integrate any data augmentation strategy you have. So yeah, we have a, is that a cut out? And then we take the default parameters. We have that cut out, let's run that again. And what we need to do, because the train data set has been modified. So we need to like get the initial train data set we had after getting from here. Let's get back down and then check this out. Okay, so we have done the transform, all couple moments, run this, run this, and now we could visualize. Okay, yeah, the cut out hasn't been implemented. That is, obviously there's a probability of 0.5. So it's possible that we don't have cut out here. Now, if you look at this, you see clearly that in some parts of this, let's increase the size. Let's see, 15. Okay, as we're saying, if you look at this, you'll notice that in some parts or in some images, we have this portions which have been cut out. So you could see, for example, this one, you have this part which is cut out and then getting back to the documentation, we have your default number of holes eight and we could copy all this out and then modify it so you see how this could change the kind of output we get. Because here you see, when there's cut out, we have one, two, three, four, five. Surely you have the other holes or cut out regions in the black spot so you can identify them. And then here you have one, two, three, four. You see, you even have this, looks like a cut out. Anyway, that's the idea. You specify the number of cut out, number of regions you want to cut out. You also specify the value is going to tick. So like in this case, when you specify zero, it simply means you're going to be having this black spots here. So let's have that back here. Okay, so let's put this in here and then you have number of holes. So that's it, you could modify this and observe exactly what goes on. To increase the size of the cut out region, you could simply specify the values here. And then you have the fill value. You have this Boolean, always apply and then you have the probability. So if you want to always have cut out, you could simply send this probability to one and that's how it works. So we've seen how this works. We could go ahead and train our model using this augmented data. But before training, let's take out the cut out as it was just meant to show you how this works. Okay, so let's, we run this again, run this and then we visualize. As you can see now, there is no cut out region. So that's as expected. That said, we'll move forward to training our model again. So we run that training process. So the other results we get after training for several epochs, you'll notice that the accuracy doesn't go up to 99% as it used to be before. And also with this validation accuracy, we even get better results. Here's the result we get after training for several epochs. Notice how here the accuracy doesn't get as high as it used to be before. So the accuracy we're getting now is, the highest value of accuracy we get is like 94.5%. And the validation accuracy we get in year, it's about that, like the highest we get in year is like 94.48%. And this is the accuracy versus epoch plot we now get. Also, after checking our implementations GitHub page, we found this solution to those problems we're getting here where we weren't able to make use of the cut out or rather we were able to make use of the sharpen and the random grid shuffle. So yeah, when we do this install, which you have here and then uncomment the sections, you should now have this working. Hello, dear friends and welcome to this new session in which we'll be building custom losses and metrics. The first method we'll look at is building a custom loss method without parameters, the next with parameters. And finally, we'll build a custom loss class. And then from here, we'll go on to build custom metric method with parameters, custom metric method without parameters and custom metric classes. Most times when building models and training them, all you need to do is to pick this loss function from one of these losses available in the documentation. Now, in a case where you have to build a loss function from scratch or a custom loss function in which defining the loss isn't as easy as just specifying this loss function name right here. So right now, let's look at how to build such custom loss functions. Here we're gonna have this custom binary custom. Let's call it custom BCE. BCE stands for binary cross entropy. So we have the custom BCE loss right here and then we'll define it just above this cell. Now we have our custom BCE loss. There we go. And here, what we're gonna do is we take our, we define this BCE object, the binary cross entropy loss object. And obviously here we have binary cross entropy. There we go. So we've defined this object of this class and then we return this computation of the binary cross entropy of the Y true and the Y red. So here, this Y true is the actual output levels and here's what the model predicts. You could understand this better by taking a look at the documentation where here you see the define this object and then in computing this binary cross entropy loss, you see specify this Y true and Y red right here. Now the binary cross entropy could take several arguments, for example, from logits, levels smoothing, the axis reduction and the name. So that's it. We have this custom binary cross entropy and just here, all we need to do is to specify this name. So let's take this off and we're fine. Okay, now we just run this. We run this binary cross entropy. We run this metrics not defined. Let's run the cell, run the cell and then finally run the training. We get this error. This function takes zero positional arguments but to a given. So we have to have this year Y true and Y predicted. Okay, we run that and run that. We could see here how this training process looks similar to what we've had already when we're dealing with a preconceived binary cross entropy loss. Now that said, let's suppose that we are going to pass in a parameter here. So we want to parameterize the way in which we calculate this binary cross entropy. That is want to have this binary cross entropy multiplied by a given factor. Let's say we want to multiply this by a factor of say 0.5. In that case, we could have this year as a factor and then we'll pass in this factor right here. So we pass this in, let's stop the training and then we train to see how that works. We run this year and run this factor not defined. We could include this factor here. We run it. Factor still not defined. And this brings us to the way in which we define models or rather define loss functions in which we have a given parameter. So right here, we're going to define a loss year, a loss function, and then we pass in our Y true, Y pride. So it's going to be like similar to what we had with a custom BCE without a factor. We have that. And then in year we have this BCE and then we return this right here. Now notice how we take this tool off. So we have in just this factor and that's what we're going to be passing here. And then here what we return is this loss method right here. So we define this custom binary cross entropy. We return this loss method and this loss method. We take in the Y true, the Y pride and we carry out the computation as we had done before. But now note that we're multiplying this by a given factor. So here we could define our factor. Let's have this factor and your factor. Then before this, we'll define this factor. Let's take this factor to be equal one because knock is actually equal one. But supposing you are having a different problem, you may want to modify this parameter as you wish. Let's have this factor here. That's fine. Now let's rerun this and hopefully we should have no error. So that's it. No error, that's fine. And then we run our training. As you can see, the training process continues normally. And we could now check out on another method of building custom loss functions, which is that of actually building a loss class, which inherits from the Keras loss class. So that said, let's add this year. And then we have our class. We'll call this custom BCE and then we'll inherit from Keras losses loss. Okay, so that's it. Now, once we have this right, we have our init method. And just like we did here, we're going to pass in a factor. So let's have this factor. Then we inherit methods from the parent class. There we go. We have our custom BCE, custom BCE, and that's it. Okay, so we have this right. And then we now go straight forward to creating this factor variable and take this value factor. And that's it for the init method. From here, we're going to define a call method where those operations are going to be carried out. Now, here we have this call method. And then what we take in is ytrue and then ypred. Okay, so we've taken this. We now do exactly the same thing we did here. We could copy this out and paste it right here. We have our BCE defined operation carried out. And here we have self dot factor. Okay, so everything looks fine. We have our custom BCE defined invalid syntax. There should be a dot just right here. We run that again, that's fine. Okay, so we have that right. And what we do now is take this off. So here we have custom BCE, which is this class where define how we just defined right here. Okay, now we should also note that you could pass in the factor here. We actually have to pass in the factor. That's it, because we didn't specify the default value. So we have to pass that. Now let's run this. That's fine. And then we rerun our training and make sure everything works well. We get in this error. The call takes two positional arguments but three were given. Let's get back to this and we see this error. We do not put in the self. Okay, so that's it. And let's rerun this. As you can see, the training went on successfully. Now we know how to build custom loss functions with tensorflow. We then move on to building this custom metrics just similar to the way we deal with a loss. So here we have a custom metric. We'll start with a custom method. So we'll take this off. And then just right here, we have custom metric. Let's add this and this. We paste this out here and have custom accuracy. We now get back to the documentation, tensorflow, Keras, and then metrics. So right here, you would have this accuracy which has been defined. Here we go. We have binary accuracy. So that's it. We have binary accuracy. We specify why true, why proud, and the threshold. There we go. We can paste this out here. This is the loss. Let's go back up to this metric. Okay, let's paste it out here. So now we have this accuracy which takes in the why true and why proud. So we just return, we basically return this. We return binary accuracy, accuracy, which takes in why true and why proud. Okay, so let's take this off and that looks fine. Now we have our custom accuracy, custom accuracy. We run this as well. We get to this compile custom accuracy, fine. We compile our model and we train the model. Everything looks good. You could see this custom accuracy. Now let's suppose we had a factor. We're gonna use this same kind of approach we had with the loss method. So we just simply copy this out. And then right here, we have this paste that. Then we have our custom metric right here. Here again, we have custom accuracy, custom accuracy. We're taking a factor. We have this metric, we change it to metric. There we go. And then we simply have this. So let's just copy this out and paste right here. So here we have this here, let's copy this out. Okay, so we have this and we now multiply this by the factor. So yeah, again, we call on the metric method and that's fine. We run that, we have this custom accuracy. Oh, okay. We haven't stopped this training yet. So let's stop this training. That's fine. We get back here and run this. Looks good. Compile and train a model. We're told this takes one positional argument, but two are given. So let's get back to this and then include this factor. Run that. And the training looks fine. We can see our metric right here. Our next approach will be the classbased approach similar to what we deal with the loss. So here we have this custom loss class. We'll build a custom metric class beside this and this code. So we have that and then we have our custom metric class. This class here is a subclass of the class metric. We have our class metric and then just right here, we have custom accuracy. So we have this custom accuracy and then yeah, we specify the name. Let's have this name. Let's say we want to put here custom accuracy. Okay. And then we pass the factor too, which by default was set to one. So that's it. Now we have custom accuracy. We have our factor. And then what we'll do is define this accuracy by calling on the add weight method, which comes with this metric class right here. So we have this add weight and then we specify the name, which is the name we've passed. We have the initializer equals zeros. So that's it. You'll notice that this will look slightly different from what we saw with the custom loss as unlike with the loss class, which has this call method, yeah, would instead use three other different methods. The first we'll be looking at will be the update state method. We have update state. Then next we will have result method and then the reset method. So we have this reset right here. The update state method here permits us update our metric state. The results permits us to output the metric values and then the reset permits us reset the metric states. So we have here reset states. Here we go. We start with this update state method. In this update state, we're going to be assigning a value to this accuracy variable. So just right here, we have self.accuracy. We assign a given value. Now the value we're going to assign is exactly the same we had here with this binary accuracy. So let's take this factor and then paste out here. We paste it here. Okay, so this is our self.factor. That's fine. So that's it. We assign this value to this accuracy variable. This method to takes in the Y true, the Y bright. And then with the result, we have a return. So here we have this return self.accuracy. That's it. And then here for the reset states, reset states, we are going to reset the state at the end of each epoch. So once we finish with one epoch, we reset the state and then reupdate the state and then output this result. So here again, we have self.accuracy.assign. The value is zero. That's fine. Let's run the cell. We get this error because this is actually metrics. So let's take this off. We have here metrics. We run that again. It should be fine. Okay, so we have our custom accuracy built. We now get into this year. So let's take this off now and specify our custom accuracy. Then here we specify the name. But anyway, we've got in default value. So we could just pass it this way. Let's run the cell and then we will start the training again. We get this error of this state, got an unexpected keyword argument, sample weight. Now we get into this error because year, we didn't put the sample weight as one of our arguments. So yeah, we should have the sample weights. We're not going to be using that. So we just set it to known. Let's run this again. Fine. We recompile and then train our model. We get this other error. The input Y of equal op has type float 32. That doesn't match type 64 of argument X. So this is actually an error which comes up because right here when competing this binary accuracy, we have Y true and Y paired of different data types. So let's print this out and you could see that. Y true and then Y paired. We run this again. Looks fine. We get down to this year, this compilation. Recall when we want to debug, we could run eagerly. So let's run eagerly, set that to true. And that's fine. We run this again and try to understand what's going on. So as you could see, that's exactly what we expected. We have this int 64. So this is an int and this is a float 32. We have the Y true. So let's, we could cast this year. So let's cast this value tf.cast. And then we specify the D type to float 32. Float 32, that's fine. And then just year we take this off. Okay, so we run this again. There we go. We now compile and then train. We get this other error, cannot assign value to variable custom accuracy. The variable shape and the assigned value shape aren't compatible. We get back to this year. And what we do is we're going to print out this output, which we assign to the accuracy. Yeah, we define this output. And then we simply take this out from year and have this as our output. And now we're going to assign this output. So since we now have the output, we could print it out so we understand exactly what we're assigning to the accuracy. Let's run this cell, that's fine. And then we get to this compile and train the model. Okay, there we go. So now we see exactly what we're assigning, but you'll notice that instead of just assigning a single value, we assign this list. And this is the reason why we're getting that error. To solve this problem, we'll also have to understand that this output actually corresponds to whether the model's prediction was correct or not for each and every element of the batch. So when we have a one like this, it means the prediction was correct, or the model's prediction was correct, and one and so on and so forth. So because we've trained a model for quite a while, you see that for all this, or for this particular batch, we have kind of like 100% accuracy. So instead of printing out 100%, what a model gives out is this list. Now let's look at this again. We see now that with this other batch, we have some zeros. So out of 32, we've had three zeros. And hence, our accuracy in this case is 90% for this batch. That said, we have to figure out a way of counting the total number of ones and then dividing by the total or by this length, which in this case is 32, or by the batch size, which is 32, and then multiplying by 100, or we could just let it like that. So we make use of this method, count nonzero, where here we could count the number of nonzeros values very easily by just calling it, and I'll be fine. So let's copy this here, and then instead of having this output, we'll count the number of nonzeros. There we go, we count that. And now we divide by the length of this output. So we divide by the length of the output. But this length is gonna be an int, so we're gonna cast this so we get a float. We have that, and then D type equals here, float 32. Looks fine. Okay, so now we have that, and then we could, let's take off this output, and then we run again. We then compile, and then train the model. We still get another error, so what we'll do is we're gonna just check in the documentation, and then here, what do we see? We see that by default, this is an int. So here, we could also cast this, we cast this into a float. There we go, we have this output. We don't really need to cast that because we could specify the data type. So here we have the output, and then D type equals TF, float 32. Okay, so that's it. Hopefully this should be okay now. We check, we compile the model, start the training, and everything looks okay now. We could now stop this and then get back to graph mode. So let's run again so that we could now train even faster. And that's it for this section. We'll see how to build custom losses and metrics. Thank you for getting around to this point and see you next time. Hello everyone, and welcome to this new session in which we'll see how to switch between the ego mode and the graph mode in TensorFlow. We'll first start by understanding what these two modes mean and also when to use either of them. Then finally, we'll see how to switch between these modes by simply adding this TF function decorator right here. So far in this course, we've been building methods like this one in the ego mode. That is, we've been following a Pythonistic approach in creating these methods. Apart from this ego mode or this ego way of manipulating data, we also have the graph mode. In the graph mode, we have data which is passed into these different nodes right here. Now, these nodes are operations. And so when we pass in our data, which could be a tensor, this data gets modified and then passed to the next node and right up to this output right here. This means that if we consider this line of code where we have X equal Y times Z, TensorFlow is capable of converting this into a graph with a single node where this node will represent this multiplication operation. So here we have this multiplication operation and then we have the two inputs, Y and Z. Then our output X. Let's draw it this way. We have Y, Z, the multiplication operation. Then we have X. Now this X, this data we get can be passed into two other operators or two other operations. So here we could have addition and here we have subtraction. Then we could combine this to finally have, say an addition operation and then get the output. Now let us come up with some code which will represent this rest of our graph. Here we've had X and then we could have, for example, R which is equal X plus a certain constant. So X plus a constant, let's call that constant key. We have X plus a constant key, let's just say constant. So this addition we have here is actually an addition to a fixed constant and that's it. So here we have R and then we could define S to be equal Y minus this same fixed constant. There we go, we have S. Now we have S. We could get an output by adding up S and R. So let's call this output T. We have T which is equal R plus S. Now this addition here is different from this one as this one takes in just one input. Since this input is gonna be added to the constant and the subtraction here takes in just one input as this input is gonna be subtracted or the constant will be subtracted from the input which in this case is meant to be X actually. Let's take this off, this is X because here you have, after this multiplication, you have X and this is X that gets in this and the same exact goes this way. So here it's meant to be X and that's it. So here we have now S. Once we get S, we have S here, this tensor S and then here we have our tensor R. Now R plus S gives us an output T. So we see that under the hood, what TensorFlow is capable of doing is taking this Python code you write and convert it into a graph like this one. One advantage of working with the graph mode is that this now becomes portable or this code now becomes portable since in a case where you don't have the Python interpreter, you have now this data structure which could be used in any environment. Hence making our code or this method more portable. Another advantage is that since we now dealing with this data structure, it can be broken up into several or separate blocks hence making it easy for parallelization leading to faster computations on devices like GPUs. The good news here is in order to convert this method, for example, into this graph, all you need to do is to add this TF function decorator. And when once this is done, when TensorFlow gets this block right here, it does what we call tracing. During the tracing process, this graph right here is generated. And once generated, each time you'll be calling this method, you now have your inputs passed into this data structure and your outputs generated without you going through each and every step in this method. Getting back to the code, all we need to do right here is to include our TF function decorator. And this automatically is turned to graph mode. That's fine. Run this. And then for this augment, we just add TF function and that's it. So there we go. Now right here in this Route 90 class, what we'll do is at this level of this call, that is when we compute in this Route 90, we will have this decorator put out here. So that's it. That's fine. We run the cell, augmented layers. Okay. Then this augmented layer method too, we have TF function. Now note that previously we had seen this Resize Rescale layers. So let's get back. We have Resize. Okay, that's the Resize Rescale layers. But if we were calling instead this Resize Rescale, and that we had already applied this decorator right here, then it would have been needless putting this decorator here. So in fact, what we're saying is if you have a big function and you have this function, say let's say we have A equal a function call to small function. There we go. And we return some value. We have that. Let's say we return known. Okay, so we have this function of this method. We have this method defined and then we have the small function right here. Then we could also, let's define small functions. We have small function and then we have return whatever value is in known. So what we're trying to say is if you have this decorator, this TF function decorator put out here, it's needless having it in this small function because once you put this, then all the methods which will be called in this method will automatically be converted to graph mode. So that's how this works. We take this off. Let's explore another interesting effect of working with a graph mode. So right here we have print. Let's say I was here. Okay, and then what we're doing here is we're going to call this method. We're going to call this resize method the first time. We have resize, rescale. We pass in the original image, original image and the level which we got from here. So that's it. We run this and we'll see we have this I was here. Now we'll repeat this several times. So let's have this. And repeating this, we should expect to have this printed out three times since we run this actually three times. Let's run this cell. And we see that this is printed out only once. Okay, that's what we notice. Now let's take off this TF function decorator and run this again. We see that this is printed twice. And the reason why we have this is because in the graph mode, what we have is a data structure, a graph data structure. So we have this graph, we have this nodes which have been linked to one another. And so when we write code, input output. So when we write code like this and it's converted into this graph format, the very first time we make a call on this method, we actually carrying out tracing. Now tracing permits us to convert this into this graph format. And then the next time we make this call, since in the graph mode, we basically storing this operations and the data that's gonna be passed in between the operations, we are gonna focus on only this portion of this method right here. So this print is not gonna be taken into consideration for the second time and for any other time. So that said, we could run this for as many times as we wish and we wouldn't. Okay, let's take this back to the graph mode. So this is the eager mode, taking it back to the graph mode, you see it's gonna be printed only once. And then you could convert all the methods we're using into the eager mode by making use of this, config run functions eagerly function. So yeah, we have tf.config.run functions eagerly and then we set this to true. So we're saying that we're gonna run all these functions now eagerly and then let's copy this out. So let's copy, let's say we take this three and then paste right here. You see, once we run this now, you see it's printed out twice. This will tell you that even though we have this tf.function decorator right here, all those methods now are run eagerly. We could also set this to false. So yeah, let's take this. If we have this false, we run it and there we go. You notice that we have nothing printed out and this is simply because the tracing has already been done. The next point we wanna make is in the case where you're working with the graph mode, then there is a specialized print you could use. So yeah, instead of using the print, some sort of this print for the eager mode, the usual Python print, you could now use this print which is specially made for working in the graph mode. So here we have this print here and you see that the output of this tf.print is different from that of the print because now this tf.print makes this method look like a normal method as it goes through this print each and every time. Let's come in this and have this. So you see how this is printed. You see it's printed only once whereas if we're having the tf.print it's gonna be printed twice. Then from here, always ensure that if you turn away in the graph mode, make sure all your operations are tensor flow operations. So if you are able to resize, for example, with OpenCV, it's actually a good thing but you have to make sure that that resizing or you have to try as much as possible to have that resizing done with tensor flow operations. And so instead of CV2, the resize, for example, is preferable to use this image resize method. And then in order to keep everything in this function right here based on tensor flow, we should avoid passing in Python variables into this method right here. So you should make this method depend only on tensor flow variables. In conclusion, we've looked at the graph and eager mode with tensor flow. How to leave from the eager mode to graph mode and vice versa. Thank you for getting right up to this point and see you next time. Hello everyone and welcome to this new session in which we're gonna go under the wood and understand exactly what goes on when we call on this feed method right here. In fact, we're gonna be training our own model without making use of this feed method. So you're gonna have this custom training loop which we're gonna build. As you could see here, we have this training block. We have this validation block. And then we have this method which we named neural learn. And this method which replaces the feed method which we used to get in. TensorFlow is not only a great machine learning library because it permits us build and deploy machine learning models. Its greatness also comes from the fact that it permits us build these models and train them without getting to know everything which is going on under the wood. So all you need to do to train a TensorFlow model is to specify that model and then make use of this feed method while passing in your data set which is made of the inputs and the output. That said, in this section, we'll see how to create our own feed method. That is, we'll go a step further in understanding how TensorFlow works under the wood. Recall that when you make use of this feed method right here, what goes on essentially is TensorFlow applies the gradient descent method to update this weight data right here. So all the status have been updated until the model converges. Recall that for given inputs X1, X2 and outputs Y1, Y2, Y3, our aim is to update this weights such that when we pass in this inputs into the model, we get an output right here which looks practically the same as this outputs. And to know whether this model outputs are the same or similar to this actual outputs, we apply or make use of a loss function which computes the difference between these two. And generally, initially, when you start training the model, that's when you start updating these parameters, there will be a great difference between these two. But as you keep on updating these parameters, we notice that the loss starts dropping. And so once this loss drops until it starts, it converges, we then stop the training. But recall also that the way we update this weights is such that we take the initial weight minus the learning rates, which is a constant we define times the partial derivative of this loss. Now the loss, which we've seen here, which we understand that is the difference between the model's predictions and the actual predictions. So here we have this partial derivative of the loss with respect to the parameter or the weight in question. And so if we want to update say this theta double prime to one, we'll simply use this same approach. Now the way tensorflow computes this partial derivative right here is by automatic differentiation. And so when you have this model with the inputs, as this inputs are being passed into the model in this forward pass right up to the output, tensorflow records this gradient or this partial derivatives. And this is done for each and every parameter such that once we get the output and we compute the loss, we could now get this whole partial derivative right here. That is the partial derivative of the loss with respect to each and every weight. And then each and every weight has been optimized or been updated via this formula. So in general, we can have theta, whatever. And then we have, let's say I G, we could have I comma J equal theta I comma J, minus linear rate times partial derivative of the loss with respect to theta I G. And so as we go through this forward pass, what tensorflow does is it keeps in mind the thetors and the gradients. Let's call this gradients the theta. So we have the theta and the theta which have been stored or which have been recorded as we go through this model that's in this forward pass. That process of recording this gradients here as we pass in our inputs in the model is similar to what we would have with a tape recorder where here you'll just need to speak in this mic, the information is recorded and then it could be replayed later on. And so here as information has been passed in this model, this gradients have been stored and they could be used in the gradient descent algorithm later on. Getting back to the code, we'll now see how to build a fit method from scratch. So here we have this, let's have for epoch in epochs. You see that now, unlike here where we just needed to pass in this number of epochs and TensorFlow does the job, here you really need to do much work on your own. Okay, so we have for epoch in epochs, what we're gonna do here is go through each and every batch of our data set. So we also have that for each epoch, we are now going to go through each and every batch. So for X, batch, Y, batch in train data set, let's have this tuple. So we have X batch, Y batch in train data set. What we're gonna do now is pass in this X batch into our model and then after getting the model output, compare that model output with actual output and obtain the loss. So here we have Y pred, which is what a model outputs. We have the model, here we've defined the model already. So we working with a sequential model we had built previously. Here we go, we have this sequential model. And then in this model, we're gonna pass our X batch. There we go. We specify that we're training. So in training mode, training equal true. Okay, so now we have Y pred and what we could do next is compute the loss. So here we have loss equal loss function, which we're gonna define loss function, which is gonna take in our Y pred and the models and the actual Y. So we can make use of this custom BCE right here. Now this takes in the Y true, Y pred. So we should ensure that we have Y true before Y pred. And here we have, we should have Y batch and then Y pred. Okay, so let's take this off and that's it. So here we're gonna use our custom BCE. We could take this off and then specify custom BCE. Now we have our custom BCE, we could go ahead and update this model's weights. But in order to update our model's weights, we need those gradients. The way we get those gradients is by recording or taping these gradients as we pass the input into the model. To do this, this part of our code has to be put in a particular scope. So here we have with gradient or tf.gradient, gradient tape as tape. We are gonna have this, let's have that. We're gonna have this two lines of code. Now the effect of doing this is we are recording the gradients in this tape or in this recorder. We could say this as recorder. And now all the intermediate values of tater and intermediate gradients have been recorded and now could be used in computing this partial derivative right here. That said, we can now obtain the partial derivatives. So we have partial derivatives equal the tape. So this, or rather equal the recorder because we changed that to recorder. So equal this recorder dot gradient of the loss with respect to all the model's trainable weights. And so this line of code here represents this operation that is computing this partial derivative of the loss with respect to each and every weight. Then we move on to optimize our model by going through this stochastic gradient descent or some other gradient descent based optimizer like the Adam optimizer which we've used so far. That said, we get back up to this compiler. We have this optimizer. Let's take this from here, copy that. And then what we'll do is we are gonna add a cell. Let's take this up. We have our optimizer here. So let's have this optimizer. That's it. Our Adam optimizer defined. We run that cell. Okay. And then what we do now is we have optimizer, optimizer dot apply gradients. So we use this apply gradients method in order to update the weights based on the gradient descent algorithm. In here, we have zip of gradients or rather the partial derivatives. So we are passing in the partial derivatives and the model trainable weights. So recall we have some untrainable weights actually. So we have this model trainable weights right here and the derivatives. Let's scroll. Okay. Let's have this here. This is linked. So this part is linked to this. And then for this, it's the whole algorithm, the whole gradient descent based algorithm which takes in this partial derivative and the model trainable weights, that's the Thetters. Obviously while defining the optimizer, we have already set the learning rate. So there we go. Then from here, we could print out our loss values. Here we could print out the loss. Print of the loss. Okay. So now we said we could now run this and see how this works. Let's not forget to specify the number of epochs. So let's have number of epochs equals say three, four stat. Let's have this epochs. And then we run the cell. We are told here that the model is not defined. So let's modify this and have a lunette model run that again. We get in this error where we told this expects the shape while what we get is this. Now the fact that we have this means that we haven't yet resized our training data. So let's get back up and then run this cells here for resizing. And there we go. Let's run the cells. Okay. We have that. And now it looks fine. Okay. So we have the cells run. Let's get back to our custom training loop. Okay. So let's run this. As we can see the training goes on smoothly. You have those loss values we should drop in and we run this actually for three epochs. So there we go. We have our loss, which is dropping. And then we could decide to print this out after a given number of steps. So here we could add this step and enemy rate, enemy rate. There we go. We could check out our free Python costs on neuralland.ai to understand this in case you're new to all those keywords. So there we go. We enemy rate. We have step. And then what we're going to do here is we're going to print our loss values only after a certain number of steps. So given that we have 689 steps, that's if our batch size is 32, we'll suppose that after say 300 steps, we're going to print this loss out. So here we're going to have, if the step modulo 300 equals zero, then we'll do the printing. So there we go. We have this print. And then for every epoch stat, we're going to print out training stars for epoch number. And then we format that. So here we have the epoch. Okay. Let's rerun this again. Here's what we get now. So we have this training stars, training stars, training stars. And then we have this intermediate loss values which have been printed out. Now, what if we include the validation? So here notice we finish like we train the model. And then after training for one epoch, we don't, we then get into validation. So for X batch, val, Y batch, val in val data set, we'll simply copy this out. So here we have this year. Let's paste it out here. Okay. So here we have the Y PRED and our loss vowel. It's still the same model, but your training is set to false. We have training false. We have Y PRED vowel. And then we have Y batch vowel. Here's X batch vowel too. And then we print out the loss vowel. So here we have validation loss. Okay. Let's run this cell now. While training is going on, we could include the metric. Here we have this metric, which is binary accuracy. Binary accuracy. Now while training is going on, we could deal with the metric. So here for this, for each batch we're working with, we are going to update the state. We are going to print out the result and then we are going to reset the states. So let's get it back to this part. And then just after this, that's for this particular batch, what we have here is we have our metric that update state, and then we pass in our Y, our Y batch. That's our Y batch, true Y, and then Y PRED. So once we update this metric, we can now print out this metric value out of this for loop. So after each epoch, we're going to print out the metric value. And so here we have print of metric dot result. There we go, print metric dot results. And then we should have here the accuracy is, there we go, accuracy is, that's it. Once we print out the result, we now go ahead and reset states. So we have metric dot reset states. And that should be good. So we reset the states and that's fine. We could repeat this exact same process for the validation. With the validation just in here in this batch, we are going to repeat the same step. So here we have the metric, let's pass it out here. And then here we're going to have metric vowel. Metric vowel, we update the states. You're going to use Y batch vowel and Y PRED vowel. Okay, once we update the states, we now do this same process here. So we print and then we reset the states. Let's have this year print and then reset states. Let's take that off. Okay, so we have here metric, vowel and then metric, vowel and that's it. Let's define metric and metric vowel. Here we go, we have metric and then we have metric vowel. So that's it. So here we see how to recreate this feed method that we have here. Now that we've seen this, we see the training is going out well. We see the training loss, the validation loss and that's it. But the training loss, we're getting years for each and every 300 batch. So let's take this off from here. Let's say we print this out after each epoch. So the training loss, we print it out after each epoch. And that's it. This means that we don't really need to have this again. Let's take that off. So we print out the training loss and the accuracy. This should be good. We now run this. And here's what we get. You see, we have the train loss. We have the train accuracy, validation accuracy. We've changed this to vowel accuracy. And then we have the validation loss. For epoch two, we have the same. For epoch three, we have this. For now, we're in the eager mode. So obviously this training process is gonna be slower as compared to if we're working in the graph mode. Now, what we're gonna do is we are gonna pick out those competition expensive units of this cell right here. Now, we know that this is competition expensive because here we have to pass in our input into the model, compare this output, compute the loss, get the gradients, optimize the model, update the metrics. So we look at this as one of those blocks. And then we have this other block. Let's scroll up. We have this. Let's take this off. So here we have this block. And then we also have this other block. Now, you'll notice that this is what we call the training step, and then this is the validation step. So what we could simply do here is create these two methods. So we have train. Let's call this training block. We have the training block. And then what goes on this training block is we take in a X. So you have X batch and then Y batch. Basically, we just copy all this. So here we copy this, let's cut that. And then we paste it right here. Okay, so here we have this training block. And then here, instead of having this, we just say for each step, we're gonna go pass through the training block. Pass through the training block. We pass in X batch and Y batch. Then we'll repeat the same process for the validation. Here we have this copied out. And let's copy this. Okay, so here we have the validation block, or let's say, wow block. Here we have the vowel block, which takes L batch vowel and Y batch vowel. There we go. We have the string block, validation block. Now, just here, we could paste out. Let's create our validation block. And then take X batch vowel, X batch vowel, Y batch vowel. There we go, we paste this out. So for this, we have that. Now, since we compute this loss in the training block, we wanna return this loss. So we return this loss. Okay, we have the loss returned. And then just right in here, we are gonna say loss equal this. So we call this loss, and now we could make use of it right here. And then we'll do same for the validation block. With the validation block, we take this here, and then we return our loss vowel. Now that sounds great. We should now be able to change this in our graph mode. So we have TF function. That's all it takes to convert this to graph mode, TF function. That's it. So there we go. We've converted this to graph mode. We can now run this. We're getting this error. Let's check on what's going on. Okay, let's run this now. We told on and then doesn't match any outer indentation level. Okay, yeah, we should put this to match up with this width. And that should be good. Let's make sure we don't have the same error here. So we have this four and that's it. Okay, so let's run again. And obviously we should have no problem. Now we could retrain our model and we should get faster training this time around. But still we're getting this error. Loss vowel must be defined before the loop. So we get back to the code. And what we could do is comment this section, which is for the training because this works already. So let's comment the section and then focus on the validation block. The first remark we have here is we've made an error of putting this for loop in this valve block. It's not actually a syntax error, but more of a design error. So it's better for us to have this valve block in this for loop instead. So that said, we're gonna cut out this from this valve block. And then what we'll have here now is for this. So we have for XY in the valve data set. You see, we're gonna pass this in here and then have this valve block. So here we have this valve block now. Everything looks fine. We should have that. And then let's now run the cell again. You now see how just recorrecting that error automatically solve that issue. And then we could go ahead and uncomment the section and we run our training. While the training is going on, we are gonna create this method. So this method, basically we're gonna copy from this. So here we should add the cell. We add the cell. And then we'll define this method, which we'll call train. Or let's call it neural learn. So we'll call this method neural learn. And our neural learn method, we are gonna take in a training data set. So we have train data set and then our validation data set. We could also get in the model. So we have the model. We could pass in the number of epochs. Let's have that year. So it looks like the model feed, which comes with tensorflow. So here we have the number of epochs. And then after the model, we have the last function, last function, metric, vowel metric. And that seems okay. So here we have that. We didn't have this call. So here we just have neural learn. And then we pass the model, the net model. We pass the last function. Last function. We pass the metric. We pass the vowel metric. Vowel metric. We pass the train data set. We pass the vowel data set. And then we pass the number of epochs. So all we need to do now is just run this and then our training process is gonna start. Getting back to the training year, you see how this training loss, you see we have the training loss. We have the accuracy. We have the validation loss and we have the validation accuracy. Let's stop this. And then we could even take off the cell. Let's take off the cell. Okay. And then we have that cell off. Let's delete that cell. And then focus on this. So now we have our neural learn, which takes in the model. So here, as you could see, we have this optimizer. We have the metric. Metric vowel, epochs. Let's get back to the optimize. Let's add the optimizer here. So we should have after this last function metric and then let's have optimizer. Optimizer. There we go. Now after the metric year, we have our optimizer. It was fine. Let's run this. And this loss function not defined. So let's scroll up and we have this custom BCE. So let's just put out the custom BCE right there. Custom, custom BCE. We run that. Training is now complete and you're the results we get. You now know how to create your own custom training loops and make them run in graph mode. Thank you for getting around to this point and see you next time. Hello everyone and welcome to this other amazing session in which we are going to see how to integrate TensorBot callbacks with TensorFlow. In this session, we are going to look at how to log in information from our training process or from our different experiments into TensorBot, how to view model graphs, how to do hyperparameter tuning with TensorBot, how to view distributions, histograms, time series, how to log image data like confusion matrices, RSC plots, and finally how to do profiling with TensorBot. In one of our previous sessions, we saw the importance of working with callbacks as they permitted us to modify certain key information during the training process and also store certain information and do certain modifications during the training. That said, we have this TensorBot callback right here, which we spoke of last time but didn't really get into. And so in this session, we are going to go in depth and see how to make use of TensorBot to visualize on a web interface vital training information. You'll notice also that this TensorBot callback is going to be used in a similar way to the way we had done with the previous callbacks. So yeah, we define the callback. And then just here, you see in this callbacks argument, we pass in the TensorBot callback right here. So let's go ahead and copy this out. We have this, we just copy from here. We've copied this clip bot. And then now we go ahead and see how to pass this in our training process so we see exactly how it works. So yeah, we open up this callbacks. You see the different callbacks we have created previously. And then now we're going to include our TensorBot callback. So just yeah, let's add this text and add a code cell. We paste this out. Then let's take all this off and simply go, let's run this cell, that's fine. And now let's move on to our training process. Here we're going to have the callback. So we have the callback equal TensorBot callback. And then we're ready to train our model. So let's check on this. We are getting an error, unexpected keyword argument callback. Okay, you should have callbacks. We'll run that again. Now while our model is training, you'll notice that there is this logs folder which has been created. And the reason why the logs folder has been created was because we actually specified that in this TensorBot callback call. So here we have this log directory argument. And we're going to store in some information or some training information. And this is training information that TensorBot will use to display very important training information on a web interface. Now you could click open here and you'll see that you have these two folders. In this train folder, you see you have this information for content in the training data, which has been stored here and information for the validation, which has been stored right here. The next thing we'll do is copy out this here, this command, and then run it just below. So let's get to where we have visualizations, like reduce this one. And we have the visualizations here. Now we're going to have this. We're going to see how we're going to replace these visualizations with our TensorBot visualizations. So it passes out. We have TensorBot, log, dire, path to logs. Then we created this log dire variable, here, which takes in our path. So let's get back. And then instead of putting our path directly, we just have actually this log dire. So we have the log dire. Now before running this, we are going to add this code cell and then load this TensorBot notebook extension. So here we have load extension TensorBot. We run this and then you see already loaded. We are loaded already. Anyway, we have that and then TensorBot now. So we run this and then we should expect to have some interface which contains all our log data. No dashboard active for the current data set. Check out on scalars, what do we have? Anyway, let's do this. Let's check on this. Let's have logs. Logs, run this again. Okay, and there we go. We see now that we have this interface, which pops up or what do we see here? We have this logs. We have both training validation. You could pick this out as you would pick only the training data. You could see only training data. And then here you have the scalars. The scalars basically stores all this information which we pass in here, like the loss and the metrics. Call that we had defined this metrics here. So we are going to get all this metrics information. So unlike previously where we had to manually do this step and do this step for the loss and the accuracy, now this is done automatically. So let's run this and then we will compare what we get here with what we get from TensorBot. You can see here we have this loss and then we get back to TensorBot. We have this level of scalars. Let's reduce this accuracy. Let's view the loss first. So let's reduce this. Okay, we check out, we have this epoch loss. Now this is what we get for the loss. Let's include the validation. So you see, you have this plot right here. Click on this, you could also expand. So you see, this is the plot we get. Now, what do you notice? You notice that it's exactly the same as what we had here. But this time around, we didn't have to write any code. All this information was automatically locked in this file here and TensorBot took care of the rest. So that's how this works, it's really very interesting. And it's a kind of tool we want to master how to use because when working on different machine learning experiments, you wouldn't want to always have to lock all these values by hand or manually, you want to have this done automatically. Now, one of the interesting point is, you have all the metrics here. You just have to select any one. So let's look at the accuracy which we've seen already. We can check out this. Let's click here, okay. So we see this accuracy and then let's scroll down. You see, you could compare it with what we have here. See, we have this zip around the, this should be the ninth epoch or let's say eight epoch. Anyway, let's come back and check here. Let's scroll up from here. So actually an interface in this other interface. So we have this, you see, at this ninth epoch, what do you see here? You see, notice on this here, you will have this name, trains, modad, value, what do we have, step, ninth step, the time, and that's it. Okay, so there we go. We see how we could plot all this automatically and then you could get at any point. So you could go through each and every point and then get all the exact values. So that's how we look at this. Let's reduce this. We could take now, let's say a precision. So you could see, monitor the precision. You could also check out the number of false negatives. You see how this, as you keep on training, the number of false negatives keep reducing. And then the false positives, what do we have here? False positives, that's loading. Why that's loading, let's scroll down and we have the loss. We've seen this loss already. We have, let's look at true negatives. That's loading true positives. Here's a plot we get for the true positives and the true negative. And in the section where we have this evaluation, evaluation, evaluations, practically the validation metric and loss that we're plotting against the number of iterations. So that's what we have here. It's just like the validation accuracy versus iterations. If you take off the train, you see nothing really changes here, but when you do this, you see all that goes. So that's our validation and we could monitor this and observe that the highest accuracy we have is 94.09%. No, it's 94.21%. Scroll down, we have precision. What's our highest precision here? It's 93.6, no, the value is 94.39%. As of now, we've been able to log this information just from compiling our model. So because we passed our loss and the different metrics, we're able to log the information and visualize it on tensile board. But there are other possibilities. That is, it's also possible for us to log information manually instead of just logging all of this information. So what we could do is log, for example, image data. We could log even this different learning rates here. So we are gonna log or we can log the learning rate values for each and every epoch. And this gives us that freedom to log in just any kind of scalar or quantity we want. So here we have this metric. We're gonna create a metric directory. This is gonna be logs and then here we have metrics. And what we're gonna do now is create this train writer because now we are doing this like manually. So we create our train writer. We have tf.summary.createFileWriter. And then in here we have our metric directory. Okay, so that's good. We have our train writer created based on this metric directory, which is gonna be in the logs. So in those logs, we're gonna create a metrics directory. So that's it. We have our train writer, we could run the cell. Then just in here we have width. So once we're done with this, with train writer as default, with train writer as default, we wanna have this logged. So we have tf.summary.scalar and then we specify that we're dealing with the learning rate. So our learning rate. And then we're gonna pass in the learning rate actually. So here we pass in the learning rate and then we pass in the epoch. So this is like, as you could see here, you have data. Look at this popup. You see here you have data and then here you have steps, or rather step. So you have the name of the scalar, you have the data and you have the step. You also have the description. So here we have the name, which is learning rate. We have the data, which is this learning rate. And then we have the step, which is the epoch. Now we'll have to do this, like with this, we have to put it in each and every one of this since either we get into this or get into that. But to avoid writing this twice, we just have to set a learning rate. So in here, we define this learning rate, learning rate. And then here too, we have our learning rate. There we go, we have the learning rate. And then out of this, we return our learning rate. Okay, so that's it. Now we return the learning rate and then we are also logging this data. So here we should change this and have learning rate. So that's it. Now we have the set and then we could go on and train. Before training, recall each time you want to log in this kind of data or this kind of custom data. First thing you do, you create your writer as you create this file writer. After creating the file writer based on a given directory that you set, you now go ahead and then put this in this train writer scope right here. So with this, we can run this now. We've run this already, we can run this now. And then since this is our shadow log callback, we'll have to add this in our feet. So let's go ahead and add it in this our feet method here. Let's take now just five epochs. So yeah, we have tensor bot callback and then we have scheduler callback. Okay, so we run this now and see what we get. That's training. Our training is now complete. Let's go ahead and check this here. We have our logs. You see, we have this metrics and we have this locked. Next step, we go to visualize. So here we have this tensor bot and then here let's do, anyway, let's run this first. So you'll see what we get. As you can see right here, we now have this learning rate which has been logged and which we can visualize. So you see, as we go from this epoch to this epoch to this, this, if we set this at zero smoothing in, we have this. Now, the reason why we, after this, we don't get any value is because of what we have here. So we sent the learning rate into this tensor and what we should be doing here is getting that NumPy. So we should have this learning rate right here, learning rate equal the learning rate because this is going to be converted into a tensor. So we have learning rate at NumPy and that should be cool. Okay, we run this again and that should work. And then to view or to have immediate response, let's set this to just one. So if the number of epochs is greater than one, then we would have the learning rate being modified. So let's get back again to training. This time around, let's see, we just have three epochs. Okay, this time around, we have this actual value locked. And so we get back, let's reduce this custom training loop. We get back to our visualizations. We will run this again. Getting back here, we see we start with this loss and then this drops after this second epoch right here. As you may have noticed, each and every time we run new training process, the previous values or the previous locked input data is being deleted. So what we could do now is we could modify this file name as this logs file we're using here, where this folder name now depends on the current time. So yeah, we're going to have date time, date time dot date time dot now. And then we get a string, string from the time and then format this output. So yeah, we're going to have percentage, the day percentage, the month, the year and then we specify the exact time. So you're going to have the minute and the second. Now let's go ahead and import the time up here. We have the time. So yeah, we import the time simply that should be imported. Okay, so we run that and that should be fine. Okay, we've imported the time. Let's get back. And then let's print out this log diet right here. Let's have log diet printed out. See what we get. See, we have this logs and this is actually on your folder. Now, if you print out this again, you see we're going to have a different folder. And this is important because each and every time we do not need to cancel our previous runs. From here, let's take this off and then set this as our current time. So current time, current time these equal this, take this off. Here's our current time. And then we have this plus current time. Okay, and then yeah, we'll do the same. We have this plus current time. Let's take this off and then have the current time here. Current time. If we get back to the straining, we can run this again. If we're now complete, as you could see here in this logs, we have this new folder created, which is dependent on the time in which we decided to do the training. And then in this metrics, we also have this. But what we want to have is actually just this one photo which contains a train validation and the metrics. So we should modify this right up here. So instead of having this metrics before, we should take this off. We should take this off and then add it later on. We have plus and then we add this. Okay, so we have this slash and then slash. Then we should take this now. So that's it. We have recreated this and then we will rerun again to avoid this kind of error. So let's go ahead and retrain our model. Let's have it to be, yeah, it's fine. Let's say two epochs and then we'll run that again. Okay, the training is complete. Now you'll see that if you open this up, you have train validation. This is what we had previously, but now we have metrics, train, validation. This is exactly what we want. We want to be able to lock all this into this one directory. And you'll now notice how we do not have to erase previous logs. So let's go down to running this again. We run this. Take this off. Okay, here we go. We have this information now locked. You see, we could take all this previous logs out and focus on just this log here. So let's have this one. This should be 0.5, same, n3.7.1. So we want to focus on this one, which n3.7.1. And okay, here we go. So we have this metrics, this train and this validation. For this, we are not interested in logging this. So that's it. Let's take this off now and then get back. So you see here, we have the learning rate. We have the epoch accuracy. We have this true negatives. We have the recall and that's it. So here we'll see now how to create this directories, which are dependent on the current date and time. Now the next step we'll be doing is how to actually do this logs we doubt or when we're doing a custom training. So let's get back to where we did this custom training loop. We had this custom training loop and then we have this feed method, which comes directly with tensorflow. So what if we now try to use, or what if we actually use the custom training loop and we do not have the possibility of just simply saying, okay, callbacks, tensorback, callback, and then the job is done. And what if we just have this custom training loop? In this case, we're gonna use exactly the same process we've just followed here. So we're just gonna create this file writer. So let's copy all this. And then just as we did here, actually, we're just gonna write in this scaler values and then create the scaler, put in the data and then specify the step. So that's basically how we're gonna function. Now let's get back to this custom training loop. We're gonna add this code cell. And then in here, we have current time as usual. We have now, let's call this custom directory. We have logs, current time, and then we have custom. It's called this custom train writer. Let's call this custom train writer. Custom train writer. And you can also define a custom validation writer. We could have that too. So let's have here custom, custom directory. And in here, we will specify also train. So notice that since you were using this feat, what we got was the immediate automatically, we automatically got this train and validation. So what happens in the background is these two file writers are created. That is the train and the validation. And we're just gonna do exactly that here. We have custom. And then let's say we have custom train directory. Custom train and then custom validation. Okay, so we have that. And then here we have custom validation. Now we specify our writer. We have custom train and custom validation. Then here we have custom train. Custom train, custom validation. Okay, so I think this is okay. We could now run this. And then let's copy out this code we had put out here in the section and the shadow. So let's simply copy this out. You see how easy it becomes when you have already done this. So yeah, you now have to say, instead of just only printing this out, you let's have this. So with our, let's get a name from here with our custom train writer. And then here we have with our custom train writer. Custom train writer. We have the loss. So we have the loss. Let's call it train loss. We have the training loss. We have the data. The data is now this loss here. It's this loss. So we have the data which is passed, which is now the loss. And then the step is the epoch we have here. So that's it. Okay, we've logged this. Let's now go ahead and log for the accuracy. So we'll paste this out. And then we have training accuracy. Accuracy. And then the accuracy. So we kind of paste out. We will see metric the results. Metric the results. Okay. Metric the results. And then we have the step specified. So this is for the training process. We could separate this block and that's it. Okay, so we've done this. We now simply copy this out and then do the same for the validation. So in here, instead of writing this out like this, we could simply put out your custom val. We have custom val. Take this off. Here we have validation. Validation loss. Here we have the loss, but this is loss of val. Loss of val. That's fine. We have metric val. Metric val. That's fine. We have validation accuracy. Validation. Validation accuracy. Now you have this too. Take this off. And then we have the val. Okay, so that sounds fine. Everything looks okay. We could run this here. So let's run this. We run this. And then we run neural learn. And then we start with a training. Training now complete. Let's go ahead and see what we have. You could check out this logs. You see, we have our values now locked in here. Custom, train and validation. So we have this locked. We now go ahead and rerun this tensor board. As you could see, you have all these values here. You could, as usual, let's take this off, toggle our runs. And then let's pick this very last one. So we pick this last one and also pick out this one because there's a train and this is the validation. Then we come right here and check out the training accuracy. Train loss. There we go. We have train accuracy. We have train loss. If we do this, you see, we have, okay, we have the train accuracy. We have the train loss. We have the validation accuracy and we have the validation loss. Now, if we want to take off all the information stored in the logs, we could have this command. So we remove all this information and that will specify the logs. So we run this and then open it up this, you see, you don't have the logs folder anymore. At this point, we'll go ahead and see how to display image data with tensor bot. So unlike previously where we've been displaying information like the loss, the different metrics, now we'll see how to implement or rather, we're going to display image data like the confusion metrics we had seen previously. Let's get back here. We have this confusion metrics right here. And what we'll do now is after each epoch, we are going to display this confusion metrics with tensor bot. That said, we're going to copy out all this code we used in displaying this confusion metrics right here. So we have this code. And then we have this log images callback right here with this on epoch end method. Then in this method, we are going to paste out this code we used in visualizing the confusion metrics previously. So here we have this level input right up to this. We have the confusion metrics based on the threshold. And then we are going to visualize this confusion metrics. But now since we're working with a callback, what we will do is at the end of each and every epoch, we are going to display this with tensor bot. Now that this is set, we are going to, for now we've actually just been able to visualize this, but how do we put this, or how do we make this work with tensor bot? What we're going to have here is we create a buffer. We have this buffer IO dot bytes IO. Here we have bytes IO. And then, so that's our buffer. We are going to save this image, the confusion metrics image in this buffer. So we have this PLT dot save fig, and then we save it in that buffer. And that will specify that the format should be PNG. So we have the PNG format and that's okay. So now we have this buffer. We've saved that into our buffer. The next step we'll take is create an image of this buffer. So from here, we have this image. Let's take this up. We have this image. We use the tensorflow image decode PNG method, which takes in this buffer. So we have our buffer get value, number of channels equal to three. That's it. We have this image. And then once we get this image, we then write this in tensor bot. So we have this image writer, which we've created right here, similar to what we've done already. We create, we use this create file writer. Let's modify this and have your image directory. So we have the image directory, and then we create this file writer, or rather we create this image writer. So from with this image writer's default, what we're going to do now is, instead of having this summary dot scaler as we used to have here, now we're going to use summary dot image. So you could see here that tensor bot permits us not only input or write scalers, but also images. And that's basically all we needed to do here. So let's have this and then run the cell. So we make sure we have this run, run this, we run this. And then we, okay, we have this log images call back here. Copy that. Now that's copied, we have to reduce this one. And then right here, we run this metrics and then compile and run this. We get this arrow where we said, where we have this value arrow, no step set. So let's get back to this call back. And then we specify the step. So right here, we have the step, step equal the epoch. So we run that again, and this should be fine. Training is going on and then the image data has been logged into tensor bot at the end of each epoch. Training now done, we could go ahead and then run this on tensor bot. Once training done, we could now visualize this confusion matrices on tensor bot. So we run this to cells and here's what we get. You'd see here step zero, step one, and then step two. This is because we actually run this for three epochs. So we could notice how, let's come back to the top. We notice how here we have 53. And then as we keep training, these drops to 35. And then finally here, let's go down a little. Finally here, we have 240. So this tells us that the last epoch wasn't helpful in improving the number of false negatives. You could see also even with the validation that here we had 16 false negatives, eight false negatives. And then this value rose up to 111 false negatives. So it's kind of similar to what we have with the test data, which is exactly what we're logging in the tensor bot. And now that you know how to log in image data with tensor bot from the example on the confusion metrics, what you could do is log in directly this ROC plots. You could also log in data like this one right here, where on the test data, you're going to put out the actual value and what the model predicts. And so that's it for the session on logging in image data. Now let's move on to visualizing model graphs with tensor bot. To visualize a graph, we're going to rerun this command to delete all the logs we've stored so far. And then we run this chance about callback once more. So we have that and then let's get back to metrics. We run this and that's fine. Now we have the training done. Let's go ahead and rerun this again. So we run this two cells again. And as expected, here's what we get. So yeah, we have the skillers. And then if you click on this graphs here, or to have this graph coming up, let's reduce this slightly. Okay, so we have this, notice that the graph tab here is up graph. And this actually means we're viewing the graph from the operations level. If you could come right here and zoom in, you see you have the conf layer. You have the conf layer again, dense layer, dense layer, and then zoom in this other way. Here you see you have the Adams optimizer. Let's double click here. Where you have this plus you double click. And then you get to see exactly what goes on in this Adam optimizer. Let's reduce that, zoom in, and then let's double click again to reduce this. Okay, so here you have that. And for this dense layer, you could double click. You see, you have that. You see, you have this kennel, you double click to better understand what goes on. And as you can see here, we have this regularizer. So it's a regularizer we had defined previously. Now let's reduce this by double clicking. Let's scroll here. We double click that, reduced, and then double click this. So basically it's hands to hands are barred. We able to visualize exactly what goes on under the hood when tensorflow creates this graphs, which in turn permeate us do computations even faster. Now, another way we could look at this is by coming right here, the stack and selecting Keras. Once we select this Keras, instead of having the operation graph we had just seen, we now have this conceptual graph. We'll see that this is going to be quite easy compared to our easy to understand compared to what we had done previously. So yeah, this focuses on the Keras model. We had built a Keras sequential model, we had specified. And here what we have is, you see the input. So unlike previously where we had stuff like the Adam Optimizer, the different metrics and the say loss computations with the up graph, here we have just the Keras model. So here you have the input com batch normalization, max pooling, drop out com batch normalization, max pooling, flatten, dense, batch norm, drop out dense, batch norm and finally dense. So that's our conceptual graph. And that's it for the section on graphs. You see, you get to understand exactly what goes on under the hood, thanks to this visualization or better to this graphs visualization made available with tensor board. Now, as we go ahead with building these models and then training them, you may sometimes wonder why this value six, for example, was picked, why the kennel size three was picked, maybe why the 16 was picked year, why 100 was picked, why not say 32, why not say a value like 100 year, why others values were picked. And then looking at the drop out rate, what makes us pick a given drop out rate, what makes us pick a given regularization rate and so on and so forth. Now, although in this particular case, we're building this model based on the Lynette model, which is some like a model, which has already been built and tested, we'll see another technique known as hyperparameter tuning, where we'll be able to select the best values for these different hyperparameters automatically. Now, this hyperparameters will not be only this, which we've picked out here. We could also have hyperparameters like the learning rate, like even the choice of this optimizer and so on and so forth. And this way of deciding this best parameters for our model and model training is known as hyperparameter tuning. Now, we're going to see how to do or carry out hyperparameter tuning with TensorBoard. So here, first in first, we carry out those imports. We have from TensorBoard plugins, HParams, hyperparameters. We're going to import API as HP. Then right here, we're going to redefine this model or we're going to restructure it. So instead of having just the regularization rate drop our rate given to us like this, what we're going to have here is here, for example, instead of having this drop our rate, we'll have HParams. And then we'll have HP drop out. And the aim of this process is to ensure that if we have this, suppose we have this model here, and then after training, we have an output accuracy, we want to be able to modify the parameters which make up this model and see how they affect the accuracy such that we are going to pick out the best parameters or we're going to pick out the parameters which permit us get the best possible accuracy. So that said, instead of having the drop out, we just have this variable drop out. And then for the regularization, we're going to do same. So you're going to have HParams. Instead of regularization rate, we're going to have this. And then we have HP regularization rate. Then apart from this regularization rate and drop out hyper parameters, we're going to include just right here. Let's start with this one. So let's copy out this. We have HParams and then we have HP number of units. So here we have number of units and then we'll call this one because just after this, we're going to have number of units too. So here we are going to be able to pick out the best possible values for this hyper parameter here. So let's have this here too. Okay, so that's it. The other hyper parameters will be included when doing the model compilation. So let's include this model compilation and then we're going to carry out the training in here. So we have this, let's take out the summary and then take this back. Okay, so that's it. Now we have this model compilation right here and then we have the optimizer. But what we're going to do here is we're going to fix this optimizer. So we're going to have the item optimizer and then we'll specify the learning rate to be HParams. And then we have your HP learning rate, learning rate. Okay, so there we go. This looks fine. We have the last binary cross entropy metrics accuracy. And then we do the model training. So once we do this model training, we are going to get the accuracy that we're going to train the model. We're going to evaluate the model. We obtain the accuracy and then TensorFlow will permit us modify all these different parameters such that we now get the parameters or the specific values which permit us maximize this accuracy. So here we're going to create a function which is going to return the accuracy. Let's have this to the right and that's it. We have this method which we'll call model tune, call it model tune. And then it's going to take HParams. So it's going to take this HParams and then it's going to like take different value for HParams. And then based on the accuracy, we're going to know after this different training steps which values for this hyper parameters best, gives us the best results. So that's it. We have that. And then we return the accuracy right here. Then we're going to define the range of values this different hyper parameters can take. So like this one, let's copy this out. Let's say we want to get this HP number of units one. Paste this out here. We're going to have hp.hpparams. There we go. And then we have the name specified. So we have number of units one. And we also specify now this range. So we have hp.discrete. So let's have discrete. And then let's say we want to pick out those values between the range, say 10 to 100. Now let's take this powers of two. So we have 16, 32, 64, 128. Okay. So that's it for the first. And then for this next one, we have two right here. Let's have that. And yeah, units two, there we go. We have that fine. Let's copy this again. And then we repeat the same for the dropout. So here we have dropout, take this off. And then here we have dropout. For the dropout, we're going to take values between 0.1, 0.2, 0.3 and 0.4. Anyway, let's say we want to take between 0.1 and 0.3. And then from the dropout, we have the regularization rate. So make sure you see that we've taken typical values. So your typical values for this number of units will be this and for the dropout will be this for the regularization, regularization rates will have different range of typical values. So here we will have 0.00, yeah, say 0.001, 0.01 and then let's say 0.1. So that's it. What's next? We have the learning rate. So let's copy this out, HP learning rates. Here we have one E negative four, one E negative three. Okay, so that's it for the learning rates. And that should be all. So we have nothing left actually. So that's fine. So there we go. We've defined all those different ranges. So we're going to search in this range, search in this range, search in this range, search in this range, search in this range. Now, notice that this is actually discrete value. So it's not like we're going to pick between this value and this value. We're just going to basically pick this value or this value or this value or this outer value. And now to perform this grid search, we are going to go for number of units, one in this range that we've specified here. So we're going to have in HP num units one, there we go, dot domain dot values. Here we have values, there we go. And then we're going to do the same for this. So we have for num units two in HP num units two, HP num units two, dot domain dot values. And then for the dropout rate and then the regularization rate in the regularization domain, learning rate in the learning rate domain. We are now going to create this HP RAMS dictionary. So we have HP RAMS right here, which is going to be our dictionary and it's going to contain the different values here. So we will start with this HP num units one. And then what we're going to put in here is this value we have here. So the value we pick from this domain is what we're going to pass in here. So we have num units one, one, there we go. We now repeat the same process. So we just have your two and then the rest. And there we go, we have the remaining parameters. And then from here, we are going to create a file writer. So for each run, because here we're going to have different runs for the different values. So for each run, we're going to define the file writer. So here we have the file writer, TF summary, create file writer, just as we've seen already. So here we have the file writer and then we specify this directory. Now we want the directory to have different names based on the exact run number. So yesterday we want to have run number here, which initialize to zero. And then we have this. So let's have this logs slash, let's just put in the run number in your string. We have the run number. Okay, so that's it. We have that and then after each and every run, we want to increment this run number. So yeah, we're going to increment the run number, run number equal, plus equal one, plus equal one. Okay, so that's it. Now we've defined this file writer. That looks fine. Next thing we want to do is with this file writer as default. So we have this default as default. We'll now log in the current hyper parameters. So we have hp.hperrams and then we pass in this current hyper parameters. Recall, we are going to sweep through all these different values and then for a particular run, we want to know exactly what we're passing in. And that's what we're going to pass in here. So we have hp.hperrams, that's fine. And then once we've notified Tensor Bar that this is the parameters or the hyper parameters we're working with, the next thing we want to do is to pass this hyper parameters in this model tune method right here. So here we're going to pass in now this hp.hperrams, this hp.hperrams which we've just defined here, which is a function of these different values we are going to be sweeping through. So here we're going to have a model tune, model tune, which is going to take in the hp.hperrams and then it's going to output, like from here it's going to output this accuracy. So here we're going to have the accuracy. So we have the accuracy, which is that. And then once we get this accuracy, we are going to log in its value. So we have tf.summary, scalar. So log in the scalar, accuracy, accuracy, and then we have the accuracy value logged in. So that looks fine. We have this run number. Why do we have a red here? That's actually okay now. So let's get back to this. Now we could run this cell. We run this cell. Everything looks fine. And then we now go straight into this hyperparameter tuning where we are going to pick out the best values for our hyperparameters, which actually give us the best accuracy. So for each step, now we want to let's say print out this. So we'll print out the hp.hperrams. So our hp.hperrams is, so we have hp.hperrams, there we go. And then we also want to print out the current run number. So what we're going to have here is the hp.hperrams are for the run, let's specify the run number. Our hp.hperrams is that, so our hp.hperrams is this. And then we format, so we have that. And then we have the run number, there we go. So let's have this here and then I'll run this cell. So here's what we get. We also modified this code here to contain each and every value in this hp.hperrams. So we get this kind of output and we make this run for like, let's count this, for like 287 times. So we have 287 different runs. And then from here we run the cell and then run this too. So from here we have the scalars, you have the different accuracy to get. Anyway, we're not very much interested in this for now because we're interested in looking at this hp.hperrams here. Now let's start with this table view. You could see here that with this table view we could see the different values, the different hyper parameters we made use of or we tuned throughout this process. So you could see, let's scroll up a little. Here you see we have the regularization rate, number of units, drop out rates. And then let's scroll down again and then go this way. You have, okay, so we have number of units too, learning rate and then accuracy. So you see here that the accuracy changes based on the different values we have for this different hyper parameters here. Now, another better way of looking at this is this parallel coordinates view. As you could see in this parallel coordinates view, you get to like pick out the highest accuracy value. And then if you follow this path, you get to see that taking a learning rate of 0.0001 and then here, let's just look at this part. So we see, notice this red part here, which you can actually turn to green by clicking on it. So you see it turns to green, you have 0.0001 and then the number of units too, it's like around 32. Here we have drop our rate 0.2, number of units one, 32, regularization rate 0.01. So these are the best hyper parameter values we have and we can now make use of this information to better create our model. So now instead of having a model like this, so now we will modify this and then instead of having, let's get back to what we had up here. So what we're saying is instead of having this, here where we had 10, now we could take this to 32 and then right here to 32, then we could pick the regularization rate to 0.01. So I have the 0.01 drop out rate. We didn't take, we didn't not drop our rate. Let's get back to that. The drop out rates, 0.1, no, no, no, 0.2 actually. So drop out rate 0.2, that looks fine. Learning rate 0.0001, so that's it. So we now see how to better pick these values thanks to this hyper parameter tuning, which we can do easily with tensor board. So let's get back to this and then we have 0.2 right here. Now we should be noted that the method we've used so far is grid search, that is we've, specified those different ranges and then we've some sort of search through those different ranges and then we've gotten the best possible parameters which maximize the accuracy value. Now another very popular technique is the random search. With a random search, we'll just define a range and then pick our random values in that range. So unlike the grid search where we have some defined or predefined values which we have to search through, with a random search, we kind of like pick values at random. But then they both have the advantages and disadvantages. And one thing you could take out from this is that with a grid search, since you have a fixed range, so you have a fixed range of values which you can pick from, it means if this fixed range of values is very large, it becomes problematic as this process of searching is going to take a very long time. So you may take a very long time before finding your best hyperparameter values, which we connect. Whereas for the random search, since we're not going in a particular order, it happens that we may even get the best hyperparameter values after just one step. So after just one run or a few runs, we may already get the best hyperparameter values just like we may get these values after 200 different runs. So if you have enough computation power, you could make use of the grid search since you're more sure of the different values you're going to be picking from. And if not, the random search will be a better option. Another very important aspect of TensorBoard is TensorBoard's profiler. With TensorBoard's profiler, we're able to evaluate the TensorFlow code and based on this evaluation, modify this code to ensure that it runs as efficiently as possible. That said, to make use of the profiler, we're going to start by installing this TensorBoard profiler plugin. So we have your pip install this, you run this cell. The profile plugin now installed, we now go ahead and run the cells. Right here, we're going to have this profile batch. So notice that we have this profile batch right here. And this value specified here has to do with the range of batches to be profiled. And so here we could take say 100 to let's say 132. Okay, so that's it. So we have that said, we are now around the cell too. Looks fine. And then we could start with the training. We have this TensorBoard callback which has been added here. So we run this to start with the training. Training now complete. We go ahead and run these two cells. Just as expected, we have the scalars, we have the graphs, we have these distributions, we have these histograms, time series. And then in here, we have this profile. And you're in this overview page. So you have different pages in this overview page. We have this performance summary. We're given the average step time, all the time, compilation time, output time, input time. Notice how this input time is relatively larger than the others. And you'll see that, or you'll notice that here, the input time occupies the largest part of the step time. So that's why you have a recommendation to first focus on reducing this input time. And then we have 7.5% of total step time sampled is spent and cannot launch. It could be due to CPU contention with TF data. In this case, you may try to set the environment variable TF GPU trend mode to GPU private. Then we have 6.6% of the total step time sample is spent on all other times. This could be due to Python execution overhead. Only 0% of device computation is 16 bit. So you might want to replace more 32 bit operations by 16 bit operations to improve performance. So this actually means we could make use of mixed precision training. Now we're going to look at mixed precision training in subsequent sections. We also have this other tools we could use for reducing the input time. This input pipeline analyzer. You could click here. You have this input pipeline analyzer. Now notice that you have these different tools here. So we have the overview page. Let's go back to overview page. And then here we have this input pipeline analyzer. Let's scroll down here. We have the input pipeline analyzer. We have this TF data bottleneck analysis. You could click on this. You see we have this TF bottleneck analysis. And then you let's get back to the overview again. Scroll up. And then you have this trace viewer right here. So you would also find this trace viewer in here. You could scroll here and you have the trace viewer. Now get into summary of the input pipeline analysis. We get to see exactly the breakdown of the input process and time on the host because we've seen already from here that the input processing time is kind of taking up close to 68.4% of the total step time. So here you could see we have the data processing, which is like the main reason why this input processing time is that large. And we have this different steps we could take. So what can be done to reduce above components of the host input time in current data? That is, you may want to combine small input data chance into fewer of the larger chunks. Data processing. You may increase number of parallel calls in the dataset map or preprocess the data offline. Reading data from files in advance. Reading data from files in demand. Other data reading or processing. And then here we have a more detailed input operation statistics. So let's click on this. Scroll up and then you could have this statistics given to you right here. So as you could see, we know exactly why our input processing is taking up much time. And based on this different suggestions, we could reduce this time. Now from here we have the kennel stats. Then we also have this memory profile. And then we have the part viewer. From this part viewer we have the TensorFlow stats. And then here we have this TensorFlow data bottleneck analysis. You could also look at this from here. You see we have the root prefetch. Look at the self duration here. 10 microseconds, 36 microseconds. See this one now is very large. So here's our bottleneck, a level of the mapping and batching. Shuffling, just 118, 373. Prefetching and so on and so forth. So we now know that the problem comes from the mapping as we have seen previously. Now selecting this trace viewer, we have this year. And we'll make use of this year. You could carry this around by clicking on this. And then you have the arrow. You have this to pull this from place to place. You have the zoom. So you have the pan. You have the zoom. You have this timing. So let's click on the zoom and then you see click. You click and you drag to the top. You could zoom in and zoom out. Now you notice the string. We have values from 100. Like let's get back to what we had defined previously. Here we have 100 to 132. And that's why you notice here we have the string from 100 to 132 to. Getting back here, you can zoom. See you have that. And then you could click here on the pan and then you pull this to one side. Now stopping right here, you could zoom again. And then you get to see all those different operations carried out during a single process. All those different operations carried out during a single training step. And with this timing tool you click on this timing tool. You can be able to like let's zoom this again and zoom and then drag this here. Zoom again. As we're saying measure the timing for a given operation. So once you click here, you just click and then you see you drag and you can measure the timing for different operations. You see you have the time here. 166.5 microseconds. And that's it. We now go on the distributions. Here we have batch normalization, batch normalization. Let's check out on this comp 2D. You see that different biases and weights. You see the kernel here is the weights have values which fall under this zone. The values fall under. The values are between 0.3, negative 0.3, 0.3. For the biases between negative 1.2, about 0.4. And then for comp this dense, we have this other dense layer here. This comp 2D2, which you could see here. See the range of values. And then we have the dense 1. We have the dense 2. We also have the different value ranges for both the kennels and the biases. So that's it for this distributions. We have the histograms too. You could check this out. Time series. And the way this time series is kind of similar to information we've seen already. So here we have this all. We could select just the scalars. So here we have the epoch loss. We have the different evaluation accuracy and so on and so forth. Now click on images. You select only images. Nothing to be shown. See that's why you have no information here. Click on histogram and then you have the histogram data we've just seen already. That's it for the section of TensorBot. TensorBot has other functionalities which we shall explore subsequently. And thank you for getting right up to this point. Hello everyone and welcome to another amazing session in which we are going to see how to work with weights and biases and integrate it with our already existing TensorFlow code base. Weights and biases help practitioners in experiment tracking, collaborative reports, dataset and model versioning, interactive data visualization, and hyperparameter optimization. It's trusted by over 100,000 plus machine learning practitioners around the world. In this session, we are going to focus on experiment tracking. It's one thing to build a model, train this model and evaluate it on a given dataset. And at least as we've seen throughout this course, this is pretty easy with TensorFlow. When we work in large teams and we have to collaborate, we have to produce reproducible results. We need to debug those ML models as a team and we also need to enforce transparency. Then a machine learning operations platform like weights and biases becomes indispensable. Weights and biases permits us to build better models faster with experiment tracking, data versioning and model management. As of now, the different products which Weights and Biases offers to us are experiment tracking, reporting that's producing collaborative dashboards, artifacts, dataset and model versioning, just like how you would do code versioning in Git, interactive data visualization, and hyperparameter optimization. Also, you could see here that this has been used by modern 100,000 machine learning practitioners around the world. Some key aspects of the Weights and Biases tool are the fact that you could integrate very quickly. You see that with Keras. So here we're supposing we are building a Keras model, a TensorFlow Keras model, and that all you need to do is to import this 1DB callback right here, start a new run as it's given here. So if you have some configurations, you set those configurations and then in the place of the callbacks, you simply pass this Weights and Biases callback which we imported right here. So it's quite easy to integrate with already existing frameworks. Now you see with any framework, you just need to do 1db.log and you could log any information you want to log. We could also visualize useful information very seamlessly and then we could collaborate in real time. So if you're working on a project, you could all as part of the team discuss the project's progression and see how to eliminate any bugs or any problems. Now Weights and Biases is designed for all use cases. So here we have a practitioner that's supposing just a single person. You have this dashboard, central dashboard. You see this hyper parameter sweeps artifacts. You could do dataset and model versioning just like you would do with GitHub code reports to share updates very transparently. Throughout this course, we'll look at these different products and in this section, we'll focus on experiments tracking. We're now going to go straight forward into signing up. So we click right here. We want to sign up with GitHub. Click here. And then we authorize 1db to get access to our neural learn account. So authorize 1db. We have the full name, organization, neural learn, and that's fine. Okay, so I agree to the terms and conditions. You could always read out the terms and conditions very carefully. And then from here, we also have to put in this username. So let's say neural learn. Okay, so we continue. How often do you train models? Let's say every week. So we have that and then get started. Here is now a homepage. Here you could create a new project. You could modify a profile, invite your team. You have this documentation. Click right here. Docs.1db.ai with the different guides, references, which you could always make use of in case you are having any difficulties or as a starter, you want to master how all those work. Then you also have this fully connected right here, which brings ML practitioners together. Now here you have curated tutorials, conversations with industry leaders, deep dives into newest ML research and a whole lot more. So you would always have this information or this curated information at your disposal. Then we also have the community and then we have this quick start for the different frameworks. So we could view all frameworks here. Getting back, you see we have PyTorch Keras. We're working with Keras. Click on Keras. You would have this quick start for Keras users, which we actually. So here you see how easy it is to get started with 1db in Keras. The very first step will be to install and log into 1db. So we'll simply copy this and then get back to our Clap Notebook. We paste this out right here and then run this cell. We have the syntax error. Let's have this and then we run. There we go. As you can see, 1db has been installed. From here, we are going to get into the login. Now you see you can find your API key in your browser here. So we could click on this link, get the API key and then paste it out here. Now let's get back to this and then copy out this API key. There we go. We have that and then we paste it out here. So we paste it out there or you could click on this link and then still get the key, which we can paste in here and then simply press and enter. So we hit enter and we should be able to log in since we have this key put in right here. Now moving on to the next step at the top of your training script, start a new run and to start this new run, we are going to make use of this init method. Getting back to the documentation, you have your run is a unit of computation locked by 1db. Typically, this is an ML experiment. So we create a run with the 1db init. So before moving on, you should note that you could create a project and in this project, you have several runs. Now one run could be for training. So we could have a training run like some sort of ML experiment as defined here and then we have evaluation and you could have other different runs or other different training processes which will act as different runs. Here, as you can see, when you just import 1db, the run is known and now you have 1db init. Once you make a call on this method, you have a run which is automatically created. And so everything you're going to log in into 1db will be sent to that particular run. So if you have say this project, let's call this project malaria prediction project. If you have a malaria prediction project and then we have this training run, everything like once we create this run, everything we log in will be sent into this particular run right here. And then if we create another run, that's evaluation run, everything we log in will be stored in this particular run here. So getting back here, you see you could create this run and you could finish or you could stop that run. And that's why once you have this init, you've created a run, you stop the run and then here normally this should be known as there should be no run going, no run created. Now, after doing this, you could also create this like here, like 1db init, you could put this in a with block. So we have with 1db init as run, then you now have all the data to be logged in here such that out of this with block, you have no run. From here, you could check out on the different attributes and then the information related to this attributes. Now let's get into this 1db init. 1db init you have as definition with all the different arguments it takes and then you have this information concerning all those different arguments. So here we could define like we have this run, which we could create by simply doing 1db init and then we'll specify the project, specify the entity, I would say neural learn, the project malaria detection, the configuration, all the information will be needed. Let's say for training, we specify a safe code to permit 1db safe code. By default, this is actually false. So by default, your code is not going to be saved to 1db. You could check this out here. Let's have safe code. Let's search that. Save code. Go up and there we go. So by default, we don't allow this and you could flip this behavior by going to the settings page. Now you could check on all these other arguments. You have this job type argumental, which is also very, very important and this is very useful when your grouping runs together into larger experiments using this group argument right here. Now that said, let's copy this part of this code and then get back to our notebook. We will notice that we have this import. So we would have to finish, put out this import here. Let's have this. We have import 1db from 1db Keras, import 1db callback. Let's take this off. So we put this tool here and then we run this cell. That should be fine. Now let's run. We scroll down. We have the model. Let's run this again and then let's actually get back here and create this run. So let's have this here. So 1db init. We're going to specify the project. We're going to create this. So this is 1db install login and install login and initialization. So we have that. Now, as we've said already, this is going to permit us create a run. So here let's have malaria detection, malaria detection and one way the like to write this out, like prefer to put this hyphen instead of the space. So let's have that that way. So we have your malaria detection, entity in neural learn and let's have that for now. So let's run the cell and then let's add this code below here. Currently login is neural learn and that's fine. From here, let's now do 1db run. See what we get. From here, we see that we have no metrics locked yet. So that's fine. We could now take this off and then go straight away to add our 1db callback to tensorflow. Before that, we have saved model inputs and hyperparameters. So 1db config, let's have this copied and then we have that before defining the model. Let's copy that and then just here, we'll add this code cell again and then let's have your initialization configuration and there we go. Okay, so we have that. 1db config learning rate specified, number of epochs, batch size. We'll include the dropout rate, the image size, regularization rate, number of filters, canal size, number of strides, pool size, number of outputs for the first dense layer, number of outputs for the second dense layer. It was us finally now around the cell. So we've now started this configuration. Then the next and last step is to simply have this callback in this fit method. So let's copy this out and then get back. So we have that, we've run our model. Let's be sure we've run this model and just here, let's have this configuration. Let's say we have our configuration, configuration equal 1db.config. And then right here, we have configuration and image size. And we have same for the other hyper parameters like the dropout rate, regularization rate, number of filters, canal size, pool size, and number of strides. We now go ahead to the training process. So we copy this out and paste just right here. So here we would have this callback. Let's take this off. Instead of this TensorFlow, TensorBoard callback, we now have this 1db callback. So we have this 1db callback. And you could always even include the TensorBoard callback. So you could also have the TensorBoard callback we had previously. Now there we go. We have this 1db callback. Let's have that. And then we run this cell for metrics. We compile the model. All the models compiled. Let's include this learning rate. So let's have this configuration and then specify the learning rates. So we have that learning rate there. And then we compile the model and start with the training. Now the training is complete. We could go to our weights and bias dashboard right here and see exactly what went on during the training process. So here we have these projects. You could see in your learn, we have the projects. We click on this malaria prediction project. And then we select this run. The run we selected here is the sandy water run. So there we go. Now you see we have 19 different chats. AUC, validation loss, false positive, false positive. Well, this is epoch versus epoch. So that's why you have this kind of straight line. And then you have this precision recall loss to negative to positive accuracy. In fact, all this is basically gotten from all those metrics we had here. So this means that with this simple callback we have put right here, weights and biases is able to get all this or capture all this information during the training process and give it to us or present it to us after we're done with the training. So here we can see those different charts with what we already used to see in this already. So you should be already familiar with these different chats. Now, apart from these charts, you have the system information. So here we have the CPU utilization. We have the system memory utilization, process memory news, process memory news. While this is megabytes, here we have this information in the percentage with respect to the total memory available. Then here we have process memory available, process CPU trace and use, disk utilization, network traffic, GPU utilization, GPU temperature. So you could have this information here, GPU time spent, GPU memory allocated, GPU power usage. And so you see how easy it is to get all this system information without writing any extra line of code. Now from here we could go on to the model. You see, we have this table right here, which shows our model. It's kind of similar to the model summary we had pulled out previously. So yeah, it's kind of similar to what we had here. So now we have, instead of that, we have this beautiful table right here, output shape, number of parameters, type and the name. So that's it. And then from here we have this logs. So we have everything that was logged out, like all we had here. So you see everything logged out. We have it in this run. Now recall that when doing this 1DB init last year, once we do this 1DB init, we actually create a run. And once that run is created, everything we do after that is going to be stored in that run. And that's why if you notice here, like you don't only have this information, not only the string information, but even this data which was logged, even this model because we run this several times. So you see here that even this information which was logged is actually stored by 1DB. And so you see that experiment tracking here is done very easily and actually seamlessly. Now you look at the files will start. You have the model best. See you have automatically this Keras model file will just start. You have this metadata. You could open this up and you have this information start. You see here that we're using a Tesla P100 GPU. So that's it. Python version, operating system version, and then other information like the GPU count, CPU count, and so on and so forth. So that's it. Let's get back to this overview. See it's still running. That's because we have our Collab Notebook still running right here. Now one thing you could always count on is this documentation right here. So you just come straight to this integration and then you pick out Keras. You would have your Keras and then you have this 1DB Keras 1DB callback. And right here you have this. Now you could see this arguments of those different arguments we have here and you have the explicit definitions here. So let's check out on this arguments. You see we have monitor the validation loss, which plays a similar role to this model checkpoints callback right here. And also we could specify the mode. So let's get back to the documentation where we are. Okay, so we get back to this mode. So you could pick out the mode by default is automatic. We could also select mean or max. In the case of validation loss obviously would select a mean and the case of if we are dealing with a validation accuracy or validation precision or recall, then we'll select the mode to max such that we're saving our model when we have the maximum precision or maximum recall for example. Here we have the safe model, safe graph, safe weights only, lock weights, gradients, training data, validation data. Now we should know that with this we're able to pass in our data set to 1DB. And the fact that we pass in this data to 1DB can permit us come up with some visualizations of what the model is predicting since now we have the data. Then we also have the generator, validation steps, levels, predictions, input type, and so on and so forth. So you could always check out on this. Now the next thing we'll be doing is take the validation data, pass into this 1DB callback right here, and then be able to visualize the different predictions as we go through the training process. So let's break this up a little. We have this training validation, number of epochs. This should actually be configuration, configuration, number of epochs, number of epochs. Okay, so we have that. And then we have the verbosity callbacks. Now in this 1DB callback right here we have validation, validation data, which is all of our data set. And then from here, let's check out the documentation. And then we have the levels and the data type. So let's specify the levels. From here we have levels. And the levels we have here, we have parasitized and uninfected. Anyway, we could create this like we could have the classes. So we could have your levels and then specify this. So let's just take this off from here. Cut that and then paste it out here. Okay, so we have that and then let's have this levels. Now we have the level. We could also specify the data type. So that said, we specify this to be image. Okay, we have that, number of epochs and all of that. Let's go ahead and modify this number of epochs here. So number of epochs, 100. Let's change this. Let's just have three epochs for now. So let's get back 1DB config. We should have this up here. So once we make this, we get this new configuration. So we run that, looks fine. We get back to the training. From here now, we could run this training again. Training complete. Let's go ahead and check out what has been logged in our dashboard. So let's have this here. We pick up this run. There we go. We see we have still our 19 different charts. And then we have this media, which has been added here. We click on this and you see what we get. As you could see here, we have the different images and the predictions. We should view full screen mode by doing this. And then just here, you could select the step. So if you pull this to the end, you see we have 36 steps, meaning we have trained for over 36 different epochs. Now, selecting this malaria detection project and having all these three different runs right here, you see you could view all the runs simultaneously. So I could click on this one. This is one run actually. This is another run. So I could view all this now simultaneously right here. See, we have that. You see the difference in GPU power usage with these two different runs. That is the sandy water to run and this exalted night to rerun. And then from here, if you want to stop a run, if you want to stop this current exalted night to rerun, what we could do is come right here. That is, let's say we have this code here. We could come right here and just simply put out 1db.finish. So we call this and we should be able to stop the current run. You can always check out on this 1db.run documentation right here to get more information about this. You see here this example given where the create the run and then stop the run. And then this shows that there is no current run. And then after recreating the run, you see that there is now a run. So this is simply how we could stop this run. And that's what we've just done here. We could simply run the cell. And after running the cell, you have this run summary. The accuracy and the other different metrics and loss values. Notice how the different metrics and losses have been put out in this command line formatting. So you could see how this loss, for example, here drops. And then getting back to our dashboard, we see that there is no current run. Then we've again recreated a new run with a 1db init right here. We've created this new run on this elevator. We could check it out here. We created this new run. And you can see this little green circle right here. Coming back to the notebook, we have this 1db callback, which we've used so far in logging information into weights and biases. But then this is limited because we are not yet able to define our own custom callbacks. Now that's set, let's get back to this callbacks we have defined here. Here we have this log images callback. We're going to actually try to log images into weights and biases using this callback, which we have created. Now recall that to create this kind of custom callbacks, you inherit from the callback class in terms of flow. And then you get to put in some code in here, which defines that custom callback. Now we've just copied this and paste it right here. The difference here is this is 1db. So this is what we had for tensor board and this one of 1db. You're going to see how easy it is, even though we're building these custom callbacks. So right here, so let's suppose we want to predict, rather we want to log this confusion matrices and we want to log this to weights and biases. Now what we'll do is we're going to take off all this here. So let's get back. This is what we did with tensor board. We saw that we had to make use of matplotlib and then log this information into tensor board. But here what we have or what we need to do to log the confusion matrix is simply just this part of this code right here. So here what we have is we'll take all this off. See, we take all this code right here off. Take this off. Let's have that. So we have this code right here taken off and then we're just left with this. So you see that instead of having to write all this code, all we need now to write is just this one. Now if you're wondering why or how to get the other plots or what other plots we could get automatically like this with weights and bias, we get into this weights and bias GitHub repository. And here we have, we have this link, 1DB client, TreeMaster, 1DB plot. You see all the different plots we have now. And since it's constantly under development, surely in some time to come, we will have many more of this different plots we could do very easily. So here we have a bar.py, confusion matrix. You see, we have the confusion matrix, histogram, line, line series, position recall curve, ROC curve and scatter PY. So this means that already we could plot out this confusion matrix and the ROC curve very easily with weights and bias. So let's get back to this. We'll check this out. First thing we have noticed is log, 1DB.log. So whenever you're using this 1DB callback here, what's actually happened, what happens under the hood is this information has been logged like this. So we have, we make use of this 1DB log and then here we are going to have, let's change this. Let's say confusion, confusion matrix. Anyway, let's write like this. Okay, here we have the confusion matrix and then you have 1DB plot confusion matrix. You could modify this and put PR curve, that's position recall curve or ROC curve. That's it. And then this props, then we have Y true, prets, the predictions and then the class names. So here we should change this class names and have parse size, parse size. And what do we have here? We have uninfected. And then you can either pass the probabilities or the predicted score. So here we are going to have props equal predicted since what our model predicts or what our model outputs are the probabilities. And then the Y true is equal levels. From here, we're going to take out the spits because we've done that already. So let's have this here. There we go. This doesn't take into consideration the threshold. So we take off this threshold. That said, now we have this callback 1DB and then we're ready to train our model. After the training process, we have those results which look great. And we could now go ahead to look at the confusion matrix loaded in the dashboard. So let's get to our dashboard and this is what we should have. Let's get back to runs and then click on this current run and this is what we get. Now we see that we have this confusion matrix which doesn't actually show us what we expect to get. And this is simply because the way this was conceived or the way this callback here, this login was conceived was such that we have a multiclass problem. So even in the case where we have a binary classification problem like in this case, we expect to have an output of two values. So that's why when you have this output, let's get down here. Click on this. Let's add this code. So that's why when you have the value like say in the output 0.9, it's considered to be parasitized although we defined already that parasitized is meant to be zero, uninfected equal one. And so when we have 0.9, this is greater than the midpoint of this two, which is 0.5. So we should consider this as uninfected. But the way as we said already, the way this has been conceived is such that even in the binary classification problem, we shouldn't or we don't have a single output but two outputs. That is if we have a value of 0.9, what 1 dB expects to see is something like this, something like 0 and 0.9. So showing that here, let's even put here zero one, even 0.8 and 0.2. Okay, so let's have this. So even in this case, it goes back to the same output. So this simply means that we are having an uninfected cell since this first index here has a higher value. Now to solve this issue, what we're going to do is, we're going to take each and every output we have and convert it to this format. So in the case where we have, for example, 0.1 as output, we're going to convert it into 1, 0 because this means that the higher, this one means that it's parasitized cell and this two means the same because here we're seeing that this zeroth index, which is the parasitized index, has a higher value. Hence, this is a parasitized cell. So that said, what we're going to do now is, for all values less than 0.5, we could say less than, yeah, 0.5, we are going to convert it into 0, sorry, we're going to convert it into 1, 0. And then for all values greater than 0.5, we're going to convert them into 0, 1. So this is the transformation we're going to make in order for this 1DB log method to correctly log our values. But note that if we're having a multiclassification problem, it will be needless doing this transformation. So let's get straight away and see how we're going to transform this into this required format right here. Let's start by copying out this first part. Let's copy this into this other cell here. And once we run this, we have this output right here. Then we can go ahead and modify this predicted. So what we now have is, we will define the spread, this other list. And then for i in range, range the length of predicted, we're going to make sure that if the value, the i, we're going to take this zero index. If it's less than 0.5, then what we have to do is append this to this list spread right here. So what we're going to append is 1, 0 because this is less than 0.5. So if it was a multiclass problem, we would have the highest or the higher value in this case since we just have in two classes, we have the higher value representing this output which was predicted, which happens to be less than 0.5. So we have here 1. And then here we'll have else, spread, append. What do we have next? We have 01. So here we have 01. Let's take this off, 01 and that's fine. So here we've modified this spread and then we could then print out the spread. So with this now, we should be able to have the expected output. There we go. We have this output. Let's add this code cell and then print out our pride shape. We run this and what do we get? You see, we have exactly what we expect. So that said, let's copy out this part from here and then we're going to add this here. So we have this pride now. Let's have this forward and then we're going to have pride equal np.array of pride. So convert it into a nonpy array before passing into this log method console. That should be fine. Okay, so that's it. Now we have this set. Everything looks fine. Let's take this off and have that pride. So we have this pride now. Everything looks okay. And we could now rerun or restart our training. So let's have this year. Let's take this off. From here we train again for just about two epochs and then the results we get. So we can click on this run year and then you could have the tables and the custom chats. Let's start with the custom chats. You could expand this this way. So here you see that we have the predicted and the actual. So we have the model. You see now that we have this number of true let's say true positives which increases. Number of true negatives which also increase. While we have this number of false negatives to be 112. While here we have 62. This is 1278. 1305. And while you compare this with this previous run you see that now we have reasonable outputs. As with this previous run we had that error where the confusion matrix method of 1 dB considers all the outputs to be parasitized. Let's have this back and take this off. Then from here we could go ahead and to look at how to plot the ROC plots. Let's get back here. Scroll up. What we do is here we just have to let's comment this here and then now do the same for the ROC plots. Let's get back to the GitHub repo and click on this ROC curve here. Here we have now those arguments which have been described. So here we have why true why probabilities. Obviously this is the level predicted and then the classes, the class names. Classes to plot. That is a set of classes which we are going to plot. We are going to exclude all classes which aren't specified in this classes to plot argument. Now this is actually optional so you don't necessarily need to have this. We also have this title. So let's copy this out. Let's copy out this here. Let's copy this out and paste just right here. We then specify the levels why true levels. Here we have red and then right here we have this class names. So let's copy this out and paste here. OK, so that's what we have. Let's run this cell. Everything looks fine. And then we start with the training. Training now complete. Here are the results we obtained. We get back to our dashboard and this is what we have. You could go back to the runs and you see that you click on this custom chart right here. No, let's click on this run here first and then we have this two custom charts. So we have this previous confusion matrix which we locked in and then now we have this ROC curve which we just plotted out because we check out the tables for the ROC curve and the confusion matrix. So here we go. We have this ROC plot and then we have this two plots here where one is for the uninfected and the other is for the parasitized class. I haven't seen how to plot the confusion matrix and the ROC curve. We could check out the documentation and look at the other plots. So here we have basic charts, lines, color, bar chart, histogram, multiline and the next model evaluation charts like what we've just done, the confusion matrix, ROC curves, PR curves, that's precision recall curves. You have them here, interactive custom charts, matplotlib and plotlib plot. So this means you could actually come up with your own plot with matplotlib and then log that to 1db and then apart from this method in which we used the 1db callback here, not this one, the 1db callback we had defined previously, apart from using that 1db callback to log the loss, metrics and other information we could directly just do 1db dot log and then we say for example we want to log the loss, so we just put out loss and then since we got the loss from this locks, we have this locks here, we scroll down and then we put out your locks, locks and then we specify the loss. So this is all what it suffices to log this loss values. From here you could also log the accuracy and all the metrics. This actually a dictionary, so let's have that and there we go. So we've seen how to log these values. Now let's also see how to log images like with this, we have this image and then we log this to tensorboard. Now we'll see how to get this image and then log it to 1db. To log those images, we'll then check out your documentation and you'll see that you could log rich media files as really any kind of media file, 3D point clouds, molecules, HTML and histograms. So that said, you'll scroll down and you have this code here which permits us to log image data. So here's login arrays as images, login PIL images, login images from files, image overlays, segmentation masks, bound in boxes and so on and so forth. So you have this, you also have these histograms, you have 3D visualizations as you could see, point cloud and molecules. So you see that 1db actually permits just any practitioner be able to log any data or any kind of data they really want to log, hence making them more efficient. So that said, let's get back to this here. We're logging arrays. Let's copy this out. Let's click here. Just simply copy that way and then you get back to the code. So here, what we're saying is, we want to be able to log these images at this level. So let's paste this out somewhere here and then copy out this part again. We're going to copy this part. Recall that with this 1db we had defined here, we kind of like got, we ended at this level where we got the levels and the predicted and automatically we got this ROC curve and coefficient metrics. Now what we want to do is get right up to this point where we actually have the image that is our own image which we've created and then from this image log it to 1db and so right here we have this piece of code here and we're going to integrate this. One thing we could do again, let's copy this again. No, let's copy this other one because here we have the image. So we'll copy this again. We consider that this is for TensorBoard. Let's write here TensorBoard, TensorBoard. Okay, we have that for TensorBoard. We have this for 1db. Now 1db plot and then this next one is just 1db. Let's add this here. There we go. Okay, so here we have this 1db. That's fine. So what we're saying is we're going to take off all this here. So we take all this off and then we copy this out. Let's cut that out and then paste it here. So there's how we're going to log this to 1db. As usual it's going to always be simpler than what we have with the TensorBoard. So just all we need to do here is just to take our image and then log as simple as that. Now our image array here. Let's see this image array. Our image array is this output which we have and then we pass this image array into this caption. Let's put a caption. Let's say confusion metrics for epoch. And then we get the epoch from here. Like we have this epoch. So we're going to be logging this confusion metrics for each and every epoch. So we just put out this epoch here and that's fine. So that's our caption. We have 1db image and we log it in here. Let's have those, let's say confusion metrics. Yeah, confusion metrics. Okay, let's have that. Now we run this. We copy. Anyway, we had that already. Let's copy this out. We have an error here. This should be closed. Okay, so that's fine. We have this 1db log right here and everything looks okay. Okay, so let's run that again and check this out. Looks fine. Now we could go ahead and start with the training. Let's run this. This should be okay now. And compile the model. After compile the model, we now go ahead and train the model. Let's say we want to have 3 years. Let's keep that aside for now. And then here we have this. Let's paste it out. It should be the same. Okay, here we have it. Now let's run the training process and see what we get. Training now complete. We can get back to our dashboard and see what we have as plots. We'll click on this year. This run, current run. We have this image here we logged previously. Now let's check in this hidden panels and see what we get. It happens that we have this completion matrix logged in these hidden panels. Anyways, we have that. What do you notice? For the 3 different epochs, you could go to the second and then go to the third epoch. Start with the first. We have the completion matrices which should be logged per epoch. Now let's do this so it appears clearer. And we see we have this number of true positives true negatives false positives false negatives and so on and so forth. So that's it. From here you can notice how this value leaves some 68 to 52. And then finally we have 53. Now another interesting functionality is you could simply have that. You could download it and now you have this full screen and you can see that clearly. So let's press escape close and then get back. So that's it. That's how we log this image very easily with 1 dB. Now getting back to the documentation, you might have noticed that you have Keras and then you have TensorFlow somewhere around. You have TensorFlow you see that these are kind of like considered to be two separate libraries. But then just note that if you're building Keras model, that's if you're building this kind of model where you have or you're making use of this method, this feed method to train your model then making use of this Keras documentation right here is appropriate. But then sometimes you want to have control or full control over what you're doing and then you want to be able to do custom training like we have seen previously let's scroll down. Okay custom training loop. So if you're having a custom training loop like with this year you would find that you would not be able to use the 1 dB callback as easily as you had done with the Keras code. So in that case you'll see that instead of having like for example like here we have training block and then we have this loss here all you need to do is come and put in the 1 dB you log and then you log the loss. So here you just say loss and then you simply log this loss, let's say loss numpy and that's all. So this way it takes now to log this loss values the different metrics and so on and so forth. The integration with TensorBoard has been made quite easy too. So if you're already using TensorBoard it's easy to integrate with 1 dB now you're going to see how 1 dB is different from TensorBoard. The ability to reproduce models automatic organization fast flexible integration, persistent centralized dashboard, powerful tables and tools for collaboration. That said let's copy this out, let's copy this again and now when we'll be creating our run with the init method we're going to instead have this. So let's get back to our code let's reduce this year and then stop this run so we have this 1 dB finish we stop the run the run stopped and then we're going to create this other new run which will take into consideration our TensorBoard logs. So let's get back to the top and just right here we'll paste this out. We've already had this so we can take this off. Now let's comment this and then let's have project entity just copy that and paste out here that's fine configuration we have that same TensorBoard that's fine okay let's run this now we get this error so let's take this off here now this isn't compatible with version 2 of TensorFlow which is what we're using so let's run this again and this now should be fine. We now have created this new run let's scroll down and we get to our custom training loop so we're going to run this we have the optimizer, metric metric validation, IPOCs number specified and then let's say we have this as configuration and then we have an epochs an epochs okay so that's it we have all this set and then we run this we've seen this already under the section TensorBoard. We get this running when using several event log directories like what we're doing right here please call 1DB TensorBoard patch and then specify the root log directory before the 1DB init so let's take this from here, copy this out and even if we check in the documentation you should have that so here you have this the 1DB TensorBoard patch and then you have this root directory right here so this means again we're going to stop the current run where are we exactly let's stop this current run it should be at the level of training we're going to stop this current run here, we run this again, we stop that run and then what we do now is we have this here and then we specify logs so we have that root directory specified now we have the root directory specified that's logs documentation, we have that specified before the init so we have to run that before the init looks fine, let's run this now let's run this cell there we go here we go, TensorBoard already patched, removed this TensorBoard true from 1DB init since we've already done this patch we should remove this from 1DB init let's take that off and run again we're still getting the error, so what we'll do is we just get back, and then have that, and then we'll just go as we started initially so let's run this, this should work now, from here we're going to continue what we're about to do with a custom training process so let's scroll down okay, so we are going to this point, we have to, anyway we'll run this already so that's fine, now let's recommend this one and then run this cells, run this neural and method, and then start the training process, as you can see training now completes, let's go ahead and launch TensorBoard, we have the loss values and accuracy values which have been logged in, you can see them here, let's reduce this, validation accuracy, and the validation loss, now from this, we are now going to look at our 1 dB logs, so we take on this run, and you see we have this log data in here you see we have the train, we have the plot for the loss, we have the plot for the accuracy, and we have the plot for the global step then for the validation, you see we have this here, we have the global step, validation loss and validation accuracy, exactly what we have with TensorBoard without adding any extra line of code for the 1 dB that is the only thing we actually did was at this point here, at this point here, we said we wanted to sync TensorBoard, so this is all we need to sync TensorBoard with 1 dB so if you have been using TensorBoard or if you are using TensorBoard on a particular project, that's all you need to log your information now to 1 dB, apart from this, you will notice this TensorBoard here, so you can click on that we are spinning up your TensorBoard instance, hang tight, take about 30 seconds and we will keep it online as long, before we have been completing the sentence you have this already, so you see we have this TensorBoard which has been logged here we have this PyTorch profiler and then time series scalers, just exactly what we have with TensorBoard, so that sounds cool we have the logs, we have the different files which have been saved system information and we have these charts and then overview, where we could see all this information and then get back to our runs, so that's it, we have seen how to sync TensorBoard with 1 dB and so now you are ready to track your experiments with 1 dB, thank you for getting around to this point and see you next time Hello everyone and welcome to the session in which we are going to treat hyperparameter tuning with Weights & Biases there are 3 methods available for us to implement hyperparameter tuning in Weights & Biases, that is the random search, the grid search and the bias and search method. At the end of the session you will be able to search for the most suitable parameter values, which optimize the accuracy. Previously we saw how to implement hyperparameter tuning with TensorBoard. We created this model tune method which takes this argument HPROUNDS as hyperparameters which are actually going to be tuned as you could see here and then we have as output of this model tune method the accuracy so what we are trying to do here is we are trying to tune or we are trying to obtain the optimal values for these different hyper parameters which maximize the accuracy of the model the exact method we used was a grid search with a grid search method we actually go through each and every option or each and every possibility and for each possibility we are going to log its accuracy as we did right here now with Weights & Biases we are going to see how to redo this but in an easier and more reliable manner so let's check out another documentation right here we have this documentation here we have hyperparameter tuning click on the sweep start which shows globally what are hyperparameter tuning with Weights & Biases we set up Weights & Biases we configure the sweep we initialize a sweep we launch agents we visualize the results and then finally stop the agent. From here we will see how to run these sweeps in Jupyter. So the Weights & Biases sweeps allow you to easily try out a large number of hyperparameters while tracking model performance and logging all the information you need to reproduce experiments now we are going to focus on this pure Python method where the sweep configurations are in the form of a dictionary like this the way these sweeps work the way Weights & Biases sweeps work is we have a central sweep server that is this one year the central sweep server and then we have these different agents right here we could have many more agents. Then once we set the configurations the sweep configurations in this central sweep server right here the agents now take over and do the actual hyperparameter tuning that is a search for the best hyperparameters which help in optimizing the model and this parallel method of implementing sweeps in Weights & Biases help make the hyperparameter tuning process even more efficient. As you could see here this could be done in just two steps. Initialize the sweep and put all necessary information or give all necessary information for this central or to this central sweep server and then we run these different agents right here. Now that's set we have the sweep configuration which looks similar to what we had when we were working with TensorBoard. So here we have this configuration right here where we specify for each hyperparameter the values you can take given that we're having a grid search or making use of a grid search algorithm or specifying simply all those different values and that was it. Now coming back to Weights & Biases you see here you just need to specify for example the hyperparameter, that number of epochs, give the values, learning rate for example, you give the minimum and maximum value and so on and so forth. But then note that here in this example given the documentation, the method you're using is not a grid search. That is you're not going through each and every possibility in the list of values. What you're doing here is actually a random search which happens to be more efficient than the grid search algorithm. Since it's possible in a shorter period of time to get very optimal values of the hyperparameters as compared to the grid search algorithm where you need to go through each and every value. Now let's get back here. We have the name, we have the method and we have the parameters. To understand where all this comes from check out in this sweep configuration right here. So clicking on the sweep configuration we have the structure of a sweep configuration then here we have the different keys and their descriptions. You'll notice this method which we had seen here. Let's open this another tab, open a new tab and have that. So let's have it this way. There we go. We have this name method and parameters which we could see here. We have name, method parameters and then for each of these configuration keys like this method for example, there is this more explicit description. So here we have the method you're going to use. If it's a grid grid search iterates over all possible combinations of parameter values the random search chooses a random set of values on each iteration and weights and biases also comes with this other method which is the bias method. This bias in hyperparameter search method uses a Gaussian process to model a relationship between the parameters and the model metric and chooses parameters to optimize the probability of improvement. The strategy requires the metric key to be specified. Here we see now we have these three different methods and that is why when we wanted to work with a random it suffices to just put this out here. Either you put it random or you put grid or you put bias. Now we move to the next. We have the parameters. We have the parameters. Anyway the name, you could put the name for the sweep. That depends on you. Here is my sweep. Then the parameters here it gets a little bit more tricky because here you have this here this key and then the value is a dictionary which itself is having its own keys and values. We have these parameters. Let's get back to parameters here. We have the parameters. Where are we? We have parameters and then we have these different possible values and the descriptions. Getting back here you will notice that this hyperparameter epochs takes values in this list that is added 10, 20 or 50 while the learning rate can be chosen between 0.001 0.0001 and the maximum 0.1. Each hyperparameter has its own distinct way of describing the values it can take. Now getting back here we have different parameters and then for these values you can say a value. Sometimes here we may decide and say we want this epoch to take only just one value. There we just have value and then in the case where we want several values you see it specifies all values for this hyperparameter compatible with grid and all of that. Now here in some cases you have a distribution and this selects a distribution from the distribution table below. So you can see here this specifies how values will be distributed if they are selected randomly e.g. with the random or biased methods so when working with the random or biased methods you may want to select your values based on the given distribution and your other list of distributions you can use. You have constant, categorical int, uniform uniform distribution, cure uniform log uniform, cure log uniform, normal distribution cure normal, log normal and cure log normal. Then from this distribution we now move to minmax. Minmax is what you actually saw here here you had this minmax so here you are simply saying you want your learning rate to to have a minimum value of this and a maximum value of that so we randomly pick values between this range and that's it. You have MU, MU mean parameter for normal or log normal distributed hyperparameters. So here you have a normally distributed hyperparameters and you are specifying the mean while here you are specifying the standard deviation. Then for cure you have the quantization step size for quantized hyperparameters Our next key is the metric and with the metric we have to define the name of the metric, the goal, and the target. Now here you could have for example like your validation loss you have this metric validation loss. You could say you want to optimize your parameters or you want to choose the hyperparameters which minimize the validation loss. Now you could also change this into an accuracy. Let's say validation accuracy or just the accuracy or just the trained accuracy in that case you would want to choose the hyperparameters which maximize the accuracy and hence here at the level of the goal you either minimize or maximize. The default is minimize. So if you have a validation loss it's needless specifying the goal because by default it's minimized and for now as this documentation there's really no automatic way of deciding whether it's minimize or maximize. Anyway you can always do that manually by specifying. Now the next we have is a target. So here with the target as you could see here for example 0.95 it means that if you happen to get a set of hyperparameters which permit you to get this validation accuracy to this target value then at that point you will stop the process of searching for the optimal hyperparameters and what happens exactly is all agents with active runs will finish their jobs but no new runs will be launched in the sweep since we've already attained the objective. From here we have early terminate which is an optional feature that's a piece of hyperparameters searched by stopping poorly performing runs. We get back here, copy out this code just copy it, okay copy it we get back now to hyperparameter tuning we paste out the code here and then we just going to try to replicate what we have done already with TensorBoard so here we paste this out here and then we have this numUnits1 we have this numUnits1 this here and have that numUnits1 values there we go let's copy this out and paste here so we replace these values which we had already and then the next will be numUnits2 it's kind of similar so we should numUnits2 so here we should just copy this and paste numUnits2 values it's kind of similar, okay we have that and then there we go so we have numUnits1, numUnits2 and then the next drop out rate we're going to have learning rate so let's just have the drop out rate here so we have the drop out rate and this drop out rate we take values 0.1 to 0.3 now what we're going to use is like this min max so let's just copy this out here and then let's copy this out and then paste it out here okay so we have drop out rate but here we're going to go from 0.1 to 0.4 let's say 0.4 and this random actually is not agreed so we have that and then here okay name, lesson, name malaria prediction, sweep okay mental random parameters is fine so we're getting each and every parameter now we have the drop out rate set we now move to regularization rate regularization rate 0.001 and 0.1 okay so we have 0.01, 0.1 and then what we could do now is we make use of distribution and then we have your uniform so we specify that we want to make use of your uniform distribution and then here so we're going to use the same again here for the learning rate distribution uniform there we go we have that uniform and that's fine here we're going to go from this let's say 1e negative 4 min and then max 1e negative 2 1e negative 2 okay so looks good let's take this off now let's take off what we've set here and then we have that so from this we'll be able to create now this sweep id we'll get our sweep id by running this 1db sweep and passing in the sweep configuration to run an agent the first step is we're going to define a function to run the training based on those hyper parameters and then we're going to pass that function with the sweep id here in this 1db agent method so now let's copy this code out and then paste it out here paste it out here and you'll also notice that this is kind of like similar to what we had done here because here we had defined this method which takes in the hyper parameters which we're trying to tune and then we go through this method here for each and every sweep now let's get back here we have the model we could simply make use of this model so let's copy out some part let's just copy out this here let's copy that out and paste this here paste it out and we have model tune the net model which we create here hyper parameters and love that and then instead of what we had here where we pass the we had the compile and the feed method what will return here will be just this model so we return the net model right here so let's have this new net model let's take out this part of the code and that's fine okay so we have this model tune here instead of make model we have model tune and we'll pass the configuration in here we add the project and the entity in our init method then we've modified this keys right here to match with those of our 1db configurations which we have seen already let's get back here let's just copy this and put right here so you could see that so as you could see instead of number of units 1 we have number of dense 1 and here we have number of dense 2 so that is it we have the different values you could take we have the dropout the regularization rate and the learning rate now right here in this model this our model we should call model tune we shall make use of this 1db configurations which we've already passed in here and so here we have config and config and we're going to do the same for all these other different hyper parameters like here we'll have number of filters here config number of filters and then you should note that in this example we are not going to turn all the hyper parameters so the hyper parameters which we want to turn should be in this parameter or parameters dictionary in our sweep configuration the method is random and the metric is the accuracy with the goal of maximizing it so if you want to tune any parameter or hyper parameter you put that in here for the rest we're just going to use this configuration which we've set already so it'll just have some fixed values so that said we would modify all this and there we go so we now have all these different modifications run this, run this and then back here on this train we could now replace all this with the compilation and the fit method now again here we'll have this learning rate so we have learning rate and then we could modify this number of epochs to say three or actually the config so we have config number of epochs and you should note that we're carrying this only on the validation set so you could try this on the full training data set that would take much more time and also you could feel free to carry out this hyper parameter tuning on more parameters so you could take up say the image size like this parameter here, you could add that up and then see how this image size affects the model performance so that said we have all this already we have our agent which takes the sweep ID which we've defined already, we have the function which is the string right here and then we have this count which is the number of runs to execute. Before we proceed note that we specified this configuration which is essentially this configuration we have here so we could just take this off and save it, this is our configuration, ok so that's our configuration and now we are ready to let our agents do their job so let's run this now after training runs here's what we get, start from here you see that it starts by picking the drop array, the learning array the number of dense 1 number of dense 2 and then the regularization rate while obviously keeping the other parameters constant here we have this loss and its corresponding accuracy and you could check out for the other sweeps down here, you see we have 20 different runs and we could get right here you see and we click on this to view the sweep and now on this page you could see the different runs we have here from 1 up to 20 the epochs or rather the run, the accuracy and its corresponding loss and then we'll skip this tool for now, let's look at this we'll go ahead and check out the hyperparameters which produce the highest accuracy which is this here, let's highlight that, we have this well let's pick this out from here there we go we have this, we have this, we have this that comes down here, we have this and then we have this so here we have the hyperparameter values which give us the highest accuracy score then coming into this let's take this off we could check out these different parameters or parameter importance with respect to the accuracy so you see that the most important is the learning rate followed by the run time number of dense 2 dropout rate, regularization rate, number of dense 1 and here we have this other hyperparameters which we did not modify now let's click on this here and see what we have we told automatically shows the most useful parameters, so let's check this out you see that after clicking on that the other fixed hyperparameters and even the run time is taken off, so we left with only this hyperparameters which we had fixed or better still which we had put in this slip configuration then one other point you could note from here is this correlation so apart from importance we could check out the correlation and here we told that this learning rate has a negative correlation because you could see with the red, the red indicates negative correlation while the green indicates positive correlation so you could check out the value, you see this is 0.051 while this is negative 3.53 now what this means is the lower the learning rate then the higher the accuracy and then in this plot to the left we have the accuracies for each and every run so that's it for this section, thank you for getting up to this point and see you next time. So we are getting into this data here and then this other preprocessed data or this preprocessed version is again preprocessed to give us this one and then finally, we have this last step, this last preprocessed step which gives us this data set version which happens to be an augmented data set version so this shows us that if at any point in time we are identified with this preprocessing which was done right here to produce this data set version we could simply preprocess from or making use of this data set version right here and so this greatly simplifies our data set management when working in our different machine learning project just as gate pyramids us do code versioning, weighting biases gives us the possibility of doing data set versioning and model versioning and this can be done using weights and biases artifacts which help us save and organize machine learning data sets throughout a project's life cycle. We are going to start with the data set versioning and the most common ways in which weights and biases artifacts have been used for data versioning are to version data seamlessly, prepackage data splits like training, validation, and test sets iteratively refine data sets, juggle multiple data sets, and finally visualize and share a data workflow before getting into seeing how artifacts could be used in data set versioning let's look at this simple example in order to obtain the malaria data set we have to start by loading this data. We have this data loader which is represented by the square and now once we've loaded this data we have our data set so let's say we have this original data set right here we now go ahead to split this data so from here we can split up this data and then have the three different parts we have the train data, the validation data, and the test data. At this point all three data sets have been passed in this preprocessing units right here so we preprocess the train preprocess the validation and preprocess the test and this gives us an output here. So for the train we have a preprocessed training data. Here we have this preprocessed training data we have this preprocessed validation data and here we have also this preprocessed test data again at this point we are now going to carry out augmentation on this preprocessed training data. This PTRD means preprocessed training data and then preprocessed validation data preprocessed testing data ok so we have that and then we pass this through this augmentation process. So we have this process was rolled with that of carrying out data augmentation on this data set to produce another data set which is actually now an augmented version of this so here we have preprocessed training data. Now we have augmented training data and this is kind of like the life cycle of our data set and this particular problem in which we are working on. Imagine that this original data set contained mislevelled examples in that case what you want to do is now to carry out this level incorrectly such that we have this data set here which now has been cleaned. Overall this leads to a high level of accountability as when working on a team and let's say you have done some modification, let's say you have cleaned the data, all the people in your team can now view this cleaned data set and decide whether to modify it delete it or keep using it. So if we suppose the team accepts this cleaned data set and everyone is happy with this newly cleaned data set, we now see that instead of passing this directly into the split, we will now pass this one into the split. So we'll have something like this, we'll go this way, let's take this off. In that case we will have to go this way we go this way this and then into the split. And then talking about data set versioning for each and every data set we've created here, like for this one, this one, this this, this, this, this, or this one we have different versions so we could have for example this PVD here, that is the preprocessed validation data and it could have a version 0 or version 0. So we have the version 0, right, yeah and then later on you may modify this preprocessing and it leads us to have another version of this validation data. So you could have another version, version 1 and the version, version 2 and the version, you could say version best, you could have version latest and so on and so forth. And a good thing is when weights and bias or when using weights and bias to store this data or when we using weights and bias artifacts the data is taught in such a way that if we have data in this original data set, which is exactly the same as what we have in this preprocessed validation data then that data wouldn't be duplicated. Now it's true that if we preprocess this data obviously we'll have all the data set or all the elements of the data set changing. Now let's take this example for the cleaned data set. So here we have this original data set. Let's suppose we have say we have 10, we have 10 different examples, different samples in this original data set and then after cleaning our data set only two of these samples have been modified. So we have 8 unmodified and 2 modified. So we have modified. We have changed the levels of these two samples here. What weights and bias will do is it will ensure that this 8 year aren't duplicated. That is we don't create extra space for those 8 orders which haven't changed from this previous original data set here. And so in that case we're going to only store these two new samples here while weights and biases keep track of the fact that these other 8 samples haven't been modified and so don't necessarily need to occupy extra space in the storage unit which weights and biases makes available for us for free. That said we could look at those different processes right here. Those processes in the square boxes as 1 dB runs while these different forms and even the versions which are data takes are the artifacts. And so we could consider those key here where this represents the artifacts and the runs. Also we see that the artifacts are connected together by these different runs. So these two artifacts, this trained data and the original data set has been connected together by the split run. And then these two PTRD and trained data are connected together by the preprocessing run. Getting back to the documentation, we'll see how we create this artifacts here which is called the new data set of type raw data and then this is created within this run right here. So we'll see how we create this run and specify the project, my project and once we create this artifact we're going to add data into it. Now one thing you could do with 1 dB is you could simply add a whole directory. We're supposing that all your data is in a given directory and so you just all you need to do is specify this path and then you make this data part of this 1 dB artifacts which we've called my data and then finally you log this artifact to 1 dB. Let's copy out this sample code, paste it out here. We have that sample code and then we could get started with our data set versioning. Now we are going to put this in a width statement so we have here width 1 dB in it and then we specify the project, project we're working on which is this malaria detection project, the entity you will learn and that's it. So we have this here and then we are going to create our original data. So we have original data there we go. We are going to create this artifact actually. We have that original data, 1 dB artifact new data set type raw data. Now to check out the different arguments we could pass in here get back to documentation let's check out here and then you could scroll down. So here you have this references let's reduce this and have this clearly. We have references on the references you have this Python library and then you have 1 dB artifact so you could click on this and what do you get? You have this documentation right here. So we have the different arguments name, type, description metadata, incremental and use us. Now note that this 4 are optional so that's why in the example we just had the name and the type. We'll now add up this here. We have name and then type and we have description with metadata. So let's have this description right here. Description simply we could say malaria data set or tensorflow tensorflow malaria data set so that's it. For the description it's actually a string and then we have this metadata which is this dictionary which contains information related to our data set so here we define this dictionary we're going to start for example the source so we could have your source and then we say tf.data.set. Okay so we have that source. We could also add other information from this homepage so let's even copy out this description here and then paste it out here. So in place of this description we have that and there we go. That's fine. Now we check out on the homepage we have homepage source code. Let's copy this out and paste out here and now we have all this necessary metadata information so here is it. We've created this artifact. We then paste out this code which we had seen previously and which premiers us load the malaria data set from TensorFlow data set and then we'll save this data set in the NumPy compressed format. So with our original data which is this one here, this artifact.new file new file so unlike here where we add directory here we add in our we created this new file which now contains this data set so with original data data.new file we'll call it original original data .npz so that's our file name and then the mode is going to be a write mode and we have this as file. So we have this artifact. We add this new file with a file name and then we save this file while putting in the appropriate content. So right here we have np save that's our compressed format call here we have npz and then here we have this file and then what we pass in is our data set which is this one and so at this point we've read in our information or this data set in this artifact and then we're now ready to log this. So here we have rounded log artifact. This basically this here. So let's do this. Let's take this off. We have there we go. We have rounded log artifact and then what we pass in here is original data. Okay. So that's it. So now we've seen how to create this run and in this run we have this artifact and then we put in the information in the artifact. Now one thing we could do is put all this in a method. So we'll define the method load original data. So we load original data and simply does it. So let's send this one step and that's fine. There we go. So we have that load original data method defined. Let's add this code cell and then we can call it right here. So we can call load original data and there we go. So let's get back to this diagram we had previously. You see that in this diagram if we take off. If you don't consider this path that this information flows this way and then gets pleats and so on and so forth. So here what we have is we've had this load original data method which takes into consideration this tool. Here we have this run which is the data loader. So we have the little run and then what it does is it recuperates this original data set from TensorFlow data sets and then produces an artifact which is this one now. So the artifact we have just created here. This artifact original data is this which we had drawn previously here. We then run this cell and load the original data. We get this output. Unfortunately this process has failed. Let's check out the reason why this failed. Cannot convert a tensor of dtype variants to a nonpy array. Now since this variable data set we have here is of dtype variant. What we're going to do is we're going to take out each and every element of this data set and save it in a directory. So from here we're going to copy out this code. Let's copy out this code and paste it out here. Click back. Now what we're having here is we're going to go through this data set. So we have for dean data set or for data in data set. Now we have a list here and then we can pick out the zero element. So pick out the zero element and then for that we're going to create this folder data set right here. So let's have this new folder and call it data set. Okay so we have that new folder data set. In that folder data set we're going to put in these different tensors which are going to be stored in this nonpy compressed format. So that's it. We have this. We modified the name. So let's say we have Malaria Malaria data set and then we give it a number. So let's have that plus we'll give it a number plus this. So here now we have this K. We've initialized K and then we're going to continually be incrementing this K. So here we have Malaria data set. We have K and then we have this extension right here. So that's how we're going to save this file and then the data we're going to be saving is going to be this. So we have data which we're going to be saving in each and every one of this file. Now let's test this out and let's put out a break here so we see exactly what we're getting. We run this cell original data not defined. We instead need to do open here. So we're going to open this file and then save that. Let's run this again. We check out now this directory. We have your data set and there we go. You see we have this information here saved. Now let's run this for all the data set. So let's take out the break and then run this. Let's print out K and then like after a thousand steps so if K modulo a thousand a thousand equals zero we should print out K. Okay so let's run again. We have now all this data locked. Let's take off this one now and then get back to documentation and you have my data.add directory there. So you pass in the directory right there. So from here we have now instead original data.add directory and then we're going to pass in the directory which in this case is data set. So we have that data set passed. We don't really need to have this again so we can take that off. Okay so that's it. Everything looks fine. Let's now run this cell right here. We run that and run this. And this time around the artifact is loaded successfully. So now we are going to check out in our dashboard and here you see that we have this raw data, new data set and then you have this version two different versions of this new data set. We have version zero and version one. Now click on this version one. You should be able to have this. Let's check out the overview. You see you have this data set. Malaria data set on all this. In fact this is metadata we put in already. So that's it. We have this API. We're going to come back to this actually because to use this artifact we're making use of this API. Then we check out on metadata. We have this metadata which we logged in. We should put in in the notebook. We have the files. You see those different files here. This directory. You see you have the root and then you have basically all those files. Now you have this graph view which for now is very simple. So here you have a run. The run is actually this run here. So this run and then we have this artifact which has been created. Now this artifact contains the raw data. So we could click on explode and basically that's it. Here you have the name of this run. Vividwater name of the run and then the data is this new data set and it's version one. Then the next thing we want to do is to move to the next step. That is be able to split this data set into this other, into these three different parts. Now what we could do other than this is process this before doing the splitting so we don't have to do the split processing tries. So instead of having this let's take this off here. We'll modify this such that here is what we now obtain. So coming back to this we have this run and this artifact which our raw data set is basically what we have here. And then the next step will be to preprocess this. So we'll do the preprocessing on our raw data set and then produce this preprocess data and then from here we'll now do the splitting. We'll have this run which whose role will be to split our data into the training validation and testing and then for the training we'll have another run. Whose role will be to do augmentation on our training data. So in fact in summary this is what we want to achieve. Like when we're done with all this we want to have a graph view which looks like this. We now go straight forward into the preprocessing and that's copied. We get back to the code that's fine. We paste this out here. We then put this in the wait statement and then run this code and here's what we get. We've now downloaded this 27,000 files. We check out here and we should have this artifacts new data set version one and then see we have all those files which we unlocked in previously. So now this means that the next time you want to work on this data set you don't really need to come and run this year. Like we don't need to run this again. So we don't have to do this again. All we need to do now is just to use this artifact which was in biases starts for us. And so now that we have this we'll get back here and then we do this resize, reskill. Particularly our processing here is resizing and reskilling. So you're going to do a resize and a reskill of all our images. You can copy this and then get back to where we actually have loaded our artifact. While we print this out we have this path to our different files. So we will have your artifacts. We can check out all those files here and then right now we're going to create this auto run. So let's copy this out and then have this pasted here. Now here we have this preprocess preprocess the data. So this is this method we're going to be defining 1DB init project entity as well. We've created this new run and then we have now this preprocessed data preprocessed data which is now this new artifact and the name is preprocessed preprocessed data set. And then the type is preprocessed data. We have our preprocessed data. Okay. Description we'll say a preprocessed version of the malaria data set. Of the malaria data set. Let's take this off and then for the metadata we could let's take all this off. Let's have this taken off but you could always put information in the middle of the data. We are now going to go through each and every file we have in this directory here. So we will have for file in let's say for F for F in OS the list of this directory here. Let's copy this from here. So we're going to create a list from this directory and we go through this list. So we go through this list and then for each and every file in this list what we'll be doing is open up that file. So here you have artifact directory which we've just defined and then we have this artifact directory plus F to specify the current file and then we read that file as file. So from here now once we've read this as file we have x y outputs. Recall this is our output. So for each file since each file we had has an x and a y we'll take this from here and then we have this load file. Now once we have this we're going to have x or better still x full. That's x data set. Let's call this x data set or data set data set x. So we have our data set x dot append. We're going to create this as a list. So we have here data set x a list and then we have data set y and not a list. So that's it. We've created these two lists and then we want to take each and every element and append it to this data set x and data set y respectively. Here we have for x and then we have data set y that append y. Now recall what we have to preprocess as this is our preprocessing method. So here we have this resize. Let's make sure we run this resize rescale and it takes in the image or it takes an x basically. So here we have resize rescale instead of passing x we'll do resize rescale and then we pass in x. Now we ensure that we'll define this aim size. Let's have this aim size defined here aim size equal to 24. Run that again. That should be fine. We need to also specify the fact that we'll allow pickle here. So we have this argument which is done to true. You could always check out the documentation right here. And then from here we are not going to take this directly. Here what we get is npz array and then to obtain x and y we have x and y which is equal npz array. We get the numpy array and we get the values. So that's what we do to get this. You could always check out the documentation to understand how all this numpy load and all of this work. Now once you have x and y this is where we pass in here. So from here we can run this. This looks fine. We have all dataset and that's ok. But before running we have to ensure that we convert this now into TensorFlow dataset. So let's get this done by having this called dataset and we have tf.data dataset from TensorSlices and what we're going to pass in here is dataset x and dataset y. Now we're going to be saving this as a file. So we're going to take out this and we have this portion of this code right here which is from the documentation. So here we have with artifact. The artifact here is preprocessed is preprocessed data. So we preprocessed data the new file. We specify the file name. Let's call this preprocessed dataset as file. We then save this with numpy. We have np.save z in the compressed format we specify the file and the data to be saved. In this case all data is this dataset right here which is this TensorFlow dataset. This looks okay we can now do log artifact preprocessed dataset or rather preprocessed data. Then before running we are going to take out just a part of all this dataset. So we'll take out only a thousand elements. And the reason why we're doing this is because we do not have enough memory to store 22,500 2500 2500 different data points as a single variable. So we'll have that for now let's have this 1000 elements so you could see how this is done. Now if you have a real world problem where you have say 100,000 different elements or 100,000 different data points then you could break them up into simpler parts. So here we have that a thousand we take out just a thousand and then one thing we'll do is copy out this part here. We'll copy out this part and then include it in this run. So just before this so we include this here in this run. And the fact that you include this in this run will link up this new dataset with a preprocessed dataset. Now with that set we could run this and then run the next cell. What we get is this error saying cannot convert a TensorFlow of D type variance to a nonpyre. So here we are having this Tensor of D type variance and we're trying to store it as a nonpyre. And to solve this problem now what we'll do is we'll just ignore this and then save the dataset X and Y as this list we have dataset X and dataset Y. So let's run this now and preprocess our data. We now have successfully logged this to our artifact. Let's get to our dashboard, 1DB dashboard let's refresh this page. Here you'll see your artifacts you could click on artifacts, media detection artifacts and then you could select from this. So this is what we had previously this new dataset under this raw data. Now preprocessed data we have this preprocessed dataset and this almost recent version there we go. You could check out the files you see this file right here metadata API which you could use now to do or carry out all the operations the overview, graph view, this graph view right here now let's explode this graph view ok we have exploded this graph view, let's zoom ok now you'll see that let's drag this one here and then what you'll notice is we have this first path which has to do with the creation or rather with the loading of our initial original dataset and then once we load this original dataset the next thing we did was to now preprocess this dataset now we'll carry out all the runs previously that's why you have this so you don't really need to take this into consideration from here we'll just continue from this point here we copy out this API here we have that copied and then we get back to our code where we're going to start now where the data is splitting you see that we're going to have, we've already had two artifacts created one was original data, the other the preprocessed data the next will be the trained data validation data and test data and so that's why we call this section the data splitting we'll again copy out this part from the preprocessing data and paste here now we have this let's go ahead and take this off from here and then replace this one here we have this artifact ok we have this artifact now which is the artifact we're using now is preprocessed dataset and then from here we have three artifacts which we're going to create we have trained data we'll just call this trained data set trained data set type let's say preprocessed data, description trained data set and then the artifact directory we could get it from this year so when we create this artifact we're going to get this artifact directory for now let's just create the other artifacts so we copy this out, trained data, we have validation data and then the test data to obtain the artifact directory we're going to run this we have this output here click on this artifact and see we have this preprocessed dataset which has been loaded and we have our prep dataset.npz file here so this here we copy this path and then at the place of this artifact directory we're going to place this path let's take this off we have this path now which has been placed we have now the artifact let's call this artifact file and then let's paste this out here take this off and there we go we've now taken all this off ok so we have this here and then we're going to have our artifact file instead past here, artifact file, so we read that as file and then we're going to look this file, allow pickle and get the array then at this point we define the trained split, valid split and test split from here we're going to paste this out we have our trained array because we're trying to create these three different arrays, we have trained array which goes, takes values from zero to the trained split we define a data length data length which is the length of array zero now recall that the array the array we have in this array is made of is at least actually made of two parts the x and the y this x has a length of a thousand the y length of a thousand, that's why we're doing array, the zero index of array and then we're taking its length, so here we're going to get data length to a value of a thousand and once we have this data length now we're seeing that we're picking out this x we're picking out this x right here and then we're taking values from zero to eighty percent of the total length and so we have trained split which is zero point eight times data length so that's what we have and then we repeat the same for this y then for the validation array, we're going to start from this trained split we're going to start from here, times that, so from the each index with respect to the total data set, that's why we're multiplying by the data length we're going to go from here, right up to the trained split plus the validation split, so now we're going to from zero point eight to zero point nine and we have that set, we'll just repeat this for the validation for the y right here, so let's copy this out we'll copy this out and then paste this here that looks fine we have that paste that, we have it here and this here this looks fine, let's repeat the same process for the test array, copy this and paste out your test array we're going to go from trained split plus valid split right up to the end, so we're going from zero point eight plus zero point one, that's zero point nine times the total data length from the nine hundredth value right to the end we'll just copy this again out here, copy that out and then paste this for the y value so we have this here take this off, paste it and then we go right to the end so that looks fine, again we have now our trained array validation array and test array, we're now set to write this information in our artifacts so here we have with preprocess data, this is instead now we train data, we call our artifacts we just created here is trained data trained data, valid data, and test data so we train data, the new file, we have our we'll call this trained data set npzat mode wb save the file and then we're saving what we're saving is actually just trained array so we're just saving the string array and then we repeat the same process for the validation and the testing, okay we have that, validation validation test test, and then here we have test validation okay we have okay, here we have validation, and then here too we have test then now we log our different artifacts, so we not only log in this one artifact but the three different artifacts stick this back, and then here we log trained data trained data valid data and test data okay we have that set we could run this, let's change it to split data we run this cell here everything looks fine, and then we move on to split all data split data, we run that cell and wait for the response here's the output we get, now the reason why we're having this is because we must have integers and not floats at this level, so when we have this indices we have to convert this all into integers, so we have this int here there we go, int and from here we now run this cell again, and then split all data, the data has now been split successfully, let's get back to our dashboard right here we have our 1db dashboard artifacts, let's refresh this page as you can see we have our data preprocessed data, and now we have the test data, validation data and our training data let's click open this training data so you can look at the file here metadata, API we can always make use of this in creating other artifacts, and let's get to the graph view, in this graph view now we'll be able to see the link between this original data set, the preprocessed data and the training data set, here's what we get, let's click and explode and you can see this clearly now, so here you see you have this original data set this artifact here, we have this run which produces this data preprocessed artifact, and then we have this run which produces this training data set, validation data set, and test data set, so let's draw this, take this off we have this path here, you see we have this here, we take this, this, and this, let's copy out this code right here, and then start with the data augmentation so from here we now download the training data set, we can check this out in our artifacts you should have training data set right here, there we go, we have our training data set, we can now copy this path let's copy this path and paste here, now we can click on this one to be finished to stop that run, and that should be fine now let's go ahead and copy out this part of the code which was used for preprocessing so here we have this here, preprocessed data, we copy this and then, there we go let's paste this here, we have our augment data and then project artifacts we're going to use this artifacts here, so let's come right here and then get this path ok, so we're going to get that path and then replace this one with this opon part or the path to the training data set this paste is out and this should be fine this is actually a file so we have artifact file now we have this take this off preprocessed data, instead of preprocessed data, we're going to use augmented augmented data wanted to be artifact, augmented data set augmented data set type, let's say preprocessed data an augmented version augmented version of the malaria train data set so everything looks fine for now, we have that and then here, let's get back to this and then copy out this here and then paste this right here, so we're getting this from our training data, obviously there we go and everything looks fine next thing to do is do the actual augmentation and then log the data set to our artifact so let's take this back and then we take this off now we have this, we open up this artifact file right here, artifact file, we open that up we obtain our array let's call that array then for images, or for image we obtain the array our array O, we've taken out the X so for image X before moving on, we're going to create this empty list here, data set X, and then we have that ok, we have data set X and then data set X with a pen augment of whatever image we want to pass in let's send this one step, and that's fine so for all images, we are going to do this augmentation then after this, we have data set Y, which is simply the unchanged levels we've had already so from here now, we have all data set X and R data set Y, then we can now run the salt, and then we run the augment data we have here augment data, and they should be fine we obtain this error, preprocessed data is not defined, so let's check back here and we see that this should be augmented data instead so let's change this, and we have augmented data there we go, we run this cells and see what we get now those artifacts have been locked successfully, we could get to 1 dB, and check this out here we have this, and then we click on augmented data set so loading the artifacts now let's load that let's click on this, and check out on this graph, so let's explode, and this is what we have now, so you see again that we have this path, we have our data we split this into train validation and testing you see here you have the training, validation and testing, and then after the training we have this run, which converts this training data into an augmented data, which is this one right here and so at this point, we have different versions of our data, which we could make use of depending on our needs thank you for getting around to this point and see you next time hello everyone, and welcome to this new session in which we'll treat model version with 1 dB now, in the previous session, we had looked at data set versioning, in which we had created an augmented version of our data, which happens to be this version right here from previous versions like a preprocessed version another preprocessed version and the original data, and so from here we are now going to see how to implement model versioning using this augmented data and the untrained model version, so we have this version of the model, which is untrained and the augmented data, which are going to be passed in this run to produce our trained model version. In the previous session, we saw how to implement data set versioning with 1 dB artifacts. We left from this original raw data right here, onto this preprocessed data we created a trained validation test sets, and then finally we had this augmented data right here, and now we are going to see how to make use of this augmented data in training our model but while training this model we are going to implement model versioning, and the two model versions we will have here will be the model before training and the model after training and so we are making use of this sequential model which we had created previously, and the good news here is the way we implement model versioning is quite similar to the way we implement data set versioning we will start by copying out this code, and then we will simply modify that and here we have this 1 dB model versioning, let's paste it out and then here instead of augment data, we are going to log out model so here we have log model, which is in fact this Lynette model right here, we have the log model 1 dB in needs, our project entity specified, the artifacts to be used here is known, so you are not going to make use of any artifact to log this model artifact right here, and so with that we have this, take off this augmented data, we have our model that's called untrained model, and then we specify that it is this untrained model type model, so we have that type model the initial version of our Lynette model there we go, now we have that done this artifact file here, we are not going to make use of this we are not making use of any previous artifacts we just log in this model initially, and then here we will take this off we are not going to make use of this just before writing in our model we are going to save that model so here we have this model to be saved we call, this is our Lynette model actually, so we have Lynette model save, and then we will give it a file name, so let's call it Lynette.h5 and that's it, so we will save this model, let's have this file name to be Lynette h5, so let's define this Lynette h5 file here, and take this off so we have now our file name now, since we have already saved this file instead of going through this process here of creating a new file and all of that, we are just going to add a file, so here we have our untrained model.add file, and then we specify the file name so that's all we need to do, and then we are also going to do 1db.save that file based on the file name, and that's it. So let's take off this part here, and then we are going to log our artifact, which happens to be our untrained model so we have your untrained model and that's fine. Now we could add in some metadata, so let's have metadata which equals our configuration, which we have defined already. In here let's modify this name, this is our untrained model so here is untrained model and that's fine. So with this we could now run this here with this successful run right here and then we can now go back to 1db. Getting back here we have this under our artifacts, we have the model so here we could click on this model, and then we specify the untrained model so let's click on this and we have this overview you see we have that API metadata, you see the metadata we've logged in, we have the files you see here the Linux model and then we have as usual our graph view which we could explode notice that it's quite simple because for now it isn't linked to our data set, so all we have is just this run which produces this untrained model or this initialized model from this point we'll go ahead to train our model and log that trained version of the model. So again here we have this artifact which we are going to use in training the model which happens to be our augmented data set. We call that we had logged this augmented data previously, we could have this overview from here, we have this augmented data set and then it's of type preprocessed data, we've seen that already we downloaded data and then we create this new artifact which is going to be our trained sequential model we call this trained sequential model so of type model, description a trained version of our model, simple as that. So that's it. We could also pass in the metadata just as we had seen here with this log model where we pass in the configuration so metadata equals our configuration and that's fine. Getting back to this augmented data method we had seen previously we're going to follow this same pattern that is we're going to download the data as we've done already we download the data and then we have this artifact file here the artifact file we're using was a trained data set because we needed to take this and convert it into an augmented data set and then we had this processing right here to produce data set X and data set Y. Now we're going to do this exact procedure for training our data on the augmented data so let's paste this out right here and then here our artifact file will be this org data set so here we have org data set and we get this right from here. Let's get back to org data set, files and there we go. We see we have this org data set.mpz which we started previously and so once we are going to download this we're going to have this file which is going to be found in this folder which is also found in this artifacts folder. So we have artifacts augmented data set version 0 which is from this. This means that if we want to use the version 1 we just simply have to specify your version 1 and so on and so forth. So that said we have our artifact file data set and then we go to this processing to obtain data set X and data set Y and then from here we'll go through this series of steps to convert this data into a tensorflow data set type data. So here we have DX, DY this data set X we convert to tensor so we convert this data set X to a tensor and data set Y to a tensor. Then from here we apply this from tensor slices method which takes in DX and DY. That's our data set X and our data set Y to create this new data set D. Now we go to the same processes we have seen already that we shuffle, we batch, we prefetch and then we have our training data set. Now note that you could repeat the same process for the validation but here we just do this for the training and you could take it as an exercise to do the same for the validation. Note that for validation we didn't make use of the augmented data set we instead made use of the validation data set itself. Let's get to the graph view so you could see that clearer. And as you could see here we have this data which is preprocessed preprocessed data, the training data set preprocessed data, the validation data set, preprocessed data, test data set and then from here we have the augmented data which is this one. And so when you want to carry out validation you're not going to be making use of this training data obviously. So we're going to make use of this one instead. And so if you are to put in the validation in here, let's convert the validation data into a TensorFlow data set type data and then use it for training and evaluation then you would have to make sure you modify this here. So that said we continue with this process we have the trained data which we just created is now a TensorFlow data set type data and so training can now go on smoothly. So you see here we have the metrics we've used to seeing this already compiling and then we fit the model. So here we are not going to include the validation we have that already we're going to do the training and after training our model we would have this so let's simply just have that lunet model that fits and then after this we will have similar to what we have seen already here in this log model method the file which has been saved and then saved to 1db and then the artifact which has been locked. So let's copy this out and then get back here and paste this. Now let's change this name let's say lunet trained. lunet trained that's our file name untrained model.add file it's actually here we have this trained sequential model so let's have this as trained sequential model there we go. We have the trained sequential model.add file file name 1db save file name run the log artifact the trained sequential model. Trained sequential model. Okay so that's it. So we have this set we could now run this method and then see what we get. We obtained this error where we've been told name lunet model is undefined. Now as you had seen previously with this log model method we made use of the lunet model which we had defined right here. So we defined this lunet model using the sequential API and that's what we had and this led us to having this year where we had a run so initially we had this run which produced our untrained model so we have this year untrained model. Let's call it um untrained model and since in our case we're trying to build our trained model it's going to be reasonable for us to make use of this untrained model which we've started already in 1db since we're doing model versioning and so instead of us making use of this lunet model we're going to make use of this version of the untrained model which was thought already in 1db. So from here we're going to run another process which now will lead us to this trained model but then to run this process we will also need data. So as you could see you have the data that's our augmented data from here we have the trained data. We have different other nodes which came before this so we call what we've seen before having the untrained augmented data which is this one here and then this augmented data together with this untrained model will produce our trained model and once we have this trained model we can now use it for prediction and so coming back again if we had to use this lunet model which was defined here in the notebook would have this data set that will have our data which will pass into the run and then we'll have our outputted trained model and then on the other side we'll just have this outputted like we have this run and then we have this outputted untrained model. So we'll have the untrained model and the trained model which will exist like two separate entities in our global graph whereas what we want to do is be able to link this two up and so that's why we're not going to make use of this lunet model which was defined here and this is why we're going to make use of the model which has already been stored in 1db. That said, getting back to the code we're going to copy this out here and then in the place of this lunet model as instead of using the lunet model we've defined here we are now going to make use of our artifact which was stored already. So let's have this here. So let's have that pasted here and then we'll make use of this artifact which is in fact called the untrained model. So you see we have this here. Let's copy this out we copy that out and then we paste this here. Let's paste this here and there we go. Just after downloading the model the next step will be to have this artifact file which is simply our untrained model which we should have downloaded. So in that case we're going to have this here artifacts stored in the artifacts directory artifacts and then in here we'll take this out from here. Let's copy this and paste out here our untrained model and then to be more specific we could get the file name from here. So here you see we have the file name you could get back here overview and then you have files in the net h5. So this is a file we're going to download and here is it here. So we have that. So now we have the artifact file which we want to download we have this path and then now we could make use of TensorFlow's load model method. So here we have now the net model which is equal tf.keras, models and load model which takes in the artifact file there we go. So now we have this lunette model which is not gotten from this definition here but which is gotten from a previous version of our model which has stored in 1db. So let's now run this and see what we get. After training for over 3 epochs here are the results we get and then we get to 1db you see here we have the model trained sequential model and then the untrained model. So let's click on this trained sequential model loading artifacts. There we go we could look at this API metadata which we loaded and the files. So here we have lunette trained and then we also have our graph view. Explode in this view here's what we get. Now let's focus on this part here and as you can see we have this run which produces this untrained model here. We have this untrained model and then this untrained model together with our augmented data set been passed through this run now produce our trained sequential model. And that's it for the section in which we'll see how to implement model version with 1db. Hello everyone and welcome to this new and exciting session. In this session we shall be building a system which permits us automatically detect weather and input which in this case is an image is that of an angry, happy or sad person. So this person may be sad as you can see here. This person may be angry as you can see here. All the person may be happy so in fact we want to be able to have this kind of input pass it into our system which we want to build and then automatically infer that this is a happy person and so to train the system we'll be making use of this data set which has been made available to us by Muhammad Hanan Asgar and since this data set is available on Kegel just anyone could have access to it. We are now going to take a closer look at this data set as you can see here we have this test and train directories you open up the train open up the test you see you have these three different folders angry, happy, sad. Here for the train you have angry, happy, sad too. So what we do is generally when trying to create this kind of data set for classification problems what you want to do is make sure that you put each or you put the different images in separate directories. So all images representing angry people are in this directory, happy in this directory and sad in this directory. Now this way it becomes easy for us to build our model taking this data set as input. Also in the previous sessions we explained the reason why it's very important for us to split our data set in this kind of manner such that while you use part of the data set as a training to actually train or build your model you're going to use the other part to evaluate this model. So although in this case we've just been given this data which is very unlike what you have in real world scenarios where you will be asked or tasked to build your own data set like this one thing or one very important thing you have to note is the fact that if for example you're building this kind of data set where say we want to monitor the usage of an app or better still monitor how the users of an app react to a certain feature then it's important to get a data that reflects or is very similar to what the data or rather what the model will be seen during inference. And so this means that if you have this kind of data or if your model is going to be seeing this kind of data during the inference or when it's going to make use of the model to predict whether there's a happy image or not then training on this kind of data isn't a good idea although this is images or these are images of happy people. So you have to ensure that the data you train on is representative of the data or the kinds of inputs the model will be seen when it's going to be deployed and left to predict or make predictions from these kinds of images. Then if you have noticed not all classes we have here have the same number of files. So here you have a happy 1000, angry 500, sad 757 for the train is 1525 year 3000 sad 2255. So this shows us that when solving your real world problems it may happen that some class or it's easier to gather data from a particular class as compared to the other. So here maybe the other of this dataset found it easier to gather happy images as compared to angry images. Another very important point to note here is the fact that the kind of problem we're trying to solve is that of multiclass classification. So unlike previously where we had a model, we had some data which when passed in we would say whether this is a parasitic or nonparasitic input cell whereas here what our model outputs isn't just either one class or the other but a model which outputs a given class out of several options. So those options could be 3, 4, 5, 6, 7 or even 1000 different possibilities. Now all the different possibilities are termed classes. So in this case we have 3 classes. The first class is that of the angry person, the next class happy person, the other class a sad person. Now to be able to download this data, the very first thing we want to do is create this new API token right here. So you get to your profile, click on account and you'll have this here. Click here and save this Kaggle.json file. So we'll get that saved. Then getting back to the Colab notebook we'll simply copy this in here. Okay and then we should have this here. Now before moving on, it's important to note that you will have to sign up for a Kaggle account. And then the next step will be to install this Kaggle package. So pick install Kaggle, run that cell then we create this directory. So we make this directory and then we copy this JSON file in this directory. So again we run this cell. Then from here we give the user full rights to read and write into this Kaggle file right here. We run the cell and then get back to Kaggle where we are going to copy this API command. Now after copying this API command, we're going to paste this just here. So here we have Kaggle datasets download. So we're going to download this dataset, this human emotions dataset. Let's run this cell. Our dataset has been downloaded. Now we're now left to unzip this downloaded file. Let's click this here. You see we have the emotions dataset.zip. We now unzip this and store in this dataset folder. So you're going to run this cell. You'll see that it's going to create this folder dataset and in this dataset you have the emotions dataset which we saw in Kaggle. This is called the dataset right here. From this point we'll go on to generate a TensorFlow dataset based off the images which are found in the directories. So we're going to make use of this image dataset from directory method which is in the Keras utils package right here. So here we have this description. Here we have these different arguments directory. Now the directory let's copy this. So we see clearly this in the code we copy and paste this out here. This directory is going to be simply one of these directories here. So here first thing we'll do is create our train dataset. So that's our train dataset which is a TensorFlow dataset. We've seen this already. We're going to specify the directory. So let's call this train directory and we'll simply copy this path. So let's copy this path and then paste it out here. Copy that path. Paste that. Now we have our validation directory. We're going to do the same thing. So basically again we're going to copy this here. Copy here and paste. Now here we're going to use this as our train dataset and we're going to use this as our validation dataset. So here's our train. Here's our validation and that's it. So with this let's run this cell. That's it. We get back here. We change this to train directory. Now here the levels are inferred and this means that the levels will be generated from the directory structure. So if you could look at this here this example. Okay you have this main directory and you have the directory structure. That's why at the beginning we made mention of the fact that it's important to maintain this kind of directory structure when dealing with classification problems. So because we have this kind of directory structure where we have a class we have images we have class and then images like we can see here we have this in the train we have the class we have the images. We have this class and this images. This class and this images. We are able to automatically create this kind of dataset. And that's the role of this inferred right here. Then the next one we have is level mode. Now for the level mode we have several modes. Here the default we have the int we have categorical, we have binary unknown. Now let's explain what these different modes mean. For the integer mode we simply meaning that when we design or the way we design our dataset is such that we have an image and we have a level. So let's say we have this image of this person. The person is happy. And we have three different possibilities either the person is angry happy or sad. So here what we're going to have is angry angry we're going to give it a value of zero happy a value of one sad a value of two. So here are the three different possibilities. And so when designing this dataset or when creating this dataset we'll criticize that we have the image and this integer which reflects what that the emotion in that image. So here in our case in the case of happy would simply have an output of one. Now that said either we create our dataset in this way or we use a one heart representation of this levels. So what this means is instead of having a zero we'll create a vector of size three which has three different positions. Where if we have an angry input so if the person is angry we'll put a one on this position and zero zero here. If the person is happy we'll put a zero here a one and a zero. And if the person is sad we'll put a zero here a zero and a one. And the way this encoding works is that where we have a zero you see the position is one for the one heart representation where we have a one happy person we put a one at the first position where we have a sad person who put a two we have a two and so we put a one at the second position and then with this there will be some differences in the designing of the last function which we are going to see in subsequent sessions. Now we have this binary for the binary is like the previous example of the previous project we worked in where we have two classes so where we have two classes we just have the level mode to be binary in our case we may pick out the integer or the categorical then from here we have the class names. Now if you decided to infer directly from the class directory then it's important to make sure that those class names match the subdirectories. So you should ensure that if you're for example you have this class names let's put out the class names here. So here we'll have class names and let's say we want to have angry we have angry, we have happy, we have sad, we have sad. Okay so here's our class names. Now if we don't put this exactly as it is here we should have an error so let's run this. We have class names there. We get back here and instead of having this here we're going to specify our class names so here we have class names there we go. So we have that. The next is the color mode, RGB, the batch size by default is 32 the image size, here the default is 256 by 256 we could always modify this. Shuffling by default is true so by default we're going to shuffle our dataset to we do not need to explicitly do this shuffling so we just have to specify this to be true and that's done. Now for reproducibility we could give a seed such that we always have the same shuffling. Then we have this validation split so here in a case where let's say we have just one directory. Let's say we just have the dataset directory. We do not have this test. We may want to split this directory into the training and validation. So here we could have your 0.2 and automatically we're going to have a split of this data set into the training and validation data sets. Now once you have the split, since you're creating the train data set, you specify that this is training. So training is actually in the documentation here you have your validation split an option of float between 0 and 1 which is logical since it was split in our dataset. And then we specify either training or validation. So let's get back here. So in this case it will be training. The interpolation bilinear, following is false crop to aspect ratio false. So these are the default values. We're not going to use this. Take this off and in this case, or in the case where we would have had validation, we just have your validation. So anyway, we're not going to use this. Let's take this off. That said, by default the validation split is known. So optional float, there's no, we're not going to do any validation. So that said, let's have this off and let's run this and see what our dataset, our training dataset is about. So here we have 6799 files belonging to three classes. Now let's go ahead and modify this. Let's add that. This is that let's run this now and see what we get. You see already we have this error because there's no match and the class name is passed in a match. The names of the subdirectories of the target directory so expected this, but instead received this. So let's get back, modify it here. There we go. That's fine. We run this here and that's fine. Okay, so that's our training data. We could do the same for our validation data. So validation or let's say a vowel and your string. Okay, so your string, your vowel and we have the vowel directory. We have the vowel directory. Let's ensure that here we call this vowel vowel directory. We get back and that's it. Okay, here's inferred, level, mode, int, class name, same class names, RGB, 32, image size, 256. Now let's change this and have configuration and batch size. Okay, so that's our batch size. Here we have this image size. So image size. Take this off, paste that out. We have here image size. Okay, so we have this configuration and then we could create this here. Configuration there we go. We have the batch size which will take a value of 32. We have the image size which will take a value of 256. So we have that. Let's run this. We get back here. This configuration let's copy this here. And that's fine. We have 2,278 files belonging to three different classes for our validation data set. From here we're going to look at our data set. So let's say for image and level in for I in the data set. Let's take one batch. We're going to print out I. So run that. Scroll down see we have the images. We're interested in looking at the levels for now. So let's go down. Scroll up to this. You see the levels? We have two, zero, zero, one. You see the level is between zero and one because we have three different classes. Now let's modify this. Instead of having the level mode to ints, let's know this is ints already. Let's change this to categorical. Categorical and this here categorical. Run that. Run this. And okay. So we'll see what we get. So here instead of having two, here instead of having two, two, what we have is this one hard representation here where everywhere is zero except at the second position. So here instead of having zero, we have everywhere zero except at the zeroed position. So that's it. Now we shall go on to visualize this there. So here we have this figure and then we specify the figure size. So we have here fixed size and we have this 12 by 12. So we have that. And then for images and levels in our train data set we have this in our train data set. You could always change this and put, for example, the validation data set. So we have that. Now train data set we take just one. Then we create a subplot here plt.subplot it's going to be 4 by 4 and i plus 1. So we have that and then we'll do an image show. So we plot out the image basically this image right here and then we select a given image. So we have that and then we divide all the pixels values by 255. So we have that and then next one, next step is to plot out the title. So now we plot out the images then we then go ahead and plot out the different titles. Then here we have the outputs, the levels basically. Select the level so select the particular level just as we selected the image and then we have let's, because the levels will be a 1 heart representation then we have to take the max. Now if you're new to this you could check out the previous sessions where we talk about this kinds of methods here. So we have the argmax of this and we have an axis. Let's specify this axis 0 that's fine. I think here we should also just do the plotting. So we should take the axis sorry. So axis and off. So let's run this. So you see what we get before we have that. Anyway here we have numpy so let's convert this to numpy run that again. You have this output there we go. You see that we have the image and it's class above. Now to convert this into some words let's make use of the class names. So we have class names class names and then here we have this. We run this again. We now get the different images and your levels. At this point our dataset is now ready for training. We just have to include this prefetching here for a more efficient usage of our data. We saw these kinds of prefetching. We explained what prefetching was all about in some previous sessions. So here we have the prefetching. We're not going to include the batching because here already we've included the batch size previously. So in this data loading right here we specify the batch size. So our data is already bashed. Now we have that. We run this solve for the training and then we simply redo this for the validation. So this is our validation data validation and now we're ready for building our model. But before going on to build our model we shall copy out this code here from the previous session and paste it out here. So we have this resize with scale layers which we're going to include in the model. Now recall that we could do this resizing like we have our data set here. So supposing you have your data set. You could do the resizing and reskilling. Here there's a reskilling and there's a resizing to the required size before passing this data into the model. So this could be done before passing into the model such that you could train your model on this resized and reskilled data. Now another method will be to pass in your data here. So let's suppose that we have let's take this off. We suppose that here we have in this middle we have the resizing and reskilling unit. So we resize and reskill before passing into the model. Now instead of doing this what we could do is we could pass the data into the model directly and then carry out the resizing and reskilling in the model as a layer in the model. Now we've seen this already but I'll just explain here again that doing this is great for deployment because when you have to deploy this kind of system you no longer want to resize it again. So here all you do now is just pass in this image and then the model on its own the model which has been deployed takes care of resizing and reskilling. Unlike in this kind of system where when you deploy this model you have to do the resizing and reskilling on your own. So this means that if you deploy this model on some cloud solution and then you're calling it from a JavaScript client for example or say some Android client then you would have to carry out this resizing and reskilling in JavaScript. Unlike here where you just pass in the image and the model takes charge of resizing and reskilling. So that said let's take this off get back to our code here we're going to run this simply and then we start with building our model. Here we have this error resizing not defined so simply we have Keras layers here so we should just as we did here to the resizing so we have resizing there we go run that cell again and then get back to our model. At this point our data set is now ready for training we just have to include this prefetching here for a more efficient usage of our data we saw this kind of prefetching how we explained what prefetching was all about in some previous sessions. So here we have the prefetching we're not going to include the batching because here already we've included the batch size previously. So in this data set loading right here we specify the batch size so our data is already batched. Now we have that we run the cell for the training and then we simply redo this for the validation. So this is our validation data validation and now we're ready for building our model. But before going on to build our model we shall copy out this code here from the previous session and paste it out here. So we have this resizing scale layers which we are going to include in the model. Now recall that we could do this resizing like we have our data set here. So supposing you have your data set you could do the resizing and rescaling here there's a reskilling and there's a resizing to the required size before passing this data into the model. So this could be done before passing into the model such that you could train your model on this resized and rescaled data. Now another method will be to pass in your data here. So let's suppose that we have let's take this off we suppose that here we have in this middle we have the resizing and rescaling unit. So we resize and rescale before passing into the model. Now instead of doing this what we could do is we could pass the data into the model directly and then carry out the resizing and rescaling in the model as a layer in the model. Now we've seen this already but we'll just explain here again that doing this is great for deployment because when you have to deploy this kind of system you no longer want to resize it again. So here all you do now is just pass in this image and then the model on its own this model which has been deployed takes care of resizing and rescaling unlike in this kind of system where when you deploy this model you have to do the resizing and rescaling on your own. So this means that if you deploy this model on some cloud solution and then you're calling it from a JavaScript client for example or say some Android client then you would have to carry out this resizing and rescaling in JavaScript unlike here where you just pass in the image and the model takes charge of resizing and rescaling. So that said let's take this off get back to our code here we're going to run this simply and then we start with building our model. Here we have this error resizing not defined so simply we have Keras layers here so we should just as we did here to the resizing so we have resizing there we go run that cell again and we're now ready for modeling Hi guys welcome to this session on modeling in which we are going to build our own model which permits us pass in these kinds of inputs and it tells us whether the input is that of a person who is sad, angry or happy. In the session we are going to start with the lunette model which we saw in the previous session while modifying some parameters to suit the problem we're trying to solve and then move on to even more complex and better computer vision models. We copy the code from the previous session and paste out here. So there is it we have this lunette model which we saw in the previous example and then we also have this number of classes here which we have to set so that set here we change this number of classes from 1 to 3. Now the reason why we're doing this is simply because in the previous session we're building a lunette model which takes in images and outputs whether these images are those of a parasitic or a known parasitic cell. Now in this example we're taking in images and our model has to decide whether is that of an angry person a sad person or a happy person. So we see that here we have 3 different classes and because of that we're going to change this to from 1 to 3. So we have this we run the cell that's fine we also have this other configuration the learning rate, number of epochs, the dropout rate the regularization rate, number of filters, kernel size number of strides, pool size number of layers or rather outputs in this dense layer and number of outputs in this other dense layer. So with that we're just going to run this cell. Now note that you could always get back to the previous sessions to understand each and every parameter we have here as we've discussed this already. So we just seem to replicate in this Lunet model we've seen previously. Now that cell you see the kind of output we get here. We have 6 million parameters. We have our different layers going from count of layers to our fully connected layers. Now the next point here is this activation right here. So this activation now with the previous case study was the sigmoid and this was because we were actually deciding whether an output would be one class or the other. It was a binary classification problem wherein we could use this kind of sigmoid activation function. With the sigmoid activation function if you can recall we have this input and we have this output. So let's say x gets in and y goes out. And here's 0 the 0.5 here we have this and 1. So what goes on here is as we take in higher values of x the sigmoid function approaches 1 while as we take in high negative values of x the sigmoid function approaches 0 and so the range of values here is simply 0 to 1 and this is logical since in the binary classification kind of setting we wanted to output a 0 for one class or a 1 for another class. Now intermediate values like 0.7 will be taken to be either one of these classes depending on our threshold. But then the role of the sigma here is to make sure that all our values that's based on all the inputs we are able to have an output which always lies between 0 and 1 as we could see here. See that we are always in this range here 0 to 1. Now in a case where we are having a multiclass problem like in this case where we have three different classes like this and that what we don't want is to just say for example here we have an output 0.1 output 0.7 output say 1. So we do not want this kind of outputs right here since we are dealing with a multiclass problem with single level. Since here our output cannot be two of those classes so in some problems or some kind of problems we may have a situation where the person is maybe sad and say angry at the same time. Now in those kinds of situations you could thought to have these kinds of outputs where say this two here, these two classes have high values. Nonetheless in the kind of problem we are trying to solve we want the model to choose or pick only one out of the three different classes. So we are not going to pick two one. We are going to pick only one. And because we are going to pick only one what we will have here or what we will try to do here is to make sure that this output sums up to one. So instead of having let's take this off, instead of having this kind of output we will have an output which sums up to one such that this here, if you have that such that one, the one with the highest class or the one with the highest value is considered as that class which the model has selected. And so we could have different kinds of values we could have say 0.1 here 0.2 but here since we already have 0.1, 0.2 we are going to have 0.7 since we want to ensure that the sum of all these values should give us one. We look at this 0.3 plus 0.7 gives one. So we make sure that our values here lie between 0 and 1 and when we sum them up it gives us one. Now an activation function which we could use in achieving this is the softmax function. So let's have here softmax so instead of the sigmoid, now we have the softmax. And to better understand how the softmax works so you clearly see that difference let's take this example from the analyticsvideo.com website. In this example you're considering that we have three classes in this output. Let's take this off here instead of 10 like what we have now here they have just these three classes right here. Now after applying the softmax we're going to have outputs here. Just like when you have some output after applying the sigmoid you have this other output. So just like for example if you have an output let's say 5 after applying the sigmoid of 5 you have a value like 0.99 whatever so it's a value very close to 1. Whereas if you pass in a value say negative 10 after passing in the sigma you're going to have a value very close to 0 say 0.001 so this is how the sigmoid works. Now for the softmax function is quite different. Here what goes on is we're going to make use of this formula here. So we have EX let's say XI or let's say XC for a particular class divided by the sum of EXC for all the classes. Now this formula shouldn't scare you as we're going to explain how it works in detail here. That said we're having this input just like with the sigmoid where you have the input coming from the output of the dense layer. Now here we have this here we have obviously three inputs because we have two different classes so we have this one this one and this one. Now you should note the values here let's take this off so you can see the value here. So you note the values this one is 2.33, negative 1.46 0.56 Now once we have this we're going to simply apply this formula and the way it's applied is as such. You take this value 2.33 you have the exponential of that value so you have e to the power of 2.33 as you see here of that particular class divided by the sum of all these exponentials for all the different classes. So here you have e to the power of 2.33 there is it divided by e to the power of 2.33 plus here e to the power of negative 1.46 plus e to the power of 0.56. So you have all this summed up. That's basically what this formula here means. For all the different classes sum up e to the power of the values that these classes take up. Now once you have that for this first class, class 1 you obtain this value. Now for the next class you have e to the power of negative 1.46 divided by this same sum here. So basically this sum is the same everywhere but the only difference is the numerator so you have this numerator which changes here we have 0.56 we have this other numerator and we have this value. Notice how as the value goes to us negative, high negative values this value is approaching 0 while when the value is going to us a high positive value it's approaching 1. So this means that if we replace this and put a value like say 10 we would have a value even very close to 1 like say 0.9. So that's it and that's basically how the softmax works. So this means that at every given point when you sum up all these outputs you have a total value equal 1. So you could take this 0.83 plus 0.01 let's say 0.84. Let's work in two decimal places. So 0.84 0.84 plus 0.14 is like 0.98 so you add this other parts up to give you 1. So basically what you're doing with the softmax is you're taking some inputs for let's say 3 classes and then you're sharing the values so you have 1 that you want to share. You have this number 1 which you want to share you're going to give this one a fraction of this whole 1. In this case the fraction is 0.83 and then you're going to give this other one's own fraction, this other one its own fraction. Such that at the end all this here sum up to be equal to 1. From here we're going to get straight into the training and start by designing our loss function. So we have this loss function which is going to be the categorical crossentropy function. So unlike before where we had the binary, now we have this categorical crossentropy. We have it right here on the documentation. You have TFKRAS losses categorical crossentropy and then one of the arguments is this from logits. Now the from logits as given here, let's get back here. The from logits by default is false. So by default we suppose that we have from logits to be false. And as given here, this from logits says whether the ypred is what the other y which is outputted by the model. So this is whether the ypred is expected to be a logits tensor or not. And by default we assume that ypred encodes a probability distribution. So by default we're supposing that what the model is going to output, in our case we have three classes, what the model is going to output is going to be such that when we sum all these probabilities or when we sum all these outputs here, it's going to give us a value of 1. And so when we have by default that's from logits to be false. So by default we have from logits to be false. When we have this it's simply meaning that we're supposing that what is going to get into this loss function is going to be a logits tensor. And that's our case here because we've had or we have this softmax activation. Now in case we didn't have the softmax activation, then we will have needed to specify this or set this to true such that what gets into the categorical cross entropy is a logits tensor. Now that's set, let's go ahead and test this example here. We have this example, let's take this example copy this and then just paste it out here. So we have this example there we go. Here we have CCE, categorical cross entropy. We're going to print this result. So we print out this result and see what it gives us. We run that and see we have a value of 1.17. Now this value here tells us how close the model's prediction is to the true values of y. Now let's modify our model's prediction such that it's very close to the true values of y. So here we have 0, might be 0.05 okay, 0.95 okay 0, 0, 0, 0.1 0, 0.8. Now we should reduce this to 0.08. So it's very close to 0. Now here instead of 0.1 let's say we have 0.0. Now let's change this to 0.05 and then here we have 0.85. Okay, so all this sums to 1. This too sums to 1. That's it. You see they are very close now. Now with this we run this cell again and what do you notice? You see the value drops by almost a factor of 10. So this shows us that these two are very close to each other. Now let's change this and say 0.0 let's have here 1.0 and here 0, here 0, here 1.0 we run that and what do we get? You see you have a value which is practically 0. And this is because these two values are very close. Now if you change this again, so let's have here 0 and have this 1.0. Let's now make these values completely different from each other. So here we have 1.0 and that. So this means that this year the actual prediction is this position here while this one or what the model predicts is a different position. Now here the actual is this but the model predicts this. So the model fails to predict for each case here. So here let's run this again and see how high this value is. So this shows us how this categorical cross entropy loss actually works. Now to gain even more in depth understanding of how this works let's consider this following formula. So this, the categorical cross entropy loss defined here is simply we have this negative here the sum of y true log y pred. Now we take this example here for this case or let's modify this such that this first prediction here matches this prediction and or rather this second prediction matches this second prediction and this year doesn't match. So this example this doesn't match. So the first example doesn't match while this other example matches. So what the model predicts matches with what the model was meant to predict. Now that said let's get into this formula and see how it works. So we understand why we're getting high values when there is no matching and very low values when there is a match. So here for example we have 0 this y true just use the formula 0 then log of 0.01 Now we have this summation so plus this next example 1 this is y true 1 log of 0.05 you have that plus next one 0 log of 0.96 Now obviously this cancels because we have 0 here till we have 0 this goes away this goes away we left only with this let's take this off so we left only with this year after this computation ok now log of 0.05 gives us negative 1.3 approximately negative 1.3 so we have here approximately negative 1.3 but with this negative here it gives us 1.3 so we have this value of 1.3. Now let's take for example here for this case where there is a matching we have 0 log of 0.1 because we have 0 it's not going to be considered here we have 0 again 0 here it's going to multiply that it's going to be 0. Now this other one we have 1 so this other one we have 1 log of 0.7 1 log of 0.7 what do we have here this gives us negative 0.15 so we have approximately 0.15 so we see that with where there is a matching the loss is reduced and when there is no matching the loss value is increased so with that we just simply take this off since it's our default and that's fine so we have this now but before continuing let's also consider a case where our outputs like let's get back here we're going to run this again get the loss we have so now we're going to consider that this Y true that's our dataset is going to be designed such that instead of having this categorical kind of output we're going to have the integers so here instead of 0.1.0 we're going to simply have 1. Now let's get back to why this is so we have 0.1.0 this is translated that's this for categorical for integer this is translated as 1 because the 1 is at the first position starting from 0.1.2 now in the case where we have 0.0.1 this is translated as 2 since this is at the second position 0.0.1.2 so that's it so we have seen this already in the previous session now getting back here let's modify this instead of having this let's put 1 and instead of having this let's put 2 so we are saying that if your dataset is designed in this way recall we have seen it here just here let's get into dataset loading just here if the liberal mode is int then you should have this kind of design there we go right here okay so we have this kind of design instead so we just have these integers so if you have this let's run this here you see it doesn't work now what we will do in the case where we have this kind of output is we're going to use a sparse categorical so instead of the categorical we have the sparse categorical you see you run that and you have the exact same answer you will have in the case where it was categorical let's get back here run that again so you have the exact same answer so that said we could make use of the sparse categorical it depends on how we created our dataset so here we have sparse categorical and we should just comment this since we're not making use of that let's take this off now and then we move on to our metrics so here we have the metrics the metrics we'll be using here will be the categorical accuracy so we have categorical accuracy let's give it a name let's call that accuracy and then we will also have the top k accuracy there we go we have our top k categorical accuracy we'll give it a name no before giving it a name we need to give it the value of k so we'll give a value of k of 2 and then we'll give it a name top k accuracy now before we move on let's explain this top k categorical accuracy metric right here so unlike with this accuracy with this categorical accuracy where if we have for example this four stations here for example this doesn't blue what the model predicts and those in red are what the model was expected to predict accuracy will be computed as such so we'll start with this first one the highest here is this 0.5 the highest here is this since there is no match we have a 0 plus we move to this next one the highest here is this the highest here is this and the highest is this at this position there is no matching so we have a 0 and that's because the class 0 was picked whereas the expected what should have been class 1 now we get here the class 0 is predicted by the model and the class 0 is also what was expected the actual output so here we have plus a 1 because we've gotten this correctly so this is correct, this is wrong this is wrong now we get here we have the same situation the highest here is 0.7 the highest here is this and the positioning is such that there is a match so we have plus 1 now because we have 4 different examples we divide all this by 4 and multiply by 100 that gives us accuracy of 50% now we get back to this case here for the top k categorical accuracy for this first case the highest class here is this one the second highest class is this other one here so we have this highest class second and we have this third so it's in this order for this one here this is our first we have two of them and then this one this is our first and this is 0.05 so here we have two of them this here is our first and this is our second because we've selected k equal to we're interested in just our first two highest predictions so with the top k categorical accuracy we are not interested in making sure that the highest prediction here matches the highest prediction here what we're interested in here is if any of the two highest matches the highest prediction we expect so if any of the two predictions of the model matches the exact prediction the model should have predicted then we'll give that or we consider that as a correct prediction so in this case here this first two do not match this so this is a wrong prediction and we have a zero plus we move to this next one here the highest here is this and it matches with the second so we have a one so this is considered now a correct prediction unlike previously where we would have considered this to be a wrong prediction now for this one the first two of these three are there so obviously this will be a prediction it also matches here the first matches with this first year so we have that plus one and then this other one year we have the highest prediction here matches with this so we have plus one so here we have three divided by four times a hundred this means that the top two categorical accuracy in this case is equal 75 percent unlike the categorical accuracy which is 50 percent now with that we go ahead and compile the model learn that model let's run this we have item learning rate which we specified in the configuration the loss function here the metrics this and that's it so we compile our model and then we set now to train there we go let's paste this out and here instead of this here we should have configuration number of epochs then obviously we have our training data set and we have our validation data set so let's run this and see what we get go down and that's it see our losses dropping accuracy increasing and top key accuracy which is clearly higher than the accuracy training now complete let's plot out the loss curves for the validation and the training as you can see here both the validation and training losses all drop together while the accuracies for the validation and training also increase up to this point here so it's almost getting to value of one now you could check this out here you see the accuracy 97.8 percent year 98.19 percent so the model is performing quite well we could evaluate this model on our validation data and here we have the lunette model dot evaluate so we call the evaluate method and then we pass in the validation data set okay so we run this and there we go we have a loss of 0.35 accuracy 98.33 percent top key or rather or better still top two accuracy 99.88 percent now note that given that the model isn't over fitting the model keeps or the models metrics keep increasing what you could do here is increase the number of epochs so we could train for more epochs to get even better results now we're ready to test out this model on some image in our testing data set so here we're going to have this image or let's call it test image we have test image which is going to be read which is going to be this image we're going to read using OpenCV library so we have here cv2.imread and then just in here let's open this up and the test let's take up say happy we copy this path here copy this path paste it out here there we go and then we're going to convert this into a tensor let's close this we have here now our test image let's say im is equal tf.constant test image and then we're going to specify the data type so here is float32 and then with that we're just going to pass this so let's print out the output let's first of all print out the shape see the image shape 90 by 90 by 3 pass this into our model directly because as we have designed this we have put in the resizing in the model and the rescaling tool so we're going to resize and then we're going to rescale in the model such that now we do not need to do that out of the model so that's sad let's get back here and all we need to do is call our line model but before calling that we need to add one dimension since we're passing this input in the model as batches so we add the batch dimension here tf.expand dimensions and then we have the image we specify the axis 0 so once we have this now let's have our line model model which takes in that image so here we're going to print out this output or print out what our model gives us we're getting this error here where we're told that there's this incompatibility issues between the input image and what the model expects that said we get back to the model and we noticed that we didn't actually put the resizing so let's get back here and we have this resized rescale layer which we have built already and we put that instead of this reskilling we have this resized rescale so we make sure we resize and we reskill ok now next thing we have to do is we have to modify this here because here by doing this we suppose that the input is going to be 256 by 256 but here what we will have is that we will have this known so our input could be any of any dimension but we are going to do the resizing here so we have the resizing in this resized rescale layer and then we also have the rescaling so that's it let's run this there we go as you could see you have from here we have this 256 by 256 by 3 and that's because we've actually passed this input into our resized rescale layers so let's go ahead and retrain our model training and validation plots for the loss and accuracy and with this we could go ahead and test our image so let's run that and this is what we get as you could see here we have 0.99 and then almost 0 so this shows us clearly that the class 1 in this case because here this is our class 0 I'll take that off this is our class 0 class 1 and class 2 so this image is of class 1 and it's correct because this is a happy image so basically you see how to create this image here, this image array from the file path and then convert this into a tensor which is then passed into the model without any preprocessing another thing we want to do is to actually print out the class so what we can do here is instead of this lieutenant model we'll have tf.argmax so we'll look for the class with the highest probability which in this case is this one we'll look for that class so tf.argmax we'll specify the axis now if you're new to this you can check our previous sessions where we treat these kinds of functions authorically so here we have that let's run this you see we have 0 no this is tf.argmax this is negative 1 or 1 let's say negative 1 is the last axis ok we have that you see it picks out this here and then from here let's convert this to numpy and then from here we use the class names to get its name so there we go, class names we run that we're given this let's see what we obtained before this class names let's take this off and also take this one off we run that ok we have this list so we should take the 0th element there we go now we have that we put in the class names and we get the name so that's it you see we get the name happy ok so with that now let's do one last test let's take sag for example and we see how easy it is to carry out such tests let's pick out an image here let's take this one you could actually view this image here so that's it this one the same image take out this other one ok so let's copy this path take this off take this off scroll up and then simply paste it here so this is the path we're trying to know exactly from the model what kind of image it is so you see here it's a sad image and this is from the test set so make sure you're doing this kind of testing with data the model has never ever seen ok so that's it our model is performing quite well we can do something similar to what we had here but with the difference that instead of just giving out these levels we'll give out not only these levels but also what the model predicts so let's copy this code here get back to our testing there we go and paste it out here ok so again we're not going to use the train but the validation data set so there we go we have our validation data set we have this plot and then at the level of the title we'll plot let's have this true level level right here and then we'll have the predicted level so let's have this move to the next line and then we have predicted level yeah predicted level there we go and that's fine so we have this predicted level and we'll get it from here so we could simply have the net model the net model here we have our net model which takes in the images so it takes in the image selects a particular image and let's do the let's expand them so we have expand dimension take in that image axis equals zero and we close that ok so we have this, we pass in the net model and we have this output now we are going to do something similar again to what we had here basically it's even this here so let's just copy this let's just copy this and replace it right here so it's the same thing we're trying to do all we're trying to do here is actually passing the image into the model and get it's class so we compare with the actual level so here we have this class names and this we have plus that should be fine and that's it with this now let's run this cell and see what we get scroll here is what we get you see the model this is supposed to be true the model here doesn't perform quite well unlike what we had in this evaluation so let's check out our code and see if there's any errors scroll this way here you see we have image so we have that that's why you see the predicted is always sad so we have always predicted level always sad because we had picked just one image and it's not dynamic so here we have those images we select the particular image and we run this again this should be fine we get in this error because we didn't add the batch dimension unlike here we added a batch dimension before passing in here so let's get back and then we're going to replace that images the image with this code here so let's get back here and then replace this with this code note that we've treated this already in previous sessions so you can always check out if you're new to methods like expand deems we run that and see what we get ok so that's it you see happy happy angry happy true level happy predicted happy oh let's see if there's any errors the model does quite well see no errors so it's almost 100% in fact it's 100% although the evaluation here shows 98% 98.33% the next thing we'll do is plot out the confusion matrix so we're going to go through our validation data for EAM level in our validation data set we are going to start those levels in this list right here so we have levels that append the level and then we also have predicted that append the lunette model which takes in the images so here we have what the model predicts and what the model should predict so here we have predicted create this list and then levels we also have this list there we go so we have now this that's it so with this now let's run the cell and then before moving on we'll convert this to numpy format so we could easily manipulate it so we have this as numpy we run this again that's fine then here we could print out the levels we have levels let's print this out there we go you see the levels the different levels now let's try to flatten out all these different levels right here so let's have this to be flattened now let's scroll up see this they're in batches of 32 so we can actually see that here scroll down a bit you see we have this batch here then we have the next batch and so on and so forth but then this output format isn't exactly what we want what we want is the classes the class with the highest core so instead of this one representation we want the integer representation so what we're going to do here is use the Agmax so let's print out the Agmax of this let's get back up let's print out the Agmax of this there we go and then we specify the last axis so we have that and we run that or we get an error or we have this error now let's let's do this let's pick out or let's simply select here up to the last value now this error should be coming in because we have batches of 32 but it happens that if you have a data let's suppose you have a data set of 48 items now if you break this in batches of 32 then you have let's even say 98 so here you have the first batch 32 the next batch 32 the next batch 32 so here you already have 96 elements and then the last batch will be 2 because you have you want to have 98 elements here you have 96 plus 2 given 98 so because of this last batch here we get in this error here so because of that we get the error so what we could do is print right up to this last batch so let's run that so we're not going to pick out the last batch here we're not picking out the last batch so we go from the first batch to right up to the batch before the last one so let's run this now and see that that's fine so this works out well we could even print out say the first two batches let's print out the first two batches so you see what that looks like see the first two batches you have this here and this now from here you could actually flatten so you could do this you flatten this so you get all this in this single or one dimension list so this is what we want to get now let's move on so let's get back to this we have right up to the last batch we run we run this and there we go so this is what we get so we've actually flattened out all these elements now if you print out the length of this here see you print out this length you see you have 6784 elements so now basically we want to compare these different predictions here we want to compare this predictions let's take this length off we want to compare this levels sorry we want to compare all those levels with the models predictions so here we could repeat the same process with the predicted let's have predicted or predicted we run that you see we have these two lists here you see already that they are quite similar although this one misses out so what we're doing is we're having this list and this other list this is for the predicted and this is for the levels so now that we have this set let's or let's redefine our pred let's say pred we will call pred to be this year so this our pred which will flatten out so these are the different predictions by the model and then this is what the model was supposed to predict so here are the levels so pred and levels okay so let's have that we run that cell and then we get back to the code for the confusion matrix which we had previously so basically here we had seen in the previous sessions we defined the threshold because we have in binary classification problem here since we have different classes we wouldn't define that we will just simply pass in as year this different predictions so for the levels and for the predicted we have that let's simply copy this out and paste out here so we've seen this already let's take this off now we have level and then here we have the predictions there we go we're going to print out this confusion matrix we have the figure and that's it so let's run this and see what we get there we go we see already here we have this confusion matrix and one thing you can notice straight away is that the most values or the leading diagonal has those elements with the highest values so it's here we have the highest values here and this is normal actually because when you have this confusion matrix like this let's redraw this confusion matrix when you have this confusion matrix like this this is a class zero so let's say class zero is angry so this is class angry yeah happy and sad now here it's angry happy and sad so whenever the prediction or whatever what the model predicts matches with what it was supposed to predict you have an additional one which is added here so we simply go through all the different model predictions for the validation and you see that 1472 times for 1472 times the model predicted angry when it was actually angry so this is correct now for happy you see 2000 here is matches here 2890 times the model predicts happy and the actual was happy and here we have in 600,198 times model predicts sad when it's actually sad and then here we have 21 times the model predicts angry when it was happy and then 26 times the model predicts angry when it was sad then we have 20 times the model predicts happy when it was angry here we have 27 times the model predicts happy when it was sad. Here we have when he was angry and we have a hundred times the model predict sad when it was instead happy so there is you see this this is the highest score we obtained for the wrong predictions and so this means that the model has that tendency of predicting sad when it's actually happy. You can also observe this plot here you see this lighter colors you see here as we go up we get to this higher values see here and then this and this so the lighter the color the higher the score and then the darker the color the lower the score. Now obviously the ideal case will be where we have this purely white so we would have this purely white this purely white and all this you're completely dark with all zero values so that said we've looked at how to have to obtain the confusion matrix which is an important evaluation metric and here we could also change this to training so anyway you just have to have your training and that will be fine. Now we have to deal with that last batch which we did not take into consideration so what we could do is we could do some concatenation here so let's do concatenate concatenate and then we have that then let's copy this we just copy this and paste out your copy and then paste it out here there we go but then we're taking the last element so instead of the all elements before the last now we're taking the last element and that's it so we add in we add in the last element here so that we could actually flatten it out separately before concatenating it with the previous elements we saw that having all of this joined together will cause an error so that's why we're doing this here again we copied this here let's copy this and then paste it out here that should be fine this for the predicted so let's change this to predicted okay so we should have that fine scroll this way and then let's have concatenate okay so let's run this and see what we get we get in this error let's add this here there we go we run that again that's fine and the same should be for the predicted so let's run this again get that error scroll this way and add this here okay so we have this the last batch which has been added up and then we'll simply copy this and paste here so now instead of having just the all the values before the last we have now all the different batches together so here is for the predict now just for the level so let's put this here we have the levels and then this is for the predicted let's copy this get right to the end and we have that okay so here we have this for the predicted space this year and that should be fine let's run this again run that and then we run this and there we go see we have slightly different answers now okay so that's it we've seen how to plot out the confusion matrix for multiclass classification before we move on we noticed that we've made a very big mistake here as with this validation data set we actually pass in the train data set so let's modify this and make sure you never make this kind of error as if you make this kind of error you feel like your model is performing well whereas we are validating on the train data so that we run this again I have to run this again now training is complete and we see that the model wasn't performing as well as we thought you see here the last drops and then at some point stats increasing while that other training keeps dropping and then for the validation we have its accuracy going to us one as we had previously whereas for validation your it plateaus at around 75% as you could see here in those values so you see the validation accuracy the highest we have goes like 75% although the top key accuracy is about 90% okay so that said we see that we see clearly the models and performing that well and the next sessions will see how to better this model performance here we could also run this evaluation now the evaluation CR 7590 and loss of one let's run this testing correctly classifies this next we're gonna try this out to see previously we had generally 100% because that was on our same train data now here you see let's start from up you see wrong prediction right wrong wrong you have here wrong right right right wrong see you see that out of the 16 different predictions we have 10 out of 16 rights so running that you see you have that 62% on this little sample which you took here now we could also plot the confusion matrix let's run this and you could see clearly from here that the model isn't performing as well as it used to perform when we're making that error hello everyone and welcome to this new session in which we are going to treat data augmentation previously we had seen how to load our data set from this data set directory right here and then we trained a Lynette model which performs very well on the training data but didn't perform as well on the validation date and then we're able to evaluate this model on different evaluation metrics like the accuracy top key accuracy and the confusion matrix in this session we are going to focus on data augmentation that is we're going to see the effect of augmenting our data artificially without actually getting to add an element in this data set right here and then seeing how this affects our model performance in the session we'll see how to augment our data like those data right here and see how this technique of data augmentation helps in making the model even more performant now we are looked at the augmentation previously but if there's one thing you have to note about the documentation is simply the fact that it promotes diversity in your data set and so if you have data like this one here let's open this we consider this original data so this is our original data this is the data we actually gather and then we have this brightness here so we modify this data or this image's brightness and we obtain this other data point and then we modify from this we rotate this and we obtain this other data point you see that is exact same image which has been modified and so now the model doesn't only get used to seeing this image right here but now you could see this one or this one and so data augmentation is this method of this technique for promoting robustness in models hence fighting over fitting as now the model can see different versions of certain data points so let's close this up and get back to the code here let's get back up here we have data augmentation we're gonna simply get back to this year we have data augmentation we had looked at this already in the previous session so you could get back and try to understand exactly how this is carried out previously we have seen that we could carry out data augmentation by using these kinds of TensorFlow image methods like this one you see you could rotate the image you could flip left right you could adjust the saturation or you could use Keras layers you're gonna use Keras layers and then also we saw how to use albumentation library which is this amazing library which permits us carry out data augmentation on different types not only classification very easily so you could check out in that video now that said let's simply copy this out here then we paste it here so we have this we're gonna we have this augment layers we have random rotation random flip random contrast let's actually get back here and this layers and then add the random contrast so we have random contrast okay so we have that and you could feel free to get back to documentation on the TensorFlow Keras layers let's get to Keras layers and then check out those different augmentation strategies here you can see random brightness contrast crop flip height rotation translation random weight random zoom and you could check out the documentations in case you have any doubts so you could just have this year and you see how to use each and every one of those augmentation strategies now getting back to the code we have this three year now what was generally done is you could actually run one and then test this or test how it helps in making the better model perform better and then you could add the other and see whether it helps and so on and so forth so it's kind of like it's not a fixed kind of method where you just have some fixed methods or some fixed right augmentation strategies which we just place in this order sequentially and then it will always work magically generally you will have to test this out different different strategies out and then see which one works well for the data you're working with now we have this set let's run this cell right here we have this is not defined let's let's get back here let me show that this we run this cell let's run this cell normally that should be fine get back and we run this one year and that's fine so we have that and then now at a level of this dataset preparation we're gonna include this we're gonna do the mapping so we have our augment layers not that we're not gonna do this for the validation data set we're gonna only do this for the training data set we have this augment layers and then we're gonna specify number of parallel calls so we have known parallel calls and then this is gotten via auto tune so we have that automatically then before we move on you're gonna copy this out here and then simply paste this here so we create this augment layer which takes this and then has that's it outputs the augmented image and the level you're not gonna have this year so let's just take this off we have just this image okay so that's it we have this augment the layers and then we define this function as augment layer so right here let's take this one off and run this again with this we're now set to train so let's go ahead and retrain our model. Training is not complete and we could see that the model doesn't perform as well as it used to do without augmentation so you see here we did augmentation the model performs even poorer as compared to when there was no augmentation let's run this here so you could see I could compare this with the previous results we got other previous evaluation you see we go from 75% to 54% and then you will go from 90% to 83% to understand the reason why we have this drop in performance let's look at this visualization of our data set we'll see that after carrying out the rotation we have images which are rotated at very unusual angles so you see like this image here is unusual this angle too looks very unusual as compared to the kind of data we would have in our test all validation set so we have to ensure that when carrying out this random operation we limit the angle at which we could carry out this rotation so that said if we have a face like this or let's say we have an input image like this we should limit this rotation such that this image cannot be rotated at say 180 degrees so let's have this here so you see that better we have this this and this so we do not want these kinds of rotations what we want is rotations where we have the face be tilted like this and that's it so we want this kind of rotations but not this type so to solve this problem what we're gonna do is we're gonna get back to the documentation that's random rotation we check out this factor right here we see that the value we put here takes us from negative 20% of 2 pi, pi is 180 degrees, pi is in regions convert this to degrees we have pi which is 180 degrees so we have two times 180 which is 360 so when you say 0.2 or negative 0.2 what you're in fact saying is negative 20% of 360 so when you have let's get back to the code so when you have your let's add this cell here when you have your 0.25 what you're having is 0.25 times 360 run on this you see you're going 90 degrees so this means that if you had an image which was already somehow tilted this image we end up in a very unusual position where the face will look something like this and so what we'll do here is we are gonna limit this rotation so we're gonna we're gonna go from 0.025 for example to this so let's limit that and negative we'll go from negative 0.025 meaning that we have here as 0.025 that will be 90 degrees so limiting this to 90 degrees and then going from negative 0.025 to 0.025 simply means that if you have this axis here and then you have the face like this put them out this then after rotation you can only go 90 degrees in this direction or 90 degrees in this or rather 90 degrees in this direction so you have a limit so you can you can only pick a random value between negative 90 degrees and 90 degrees in this direction so you can only go this way 90 degrees or this way 90 degrees so that said the extreme will have a face like this that's after rotation so we'll go from this blue to this red or you could also get something like this so this what we are gonna get after rotation unlike with the case of 90 degrees where if you have a face which is already let's let's change this let's do this if we have a face which is already say tilted like this you have a face tilted like this after rotating 90 degrees what you have now let's let's have here this 90 degrees what you have now be a face tilted this way and this is in a very usual position when taking an image so or when taking a photo so the image is no validation or test data set wouldn't look like this and so that's why we actually limited this year so let's get back to our code now and we run this let's run this training data and then let's visualize our data set there we go as you can see you do not have images which are upside down as we had before and that's it so we have this now we now go ahead and retrain our model and see what we get after training for over 20 epochs your results we get you see that we go up to 78 percent so those are highest we get and when we run the evaluation you see here we have this when we run the evaluation what we get is 77.8 percent for the accuracy and the top case 91 percent so improved compared to what we had before or what we had before the data augmentation now we are going to use another data augmentation strategy which is a cut mix now the cut mix isn't like this other data augmentation strategies where we just modify a single image with a cut mix as we have seen in the previous sections we actually combine two images so what we're going to do here is we're going to simply copy out this code and then put it out in this code base right here now if you haven't or if you're new to the cut mix data augmentation you could check out the previous sessions where we treat this in detail so let's get here and there we go we're going to apply cut mix and see the effect it has after training our model so we have data augmentation in here let's let's add this cell and have your cut mix augmentation add a code cell and let's base out that part of the code we also paste out this part where we have the train data set 1 and train data set 2 then we create this mixed data set so from here we have augment augments layer stick this off and here we have augment layer so we carry out augmentation for this tool separately and then once we have this two year we're not we're not gonna shuffle on the shuffling already so we just do the mapping we do the mapping and that's what we're saying once we have this two year you're gonna combine this into this one mixed data set so that's it let's have that and then from here now we build our training data set so let's take this year let's take this up there we go data set preparation and then we're now gonna comment this part so we could comment this one and we have a validation data set that's fine everything looks fine and that's okay so we have this set let's run this now for cut mix you could always try to mix up all the cut out augmentation strategies and see how it better or how it ameliorates your model performance so let's have this let's run this there we go this our cut mix here and we combine the two data sets and then once they combined we now apply the cut mix augmentation here we have this inside is not defined let's actually you should have okay this should be our configuration so we should have this configuration run this cell now run that again run the cut mix and we run the cells and now everything should be fine so there we go we have all this fine and then we get to validation run that training data set and validation data sets okay so that's it let's go ahead and retrain our model and see what we get after training for 20 epochs we noticed that unlike previously where the training was went to about 99% while the validation was about 77% year the trainings about 80% and the validation let's scroll this way the validation is about or the highest we have here is 78% so this shows us clearly that the model isn't overfitting because the training and the validation data set both are evolving in a similar manner so let's scroll down here and look at this curve you see here before look at the accuracy plot before what we had was something like this we had the training and here we had one and then the validation was like this but now we have these two curves which are evolving in similar manner though sometimes we have these kinds of peaks anyway clearly our model isn't overfitting so what we'll do is we're gonna train for more epochs so that said let's go ahead and retrain this model for more epochs so there we're gonna modify the number of epochs so here we just have to our work and those in place we just keep training so we train for 20 more epochs so let's run this and then we wait for 20 more epochs train are complete the other results we get as you could see the validation accuracy starts to stagnate around this and then after evaluation we obtain an accuracy of 79.71% but what's interesting to note here is the fact that our training accuracy is still having this value of 86.25% unlike previously when where our model was overfitting we had this accuracy of about 99% so that's it we have that evaluated we see these values and then testing on these values here you see that we have 14 out of 16 images predicted correctly and here's our confusion matrix right here so that clearly this model performs best or performs better than all the other previous models hello everyone and welcome to this new and exciting session in which we are gonna treat tensorflow record tensorflow records helps us build more efficient data pipelines as they help us store data which we are to use to train our models more efficiently and also the help in parallelizing the reading of the data hence helping in speeding up the overall training process and so in the section together with the tensorflow data sets which we've seen already we are going to see how to implement an efficient training pipeline with tensorflow in the previous sections we've been working with tensorflow data sets in the session would see how to convert our tensorflow data set into a tensorflow record and then get this tensorflow record and convert it back to tensorflow data sets to be used for training now the very first question you access office given that we've already carried out the stringing process successfully without any problems why do we need to work with tensorflow records now there are two major problems tensorflow records come to solve or their two advantages of working with tensorflow records the very first one is a fact that you can now store your data more efficiently now notice that every time you have to create this data you're creating this data from this data set which will load a year which is made of little files of like say a few kilobytes let's open this up so you could see for yourself we have this year let's take this one open this folder and then we will see the size of this so you can see the size is about 17 point 26 kilobytes now the fact that we have to do with deal with this kinds of files means that we are not going to always have to load this data very efficiently now it's true working with a tensorflow data sets brings in some efficiency but what if we start this data in a very efficient manner that is instead of storing let's say for example 17 kilobytes files like this what if we just store say 10 megabyte files or say a hundred megabyte files so every time we want to read from a file we don't have to read from many of this kind of files but from a single file like this one then another good thing with having to work with tensorflow records which is stored in this kind of form is that this time around you could carry out the preprocessing before storing the data so you could augment your data so we have augmented data which is now stored as tensorflow records so instead of having the images and then getting here and each and every time you have to carry out the augmentation or some preprocessing before training you store the data which has been augmented already so suppose you have your initial data so you have your initial data here it passes through some preprocessing so it passes through some preprocessing and then let's change this color so here we have the data and then here we have the preprocessing and then from the preprocessing we have the augmented data so after this the data has been augmented what we do is instead of having to go through this each and every time what we do is we pass through it will go through this once and we store our data in this is augmented form such that the next time we want to train our model all we need to do is just make use of this augmented data right here apart from this it should be noted that sometimes we have models or we have sections of a model let's draw it this way we have let's suppose that this is our model here so this is our model is made of section 1 and section 2 the section 1 is fixed meaning that we are not going to train this part but we're going to train only this part this means that when you pass in data right here when you take this data and you pass in here the output you would have here is going to be the same each and every time given that this year is fixed the weights here are fixed so what we could do is instead of storing the data we are going to store this outputs which we could call embeddings so instead of storing the data we store this embeddings here such that we now make use of the embeddings directly and train our model or train this part of the model which is actually trainable on this embeddings instead of working with a data so this time around we see that we have data the preprocessing the augmentation and then we have the embedded or embeddings from this data so here we have the embeddings so we can now store this as tensorflow records so you see that gives you it gives us some kind of flexibility as to what we can store and be able to retrieve the stored data and make use of it as we wish the second major advantage of working with tensorflow records apart from this efficiency in storing data is the fact that they encourage the parallelizing of reading data now this means that if we're having to train our model on several hosts let's say we have four different hosts or say we have four different machines on which we train our data like this and we have our data set right here what we could do is we could create shards of our tensorflow record data so here for example this let's suppose that this are our data set our complete data set we could break this up into several parts so let's say we break this up such that each host takes care of two so we have one two one two one two one let's add another part here so each of this takes care of this so this this host here trains on this data this sort of one trains on this this sort of one trains on this this one trains on this now as a rule of thumb is generally advisable to make sure that each host has about 10 of this packs of 10 to 100 mega bytes of our data set so that said if we have a 10 gigabyte data set like suppose that all this now is 10 gigabyte let's take this off we have 10 gigabyte data set let's write this in here so we have your 10 gigabyte data set what we want to do is have here nice each and every host taking care of at least 10 packs of our tensor flow of our tensor flow record which we've created so let's take this off it's no longer two we have to create 10 packs or 10 charts of our tensor flow records and allocate that to each and every host we have here now that said given that we have four hosts then we will have 4 times 10 charts to create so we'll break this 10 gigabyte data set up into 40 different parts so we have 40 different part packs or charts of our tensor flow data set all right off our tensor flow record right here and then each and every one of them will be approximately 250 megabytes since 10 gigabytes that's if we convert this to megabytes we would have 10,000 suppose that a thousand megabytes equal a gigabyte so we have your 10,000 megabytes divided by 40 which will give us 250 megabytes per chart or per pack that we've created your split of our tensor flow records and so this will lead to some reasonable gains because we now can train our model on this parallelized data and then we could also prefetch huge chunks of our data set to be precise 250 megabytes such that once the model is ready the model just fits on this data which has already been prefetched now you could check out the previous sessions where we talked about prefetching under tensor flow data sets that said what we actually store in this tensor flow records are this protocol buffers and the way tensor flow manages this is by making use of this tensor flow example class which is defined here let's check this out your tensor flow example which is defined here as a standard proto storing data for training and inference and so if you have to convert your data into this into the proto files you would have to make use of this tensor flow example class and then you would have to understand this representation right here and so in our case where we're dealing with an image and its corresponding level let's say we have the image of this person smiling we have the level one so we have this image and the presence corresponding level would have to convert this data into this format before creating our tensor flow records now here we have this dictionary as I said here it contains a key value store example features where each key which is a string maps to a feature now note that here we have this feature with s and here without s so we have this features and each and every one of them is a feature so we could click here you click here and you see this feature you check out the documentation and we have the this content let's take this off the content list can be one of the three types a byte list generally this is our information flood list or an int64 list so you would pick your feature depending on the kind of data you're having now here you have this features which is like a combination of these different features here so you have here the int list you have the float feature the float list the byte list and then here you create the you create the features from these different features here so from this from each and because this is a feature this one year this one year is a feature let's take this off this one year is a feature this is a feature and this is a feature this has been defined already here because the ints feature is of type feature there is it float feature the same there is it bytes feature the same that is it now all this combined forms features which is of type features see the difference you have this and this we doubt the s obviously is a single on this plural so that said if we want to create our tensorflow records I want to convert our data sense tensorflow records we have to take into consideration this formatting of our data so we get back to the code and then we add this two imports here here we have this tensorflow train we import by list float list and in 64 list so these are the types of our feature and then we have from tensorflow training again we import example we employ features and we import feature so with that let's run this cell get back to our code we have your tensorflow records now what we'll start by doing is unbatching our data so we'll start by unbatching our data note that to run this we've taken we've taken off this prefetching as we will not need this we just take our data as it is so we have our train data set which has been augmented you could carry out any preprocessing you want before storing this data and then the validation data remains validation so we have that validator set there then from here we'll run the cells already from here we'll go ahead and unbatch this data there we go we have training data set we run this we have this year then we do the same for the validation so here we have validation data set and validation data set unpatched there we go we have that you could see for yourself training data set training data set this is gonna be unpatched so you have the unbatch data set notice that the batch dimension is taken off could simply have this for the validation there we go validation you run that see that taken off too and then we get back to this documentation recall that before creating the tensorflow records we need to put our data in the sediment format more specifically we need to create proto files and the way we do this is by making use of tensorflow example and then to create this tensorflow example we have this features which we need to combine to create this tensorflow examples and in this documentation here we have all that is needed to create this so here you see for example this example here we could copy this out simply and then we paste this right here so you could see the int the floats the bytes but since we are not gonna use the floats here we could take this off we're gonna use this float feature what we're interested is the byte feature because recall we have the image and we have the level so we have the image of the person happy and then we have the level one so this level will be this int feature and the image will be the bytes feature so let's go ahead and get this done we have here let's put this first so here we have our bytes and then we have our int now we're gonna create an example from here you can see clearly we've imported this already so we just have that even here could have this features there we go here we have bytes list see here we have feature here we have feature here we have int 64 list feature of type in 64 list and that should be it so we have all this here we need to take this off we don't need that so here we have we change this name we put we call this image we'll call this image our images and then we'll call this levels so that is it okay so we have this year back that's fine we have that the bytes does images and we have the levels now once we have this the next thing to do is to put in the correct values in here so instead of having this year we take this off and then here we're gonna create a method which will call create example call this method create example is going to take our image and also the level so it takes in the image and the level and then what it returns is our serialized example so we've had this example which have created and this are serialized example so we pass we call the serialized to string method right here now that's set instead of this year we'll take in our image and then instead of this year we'll take in our level so that's that should be fine we now run this and the next thing we'll do is define the number of charts here we have 10 charts and then the path so you call this tensor flow records let's create this new folder here TF records that's okay TF records that's fine and then we will have the charts with your specific number so here we have the chart or basically we have this name let's have your file name and then here's the extension TF record okay so that's it and then we can run this and then the next thing we want to do is to get back to documentation TF.io and then we get this TF record writer where we are going to see how to write in a TF record file so we have this year here's a simple definition arguments to specify the path and you also use this example so we have here write the records to a file so let's copy this simply and then we paste this year so we have with this TF record writer will specify the path here our path is gonna be this path we have that path and then as file writer we're gonna write our information in that now we want our file to get those different names so here we have path.format and then we'll specify a given chart so a given part of our data so let's get back here recall that when we create our tensorflow record we could create this as a block like this and then we'll later on chart this break it up into different parts so here exactly we have 10 charts so break this up into this 10 different paths 1 2 3 4 5 6 7 8 9 10 okay so there we go we have this 10 different charts and then we want each chart to have a different file name and that's why if you look at this year you see we have this formatting such that we pass in a given chart number in here so we have the chart number so that's it and what we'll do now is for each chart as out of this 10 charts here for a chart number in range number of charts that's more of shots will specify here on more shots 10 so it's basically for short number in 10 so we are gonna go through we're gonna loop through this and we are gonna create a file for each chart so breaking up here so for each and every one of this we're gonna create a file this one a file this a file and so on and so forth so let's take this off there we go now getting back here once we have had this in place or once we've set this up now recall that we had created this create example matter right here and if you notice you find that in this year we actually all this example it's kind of like doing exact same thing with this create example where we have those different features which are created and then at the end we have this serialized to string meta which is called so you could see here those different features see those features and then they all combine to form the example and then serialization so this means that all we need to do here is pass in we have here instead of this we have create example and then we'll be passing in the image and the level now we'll take this off yeah we don't need this physically we don't need that and then we will go for image and level in our data set in our tensorflow data set let's get back up the what we're doing with now is our training data set so for this in our training data set that's it we're gonna write this so we're gonna have to write this image and this level in our file will be created so in this our tensorflow record file now given that we are we have to write a given shard and not just a full data set here would change this to shard sharded data set and then let's have this year we have our sharded data set which is going to be equal you have that our training data set and then we'll shard so you could get to the definition or you could get to the tensorflow data let's scroll this tensorflow data data set here you could have this or you could find the shard method right here let's scroll down and click here see we have let's click on shard shard okay so you see here we have the definition takes the number of shards and then it takes the specific index so here we have 10 shards and then for each index we're going to pass in the value here dynamically so here you see that this creates different packs or parts of our data set so here we have each pack which is going to be created so we specify the number of shards there we go and then we specify the shard number so that's all we need to create a part of our data set so that's it once we have this let's now run this and see what we get get in this arrow for sharded number run that again what do we get this but expected one of time bytes so what we pass in years tensor but we expected a byte now the next question is how do we convert this tensors to bytes so we do a quick go search here convert image to bytes in tensorflow there we go we have decode image but in fact what we'll be using to convert this image into bytes is this encode gpeg right here so we haven't gpeg images so we could use this here using it is quite simple we just simply pass in the image and then we'll consider all this to be default values so here we have encode gpeg and then we're gonna create this encoder method here encode image takes in the image and the level and then what we have here is that we're gonna start by having getting the image we'll say image is gonna be the encoded version so we have tf.io encode gpeg that's it we pass in the image and then we're gonna return the image and the level so once we have this let's add this code so let's run this and then we're gonna create a new encoded data set encoded data set equal this we have training data set there we go we're gonna map encode image that's it so that each and every time we want to pass this into this file writer we're gonna make sure that we have this in the form of bytes so that matches up with this here with this feature which is defined in our create example method right here so we need to match this up so that said let's get back we have encode encoded data set take this off we run we have encoded data set we told that this image has type float 32 that doesn't match expected type of unsigned int 8 so what we need to do here is we need to convert our image first into this unsigned it so we have here now to get a solution we'll get into tf image convert image dtype so that we could convert let's actually convert let's get to see convert image dtype so as we're saying we could actually convert our float 32 to the unsigned int so here all we need to do as you could see here you see you you have the tensor and then you have the dtype specified so that said let's copy this and we have that anyway we are we're going to take the default value for the saturation so let's get back here and then at a level of this or before the encoding we're going to convert that so here we have image image equal that and then we pass in the image here so we have tf dot unsigned int 8 int 8 and close that so that's it then set we can now let's run this and run this again so we could see our encoded data set see that works fine now so we have our encoded data set and then in here instead of having this we will call this encoded data set so we have encoded data set and then we'll run this and see what we get value must be iterable so we must convert this into an iterable data set so to do this we just have your as nonpy iterator we have as nonpy iterator we run that again we are getting here 255 has type int but expected one of bytes this error is coming from our create example method so let's get back to create example everything looks fine but here we should have a list so let's have this year there we go and run this again and that's it the creation of the different files now complete you could click open year and you see we have all this 10 different charts at this point what we could do is we could save this files in the drive such that we could use it next time for training so let's go ahead and see how we could make use of this for our training process now what we want to do here is to convert this back to a tensorflow data set so we had our tensorflow data set we converted it into a tensorflow record like we see here we converted this into some different tensorflow record files and now want to reconvert this into our tensorflow data set now to get this reconstructed data set we are going to pass the different file names in this TF record data set right here so what we have here is this list made of all these different file names right here so this list in essence let's call it L will be our path which will format and then we'll pass in this variable P for P in range the number of charts for P in range number of charts let's print out this L run that and we get this list which is made of all the different files here so that said what we're gonna have here is we're gonna copy this we're gonna copy this here and then simply replace this list with it so we have this list now let's have this we have this list now and then we run this and get our reconstructed data set but then we need to parse this TF record data set such that we could get our original data which was in the form of the image and the level where the image was an array and the level was some integer and so with that we have here parse single example method which we're gonna make use of which takes an example which is basically what is contained in our reconstructed data set takes the example and permits us to split this into the image and the level so right here we'll have example and then we'll get the image we have the image then we'll make use of the decode gpeg method so previously we encoded the into gpeg or we convert it into bytes now we're gonna convert from bytes back to the unsigned integer so here we have decode gpeg and then we're gonna pass in the example image so we have that image we specify number of channels to be three now we have all the set what we're left to do is specify this feature description right here nonetheless this feature description is basically what we had in this create example and so here we have this dictionary let's have this images levels your images we have this dictionary which is made of the images and the levels and then we have the data types of the images and the levels respectively so we need to pass in this feature description in this parts single example method now we have that always set to return our output so we take example and we have images and then example and we have levels so what we're doing here is we take an input example and then we're breaking it up into the images and to the levels while converting these images from the bytes back to the unsigned int so we have that now let's run the cell and then here we have our parsed our parsed data set there we go we have parsed data set and we have recounts data set there we go we map this method here so we have parsed TF records parse TF records so we have this method let's run this that's fine and then let's see what is contained in our parsed data set so let's take a single value and then print this out from that we get in this arrow for I in this let's run that and there we go as you can see we have our input and then our output level right here now the next thing we could do is specify the batch size so let's have this batch size here we have our batch configuration and then batch size that's fine and then we could do some prefetching so let's carry out the prefetching let's just do auto tune auto tune and then run this again there we go now we should have 32 elements let's look at our batch our parsed data set looks fine you see we have this four different dimensions and then we have output here so that's it we now have our parsed data set are we ready to train so as we've seen already it's important for you to save this in some location say for example in the drive such that the next time when you want to do training all you need to do is start from here so all you need to do is to reconstruct this so you just come and reconstruct this data set you will parse it and then you're good to go with this parsed data set right here so you don't longer have to load all these images from memory and all of that now before we move on you should also note that while encoding the image or while encoding the data that the image and the level what we could also do is take the argmax of the level so if you have a level so let's suppose we have an input image like this and then you have an output level say 0 1 0 instead of taking the considering the levels to be this we could take the position with the highest value so this is 0 1 2 so it tends to 1 now if we have 0 0 1 then after getting the argmax in this case it's going to be 2 because here we have 0 position or first position second position and this is the one with the highest value so this is another way of encoding our levels and that's what we are going to do so we will take this and we run again and create our data set or create our tensorflow records and we are still going to run this cells again so let's rerun this and we have our parse data which you could see here see now it's different from what we had before so now we have the image we have the images and then we have the levels then we could go ahead and run the model we have the loss function defined now the loss function is a sparse categorical cross entropy and this is simply because instead of the one hot notation that is instead of representing our outputs like this for example we convert them into this single integer so like this one for example would be 2 if we had 1 0 0 then this will be 0 if we have 0 1 0 then this will be 1 so we've seen already that when we have this this kinds of outputs then we use this parse categorical cross entropy and so that's it we have the sparse accuracy categorical accuracy and now let's go ahead and start with the training so there we go you can see that training has begun and we training as we usually do so that's it for this section on tensorflow records see you in the next section hello everyone and welcome to this new and exciting session in which we are going to look at other stateoftheart convolutional neural network based model and talking about stateoftheart models ten years ago in the image net visual recognition challenged the Alex net convolutional neural network beat all stateoftheart solutions previously or before this Alex net solution stateoftheart methods achieved a top error rate of our top 5% error rate of 30 25.3% but with Alex net we drop this error rate to 15.3% now this is 25.2 this breakthrough has led to a widespread adoption of convolutional neural networks in solving recognition tasks like this one and although today we would hardly use this kind of model that's to say the Alex net model we are gonna discuss this model because it was a precursor to most of the modern confidence we have today like the mobile nets dense nets and efficient nets that said we are gonna see what makes or what made the Alex net model so powerful the Alex net model was first published in this paper entitled image net classification with deep convolutional neural networks just from the title you could get some idea that this was one of the first times conv nets were used for this image net challenge this paper was by Alex Krasinski, Ilya Sutsicker and Jeffrey Hinson in the abstract the start by presenting the results as you could see here on test data they achieved top one and top five error rates of 37.5% and 17% which is considerably better than the previous stateoftheart now this model was a 16 million parameter model composed of conf layers and max pulling layers with some final three fully connected layers as we shall see in the model section shortly now here 1000 ways softmax because we have 1000 classes they also try to reduce overfitting by implementing strategies like dropout and data augmentation that said here to discuss the data set which was used obviously the image net which is a 15 million labeled highresolution image data set although the finally used roughly 1.2 million training images 50,000 validation and 150,000 for testing then it's also important to note that the images which were used for training were down sampled to 256 by 256 images as for the overall architecture as we could see here we have the conf layers followed by max pulling layers sometimes you see your conf layer max pulling conf layer max pulling then we have several conf layers this max pulling layer and then we have three dense layers you see here we have this dense layer this dense layer and this final dense layer with 1000 way output then another point to note here is given that at a time many times the nonlinearity used was the tangent or the sigmoid let's get back to this top year you see here they talk about the nonlinearity which was used just here the previously were the mostly use a tangent or the sigmoid as the same year but it turns out that after working with a relu the relu if you can recall we've seen this in the previous section the relu is simply this function which takes in a value x and then if x is negative the value is 0 if x is positive remains the same value so basically we have x f of x f of x here which is our relu function which is 0 if x is less than 0 and it is x if x is greater than or equal to 0 so this is our relu function right here and what the discovered was that the relu permitted them to train your model much faster than this previously used nonlinearities like the tangent of x so as you could see here just after a few epochs just after let's say five epochs they attained this training error rate as compared to this other nonlinearity here so this is what we get when we use the relu and this is what we get when we use some other nonlinearity like the tangent x and it's important to note that till date most conf nets we build make use of this relu nonlinearity another thing the deed to speed up the training was to use multiple GPUs then the actual number of GPUs they use here is two and the device a method of communication between these two GPUs to speed up calculation from here the authors make use of this normalization strategy for regularization known as a local response normalization and this normalization strategy was used alongside the relu nonlinearity so from here we have some input let's take this schematic from this post by Akhil Anwar where he shows this even more clearly here we have some input and then we normalize it based on its surroundings hence the term local response normalization here he explains that there is inter channel local response normalization and there's intra channel local response normalization here as you can see this is between pixels of a given channel or neurons of a given channel and here this is inter channel so this is carry out between pixels of different channels now that said the exact mathematical formula use here is this one so here we have given neuron and then we divide its value by this summation right here which is a square of some neighboring values and according to the authors terms here you see this sort of response normalization implements a form of lateral inhibition so take note of this and is inspired by the type found in real neurons create and competition for big activities amongst neuron outputs computed using different kernels so this means that if we consider this three neighboring channels from or rather the three neighboring neurons from these three different channels if we take this particular neuron right here and we try to normalize it or pass it through a normalization layer given that it is surrounded by this pixel whose value is relatively high because of this square term right here and this is a square right here meaning that you take this value say 1 divided some by some summation and then you have this alpha obviously you have this K right here let's omit that let's just put it right here we have this alpha and then we have this value squared so obviously this would be a function or basically this will be this value here so when you square this value it means that this overall value here will become very small hence the term lateral inhibition and so for a neuron to maintain a relatively high value after going through this local response normalization layer right here it has to ensure that it has one of the highest values among the surrounding neurons nonetheless this local response normalization as compared to other normalization techniques like the batch normalization layer normalization and the group normalization hasn't proven to be very effective when it comes to regularizing a neural network hence not used by modern conf nets we had seen in the previous sessions that the pulling layers permeate us down sample information from the inputs such that as we go deeper in the neural network we have a reduced number of features now on this paper they make use of this pulling layer more specifically the max pull layer and the way it works is quite straightforward so we're supposing we have this kind of input and then we have a 3 by 3 max pull layer with initially a stride of 1 what we're gonna have here is we have this positions which we are gonna fix here let's get back and then fix some values so let's say we have a value of 1 2 and then all this other values if we want to carry out the max pull operation with a stride of 1 we'll start with this year see and because this max pull we have we're gonna pick the max of all this so we have the highest value yours 11 and so here we're gonna have 11 and then the next thing we'll do is we are gonna shift this so we shift this here this is a stride of one we're gonna go one step to the right and so we have this now it's here notice how we still pick out 3 by 3 pixels let's not take this one off so we've done this shift and now we have this position we take the max here the max here again is going to give us 11 so we have 11 year and then we're gonna do another shift so from here we're gonna take this year let's take this off this other shift and then we still have this the max here again is gonna be 11 so you see at this top we have 11 11 11 and then from here we'll move on to this next one so we will go downward one step downward we'll have this here and the max here is gonna be here's 11 still so we're gonna have 11 year and then we'll move this way this way we'll go downward and all of that so we will move this way I think we should have 11 still the way 11 we go downward still have 11 practically we will have 11 everywhere so we'll have 11 and your 11 so this is going to be our output from this input here after the max pull operation now when we modify this stride number from one to two take this stride number from one to two as it was illustrated in the paper instead of having or instead of moving through one step we move through two steps so if we take this off here you would see that we'll start with this so from the first one we're gonna get 11 so this was tried equal to so the first one we get 11 and then from here instead of moving just one step like previously we had this year we have this year and then we move one step now we're gonna move two steps so we move this way and then again we have 11 and then instead of going one step downward we're gonna go two steps downward and so we'll end up here and then we have a maximum of 11 and then we'll go two steps again this way so let's take this one off take this one off we go two steps again and then we get a maximum of 11 right here so as you could see here when you talk of overlap and pulling they actually use a stride of two just as we have described and then the founders to give them to give an improvement in the results though this improvements aren't very much so in practice we generally use the classical max pulling with S equal one does stride number equal one and we also use two by two kennel size so instead of using three by three kennel sizes as you see here most times or in modern conf nets generally use two by two pulling size now getting back to the general architecture we could see here that this very first conf net has a kennel size of 11 by 11 and although these kinds of kennels permit the network capture much larger spatial context we'll see that they are computationally much more expensive compared to this kennels with smaller filter size and as we'll see in subsequent sections the confidence developed after this didn't use these kinds of large kennel sizes as they were able to make use of these kinds of smaller filters to still capture this large spatial context the 11 by 11 filters capture then to overcome overfitting the others make use of data augmentation and the dropout technique so you could check out on our previous sessions where we talked about this two different techniques now that said you would see here the training details and then one very interesting advantage of working with the 11 by 11 kennel size filters is the fact that we could have visualizations like this so because those kennel sizes are large enough we could visualize them in this manner and then clearly from here we see how our conf layer captures these kinds of low level features like here we have a slanted line here we have yeah many slanted lines we have this vertical line we have this horizontal lines right here and then we have this this checkerboard pattern we have this colors sometimes dwell sometimes single color and so we'll see how this first count layers permit us capture low level features in this session they discuss this record breaking results as you could see we have this top one error rate which for now or at that time was about 45.7 percent and then with this curve net model this was dropped to 37.5 percent then for the top five we have dropped from 25.7 to 17 percent now they also developed this other variant which comes with an even better top five error rate of 15.3 percent and so as you could see here we move from this previous method does a shift plus FVs which had 26.2 top five percent error rate to the CNN which has 15.3 percent error rate then in the session on qualitative results we see the different inputs the correct levels and then what the model predicts on the top five best predictions see the model does well here does well here correct correct prediction yours wrong see it predicts convertible when it's actually a grill here it does this wrongly here it's also wrong but unlike here this level doesn't even occur among the top five best predictions and so that's it for this breakthrough model we're gonna look at other confidence models in the next sections hello everyone and welcome to this new and exciting session in which we are going to discuss the VGG model VGG actually stands for visual geometry group and this was presented in the paper by Karen Simone and Andrew Zimmerman entitled very deep convolutional networks for largescale image recognition in the session we are going to discuss different methods which the authors of the VGG paper used to drop the top one validation error rate from 38.1 to 23.7 where this 38.1 was achieved by the breakthrough conf net model which is the Alex net model now in the previous session in which we treated the Alex net model and we saw the power of working with conf nets and solving recognition tasks one thing we could notice already or very clearly from this model is that it's quite shallow and so Simone and Andrew Zimmerman go even deeper with the VGG model in this paper the authors investigate the effect of the conf net of the convolutional network depth on its accuracy notice words depth and accuracy so unlike the Alex net where the depth is relatively small and is actually a shallow network here the authors use a deeper conv convolutional neural network and make use of smaller convolution filters now recall with Alex net from the very first layer we already had 11 by 11 filters and we argue that this helped in capturing large spatial dependencies now we'll explain how is possible for us to make use of the smaller and more economical convolutional filters while still capturing large spatial dependencies like the bigger five by five and eleven by eleven filters will do to better understand why it's better to work with two three by three comp layers as compared to working with a single five by five comp layer let's consider the following examples so here we have this three examples let's start with this first one here so let's have this we start with this part here and this one if you notice is five by five so here you have kennel size of five input size of ten and no padding dilation of one and stride of one so here we have this year and you can see we have this output which is six by six so you have the six by six output and the way each and every pixel on the output is garden is quite simple you have the kennel right here let's have this kennel we have this kennel which has been passed on a particular patch on the input and then this produces the output so simply take this kennel values multiplied by these values add them up and then get this output now this is the case of five by five so the receptive field Spaniard is quite great see we could get we could capture this information in this patch in the input right here now for the number of parameters you could simply count this we have five times five which is 25 parameters here we have 25 parameters and then for the learning capabilities this one is quite limited here here we have 25 and here we'll say this is quite limited because if we suppose that we have an input let's say we have this input which is passed through a single conf layer and then obviously the conf layer ends with a nonlinearity in our case the relu nonlinearity so let's say we have the nonlinearity line right here and then we get the output now this doesn't capture as much complex information as we would be able to capture if we had two conf layers stacked now here when you stack this first one this is a case of three by three and this is a case of five by five so here after this first three by three we then have this other three by three with this other nonlinearity so here we are able to capture much more complex information from our input information of my input data as compared to when we just have one single conf layer so that's why here let's take this off here we the learning capabilities isn't as much as this two three by three now let's get to the receptive field span for the two three by three to understand this we are going to take this example right here so take note that here we have input size of 10 so we're gonna maintain this we have input size 10 okay now the candle size is three unlike here where we have five padding dilation and stride number the same now notice that the output we have here is eight by eight unlike here where six by six now since we are having two three by three conf layers we're gonna get this output just like the output we get here is this so let's draw let's put this here we have this input the here is it and then we get this output which is gonna be input of another three by three so this is the first three by three and then this is the next three by three so this will be the input of another three by three comp layer and then we should produce the output so what we're trying to show here is when we have this output here which is the input of this next layer so this is this two here are combined as one this forms one so this year this eight by eight now is an input so here instead of having ten we have eight so we take this input size of eight there we go same candle size patterns dilation stride the same and what we'll want you to notice let's take this off what one you notice here is the fact that this output is six by six and so this means that this year as if we follow this let's take the mouse if we follow you see this year captures the same information as you would as this one year would capture and so we could confidently see that the recessive field spannier is quite great now for the number of parameters here we have nine and here we have nine nine plus nine is 18 so 18 you see clearly that this model now is cheaper as compared to a model which uses a five by five or even eleven by eleven now for the learning capabilities we've seen this already because we stack two we able to capture much more complex information from the inputs and so this is great so in all we see that is better to use this conf layers with smaller kennel sizes we want to thank Edward young for providing this convolution visualizer which you can find on as young that github.io so at this point we've understood why the authors of the paper prefer to work with this smaller kennel size convolutional layers that's three by three and with a smaller conf layers they were able to push the depth to 16 that's between 16 and 19 layers where the 16 layer version is VGG 16 and then the 19 layer version is VGG 19 in this table we have the summary of this models focus on the 16 and 19 weights layer models so yet we have 16 weight layers you see we start with two conf layers and then max pull and then to conf layers and the max pull and in three couple layers max pull three comf layers max pull and then three conf layers then from Europe we have the max pull we have a flattened layer and then we have this three fully connected layers which end up with a softmax since we are dealing with a multiclass classification problem. The authors also noted that the usage of the RNN normalization as a local response normalization which we saw in the AlexNet did not improve performance but instead led to increased memory consumption and computation time. In this section to describe the training process then from your testing and we could get some results. Here we see we have this top one validation error and we also notice that we have this ConvNet layer or this ConvNet model here A. Now the ABCDE you could get them by checking on this table here you have A with the RNN normalization B, C, D and E right here. So basically these are the different models and your other results. So we are working with the local response normalization we notice that this seems to have even the highest error. So that's why the authors did not make use of this normalization technique. Now we get the best results with the VGG19 so this year VGG19 we get 25.5 for top one and then 8 for top five. Now if you are new to this notion of top one valid error and top five valid error you could check out the previous section where we discussed the top one accuracy and top five accuracy. Here again now we have this comparison with the state of the art solutions at the time you see here AlexNet, Overfit, Inception, MSRA, Clarify, this model by Clarify AI and the Zeller and Fegris model. So here we see that the VGG at the time had the best results and with this we can conclude that stacking up those Conv layers with smaller kennel size actually helps in getting better results and in subsequent sections we'll see the limit of just stacking up many Conv layers as with the VGG. Hello everyone and welcome to this new and exciting session in which we are going to discuss the ResNet model. This model was first introduced in this paper entitled Deep Residual Learning for Image Recognition by Kaimin Hayao and up to date that's about seven years later this model is still greatly used and the high performances gotten when working with the ResNet model come due to the fact that the ResNet model relies on this residual block right here which permits us get even better error rates as we could see here as compared to the VGG and Google Net models. In this section we are going to focus on understanding how this residual block works and how the ResNet model is constructed based off this model. This curve right here depicts the limit of models like the VGG which are just based on stacking up Conv layers. To best understand this plot, recall that with the AlexNet we had fewer number of layers so we started out with AlexNet fewer number of layers and then we moved on to VGG where we had the VGG 16 version then the VGG 19 version and then we expect that if we keep increasing this number of layers then the error rates should be dropping but what happens actually is the opposite. So what goes on here is you see for this 20 layer network we have a lower error rate as compared to this 56 layer network. So we expect that this instead should be lower than this because this we've stacked more Conv layers but that's instead the opposite. Now this same phenomenon is witnessed with a test set. So here there's a test and there's a train. So for the test tool we have the 20 layer performing better than the 56 layer. And so it's clear that just blindly stacking up Conv layers wouldn't help in making this or in dropping the training on test errors even though they are more expensive. And this is why the ResNet model introduces this residual learning which is based off the residual block which we've seen already here is the residual block. Now note that this weight layer here or this weight layers here are simply convolutional layers. And so now unlike before where we will just stack this here let's call this WL for weight layer stack this weight layer and then we'll stack this other one and then we'll keep stacking up that way just keep stacking up like this. Now what we'll do is we create a connection between the input and the output. So we create this connection and then here there's some addition. So we get this output and we add it with this other output right here to produce now this new output. So here if we suppose that this input is x and then what goes on in here is f of x. So let's change this to red what goes on in here is f of x that's this f of x here we have this f of x then our output can be given as h of x which is simply equal f of x plus x that's the input plus its output. Now to produce this new output h of x. But again to better understand why we need this residual block right here. We need to first understand why models which are based on just stacking up the comp layers like the VGG actually under fit even when you add up or when you increase the number of layers. Now the reason for that is exploding and varnishing gradients. So let's explain what it means for gradients to be vanishing. Now recall that in the gradient descent process we have a weight and this weight the way this weight is updated is such that we take its previous value. So we have the previous value of the weight minus the learning rates here let's call it our LR really denoted alpha minus this learning rate times the partial derivative of the loss respect to that given weight. Now during the training process in order to compute this partial derivative right here very efficiently the method used is back propagation or one of the most common methods used is that of back propagation. Now the way back propagation works is that you have say this model let's call this model M and then you have some input right here with the output obviously you have what the model outputs and let's let's call the model output y cap and then y that's not from model we have y right here which is what the model is expected to output. Now is this difference that produces the loss and well finding that partial derivative of this difference with respect to each and every weight which makes up this model. Then if we split this up into different layers let's just say we have different layers like this note that each layer is composed of several weights but let's let's say we just split this up like this now the layers this layer has its own weights but one point to note is that during the back propagation process to obtain the partial derivative of the loss we respect to this weight here we make use of the partial derivative of the loss with respect to weights which come after the layer or which come after this layer right here. So we to get this year we will need this different partial derivatives to get let's let's get back let's get back to get this for example to get the partial derivative with respect to this weights here we will need this others right here now the problem is if we have this year and and also before before going to explain the problem with this is that we need to understand that to get for example let's take this one to get for example the partial derivative of the loss we respect to this different weights there are many weights in this layer let's say weight for this layer is actually equal some values let's say alpha 1 times whatever value times this partial derivative of the loss we respect to this weights here so let's consider that this is the sit layer 1 2 3 4 5 6 then L6 here we have L7 layer 7 so because we're multiplying here it means that if while getting this partial derivative we obtain a value very close to 0 if we get a value very close to 0 say 0.00001 for example it means that it's going to affect this other partial derivative in the sense that this two will be a very small value and if this partial derivatives are too small then we will not get a change in this weight because you have the the new way you're trying to get being equal the previous weight minus a very small value here and so there will be no there will be little or no changes in the weights and that is why even though you keep increasing this number of layers let's take this off even though we keep increasing this number of layers we cannot achieve better performances due to this vanishing gradient problem as the model is now finally difficult to update its weights such that the training error can be decreased since the gradients right here are vanishing that is getting towards 0 so now we've just seen that making our network deeper or increasing the number of layers makes it difficult to propagate information from one far end to the other end and so what the authors suppose is that if the added layers can be constructed as identity mappings a deeper model should have training error no greater than its shallower counterpart so this means that you have this model let's suppose this model and then we this is the shallower model and then this is the deeper model right here and we're saying that if we construct this deeper model such that it's identical to the shallower model so basically it's the same as this here the changes in blue so we have this same shallower model and then the remaining layers here this other layers here are constructed such that this is the identity function or a group of identity functions which are stacked together then the training error of this one although deeper shouldn't be greater than this training error right here or the training error of this shallower model and so this means that if we want to pass information from this point to this point right here and that this weights or the values in your dampen this information then there is this path right here which permits us simply copy this input information to the output and obviously this is simply the identity function and so here if after passing through say 20 layers and we get to this point so let's consider each and every one of this is a single layer so we've gone through 20 layers and then we've gotten to this point where the values we get here are almost zero such that when this information passes is going to be also or practically zero then there is this path which at least restores this exact same input we have here and so this means that just as the authors of the papers supposed in this example we took here if we make our model or neural network deeper by adding this residual blocks then there will be no increase in the error rate and in practice this instead leads to a decrease in the error rate which is exactly what we want and one other argument which accounts for the fact that this residual blocks help in improving the performance of the model is the fact that since we have several paths this residual model now looks like a combination of several shallow model so it looks like you're combining different let's let's draw it better let's get back it looks like we're combining this shallow model here with this other shallow model with this other shallow model and then producing what we call an ensemble of models or an ensemble of shallow models to be more precise which help in making the overall model much more performant as compared to when we just have a single path. So another way you could look at this is that for this let's take this first shallow model you could have information which passes this way and then gets here and then goes this way so that's the first model and then another time you could have information which goes this way goes this way goes this way and you have this other model you could have an illustration where the model goes this way it goes this way and then goes just straight and giving us this other model later in this paper entitled visualizing the lost landscape of neural nets Lee et al produced this visualization here which shows a resnet without skip connections and a resnet with skip connections and this shows how easy this or how easier it is for the weights to find their way or to get the optimal weights which minimize the loss as compared to when there are no skip connections here it should also be noted that this addition we have here is an element wise addition and so we have to ensure that the dimensions of this input should match the dimensions of the output we have here for this operation to be valid now in the cases where on the case where this two aren't equal then we need to do some modifications in this skip connection right here now when we look at this three models compared we have this VGG 19 we have this 34 layer plane convolutional network then we have this 34 layer residual network we find that we have this skip connections that's our residual block so this is our residual block right here we stack this residual blocks now instead of just stacking the conf layers as we do with the VGGs now we stack the residual blocks and then sometimes you could see here sometimes this line is solid sometimes it is dotted now when is dotted like this it's simply because there's going to be a change in the dimension so if you notice here you find that every time we have this dotted lines you'll find that there's a change here the number of channels so here you have 64 channels and then here you have 128 and so since we're getting this 64 channel input and we want to match this with 128 output we're getting here then we need to do some adjustments here now as we've seen here or as we can see in the paper here there are actually two ways of making these adjustments the first way is this A the next one is this B for the A the shortcuts still perform identity mapping with extra zero entries padded for increasing the dimensions so to get from 64 to 128 we add this extra zero entries then either we do that or we take the B that's the projection shortcut which was presented in equation two is used to match the dimensions but this is actually done using one by one convolutions now to better understand how the one by one convolutions work let's take this example where we have this input size 10 by 10 cannot size now since it's one by one then the cannot size is one so this is we have just this one weight right here and then you see just goes through each and every pixel value so here we have this and then notice that the input is the same shape as the output and so if for example you have this input made of two channels let's add a second channel let's suppose this input has two channels so we have this one channel and this other channel right here then to obtain an output of say four channels so we have this input two channels so it's ten by ten by two two channels and if we want to have this output to get to four channels all we need to do now is just make use of the one by one convolution and then we add the we have four of this different weights or this fall for the different kernels since obviously one by one if we're three by three then we'll have something like this would have three of the four of this now is one by one we have is here four of this and then in that case this one will give this output then this other one will give this other channel here this other one let's change the color this other one will give another channel which we'll just add here then this other one here stick that to be green will produce this other channel so that's how we can move from these two channels to four channels and so in the case of the resnets where we want the inputs we get some inputs let's say 64 channel input and we want that at the end we want that this input what we have here should match with this output which is already 128 oh yeah 128 year then we need or we could add this one by one convolutions with a certain number of filters such that we could get this desired number of channels here so here now we could have 128 one by one filters and then this now will match up with this output right here such that we could carry out the element wise addition and now the difference with the vgg and other previous conf nets is that instead of making use of the max pool as you will have here you see this pooling layer with pool size two what we do here is we use a three by three convolutional layer but we use strights so we specify the stride number of two and this permits us to down sample this feature maps right here again we could check this out here let's suppose we have this three and if we're to do or if we want to down sample this what we could do is increase number of strides let's take that to two and you see this is going to be down sampled and if we take the padding to one you find that the output is half of what we have here then the authors also make use of augmentation and then batch normalization where they apply this batch normalization layer right after each convolution and before the relu activation now batch normalization is this technique for accelerating deep neural network training by reducing internal covariate shift to better understand batch normalization we'll start by explaining the notion of covariate shift to better understand the notion of covariate shift let's suppose that we're trying to build a model which classifies or which says whether an input image like this one or say this one is not of a car so it's not a car or it is not a car now if you're building this kind of system and then you start by or you create batches of these kinds of toy cars and you pass through the model and model learns how to see this and know that it's a car and see some other image and knows that that's not a car then later on when you take a car from this other distribution and you pass into our system it becomes difficult for the weights of this model to adapt to this change in distribution though the inputs are all cars and to visualize this let's consider this plot right here what we're gonna have is uh someone like this so we'll have this uh year for car and then this for not car and then we'll build this classifier or this model which distinguishes a car and an image which is not a car by say this function for example now when you bring some other distribution like say this distribution you would have uh something like this you see you have something like this here it's other um distribution and not car let's say not cars about this and then you need to have something like this to separate the cars from images which are not cars and this then makes it difficult for us to have a function which separates uh images which are cars from those which are not cars when those images come from this two different distribution now this shift here is known as the covariate shift and so that's why most times before passing the image into the model what we do is we normalize this input so let's suppose we have an input x we generally carry out some normalization in order to account for this covariate shift so now after normalization what we're gonna have is that all those images be from this distribution or this other distribution right here will now have been normalized to reduce the effect of the shift and so now we could have our single or could have this uh uh function which separates the cars from the non cars and with much more ease now that said what if this kind of covariate shift instead happens in the hidden layers that's those layers which make up the model right here so let's suppose that we have some confidence like this stacked with the activation functions and then we have this weights that's these parameters which make up or which are part of the layer now coming from different distributions then in this case we have an internal covariate shift and to remedy this situation we now make use of the batch normalization and the algorithm for the batch normalization is described in the paper so here we have a mini batch and we obtain its mean so here we try to obtain the average value of the different weights then we also obtain the standard deviation which is sigma and the variance which is sigma square so basically we obtain the mean and we obtain the variance and it's this that we make use now to normalize our data so now you take every weight you subtract by the mean that's here which is calculated here and then you divide by this standard deviation and then we add this small epsilon to avoid having a very small number or zero at the denominator so that's it this is how the normalization or the batch normalization process goes on and it should be noted that there are other normalization techniques like the layer and group normalization which are kind of similar to this but different in a sense that with a batch normalization this mean is calculated over a given mini batch so like here this is the mean of values calculated in the mini batch and the standard deviation from that mini batch now after getting this new value of x x shuffle what we now do is we multiply it by gamma and add better now this gamma and better are actually trainable parameters so when working with batch normalization in say TensorFlow or PyTorch you'll notice that the batch norm layer will also have its parameters now here the role of this gamma and this better is to scale and shift and these parameters are learned along with the original model parameters and restore the representation power of the network and so when we set gamma to be the square root of the variance and beater to be the to be the expectance or the mean of x then we could recover the original activations if that were the optimal thing to do so essentially what they're saying here is if it's instead optimal for us not to use the batch normalization then we could adapt the value of gamma and better such that we get this original value of extra here and the way this can be done is quite simple all we need to do here is multiply this x by let's say gamma square plus epsilon so we have gamma square plus epsilon and then once we've multiplied this you see once you take this here I multiply by this you set this to like this do this this cancels out with this and you're left with xi minus the mean now when you're left with xi minus the mean if beater is equal to mean then you will have xi minus the mean plus beater which in this case is the mean and you see it cancels out and gives you xi which is this original value of x so if we set our beater to be this our gamma to be this and our beater to be the mean then in that case we retrieve our original value of x and so that's why you see these two parameters are trainable such that we get the best values or the most optimal values for gamma and beater then there's also this initialization that's the model or the network is trained from scratch stochastic gradient descent is used with a mini batch size of 256 linearity starts from 0.1 and is divided by 10 when the error plateaus so basically uh when we get to the points where let's have this when we get to a point where the arrow starts to arrow starts to plateau then at that point we could update the learning rate from 0.1 to 0.01 and then if it plateaus it if it drops you see any plateaus let's go this way it drops and plateaus again then we carry out this same computation now that said is for over 60 times 10 to the 4 iterations weight decay and momentum are used and there is no use of dropout so again in testing year we have different scales which are used and average so we pass the image at these different scales and then the average value or the average scores are recorded now before we move forward to check out some results it's important to note here that after this last comp layer we do not carry out flattening instead what is done here is average pooling in order to better understand how the global average pooling works let's consider this example from peltarion.com so right here we're supposing that we have this as the output of the final comp layer then what we do is instead of just flattening that's just picking all those values and flattening them out and then passing to a fully connected layer what we are going to do is depending on the number of neurons we want the next fully connected layer we are going to create a certain number of channels so here for example we have this depth here the number of channels here is three we have the height and we have the width and then since we're dealing with we're actually doing global average pooling then for each and every one of these channels for this channel this channel and this channel we're going to get the average value so you have the average value is this eight here the average value is this three here the average value is this five and so if you want a thousand of this then you should have a depth of a thousand right here and now with this you see that it looks quite similar to the flattened layer as now you have all these different values or these different single values which cannot be passed into a fully connected layer here now it should be noted that in certain tasks like in classification which we are trying to do this global average pooling will be great as the position of the pixels don't really matter now this simply means that here since you get this average and here you get this average you get this average it means that this position or this pixel wouldn't be close to this other pixel as in the case of the flattening but since like in our case we're interested in saying whether a person is angry happy or sad the positions of this output values right here won't really matter as much as would would have mattered in the case where we're dealing with an object detection or say object counting problem where the particular position of the person or of whatever we are trying to detect actually counts so to better explain this again let's consider these two examples example one example two right here for classification problems all we're interested in detecting that this person is happy so whether we have a face this way or this way the position doesn't really matter all we're interested in is in knowing whether this person is happy angry or sad now for object detection the exact position of the person matters and so the exact position of this neurons here will matter and so employing global average pooling for such tasks isn't a great idea so in summary if you have a task where the position doesn't matter that much then you could use a global average pooling if not then your advice to use the flattened layer from here you could see the different variants of the resnet you see here we have the 18 layer resnet as a resnet 18 resnet 34 resnet 50 resnet 101 resnet 152 and here we could see that with the plain networks that's without the res residual block you find this 18 layer performing better than the 34 layer but once we have the resnet block you find the 34 layer performing now better than the 18 layer meaning that we could now go deeper we also have this table right here which shows the vgg model the google the net the prlu unit and then the different plain networks and the residual networks it shows clearly here that the resnet 150 performs best regardless of the fact that it is deeper than the 101 and 50 counterparts and before we move on also note that here this resnet block as you can see here is composed of these two conf layers for the 34 layer while from this 50 to 150 layers the resnet blocks are composed of three layers or three conf layers as you could see right here hello everyone and welcome to the session in which we are going to implement the code for resnet 34 in tensorflow 2 now here is a resnet 34 model right here we have other variants like the 50 101 and 152 after going through the section you'll be able to implement this other variants right here and also you'll be able to get results like this where we can see clearly an improvement in the accuracy of our model we are going to construct our resnet 34 model while making use of model subclassing so you could check out in the previous section so you better understand how model subclassing is implemented in tensorflow 2 now that said here we have this resnet 34 model right here and then the first layer we have is our convolutional layer which has seven by seven filters and there are 64 in number also we know that the stride here number of strides equal two so you could get all this from the paper we have information from the paper seven by seven 64 stride two and then followed by a three by three max pool with a stride of two so we get back here we have this three by three stride of two now from this we then get into this residual blocks so we'll get back to the paper you see we have three of this residual blocks now that's it's actually this because this is a 34 layer we're implementing right here meaning that if you are implementing a 50 layer resnet then you would have one by one three by three and one by one but for the 34 version we have three by three and three by three now these are repeated tries so that's why you will notice here we have this repeated three times and each one of them is our residual block now our residual block here our residual block is this block right here so we are going to later on implement implement this residual block here let's get back to the code we have simply our residual block you see the parameters here number of filters 64 64 and year 64 just exactly as we have it in the paper you see for each block we have number of filters to be 64 now if we want to get into our into this residual block we could or we would see exactly how it's implemented i would see that we're gonna have this two count layers one three by three and another three by three but for now let's do it this way let's just consider that that has been implemented now another reason why i want to implement this is this way is because now if you want to convert this to a resnet 50 all we need to do now is just to update the code for the residual block since what makes this different year is just this residual blocks right here so that said let's get back again here we have now this four resnet blocks so we have this four residual blocks actually here you see you have oh this is four and it's similar like here you have the three by three three by three and then here the number of channels equals 128 so you notice here that we have 128 and we have four of them now because uh here we're living from or we're modifying the number of channels we need to take into consideration this number of strides we have right here as this permits us to down sample our features now getting back to the code you have this year oh we here we have the down sampling and then we move on 128 128 128 again here we have down sampling now we go to 256 exactly as is in the paper oh we look at this directly from here here we have 256 and we have a certain number of them which are aligned we could see that in the summary year six of them check this out here you see we have six of this aligned and then again we have 512 three of them as is in the paper you see here 512 and that's it now from here we have the global average pooling 2d global average pooling and then we have this final fully connected layer which with an activation which is softmax as we've seen previously now that said uh we just in this our call method we just simply gonna call all those different layers which we just created by passing in the input so here we have our input x which goes through each and every layer and we get the output right here now that said we're going to move on to uh looking at this residual block right here so we're going to implement this residual block and now this is basically our residual block let's increase this so we could see that clearly this is basically our residual block right here so we could take one of this let's let's take this one for example we have this residual block right here and then we'll get back to the code see it here now this is our residual block it's a layer unlike the full model here we have this residual layer and then you would see that we have this um dotted brilliant right here which is true when the number of strides is driven from one so let's uh run this here let's let's have this we have dotted and then let's specify number of strides say equal one we let's print out dotted after this so there we go we have your dotted we run this take this off see it's false now when we set this to true turns to true so basically that's what dotted here does and if you get back you will notice that we let's get back to this we have this we selected this part but since this is this isn't a dotted uh link you see this link is a full line so here our dotted variable will be false and when we get to this our dotted variable will be true now let's get back to the code you see we have let's take this off you see here we have um this which we understand already and I will get back here then after we have our two convolutional layers now we'll define this custom come to d which again we are going to break up uh subsequently so let's just understand that we have this custom come to d which is presented by this uh year so when we have this this is uh it right here and this is the other one right here now you'll notice that the number of channels has been passed here and the number of strides has also been passed and that's exactly what was done here so you see we passed in the number of channels and number of strides now since by default our number of strides is equal one it means when we don't pass we simply say number of stride equal one but in cases where we have these transitions we have number of strides equal two and so our value here is going to be changed now that said we see we define this uh conv layer which has number of channels the kind of size three as in the paper number of strides and then we have the padding same so we ensure that the height and width of our input features remain unchanged now that said also notice that we have this number of strides here for the second which is equal one and getting back to the paper that's simply because even when we're getting or even when we have these transitions here where we're getting from 64 to 128 and that we're also doing a max pooling we have this stride or rather that we have in the striding not max pooling we have this stride value change for only one of the conv layers and not the two so you see here only one another two and so that's why right here you see only this one which actually changes for this other one it remains fixed always one now that said we have the activation layer and then uh if it's dotted then we're gonna have this link right here so if it's dotted we're gonna have uh let's draw this here we're gonna have uh one by one conv layer uh one by one oh it's actually here so we're gonna have our one let's take this one off you're gonna have our one by one conv layer just right here to ensure that this two number of channels match that's the number of channels we get as input here and as output actually match up so um that's the role of this we've seen this already now we get back to the code we see here let's take this off we see here this here and then we see the number of the the kernel size here is one unlike here where we have a kernel size of three and then we also specify the number of channels to ensure that it matches up with uh what we expect now the number of strides here is gotten from this so if it's two you're gonna have two if it's one you're gonna have one from here we have uh this set and then we can go ahead and do the calling so again please check out on the previous sessions where we treat models of class and so you understand exactly what's going on so here we have uh the input see the input it gets into the first conv layer then gets to the second conv layer that's the output from the first gets as input to the second and then we get this output and now let's suppose that uh we have a normal layer let's say we have in this one let's get back to this let's suppose that we are working with this one right here in that case then the input will be added to the output directly so here you see we have this add layer right here this add layer then so flow takes the output which is this and adds this to the input and we now get x add goes through the activation the relu and that's our output now in the case where we have this the case where we have this one by one convolutional layer let's specify this in the case where we are at this position then you'll see that we will take the input and modify it before passing it to the output or before adding it with the output so you see here we have this output there we go it remains here and then we modify this or we modify this input sorry so we take this input there there is it here let's take all this off and try to redraw it so we have this we have our red block then we have our output we have the addition which is going to be right here addition and then we are going to have our one by one conf layer right here which comes and adds up with this now this one by one convolution is exactly what's going on right here and that's what we define here since kennel size one now that said you see we add this up and then we get our output x add so if we are having dot it we have that else we go through the normal path and that's basically why you see here we specify this sometimes and we don't know the times now we have understood how this works let's go ahead to look at the custom conf 2d layer now the custom conf 2d layer is basically made of a usual conf 2d layer and with a batch norm remember the resnet model the resnet paper makes use of the batch normalization layer so basically here instead of writing batch norm batch norm every time in our code what we just want to do is combine this two and then we have our batch normalization with our conf 2d layer together so that's it we now run the cell there we go we run the cell we again run the cell and then we can define our resnet 34 which is our resnet model which we've just seen resnet 34 there we go we have the resnet 34 summary let's run this we get in this error we need to build our model so what we're going to do here is quite simple we will now take this resnet 34 and then we'll call this resnet 34 so we will pass some inputs into this our resnet 34 model so here we'll suppose tf0s and then we have 1 by 256 by 256 by 3 so we have this kind of input we run that and we see that now we have our model so our model summary so this our model summary 21 million parameters and that's it now we go to the training but this time around we're going to include some checkpointing so we got this from our previous session where we treated checkpointing the model checkpointing so here we're going to ensure that as we train we save our best model weights so that said here we have this checkpoint callback again you could check back on our previous sessions where we treat these callbacks so here we have this callback which will permit us total weights for our best or our best performing model our best performing weights actually so here we have a monitor we want to monitor the validation accuracy so let's take this off enter validation accuracy save best only true so that's it let's run this our last function but before we move on let's get back to the section where we have this parameter training right here now we have this custom conf2d model let's delete the cells we have this custom conf2d model which we've seen already and then we have this batch norm layer but it should be noted that with the batch norm layer we have to specify whether we are in training mode or in inference or testing mode now the reason why we are doing this is because the parameters of our batch norm layer will react differently or behave differently in these two different modes this means that during training this layer will normalize the inputs with the mean and variance of the current batch of inputs now we've seen this already and then when we're not training that's when training equals false we're in inference mode the layer will normalize this input using the mean and variance of its moving statistics learned during training so this simply means that we have this layer right here with some parameters let's call the parameters say p let's call the parameters p so you have some parameters right here and then during training our layer updates these parameters but then during inference we do not want to update these parameters as they were learned during training and so we have to specify or we have to pass in the training or set the training to false when we are not training the model or when we are evaluating or testing the model now what this simply means is that here we'll have this to training so here you see we've passed in training and then here we have training so by default we could set this training to true so by default in training mode and that's what we have now let's run the cell and then get into our residual block for residual block here we have training again so we have training and then since we're calling this we would have training there we go we have training and that's it we have training let's run this we get back to our complete network and here we're gonna have this training so paste that out here and that should be fine so now when we are not in training mode we could specify the training parameters such that the batch norm layer or the batch norm layers parameters aren't modified so we have that and that's it okay so we have that set now let's run this and this time around we're going to set this training so let's set training to be true we have that true let's make sure that this was passed in here so let's have this training and the default is true okay we run that we run this and that looks fine now we could set this to be false or we don't we don't put any value it means it's true so we could set this to be false it's training to be false and that's it so there we go we have this set now let's get back to our last function so we were at this point we have our metrics we run the metrics we compile the model but this time around we use a higher learning rate so one thing you could also do is as described in the paper decrease this learning rate as soon as the model starts plateauing so we could start with a learning rate in the paper it should be 0.1 although here we're going to start with 0.01 so here the say you you have this learning rate and then when it starts plateauing you drop to 0.01 and then you go with that and then it starts plateauing you drop and so on and so forth so this is what they proposed in the paper and you could always implement this and we've seen this in some previous section where we implemented this kind of callback which permitted us to schedule our learning rate so that's it let's take this off get back to the code and we have we've run this already so we could start with the training now yeah we're going to train for 60 epochs and we're going to include the callbacks so let's have this callbacks and we have our model checkpoint callback which we have defined here let's copy this there we go copy this and we have it here so that's it we can now run this cell now we've launched this training and we've had this error here saving the model to the HDF5 format requires a model to be a functional model or sequential model it does not work for subclass models like in our case because such models are defined we had a body of a python method which isn't safely serializable hence consider saving to the tensorflow saved format by setting save format to tf or using save weights so now we're going to save these models in the tensorflow format all we need to do here is specify this folder and that should be fine the mode is max now since we want to store the weights which have the highest validation accuracy that's fine let's now run this again training now complete and we achieve an accuracy or the best accuracy of 83.6 we have our accuracy plot right here and then we could evaluate the model here we get 82.3 percent and 94.6 percent for the top k accuracy now what if we load our best model because the model we have in here is the latest model the very last one now let's load our best model and we evaluate this let's add this code cell then we go ahead and load our best weights here we have resnet 34 dot load weights then here we have our best weights so this should be a string since our folder there's a photo where we store the weights resnet we run this that's fine and then we evaluate our model see here we get in this accuracy of 83.6 and then top k accuracy of 95.6 percent now we go on to test this that's fine and here are some results we get happy angry sad happy yeah we miss we miss one we miss this one oh that's two three and that's it so we miss three meaning we have let's add the cell we have 13 out of 16 angles correct 81.2 percent correct okay so that's it we've gone from 79 percent to 83 percent by modifying or changing our model now let's plot out this confusion matrix and see what we get there we go your results which are much better than what we have had so far hello everyone and welcome to this new and amazing section in which we are going to treat the mobile net architecture this was first developed by google researchers in 2017 and we had the mobile net version 1 and 2019 the again developed the mobile net version 2 and after this there was a mobile net version 3 but yeah we are just going to focus on this mobile net version 2 entitled inverted residuals and linear bottlenecks by just looking at the title we could guess the type of environments for which this model was built for in fact the mobile nets have been built for environments with low compute resources like the mobile and edge devices so in this section we are going to focus on what permits this model that's the mobile net v2 to perform quite well in terms of speed while producing high quality results there are two major techniques which make the mobile net version 2 very powerful or which permits them or which permits us work at higher speeds while still maintaining reasonable quality results now these two are the depth wise separable convolutions and the inverted residual bottleneck which we have right here here's the separable convolutions this is a regular convolution your separable convolution and we'll start by explaining what a depth separable convolution is a depth separable convolution is simply a combination of a depth wise convolution and a point wise convolution now this point wise convolution is not in different from a normal convolution layer but with countout size of one so one by one convolution here that's a point wise convolution and before this we have the depth wise convolution now to understand like this now the depth separable convolution which is this two put together in sequence or sequentially now to understand what this depth wise convolution is actually let's get back to this demo where we saw how usual convolution operation works as you could see here we have this input basically let's have this let's toggle the movement so here we have some input now this input is threedimensional as you see here we have one two three so zero one two we have these three dimensions let's change the color so it's clear for you to see we have this first dimension we have the second dimension and then we have this other third dimension now what goes on during a convolution operation is that we have this uh kennel so we have this here and then we have also some kennel so in this case three by three kennel uh we have this on a kennel here three by three and the reason why it is three by three is because we have three channels here in the input so because the inputs are three channeled or have three channels we have a three channel kennel now here we could add this we could add a foot channel right here add this foot channel and then we will also have here four of these kennels and then what happens is going uh here during the convolution operation is exactly what we're seeing here so we have this one which uh is placed at a given position let's pick its color it's placed at this position for example and then we multiply all the values like you could let's uh say we add this point here you see at this point we can see how this filter in this case this filter so this filter is actually we match this filter with this one we match this one with this in red we match this one uh with this in green uh now since you're three obviously we don't have four now uh let's get back to this operation where we take this one and multiply with all the values we have here so when we multiply all the values we have here we get for example in this case one by zero plus zero times zero plus one times zero you see all this at the top are canceled then we have zero times zero zero times one canceled we have one times one here so we have one and then we have one times zero zero negative one times one negative one so we have negative one and then negative one times two we have negative two so this gives us a value of negative two and then we move to the next one this one let's change the color you see we have one times all this at the top is zero so when we multiply all this by because it's just simply matching so when we have one times zero we have zero zero times zero zero one times zero zero one times zero zero negative one times two negative two then negative one times zero we have zero negative one times zero zero negative one times two negative two so this gives us negative four at this point we have negative four we move to the next one here we have negative four all this is zero obviously so we just get to this one one times 1, we have 1, 0, negative 1, and then 0. So we have 1, let's write that, we have 1, and minus 1, because this is negative 1 times 1, minus 1, it gives us 0. So here we have negative 6, when you add all this up, basically we're taking this, we multiply, and then we add, here we take this, multiply, we add, take this, multiply, we add, and we add all these values, and this gives us here negative 6. Now because we have this bias of 1, we add plus 1, it gives us negative 5. That's how we obtain this value here. That's how we obtain this 1 right here. Now, we'll repeat the same process for all these filters, all the different positions. So basically, let's toggle this, so you see what happens. You see, we repeat this process, we move, we move, we go down, and that's it. There we go. So we repeat this to the end until we have this final value right here. Then if we want to have, you see that we get this, so what we have is, let's take this off, let's take this orange off. What we have is, now we have this output, so we have the input which is of the channel, which has number of channels 3, here we also have this number of channels 3, and then we have an output which has number of channels 1. But if we want the number of channels to be equal, say 2, like in this case, for the output we have 1 channel and this other channel, then we need to increase the number of filters we have here. So basically, here we have how many filters? We could create this again, so we will have now 2. If we want to have output 2, then we will need to have 2 of this, of this 3 dimensional filter. So we will have this again, we will have this with its own weights, obviously, and we will have this, and that's it. So because we have this 2 now, we will no longer have an output with 1 channel, but now an output with 2 channels, as you could see here. Now notice how, as we take this, we move to this next. So this year is this one year, let's pause this. So this year is this one right here, and this one is this one right here. Now with this year, we are able to get this other channel for the output. And so that's how the convolution operation works for a normal convolution. Now if we get to the depthwise convolution, we will find that it will be different from this method. For the depthwise convolution, as the name goes, these computations are done depthwise. So first of all, now here, this output will only be gotten from interactions of this channel and this channel right here. So what goes on here is, we take this, and then we pass it around as we usually do, and then we obtain this new output right here. So if this is 3 by 3, then we obtain some values 1, 2, 3, 4, 5, 6, 7, 8, 9. So these values will be different from this one because the way we compute these values is different from the way these values were computed. The way we computed this value was, we take this year, pass it here, take this, pass this, take this, pass this here, and then add all our resulting sums to obtain this value of negative 5, as we saw. But in this case, what we'll obtain for this first value wouldn't be this 5. What we'll obtain here will be 1 times 0, 0 times 0, plus 1 times 0, 0, 0, 1 times 1, we will have 1, and then here we'll have 0, here 1 times 1, negative 1, negative 2, negative 1, negative 2, it gives us negative 2. So what we'll obtain here will be negative 2. And then we'll move on to the next, we'll move on to the next, we'll go to this next position, we'll get some value as we've seen here, and up to this last value right here. So the way we got this was, we took all this for different channels, and then we added them up to get this by year. We just get this directly by taking for each and every channel and just producing the output like this. So this means that with this we are going to have, like here, let's take this off, with this, because we are having each channel for the filters producing its own output, we're going to have this three producing three different outputs. So here already we have three outputs, unlike here where we had two outputs. And the two outputs here were controlled by the number of canals we used, because here we used two canals, we have two outputs. But here we, this doesn't matter. Here the number of output or the number of input channels we have here will dictate the number of outputs. So here we have three channels. So we just obviously have these three different outputs. Now since we have these three outputs and we want to be able to control the number of output channels, what we'll do now after the depthwise convolution, which is in fact what we've just explained here is we're going to add now the one by one convolution as a point wise convolution after adding the one by one convolution, we are going to specify the number of channels here and this number of channels of this one by one convolution that will permit us leave from a certain number of channels, like in this case three to another number of channel or to a given number of channels, let's say two. So after getting these three channels or getting this output with three channels, we now get, we pass to this point wise convolution and now we're going to get just two channels. To better understand this, let's take this depthwise convolution image from paper suite code. So here, let's try to reduce this. Anyway, let's have this. You see we have one, two, three, you see this one, two, three, this is a five by five canal. And then we pass this, notice how each and every one is now responsible for its own output. See this, see we have this orange with this for this particular channel gives us an output, the red gives us an output, the yellow gives us an output. Unlike previously where we'll take this pass this year and then add all this together. But now instead what we do is we just simply carry out that addition at the level of the channel and then we get this output right here. Now let's see why the depthwise or the depth separable convolution is more efficient. And we'll do this by calculating the number of filters. So here we find the number of filters we need to get from this input, which is one, two, three, four, five, six, seven. So we have this seven by seven by three inputs, which want to convert to this three by three by two output. And here the number of filters or number of parameters we used can be calculated as here we have nine times three. Obviously in this nine times three comes from the fact that we have for each of this, we have nine and times three. The three is from here actually, because we have three input channels, then we will have three filter channels. And so we're going to have the filter size, which is three by three. I think we should change this color. So it makes it clearer. Let's change this. So here we have seven by seven by three, the three is for this number of channels. Here we have three by three. And then this is three by three for a single one, then times three, this number of weights. But now because we want to have, let's change this color again here, because we want to have an output with two channels, then we would multiply this again by the number of output channels too. So this is like some general formula to get a number of parameters. We're going to omit the biases. So here we have three by three by 327 times 254 parameters. Now this means that if we have, if we want to have number of channels of say 16, then this will give us 432. Now let's consider that we're dealing with depth wise convolution and the point wise convolution, which would form the depth separable convolution. With a depth wise, what we'll have, first of all, we'll have to note that this is no more needed, we just need this. So what we'll have will be three by three by three. Now this three comes from the number of inputs by now to get the number of outputs, we wouldn't carry out any multiplication here because basically the output from the depth wise convolution is a three by three by three tensor. In this case, or we could just say it's a three channel output. Since we have three channel inputs, we'll have three channel output. So here to obtain the outputs, we just need this, we saw this already, we just take this multiplier, get the output, get this first one, we take this multiplier, get this next one, take this multiplier, get this next one, and we're good to go. So once we have this, we now add the weights for the point wise convolution. Now for the point wise, it's a one by one canal. So you see it's quite cheap as compared to the three by three canals. So one by one, here we have one by one. So let's just put this here, we have one by one. Now times the inputs, just like with the usual convolution, it's the same thing actually, because here for the usual convolution, here we have to calculate the number of weights, just get the canal size, then like one of the canal size times the canal size times the number of input channels times number of output channels. So here we have the canal size times the canal size times the number of input channels, which in this case is three, and the number of output channels, which is two. So that's what we have. Now if we multiply this, we have 27 plus six that gives us something like 33. You see that this gives us 33. Now one interesting point to note here is if we modify this and say we want to have 16 output channel, then we will change this to and put 16. In that case, our answer will be 27, here we have 27 plus 48. Now 27 plus 48, we get that quickly. 5, 7, that gives us 75. So you see that here we have increased this number of channels from 2 to 16, number of weights 75. But when we did that year, number of weights went to 432. So clearly, the depth wise or the depth separable convolution is one that is way cheaper than the normal convolution. And in the paper, the authors argue that this kinds of convolution permits us to reduce the computational cost by eight to nine times than that of a standard convolution while we have only a small reduction in the accuracy. That said, from this diagram here, you should now understand why when representing a regular convolution, the authors have this filter, you see the filter here, which has this depth into the input, you see this depth right here, you see this. Whereas when representing the separable convolution, we have this filter which doesn't have any depth. And that's because here we have no interchannel computation or calculations as compared to this one. And then after we have this point wise. Now the point wise is a regular convolution. So we see we have this depth gain. But now it's smaller in size because it's just a one by one filter. The next improvement we should look at is this inverted residual block right here. So it's first of all called inverted in comparison to the residual block. Now the residual block, as you may notice, you have this large, relatively large channel, and then it becomes small in the middle, and then it becomes large in the output. So this means that we have some input, we have some input, and then we have residual block and then we have some output. Then we have obviously this link right here from the input to the output. Now with this, you see this becomes the data or the inputs big, it goes to small and then big. But here what we have is we pass in a relatively small channel, small number of channels on input with relatively small number of channels. So here is small and then in our block, it becomes the number of channels increased. And then as output number of channels reduced. Hence the term inverted residual block. Now in addition to the fact that we're using depth wise convolutions, instead of the normal convolutions, the fact that we have relatively lower dimensional data getting into this block and lower dimensional data getting out means that we could transport very low dimensional data throughout our mobile network. So here we have low dimensional data getting in, low dimensional data getting out, and then inside we have this expansion layer right here. As this expansion layer permits us capture as much information as possible from our input features. One thing to also notice the fact that we're using a relu 6. The relu 6 is different from the usual relu in the sense that with a relu, we have for all x less than zero, the value is zero. For all x greater than zero, the value is x. So we have this y equals x line right here. But with the relu 6, as from the value 6, we actually clip this output. So what we have here is all values. Let's get back. What we have here is we have the values, it remains x, but once we get to 6, it gets clipped. So for all values of x greater than 6, the value remains at 6. So that's a relu 6. And then one of the important points is the fact that because we're carrying out this projection from high dimensional data to low dimensional data, this relu nonlinearity generally will cause us to lose too much information. And because of that, there is no relu activation in the final layer here. You can see this in the summary right here. You see, we have the input. Then we have the expansion factor, which is T. This means that now we have this hyper parameter, which we can tune. So if we want better results, we can tune this expansion factor so that it permits us to get this better results. So here we have this expansion factor T. It's here, which expands the morph channel. So we get in with K and then we now move to TK. And then we have this TK, which now takes us to K prime. Now also note that here we have the relu 6, relu 6, but here we have no activation. Now that said, this is a summary of our mobile net version 2. So here you have mobile net version 2. You have the different bottlenecks. There we go. And then we have this conf to D average pool and then conf to D. This figure right here also shows us how the mobile net version 2 outperforms the mobile net D1 shuffle net and the NAS net. You see that with a mobile net version 2 right here. If you pick this, let's pick, for example, this two, you see the number of operations or the competition cost here is almost similar, but we see this great difference in accuracy where the mobile net V2 outperforms the mobile net V1. Then apart from classification, the mobile net V2 has been using other tasks like object detection, semantic segmentation and other computer vision tasks where we have low compute resource. Hello everyone and welcome to the session in which we treat this modern convolutional neural network architecture known as the efficient nets. In this efficient net paper, the authors proposed a more controlled manner of designing convolutional neural networks such that it suits our demands in accuracy and speed. And as you can see in those plots, you see that we could choose suitable parameters such that we could modify or increase our accuracy while taking note of how this affects the speed. That said, in this section, we'll see how Ming Xing Tang and Kwok Lei built the system for automatically scaling our convolutional neural networks much more efficiently. Conf nets are commonly developed at a fixed resource budget and then scaled up for better accuracy if more resources are available. So with the case of the ResNet, we had ResNet 34. Then after we had ResNet 50, ResNet 152. And depending on the kind of setting, we are going to pick this ResNet model which will permit us to run without any problems of latency while maintaining reasonable accuracy. So this means that if we are working in a high compute environment, then we could afford to work with this. Whereas if we are working in a low compute environment, then we would have to work with this model with fewer conv layers. Now that said, in this paper, the authors propose a more systematic study of how this model scaling can be done. And unlike other methods where we just scale by increasing the depth, here the proposal scaling by increasing the depth, increasing the width, the number of channels and the resolution. That is the size of the input image. And so here the proposal new scaling method that uniformly scales all dimensions of the depth with resolution using a simple yet highly effective compound coefficient. You can see the results right here. You see, for example, the ResNet 50. Let's extrapolate. Let's pick this here, although this has more parameters on the ResNet 50. Let's take instead the B4 because it has less parameters. So you see, it has fewer parameters than the ResNet 50. But its accuracy, that's the top one accuracy on the image net, is much greater than that of this ResNet 50 here. So we have the EfficientNet B4 version, which is about 83% to 1% accuracy, while this is only at about 76% to 1% accuracy. Now in this figure, we see how we have this baseline, some sort of baseline, like in the case of the ResNet, we could say this is ResNet 18. And then we have this deeper model depth scaling. This could be ResNet, say 50. Now, in this case, they have this baseline. First of all, this baseline is gotten by carrying out an automatic network architecture search. So we get this baseline and the different layers we have for this baseline. And then note that this baseline has a depth, as you can see, it has a depth. And when we scale deeper, when we carry out depth scaling, you see, we have much more layers added to this one. And then when we do width scaling, we increase the number of channels. So you see, we have this smaller channels for the baseline. And then the width scaling permits us to increase this number of channels. Then also, we have the resolution scaling, which has to do with the inputs. So there we have this input height times width. And now after currently resolution scaling, we see we increase this resolution. This means that we may work with a base of 224 by 224. And then after scaling, we may get to say 640 by 640. Then from here, we also have the compound scaling, which is what is used in this paper, where we don't only focus on the width or the depth or the resolution, but we scale all this systematically to achieve the best possible results while maintaining reasonable speeds. That said, we could see from these different plots that when you increase, like here we have the width, there's number of channels, which is increased. You notice that as we increase this number of channels, at some point it starts to plateau. And then when we increase the depth at some point, it starts to plateau. Then when we also increase this input size as a resolution, at some point it starts to plateau. And so this is why the authors proposed a technique where we could combine all this such that we get even better results. And there we go. We see the effect of compound scaling. You see that we have this D, depth, and then our resolution. You see when the depth is one, our resolution is one, we at this blue here, see we have worst results here. Whereas when we double this depth and then increase the resolution by 1.3, you see we have this best results right here. That said, we'll now dive a bit more deeper and look at this compound coefficient, which they spoke of at the very beginning. So we go down here and we have this formula right here. See this formula right here. All right at this equation, which is equation three, where we have this depth, we have these different formulas, these three formulas. The depth equals alpha times phi. And now these phis are user specified coefficient that controls how many more resources are available for model scaling. So this is some sort of scaling coefficient right here. So it's phi, phi, phi. And then here we have alpha, beta, and gamma. Now this is designed such that alpha, beta squared, gamma squared, is approximately equal to 2. And alpha is greater than or equal to 1, beta is always greater than or equal to 1, and gamma always greater than or equal to 1. So now we are going to carry out a grid search. So we're going to search for the best values for this alpha, beta, and gamma, and then fix them. Obviously, they are constants, so we're going to fix this. And then now start varying phi such that we carry out the scaling in a more systematic manner. And there we go to carry out or to find the values for alpha, beta, and gamma, the fixed phi to be equal to 1. And then they obtain alpha 1.2, beta 1.1, and gamma 1.15, all of this such that we have this constraint. Now they didn't fix alpha, beta, and gamma as constants and scale up the baseline network with the different phi as we already explained. And it's based on these different values of phi that we obtain the different versions of the efficient net going from B1 to B7. Now before moving on, it's important to take note of this efficient net B0, which is our baseline network. Remember, we have some baseline network, which we have seen here. Let's go this way. We have this baseline network right here, this one, this baseline network, which we are going to scale such that we have better results while working with compute constraints. Now that said, let's take this off and then scroll down back to this baseline model, which is giving just here in this table. You see we have this baseline model, efficient net B0. And then you'll notice first that the resolution is 224 by 224, meaning that we're going to start with image sizes of 224 by 224. But note that different image sizes could be used for the different models, although the best or the most adapted resolution for each model should be preferably used. Now that said, here you see we have a usual conf layer and then we have this MB conf right here. Now before getting to the MB conf, also note that after carrying out the neural architecture search, the authors notice that we could also make use of this 5x5 kennel or 5x5 kennel size filters. So unlike what we had discussed in previous sessions, this 5x5 kennel size filters are still very useful. Then getting to the MB conf we find here, here to say its main building block is this mobile inverted bottleneck. So recall the mobile inverted bottleneck, which we found in Sandlai R. Sandlai R is the MobileNet version 2 paper we had seen already, to which they also add the squeeze and excitation optimization. Now in the MobileNet version 3, the squeeze and excitation optimization was added. So here we have basically the MobileNet inverted residual block, which we had seen already. And then if we check out in this MobileNet v3 paper, which you can feel free to look at, you would have this squeeze and excitation right here. Let's zoom into this. See here we have the MobileNet version 2 with bottleneck residual, this residual, then our bottleneck as usual. Here we have this low dimension input getting in and then it gets expanded and then we have this low dimension output, which is produced in this final layer right here. Now with this squeeze and excitation, to better understand this squeeze and excitation layer, we should or we could get back to how the conf layers actually work. You see that to get this output, let's take this off, to get this output, for example, we carry out multiplications and additions for each and every channel here, that's for each and every channel on the input and those filters which correspond to this channel. And then to produce this negative one right here, all this are added up with equal weights. So the output from this computation, let's call it alpha, will be put here, plus the output from this computation, let's call it beta, we will put here, plus the output from this computation, let's call it gamma, will be put here and we'll get this value or this output of negative one at this position. Now what this squeeze and excitation layer brings in is some weights on this addition operation right here. So instead of just having a weight of one year one and year one, we're going to have some modified parameters or some parameters added here such that certain channels influence the output more than some others. And so here we could have instead of one, we can have a weight A, you have a weight B and your weight C. Getting back to this paper, the way this is done is as such, we start by carrying out some pooling and the result of this pooling will be one by one by C output. Now C is the number of channels. So if here we have C channels, here we'll have this output here, C. So this output here will be one by one by C. You notice how this is small and then we have C. This size, the size C is exactly the same size right here. So we have exactly this size is the same as this. And then for the height and width is one by one. Now once we get this, we pass this through two fully connected layers, you see with this radio activation. And then here we have this hard sigmoid activation after this fully connected layer. And then here we get this output of this same number of channels, C, which will match with this one. But now what we get here will be multiplied by each and every channel here. So this now we serve as the weights, we serve as, because remember we designed this as A, A alpha plus beta plus gamma. And then we had A, B and then C. So this A, B and C is actually this output right here. We're supposing that C, the channel size is equal to three. So we have this three here and these are the values which we get after going through this fully connected layer. And then we take this now and multiply by each and every channel. So if you break this up into three parts, let's remove this. Let's erase this. And we could cut this, let's cut this into three parts. So we have one, two, three. So we suppose there is one by one by three. So if we have this, this first part will multiply this chunk. So we'll multiply this chunk and then this other part will multiply this next chunk. And then this other part here will multiply this other chunk. And so now we have this channels whose contribution to this output is now weighted. That said, we also have the expansion factor here is six. Then getting back to the results, we see how the efficient nets perform better than the corresponding other confidence with similar number of parameters or even more number of parameters. Like here we see how the efficient net B zero outperforms the resnet 50 though you see this great difference number of parameters as the efficient net is more efficient as or has fewer parameters as compared to the resnet 50. We see resnet efficient at B one compared to resnet 152. You see 60 million years 7.8, but this one is more accurate than the resnet 152. You could check out from this right up to efficient net B seven. You see we have this G pipe. They are 97.97, but this one has 557 million parameters while this is only at 66 million parameters. And we could also look at the floating point operations. You see here we have fewer floating point operations for the efficient net B zero while still having higher accuracy. We also see that if we scale the mobile nets and the resnets will still, they wouldn't still get better results compared to the efficient net. And it shows the power of the network architecture search, which was used in getting our baseline. Now we'll go down and check out this year. We have this results right here. You see the class activation map, which is a visualization technique, which permits practitioners understand how the model or rather what portions of the inputs helped in producing the outputs shows clearly here that when we use compounds killing, we have the map, which is more focused on relevant regions, as you could see right here as compared to the baseline model and this other models with the deeper with depth scaling with scaling and resolution scaling. Hello everyone and welcome to this new and exciting session in which we are going to treat transfer learning and fine tuning. Transfer learning can be applied in several domains like computer vision, natural language processing and speech. In order to better understand the usefulness of transfer learning, we have to take note that deep learning models work best when given much data. And so this means that if you have a data set of only a hundred data points, then you're most likely going to have a poorly performing model. But what if we tell you that it's possible to train your model or to train a given model on say a million data points and then use that model or adapt that model such that you can now train it on this very small data set such that you start getting very great results. This is very possible with transfer learning and that's what we shall be treated in this section. At this point, one question which may be going through your minus, how is it possible to train a model made of say a million data points and then use that same model on a smaller data set which is obviously different from this data set with about a hundred data points. The answer to this question lies in this figure we have right here. Notice how you have this image here, this image of this truck and then we have this model which takes this input and then produces some outputs right here. This model or the kind of model we'll use for this image tasks are generally the conf nets as we've seen already in this course. And with the conf nets, we generally have two main sections. The first section is the feature extractor. And then as we go towards this final sections here, we have the classifier. And so the very first thing the conf net we want to do will be to extract low level features. And then as we go towards the end, we focus on extracting more high level features. Notice how you're with this, let's pick out this feature right here or this feature map. You see that we pick up these edges. You see for other feature maps, we actually filtering out some low level features or extracting this low level features from our input. Then as we go get towards the end, we get more high level features like for example, whole portions of the image like the tire. And then after this, we generally have a classifier which now permits us to pick between a set of options which one the model thinks this image is actually. Now because the conf net works this way, it means that if we have two data sets which are similar, then we could build some sort of feature extractor or we could build a model which will extract features from this very large data set. And then because these weights have been tuned or have been trained such that the extract features correctly, then when we pass in this very small data set, this section of the model will do just its job. That is of extracting useful features from this data set. And you see that because these two data sets are similar, it's going to do a great job. And so we will not need a very large data set in order to extract features from this small 100 size data set right here. And so in fact, what we're saying is we have a model, let's say we have this model, then we have this feature extractor unit and then we have this classifier unit which we've seen already. Generally this starts after the flattening all the global average pooling and then here we have some dense layers while with this we have a ConvNet or some convolutional layers with some max pool and batch norm layers. So what we say now is we have this small data set which we're going to pass in here and then we'll get at this point. So it's here that we're going to get this outputs from our feature extractor unit. And then since generally we have for example or we pretrained this model, not the word pretrained because we are somehow using this for the first time in this course, we are pretraining or we pretrained pre that we did the training before, we pretrained the model on a large or relatively large data set like for example ImageNet. Let's suppose that we pretrained on some large data set with one billion images, then this unit year has lent to extract features from whatever image you give it. And so now when you come with just a hundred images, what it does is it extracts those features and then since at the level of this classifier we have a different setup for the pretraining, we now have to modify this classifier. So this means that if before let's suppose that let's suppose that before we had after the global pooling we have say a hundred unit dense layer and then 1000 output dense layer right here. Then in our case where we have just three outputs, what we'll do now is we'll simply replace this here, we'll take all this off and then now we may pass this say directly to the three output dense layer or we may pass this first to say 128 and then to this three output dense layer. So from here we see that this new model will focus more on the classification while allowing the previous pretrained model to take care of extracting these features from our data. Now it should be noted that we generally use this concept of transfer learning when we have a very small data set and obviously since deep learning models perform best with large data set, we want to get the best out of them and so we want to use the transfer learning when we have a small data set as we've said and we have this model which has been pretrained to extract these useful features from those kinds of images. So this simply means that we should have two similar data sets. Another advantage of using or working with transfer learning is that you get to gain in terms of training compute cost. That is this model which was pretrained may have been trained for say three days and then now all you need to do is just get this pretrained model and then apply transfer learning on your own specific and smaller task. And so when you're running on limited budget you find that working with pretrained models is going to be really helpful. Now apart from transfer learning we also have finetuning which is quite similar in the sense that unlike with the transfer learning where we have this feature extractor's weights which are fixed and then during training we update the weights of this classification section with finetuning what we could do is also update the weights of this feature extractor section. Now generally we start finetuning from the top so we suppose that this is the bottom here so we have the input and then we have this final layer so we would say that we'll start finetuning from this final layers going to this initial layers. So yeah we have this finetuning process we could get this first this top layers with that is we fix this layers here we keep these layers fixed so their weights aren't updated during the training process and then we update these weights while obviously we update this weights already but the difference is that this weights were initialized from scratch that is we randomly initialize this weights whereas this weights are initialized from the pretrained model. Now this weights here are from the pretrained model but they are not trained. Now we could again depending on the kind of results we're getting meaning that if we apply finetuning to this top layers or to this final layers and we get better results you see that what we could do is we could keep finetuning so we could keep increasing this or this section of weights we could update in the feature extractor unit. So yeah we'll take this off take this off and now we have this part which we can train so this is trainable this is untrainable or not trained so we have something like this now it's also possible for us to go ahead and just say okay we're gonna train all our model but while carrying out finetuning if there's one thing you need to note is that you have to use a very small learning rate and the reason why you're doing this is to avoid disrupting this weight values which have taken very much time to attain and so as we do the finetuning we're gonna update this weights but very slowly and by getting this by updating this weight very slowly we mean we're gonna choose a very small learning rate and then observe how this affects our models performance. At this point we'll get straight to the code and we'll look at some pretrained models so here you could see you get into this tensorflow applications you have called next model, dense net model, efficient net model, efficient net v2 we've seen the efficient net already, inception net, mobile net which we've seen mobile net v2, v3, net net and the famous res nets which we've seen already with the VGGs and the exception net. So here you have the choice of speaking out any one of this we're gonna go straight to the efficient net so we could have this here or you could pick the efficient net v2 so you could pick any one of this so here we're gonna pick the efficient net v4 this one since he has slightly fewer number of parameters compared to the res net 50 and it outperforms the res net 50 by very large margin so we're gonna pick this efficient net v4 and if there's one thing you can do with tensorflow is simply the fact that you could use these models without having to code them out from scratch so as you could see here we have this tensorflow Keras applications efficient net, efficient net v4 and then we just have this argument with this we get we're gonna define our efficient net model we paste this right here and we'll call this the backbone so we've seen this already now here we're not going to include the top recall that as we had seen here we have this old model and then let's take this finetuning part off we have this whole model and then what we are interested is in this feature extractor unit so we'll set that include top to false let's get back here we have include top we're not going to include this set this to false take this off and then the weights have been pretrained on the image net data set we're not take this input tensor we have the input shape now this input shape will have configuration so basically we have configuration the image size the image size there we go in size by configuration and since we are not including the top as we've said here we're not going to take into consideration those classes now the classifier activation so we have that off and then all we could decide whether to include the pulling layer or not if we want to pick the pulling layer then we'll have to specify what pulling layer want to work with either the average or the max so with this we are just gonna take this off and specify that later on so here we have our backbone we run the cell and then what we do to freeze what we call this is freezing so what we do to freeze this backbone such that the weights aren't updated during training is by simply setting this here to to false or setting this trainable parameter to false so backbone the trainable equal false and that's it so this all we need to do to freeze our model now we're frozen our model the next thing to do is to add this other layer right here all this other layers so we'll go straight away then yeah we're gonna define this input with the image size now once we have this input we now pass in all we have the backbone so we have the backbone here which has been defined and its parameters have been set to be frozen and then from here we have the global average pulling so we have global average pulling here then we now have this dense layer now the configurations will set this number of dense layer 1 to 1024 and number of dense layers 2 to 128 so let's get back to this there we go we have that and then from here we're gonna have batch normalization layer there we go let's copy this paste it out here we have another dense layer now this time around second one that's fine and then finally we have this dense layer activation softmax so here we have our softmax activation and then this is number of classes here we have number of classes and that's fine so let's take this off now and then run this cell right here we're getting this error because we have to specify this this way that's fine that's it so now we have this model you see total parameters 19 million the trainable parameters just 1.9 million and the non trainable parameters are 17.6 million so it means that the backbone itself is 17.6 million and then this additional parameters here come with the remaining 1.9 million parameters now with this we have our model already set you see with minimal code and we could go ahead to start the training then here we can now start by training our model again we compile the model and we run the training process now training is over we could go ahead and evaluate our model so let's run this and what do we get we have close to 85% accuracy and 95.3% top key accuracy and this does slightly better than the previous models which we had worked with let's go ahead and test this so changes to model and we run this here incompatible found shape this we are going to resize this before passing into the model so here we have this image and just here we have let's say we have our test image which will resize so we just open CV to resize this image we pass in our test image and then we specify this in size so here we have in size copy this and there we go let's run this again and here's what we get you see we have the side output now let's go ahead and check this out here we run this here we have one miss no miss no miss the second miss here we have two misses and that's it so out of the 16 we have two misses that is 14 divided by 16 about 87.5% accuracy on this small batch of images which we took from the validation dataset we go ahead and check out the confusion metrics oops let's run this run this get back and here's our confusion metrics we get even better results but one thing we have to note here is that our dataset was not that small and so we may not see this change or this difference between training from scratch and using transfer learning so what we'll do is we'll take this we'll take just say 10 that's 320 so we'll have a dataset of 320 data points and then we'll see the difference when we train from scratch and when we train with a pretrained model so right here let's get back to this we take this down and we're gonna use okay use any of this one so let's let's pick the Lynette quite simple pick the Lynette and then here we have in this let's scroll down we have training loss function the same metrics the same and here we have Lynette model so run that and yeah Lynette model that's fine and so yeah we'll train on this small part and validate on the full validation dataset we'll do this for just 20 epochs so after training for 20 epochs on that very small dataset you see the model doesn't perform well you see it doesn't even get up to 50% validation accuracy while the trained accuracy keeps increasing so the models over feeding now from here we are now going to change this so we're gonna use our model this pretrained model so we have this here we just run this again so we initialize this parameters and then we'll compile the model this is model so let's let's call this pretrained pretrained model let's change this name here to pretrained model there we go we have pretrained model and we'll get the pretrained model summary that's fine yeah we have that and then here we have pretrained pretrained model okay so here we're gonna run this pretrained model compile and then we'll start with the training again so just note that we we had the accuracy validation accuracy below 50% previously and now we're gonna check out our validation accuracy when working with the pretrained model but already one thing you could notice just after two epochs like here see after this two epochs we are gonna see this validation accuracy which is already greater than 50 even from the first epoch it was already greater than 50 it shows you the power of working with pretrained models as we are now making use of those extracted features to get this much more performing model see how the accuracy keeps increasing now we're done with the training you could see that this model which before we couldn't cross this 50% mark for the validation accuracy now is able to cross this mark as we'll see here you see we have the validation accuracy of 71% while just training on 320 data points so let's get here let's run this and you could see what we have here you see it gets just above 70% so now with pretraining we get above 70% and we even got greater than 50% just from the very first epoch and so what we could see from here is the very first thing is get as much cleaned data as possible and if you can't lay hands on this or try some data augmentation and then from here if your data set is still very small then you could then apply transfer learning but if you have a relatively large data set it will be needless applying transfer learning as training from scratch should normally get you better results so here we could evaluate our model so you could see that this is our pre pretrained model with just 10 out of 213 batches which produces 71.3% validation accuracy we now move on to fine tuning as we've looked at transfer learning and now we get into fine tuning but before getting to fine tuning we would first convert this R code which was built with the sequential API now into the functional API so here's our converted model it's basically the same thing we have the input we have the backbone which takes in this input produces output with our average pooling dense layer batch norm dense layer and this dense layer right here then we have our fine tuned model right here let's run this code cell and then we could view a summary which is meant to be identical to what we had already with the pretrained model so here we we have this 17.673 thousand see this is 675 it doesn't match with what we expect so let's get back here and we noticed that we did not put this here so let's um x we did not include a batch norm layer so let's run this again and then we now have this summary right here which is exactly the same as that of our previously built model with a sequential API and then now we want to fine tune our model that is all these layers which were frozen that is not trained we now want to make them trainable so right here we'll get back and then we simply have backbone the trainable and we set that to true then here we are going to set this trainable to false now recall when we're building this resnet34 model right here we have this trainable parameter which we made use of because remember here we had training sorry it's not trainable let's get back here that was training so we should have training here now trainable is different from this one so take note of that this is trainable and this is training so while we set this training to false and we'll get back here you would find that this batch norm took in this training parameter and the reason why we need this specially for the batch norm is simply because the batch norm works differently during training and inference and then during training the batch norm layer normalizes its output using the mean and the standard deviation of the current batch of inputs whereas during inference the batch norm layer normalizes its outputs using a moving average of the mean and the standard deviation of the batches it has seen during the training so since at inference or when we find tuning we do not want the batch norm to take the current mean or rather the mean and the standard deviation from the current batch of inputs we are instead going to compute this from what it saw during training and so that's why this training parameter is very important we have this training right here you see it takes the input and the training where we could set training to be false for training mode and training to be rather training to be true for training mode and training to be false for inference or let's say fine tuning now before we move on you should note that this layer the trainable set to false is different from setting training to false when you set a layer's trainable parameter to false it simply means we do not want to update the weights when training but when we set training to false it means we work in an inference mode in the case of the batch norm this gamma and beater are trainable parameters and so when we say layer the trainable equal false it means that they are not going to be updated during training but on the other hand this mean and variance aren't trainable parameters instead they are parameters which adapt to the training data and that is why when we add inference mode that's when we set training to false we do not want to disrupt the mean and variance values gotten during the training based on the training inputs and so as we saw already this mean and variance at inference mode will be simply the moving average of the mean and standard deviation of the batches it has seen during training and so clearly these two concepts the that is setting the weights to parameters not to be trainable and so clearly the concept of setting these weights not to be trainable is different from that of working in inference mode nonetheless it should be noted that in the case of the batch norm setting trainable to false on the layer means that the layer will be subsequently run in inference mode now although we've seen that these two aren't they don't mean actually the same thing now also note here that setting trainable on a model containing other layers will recursively set the trainable value of all inner layers and if the value of the trainable attribute is changed after calling compile method on a model the new value doesn't take effect for this model onto compile is called again okay so that's it you could check out all the dropout here we have this dropout layer which doesn't have any trainable parameters but remember that the way the dropout works is that you have let's say some inputs let's take this off you have some inputs and then if you to pass to the dropout layer at the end you have some of these inputs which will be not considered so you have maybe this will proceed but this not taken into consideration maybe this moves and then this not taken into consideration so the dropout of 0.5 simply means that half of our inputs will be will move forward and the other half will not be taken into consideration and so you see that at inference that is when we are actually trying to test our model we do not need to drop out some of this neurons right here and so generally the dropout also takes in this training parameter here where when we set training to true it means that we are in training mode and so we could actually drop out some of these values whereas when we set the training to false then we are in inference mode and so we do nothing so we just allow the inputs to pass without any modifications and you could also see clearly from here that the layer that trainable doesn't really apply because dropout doesn't have trainable parameters whereas with this training we could decide whether it's true or false that is what is in training mode or inference mode so in fact what we're saying is we have this model here so in fact what we're saying is we have this model here we have the backbone and we have the head for classification we apply transfer learning by freezing all this we freeze all our backbone so no parameter here is updated during training and then now we move on to fine tuning where in fine tuning we want to update these parameters with a very small learning rate and then we also want to avoid a situation where those mean and variance statistics which were gotten during the training process will be upset during this fine tuning process and so the batch norm is kind of like a special layer where even during the fine tuning where we want or where we have set the trainable to true that is we want to update this weights during the training we do not want to modify or upset the batch norms mean and variance and so we are going to set this training here to false so it still behaves as if it were in inference mode so getting back here we have our training which has been set to false and then we could start training our model again but one point to note here is we do the fine tuning on a pretrained model which has already been trained so what we'll do is we're not going to start training this model this way we'll start by training the pretrained model so we start by having this backbone to be set to false and this training set to false so we're going to repeat the transfer learning process again before then applying fine tuning so each time you want to apply fine tuning make sure you have this set to false training set to false and then you go ahead so yeah let's run this again uh here's our fine tune model which achieves uh best validation accuracy of about 70 percent okay now we're done with transfer learning we're now going to apply fine tuning to do this we're going to set this to true so all we need to do here is set this to true we're not going to rerun this again we is the same model and that's even the idea the idea is for us to start with the backbone which is not trainable and then later on make it trainable uh while now just simply recompile in the model so don't forget to recompile this model to take into consideration the fact that some parts of the model are now trainable so let's get back here recompile the model and see what this gives us now as we start training we notice that this validation accuracy here isn't uh looking like what we expect because before getting to the fine tuning we already had a model with a validation accuracy of about 70 but now we're getting this 33 and the simple reason why this is so is because our learning rate here we still maintain the same learning rate instead of reducing it before the fine tuning so we would have to stop this here then we would get back to this here set this to false so we would have to start back the whole process set this to false and then we retrain the model we get this accuracy of about 70 now we get back here and we set this to true so we set this to true that's fine we run this cell backbone now trainable and then we are going to make sure this learning rate here divided by 100 so we're going to make use of very small learning rate now once we have this we're going to run this again so we're going to compile our model and restart the training process training are completed the other results we get you could see that the validation accuracy increases up to 72.2 percent so we make an extra gain of 2 percent for the validation accuracy after fine tuning our model and this makes sense since fine tuning permits us to squeeze out some extra jewels from this backbone since this time around it's actually trainable and that said we've just completed section on transfer learning thanks for getting around to this point and see you in the next section hello everyone and welcome to this new and exciting session in which we are going to visualize the convolutional neural networks feature maps one very important part of building robust deep learning models involves understanding how these models work or understanding what goes on in the different hidden layers and so in the section we'll focus on taking a model which has already been pretrained and then generating these feature maps so we get to see exactly what goes on under the hood the pretrained model we'll be using here will be this vgg16 so we'll simply copy this get back to our code paste this out there we go we have our vgg16 we're not going to take the top so we'll take all this off now our input shape and we'll take this input tensor off we're not going to include the top so we set this to false that's it then this input shape will define it as our in size here so we have configuration in size that's fine and we add this here okay so that's it we set this and we give it this name vgg backbone so we have vgg backbone right here we can check out a summary vgg backbone summary run that there we go you see it has about 14.7 million parameters we now move on to the next step where we're going to create this other model which will permit us visualize this feature maps now to explain how this works let's recall that we have this vgg right here so we have our vgg and then what the vgg does is it takes in an input image so we have an input image and then it produces a single output now if we have say not included at the top then we will have this output which is 8 by 8 by 512 so here we have 8 by 8 by 512 and when it took in this 256 by 256 by 3 input now since we have only this one single output and we are interested in visualizing the hidden layers that what go that's what goes in the vgg model what we'll do now is we'll create a new model we'll create a new model right here which instead now has many different outputs and these different outputs will come from these different hidden layers so we could take this one and it becomes now an output this one it becomes an output this one output and so on and so forth so basically this hidden layers now or this hidden or the outputs of the hidden layers that's our feature maps will become our outputs so we'll now have this model with this as input and then this as output so this will be now our different outputs instead of just a single output now we have this different outputs there will be about 17 outputs in total now you may also decide to pick the specific output so you may want to take only the conf layers the only the outputs of the conf layers so you omit the max pull layers here here here and here with this one but it all depends on you and we'll see how to do this so let's get back to the code well we're now going to build our feature maps so we'll take the feature maps and then we'll put this in a list so here we'll get the layer output and this for layer in the vgg backbone layers so we we have this vgg backbone layers right here or this vgg backbone model here we're going to get all its layers starting from this one we're not going to pick the input so we'll simply have this and that will be it so we take these layers and then from here we'll build this new model which we'll call feature map model and sakura's model from here it takes in as inputs the vgg backbone so we have vgg backbone input here we have the vgg backbone input is the same input but now what the difference is that the outputs is this feature maps here so we don't have this just one output but all this other um hidden layer output will now become our output or be part of our output so here we have feature maps so that's it we'll build this new model we'll uh view the summary feature map model that summary and there we go so that's it uh it looks similar to what we have but if we had picked say from model one to just model four or sorry from layer one to layer four then you see it's shortened because this all we need for this our new model but since we're getting right up to the end you see that we actually go through the whole vgg model but the difference is that now we have outputs that we have many outputs and we have just this one single input unlike before where we have one input and one output so from here we have this model which we've just this new model which we've just designed start from one run that again you have that and now let's head on to passing an input through this model so what we want to do now is we take this input image and then we pass it into our model and now since our model outputs the different feature maps the different hidden layer outputs we will now be able to visualize what's going on inside our vgg model now to get this output we are going to use something similar to the testing which we've seen already recall we did we carry out this testing here where we take we read this image we could simply copy this where we read this image and then we pass it to our model to get the output but now in our case the model let's reduce this the model we'll be working with is our newly created feature map model so let's have this here and that's it we have our test image we resize we pass this in this feature map model and then you'll see that oh when we run this we now check out feature maps so we'll say for let's say for i in range length of the feature maps we want to print out want to print out the fmap shape so we have that list has no attribute shape oh okay fmaps i so let's pick out this i we run that and it's and you see this so you see that the output starts from here from this one instead of the input starts from it actually starts from here as we since we have decided to start from this one since we decided to start from here because we don't want to include the input as part of our output so that's logical we have that we start from this right up to this very last one year now here we've picked the conf layers and the max pull layers so we have all this now so we can now um visualize this different feature maps right here now let's note this length let's print this out let's note this length is here you have 18 different outputs we get back here and we modify this so we'll say that we'll only do this if is conf of layer that name is true so if this is true if this is true then we are going to attach this to the outputs now oh we could also do this we could just simply have it like this and that's fine now so what this means is we are only going to take the conf layers as part of our output now we could define this is conf this is conf takes in a layer name so basically this layer names uh what we have here does you see why it's important to always give your layers some names because now you see it's helpful always it's used now to differentiate between the different um types of layers so here we have this layer name and then we're going to say that if this layer name if this layer name or rather if conf in the layer name we return true then else we return false so we run that there we go we run this again it looks similar to what we had before but one thing you'll notice now is that this length is reduced so we've taken off five we've gone from 18 to 13 so we've taken off five layers which correspond to this max pool layers since this conf uh this conf isn't in this name right here so that's it we have that we could also decide to say okay we want to take only the max pools so we will say if pool uh pool we run that and we check this legacy now only five there we go so that's it oh let's get back to conf and we have that okay so now we've run this and we have the different shapes now to carry out the final visualization you see you have this f maps here so we're going to go through each and every feature map so for i in range the length of uh feature maps we now create this figure and then we specify the fixed size so we have fixed size equal to 56 by 256 now we have that we call this method right here and then from here since we're going through each and every feature map is important for us to get the feature size so we want to get these values for each feature map now with this we just simply have f maps we have k and then all rather i here so we get in this we pick a feature map we get that and then we get the shape uh one so this will permit us to get this value so this is shape zero shape one shape two shape three so we get this feature size the feature map size then we now get to this number of channels so we have n channels equal the feature maps i shape three three because this is zero one two three so this is how we get number of channels now we have this already set we want to be able to visualize this such that like here such that all these channels are lined on a single line so because we have this 512 16 by 16 let's let's check this earlier ones we have like here we have 64 256 by 256 images so let's suppose that this is one of them here we have this 256 256 by 256 right here and then we have 64 of this for the 64 different channels right here now what we want to do is take this one let's take this one put it here take this other one and align it take the next one align it so we could visualize this in one line up to the very last one right here so to do this now what we'll do is we'll create another array which we'll call joint maps joint maps uh mp1s we initialize that way and then the size here we take the feature size so we have f size and then to get the this for the width this for the height actually so we we know that the height in the case of 256 by 256 this height is 256 so we have this height 256 this distance 256 but then now the width is going to change so the width is no longer 256 by 256 times in this case 64 so here we have um 256 here we have f size times the number of channels so that's how we do that so with that we now have this joint maps which is initialized to one so we have this array now set now the next step will be to fill in this value this output this features in this array now we understand how this joint maps here was created we now go ahead to fill this um information all these different uh features in this giant maps array so here we'll do we'll go through the different channels so for j in uh range uh n channels so basically n channels we have that and then we fill in the joint maps now the way we'll do this is we'll keep the height fixed so we have the height fixed and then in this width dimension we'll fill this information in a way that as we go from one channel to another we are going to keep steps of 256 so here we will have um f size our filter size our feature uh map size is 256 f size times j and then we go to f size times j plus one so what this means is would we fix the height as we've said already we've we'll fix this height here the height is fixed 256 that is it here so we'll take all elements in the height and then for the width if when j equals zero for example we have zero up to zero plus one so the here we have zero zero plus one so we have zero and then zero times f size is zero then one times f size is 256 so this means that in the width dimension we're going to go in the height we have 256 in the width we have 256 now when j goes to one now here we have one one times f size is 256 so we have 256 and then one plus one is two two times 256 is 512 so now we'll skip 256 steps at this point we are at this point now and we get this one and then we repeat the same process again when when j equals two we have 512 and then we move to this should be 768 um no 256 yeah 768 so now we go from 512 to 768 and so that's how we're going to be filling up these different positions right here now once we have this already set we now go ahead and and pass in the data while we have to fill in here so we have f maps and then we take in i if we consider this case where j equals zero that's we've picked this zone from zero to 256 and we've collected all the height then we have this patch right here and to fill up this patch we have our feature maps which we've seen already but then we're going to take a particular feature map obviously this i here picks that particular feature map and then once we pick the particular feature map we can now go ahead and set this here to the values of the feature maps while selecting the particular channel so now we when j equals zero for example we take the zero channel and so we'll take all the values which come before and then pick out now this zero channel for example now here's j so that's it we have we have our join maps which has now been created and then we could take this off and we're now ready to plot our image so we have this image show and then we pass and join maps now if we want to pass all this is going to be very ram consuming so we just take the we select all the height and then we'll pick some values so we'll go from zero to for example 512 so we'll have that and now we could run this but before running this we need to set the different axis here we have axis then we have plot that sub plot uh the length of the feature maps so here we'll basically if we have certain feature maps then we're gonna have this um different sub plots here one and then here we have i plus one so that is it so once we have this set we can now run this and see what we get takes a while to run and now that's complete we could visualize the results here let's simply scroll down and you see this result you see for the initial layers we have this low level features which have been extracted so we have we could see this clearly here and as we go down or as we go further or deeper into the network this uh the features we extracting start to become more high level features so you see this see here see this one um focuses see here we extract this mount here unlike before where we're more focused on um edges so that's it we keep going deeper and we see the outputs or the results we get in and that's it we've just visualized a trained model of feature maps now another thing we could do is suppose at the beginning here that we have known right here so we don't we we don't want the pretrained weights so we run this run this again and check out on what we are gonna what the model is gonna produce here is what we get you let's scroll so you get to see this you see the inputs um and as we go deeper we'll notice that not much information is yet to be extracted from the input hello everyone and welcome to this new and exciting session in which we are gonna see how to implement the grad cam technique which permits us visually explain how the deep neural networks work this was first developed in this paper by Ram Persad et al and entitled grant cam a visual explanations from deep networks via gradient based localization so as you could see here we have this input for example let's reduce this we have this original image and there we could see this grad cam output now the task here is a classification task where we're trying to say whether there's a cut or darken the image and then with a grad cam for the cat we're able to detect the portion of the image which influenced the model to say that there's a cat in the image and also for the grad cam dog we have this part here which shows us that this portion influences the model the most in detecting or seeing that there is a dog in the image now we also have the grad cam for different models you see this grad cam for the vgg model and we have this resnet grad cam right here so we see that two different models can produce two different grad cams though generally they should be similar we said with a resnet we'll have this larger surface as compared to this vgg and we could also see this for the cat now that said we are going to implement the grad cam technique which generates or which permits us to generate these kinds of visualizations which tell us what parts of the input influence the model in taking certain decisions as we could see in this figure two from the paper given an image and a class of interest for example the tiger cat or any other type of differentiable output so um like here we have this image you see the image and we're giving the class tiger cat so here the task is image classification so here we have this class tiger cat now um for now we'll keep out this year we'll focus only on this image classification so essentially we have this image as we said already we we have the class then we pass this through a cnn that's a convolutional neural network you have that right here where let's say we forward propagate the image to the cnn part of the model remember um this cnn has already been trained so we forward this to the cnn part of the model and then through the tax specific competitions to obtain a raw score for the category so task specific year in our case our task is image classification so this year is specific to um our task which is that of image classification now the gradients are set to zero for all classes except the desired class then the signal is then back propagated to the rectified convolutional feature maps of interest so you could see from here that when we forward propagate this to our cnn we have this output feature maps here you have rectified conv feature maps so these are feature maps and then we pass this through this fully connected layers then we obtain the outputs from here we back propagate as we said already to this rectified feature maps and you can see that the resultant of this back propagation is what we have here notice how this is colored compared to this so here's our feature maps here's the output from back propagation from this output back up to this feature maps so we we obtain this by doing the the derivative of this output let's say y let's say y tiger cut let's call it tc y tc y tiger cut with respect to x feature maps let's call it fm so we're finding our we're computing this derivative of the output with respect to each and every value we have here for the feature maps and obviously this will give us a corresponding feature map it's just like having uh let's let's pick any position let's say this random position here we take the output with respect to this we take the output with respect to another position take the output with respect to another position and so on and so forth and by doing this we reconstruct somehow those feature maps but this time around um it's going to be the derivatives of the output with respect to each and every position then from here as you could see let's take this off from here what we do is we are going to obtain the mean for each and every one of those features so you see this one this one that's pinkish you have this here so we obtain the mean of for this we take for this we obtain this mean um this and so on and so forth so we obtain all this and so all these mean values now are going to be multiplied by this feature maps so for this for example this one here we'll take this and multiply um these features we'll take this multiply these features and so on and so forth then once we get this we're going to combine all this or add all these features up and then pass this through a relu function which then produces our grad cam and so that's how this works in theory that's how we obtain this grad cam we have here let's dive into the code and get this practically so first things first we have this efficient net d5 model which we had trained already so we'll just um load the weights let's go ahead and load this weights so we have pretty much a lot of weights and there we go from here we could test out the model so we have this image here basically what we've seen already in testing so we have the test image um we expand dimensions and then we have the pretrained model which now takes in our image array so this is the output right here you could see that the max is at this um index one which is logical because it's a happy image we could um see that right here person is happy so there's our image and here's what our model produces remember as we've seen already we have actually three classes we have angry happy and sad and um this is the zero index index one and index two so i'll put this index one meaning this um prediction is correct at this point if you could recall from here this model was broken up into two parts so we have let's take this off we have this first part which generates this feature maps and then this other parts which takes in the feature maps and then outputs the specific class of this input image but given that when defining our model this was in a block that is we basically define this as one not as two separate entities like this we are going to look at how to separate this or better still how to create a model which is made of only this first part and then the model made of this other part and then one thing you should note is that the way we created this model was a bit different from what we had seen so far so previously we get back here with those pretrained models let's get back modeling we have pretrained well let's pick this one transfer learning with this you see that we have a backbone defined and then we have the backbone we have global average pulling and so on and so forth and then when you look at the summary if you look at the summary you have all this represented as one so the backbone was uh you don't get into the details of what is contained in the backbone but now when you look at this other summary let's get down to grad cam when you look at this model summary now you would find that all the details of the backbone are given and this has gotten differently because now what we do is the input our x which is passing to the global average pulling is simply this backbone dot output so when we say when we specify that you want to get a backbone output this permits us get a summary with all the details like this and then when it comes to the inputs here you would say you want to have the backbone inputs so let's this input right here is actually useless you need to take it you could take this off so that's it so once you specify this and then you have your usual output then you would get this full summary okay so that said now let's go ahead and create or make this model out of this full model we have here so in order to create this last conf layer model which is essentially this model here where we would get the output to be this rectified conf feature maps we have this Keras model which takes us input the pretrained model inputs so it's quite similar to what we had already here where we just take the inputs to be the backbone inputs now the our inputs is the full pretrained model inputs but then the output take notice the output is this last conf layer output now what is this last conf layer the last conf layer is that layer whose name is given as top activation you'll notice that this is this last conf layer where we have this feature maps it's 8 by 8 by 2048 and then from here we move on to the global average pooling then there's one then two and then we have our output so this part from here upwards is this initial conf layer or this conv convolutional neural network and then this here is this classifier unit to the right and so now that we have this name of this last conf layer that's a top activation we're going to make use of that to produce our output you see last conf layer is simply the pretrained model and we get the layer whose name is last conf layer name and last conf layer name is top activation so we understand where we get this top activation from so now we have our last conf layer as we've said already we are now able to have this last conf layer model or this initial cnn model here which has as input the image and as output the feature maps so let's run this and see what we get there we go we could we could do last conf layer model model and check out the summary there you go we have this model let's check out the output you see here's the model output and here's the model's input see 256 by 256 by three so we have the input is the image and the output the feature maps now we've built this first model or we've created this first model from our overall model let's go ahead and check out on our classifier model so for the classifier model you see we have this inputs the input now is going to be this feature maps remember the output of this was the feature maps and the inputs of this is the feature maps from here given that we have the inputs to this classifier model we are going to simply pass this through each and every layer which makes up the classification part of the pretrained model so if we get back up here let's scroll back up you'll notice that we had this global average pooling we had this dense we had dense one we had this dense two which all make up the classifier path so let's get back here and then you see this classifier layer names is given here so we understand where these names are coming from and then we simply go through for every layer name in our classifier layer names so let's have this so for every layer name in our classifier layer names we are going to pass the input this input classifier input into these layers so we take this input pass into the global average pooling then the the output will be x we take that again pass this into the dense then we take that again pass into dense one and then pass into dense two and then we have the output which is in this case x so our input is a classifier input which is this and our output is the output after we pass this input through these different layers the count block and the classification unit designed the next step will be to compute the partial derivatives of the output with respect to the features here now we could compute this partial derivatives or this gradients making use of this gradient method which takes in the top class channel and the last conv layer output so this year is our top class and then here we have our feature maps to obtain this feature map that's this last conv layer output we simply pass the inputs that's the image into our last conv layer model our last conv layer models this conv layer or this conv model right here take the input pass in here obtain the feature map so here's our feature maps and then from the feature maps we pass this through our classifier model so we take the classifier we take the feature maps pass in the classifier model obtain the predictions here turn those predictions and then get the top prediction index now the top prediction index will depend on our example now given that the person is happy we expect this index value to be equal one so we could we could even print this out let's print this out top prediction index there we go so we would have this top prediction index and then to obtain the exact score we are going to simply pick that score from the predictions so here we have the pretz obviously the pretz is from this so we have the pretz and then by specifying top pred index or by simply specifying one what we're saying is we want to get this score when we pass this input into this classifier model and now this score we're talking about here should be what we had right here so should be this value right here so we're computing the partial derivative of the output respect to the feature maps and we're making use of this gradient method to help us do that so let's take this off you see here we have this here there we go we run this and then we obtain this output shape you see takes the exact shape of the feature maps now once we have this here does this gradients the next thing we could do is simply obtain the mean values at every position right here so uh we simply make use of this reduced mean while specifying the axis so let's run this and print out our pulled grads shape there we go you see we have 2048 so basically this vector now where each position is a mean of a single channel remember from here we had 2048 channels and each and every one of them was eight by eight so you should all could say one by eight by eight so we had this for each and every channel and now we've reduced each and every one of this into a single value so this is just a single value this is a single value which we have here which we'll see here essentially and so now we have this 2048 dimensional vector and that's what we've seen here so this is essentially this now the next step will be to take this here and multiply by our feature maps to carry out that multiplication we'll simply go for iron range 2048 so for each and every one of these different positions we have here we are going to take the corresponding last conflayer output which is essentially this feature maps so we take the feature map you see we specify i so if zero then we're taking this feature map of this channel and then multiplying by its corresponding pulled grads which we've seen already to be the mean of each and every channel we have here for this gradients so that's how we obtain this now note that we specify that one is zeroed element because this is one by eight by eight by 2048 and specifying this will leave us with a tensor of shape eight by eight by 2048 so we get rid of this here so that said we now run this and we obtain our last conflayer output so it's going to be obviously the same shape you could see check that out last conflayer output shape and there we go now to obtain the heat map we are going to sum up the values at different positions for all the channels so let's suppose that we have something like this let's say for example three channels then this position see we have this position we'll take this plus this plus this and then we'll have this output then we move to the next position this plus this plus this and then we have this and so on and so forth so that's how we're going to go from this eight by eight by number of channels output or let's say 2048 output to an eight by eight heat map so this what we have here we're going to sum this and specify the axis is a channel axis so we run this already and then now we could visualize our heat map but notice that there's also this really right here so we have the relu before we visualize the heat map there we go here's our heat map which we then resize using open series resize method so you see how we get from this heat map which was eight by eight to this one which is 256 by 256 and the reason why we're doing this is because we want to add this with the image or purpose this on the image such that we have an output which shows us clearly where or better still regions of the image where we have the highest contribution to our output which is that of a happy person so the model pretty that this person is happy and now we know which parts of the image contributed the most to that prediction now try now with a different image and modifying this slightly so that we could get this image here we find that is this part so previously was a small region now is this this zone we see this wrinkles right here we have this zone and this zone which influence the prediction now with this we've just implemented this grad cam method thank you for getting right up to this point and see you in the next section hi there and welcome to this new and exciting session in which we shall be treating or we shall be using this transformer network right here to solve problems in computer vision and more specifically in the task of image classification up on to this point we've seen different convolutional neural networks like the lunette the vgg the resnet the mobile net the efficient net and now we'll be looking at the vision transformers this vision transformers were first developed in this paper entitled and images what 16 by 16 words where they build transformers for image recognition at scale in this section we'll take a deep dive into how this whole architecture year has been constructed and how it works and also how and why transformers perform as well as their convolutional neural network counterparts the very first point you want to note here is the usage of transformers for computer vision tasks has been developed in very recent times you could see here from this date this paper was published the authors say here that while the transformer architecture has become the de facto standard for natural language processing tasks its application to computer vision remains limited in vision attention is either applied in conjunction with the convolutional networks or used to replace certain components of convolutional networks while keeping their overall structure in place we show that this reliance on the convolutional neural networks is not necessary and a pure transformer that's without any convolutional neural networks apply directly to sequences of image touches can perform very well on image classification tasks they even go ahead and tell us that when pretrained on large amount of data and transferred to multiple midsized or small image recognition benchmarks like the image net ciphar the vits that is a vision transformer attains excellent results compared to the state of the art conf nets like the efficient net while requiring substantially fewer computational resources to train now it's possible that you've never heard of this term transformer or maybe you form an electrical engineering background and you've only heard of this when it comes to stepping up and stepping down electric power now we are going to go straight away to explain terms like the transformer or even this attention which was mentioned to better understand the transformer and the role it has to play in this vits architecture right here we would have to get back in time to understand why do we first developed in 2017 this paper entitled attention is all you need was first developed by vaswaniya al and it has turned out to be one of the most influential papers in the modern deep learning era with the development of this transformer architecture right here at the heart of this transformer architecture we have this self attention models and more specifically in this paper they used the scaled up product attention that we could see here but then as we said the whole purpose of the domain in which these kinds of architectures or those kinds of networks were built was for natural language processing but the question is how does this work in natural language processing to understand how and why the attention and also the transformers are used in natural language processing we'll take the following example which is that of translation which we used to already do in with google translate so here we're going to put in i love the weather or could you see the weather today is amazing and we'll translate this to french le temps je d'hui and choir now initially the kinds of deep learning techniques which were used in solving these kinds of problems that is taking us from one language to another where the recurrent neural networks the way this recurrent neural networks work is quite simple so we'll start by putting the text here here we've put out our example from google translate and then we've added this extra blocks right here now these blocks we've seen here are recurrent neural network blocks recurrent neural networks generally reading are enhanced here are one of the first deep learning based models used in natural language processing tasks like the case of translation we have here now the way this works is we have our initial text or we have our source that's english text the weather today is amazing and then we have the target which we want to generate so initially we have this input and this output which we're going to train and later on when we pass in some random input we expect to get a reasonable output now the way we have this or the the way this is structured is such that each and every one of this is called a token so we have this word here which is a token this this is our first token next token this token this token and this other token then these different tokens have been converted into vectors and then been passed in this rnn blocks right here now we carry out some simple computations like multiplication and addition and then some information is been passed in from one block to another hands the term the recurrent neural network now the importance of passing information from one block to another is that this token's computations in this block will depend on this other previous tokens that you could see here so it depends on this depends on this and also depends on this other one and then once we're done with convert or passing this information from one block to another up to this we are then going to take this here we're going to have some information we're going to be passed onto this other rnn block here so this is our encoder block so here we encode the information and then yeah we decode this information so here we have the encoder and the decoder and then again here a similar process is repeated where we have these computations which produce an output here and then we could take this output and feed it in this one to produce this other output and so on and so forth up to this final output but then the problem with this technique or with this method is that first of all if we have a very long text then it may happen that it has becoming difficult for information to flow from this first blocks here to this final blocks and given that even as humans we know the importance of taking into consideration some previous context when trying to carry out attacks like for example translation this kind of problem will lead to very poor results now another problem here is each time we're training we have to pass this information for one block to another sequentially so here we pass all this information sequentially and because this information is passed sequentially it makes it difficult for us to implement parallelization very efficiently and so this makes the training of these kinds of neural networks very difficult now to tackle the issue with longterm dependencies attention networks were developed so right here instead of depending on just this final vector we get or this final output we get from this hidden layer here which has been passed on here to relay this information from the source to the target language what we'll do is for each and every unit we have here each and every recurrent neural network block we have here we are going to take into consideration inputs from each and every block here so this inputs will be taken into consideration so each and every block now you see all of this is passed and then we have this attention layer right here which then processes these inputs from this different source RNN blocks such that the layer that all this attention layer produces an output vector which is now passed as input into this RNN block and so when we have this source and this target we pass in the source then we get all we combine those inputs from each and every RNN block right here pass in this as input into this RNN block get an output in this case is L then we take this output and pass it as an input in here but again once we shift and go and get to this time where or this time frame where we want to get this second output what we'll do is we'll have another tension here which again takes in all these different inputs so we take again all these different inputs here and here and here and here then carry out some computations based on the type of attention we are implementing and then from here we get an output which is passed together with this right here so from here we get this output and then we repeat the same step that's passing in the tone that is taking this output passing in here and then also taking in this inputs from these different RNN blocks and so as you can see here for each and every block we have here it pays attention to each and every input and for this or from this we could even come up with an attention map where we would have this text this text in English the weather today is amazing and then this other side we have the tone or the inquiry app so now after training this kind of model we can be able to see how much attention this pays to each and every input here and then it's biological that this law will pay the most attention to the and then actually here is weather so this will pay more attention to almost attention to weather then we pay most attention to today we pay most attention to east and inquiry app will pay most attention to amazing and if we get to this paper entitled neural machine translation by jointly learning to align and translate that is a famous Badenau et al paper you can see some of this attention maps here let's have this you see some of these attention maps the you see the economic European then the agreement on the European economic area was signed in august 1992 so you see this attention maps here where we see clearly which words attend most to one another so here we have this image which shows exactly what we're describing previously so here you have this inputs and then to get this output y of t you will find that we are going to take in or we're going to attend to each and every input here and then pass this year to obtain our yt now at this point we are going to move on from the attention to selfattention and to better explain the selfattention we'll consider a whole different problem which is that of sentiment analysis so here we want to we have this model we could now take this off we don't make use of this although you should note that we still use selfattention in the translation problems but it will be easier to grasp this concept in the context of sentiment analysis so here what we are having is we we have the weather today is amazing i want to be able to see whether this is a positive or negative statement so now we have this model which takes in inputs like this and then let's draw this model here like this we have this model and then outputs or tells us whether the statement we've made is a positive or a negative statement now here for this selfattention layer we are not going to need this recurrent neural network hidden states anymore in fact what we could do is we could take all this off actually because basically we're having this selfattention model which we'll see in a minute how it works and then what we're passing in here is some vectors so we have this vector we have this other vector we have this vector this one and finally this one now if we combine all this we'll find out we have a sequence length so we have one two three four five suppose our sequence length is five so we have a sequence length by let's say embedding dimension metrics which we which we get from here now let me explain let's suppose that the sequence length is five as we've all seen and then the embedding dimension is let's say three so we have this five by three metrics which we are going to pass into this selfattention layer right here now this embeddings all these vectors which we pass into the selfattention unit are going to be designed in a way that words which look alike are going to be close to each other while words which are opposites are going to be far away from each other now let's since we're working in three dimensions it means we'll have one two three values here one two three and then finally here one two three so let's let's do something like this three dimensions what we'll have is the word happy which in this case can be represented by this vector or this embedding will be or can be plotted out like this and this will be close to a word like smile while a word like sad a word like sad will be far away from this uh two words because they are actually opposites to each other so we have sad and we could have angry right here now for this one year or for this text here we could pick out these two words which are most likely to be very close to each other we could have the right here and we have is somewhere around here and so now getting back to this model we have this five by three input which is passed into our selfattention layer so we could let's let's have this matrix here five by three would have the the word there yeah we'll have its own embedding so we will have some value some value some value suppose that we're working in threedimensional embedding and then whether we'll have its own value its own value its own value today its own value this value this value could you could take say two point three one zero point five negative five one whatever value one year and then you have this and you have this and you have this this is four already and then amazing will have its own so you see that each and every one of this year has its own embedding so this is these are the different words we have here then at this point we'll implement a special type of attention known as a dot product attention where we'll take this here and multiply it by the transpose of a matrix which has the same shape at this matrix here so we'll take this we'll call this the query and then we'll multiply this by the transpose of the key now this key is going to be three by five since it's going to have the same shape as this query now this is our query we'll call this a query and so here now we have this three by five matrix and then this product will give us a five by five matrix now after this after getting this five by five matrix we could pass this to a softmax layer now we've looked at the softmax layer in previous sessions but one thing you should note here is once we have this five by five matrix it produces this attention map similar to what we have seen before where we have this the weather today is amazing to the side and the same again to the side and then words which are most similar to each other in a certain context are going to have the highest values and so if we're in the case where you have uh say let's replace this weather by happy and then we have uh amazing let's uh no let's let's live amazing so if we have the happy today's amazing uh it sure doesn't make sense but let's consider this let's suppose that we have the happy today's amazing then this uh second row of foot column because amazing will be around here so we'll have this value which is going to be relatively higher than all the other surrounding values and this will be because after training the model the attention map values would have been modified such that values or rather words which are similar to one another take higher values while words which are not similar to one another take very small values now from here we have this uh five by five matrix which now when multiplied by another five by three matrix will give us a five by three matrix generally we call this matrix which is multiplied by this attention matrix the value so we have query we have the key and we have the value of this you see that we have this input which got in here which was five by three and now we have a five by three output then this year now we pass through some fully connected layers and then we'll have an output or a fully connected layer with one neuron in this output which will tell us whether an input statement is a positive statement or a negative statement and so as you've seen we've gotten rid completely of the recurrent neural network blocks as now we're just making use of this selfattention blocks to extract information from our inputs now one of the first papers if not the first paper which made use of just the attention and getting rid of the RNNs was this attention is all you need paper and it happens to be one of the most influential papers in modern day deep learning so here in this attention is all you need the paper or the transformer paper to present this new network which you could see just right here and then a single block let's take this off a single block which makes up the transformer model is this multihead attention so as you could see right here we have this single block and then here we have this multihead attention so let's look at this multihead attention this actually the multihead attention here so you have this year which is this whole block and then in this multihead attention you have the skill dot product attention which is this selfattention we just talked about you see we have the query the key and the value so since it's selfattention you'll notice here that we have this input and all these come from the same input so we have this input which is split it up into cure k and v query key and value now this resembles or is analogous to data management systems where data is stored in key value pairs just like say python dictionary so you have data in this key value pairs data start this way and then when you want a particular information you have to pass in a query now when you pass in a query let's change this color when you pass in a query you have a particular key which is selected once the key is selected we now obtain the value which is the data itself and is kind of similar to what we have in here and then from here in this a level of the split note that before the information has been passed into this skill dot product attention we actually pass this cure k and v into some different linear layers and so this means that even though we have the same input they'll end up being projected into three different inputs and so now we have this cure k v we are going to carry out cure k transpose here we have the matmul as we saw already cure times k transpose and then we have this scalene which you can see right here in this formula this attention formula we have cure k t divided by this d k then from here we have softmax of all this and then we multiply by v so let's get back up to this here that's fine now that we have this output you now see that we have this multi head so we we got this we have the softmax we have the matmul where we take this softmax of this multiply by v so that's how we get this recall how we saw that with a example we had previously and then we have this multi head attention now this multi head attention here simply means you take this year as you pass in your information like this you get this cure k and v and then you again pass the same information into this block so let's suppose that this is our skill dot product attention block this is skill dot product attention which is right inside here and so once we have this let's let's make that smaller let's suppose that this is what we have here so this year is actually this now to obtain a multi head attention we would have this other one year and then we'll have let's suppose that we have three heads if we have three heads then we would have three of this stacked in this way you have one two let's change the color so it becomes clear we have this one in red we have this next one in blue here and then we have this other one in green so there we go we have this three and then when the information gets in so you have your cure you have your k and you have your v we pass this to uh this separate linear layers see for for each of this we have some linear layer here all of this came from the same inputs as you could see here and then now this information is passed here so we have cure k v passed into this one into this block here and then this same cure k v also is passed into let's change the color we will now have some other here some other linear layers let's put it besides this we have some other linear layers here we'll pass v we will pass k we'll pass cure right here and then this now will be set into this selfattention block right here then we'll also finally have this for the red so we'll have something like this red we have the k something like this we have the v something like this so now this cure k v is passed now into this red here and that's it and then the outputs here the outputs will get at the end of this three selfattention blocks will now be concatenated and then pass through a linear layer so this linear layers is like our dense layer in tensor flow now once we have this you see we have our multihead attention which is this block and then now we'll take this input add it onto the output and then go through a layer normalization then from here we pass this through a feed forward network that's like our fully connected network or dense layer and then we'll again repeat this addition and normalization a little similar to what we have with the resnets now once we have this now we can then repeat this n times now you notice that this is similar to this except for the fact that now we have this two multihead attentions and we also have this maxed anyway we're not going to get into all those details what's important is for you to understand how this encoder here works and now we understand how this works we will now get back to our V paper that is this paper entitled transformers for image recognition at scale and now you should be able to understand this transformer block which we presented earlier in this paper and now with this understanding of how this transformer encoder works let's now get into this unit here where we break this image into these different patches as you could see right here to better understand how and why we make use of patches right here let's not forget that what this transformer encoder takes in is some input sequence so we have this input here uh initially we had words where each word like this could be represented by this vector or this embedding vector and then this now combined is passed into the transformer here since our input is this image in others for us to represent it this way we'll have to break this up so what we could do or what we could think of at first sight is we have this image let's suppose the image is 256 by 256 by say three channels then we could take each and every pixel here so let's let's omit the channel for the channels for now so what we could have here is for each and every pixel in this 256 by 256 image we would have a vector representing that pixel and then this other one is vector this other one is vector and so on and so forth but don't forget that unlike previously where we had only five words now we have 256 times 256 words because if we have an image like this and we have to get each and every pixel then we'll have 256 by 256 which is more than 65 000 different vectors which we'll have to pass here and so before we're in our attention model we had an attention map which was five by five recall we saw that already with words we had a five or an input sentence with five words we had five by five attention map now we would have a 65 000 by 65 000 attention map you see that working with these kinds of matrices and memory isn't very feasible and so instead of going pixel by pixel the authors decide to work patch by patch let's create this again so you get to see that take this off you see here we go patch by patch so you could see how this image instead of taking each pixel we bring this up into patches so this is now like a pixel and then you see this patch you see this patch this patch this other patch this patch and so on and so forth up to this patch right here now this is what is like the word now here so with images we have to break this up like this and the authors choose to work with 16 by 16 pixel patches so each patch here is 16 by 16 and so given that we have 16 by 16 if we have this patch for example then we would have 256 different pixels for each patch here we have 256 here we have 256 and so on and so forth so unlike with the images or rather unlike with the words where we had five by three so we had five words and each word was represented by the threedimensional vector here each patch is represented by the 256 dimensional vector now this doesn't mean that in nlp we generally work with this this was just done to make it easier for you to understand so getting back to computer vision you see that we have this 256 256 256 and so on and so forth now when working with the transformer we may not want to work with this 256 dimensional vectors maybe we want to work with say 512 dimensional vectors in that case we would have to do this linear projection of the flattened patches such that we leave from this um say let's suppose that we have one two three we have nine patches so the sequence line is nine so we have this input which is nine by 256 and then after going through this linear projection we now get to nine by 512 and this will be the embedding dimension for our transformer in the previous example our embedding dimension was three three so if this this permits us to be to to work flexibly as now we could decide on what size we want for one embedding dimension now that said we have this output you see nine by 512 and then we're ready to pass this into the transformer encoder but just before passing this we would add this position embeddings you see there we have this input you see in this different this color you have them getting in and then we have this position embeddings here is notice zero one two three and up to nine now the way this works or or let's start by first the reason why we even have to do this is because unlike with the conf nets where where the convolutional or the way the convolutional neural networks work is that for computing the feature maps it takes into consideration locality so this means that you see these two portions here um when passed with a conf filter will produce a certain output and so this means that exhaust which belong to a certain or to a small locality like this one will be used to produce the output and this clearly gives an gives the CNNs an upper hand over the transformers as when trying to understand an image the positions of particular pixels actually matter so this means that CNNs already have an inductive bias due to the way they actually work and so to give a helping hand to the transformer network will now need or will need this position embedding which gives this transformer encoder an idea of the location of each and every patch which is passed in but again it should be noted that this will have to be learned automatically by the model now if you notice we have this extra input right here and the reason why we have this extra input is simply because we do not want the situation where after going through this encoder or this transformer encoder right here we pick one of this outputs because we would have outputs here we don't want we don't want to pick one of these outputs to be used for the MLP head or to be used for this fully connected network in this classification unit right here so to avoid this sort of bias where we would be picking one of this the authors add this extra learnable class embedding right here which will be or whose output will be passed into this MLP head and then will be used for classification another important point to note here is the transformer encoder or this visual vision transformers are some sort of hybrid architecture because we may decide not to pass in those image patches directly but instead pass those image patches through a convolutional neural network then get the output embeddings and pass in your directly instead of this image patches it should be noted that the multilayer perceptron contains two fully connected layers with a julu nonlinearity here's the general julu nonlinearity compared to the relu and the elu so you see we have this relu where all values less than zero all values less than zero gives output of zero and all values greater than zero give the exact same value but with the julu we have this curved function right here so that's it the type of normalization is the layer normalization as we mentioned already and the layer normalization here we could visualize this in this paper by Sheng et al entitled power norm or thinking batch normalization in transformers where you see we let's zoom this you see we have layer normalization here and we have the batch normalization put side by side so over the layer normalization as we were saying if you consider some inputs let's let's here we have the sequence length or the sequence dimension we have the features or the embeddings or like a vector actually so we have the different vectors and then we have the batch dimension so basically what we're saying is we have in this sequence length or we have these different vectors here which have been passed into some layer and then instead of doing or carrying out normalization for for throughout the batches as is in the case of the batch norm here we're carrying out this normalization for each and every vector and the reason why we do not use the batch norm with the transformers is the fact that the batch statistics for nlp data have a very large variance throughout training and this variance exists in the corresponding ingredients as well and so to avoid this kind of situation it's preferable for us to carry out this normalization on the features instead before we move on to the experiments let's look at how the vids are being used in real world so actually the vids are pretrained on very large these sets and finetuned to smaller downstream tasks obviously when finetuning we remove this head and replace with a head which now correspond to our number of classes so this means that initially we may have a thousand class head and then we move this to k classes or let's say three class head to better understand why we're going from while we have a d by k output let's get back here so after those inputs have been passed in here we have an output sequence length plus one this plus is one year or let's just say we have here we have say from here a one by d output if if we're considering all the sequence length will be a sequence length by d output this deer is our embedding dimension which we had fixed from this linear projection right here so we have this one by d and then we pass this through obviously it becomes it becomes like simply uh d neurons so we have one two we now have d neurons since it's just one by d and then we have this output let's say we have a thousand classes then we'll have this fully connected layer which brings all this year this d inputs to this k outputs or in this case to this 1000 outputs now where we want to fine tune we're going to take this off take this off and replace this now with k outputs so we now have k outputs right here and then we initialize this weight of this fully connected layer the others also make mention of the fact that during fine tuning is better to work at higher resolutions so this means that the model could be trained at 256 by 256 and then later on fine tuned with 512 by 512 images and then since they keep the patch size the same this results in a larger effective sequence length now let's explain or let's visualize this statement so here we have this input which is say 48 by 48 let's say we have your 48 by 48 and when we divide this our break this up into three parts we have 16 16 16 16 16 16 so we have 16 by 16 patches now if we want to fine tune on a higher resolution image then uh let's say the higher resolution image is say 96 by 96 so could have something like this so if now we're fine tune on the 96 by 96 image and that we still maintain the fact that this year or the patches will be 16 by 16 then this means that instead of three year we're going to have six so we now have or one two three four five and six patches six patches this way two three four five six and so on and so forth so now we're going to have 36 different patches instead of nine patches as we have here and that's why they make me have the fact that the sequence length is going to be increased and that's so long as they can fit in the memory now due to this modifications the pretrained that's what we had before the pretrained position embeddings may no longer be meaningful so they therefore perform 2d interpolation of the pretrained position embeddings according to the allocation in the original image the experiments here we we could see those different models they have the vid base the vid large and the vid huge number of parameters 86 million to 632 million then here we have 12 layers recall let's get back here recall we have this number of layers here so basically you're repeating this you're repeating this year 12 times so we'll get back so there we have 12 layers for the vid base as we stated here and then we have 24 for vid large and 32 for vid huge then this hidden size this d this embedding dimension is 768 for base 1024 for large and 1284 huge the mlp size that's uh fully connected layers uh to 3072 4096 here 5120 now the number of heads remember the attention heads 12 16 16 the experiments were carried out on this gft 300 median data set i will see how this 14 by 14 patch version of the vid outperforms this resnet 152 now this uh performance although not largely uh greater than that of the resnets requires less computation resources to train as we see here we have 2500 tpu core days required to train this model as compared to this one which requires 9900 tpu core days also from the plots you see that when you increase the number of pretraining samples the model which performs the best is this vid right here see here we have this vid and this outperforms the resnets whereas uh for a reduced number of samples the resnets outperforms the vids while here the smaller the patch size like here we have this fortune fortune by fortune patch size we have the better the results now in order to understand the reason why as you increase this uh data set size the vids start to outperform the conf nets we have to recall that when working with confidence like the resnet there is some inductive bias in the sense that the fact that this resnet takes as input this twodimensional image already gives this conf net a helping hand when it comes to extracting features from here and so even with relatively smaller data sets these confidence can make sense out of this input image now with the transformers which are some sort of generic neural network the model doesn't get to see the image in this its natural form what it sees is some patches which have been converted to some vectors and so at the very beginning or with small data the transformer model finds it difficult to make much sense out of these patches but as soon as we increase this data set to considerable amounts this transformer model now free of the inductive bias can even do better than the confidence and interesting enough you'll notice that after training a transformer model this position embeddings we call the position embeddings which are added onto the patch embeddings before passing to the transformer actually learn on their own to encode the position of the patches you can see from this uh plot here where we have the input patch row and the input patch column you see that this one one you see the position this is gotten by the model or this is learned automatically by the model during the training process you see two one it goes a step in the direction and maintains this direction or maintains a row and you see three one maintains a row that goes three steps you see this you see you see that and then finally here you go you go several steps to the right and then seven steps uh downward then here to the left you could look at this you see this um this embedding filters right here we have this embedding filters which we see here which look much alike to the the conf net filters then to the right you have this plot which summarizes the reason why the vids end up being more powerful than the confidence to understand this let's take this here who consider a conf net with a given depth now with a conf net the initial layers let's let's let's have this conf net and we break this up so we break this up we have our initial layers and then we have our final layers this initial layers permit us um extract low level features while the final layers permit us extract high level features and so if we have an image like this like this one and we have this head and we have this then given that we're passing these filters or this conf net filters here you see that this pixel for example attends to this other pixels which are found around its locality and then as we go deeper in the network we would have this pixel here which now tries to attend to this other pixel here which is much more far away from it to better picture this remember the example we took for three or rather two three by three filters compared to a single five by five filter let's let's let's draw this here we compared this with a single five by five filter and we saw that although this five by five filter had a larger receptive field compared to a single three by three filter which we have here making or stacking up this three by three filters that is making our network deeper permitted us to still be able to capture this part of the image and so this shows us that with the confidence in the earlier layers when the when is not yet deep enough we still capturing this local information and then as we go deeper we start capturing much more global information and so if we're to have this kind of plot here where this we have mean attention distance and here we have the network that would see that for a conf net will keep increasing this up to a point where we may we will no longer be able to continue increasing because as this this network depth or this will increase the number of layers we are able to capture much more global features and so this mean attention distance keeps increasing but with the attention or with the transformers since each patch attends to each and every other patch as we have seen already with the selfattention each and every patch will attend to the other right from the very first attention layer we are not going to have this but instead this plot we have here and so this means that if we train our VIT with a very large data set right from the very first layers we are able to capture both the local and the global features and this is what makes the VITs more powerful compared to the confidence when we work with big data here we can also visualize what the model sees by looking at this attention maps you will notice that after training the model you see we have this attention here see here these are pixels which pay much attention to one another see here these pixels are paying attention or much more attention to one another as compared to the other pixels in summary to understand or to visualize what goes on when training to CNN and VIT model side by side you'll see here that with the VITs as you increase this data site this increase this site and this is increased data size here so as we increase the the data size or rather when we start with small data sizes we have this kind of accuracy while for the CNNs we already have reasonable accuracies even with small data size and then as we keep increasing this data size as we keep increasing this data size you see this accuracy keeps increasing while for the CNNs VITs start to plateau at some point and this plateauing is simply comes due to the fact that this CNNs here are limited by the inductive biases whereas these transformers which are more generic neural networks are free to learn even better from these larger data sets hello everyone and welcome to this new and exciting session in which we shall be building our own VIT model from scratch in the previous section we saw how this transformer model which previously was used for NLP tasks could be used in computer vision with proper preparations and so in the section we'll see how to convert or create patches of this image and then carry out those linear projections and pass this output from those linear projections into the transformer encoder to then create or to then train an endtoend model which learns how to see whether an input that image is that of an angry person a happy person or sad given that we've been working with 256 by 256 images if we have to split up those images into 16 by 16 images then we would have 256 different images because here if you have 256 year and then uh break this up year into 16 by 16 so here you have 16 pixels by 16 pixels then you would have to go you have to do this 16 times horizontally and 16 times vertically to form a 256 by 256 image and so um from here you would have 256 different patches like this one and so what we'll use to create these patches will be this extract patches method right here now this extract patches takes in the image it takes in the sizes takes in the stripes and the rates with also the pattern so let's look at how this works from here we have this picture here or we have this output which will help us picture how this works but before looking at that let's look at its arguments we have the sizes or we have the images for the tensor we have this sizes which specifies the patch sizes so in our case we have 16 by 16 patches so our patch size will be 16 we also have the stripes which tells us how much we should shift while creating these patches and then we have the rates which will better understand with some figure so let's let's now get back to this year and then we see that we've been we here we pass in the sizes of one three three one so this list we pass it in here we got this from here so you see we told it must be this list where we have one the size row the size column and one but since the size row equals size column equals 16 then here we would have 16 by 16 now in the case where we have three three it you you could see here let's let's have this here you could see here let's take this off you could see here that we have this three by three so here unlike with our example where we're working with 16 by 16 pixels here they have three by three pixels and then the next one you have the stripes five by five now you will notice that let's scroll this way you will notice that when this box or when we want to get the next patch we shift or we go five steps to the right and then get this next position or this next patch right here so you can see after shifting we get to this so originally we had this image here and then we're trying to get these different patches now the patches are what we have in stars we can see and then again we shift five steps downward we go downward again and then we get to this you will see that if you can count this one two three four five steps and then we get to this and then we shift again and so on and so forth so that's how we obtain all this three this four patches right here now the next thing we will look at will be this rates now the rates again you have this one and this one and then you specify this two rate values here now we'll look at or we understand these two rate values by looking at a dilation or by looking at a dilated convolution operation we can see this visualization from this medium posed by Sig Ho Tsang where unlike with the usual convolutions where when we have in this filter which passes through the image you see we have this three by three filter which is compact here so there's no spacing between each element of the filter but with a dilated convolution you see the spacing here so it's kind of similar to what we will have here as let's get back to documentation as with this if we decide to have patches where there is some spacing between the pixels then we could specify how much paste we want with this rates right here now the pattern here is set to valid and so this means that if some pattern needs to be done to match our with the extraction then it will be done automatically now that said we could copy this out copy this simply and then we paste it out right here so here we have this and then we'll call this patches so we have this patches and then we'll pass in our test image right here there we go we have test image that's fine and then here we have 16 by 16 now this is the patch size so we could do well to put that in the configuration so we could we could add that to the configuration and the strident 2 is 16 because we want to let's get back here the strident let's see why we need the strident to be 16 now you see that we're interested if we have this kind of image we're interested in we are not interested in skipping this part so we don't want to skip any parts we want to have this then we want to have this next you see this what we want to do then we want to have this next and this next obviously there'll be padding year and year so account for this space here and then we want to have this next and so on and so forth so because we want to have this we'll make this value to be the same as this such that when we have this three by three patch we skip three steps and move to this next and so on and so forth so that said if you want to have this kind of compact patch extraction then you want to have this this the value here for the stride the same as the size so with that now let's configuration patch size we have that the 16 by 16 that's fine and then now let's run this year let's also run let's run this year this image test image then once that's fine we run this we get this error let's check out it must be four dimensional okay so what we'll do is instead of taking this test image we'll do expand deems so we create this extra dimension and then we add this to the zeroed axis so we add this extra dimension run that again you see we have our patches now look at what we're gonna have when we print out the patches shape patches that shape what do we get see we have this 16 by 16 by 768 now let's explain why we have this now recall that we have a 256 by 256 by three image meaning that we're going to have three channels like this 256 by 256 then by three now since we're dealing with patches i think it's preferable we should take this at once so this is 16 by 16 patch 16 by 16 patch here we have this 16 by 16 patch and then here again we have this 16 by 16 patch right here so now we have this three 16 by 16 patches and then given that each and every one of those patches is 16 by 16 and 16 by 16 is 256 you'll see that if we pick a given patch like here it's a 16 by if we pick a 16 by 16 patch then this third dimension will be 768 simply because for each patch we have 256 pixels per channel so here we have 16 by 16 256 pixels which make up this patch now this other one 256 and this one 256 and now if you sum this up it gives you 768 and that's how this value is gotten now to plug this out we are going to go through each and every patch so we'll take this both vertically and horizontally create the subplots 16 by 16 because we'll have 16 by 16 different subplots and then for each subplot we have this image is here we pick ij we pick a given patch out of this 256 patches we pick that patch and then we're going to reshape this because when you pick this patch you're left with this 768 here when you pick the patch you're left with these pixels which have all been flattened out to this 768 dimensional vector that we calculated here and so now what we need to do is reshape this back into a 16 by 16 by 3 pixel obviously 16 by 16 by 3 will give you 768 and so that's it let's run this now you can always increase this figure size or reduce it depending on how you want to view this so running this this what we get is output there we go we see this image which initially was compact now we've breaking this up into several patches now before we go on to create our patch encoder which is this whole section right here what we'll have to do is convert or reshape these patches here so recall that in this paper after doing this year that's after creating these patches we have to take each and every one of them and this will be considered as one element of the sequence so here we have nine patches and here we have nine values which have been passed or nine different inputs or vectors which will be passed here and so if we get back here since we have 256 of this then we'll need to reshape this such that each and every one of this will be considered as a part of our whole sequence so here would print out let's print out let's let's reshape that so we have patches equals reshape and then we have the patches and then this will give us patches that's a batch dimension patches shape zero then the next one with negative one and then here we have 768 now either we put negative one or we explicitly write 256 so this means that we have decided to put this now into uh 256 uh sequence length tensor now with this let's print out the patches shape and you see what that gives us see here we have 256 now if we put negative one year it's going to automatically give us uh this 256 and that's it so we we have this oh let's let's rerun this so we get that difference so you see here 216 by 16 and then 256 and that's it so each and every one of those 256 now will be passed into our transformer now we could modify this uh print the this plot of the image so let's let's have that and then we'll say we have uh patches shape uh one so we'll take 256 and then yeah we have system by 16 and then we have simply i plus one okay so we have that we could take off this k now we don't need this again take that off and then here we have this i here which will now reshape into this and this should be fine so there we go let's run this again and that's it we get the exact same output which is normal since we just uh restructured those inputs so with this we see that we are now set to create our patch encoder layer and this patch encoder layer will be similar to the kinds of layers which we've been creating so we could just copy this here reduce that and then paste it out here so we'll call this our patch encoder layer there we go we have patch encoder and your patch encoder so this patch encoder will be responsible for first of all converting our image into patches and then um carrying out the projection and adding the positional encoding patch encoder that's it and now here we have this linear projection which we would um create here so let's call this linear linear projection and we could do this with our dense layer so we have this dense layer stick that off and then yes we could select our hidden size dimension or embedding dimension now since our input is already let's uh write this out here we have inputs of uh let's take this off uh batch dimension or let's let's take the batch dimension off we have inputs of 256 by 768 already so this is our hidden size dimension for now but what we could do is we could convert this into 512 so let's do just that uh we could let's no let's let's take 1024 let's make the model bigger but this is too many parameters already it will take much time to train so let's let's let's stay with this okay so we stick with that and uh let's get back to the code we're going to specify our embedding dimension uh let's say hidden size as is in the paper so our hidden size we specify the hidden size and that's it we're going to pass this in here we have hidden size there we go and we do not need this batch norm we have the linear projection we have this hidden size okay let's leave it this way for now so uh we have this call meta we should take this input and that will be it we'll have this calculation we should saw already here so we would have this patches let's copy this uh from here okay so we we get the input we get this input now this input we'll be getting here we wouldn't need to do this expandings because the when training we already have the batch dimension so we'll just have x let's call this in no let's let's let's have that as x okay so we have that x and that's fine then we'll do the reshaping that's it and then from this reshaping we'll have our output uh output will be this uh patch which has been projected to this new dimension and so here we would have output and self linear projection which takes in the patches then to make this value dynamic we're going to have let's take this off we're going to have the patches shape negative one so this value we have here is the last dimension of all patches so we have that and we get that last dimension now that we have this set we now ready to add up this positional embeddings we have here so we will add this different positional embeddings here onto our linearly projected patches we should also note that we are not going to take into consideration this class embeddings which I mentioned in the paper as in practice this is not really important we have here Lucas Beyer from the Google brain team who says the main aim of including this was to reproduce the exact transformer network but on image patches and so in practice we are going to stick to this linear projections the positional embedding now will be constructed using TensorFlow's embedding layer so here we have positional embedding as described in the paper embedding on encoding and then here we have the embedding layer now let's check out on how this embedding layer is constructed or first of all what it's about now this embedding layer here as described Tfkrel's layers embedding turns positive integers into dense vectors of fixed size now getting back to the paper at this point you see we have this linear projections here we have this linear projections which need to be added up to the positioning coding which is this once now the linear projection we're going to get from here if we take if we include the batch dimension we'll have an output batch size by number of patches so let's call this np number of patches because obviously we have these patches here and each patch is going to be a vector so it's number of patches by that last dimension which in this case is 768 so you could always fix this hidden dimension this all hidden dimension let's say batch by np by hidden dimension and then here let's take this off let's take a batch of one so if we take a batch of one we would have one for the batch dimension then the number of patches would have will be 256 because if we have 256 by 256 image and we break it up into 16 by 16 pixel patches then we'll have 256 different patches so here's what we have for this one and now with the embedding layer what we'll be able to get will be another tensor which is like this in the output of that embedding layer and so as we can see here it takes this indices and then turns them into the dense vectors which we're interested in now the arguments we have here the input dimension output dimension initializes regularizes as we can see here and then we can see this example where it converts for example this number four into this 2d vector this index 20 into this other 2d vector and then from this example which we could copy out and test in our code let's test this out let's add this here so we have this which we could test out so you better you see better how it works you see the model takes us input this integer matrix by dimension by input length and then gives this other output now in our case since we're working with image data we do not really have this vocabulary we should talk about here and so instead of this vocabulary we replace this by the number of patches now if this were a natural language processing then this will be the total number of words that our model can treat and so if we could have a vocabulary of say 300 000 words and that's what we're going to pass in as this input dimension but as we said here we're going to take this to be our number of patches now this input dimension is as I say here is a dimension of the dense embedding so in this case they they put in the value 64 which we will change to 768 shortly now that said we will have this model which you define take that off we have this input you see the size of this input is 32 by 10 input there's a batch dimension and here are the different indices so we have this input and then we're going to print out the shape from here so let's run this there we go you see it takes in this and then outputs this one now as a vector so basically what we have is we could have let's take this off what we have is we have some indices let's take this off we have some indices let's say we have like in this case they have 10 indices so we have this this this this seven eight nine ten okay so here we have this 10 indices and then oh we're not a batch dimension so we just have this 10 indices here so this 10 indices now could be projected into its twodimensional version where this one will be represented by a vector so just like the patches so you have a patch represented by this vector this one presented by this vector so this is no longer represented by an index say let's say because here this random values take value between zero and one thousand so it's no more a value like say 900 so this maybe was 900 now will be converted into a vector and the the size of this vector we see here will depend on this value we set here so you see this this means that this this this one year now becomes a 64 dimensional vector and so each and every one of this becomes 64 dimensional vectors and that's why we go from this 10 to now 10 by 64 which you could see here if we ignore the batch dimension so what we'll be doing in our case now let's take this off what we're doing in our case is we have this 768 and then here we have 256 and then the size of this input which is going to be some random input we take the size to be 32 by 256 since we have 256 patches so we have that 256 let's take this we could take this off you could check out the the reason why we need the input length documentation but for now we don't need that so we'll have 32 by 256 and then we expect to have an output which is 32 by 256 now by 768 let's modify this let's take one by size of one so let's run this which we have 1 by 256 by 768 and you see that it would match now with this year with this year from the projections or from the predictions of the inputs now that's set let's simply copy this year we have this layer which we're going to copy and add that year so instead of this embeddings we have this embeddings there we go and then here we'll pass in the number of patches so here we have number of patches and then this one is this hidden size so here again we have our hidden size and that's it so now we have the hidden size we get here and then instead of having this we have this plus the positional encodings so we have self dot positional embedding we could call that embedding and then what this will take in will be an input of length number of patches so we get back here and then we would have embedding embedding input okay use the tf range and then we have the start we could start from zero this year the indices so we have this and then we have the limit our limit is going to be the number of patches because we want to get the this indices going from the zero value to some n value such that the length of this tensor we're going to create is going to be such that is equal the number of patches so our limit year is number of patches in that sense we could have this year so we have number of patches already from here so we could get self number of patches there we go we have number of number of patches okay so we have our number of patches that looks fine and then we could make use of that year so we have that number of patches and then we are going to set our delta so we have delta will take to be one now obviously we set delta to be two it means you're going to go from zero to number of patches why it's keeping two steps and that's not what we want so we we want number of patches elements in this embedding inputs so with this now we could pass an embedding embedding input and that should be fine okay so we have this year looks fine and we'll take this off so now we return we could return our output simply now it should be noted that this embedding layer here is similar to the dense layer but with a dense layer when you have an input x you see with a dense layer when you have an input let's write it here for the dense layer when you have an input x that input x is multiplied by the weights and you add the bias so this is how we get the output but with this embedding layer is a simple metric small application so when you take an input x you multiply it by the weights and you get the output with that now let's run this cell here and then we move ahead so we have this run and we could delete this two cells now now to test this out we could define patchank we have our patch encoder patch encoder which we've just created here so a patch encoder and this takes into the number of patches and hidden size so let's specify those 256 and 768 so we have this and then now we could have patch encoder and then pass in an image and see what we get as output here's image we have the zeros there we go 1 by 256 by 256 by 3 so now when we pass an input image we expect to get some output of shape batch size by number of patches by the hidden size or the number of hidden units and now that we've built this year up to the point where we have our embedded patches let's go ahead and build this transformer encoder right here as you can see we start with a layer normalization multihead attention and then we add this this input to this output we have the layer normalization again then the multilayer perceptron and again we have this addition and then we get the output now to build the code for our transformer encoder we're going to paste out this patch encoder here we remove this we have transformer uh there we go and then this transformer is made of a first norm layer and then the second norm layer let's have this layer layer norm one it's our first norm layer layer normalization there we go that's fine and then we have our second layer normalization layer so call this layer norm 2 now we've defined our layer normalization layers we can go ahead to define our multihead attention layer so here we have multihead attention and there we go again your multihead attention you could check out the documentation for this so in documentation you have the different arguments which we could pass in the type of values which the multihead attention takes in so here uh we will specify number of heads and the key dimension here this key dimension is the same as the hidden size or the number of hidden units which in our case we fixed at 768 we could also have the value dimension drop out and this other arguments now what this takes in as you would see here is a query value key and attention mass which we will not use here return attention scores will not do this this training will not specify this but what we'll specify will be this query and this value and then since we are not going to specify the key it's going to consider that the key is the or rather it's going to consider that this value here is a key so it'll be the same value that said here we would have the number of heads let's take this off multihead attention number of heads and then we'll also need the hidden size uh yeah so we fully have the hidden size so we'll leave that away it will replace number of patches by number of heads okay so we have our multihead attention number of heads hidden size and then the next will be the dense layers so uh as you saw in this model here you see we have after the multihead attention we have layer norm 2 and then we have this MLP here which is made of two dense layers so we have our self dense one which is our dense layer and then the number of units here will take it to be hidden size and then we'll specify the activation to be a JLU activation as specified in the paper so we have JLU and then we'll repeat the same process for the second dense layer so here we have that and dense 2 okay so that's it we will specify those two dense layers and we now get our output so we should have that and then now we could go ahead and call so let's before going to that let's modify this here transform my encoder that's fine the name transform my encoder there we go that should be fine okay so we building our transform my encoder layer already and now we set to pass this input into these different layers we have our input here x there we go we have x let's take this off and then x gets into the layer norm 1 so we have our output x x equals layer norm 1 which takes in this x here takes our input x and then our output is this we could call this in and then we have this input input we have our input which gets in here and then produces an output x and then from here we get into the multihead attention layer so we have x again this is multihead attention and this takes in x so now we have this input which gets here gives gives us output this output gets into this multihead and gives us output now that we've had this remember from the paper that we have this addition to do so we have this add layer which we need to specify here because after this output got into the multihead we got this output which now needs to be added to this input from the patches so we need to create this link here so that said getting back to the code we have x now which is add and this add will take in this x and the input then from here again we take this and pass into our layer norm so now we'll call this let's let's call this x1 because we'll need this x1 again so we can't just be working with x you understand shortly why we need this x1 so we have this and we have your x1 okay so we have that and that's fine everything looks fine then from here we take this and pass or take this output x1 and pass into our layer norm 2 so we call this layer norm 2 that's it and then this takes in our x1 produces output x1 then from here we take this and pass into our dense layers so we have this x1 which is equal dense 1 and this takes in x1 and again we have x1 which takes in dense 2 sorry which takes x1 which is gotten from dense 2 which takes in x1 and now let's say this produces an output x2 remember this like our final output so let's call this output let's let's call this output okay so that's our output right there now let's do this then for this output again we're going to do this addition you call from the paper you see after doing this addition you get an output we get an output that we take this output from here and add onto this that's why you see we had to create x1 for this and then x2 for this so let's get back to our code we have that and then here we have our output now which is equal add that's it and then it takes in our output and this x1 here or rather this x1 after this addition so we need to have this as x2 so let's change this to x2 so we could make use of this now in this addition because if we if we have this if we maintain this to be x1 then the value we're getting here would be this final x1 we got here and that's not what we want what we want is this output from here so that's why we had to change this variable name so we have x2 this takes in x1 then that's it so from here now we take this output and add it to this x1 there we go we have that x1 we get now our output so we could close this up and that's fine so we return our output and we have our transformer encoder right here now we can go ahead and test this so we have our transformer encoder number of heads hidden size that's fine and then we also have this input right here recall that the input or the patches take this form so let's run this and see what we get we'll get an error from this yeah this is logical because we need to pass in at least two inputs so here if you recall from the from this year from multihead attention you see this we have this two which must be passed in then this key is optional this one optional and all this rest optional so we must pass at least this two so that said let's get back and modify that so in this multihead attention we have this and then we have x1 so that's fine so now we have that let's run this again and see what we get you see that's fine you see we get exactly what we expect so there's a kind of input we get and here's our output after going through the transformer encoder now although this works make sure that you check your work or check this code and be sure that everything is working as it's supposed to be so be sure that you pass in the right inputs and get the right outputs and so that you get the exact output you ought to get and not some outputs different from what you're what you're supposed to have so that's it we have our transformer encoder and now we'll head on to building our VIT model so here we would have our VIT model let's get back up here let's copy this rest net from your complete network we had this so just simply copy the same structure and then paste it out here now although there's not much difference we'll just the main difference here is that we will have this model instead of layer so here we have a model and then here we will call this VIT so here's our VIT model our vision transformer model and then here we change this we have the vision transformer and we'll call this our vision transformer vision transformer does it okay so there we go we could take this off take this off and all this off then we're building in such a way that it takes in the number of heads the hidden size then from this patch encoder the number of patches so we just need number of heads hidden size and number of patches so with that would specify number of heads there we go hidden size and then number of patches okay so we have that and then here we would have our patch encoder which will be defined so we have patch encoder and this patch encoder which we have in right here is simply what we created already so we just have patch encoder and what does this take so we get back here and we look at this format see number of patches and hidden size so that's why it's important for you to test this as you go on so since we've tested this we show that this works when you pass in an input image so your patch encoder number of heads and hidden size alright a number of patches and hidden size so we have number of patches then the hidden size which we'll get from here now we have this patch encoder we have our transformer encoders recall that we have several transformers here see we have l transformer encoders so we get back here and then we define this transformer uh let's call it trans encoders okay so this transformer encoders will be at least made up different layers and the length of this list will depend on the number of layers so we have your number of layers and then what we'll do is we'll say for underscore in this number of layers or rather in range number of layers see we are then going to define the transformer encoder so we have transformer encoder so we have the separate transformer encoders and then we'll specify number of heads and hidden size so here again we have number of heads and then we have the hidden size the input gets through this patch encoder so here we have our patch encoder that is it this way we have x let's call this input so here we have input and this passes through the patch our patch encoder and then produces the output x then once we have this output x what we are going to do is we are going to loop through or we're going to go through each and every transformer encoder layer so here we have for i in range of self that number of layers number of layers let's define this here number of layers equal this number of layers from here so set our number of layers and then we take this in here so now we're going to go through this and then what we'll be doing is we'll be having this x which is going to be the output from this transformer encoder layers so we have trans encoders i which takes in the different inputs x so we'll do this for a number of layers times and then for each of this we have trans encoders as we've defined already here and we pick the specific transformer encoder now once we get the output from here see once we get our output from here we'll now obtain an output which will flatten so from here we'll get we'll say x equal we'll flatten this out take an x and then from this we'll have this MLP block right here oh let's get back here oh we have MLP block here okay so we have this MLP block right here that's after this now on the paper what they had done was since they had this class token they picked this last or rather they picked this first output here so this means that if we have let's take from here if we have an output by size one let's take this off one by 256 by 768 then out of this 256 here or 257 because they'll have this additional so out of this 257 we'll pick out only one from here and so we'll be left with that one which has 768 heating units so basically this will be like having a one by 768 output but now since we're taking all this into consideration we'll just flatten this out and then pass this through our MLP head right here so that's it let's get back to the code we flatten this out and then we we we specify the dense layers which make up the MLP head so we have dense one dense layer this dense layer has certain number of units let's say a thousand or let's let's let's give this dense units here let's let's have it as this argument so we have number of dense units okay so we have our number of dense units there uh let's specify this number of dense units and then we have the JellO activation CF and JellO okay so we have that for dense one and now we should have or we should write this code for dense two so we have now dense two and then here same number of dense units you could always modify this and that's it next from here we have x equal dense one that's it dense one this is self okay so here we have dense one which takes an x and then dense two which takes an x now our final this our final output dense layer has to consider the number of classes so just as what we have been doing so far in this course you let's take this one this complete block here so so far what we've been doing is we always ensure that our output dense layer has the number of classes number of units in this output so here we would simply copy this scroll down back to our code and put this right here so that's it let's call this dense three dense and that should be fine okay so here we have dense take this off that's fine we have that now let's run this should be fine let's go ahead and test this so here we have our VIT model and we'll define VIT because of the IT and the parameters those arguments here a number of heads hidden size all of this we're going to define this just down here scroll down and then define this here okay so we have now this number of heads uh we say eight heads hidden size 768 uh number of patches 256 the number of layers let's say four layers the number of dense units let's say 1024 okay so there we go we have our VIT and then this VIT now from the we pass in this um input to 1 by 256 by 256 by 3 and we should get a reasonable output we could print out the summary so VIT summary and we could check out this model so you see here we have 283 million different parameters for VIT model now let's reduce this we could say four heads just two layers and then this number of dense units here we could take 128 now let's run this again and there we go you see the two layers you see the patch encoder and then you see the dense layers we should follow this uh transformer encoder layers so now we have this model we'll change this to the batch size so we have configuration and then batch size now the reason why we're doing this is because when we're training our batch size is going to be known so we don't want to have this uh here like that so with that now we run this again and run this summary uh yeah we need to change this to 32 you see when we change this it doesn't work so here we have 32 same as our batch size that's it and now we can go ahead to compile and train our model our model on our training note that you we wouldn't get the best results because obviously the VITs need very large data sets or even extra large data sets to perform as well as a confidence and so when working with the VITs generally we want to train on a very very large data set and then later on fine tune on the smaller data set towards the end of the epoch we still get this error and the source of this error is the fact that since we've fixed the batch size that's uh if you get backed up here in our training you see if you look at this here we fixed this and so since this isn't dynamic now we have a data set which has been broken down into batches of 32 and now towards the end you may have a batch of say for example eight obviously because the data set the data set isn't necessarily divisible by 32 that is the number of elements you have isn't necessarily divisible by 32 and so you have a remainder and so when you have this remainder and that you fixed this here you should get an error because now you've told the model to always use at this position a value of 32 so to avoid that instead of doing the patches shape we're going to do tf.shape so with tf.shape before we had patches we got the shape and that was it but now we instead using tf.shape so we call it the shape method from tensorflow and then here we're going to pass in our patches so once we have those uh once we have our patches passed in we now select this batch dimension and that should be it so let's run this again you would see that even from here we could modify this we could put 32 and it'll still work fine okay let's see too so with that now let's go ahead and compile and then restart the training the model is training and as you could see it starts to stagnate around this 44.4 percent accuracy and 44.16 percent validation accuracy in the next section we'll fine tune a vid which has been trained on a very large dataset hello everyone and welcome to this new and exciting session in which we are going to be looking at how to fine tune an already trained transformer more precisely vision transformer model using the hugging face library and tensorflow tool hugging face today is at the forefront of practical AI and it permits practitioners around the world to build train and deploy stateoftheart models very easily it's also used by thousands of organizations and teams around the world so here we have a host of different tasks which we could solve with readily available hugging face models like audio classification image classification object detection question answering summarization text classification and translation in our specific case we are dealing with image classification and if you click open right here you see we have this uh image classification page where you could already test an image on this vid model here so this is the vid base model with patch size 16 and the image size 224 we also have this facebook's uh DIT base distilled model and with patch size 16 image size 224 you could browse a host of other image classification models here as you can see here we have these different models sorted in out of a number of downloads see 219 000 times this model was downloaded now we'll be working with this vid oh we're fine tuning this vid by google right here here we have the model description so you could have that we've seen this already in the paper um the intent that uses and limitations and then how to use this model without any fine tuning so here you get passing your image and then um already run classification on this image using this vid model right here vid for image classification model now here you will notice that this is uh we suppose that we dealing with a pytorch model so we could check in the documentation here where we would see this vid model on the left side together with many other different models which are available for free and you see here we have this vid you you could also check out the DIT the distillation transformers let's get to D we should have DIT around this there we go here's the DIT you see you have the DIT and this documentation right here so uh we also have the swing we should have seen we looked at the swing previously so if we check out the swing somewhere here uh swing s where we are swc you have the swing transformer right here so that's it see the swing transformer we have seen already now let's get back to our vid vision transformer okay so we have this vision transformer right here and you could uh here go through the the whole documentation so we have the vid config vid feature extractor let's analyze this so it becomes clearer there we go we have this feature extractor the vid model vid for masked image modeling vid for image classification and you'll notice here we have tf vid model so the difference with this is this is pytouch the model and then this is the tensorflow model which we'll be using now the the tf vid model and tf vid for image classification now recall that you have your vid model which starts from the patch you have the patch once you have the patch you have the transformer encoder and then from here you have you take this you have this mlp head and then you have your output right here now with this tf vid for image classification we have all this full year so we go from this patch right up to the mlp head but with the tf vid model this part this part is not included so what you get will be only this output right here so with a tf vid model we just we have this whereas with the tf vid for image classification we have all this now apart from tensorflow we also have the code for flex so you could check out flex vid model flex vid for image classification now that said we are now going to focus on fine tuning this vision transformer model we'll start by first installing this transformers library right here so we have pip install transformers now that's fine we move on let's go ahead to start with the fine tuning but then let's get back to documentation and here we have the overview we could check out those vid config right here and you'll notice that all those different configurations are basically what we've seen already so we have this hidden size 768 as default value number of hidden layers 12 so here the stack in 12 different transformer encoded blocks number of attention heads 12 intermediate size 3072 now for this 3072 actually is for the dense layers we have here now the input you get in from this normal layer or right from here we have say 1 by 256 by 768 then this gets in and gets to this point where we still have the same so up to here is the same and we expect that this output here should be of the same shape but then in this mlp layer we have two dense layers now the first dense layer will convert this to 1 by 256 by 3072 and then the next dense layer would convert this to 1 by 256 by 768 so that's why they call this the intermediate size right here so this intermediate size you can see dimensionality of the intermediate feed forward layer in the transformer encoder we have the hidden activation JLU hidden dropout probability so you have some dropout here attention probabilities dropout attention props dropout probability initializer range layer norm epsilon value to better understand this we could get back to the layer norm documentation in tensor flow you see epsilon here by default is one times ten to the negative three and then we could see where exactly it's been used so recall that with normalization we have x minus a given mean divided by a standard deviation now we we do not want a situation where this here is zero and then we have an infinite output so we generally add some epsilon right here now this epsilon by default as you see here is one 0.001 and in hogging phase here it is one times ten to negative twelve now it's only encoder so there's no encoder decoder that's why this is set to false image size 224 the patch size 16 number of channels three what are we going to add a bias into the query keys and values here true encoder stride 16 now you can remember the the use of the stride when we're trying to get the patches we once once we have an image like this and then we have a patch size of 16 by 16 would move through 16 pixels to obtain the next patch so that we have no space here so we have something like this actually okay so that's it we understand this config you could check out your this usage of the config so let's copy this code and get it back here there we go get back here and then we have this code paste it out here okay so that's it you see clearly you could easily create a VIT model without necessarily going through all this process which we had done right here so this was just for educational purposes and means that if you want to build your own VIT it's simple you just have to do this specify the configuration that's the config and then here you initialize the model and that's it so let's let's run this and then let's also print out this configuration let's look at that you see there we go we could change this we could change let's say we change this hidden size to some value 144 so here let's have our hidden size so you see this how you could change the hidden size to suit your needs so that's it you change this that's fine you look at that and you see you now have this new configuration and that's it now for the next we'll look at this feature extractor the feature extractor is similar to what we have done already that is taken in the input resizing it and then clearing out some normalization so that's it for the feature extractor you could check in the documentation this VIT model here is a PyTorch model so let's go to tf.vit so this is for tensorflow now so here you see there we go we have our tf.vit model you can expand these parameters and here we have all those different arguments which we could check out now here you have this example of how we could use the tf.vit model directly without going through any stressful process so here you see we have this first of all we have the data data set and then we have this image which is extracted this you could you could get this image from our own data set so that's it and then you have this feature extractor so that's a feature extractor we have this model and now note that here the tf.vit model is from pretrained so this means that we are going to use this model which has already been trained and you see this specifications here VIT base patch 16 to 24 in 21 key okay so that's it the inputs now pass through the feature extractor before then pass to our model so we'll see how to adapt this code so that we could fine tune our own model in tensorflow and as we have said before this tf.vit model different from the image classification model in that the outputs here are not the final output classes but these hidden states from the transform my encoder so let's scroll here maybe let's read an example okay let's an example here you see this output here that gives you directly a cat so a model produce one of 1000 imaginary classes so that's it whereas here we have this hidden we have this hidden states okay we paste this out here let's take this off take this one off and then we could get started with building our own VIT model based off this hug and face tf.vit model so we wouldn't need this data sets here we already have our own data set that's fine we wouldn't make use of this feature extractor we have this model here we have our hug and face let's call this hug and face model and then let's just take all this off actually so we have this we have the hug and face model we have our tf.vit model from pretrained and that's it now we're going to define some input so we have here our input it's equal the input layer and then we specify the shape so here we we work with 224 by 224 by three and then using the tensorflow functional api we'll get an output here x which we take in this hug and face model let's call this base model so taking the base model takes in inputs let's call this let's change this to base model and so from here we have this base model of v takes in the input and then now we have the output here x now when we run this you see we download this 330 megabyte pretrained model let's have this hug and face model here so we we get the inputs from here and then we have the outputs let's call this output x okay so we have that let's run this again we have a hug and face model now set and we still get this error you see it's linked to the positioning of this input here so let's let's change this let's say we have three by 224 by 224 we run this again you should see now that everything works fine see uh it now works fine so this means that the inputs of this uh base model here so hug and face model should be of this shape so it's three uh let's take this here instead of being 224 by 224 by three i was inside three by 224 by 224 now this means that we need an extra layer which will convert this into this before passing into our base model right here so let's build that extra layer and we'll take inspiration from our resize rescale layer which we had built already let's get to resize rescale right here so we have that uh we'll modify the resize rescale specifically for this um hug and face model so here we have a disco resize rescale for hug and face okay so we have this resize rescale we'll resize make sure it is uh 224 by 224 so every image which passes here will be 224 by 224 we're gonna rescale and then after rescaling we are going to permitate the value so we call on this permit here permit layer and the way we'll build this or the way we'll call this permit layer will be such that we move this from this third position this is zero one two three batch by 224 by 224 by three so move it from this third position to this first position so here we have three uh going to this position and then this uh one two shifts to the right so we have uh three this one goes here three one and then this two comes here so that the output now will be batch the you see the batch remains intact by uh three which has been shifted by 224 by 224 so be careful not to do instead two one this is this is one two and not two one because here we haven't uh height by width by channel so we want to change this to channel by height by width okay so that's said uh we do this here so we just do three one two and that's it so after this input layer before getting here we'll call this x we'll take in our resize rescale and that's it so this resize rescale takes in the inputs and then here we'll pass it will be x let's run this again we should get an error because when we permute it it goes back to 224 by 224 by three so let's have this see we have that 224 by 224 by three as we used to working and then we run this now and everything should be okay oh we get an error resize is not defined uh let's run this oh let's make sure that's how we called it let's go up resize oh we need to run this actually this should be fine now um that's it you see everything is okay so now what we'll do is we'll pass in some input let's let's pass in some input we have this test image right here and then we have a model which takes in the test image now we need to also convert this or rather add the batch dimension so let's expand dims and take in the test image right here so we have that we run this and see what we get let's add this to the zero axis axis zero run that again and then we told that we expected this but instead found this now let's get back up here and we could change this to 256 by 256 and then knowing that this resize will convert it back to 224 since our model takes 224 so let's run this again and there we go here's our output you see we have the last hidden state one by 197 by 768 and as we scroll we have this puller output one by 768 and then from here we we scroll down again let's see if we have another output and that's it okay so this what we get is output we have the the last hidden state and this puller output from the documentation where we had the parameters tfv model the parameters you see here we have this last hidden state the puller output and then we told that these are the two outputs we will always get and then we told that this one is optional the hidden states is optional but the last hidden state is an optional get we always get the last hidden state we always get this puller output and then this attentions here are also optional so if you want to get this attention so all you need to do here is to specify config remember the configuration config config config that output attention set that to true and so this means that by default this will be set to false and then for the hidden states we also repeat the same so config that output hidden states we set it to true now here in the documentation they explain a difference between this puller output and this last hidden state but getting back here you should see the shape so you see this is just one of this year while this is all our full hidden state our full last hidden state one by 197 by 768 but this model or this hugging face with model was built taken into consideration this class embedding right here and so this means that if you want to uh um carry out some classification is better off or we're better off taking this final year this final um or this class embeddings final hidden state now if we want just those hidden states we could specify this pick out the the zero index we run this and we should get only the output or the last hidden states so that's it we get this last hidden states and then since we are interested only in that output corresponding to the class embedding and the class embeddings are the zero position here the zero position we are going to take we are going to we are going to do this here we're going to take um this we're going to take for the first dimension here we take all all and then for this next one we take we select the zero index and then the next we take all so let's run this again and see what we get and that's it we have this output right here and now since we've converted this hugging face model into a tensorflow model we could do a summary so let's run this and check out our model summary so that's it we have this model summary we scroll down see 86 million parameters we have the input sequential the sequential is corresponds to the resizer scale layer of it and then the slicing operator which we have here which permits us get our specific output now getting back here we'll now add our final classifier here so we have this and then we have we'll call this output that's fine let's just call this output so we have this output takes in the dense layer has the number of classes specified here and then we have activation softmax as usual so that's it we have this here output okay now let's run this and see what we get we're getting an error because we didn't pass in this x here so let's run that again now as the model is training just remember that the learning rate we use here isn't appropriate as we can't be using this type of high learning rate when doing fine tuning so we have to change this and use some lower let's say five times ten to the negative say five okay so let's stop the training and then restart this process you see now that when we initialize our model and then we modify this learning rate you see the loss drops now much lower than what we had before with a higher learning rate and our accuracy is already at 75 percent and we are still at the first epoch so be careful when you find tuning or when you're updating all the parameters of an already trained model you have to make sure you use a very small learning rate hello everyone and welcome to this new and exciting session in which we are going to log our data to 1db in previous sessions we already saw how to do 1db logs that is logging our metrics like this also logging this confusion matrices as we can see here we also looked at hyper parameter tuning data set versioning and model versioning in this session we'll not just only log metrics but also log tables like this just as we had done before we're going to start with this weights and biases installation so we we have this pip install 1db that's fine and then we import 1db and then from 1db krass we import the 1db callback so we run this and then we go ahead and log in now we've we've gone through this already in some previous sessions in here you just have to click on this uh authorize so you get here you should have this link or this code to copy from here now if you do not have this uh this key if you don't see this key to copy then you would have to sign up on the weights and biases platform so let's get back here we now paste this and then we press enter so that should be it now with with that we initialize this project and we call the project emotion detection entity neural learn so we have that and that should be fine so you could change this to whatever name you want and then uh whatever entity you corresponds to you to yours so that's it currently logged in as neural learn use this to force we log in okay so we have that and then we have this configs here so we run that um train and validation directory that's fine now we go ahead and train our model and we include this 1db callback right here so all you needed to do here is include this 1db callback and training can start now we just run this for say three ebooks just do that for three ebooks and then let's just take a small part of our data set so let's take the small part so we have that and we run this now as we train this model you'll notice that with just a very small part of our data set the hug and face model performs quite well as you see we already have 77 of validation accuracy with just this small part now the other point is we get back to 1db you see here we could check out on our profile there we go you see projects you select emotion detection and then while this loads okay so we have this and we click on this hopeful plans right here this is what we currently haven't this is our current run so you see it shows us already this charts here we have our top key accuracy validation accuracy validation loss and all of that just by specifying that 1db call so this is already information which has already been locked in and as we know you could always get access to this anytime since this information is locked on weights and biases servers from here we see that the validation accuracy goes right up to about 79.63 percent so it's about 80 validation accuracy while the accuracy itself goes to 82.6 percent and you could check this here you see 82.6 and then you're 79.63 so that's it we able to lock this data very easily the next callback will be adding here will be that of including confusion matrices so here we'll put log confusion matrix okay so we have this log confusion matrix and now we'll go ahead and put out a code for this note that for the 1db callback we didn't necessarily need to pass in any information here but with this log conf matrix this is our custom callback we are going to write code such that each and every time we're done with the training we're going to automatically generate this confusion matrix based off what the model predicts now recall that we've already treated confusion matrices so you could click on this and then we would copy out this code copy out this part which permits us get those predictions and the levels and then we will not make use of this any longer as 1db as a method which takes in our confusion matrix matter which takes in these predictions and those levels and automatically produces the confusion matrix so let's copy this here and then let's get back to our callback there we go so at the end of each epoch it's here we get the epoch logs and then we go to the validation data set and then we have our image the image is patched to the model to get the predictions and then we have the levels obviously coming from this level right here then from here we produce a simple list of the predictions and the levels so now once we have this list once we're able to get this list which we've treated already in some previous sessions you now just have this part so this you could check out the documentation for these plots let's get to the documentation the 1db documentation you could check this out here let's type in confusion matrix there we go we have confusion matrix 1db of that let's check out this logplots right here okay so here we here we in this experiment tracking and log data with 1db log log median objects then logplots okay so we have this we the ROC curves or PR curves and our confusion matrices so we saw this already in some previous sessions so you could get back to looking at how to make use of this 1db so that's it you see we copy out this piece of code here where we have this confusion matrix we have all the ground truth that's the levels and then we have what the model predicts then we obviously have the class names so that's it and then here you see just 1db log and then you specify this string right here and then you pass in the confusion matrix cm which is this and that's basically what we are doing here so we just copied out that code so with this year we have our white true prets class names which is specified already and then we just run this so we run this and that's fine now the next thing we'll do is we're going to create tables and now we're going to see all how important working with tables is as compared to just simply log in values tables will permit us visualize our data in a more interactive manner and here you see under this data visualization you have those log tables and we'll see how to create a simple table and log this so here we have the data you notice that we have different rows this row one row zero row one two and three and there you have the image and you have the predicted and the truth levels so basically what we're doing here is we're taking this image then we log so here's our image then we log its predicted value let's say for example class eight let's say predict class eight and then the truth level is class zero then we take some other image uh let's say predicted is one and here's one so basically this is what we're doing we created now this table where we have different rows and different columns okay so that's it we'll we see how to create this and we also see how to log this tables into our 1db runs so getting back to the code here you see we have this different columns image predicted and level and then we're going to call this table our validation table we have those columns which takes in our columns here obviously you could add this you could increase this and put whatever values you want there and then fill that so we have this uh validation table which we've just created and then now we're going to fill this table color row by row so for our first row you see we have the predicted values uh our model takes in the image that's it and we have the levels and then here you see for a given row we have 1db image we pass in our image that's it we pass in the predicted scores then we pass in the levels scores to for corresponding to this three columns right here and then uh for this valid data table that we created here we're going to add this data so we're going to do this for each and every element of our validation data set now here we decided to take a hundred and then we'll get him back up what we'll do is we are going to uh somehow unbatch this so we're going to change this batch size to one because we want to be able to play around with a single validation element and not uh say for example 32 of them so because of that we'll just change this to one and we run this again uh this should be fine okay so so we have this uh validation data that's fine and then here uh training okay we run this again and that's it we have our validation data now let's get back here as we said we've we've taken just a hundred of this so take just a hundred let's let's even reduce this let's say 25 okay so but you could take all the elements actually we just want to make this a bit faster so we have that and once now you feel this you see this follow permission to fill each and every row so you just keep filling rows in your table and that's it you could get this uh here see this we you see you have this table the columns and you have the data right here and then you just log this information so this from this uh documentation you could be able to uh understand exactly how we build this table although it's quite straightforward so from here we do one db log and then we have our model results and we're passing the vowel table so let's run this let's get back here let's run this two cells we run this two cells now and then we start with the training so we have the training now over and we could check out this custom chat which is that of the confusion matrix you see here we could get the number of uh times the model predicted angry when we're supposed to be angry here is the prediction happy when it's supposed to be happy and then here is prediction sad when it's supposed to be sad so we see this is 357 times for happy 876 um for sad 682 and then here we have the times when the model predicts sad when it's supposed to be angry so this is our confusion matrix uh which we generate automatically using one db confusion matrix method and then we move on to this tables so here you notice that the tables contain a hundred different elements so uh you could keep going on to the next so move on to the next 10 and so on and so forth so let's get back or let's let's just work with this now here you see that with this table you can actually like here you see if you scroll if you get get to the hover on this and you have these three dots click on that and you could group so check on this you see you group by image for this one what we have here is this predictions you see happy the level happy uh yes sad sad angry angry and so on and so forth so this model predictions here are quite great now if we increase this move you see we have this example here where the model or rather the the true levels is happy and the model predicts angry but when you look at this photo it's uh it shows that this person is not necessarily happy so we understand why the model may be thinking that this person is angry and so with this one db tool you see that it becomes very easy to detect mislevelled images like this one now another way to get this more systematically is by getting here but since we do not have the group we need to start by ungrouping right here so let's get back here click on this we ungroup now we've ungrouped already and then we get to this level and then we group by those different levels you see that here we have happy sad angry now um this is what the model predicts the model you see here the model predicts mostly happy because obviously the level is happy because we grouped by happy level here sad you see mostly sad angry and mostly angry now what is interesting to note here is for this angry well the level is angry the model never predicts happy anyways let's uh get back here you should be able to scroll through this you see you should be able to uh scroll through all the different happy images and here you should be able now let's uh look at this you should be able to detect cases where the model or rather the data set has been mislevelled like here this person isn't necessarily happy so already you could detect uh these kinds of misclassified or mislevelled data points then we also have this image which is almost completely blurred out you could always could check on these kinds of images and uh take them off actually because if your model isn't going to be seen this in real life then it's preferable to take this off nonetheless if uh in the real world the model is going to be meeting these kind of images then you could allow this uh let's keep going this way you could also check out for those sad images and be sure that those are actually sad people here again you see this person actually smiling and it's leveled as sad then for the angry you see this uh little boy was visibly happy but instead level angry so the 1db tables permits us check these kinds of errors in a very interactive manner now we will again ungroup so we ungroup this and then we could check this out for a predicted group by uh predicted levels so this is what a model sees so the model predicts that this is happy oh this is sad and this image is angry you see what you notice here is because this image you know recall this image which we just saw because this image has been had been leveled as angry the model now predicts that this image is an angry image which is not true and so when you deploy this now into production you would find that this kind of images or this image will be predicted as angry whereas it's meant to be happy and the worst part of it is that even your evaluation wouldn't find this as an error because already the level or the the the class for this was set to be angry then another thing you could do here if you get here um let's have this back another thing you could do is you get to this column settings here click on this you could uh get to column settings just by clicking on this column settings this column settings click on that and then you're you select the levels so here you're picking all the levels where the printed value is different from what the level or rather what the model predicts so here we could say row predicted so when row level is different from a row predicted we unless it so we have that so press enter and then what we notice here is we have this true this false and uh and that so basically the reason why you have this true and this false is the true is for when let's get back to this year the true is for when the row level is even from the row predicted so this is this are the wrong predictions so what we could do here is we could modify this to equal so that's equal uh that's fine this should be reset so you see okay so you see now it's true meaning that this false is when the the prediction is not the same as the level so that's it we could increase the size of this table let's get to this increase the size and basically that's it so you see uh the greens are for when the coincide and the reds are for when what the model predicts is different from what the model ought to have predicted now we could get again to the stop and then check this column settings and take this off so you see you could simply take that off okay so we have that reset again and you now have your levels as you had originally you could do the same for the predicted now you could also get here see this take on this and then you you check the row row we're trying to get all the rows where the row level is uh different see different from the row see type row predicted so that's it because if you say row level different from row level obviously you have no row so let's test that and you could see if you apply you should have no element in here see there's no element uh that's not actually possible now let's take this off and now we take row predicted so row predicted and then you apply you apply no um some connectivity issues so here we apply we check out where the the the predicted is different from the level so here we have this here see happy and the predicted either sad or angry so you are the wrong predictions so with this you could check out and see the kinds of images which the model finds difficult or find difficulties in correctly predicting so like with this one you see happy this the the level here is happy but clearly this man isn't happy this one doesn't look too happy doesn't necessarily even look happy this one is happy and we could look at that so generally for this our problem uh we see clearly that many many problems come or many of these misclassifications come already from our data set so this means that the data set we have to look at the data set and ensure that it is cleaned before making use of it so you see this happy this person isn't happy this this happy this happy uh we could get to sad you see this person is happy but this level that's sad so with these kinds of interactive visualizations we could be able to say for example we want to see where the model was supposed to predict a certain value and it didn't predict that value now we could also check out where this is equals so we could check equals and say okay where the the row level is equal the predicted we apply and then we check out the kinds of images we have you see from here that oh most times the leveling will be correct you see the level happy level sad the level angry uh we are we still have this image here this shows that we have even gone as far as teaching the model to look at this kind of image as an angry image Hi guys and welcome to this other exciting session in which we are going to look at the ONIX format of representing machine learning models. ONIX actually stands for Open Neural Network Exchange. This open standard for machine learning interoperability was codeveloped by Microsoft, Facebook and AWS and in this session we'll learn how to convert our already trained TensorFlow model into this ONIX format and then carry out inference on this our newly created ONIX model. So we've gotten to this point where we've finetuned our hugging face based vision transformer model. Now we have this TensorFlow model which we've created and evaluated and it may happen that another developer using a different framework like for example PyTorch wants to make use of this model which has been trained in TensorFlow. So thanks to the ONIX format we can now convert this code which was written in TensorFlow or rather convert this model which was built in TensorFlow into an ONIX model and then later on convert this model from the ONIX format into PyTorch such that we now have this PyTorch model which this other practitioner can use. Now another possibility is the reverse that is we could go from this PyTorch model to a TensorFlow model thanks to the ONIX format's interoperability. Another reason why a developer will want to say take this model from PyTorch and make use of it say in Cafe for example maybe because that model is more efficiently run in this other framework and the reason why we have these kinds of differences for different frameworks is because for example you could have a convolutional neural network or a convolutional layer which is built in PyTorch with a certain implementation and the implementation in Cafe may be slightly different and maybe more efficient as compared to that which was done in PyTorch. Now this is just for demonstrative purposes and we are not saying that the implementations in Cafe are better than those in PyTorch. And so in summary the ONIX format allows models to be represented in a common format that can be executed across different hardware platforms using the ONIX runtime. So now developers can feel free to build their models with just any framework say for example TensorFlow or PyTorch or Paddle or whatever framework they actually want to work with knowing that they could deploy your model on whatever hardware they want to since they could now convert these models to the ONIX format and then run inference on these models via the ONIX runtime which is in fact an inference engine which is lightweight and modular and permits us run our ONIX models on just any hardware we choose to work with. So here let's look at the list of supported hardwares. You see for example the TensorRT which is very popular with the NVIDIA GPUs and permits us to attain very high speeds when carrying out inference on neural network models. The other great advantage of working with the ONIX models is the ease with which we could convert TensorFlow models into this ONIX format. So right here you see we evaluate in our model this our hugging face model which we finetuned previously and we get in 90% validation accuracy. Now we can go ahead and save this model so yeah let's take this off HF model let's call this hugging or let's call this VIT finetuned. Okay so that's it we have this finetuned VIT model which we're gonna save we check this out here. There we go we see a VIT finetuned and you could check you see you have the saved model Keras metadata variables the assets folder and then here if you you could see that this is almost one gigabyte model so uh you could view this here you see uh 984 just just check as I hover on this you see here 984.39 megabyte model and so this means that if you were to deploy this in a real world scenario then you need to always allocate this amount of space in order to run this model. Now let's move on to converting this model into the onyx format. We shall have this two installs so we start by installing the tensorflow to onyx tool and then we'll install uh the onyx runtime so we run this and then now we'll go ahead to convert our model which is this one here VIT finetuned so we just have to specify your VIT finetuned into the onyx format so here we just say VIT finetuned and then now we have this model I'll give it this is the name of our onyx file so call this VIT onyx that's it VIT onyx tf2onyx.convert and that should be it so now we'll run this then while waiting for that run to be complete you could get to the onyx github repo tensorflow onyx and then you'll have the documentation for how to convert from tensorflow to onyx so let's get back to this uh to running because here we have a series of warnings then uh specifies the tensorflow on tensorflow onyx and tensorflow to onyx versions then the upset then um it's optimizing and now we have this successful conversion so onyx model is saved as VIT onyx dot onyx now let's open this up see VIT onyx dot onyx you see you you we've moved from this uh let's open this again we've moved from this year where we had uh 984.39 megabytes to this optimized onyx version which is at 327 megabytes now another option will be to convert the model from this Keras format to the onyx format so this first one we saved this as a tensorflow saved model and then now we could just have this year so let's say we have the model we saved this as a Keras model so we have that h5 and we run that um taking here you see we have this still 984.98 megabytes close to one gigabytes and then from this Keras format we are going to now convert this to onyx now here we're going to have this specification and then we'll pass in the image size so here we have um the batch by 256 by 256 by 3 it's float 32 and it's our input then we could specify this output path so this one was VIT onyx let's let's let's have this as um let's let's call this VIT Keras dot onyx okay so this is going to be our output path here and then we will have this tensorflow to onyx here which contains this from Keras method which takes in our model hug and face model and then the specifications which we just mentioned right here so here our specifications right here we pass this in this uh our passes as our input signature we have this offset value and then we also have our output path which we have already specified here then we have our output names which we shall get automatically done so that's it let's run this while that's running also note that you could check out this conversions on the onyx runtime.ai platform where you can get the details of all what we're doing so let's get back here still still running and that's it complete let's check out our VIT Keras you see 327.59 megabytes then from here we move on to the inference where we'll see whether what we get from the onyx model coincides with the initial Keras or tensorflow format so right here we have this provider now here we specify this provider to be this CPU execution provider as we'll be running this on the CPU and then we have this onyx runtime as RT which we are imported right here and we should be making up making use of here so the when we want to run an onyx model you see the first thing you have to notice we do not need tensorflow anymore so even if we restart this whole process all we need to do will be just to install the onyx runtime and then import this this way so now we have our onyx runtime we we have this inference session which we create by just specifying this path our output path here our output path is the path to this model the onyx model and then the provider is this one here so we just need to specify this path and the provider and we're good to go so now we have this the next thing we want to do is to run the inference so the onyx prediction is this m.run and then now we'll specify the output names now this output names is gotten from here so let's run this let's print out output names so you see what it contains that's it you see we have that dense and if we get back to when we're creating this model let's get back to this okay you see that the name here we have is dense so if you specify the model name to be different from this you would have a different output name so that's it let's get back to our onyx inference we've converted already and that's it so so that's it we have our output names which we created all right i wish we generated automatically from here does it and then obviously at least because we could have several outputs and then from here we pass in our input image now this our input image is simply what we've been having already so we just we could copy this copy this and then test this out here with our onyx model um let's have this here add this code paste this out all we need here is just this basically so we we could run this let's run this that's it we have our image and then let's run this and yes let's run this and then we print out the onyx thread so we get in input must be a list of dictionaries or a single nonpy array so what we're going to do here is instead of tensor flow we use nonpy so you see already that we do not really need tensorflow like even with this we could get the the test image from here let's let's get this test image directly so we could have test image and that's it we run that we have our image this not defined um test image let's run that again that's fine we get this other error because this input isn't a float so here instead of this we're going to have test image dot as type np dot float 32 okay so we have that set now let's run this again no before doing this let's make sure we pass in this image instead here so we have that we run that that should be fine okay so let's rerun this again and you see we have now our onyx thread so there we go you see it shows us that it's a sad image because this is angry happy sad so it's sad because it has the highest probability and you see the image here is sad now if we get back to this top here and simply run this the same image let's run this let's get the probabilities actually so let's let's let's say we want to have let's print out the model image let's get those probabilities you see this is this what this model gives us is output and here's what we get from the onyx model right here now before we continue another thing we would like to check out is the speed or the time or the latency of the model so let's get back up here and we could add this code so let's import time we could just do it below so let's yeah let's do let's just do that below so let's take this off and then here we could say we want to have let's import time there we go and then we have t1 which is time the time and then we do hf model taking an input image then we record the time the current time minus the t1 so we could get the time which is elapsed after running this model you see 0.14 that's a 0.14 and you should know that here we're supposing that we're running this on a GPU so you check uh manage sessions we're running this on a GPU now for the onyx model we could have here t1 time dot time and then we could print out the the difference in time so we have that minus t1 let's run this and see what we get input lists a single numpy oh let's run this again okay so you see with uh we here we're making use of the cpu and we get in 0.279 seconds while with the gpu here we get in 0.143 seconds now we can be comparing the onyx runtime results on a cpu with that of this uh hungry face model with tensorflow on a gpu you could you could even see from here if you do get device you see from here that what the onyx runtime is using is the cpu and so if we want to make use of the gpu we would have to install onyx runtime gpu version so we could compare the the two um models so for now just note that tensorflow with the gpu is 0.15 seconds and then we'll need to get tensorflow with a cpu and then we'll also get we we got onyx with a cpu onyx with a cpu give uh let's say 0.3 seconds give about 0.3 oh let's run this again we run it again we should get let's run this again oh getting an error now that was because i had moved this uh file into the the drive so here we have the vcarass here and we run this again we have about 0.38 to see that's it about okay let's say 0.5 so let's let's say this is 0.5 uh here and then we need to get onyx with a gpu and then we also need to get a tensorflow with the cpu so that's it uh what we'll do now is we'll go ahead and install the onyx gpu so here we have pip install onyx runtime gpu and that should be it we already installed this okay so we have this um onyx runtime gpu and then if you if you if you do uh import onyx runtime as rt and then you as rt and then you say rt.get device we are getting the cpu so what we'll do now is we are going to restart this runtime so basically restart this runtime so that um this the the gpu version of onyx will be taken into consideration now this time around we're going to install onyx runtime with the gpu and that's it as you can see now the device we're using is a gpu okay so we have that let's now get back here uh we already have our model in the onyx format so we just run this and then if you run this you'll see that since we we we specify the provider to be the cpu we shouldn't have any much difference here so let's run this again we import a time output names not defined okay our output names is basically this so let's have this list and let's define this here so let's have output names um output names there we go output names dense so let's run that again that's fine and then oh we get back here but note that these output names were generated from here so you could um get back to start generating this or you could just make use of the output names as you can see since we restarted this runtime we do not have those files anymore apart from the onyx uh which we start in the drive so if we have to do this we need to retrain our model since we lost them already okay so we have our output names here which was specified and then we get back here we run this again and check out on the time it takes to run the model you see 0.34 not bad uh 0.34 okay let's say 0.34 so here normally this should be zero point yeah let's say 35 okay so the tf with gpu 0.15 onyx with cpu 0.35 now the way we're going to use this is we'll get here you see um in this documentation you have the provider you will specify the coder execution provider so let's copy this and then paste it out here so instead of instead of here for example instead of cpu now we have the gpu so we could make use of the coder execution provider so here we'll take this off and then paste this out so this is our provider now note that since these providers uh can be a list you could have several providers like here when we do this when we have coder execution provider before the cpu execution provider it means that that the priority goes to the coder execution provider and so in the case where we are having a cpu then we'll start with this one and given that this cannot work in the situation of a cpu we would then move on to this but if we have a gpu then directly we will use this scooter execution provider and that's it see 0.048 seconds or let's say 0.5 seconds which is three times less than what we had when running our model or when running the tensorflow model so you see already that the onyx uh framework permits us to optimize our initial tensorflow model so here we have onyx gpu now this is 0.05 so it takes us now 50 milliseconds to run this hug and face model so that's it now before we proceed it's also important to note that generally the way we measure this time it takes for the model to predict or an output we usually can test with several input images or we could repeat this process several times so let's say for underscore in range let's say 10 we run this and then we get the time elapsed instead of just testing once we could test 10 times so we could get the average so let's run that and you see dividing this by 10 we have 0.035 so that's 35 milliseconds per prediction so this means that for us to have 100 predictions we would take 3.5 milliseconds we could test that out let's run this and there we go we take 2.3 seconds for 100 predictions now if we divide let's let's say let's call this number of predictions uh we set this to be 100 and then the time taken this is uh time time for a single prediction there we go time for a single prediction all this divided by the number of predictions so we get the average that's fine so let's run that again so we get this time okay so we see 0.023 seconds that's 23 milliseconds while if we repeat the same for the hugging face model you see it takes 0.15 seconds uh just as we had already now we could see that the the difference in speed here 0.15 divided by 0.025 you see the onyx model is six times faster than the tensorflow model now what if we get back and run the tensorflow model on the cpu so change this runtime known and we save that so we'll have to rerun all this again now running our hugging face model here you see this takes 0.8 seconds 0.8 so with the with the cpu we have 0.8 divided by 0.35 the onyx model runs about twice as fast as the tensorflow model hi guys and welcome to this new section in which we are going to be looking at quantization for neural networks in the previous section we looked at the open neural network exchange standard which is an open standard for machine learning interoperability we saw that not only does this onyx format permit us convert models from one framework to another but they also allow us optimize our models for different hardwares and so in line with these optimizations we are going to look at quantization which is a technique for performing computations and storing tensors at lower bit widths than the usual floating points which we have been working with so far in this course model quantization is a popular deep learning optimization method in which model data that is both the network parameters and the activations are converted from a floating point representation to a lower precision representation typically using 8bit integers now defining quantization in this manner may not seem very clear so let's try to understand first of all why quantization or quantizing a neural network model is important so here let's consider this very simplified model where we take in some input multiply by a weight and add the bias now we have several layers so we just simply stack this up and we could say we have our model which has already been trained and this model has a hundred million parameters and occupies let's say one gigabyte of space so we have this model 100 million parameters one gigabyte of space and if you're doubting what this space is for you should note that it's for storing this weights and biases and so obviously the more parameters we are going to have the heavier our final model file will be now supposing you want to use this in some setup like a mobile phone so you want to use this in your mobile phone it means that you need to allocate at least one gigabyte of memory space if you want to run this model and this is where the techniques like quantization come in so now thanks to quantization instead of storing this weights in a 32bit space we are going to store them in 8bit memory space so we're going from floating point 32 to int 8 now if you're not familiar with the floating point arithmetic you could check out this resource by Fabian Sanglar where he explains in a very intuitive manner the core concept around floating point binary representation essentially if a single weight value that's your model weight let's take this off if you have a model weight value which is for example 3.14 the way this is represented in memory is by first of all allocating this 32 spaces we have here where each space takes a 0 or 1 at this first position here the 0 or the 1 is to specify whether we're dealing with a positive or negative number and then for the next eight positions we are going to see whether this value 3.14 lies in the range 2 to the minus 1 to 2 to the 0 or 2 to the 0 to 2 to the 1 or 2 to the 1 to 2 to the 2 and so on and so forth now in our case 3.14 lies in this range and given that this exponent we have here is 1 we are going to apply this formula where we have the exponent minus 127 should give us this power we have here so we'll have that 1 so we have e is equal now 128 and if you convert 128 to binary notation you would obtain this right here and now after encoding this integer position the next step will be to encode this decimal value right here and that will be the role of this 23 other positions remember this is only one box this is eight boxes and here we have 23 boxes for this eight we'll see that it helps us locate our number in this range which we've seen already but for this other 23 boxes we are going to suppose that since we have 2 to the 23 possibilities thus let's try this here 2 to the 23 possibilities which is actually 8.388,608 possibilities so we have 8 million possibilities here this simply means that for every given range which we've seen here for this range this range this range this up to the end we are going to divide it into 8 million different parts and so if you see here you see you have 2 to the power of 1 this is 2 to the power of 2 if you break this gap if you break this here this here this gap into 8 million different parts or better still if we consider that the distance to move from 2 to 4 is 8.388 million then the finding this 3.14 or encoding this 0.14 right here or let's just say encoding 3.14 will until calculating the distance from 2 right up to 3.14 knowing that this distance from 2 to 4 is 8.388 million we cannot compute this distance by simply doing 3.14 minus 2 that is 1.14 divided by all this distance that's 2 so we find this and then multiply by the 8 million we have 4, 7, 8, 1, 5, 0, 6 so now we shall convert this to binary and we obtain this here so once we obtain this we then fill up all this 23 spaces right here and that's essentially how a number like this is stored in memory and so getting back here if we have to store this let's say 3.14 3.14 which was previously stored in this 32 box memory and now we want to store it in an 8 box memory now we move from 1 gigabyte to 256 megabytes here we have 256 and our mobile phone will now need only 256 megabytes of memory to run our model now it doesn't just suffice to say we are going to go from the floating point 32 to the int 8 we need to describe exactly how this is done and the way it's done is actually by a simple linear mapping where we shall start by defining two ranges of values the first range is for the floating point values and as you could see here the defined negative a max to a max well one good thing about deep learning models is most times your weight or your weight values lie between negative 1 and 1 and so getting back here we could have here negative 1 so a max will be 1 and so negative a max is negative 1 and then a max is 1 so we go from negative 1 to 1 and then if we want our output to be unsigned ints instead of going from negative 128 to 127 we shall go from 0 to 255 now notice that the number of values we have between 0 and 255 is the same as number of values we have between negative 128 and 127 but with the unsigned ints all our values are positives so instead of the int we have unsigned int 8 and so at this point our aim is to take values ranging between negative 1 and 1 and map them in the range 0 to 255 and now we will use a simple linear function which has the form y equals to ax plus b now our wire will be the output value so we'll have the let's call this x we'll call this x quantized this equal x floating value so this is the original value of the weight let's let's put this in blue we have the original value of the weight or the float value divided by the certain scale plus a zero point value we'll call this z so simply one over s is equal a and b is equal z then y is x q and x is x f so now our aim is to look for the value of s and z such that when we have any value in this range we get its corresponding value in this other range the way we'll get s let's have this the way we get s is by doing x float max x float max you say that x float max is simply a max minus x float mean which is in this case negative a max divided by x quantized max minus x quantized mean now if you replace all this by the corresponding values we have here we would have one minus negative one divided by 255 minus zero so 255 minus zero this means we have two divided by 255 that's our s which is our scale and then z our zero point is x q max minus x f max divided by s this s we just had here so if we replace again we have x q max which is 255 minus x f max which is one so we have 255 minus one divided by two on 255 that will give us 255 divided by two so essentially 127 127.5 so that's what you get now the way you can look at this zero point is it's the quantized value we get when the floating value is zero so when we convert when we have zero here we have zero on s which is zero um the quantized is the quantized value or the corresponding quantized values equals z so that's why we call that the zero point and then s which is the scale simply scales our inputs here as we go from this range of values to this other range now you could take a simple example where you could leave from um x f to x q if you have negative one here you see you have let's say x q is equal negative one that's x f let's suppose we have negative one then divided by s we said s was 250 two on 255 um plus z z is 255 divided by two so this gives us zero which makes sense as we go from negative one to negative one one two zero two fifty five this means that this here or this boundary values should be almost the same now let's take another example in the middle let's say negative or let's say zero point three so we could have x q which is zero point three divided by two on 255 plus um 255 divided by two so in this case now we have a value of 165.75 which if we run up we could have 166 so essentially we're going from 0.3 to 166 and apart from running the output as we've just done we would also see that we could clip any outliers so in case we've decided to have here a max or our x f max to be one and that it happens that we have a weight or weight value which is more than this one then the output um unsigned int would be 255 so any value greater than this is going to take this value any value less than this is going to take this value of zero and so we've seen how this um simple technique permits us reduce our memory used in storing the weights now it's logical that we're going to have a drop in the accuracy because if you've trained a model for example to have certain floating or certain weights which are actually floats and then you convert these floats into integers where you have some extra transformations like the rounded and the clipping then you'd expect to have a drop in the performance of the model nonetheless this huge gains in terms of memory are enough for us to sacrifice a bit of the accuracy or more generally the model's performance and that said apart from this model weight size which is dropped or reduced it should be noted that arithmetic operations like multiplication and addition of our quantized integers can be carried out even much faster and so we not only have a model which occupies less space but a model which is even much faster we have generally three ways of carrying out quantization the dynamic quantization static quantization and the quantization are where training now given that during the quantization process the weights and the activations are stored at lower bid weights in the case of the dynamic quantization this quantization parameters does a scale under zero point which we've seen already for the activations are computed dynamically or on the fly and because this year have to be computed dynamically there is an increase in the cost of inference so it will take a little bit more time to produce an output as compared to other methods like static quantization nonetheless year we usually achieve higher accuracy compared to the static quantization methods for the static quantization method we first of all compute the quantization parameters using a much smaller data set which we'll call the calibration data so essentially we have our model and in here we have our different quantization parameters but instead of dynamically computing these different quantization parameters we are going to pass in some input and output carry out several runs such that we are able to obtain the most appropriate quantization parameters based on this data we've passed in and then now when we want to run or carry out inference we do not need again to compute these parameters unlike with the dynamic quantization where at inference time we always have to compute these quantization parameters here we compute these quantization parameters before via calibration data and then now when we run an inference we just pass in inputs and we already have the quantization parameters set but the problem now with this method is that if this calibration is done poorly then we would have low quality values for the scale under zero point and so because of this we would then have a lower accuracy as compared to the dynamic quantization method that said these two methods we've just seen are postquantization methods so dynamic and static is postquantization meaning that we train the model first in floating point 32 and then after training the model we convert this model to one with weights and activations which are unsigned ints now sometimes the posttraining quantization that is PT cure is not able to achieve acceptable task accuracy this is when you might consider using the quantization aware training that's QAT the idea behind the quantization aware training is simple you can improve the accuracy of the quantized models if you include the quantization error in the training phase so unlike posttraining quantization where we train the model first before quantizing here the network adapts to the quantized weights and activations during the training so as we were saying we include a quantization error in the training loss by inserting fake quantization operations into the training graph to simulate the quantization of data and parameters these operations are called fake that's fake quantization because they quantize the data but they immediately dequantize the data so the operations compute remains in float point precision that said the posttraining quantization is more popular than the quantization aware training method thanks to its simplicity as it doesn't involve the training pipeline the quantization aware training almost always produces better accuracy and sometimes this is the only acceptable method and that's it for the section in which we've looked at quantization of neural network weights and activations to help reduce model size and also speed up computations hello everyone and welcome to this new and exciting session in which we are going to see how we'll move from a tensorflow model which occupies one gigabyte of space to an unequal quantized model occupying just 83 megabytes at this point we now understand the concept of quantization and we're going to see how to apply or implement quantization specifically dynamic quantization to make use of our model even more efficiently before we move on we should also note here that the tf size tf size is one gigabyte it's about approximately one gigabyte that's 1000 megabytes while the onyx size is 328 megabytes so onyx size 328 megabytes now we're going to look at the onyx quantized let's just copy this so we have the onyx quantized cpu onyx quantized cpu which we shall get shortly the onyx quantized GPU and then we'll also get the onyx quantized size so let's take this off for now and then get back here now here as you could see we've imported the onyx onyx runtime quantization we've already imported the onyx runtime so here we just imported this quantized dynamic and quantized then from here you see we have the two models that's the floating point and the quantized model now this year this model is what we had already so we'll get back and then copy that and then we'll call this one the vid quantized so that's it now all we need to do is we have this quantized dynamic which takes in this model takes in the path to the quantized model it's still an onyx model and then the weight type now here this weight type is an unsigned int so let's run this and see what we get we get in an error this name isn't defined okay we should run this before running this one so that's it this should be fine this time around and we should be able to get this file right here this quantized onyx file again here you could check out documentation on quantizing onyx models you see we have quantized onyx model we have an overview and all of that so let's get back here and check out our model oh there we go we have our vid quantized and what we notice is we have 83 megabytes so this means that we've gone from one gigabyte or 1000 megabyte to just 83 megabytes and now we could go ahead and check out the speed or the cpu speed so let's get back here or rather let's let's get back here there we go copy this path let's copy this path and then we have this year we paste that out here and then this this one here this is an cuda this this is cpu cpu execution provider that's fine everything looks fine and let's run this you see we get 0.39 that's practically 0.4 seconds per prediction and so here we have 0.4 seconds now let's let's check this out again um let's check this out again for the original onyx model so let's run this and check this out you see here we have 0.49 that's practically 0.5 so it means that this isn't exactly true this should be 0.5 so we see that the quantized model uh is faster and way much lighter than the onyx the original onyx and the tensorflow models but we have to be careful as quantization generally comes with a drop in the accuracy now we switch to a gpu and then test out our quantized model so right here we have this quantized model which we're going to run and then check out its latency here is where we get 0.27 let's say 0.3 and this shows that the quantized model doesn't benefit as much as the onyx model from the usage of the gpu now if you check out the documentation we'll see that there is quantizing uh an onyx model and this is quantizing on a gpu so the quantization on the gpu isn't as straightforward as that with the cpu as here we do that we will need a device that supports tensor call int 8 computation like the c4 or the a100 and let's say here that order hardware would not benefit from quantization though if you want to proceed with quantization or quantization with a gpu you can make use of this tensor RT execution provider and here they give the overall procedure to leverage this tensor RT execution provider so with that we're going to get back here and the next thing we shall do is ensure that the quantization process hasn't led to too much drop in accuracy as when we quantize the model generally we may have drop in accuracy but our aim here is to be sure that this drop is minimal and so to do this we are going to evaluate our model so here basically we'll define this accuracy which takes uh the model and then for a hundred we'll take a hundred elements a hundred elements in a validation data set where the evaluation data set as a byte size of one we are going to compare each time the output or the unexpedition with the level so here we compare the level with the unexpedition and if they are the same we increase the accuracy variable value year by one initially they are zero total accuracy is zero but the total is always increased and then the the accuracy is increased only when we have this two the same so with this basically we implement this accuracy uh method which now we take the two models that is the onyx the original onyx and the quantized onyx so here we have these providers to to to make this run faster so we run this now you see here we have 90 for the original and then 89 for the quantized model the next thing we'll look at will be how to visualize onyx models using Lutz Rodas Neutron app so you could get your Neutron app and you you have this interface right here now we're going to open the model there we go it's loading and here's what we get you see we start with this transpose you could recall we have this transpose and then we moved on to this resizing then matrix multiplication and then the rest of the VIT model right here so here is the VIT model in this onyx format onyx quantized format screwed right to the end right here and then towards this end you see we have this matrix multiplications for a linear layer and then we have this softmax you could also export as png so you could open this up in this png format right here so that's our model and that's it for the section where we've left from a 1 gigabyte model to an 83 megabyte model with just a 0.01 drop in accuracy hi there and welcome to this new and exciting session in which we are going to treat quantization our world training with tensorflow now in some previous sections we started by explaining what quantization is all about and the advantages of quantizing models we also looked at different quantization methods and we looked at the relative advantages and disadvantages of these different methods that said we are going to see in this section how to quantize a full model or just some layers which make up that model in tensorflow the special model which permits us carry out quantization is this tf.mod which stands for tensorflow model optimization and so here we start by installing this tensorflow model optimization model and then we'll import this as tf.mod then since we want to do quantization we could get in here Keras and that's it here we have different methods and classes let's get into this quantize model right here as you can see quantize a Keras model with a default quantization implementation and so here we have to simply pass in this to quantize argument which is the model to be quantized and then we should get a quantization aware model so here for example you see this model define the sequential model and then we also have this functional model then to quantize this we just call this method quantize model right here and then we pass in our model and we have our quantization aware model so that said let's go ahead and implement this here we have our model in this case let's let's start with our hugging face model so we have this our hugging face model which we've declared already and now let's say we want to have our quant aware hugging face okay so we want this quantization aware hugging face model and then we want to use tf.mod.quantize model so basically let's copy this here and then paste it out in the code so we have that paste it out and then here we have our hugging face model so from this we're running this should give us our quantization aware model now we get an error quantizing the tf keras model inside another tf keras model is not supported so as of now this isn't supported now um let's try out with the efficient net model though this should it should be the same error because the efficient net model let's get up here the definition for the efficient net model first of all you can see the hugging face model you have this model in this model so that's the reason why that doesn't work and then we we get to the efficient net model transfer okay so we have this model right here and we could see that we have this keras model which is this backbone here in this model and so if we had to use this we have to look for a way to break this backbone up into its different layers but as of now what we've been doing is just making use of this backbone as is here we just have this backbone and that was it we didn't actually break this model up into different layers now that said let's copy out this efficient net model right here uh to have to take all this off and then now we are no longer making use of this input right here so we wouldn't use this we'll use the the backbone's input directly so let's look at that we have this x so from here we have the backbone's output we should get into this global average pooling layer so here we have backbone output that's it we have this output which gets into the global average pooling we have this x here that's it which now passes through this dense layer and then to the batch norm layer and then to this dense layer search that we have an output right here okay so we have that let's pass in this x values there we go and finally we have this then from here now we create our model so it's our pretrained our pretrained model is a Keras model and which takes inputs the backbone input so now our backbone input is our input and then our output is simply this output right here so we have that output okay so we have this set now and everything should work fine so let's run this here and what do you notice you will notice that the Keras model what we had previously as our Keras model has now been broken up so you could see i think we should have just um set uh here pretrained functional model let's call this functional model okay so let's let's go back and run this other cell here let's get back here um pretrained model uh okay so let's run this again this is our pretrained model and then so that we could we could run this um summaries down here and you could see Clara so you see here we have this model is exactly the same model we're dealing with so far this is the exact same model what we want to do is just to paste this so have the pretrained so this is a pretrained model uh let's get a summary pretrained model summary run that let's reduce this so you could get into the space we have this and you see we still have this exact same total parameters is here the same number of parameters number of nontrainable parameters exactly the same so it's basically the same thing but the difference here is we do not have this um let's open that up again we do not have pretrained summary we do not have this carat model here so we do not have this uh model right here and so because we don't have this now it will be possible for us to make use of this method and quantize our full model so that's it we have this pretrained model now let's run this again so we we have this pretrained pretrained functional model let's run that okay we have our model set and now what we can do is we would run this now so let's run this again and see what we get just increase the size and there we go we get another error this uh the same error actually let's get back here oh this should be pretrained functional so let's run this now it's taking more time hopefully everything should work well now instead we're getting this other error yeah well we told that this rescaling is not or this this layer here is not supported and this is normal since here in this rescaling layer we do not have any weights and so we are not going to be carrying our quantization for such layers so um what we can do now is instead of quantizing the whole model we'll select some layers we want to quantize so here there we go what we'll do is instead select some layers so this means that if we had let's define a simple model so let's let's uh get back to the top and then we define for example this learned model without this resize rescale so that's it uh quite simple model we have that now let's run this let's run this cell and then oops we're getting an error so that's because we we took up the resize rescale i would not specify this your exact for an exact uh input size so let's run this again that's fine now we have our lunette model let's do this lunette model and run that we get another error let's check that out this batch norm is not supported so you cannot quantize this batch norm layer so what we'll do is let's let's basically remove the batch norm layers but later on we'll see how to have to to to to quantize only uh some layers so for now let's just remove this batch norm layers uh batch norm off drop out let's take the drop out to batch norm off and that's it uh so we have that let's run this again uh see what we get okay so that's fine you see now uh we've been able to make this lunette model quantization aware and we've done this for the whole or the full model now in cases like this year this model here this uh efficient net model where we have this backbone which is uh our pretrained uh backbone we cannot start taking off the normalization layer for example here and taking off this rescaling which comes with the backbone and so on and so forth so what we'll instead do is we'll move layer by layer and select the layers which we want to actually uh make quantization aware so that's basically what we'll do and so instead of proceeding as we did here with this lunette that is uh quantizing the whole the full model we're going to go layer by layer so with that we could comment that section there and now take this model off now in order to quantize only some layers of the model we'll make use of this quantize annotate layer method right here so we see again we have quantization kiraz quantize annotate layer and this takes in the model to annotate with some quantization configurations so here what they explain is this function does not actually quantize a layer it is mainly used to specify that the layer should be quantized so you see it's there to specify that the layer should be quantized and so the layer then gets quantized accordingly when we do a quantize apply so this is the quantize apply method here click open that and that should be it so let's get back uh oh let's let's just get let's just look at this example here where you see this layer you see we have this model but in this model we want to quantize only this layer and so as you could see we have quantize annotate layer and then once this is done we do a quantize apply to get our quantization aware model which here is called quantized model so let's go ahead and see how to implement this with our pretrained efficient net model we'll now define this method apply quantization to the conf layers which takes in a layer and then if that layer the name all right here if if this conf is in the layer name we are going to carry out the quantization so we're going to apply the quantization on the conf layers so here we have layer okay so in the case where we don't have that we'll just return the layer itself so the layer remains unchanged whereas conf layers will become quantization aware so we have this apply uh matter right here which will run there we go now once we we have this method defined we'll make use of this clone model method right here to create a new model but one which takes into consideration a certain clone function we get back here and then we paste this out oh we wouldn't making use of this in input tensors we'll just make use of the clone function and our clone function here is this apply quantization to the conf layers so that's it now you you check this year if you check out here you'll see that wherever we have the conf layers you see like this one we have this conf uh yeah we have this count for the depth wise convolutions and so on and so forth now you could also include this for the expand and reduce layers but let's just work with only those counts so that's it you now understand how to pick out certain layers or how to leave out others from the quantization awareness process so from here we have this apply right here and then we'll call this our quant aware efficient net so that's it we have this quant aware efficient net and then we run this uh no this model this model here has to be our our pretrained model so it's our pretrained model we run that again and now this should be fine okay so we have our quantize our model which is now quantization aware and you notice that when we do quant aware efficient net summary we should get something slightly different from what we used to get in um we have in this now oh this is no this should be func so we should have func model let's run that again and run now let's run this let's get back here and you see we have this quant aware efficient net and you will now notice that let's get back to the top you will notice that wherever we have this uh conv layers see wherever we had the conv layers we now instead have this quantize annotate so as we scroll you wouldn't see a conv layer but instead we have the quantize annotates so that's it but uh yes yes because this didn't have there's no conv in this name so we could as we said before we could include this as we expand and as we reduce um layers so that's it we we now have this quantize annotate layers which wasn't what we had before making this model quantization aware or some layers of this model quantization aware so with that now we are done with the annotation and we are ready to make this actually quantization aware so here we call this quant aware model um yeah this is quant quant aware let's get this exact name right here quant aware efficient okay so we call that quant aware efficient there we go that's it and now we have our quant aware model let's run this again and then see what we get as a summary and that's it is now quantization aware so we know we no longer having the the annotations but now some wrappers so here you see the the layer name but now we have this quant which is added to these layers um let's scroll down check out on this you see we have this other ones here and so on and so forth so that's it we we we now have our quantization aware model and we're now ready to compile this model and train it like every regular model we get back to our training right here and then at a level of this compile we have your quant aware model that's it um this this is the same linear rate so that's it okay let's run this and then here we also have our quant aware model there we go so let's run this we're getting this resource exhausted error so i'm going to restart the session and hopefully everything should work fine there we go we started the session and now we able to train anyways we see how to implement quantization aware training with tensorflow and in the next section we'll dive into post training quantization hello everyone and welcome to this new and exciting session in which we are going to look at post training quantization with tensorflow in the previous session we looked at quantization aware training still with tensorflow and now we'll look at how to do quantization for model which has already been trained now as you could see here we have this pretrained model model which obtains an accuracy of 84 percent and a top five accuracy of or rather top two accuracy of 95.6 percent and then in the section we'll quantize this model check out on whether this quantize model occupies less space as compared to the original model and then also verify that not much model performance is lost before we start with the quantization process we should note that we are going to be using this tensorflow light library now this tensorflow light library is a mobile library for deploying models on mobile devices microcontrollers and other edge devices so this means that in these environments where the compute resources are limited it's important for us to quantize the models since we will now get smaller and lighter models and also faster models here we have a general overview of how this works you will see you pick a model like for example efficient net based model convert this model to tensorflow light using tensorflow light converter which we are going to see shortly we then deploy this by taking a compressed version or compressed tf.light file now we were already working with for example Keras files which are hdf5 files now this tf.light is some sort of compressed file and this compressed file will be loaded in the environment in which you'll be working in and then from here we will also quantize this from 32 floats to 8 bit integers which can run on devices with low compute resources so here in the documentation we have here tf.light and you have tf.light converter basically this tf.light converter as the word goes is going to convert your models into the tf.light format and here as you can see these examples you have the tf.light converter from a saved model from a Keras model from a function from a GX model so let's say for example we work with a Keras model just have the model passed here and then you generate this tf.light model from this model making use of the tf.light converter now apart from these arguments we also have the attributes so here you could specify the optimizations the representative data set which is very important in the case where we're working with static quantization remember that with static quantization we have to obtain the scale and zero point values by making use of unlabeled data so basically as we had seen previously all we need to do is to pass in the inputs which in this case are images and then these values will be inferred from the model's interaction with the inputs then we have the target specifications inference input type inference output type whether to allow custom operations or not and then whether to exclude the conversion metadata or not so that's it that's our converter right here now we paste out this code from the documentation and then let's let's take this first here now apply some of the attributes that we have this converter dot optimizations let's get back to the documentation the optimizations and here we could set this optimization with this tf.light optimize so let's open this up there we go you see this takes different values we have default whatever we want to optimize for size this is deprecated does the same as default optimize for latency does the same as default this one is experimental hence subject to change so what we're going to do here is simply take this default so that said we have the tf.light tf.light.optimize and default just as we had in the documentation right here now we could also specify the inference input type and the inference output type so let's get back here we have there we go converter inference input type is going to be unsigned int eight and then let's copy paste this we have the output type which is going to be the same so that's it so we specify this the inference input type and the inference output type and now let's specify the representational data so this representative data set right here which in fact is a generator which permits us output the input values because recall we all we need in the static quantization is just as inputs so here we have the generator which yields the inputs and then here we just say converter dot representative data set equals a representative data generator here we have our training data set now we could take we could take all our training data set or just a few we obviously don't need to take all the data set so we could just take like 20 and use that to obtain the values for the scale and the zero point now if you new to this notion of scale and zero point it's important you check our previous sessions where we treat this so let's run this here we also run this and now we set to convert oh this model here is pretrained our pretrained model so let's run that and that should be fine so we now set to carry out the conversion now we're done with the conversion we are now going to save this in the tf.light format so we have this path and this path we're going to have this file so let's run the cell and that's fine so we get that we have the the file size here and when we check this up you see we have this 21 megabytes so we're going from this model which we could check out here we're going from this model which is 90.7 megabytes to a 21.12 megabyte model now before moving on we should note that if we want to implement dynamic quantization here then we wouldn't specify this representative data generator so that's it let's get back we install this tensorflow light runtime now talking about installing the tensorflow light runtime once we already have this tensorflow light file right here if we want to run this in some other system say for example want to run this in our raspberry pi all we'll need to do now will be to install this runtime and that'll be it we wouldn't need to install tensorflow any longer so we just have this we run this that gets installed we import the the tf.light runtime then we prepare our test image so we just run this we've seen this already we have our pretrained model we're going to get the argmax and then the corresponding class so that's it we should get angry see it matches with what we expect we could try out this other example here let's run this and there we go so this our model now we're going to use this runtime to run our tensorflow light model now we've restarted the session and you'll see that without tensorflow let's let's let's do this let's say tf zeros and one by two for example we run that and you see this is not defined so we have no input for now now let's take this off and then we get back up here we install our runtime import the runtime uh i think we'll be needing numpy so what we'll do is we're going to take this numpy so we try we tend to work without necessarily needing tensorflow so we have this imported as numpy that's it here we have this test image open cv so import cv2 that's fine uh well here we we make use of tensorflow but we will see how to get rid of this dependence on tensorflow so first of all here we have to note that tensorflow was used here to convert this test image here let's print out our test image to convert this test image which is uh an unsigned int with it be so here let's do this you see it's an unsigned int i wanted to convert this into a float and that's why we made this uh change here so here we wouldn't need this any longer and here we could use numpy so we have that and everything looks fine okay so we have this uh test image and that's fine so now let's run this and we have our image now image is not defined let's get back here oh no this is the test image test image run that again this should be fine okay so we have now our image and then here we see we have this interpreter which loads our tensorflow light file which we've saved in the drive and then we allocate tensors once this is done we move on to get the details the input and output details which we had from the conversion process now here here we have uh the unsigned int and here we also have the unsigned int and then you would see that um we have this test image which we would change the type so first of all you notice that this image is tend to a numpy array which we don't need any longer because it's already numpy and then even this type we do not really need to do this although um if you print this let's comment this section and if you print this input details and we get the data type you will see that we have an unsigned int now tf not uh defined well here we use a tf.light so let's let's have this run that again we have tf.light runtime has no attribute interpreter now what we'll do is we'll have this dot interpreter okay so we should have this and this should work now okay that's it now we have this we run this again and that's fine so you see we have you see the unsigned int which is what was expected because when doing a conversion we have specified that we wanted this uh data type for input and our output so that's it let's take this off what we're saying is we don't necessarily need this step right here so uh this will be useful if we had this as a tensor as a tensor tensor for tensor and if we did not have this as an unsigned int already so now that we have that you see we set the tensor so here we have our test image that's it and then the input details index you could print this out so you see what's in here input details index as you could see it's zero though now in this line we get an error we got three but expected four for the input so let's get back here um oh okay here we here we had this change to in let's have we actually let's get back so what we're saying is here we have this expand dims to get from three dimension to four dimensions and the name was still um so let's have that we run this again here we have our test image now which has uh four dimensions and then now this should work so here we set the tensor and then we run the inference so that's it we run the inference here and once we run the inference we should be able to get the tensor uh at the level of the output so let's take this off and then run this now takes a while yeah this there this is the inference uh process now it should be noted that tensorflow light has been built for mobile and embedded CPUs so uh general purpose CPUs like this collapse CPUs aren't the best match for tensorflow light models in terms of speed now we print out the output uh i could just let's print out the output see what's what's in there see there we go you show that we have this here the highest value then we could do np.arc max of this output right here run that again that's it uh now let's do let's take this here so we could get the class name automatically then now let's run this and we get the class happy so this is what we expected and again we've done this without having to import tensorflow so we did not we didn't need tensorflow once we already had our tf.light model you could see here tf not defined now our next step will be to measure the accuracy of our tensorflow light quantized model so here we do exact the same process as we had before we basically have the model path and then this input details output details and then we go to our validation data set take a hundred elements in our validation data set so this means that we'll have to import tensorflow for this process since we are trying to evaluate the model's performance so here we have this we'll need the validation data set so we'll need to get back and run this cells here so let's get back here we will run this there we go we will be running all the cells now when we want to import this when tensorflow is already imported we get in this error so what we'll do is we just have this here we run that there we go and then when we get to this accuracy you will notice we use that the tensorflow light so it's tf.light not tf.light so we're using this model from tensorflow and not from this package which we had installed here so that said now we have that set our accuracy we have input output as we're saying our validation data set test image which is going to be passed here the inference we get the output we compare if they're the same we increase accuracy if not we skip and then we move on but to increase the total and then from here we have accuracy divided by total or let's let's say positives divided by total or let's say correct correct yeah correct predictions so let's change this to correct so here we have correct predictions and that should be it so we we have this accuracy here and then we specify the model path let's get back here and then take this path of our tf.light model now we we have simply accuracy accuracy and then we specify that path okay let's have this and we run this now there we go we're done with computing the accuracy for our tf.light model and we get 0.82 that is 82 percent as compared to the 84 with the original model now to get the more accurate value for this it's advisable to use the whole data set so you could take this off and so now you have your model which performs at 82 accuracy and which now could be deployed in some mobile device hello everyone and welcome to this new and exciting session in which we are going to look at apis now api stands for application programming interface and in this section we're going to look at why we even need apis and also how they work now supposing you've just built this model right here let's call this a model m1 and this model can take in an input and produce an output the question is how do i make my web app like this one or my mobile app or even my desktop app access this model which i've built such that a user of let's say this mobile app can and just by pressing a button get access to this my model's predictions the way we could go about this is by making use of apis that's application programming interfaces as defined here on the g2.com website an api permits software development and innovation to be easier by allowing programs that is for example your web apps to communicate data and functions safely and quickly apis does application programming interfaces accelerate innovation because more developers can build products based on existing data and functionality getting back to our example this means that thousands of users now on the web can get access to our model and make predictions without these developers of these web apps or mobile apps necessarily mastering the art of model creation to even make these concepts clearer let's take this example so here we're supposing that you get into a restaurant you give a command to the waiter who then takes this command and then tells the cooks to make available the food you have in the command then once this food is ready it sends it back to the waiter that's the api and then this waiter now passes on this food to you or then eats it and is happy now in the case of computer software where we have say this mobile app which has to communicate with this api right here then this communication has to follow certain rules or protocol known as the http this protocol guides the way information is being transmitted via the web now http stands for hypertext transfer protocol so this is hypertext then transfer protocol now the type of data exchange could be text could be images could be video could be any kind of data which is understandable by both this client right here and the server also communication following this http protocol is connectionless that is each time this client needs to communicate with the server to know what objects are found in the particular image a connection is created between these two now once the connection is created and then the client receives the output that is the particular location where the objects are found in an image that connection is closed and so we we now have this which is taken off here then once we want to connect again with the api which is found in the server we still create or recreate another connection then also as we have said the data exchange here isn't of a particular type so really this protocol doesn't force you to pass or receive a certain type of data so far as the data you're passing or receiving is understood by the particular entity which is sending or receiving then everything works just fine then the last property of http protocol is it's actually stateless that is once you make a request and then you receive a response no client information is being stored in this request response cycle hence once data is being passed to the api and the response received you should not expect to be able to retrieve this data which was being passed to the server from this point we are going to go in depth into how http works so you get to this address www.postman.com you could sign in or sign up in case you do not have an account with postman now i have an account so i'm just going to sign in you get to this point here you see you have workspaces my workspace and then you have this link here which comes by default you have this url which is this information you pass in each time you want to gain access to a particular site in this case it's postmanico.com now url actually stands for uniform resource locator and so this address right here permits us get access to certain resources which are located somewhere in the web and this brings us to the http methods if you click right here you'll see that we have different options like if you have you have the get you have the post you have the put you have the patch and so on and so forth these are known as the http method and each time you want to get access to a resource which is located somewhere in the web you have to specify the exact method so like with a get for example we are saying that we want to get a resource for the post unlike the get method where our main interest is to retrieve some data here we submit an entity to the specified resource which often causes a change in state or side effects at the level of the server so this means that we could make use of the get method when we want to retrieve a user's information and then we make use of the post when we want to modify some data or add information so like for example if you want to get ready to start on a platform then make use of this post request as we're going to add a role on the database so suppose we have this database right here where we have the user's id we have the user's name and for example the user's password so here we have this small database and then we have id let's say id 0 we have the name fred and password let's say whatever value we have here now then we have id 1 let's say sally and then we have whatever password here then we could take 2 we have rita and we have whatever name we want to have here or whatever password we want to have here so this is it now with the get request we could just retrieve that fred has an id of 0 its name is fred and the password is a given password whereas with the post request we could add a new user so that's why they speak of constant change in state or side effects on the server so we could actually add this new user by passing information via this post request so here we could add this third and we could add say mac let's say mac and that's it so that's it for the post request now with the put request we could update this data so this means that here we could say we are not interested in maybe modifying the name but if we want to update this password we could change this password now to some new password so now we could update this row right here in the database now we could also delete so we could just simply take this off with our delete request and that will be it for that row we also have other methods like the head the connect options trace and patch which you could check out in this document right here but most times we make use of the get post put and delete then also those responses have some status codes which we have seen already so you maybe have seen 200 for okay so okay when you make a request and it's okay you could receive that let's let's make this request here uh url empty let's get back here and you see you see your stitches let's highlight this you see your stitches okay so this uh uh as we have seen here let's get back uh this as we as we have seen here http response status codes so you could again check in this documentation and have uh every detail about the different status codes and you need to understand how to work with them as they are very important when you're dealing with apis so here we have informational responses successful responses lying between 200 and 299 and that's why you see here we have this 200 status code which is a success and then we have redirections we have client error and we have server error so this error is coming from the person sending the api request then you'll be you'll be between 400 and 499 the errors from the server will be between 500 and 599 so when the server is down sometimes you'll see this 500 so you could check your internal server error the server is in color situation it doesn't know how to handle so you could check out all this in this documentation now let's get back here and make this error so let's let's put in whatever value one year write that again and you should see here you see 404 not found so you you get here you see 404 client error so this error comes from the client we are going to paste this out here and then select the post request get to the body form data this is the information will be passing in so here we're going to have email teams at neural learn.ai the password not a password and then job neural learns and then click on send okay what do we get you see we have this output right here we have our email the username the job and a token status 200 now you'll notice that unlike with a get request we have now introduced this body data so if we don't pass this body data you see that we wouldn't have the right output and this is because to login to just any platform obviously you need to pass in your credentials which in this case are the email the password and the job so this tells us that when doing an API call like this one so when trying to make the client communicate with the server we need to specify the HTTP method we need to specify the URL then we also need to specify the body which in this case is all this information right here that's the body information and the header information now this header and body information could be broken up into the request and response so this is the request body and here is a response body and then here we have the request header and the response header which you can get by simply clicking on this right here you can also check out a list of HTTP headers on this developer.mozilla.org platform right here then at a level of this response headers take note of this content type which is Gizen. Gizen actually stands for JavaScript object notation which is a very easy and lightweight format for storing and transporting data from a client to a server and vice versa. The Gizen format is programming language independent and so if we have this client with code reading in JavaScript it can communicate Gizen data to this API via this request which is reading for example in another programming language like Python and then after processing data the response in Gizen format will still be understood by this JavaScript client. Now to better understand the Gizen formatting let's look at this output which was generated after we made the GET request on this postman echo API right here you would first notice that it starts and ends with this curly braces so we start like this and we end this way and then we have information stored in key value pairs so we have the key and then we have the value and then each key value pair is separated by a comma so we have the first key see here key and then we have the color and then we have the value we have a comma and then the next let's say key one value one and then next we have key two value two and then separated also by a comma from the rest and so on and so forth so here you see we have this is key one here key one value one we have the comma and then we have key two this headers key two headers and then value two now value two years all this year so all this data is our value two so all this so value two and then we have a comma just like here we have this comma and then after we have key three and then value three now for a particular value we could also have some sort of dictionary and so in this case let's take all this off in this case where we had this empty dictionary this was quite simple but here we have this dictionary filled with its own key value pairs so here we have this key and we have this value and then this value is a dictionary made of its own key value pairs so you have the commas again and just like that so we have the commas uh separating each and every key value pair and then here you just have this string so basically what we have in here is we have in a key which is a string and we have a value which is some variable now this variable can be a dictionary like in this the first two examples or it could be a string or it could be an integer or even a boolean now if you check on this other request here the push request you'd see this same formatting so we have the curly braces which open and close and then we have each key and its value the key is value key value and finally key value now this was for the response so this was the response body let's go ahead and check at the request body request body we had this form data but actually we could have raw gson data which is passed so instead of using this like basically using this gui we could have this raw and then pass in gson data so you see here it's actually very easy to to write this out let's have this year let's check back at the form data we have email password and job um yeah we have the same so we have email there we go we specify the email uh neural learn dot ai and then we have the password our password is not a password so not a password there we go next on final one we have the job we have here new uh learns okay so we have that so notice how here we considering this raw data which we're going to pass in and we're going to see the output we get here so let's send this and what do we get here's what we get as output you see here unsupported major type text planing request so this means that the request we sent uh was text type and not the gson format so you'll notice here we have text which was selected and we just simply have to change this to gson so changing this to gson what do you notice is he changing color that is because this now knows that here are the keys which are in red and the values in blue it takes back to you see turns back to all black let's get back to gson and then rerun this so click on send and let's see the output gson parse error check that yeah let's take that off and then send again okay so you see now that once we correct that and send this gson data we have this output so basically using this form data right here which we could now take off let's take this off is the same as passing this raw gson data so the rule of postman is to make it easier for people to test their apis using these kinds of graphical user interface but you could always actually do it by passing this raw gson data as you can see here and so now you have the status okay whereas when we had this text let's send so you could see the status you see 415 unsupported media type let's get back to gson and send and that's what we have you also notice again that level of the headers we have this gson format so the body the output we have here is gson data then instead of passing this let's add in some random stuff here and click on send what do we have we have for not found but we'll notice that the content type that we have now is no longer the gson but instead html and when you check out the body you see we have this year preview page not found this shows us that we can have different content types exchanged between the client and the server hello guys and welcome back in the session we'll see how to build our own apis using the fast api framework in python language previously we have seen that we could wrap our deep learning model or more precisely our object detection model into an api and then let this clients consume this api that is allow them send requests like this and then get responses so yeah in black we have the requests and then now we have the responses in red and all this is done using the http protocol and so now that we understand the concept of an api and how this can interact with millions of clients everywhere in the world we are going to move straight to building our own api and to build our own api we are going to make use of this python framework known as fast api now other python based frameworks which can be used in building apis are django and flask but in the recent years the fast api framework has gained much popularity amongst python developers thanks to the fact that it comes with some key features like its speed so it's a high performance framework time taken to code is reduced that's now developers could now build features even faster with fast api you even have fewer bugs it's more intuitive it's easy it's kind of short robust and it's standards based so because we need all these features while building our object detection api we kind of have little or no choice but to turn to the fast api framework and you're going to see how easy it is to create or build a fast highly performant and robust api very easily using the fast api framework first things first feel free to check out this documentation at fastapitiangulo.com working with the fast api documentation doesn't really feel like a usual documentation as most of the concepts are well explained and broken down to enable just anybody with minimal python knowledge understand and use fast api efficiently so right here we have the features fast api people could get straight into this tutorial here user guide you see you have this wellwritten user guide you also have the advanced guide so once you're done with this user guide you could get to the advanced user guide you have special topics like on currency deployment project generation and all this and so before we move on don't forget to start the fast api github repository one of those cost purposes is having used python at a very basic level and so we're supposing that you've already installed python without which you could head onto python.org download and install python based on your own operating system so here you could see we have python installed python version check that out 2.7 and then python 3 version we have 3.6 this should be 3.6 okay there's an error there we have 3.6 okay now the next step will be to install fast api so we head back to the documentation you have here this installation all you need to do is pip install fast api all you could do pip install fast api all so you get other optional dependencies and features we'll start by installing fast api we have pip3 install fast api there we go api now installed we could do python 3 and then you import fast api you could check out the version right here we have fast api and then we have the version okay so that's it 0.78.0 so the next problem we may face will be what if we have this project this object detection project in which we have the fast api version 0.73.0 for example installed let's say 0.73.8 installed you have this other project on optical character recognition which needs a different version of fast api that is 0.75.0 you'll find that with this there'll be a conflict as what we've installed is this 0.78.0 version now the way we could solve this is by making use of virtual environments the way virtual environments work is you could have this project this object detection yellow x project have its own isolated python environment with its own interpreter where you can install these libraries in this python environment without it affecting this other project which is with another or which is built in another virtual environment where here we have a different set of libraries and library versions installed so this means that in this environment here we could install a version of numpy say let's say whatever version and then here again we could install another version of numpy which could even be the same as this version but the difference now is in this virtual environment here we have a version of fast api which is 0.70 0.78.0 and in this other virtual environment we have this other version which is 0.750 and now this problem of conflict at a level of the dependencies is resolved now that we want to create our python virtual environments we are going to make use of this python model which is the python vamp model so here we'll simply do apt install the python python3 vamp and that's it so here yes okay and that should be installed now we have the vamp installed let's get into our neural learn projects directory and then from here what we'll do is we are going to create our virtual environment we have python3 vamp and then let's call our virtual environment vamp emotion detection detection there we go we should have this created and from here you could do this and you see that we have this vamp motion detection here created let's get into this vamp emotional detection directory so let's do vamp emotion detection and there we go you see we have bean include leap leap 64 by vamp and share now let's get into the the bean so there we go we have bean and then you see we have this activate right here now let's get out of this and then let's do source oh well let's get back again another step so let's do source bean activate and we get in this error well let's get back into vamp emotions detection and then let's redo source bean activate and there we go so one thing you could notice now is the fact that previously we have this root and this but now we have this vamp before this so we see that everything we're going to be doing from now henceforth will be in the context of this our virtual environment now let's do python3 there we go let's import fast api you can see that we have modeled not found so it's telling us that fast api is not installed now let's get let's exit and then deactivate the way we deactivate by simply typing in deactivate and you'll find that when you deactivate you do not have this any longer so we are no more in the context of the virtual environment so now let's do again python3 and then let's import fast api you will find that here we have fast api installed you see we could also get the version you see we have fast api installed but in this virtual environment fast api isn't installed now let's exit and get back to our virtual environment so we have source bean activate there we go and now let's do keep install fast api so one thing you could notice the fact that we go out of the scope of this visual environment we have this version of fast api and now we could install another even different version of the fast api so that's it we install fast api and now that fast api has been installed like we could do let's in this visual environment we could do import let's do iton3 and then let's import fast api and there we go so we have fast api now installed we could get this version and you could see clearly that this version is different from the other version of fast api another thing we could do is create our virtual environment with a specific version of python so here we could have python3.8 and then we have venv let's say emotion or let's say emo detection okay so there we go we run that and then we create this new visual environment where the default version of python is that of 3.8 now let's again have this you see we have this here venv emo detection and then we could get into that emo um detection and then let's activate this um there we go and see this right here now when you do python you see that the default version is this 3.8.13 so again here if you try to import fast api you see model not found let's exit and then keep install fast api all and run that you'll get to see now with the speed freeze that we have many more dependencies installed because we decided to do that so here you see we have type in extension starlets nephio uh pydantic which is very useful and those others now the next step will be to install uvcorn and uvcorn is an asg server so asg here signifies asynchronous server gateway interface being a http server this uvcorn right here is responsible for taking requests from different users so supposing we have these three users right here the one to get for example some predictions from a model which is here in our fast api which we've bundled in this our fast api code so what we'll go on here is this user for example will make a request see to this server and then the server gets that request interacts with fast api obtains a response and then sends back this response to this user now if we want to connect with this web server locally we could pass in this ip address here which is that of the local host and then we specify a specific port such that we now here does the client sends the request which contains the method which could be for example let's say a post method and then the request body and headers such that once this web server receives this it processes the information and produces the required responses we are now going to dive straight away into some particles so here you get this fast api you have this simple tutorial here which we are going to follow and see how to easily work with fast api right here we have this which we are going to copy to open up the code with visual studio we'll get back and then we'll just write code and there we go so um now let's actually get out of the spy user mode and then we get back in here um neural learn there we go and then now let's just do this again and you see we have our visual studio which pops up we have our neural end projects folder and we could create this new file so here we have main.py create this new file and then we paste out that code now if you're working with vs code is recommended to install this python extension so we just go ahead and install this extensions see we have that installing let's get back to the code let's just get back to this so here for example let's like this so it's clear here for example you see we try to create a file main.py and with this code we are going to put in there now to keep things simple we are going to comment this region and then also comment this right here so you're not going to make use of this we're going to comment that and that should be fine okay so we we just have this portion of this part here now you see here we have this app which is an instance of fast api see we've imported this from fast api and then we have this decorator here and we have this read root method now this read root method we could actually change this so we could say read neural learn root or whatever name you want to give it so let's call that read neural learn or let's just keep it short let's just say read so we have this read method text and nothing and then it returns this dictionary or this gson hello for the key and world for the value we have this decorator right here where we specify the meta type recall we had different methods like the get the post put the delete and so on and so forth so here for example we specify this method so this is the get method and so if we are making use of this get meta right here we have this path here we have this path and then this means that if our web server is hosted on an address for example let's say htps whatever whatever.com then this root path will correspond to this path right here and then if we want to do say a login then we'll have your login this login path will correspond to having this login right here obviously we'll have a post so change this to post and that's basically how it works you see how easy it is for us to create these kinds of paths with fast api and also specify your methods very easily so here we have that and we could have slash or let's say a whatever we want to have there and then all we need to do is we have this here a now obviously we could have some variables here and we're going to look at that shortly with this other example below anyways we understand that we want to get to an address here whatever.com or let's let's even say google.com let's say we're trying to build google.com then we will just have here this and that'll be and so with this now or specifying this is going to get into this method and return this gzn now to run this we'll just use what we've been giving here in the documentation let's copy this out and simply paste this out here so here we're going to open up the terminal so open up this new terminal there we go clear that's it and we neural and project so make sure that you're in the same folder as where your main.py is found we could do uvcorn and we run that error as the app attribute app not found a model main so let's check this out let's save this and then run this again there we go see now it works we have started server process waiting for application startup application startup complete so it waits and is completed and now uvcorn running on this address on our local host so what we could do here is we simply control and we click open this so control click and it opens up this page right here there we go the first thing we notice is this message where we have method not allowed but why we why will we have this method not allowed message the reason why we have this is because here let's open this the reason why we have this is because here in the code we actually use this wrong method see we are supposed to use a get method instead so we should change this and have this get so let's save that again and then we would stop this and then run that again click on this open this up and there we go we have as we expected the JSON output here we have the id or like the key hello and the value world now another thing we could do is we could get back here let's stop this see we stop that and then let's do um let's say entry point let's have that and we save this so we save this and then we run this again and then we click on this and you see little not found so here because we do not have this path we output not found so let's let's do this let's say entry points and run that and you should have an answer um let's check this out here we have entry point and then here we have entry point entry point um let's get back here and then we should have this to resolve this issue we have to put the slash before the entry point so you have to be very careful with the way you write this now you save that again and then you reload but aren't you tired of always having to reload i guess uh as myself you would also get tired of doing this so let's do this instead of having that we'll add this reload here so that once we save or do modifications and save here automatically our server is reloaded so now we'll we'll run this uh application startup started let's click on that we have this and then we do entry points that should be fine so that's it we have hello world as we expect now let's go ahead and update this let's just say entry you see when you save now you see here you see it's shutting down waiting for application shut down application shut down complete finished server process started server process waiting for application startup and application startup completed so now we have this uh which has been reloaded so unlike previously where we would have to stop and then start again now uh once we make the modifications it's automatically reloaded so let's get back here oh not here let's get back here oh not here actually let's get back here and then we are going to have uh let's run this you see doesn't work but when we do entry it works now and we didn't have to stop the server for that the next thing we'll look at is another interesting feature of fast api which is the swagger ui so here we'll save this reloaded our server and then we get into this and instead of having that we'll do docs so you see we have docs so we have this page which pops up and you'll notice that we didn't need to specify that in our code so let's get back here and what we see is we have this uh method here and then when we open this up see that does a path and then we have the parameters there no parameters so we have this simple get request and then we have the response so here the the code 200 and that's a successful response and then the media type uh gizen and that's it so now let's go ahead and try this out so you see when we try that we click on try that out and then this comes up we click on execute and what we have is this output right here so here you could see the response body and there you could have the response headers so that's it you could also repeat this on the post map so we have that you see the request url and then you have the curl um this you could use also to make the api calls so that's it we see how to make use of these docs and now what we'll do is we are going to uncomment these two parts here so here we uncomment this and then we'll uncomment this part and then we save this and then you see here we have these items now we will notice that when we get back to our code and then we refresh this we have this automatically put out here so this means that no matter the number of different parts we create we can automatically test them without any extra effort thanks to this fast apis swagger ui so like here for example you not only can test but you can also see the documentation like here we told that oh this is an ism id it's an integer it's required then this is a string cure query it is optional because here you see it's not required and then you could try this out so let's go again try that out we uh you see well let's cancel and then you see you cannot do anything here once you want to try out you could put in an id so let's let's not put anything there let's just execute you see we must put out something so let's have that 20 20 uh the string let's let's leave that for now and execute so what do we get as results is uh this output item id 20 cure null now if we modify the code right here so let's modify the code and then let's just say uh item id save that we get back here and then let's reexecute actually let's reexecute what do we get you see it's auto automatically we have these modifications and then now we could do this let's say hello we execute that and you see here we have our optional string which has been shown to us in this uh gizen so we had seen the gizen format already here we have the key we have the value the key we have the value and so on and so forth and then also we have the response headers getting back to the documentation you could see here let's scroll back up uh no let's let's get down you could open this up you see the main the file the app the object created inside of main.py with a line abacore files api reload make the server restart after code changes only do this for development so uh they're saying that if you're in a production setting that if you've deployed this as we'll see in the next sections where you want to deploy this you don't want a situation where you get to have this reload automatically so from this now you see uh this basically explains what we've said already uh receives HTTP requests in this path and in this other path so we've seen this already now both paths are get requests so we have the get methods the path this path has a parameter item id that should be an int this other path has an optional string query kill so that's it and then here you you you they get to talk about the swagger ui and then you also have this alternative doc api docs here which is redoc so basically what we'll do is we just get here and then instead of having all this let's say we have that we just do redoc we run that and you have this alternative uh ui here where you could still again test this apis so that's it uh you could test this very easily you see we have that and then you should be able to test this now you have the response samples from here so for this you have this response sample uh you have the other successful response or the response is unsuccessful where we have this 422 now you could check out in the Mozilla platform where you see the exact meaning of this 422 so let's let's check out developer mozilla 422 so you could always get back to all this so you don't have any issues understanding any code you receive as output so here you see unprocessable entity and that's it that's it for this here we have different responses that's fine the next feature we'll be looking at will be that of schemas what we'll do here is we'll simply take this off keep it simple we'll not use this and then we'll change this into a post request so we have this post request right here which takes in a body obviously we have a body and then it returns the elements in this body now here we'll take this off and then we have our post request so there we go we have our post request it's no more read item but now add item now we here we're going to have an item which is going to be of type item so this is going to be a special type we're going to create now here we are going to import pydantic or from from pydantic we're going to import the base model so that's what we're going to do we're going to import this base model from pydantic and then we're going to create this item this class item which is going to be a subclass of this base model class right here so we have this item and then we have base model so that's it and then here we're going to say okay we want that an item should have a name and a price so we have name which is a string and we have a price price let's let's say integer let's say we our price and integer okay so that's it so there we go well what we're going to receive here is this item of type item it's no longer the usual types but now this special type item which we just created so now here we have item and then what we'll do is we're going to return this so let's call this item name here so we have item dot name so we get the item we return its name and then we also have the item's price so item price and then we return the item dot price okay so that looks fine let's save this and check our code here or check this API let's refresh this and then here we see we have our post we check that out let's try this out execute what do we get we get in response body item string item price zero now if we come here if we get here you see a level of this name and then we put let's say back and then price let's say 20 and then here we execute what do we get you see we have back 20 if there's something you can notice is the fact that this request body here automatically takes this name and price so get back to the code you see that if we modify this let's let's say we want to get the name want to get the price or let's say we want to get the cost or let's say we want to get a discount so let's say this is discount on the item discount and then let's put this int okay we save that and then we get back here we refresh this try that out and you see discount automatically is here so the role of this schemas is to provide a model which the request body has to always follow so this means that if you get back here let's get back here and let's take one of this off you see take one of that off we execute check out the response you see here expecting our property name and closing double quotes all of that you see here we have this unprocessable entity error now this is because here at the level of the request body we didn't specify the discount which we've already said in this schema that we need to always have so doing this means that each time anyone has to make a call on this api on this endpoint right here that person has to specify the name the price and discount for the request body now we could also enforce this kind of measure at the level of the output so we could specify a response body and then we define say let's say this is input item let's call this input item and then here we would have output item so we want to create another schema here let's copy this and then we're going to say that okay we want this output to be the price minus the discount so here we have output output and then instead of the price and the discount we just want the selling price so here we have selling price and that will be an integer okay so we have that now we have the output we have the input and we have the output schemas they're both subclasses of base model and then here what we'll do is we'll say okay we want to have the selling price we'll define the selling price to be the item price minus the item discount okay so we have that so we have this now and then here we have the item name still and then we have the item we have to be careful the same as this so we have to say selling price anyhow let's leave that for now let's just say we have here selling price so let's say we have selling price and then let's save this and check this obviously already post got an unexpected keyword argument response body this is actually a response model not a response body so that's the output response model let's let's let's change this to input so now this input item for the request and then this output for the response now let's save this that's fine let's go ahead and check this out so here we try this out there we go you see as usual that's fine and then let's let's give this a price let's say 20 and no discount and then we execute so what do we get your internal server error now let's get back to our code and check out what's going on here field require this missing now you see that everything looks like normal but we told that there's a field that's required at the level of the output and the reason why we have this is simply because this output expects to see selling price you see it expects to have selling price so because we do not have selling price here we get in that error now let's save this we get back here i'll refresh that and then try this out again execute no let's let's put 20 anyway it should it should be the same it's just that we want to have to see the difference in the or to see how the selling price is computed here again we have internal server error now this looks funny let's get back and see where we get an error we told field required and it happens that we even do not specify this correctly so here this here's name price discount and this is name so here we should also have name so not item name we save that again we go ahead and check this out here um let's refresh that try this out let's just execute and that's fine so you see we have that now and this is because we've put out our outputs correctly so let's have here say 200 and then 180 or let's say 20 and then we execute that um and see we have this output so that's it we see how thanks to this kimas you could now build apis which will meet the developer or whoever's making use of your api to have to specify this without which nothing is really going to work so this means that you could have a situation like here let's let's see we have this input item and then we let's let's see that again and get back here we refresh this page and then let's try out while taking off this so we take this one off and then we go ahead and we execute you see that the output here 422 is a client error so this means that the client knows that this error is coming from his or her side now we get back here and then before we move on we are going to define this structure or this code structure for our first api projects we'll be working with so what we want to do is separate the apis that's actually the end points from the core now when we talk about core we're talking about let's get back here we're talking about carrying out computation like this so you see this computation we want to have some separate file which is in charge of doing this in the case of some deep learning model want to have a separate file which will be in charge of say taking the inputs loading the inputs passing this input to a model doing some processing and then from there now the output from that is asked on to this end points because here this one end point and this another end point so this like read this add item now getting back here you see this like having read here this one endpoint read is under endpoint add item this may be under endpoint say update item so like root you have get items post items and so on and so forth so now at the level of the core we want to have our business logic in our case for this simple example our business logic was basically just taking the the price and then subtracting the discount so that was our business logic but we want to have this in a separate file now for the schemas we want to also have this in some separate file so we want to have this kind of structure where we have the core that's the business logic schemas and then we have the api in charge of all the different end points now to do that we are going to create our service so from here we have we're going to add a new folder which we'll call service and then in the service we're going to add a new file which is no basically let's copy this mean into the service here so we could do this we could copy that move that now let's stop the server okay we've moved this into our service and then in the service we have a new folder which we'll call api and then we'll create another the new folder which we'll call the core now in this call we have a new folder which we'll call the logic and we also have another folder which is called the schemas schemas okay so we have this no this is this is this call has this new folder which is called schemas and not the logic so let's let's delete this folder here let's delete this move to trash okay so we have that we have the car which has logic and schemas and then we have the the api now which will have different endpoints now given that we need to carry out our inference making use of this quantized onyx model we are going to start by pip installing onyx runtime so here we have pip install onyx runtime and there we go now we're getting this error so you have to make sure you run this as a super user we now we run this as a super user we have pip install onyx runtime that's now installed you see we could do python and then import onyx run time and that's fine okay that said let's do pip install open cv python we could also check that out let's clear this and python and import cv2 and there we go so you see we have now the onyx runtime and open cv installed we will then move on to create the endpoints so in this api we have this new folder which we'll call endpoints and then here we'll create new file which we'll call detect.py and in here again we have another file which we'll call test.py now this detect.py here will play the role of actually being that endpoints through which we are going to carry out the detections and then this one will be the endpoints through which will test if our api is alive or not now in our detect.py we are going to import the api router so from fast api import api router there we go so previously we had imported fast api but this time around we'll be working with this api router then now we'll create this object emo router of the api router class where this will play the role of router information from the detect.py to the main.py now we have our emo router decorator and then we have we specified i want to have this get method and our path will be detects okay so here's our path and then now we could define this detect method in here with this detect method given that we want to carry out detections based on input images we're going to have as input this time around this image and then you'll find that given that it's an image it's going to be of type upload file because the user would have to upload this so that's the type upload file we we have that we could simply import this from here upload file there we go and then once we once we define this we can now get into the body of our method so from here we could start by just let's say we output return return say hello hello world so let's just return this so now we have this method which takes in the image which is uploaded by the user and then returns hello world now to dive into what this detector is going to be doing we'll need to import this image from peel so from peel you're going to import image there we go so we have that imported and then we are going to open up the file so let's let's say we have our image we want to get our image image there we go we open up the file so we have image.open the file which has been uploaded by the user so we have em which is this um dot file the read but this um open method here takes in bytes so would have your bytes bytes IO will convert this to the byte form and then you would simply import that so we have from IO we import um bytes IO and that should be fine okay so that's it we now have our image image which would then convert into a nonpy array so now we have image equal um nonpy array which takes in this image and then now instead of returning this we could have this method emotions detector emotions detector which takes in the image so it takes in this nonpy um array or this guy takes in our image and then is going to tell us what class that image belongs to so for now we have this let's import um nonpy let's import nonpy so we have um import nonpy as np okay so that's it we now have this imported we are also going to check if the image or whatever file we are uploading is actually an image file so we get the the the file name we have the file name and then once we obtain the file name we try to get this extension so right here let's take this off we have that um split method then we obtain the extension and if this is um or it's part of this that's if it's gpg gpeg or png then we know that this is an image and so we're simply going to pass um if this is not a case then we'll raise an exception so here we have um race race HTTP exception and then we will make sure we import this um HTTP exception then we specify the status code as 415 and our detail is not an image so that's what we're going to do before calling on this emotions detector meta right here now before we move on let's let's pip install on pillow because you could see here that we have this um we have the pillar library which is yet to be installed so we're just going to do here pip install pillow there we go while that's installing also take note of the fact that this is meant to be a post request and not a get request so we replace this with post now that we have pillow installed you see we do no longer have the warning we had before so that's it now we could um check out on our test.py this time around we're going to create this test router so here we go we have a test router and then we'll just call this test um testing and here we'll take this off and this is a get request so here we have this get request and we will have this test so this is our entry point and then we could take all this off um there we go yeah we now have we're going to return um testing um oh let's see yeah testing um testing so that's it we have testing testing ipad working so this is what we're going to output when we um have this or pass this through the browser now that we have these two endpoints defined as this test and this detect right here what we are going to do is we are going to create in this api folder the api.py file so let's create this new file api.py notice how this um two are in this endpoints so detect.py and test.py are in the endpoints folder or directory and then this api.py is found in this api directory so now in this api.py what we're going to do is we are going to create this main router and the role of the main router will be to include this detect router and the test router which itself now will be included in the main app so as usual we have to import this api router from fast api and then once we have this imported we're going to import the detect router and the test router so from service remember we've created this already api endpoints detect our detect.py right here we are going to import detect router and then let's copy this space out here now we're going to import the test router from test so there we go so we have the detect and the test router is now imported the next thing to do will be to create our main router we have this main router um api router which has imported already and then now the main router is going to include um the detect router and it's also going to include the test router so we have main router including the detect and the test router so there we go again we can access the app via the detect or test or maybe some other let's say some lambda router right here and then all of this get included in our main router which now is also included in the app back to our main.py we have from fast api this time around we're going to import fast api take that off and then now we we import the main router so we have service api.api import main router there we go so we have this main router imported as usual we have the app fast api then we could specify the project name uh there we go emotions detection and then we have to include now our main router so this time around our app now or with our app we have the include router method which takes in the main router so that's it we have this um now included and we will now turn all this into packages so at this end point we're going to create a new file um init.py there we go on the api api new file we have init.py then for the core we have under logic we are going to create this new file init the py schemas new file init.py under core oops core new file init.py then finally for the service itself we create this new file we have init.py so we have our service package api package and core package now remember we have this emotions detector or meta right here which we had to define we're going to get that defined in the logic so on the logic we're going to create a new file we'll call this onyx inference.py there we go and then here we'll simply create this method um emotions detector emotions detector which takes in the image array and then we're going to follow up this simplest test we had seen already from the collab notebook we start by the import then we specify the providers then we there we go we have the quantized model the image path the test image we just created and then now we finally run to obtain our predictions so let's just copy all this out and then put out in the code there we go let's paste it out here um back here there we go and then finally we have those predictions so that's it now let's make sure we import onyx runtime see onyx import onyx runtime onyx runtime and then also we need to import open cv and what do we need again we need numpy so import numpy as np um there we go so yeah we import onyx runtime import open cv and then numpy now this is our our method which we are about to define let's send this this way there we go and then now we're going to return this output actually it's practically this here so let's copy this from here um let's copy this from here and then paste out here well so that's it we have this now let's modify remember what we'll get in here will be the image array and so um we no longer need to specify this image path let's take all this off take this off and then from here we would have this uh quantized model there we go we have our quantized model we've already specified the the providers so this our quantized model for our test image you see we're going to take this image array and then resize it so let's take this off from here we have 256 by 256 take all this off and there we go now we'll import this onyx runtime as rt so that should be fine now um that said let's get in here and see what we have um we'll print this out well let's just let that for now and then we see we have our predictions as we know already and then from here we have the output now one thing which is left out is this um this file actually we we get to get or put this file in this right position we've copied this onyx file here which we trained already and then let's update this then uh once it's fine let's get back to our detect.py and make sure that we import this emotions detector so here we have from service.core.logic.onyx inference we're going to import emotions detector so that should be fine we have now import this imported let's check that out and then let's save this uh get back to onyx inference and then now at this point we're going to say if if um np argmax onyx spread which is this here let's copy this and paste out here if this is equal zero um then what we'll do is we're gonna have the emotion here let's have emotion um set to um angry happy or sad so here we have emotion equal angry uh yeah that's angry and then um let's copy this well let's say let's have this here let's copy this here paste out here now lif lif this is equal one then this is happy and else the emotion is sad okay so that's it we have angry happy or sad and then what we're going to return here would be this dictionary where we have emotion and then the output will be simply the emotion so that's it let's take this off now there we go we have that set and when we get back to detect.py you see that this is fine we're going to pass the image and we're going to get the corresponding emotion now we have all this set what we could simply do is um run our server before launching the server let's add this root endpoint right here and then now let's do let's launch the server uvcon service main app reload so that's what we have we're getting this error cannot import name detect the detect route from service api endpoints detect well what we'll do now is we're going to create this new folder and we'll well this shouldn't be in here this is going to be here so in um neural end projects we'll create this new folder um new folder which we'll call emotions detection and then now we'll copy this service in our emotions detection folder um let's move that there we go so now we have um in emotions detection we have the service we're going to relaunch the server again you see we still get this error now let's get into this um detect file you see here we have this you see um it's actually emo router and not detect router so if we get here we should have emo router there we go emo router and here we have emo router okay so uh we reload that we took another important name test router well let's get into tests uh we have test router right here this should be fine now that we're getting this error let's make sure we have this here so we we created this um main router which is an object of api router class we save that and then um this should be fine this time around so application startup complete gets into the local host you see we get what we expect we have hello world so this is uh what we get when we pass this in our browser we now open up the swagger ui you see we have this detect here as expected we have testing and then we have the root okay so that's it let's open up this detect and you see we could try this out then we could pick out a file which we could actually test so let's go ahead and test with this file let's open this up and now let's execute so let's check out the output field to execute this and so that's what we get in we get in um an error let's get back here one thing you could see is the fact that this you could request your error right here isn't what we expect we expect to have a slash before the detect so let's get back to the code and then modify this so take this out from here save this and that should be fine so we should have the same with test save that and yeah okay so that's fine so let's now get back and then test this out let's refresh there we go we have detects we want to try this out we want to choose a file let's go ahead and pick a file this file and then let's execute okay so let's check this out we get an internal server error let's get back here and try to see where we have the problem so we've been told that this file doesn't exist all those will make sure that we put out the right path now we have service and there we go so we have this efficient quantize dot onyx model which is in our service directory you could find service right here so that is it let's save this let's save this and then let's go ahead and retest this so reset console try out and then let's retest this once more execute since that's going to work this time around let's go ahead we still have an error now let's get back to the code and try to understand why we have this error we we told invalid rank for inputs input got to be expected for please fix either the inputs or the model so this what we are getting here now um let's get back again to our code we told that what the model gets isn't um what is expected so let's print out the shape of this image array right here print out shape image array shape and let's save this and then try out run this again um reset console try out and then we test this once more just execute that and obviously we have the internal server error but now we want to see exactly what gets into the model so let's scroll and we should get the output okay so this the output we get now you notice that here we have this um image which is of shape 90 by 90 but what we would have expected here should have been 90 by 90 by 3 so it's um a grayscale image when we expect an RGB image now what we'll do is we'll say here that if um the shape is grayscale or rather if the image is grayscale then we are going to make sure we modify that so if the length of image array um shape is equal to then what we'll do is we'll ensure that we convert this from gray to RGB so let's do this uh let's go this way let's do this and then let's print uh what we have from here let's copy this and then paste out here okay so uh there we go we're gonna paste this again uh so we see what we get after we convert the gray to RGB so that's it let's save this again let's save this this should be fine so we'll save that then we get back here okay so let's test this again out um reset cancel try out pick the image execute and there we go you see we have the emotion happy right here so we have the expected response so we have your emotion happy now let's get back here and see see we have 90 by 90 now it's 90 by 90 by 3 after the conversion and then we have um after it's resized we now pass into the model and we have the expected output so that's it let's um take this off and that's it so there we go we've just tested our api and we see that it works just fine now we could take off whatever prints we have here take this off take this off and there we go we'll then move on to our schemas where right here we'll create this input.py and this output.py file now in our output.py file we'll start by importing base model then we'll create this api output class which inherits from base model and then we'll specify um what we want our output to look like now given that we've seen already from here our output is this string emotion so let's get back to the code and see that this is exactly what we want to have so let's save this or let's let's say we put this as an int and save this then let's go ahead and test so let's um well before doing any testing we have to make sure we integrate this with the detect.py so let's get into this here endpoints detect.py well you have this response model here but normally this isn't there so before we didn't have this so we we should have this now you see response model and then specify api output so um you are controlling what or how this output here should look like so now we have that make sure you you import api output from here and that should be it so let's save now again and then let's test this out so we execute and then we check out the response you see we get a response internal server error which makes sense value is not a valid integer so you see that we are able to tell our api what we want our output to look like now let's um get back to our schemas let's take down your output and then modify this and say we want a string so let's save that now and then go ahead and we test this so there we go we execute and we check out the response you see now we have exactly what we expect so that's the role of the schemas now we're not going to um do the same product inputs just um we just do only for the output and then from here we'll move on to the next part we'll then go ahead to measure out the time taken by the model to produce the output so we get into this onyx inference.py and then just before this well let's say at this level once we obtain our image we're going to have time init time init which is going to be equal time the time so we get a time this initial time let's import time right here import time there we go and then once we import time and we have said this time init we want to get the time elapsed so um after we've gotten the prediction we want to get a time elapsed we have time elapsed which is simply the current time minus that initial time and then for the outputs we want to have this emotion and also we want to get the time elapsed so let's have this well let's return emotion and then we also want to return this time elapsed which is simply our time elapsed we'll convert this into a string so we have that now let's save this well this should be time elapsed let's save this and then now let's go ahead and retest so right here we're going to uh execute and see what we get we get in um this output but we're not getting this here so let's let's reduce this and save this again let's get back here let's save this again um we run this execute there we go we're still getting this without the time elapsed well what we'll do now is we'll get back to our schemas get back to the output and then include this time elapsed and it's a string so let's save that and then kick back here um there we go we are going to execute and we should get this response so you see it takes about one second to produce our output now that we've seen that it takes us about a second to produce the output what if we loop farm means to reduce this time so let's get back into our code you see we have the provider specified we have the model from here and then we preprocess the image and then pass the model pass this input into the model and then we'll obtain the output which is um this now this model loading process here we don't know exactly how much time it takes so what we'll do is we are going to include this time elapsed loading so we have time elapsed as a total time elapsed for loading and running now want to also get the time elapsed for just loading the model like this so what we'll do is now we're going to include this in the output we have um time elapsed loading and then we have time elapsed loading okay so we'll include this in the output let's save that um even output.py we have this already so we have time elapsed and time elapsed loading let's save this and then get back here we execute and let's check out the output okay so you see here we have about 1.1 second total time but the time for loading is 0.72 so this tells us clearly that a huge portion of this time taken to produce the output is wasted on the model loading see this about 66 percent of the time is for model loading even though this is 1 this is 0.66 so let's see how to load this model once we launch our API so instead of loading the model each and every time we make an API call what if we take this off from here so instead of having this here let's take this off we are going to put this in our main.py um main.py here we will have the main.py we will have our model loading so import import onyx runtime as rt there we go then um here now we we're going to load our model so we have our model now loaded here we'll save that and then get back right here and then say okay every time i want to make use of this we're going to have this here we're going to import import um we import service.main as s so we have that service.main as s and then here we'll have s dot model quantized so we've taken this from our main.py from here so that is it we save this now and we had we still we took off let's take off this time elapsed loading we don't need this any longer because we we no longer load the model well let's um let's cut this from here and compute the time it takes to preprocess this image here so time elapsed preprocess take that off then let's modify this here time elapsed preprocess well let's okay preprocess so we have now this taken off preprocess okay so let's get into the schemas we have output time elapsed preprocess save that then go ahead and run this right here so let's execute once more there we go we execute and we check out the output you can see here time elapsed now is 0.45 seconds which is um about half or more than half um of the time we were taken previously so or less than half of the time were taken previously actually so here we have the time elapsed for preprocess this is actually very small so all this was almost of this was for the model to compute the output so let's let's rerun this again let's execute that again see 0.49 seconds now we've we understand how this works let's stop this run this again and then check out the time when we just um start our server um execute and let's go ahead and check the time okay so you see you have 0.41 second that's practically about 400 well 450 milliseconds so that's the time it takes to produce the output now we went we left from one second to now 0.45 seconds one other great advantage of working with the fast api and building your apis is the fact that you could build um asynchronous apis very easily and it's just by simply specifying your this async keyword so here we have async save that um test here async there we go and the main we have um async and that should be it so that's all you need to do to um make your code asynchronous and so this means that when working with fast api if we have this task for example this one which takes much time to be completed what goes on is this other tasks can be run asynchronously and so while waiting for this text for example let's draw here while waiting for this task we could already start working on this other task and then while waiting for this other task to be completed we could also start working on some other tasks this means that overall we have taken up this time to complete this three tasks whereas for synchronous code we'll have to wait for each and every text to be completed before taking on the next one so you see the the time difference between these two different methods we have here the difference we see this difference here so this again in time when working with fast apis asynchronous manner of running code nonetheless it should be noted that there are some tasks which are compute bound that's the actual cpu bound meaning that even if you have to run this code asynchronously this first task will still take up so much time so you would end up with something like this so there will be no real gain even though you had to run this other task asynchronously and this is very common in computer vision and since we're dealing with computer vision more specifically object detection we have to understand that even though our code is running asynchronously if at the level of this detector py we have this call on this emotions detector and we're still waiting for that to be completed no matter that we've already completed these two tasks right here we'll still have to wait for this to be complete so for such cpu bound tasks like computer vision machine learning deep learning will instead take advantage of parallelism with parallelism instead of having one worker one cpu worker to run all these different tasks what we could do is we could allocate say for example let's say three workers so we have now three workers where each worker can focus on a given task and now we can now take advantage of the fact that our code runs asynchronously and takes or makes use of parallelism now so far we've been using this uvcon http server which is an asgi web server implementation for python asgi actually means asynchronous server gateway interface which now serves as a minimal low level server interface for an async framework like fast api now although uvcon is coming with speed it isn't mature enough to be used in the production setting hence we have to or we tend to use gunicon which is a mature fully featured server and process manager for our production settings so that's why uvcon includes a gunicon worker class allowing you to run asgi applications with all of uvcon's performance benefits while also giving you gunicon's fully featured process management so we are now going to go ahead and keep install gunicon gunicon once we have gunicon installed we'll go ahead and run this command which we saw already gunicon service main app and then we specify number of workers let's change number of workers to three for example so let's say we want to have three um workers and then our worker class is uvcon so let's run this and as you can see we have this happening uh occurring tries started several process with a specific um id then we have here started several process with another id another started several process with this other id indicates in our three different workers now we could go ahead here refresh this page um try this out choose file then execute and see there we go so we have our output exactly as we expect so that's it now you could obviously reduce the number of workers if you reduce this um to two oops reduce that to two you would find that we have only this process and this other process so that is it in the section we've just seen how to build our own api and test it out locally in the next section we are going to deploy this on the cloud hello everyone and welcome to this new and exciting session in which we are going to deploy our api to the cloud now in the previous session we have looked at how to build this api using fast api where we could simply pass in an image like this and then obtain the output where we we get the emotion that is happy and we also get the time it takes to produce this output so we could execute that and you see we have our result right here that said the platform we shall be using to deploy our api is ero cool and ero cool is a platform as a service that's a pass that enables developers to build run and operate applications entirely in the cloud so uh first since first you could go ahead and sign up so you click here sign up and if you've already signed up you could go ahead and log in so once we've signed up and log in could get back to the code here we'll stop this let's clear that out and then now what we'll do is we'll do a pip freeze so let's do pip freeze and you see that we have this output which is basically all the different packages we have in our virtual environment so now um let's write out or put out all these um packages right here in requirements.txt file so we'll do pip freeze and then we'll say want to pass send this to our requirements.txt file now note that we are in this directory emotional detection does this so when we do this here you'll find this requirements let's take this one off we did this previously um move to trash okay so you find that now we're left with service and we we have this requirements.txt that said let's open up this requirements you see all the different packages which we installed already now we'll get into open cv we have open cv well that should be python let's search for open cv okay so it's open cv python and now what we'll do is we would modify this and the reason why we're modifying this is because this version of open cv right here is one which you could use even with a desktop application but what we need for the cloud is one which is headless that is one which doesn't contain all the functions it's really functions like those of visualization given that what we're doing with open cv here is essentially maybe resizing and that's that's basically it so we don't need this open cv python we instead going to make use of this open cv python headless so that's it we're going to um take this version 4.2.0.32 okay so with that done we save this uh fail to save well once we save that as a machine the next thing we'll do is we are going to create this proc file so in here we have well new file so call this proc file there we go see recognizes that already and then what we'll do is we're going to paste out essentially what we run each time we want to launch our server so we have web and that's it so here go specify number of walkers to two and that's it so here we have gunicon service main app number of walker set walker class set and that's fine so let's save this too and then we create another file runtime.txt now this runtime.txt is essentially going to take this python version so we'll copy this and we paste that out here so here we have python and this should be fine now also make sure you include this hyphen before this python version with our proc file ready the requirements.txt file ready and the runtime.txt file ready we are going to get into the eroku platform where we'll get right here and click on create new app so we want to create a new app and then we're going to give this app a name so while this loads there we go we're going to call this emotions detection now we could add this to a pipeline or deploy this directly so we want to deploy this directly we have three options using the eroku git using by connecting to github or using the eroku cli so we want to use eroku git and so we're going to follow this simple instructions right here so it's not very long now first since first eroku login so let's get back here we have eroku login now you find that this will work and now but you should note that let's let's stop this you should note that if you don't have eroku initially installed this wouldn't work so the first thing you want to do is do a sudo snap install eroku sudo snap install eroku before doing the eroku login now given i've done that already i'll just do eroku login and then that should be fine so right here we have eroku login pressing the key to open up the browser pressing the key and you should be able to log in very easily there we go you will click on this link right here and if you click on that link you should be able to log in so you could see here we have logged in so that is it we now log in the next thing we want to do is from here what we want to do now is get into our project directory so once we get into our project directory let's zoom this we get to our project directory and then from there we do a git init now if you're not familiar with git you could take a crash cost on git so you get familiar with all these terms so let's get back here and do let's clear this clear and then we do a git init there we go see we in this emotions detection directory now once we've done that you see initialize mtg repository in our directory now the next thing we want to do is have this here let's copy this and then um paste this out here so we have this arrow code git remote and then we specify the project name notice how this project name is automatically generated because this is what we put in let's get back here run this and that should be fine so that's what we have now our command field we told that to well we command field git remote fatal unsafe repository to add an exception for this directory called git config and that's it so what we'll do is now we're going to copy this and then paste out right here so there we go let's now get back and run this and this should work so that is it so set git remote um arrow code to this and that's fine now the next step we want to do is um deploy the application so we have git add git commit and git push so let's go ahead do git add we want to add up all uh files so pick the dot and then we have let's copy this git commit um git commit make it better paste that out well let's say our first first um commit of uh emotions detection app oh well api so this is a string which you could obviously modify now run that and then now oh you see we have all this um created and then now let's go ahead and do the git push arrow code master so here we have git push arrow code master and they should work fine what we get here is this push rejected to emotions detection neural learn so let's scroll up and try to understand why we have this problem so we told you request that um runtime is not available for this stack so open this up and then um let's try to check out on those different python versions available for the stack here you could see we have this um 3.9.15 which is on all supported stacks so let's get back here and then um a level of the runtime let's let's take all this off paste that now save that save that and then we'll do um git add um git commit now what we'll say here is we have um updated runtime dot txt file with uh python version 3.9.15 run that and then now we're ready to do git push arrow code master once more we still get another error right here where we told that um no matching distribution found for open cv python headless so now we're going to take off this version and then we do the git push one more time once they get pushed down you see that we now have this release and here's our link so let's click on this we open up and there we go see have hello world so this is what we have online now and we could go um docs and we check out on our swagger ui so let's reduce this and let's try this out so you will try this out online we've deployed this on the cloud let's execute and see what we get okay so it even takes less time as compared to with uh the pc's taking like uh 0.23 seconds that's about 230 milliseconds let's reduce this so you could see that um okay so that's it so we see that our model is working we're able to obtain our outputs and then now what we could do is we we could actually from here let's copy this here and then try this out on postman so there we go we have this enter the url and then we select post request um well here we have the body here we have form data then our key our key here is going to be image in and then the value given that it's a file we will now select the file there we go we have our file selected click on send and we should get our expected output is here we have happy and time elapsed 230 milliseconds so that's what we expect you could see from here we have this different timing we have the status and we have the time it took for all the different events right here and so that's it for this section in which we've deployed our api to the cloud and the next section we are going to carry out load testing on our deployed api hello everyone and welcome to this new and exciting session in which we are going to be looking at load testing the load testing tool will be using will be lockers and lockers will permit us not only define user behavior with python code but also swarm our system or api which we've built already with millions of simultaneous users so in this course we'll see how to test or better still to load test the api we've built previously both locally and on the cloud at this point you've already tested the model which has been deployed on eroku and everything looks fine but then what if we tell you that we will have to still carry out some load testing now what's the point of load testing the point of load testing here is to see how this your api which you've built will react when you have many users so you wouldn't generally want to create api to be used by yourself only but maybe to be used by tens of thousands and even millions of different simultaneous users so we cannot get access to this millions of people so you wouldn't start calling each and every one of these people here to test your api and what you do is to have an automated system where this millions of users are simulated and this tool we will be using here to simulate this millions of users is lockers which is an open source load testing tool that said we are going to create this locusts.py file so here we have locusts.py there we go and then in this file we are going to write out all the code we'll need to be able to control or simulate all the users who will be using the platform so here we have from locusts we're going to import sequential task set now one thing you should notice the fact that with locusts we using or we defining different tasks and here we are going to define a task whose role will be to simply open up a specific image and then pass this image via post request to our api and we obtain the required outputs so let's define this emotions so emotions task or let's say detector task detector task and this is sequential text set so we have this and then we have this task decorator we have our task decorator right here we're going to define our detection method and here this detection method is simply going to be this method where we define what's actually going to go on when we want to test our application or load test our application so here we have we open we open up a file which we're going to create locally so we have this image.jpg and there we go so we open this file as image and then now we are going to carry out our post request so we have that post request and then you see we're going to specify the path this URL and that's detect actually want to do for detect meaning that you could replace this with test or some other path you want so for now is detect and then let's get back here for now is detect and then now we specify that we're passing a file so here we have this files and we have the name em it's actually em and then here we have image so remember we've opened this up as image so this is image we're passing here now from the API we've designed already its name yeah what we have is em so that's why we're specifying this is em and then once we have this set so this is our detector task as we said the way we build this with lockus is we define different tasks and then once we define these different tasks now we could simply define our load tester class so here we have class load tester and then we have HTTP user so this inherits from class HTTP user and then now we specify the host our host in this case is going to simply be what we've been using already so it's this which we deployed on Iroquois let's copy this get back here and then paste this out so we should we should take off now this detect so we'll take that off and that's it so we're just passing in this as our host and then once we pass this as a host we now specify the task so this is a task in this case our task is the detector task okay so that's it we've we'll specify the task remember you could obviously create many more different tasks but for now we just want to work only with this detector task so that's it we have that set we could save this and then now we could go ahead and launch our locust so we have locust we specify that we have a file and it's locusts.py oh there we go so let's run this um locust command on the phone oh we forgot to install locust so let's do pip install locust there we go that should work fine so install locust and once we install locust we are going to we run this command right here and now that we have that installed what we're going to do is run back this so there we go we run this and we could click on this so let's click there see what we get see here uh we have our lockers running so uh yeah we to start new load test so now the number of users uh initially is one the spawn rate is one we're going to check this on later on and then notice how we have this our url which we had specified so that's our host we should specify it already now let's start the swarming by our locusts um there we go remember this is hosted online so it's going to check that up um looks like none is working well check out this chart statistics well let's get back here let's get back here we get in something like this so we got some errors we could see here no tags define on load tester let's save this and then we run this and see what we get we run that again but we're still getting this error so we had to stop and let's check out why are we getting this error um yeah we have no tags define our load tester but you have set a task attribute on your class maybe you meant to set tasks here um here yeah this should be tasks because this is actually a list remember as we said we define different tasks and then we have our load tester right here so we could have this task here we have some other task oops we could have some other task and so on and so forth so let's say we have two tasks let's say we have tags and then we'll call this tags too then we are going to come right here and then put this in this list but given that we just have a single task let's take this tags two out and then now as we saw here we should have your tasks and not just tasks so let's save that and then run this again and now everything should work fine okay so we have that we click open um starts forming looks like isn't working let's get back here stop this now let's check out the error we're getting okay no such file as this okay so remember we don't have this file in this directory so let's just go ahead and copy that file now we've copied the file you can see that here we have our file here let's go ahead and rerun this and now everything should work fine so let's click on this open up start the swarming and well let's get back we're still getting an error um stop this and check this out here we told that no such file or this well let's let's rename this rename um this should be test image okay so let's run this again sign open up start the swarming and looks like another error um all users want well let's check back here and you can see that we have this working as expected so you can see we have certain number of requests which are sent already zero fills we have the median um latency we have the 90 percentile latency 99 percentile the average the mean and the max so you can see that it's taken about two seconds for every user to receive whatever output the sent in so that's it so we have this mean and we have this max now we could get into the charts and you see that this is increasing slowly and we have zero failures so that's very important we have zero failures with just a single user and that single user is um as we saw here getting an average of now is 1.7 milliseconds to receive the output from the api so that's it now another thing we could do is actually stop this and then carry out a new test where we would have say 10 users so we want to test when we have 10 users at a time now this spawn rate here fixed at the value of two will simply mean that after every second we are going to add up two users so we're going to start with two users and then after one second we go to four after two seconds we go to six and so on and so forth until we attend the maximum row of users which we've said already here so let's start the swarming you can see here we start initially with two we're going to six and then ten and um after some time you're going to have the first requests so there we go we already have two two requests not that we have dealing with 10 users here and the average time it takes for each user to receive the response is 10 seconds meaning that our system will take about 10 seconds to reply to well whatever user whenever we have 10 simultaneous users so let's get into postman remember here we have this time elapsed note that this time elapsed was for the model but what we're getting now is what each and every user located in some place in the world would have as actual um time or latency if we have 10 users now we start seeing that we start getting failures so let's check this out when you run this um let's run arrow cool logs so yeah we let's clear this so you could see that clearly so we run arrow cool logs and then we specify the app emotional detection neural learn and specify tails we could get the last um locks so run that and uh this should run you could see that we have successful calls and we also have this calls which field see the stitches and your connection close without response so let's get back here remember when we had a single user we didn't have all those failures so let's stop this run your tests and reduce this maybe it's just three users and then see what we get well as you can see with just three users we have a few failures that's about seven percent as you can see here and it takes on average of 4.5 seconds to receive output when you send input images and so with this we've just built and load tested our api they showed us clearly that if we want this to go faster or to reduce our latency and reduce the failure percentage which as of now is at 2 percent then we would need to increase our compute capacities so that's it for the section see you in the next section welcome to the section and object detection object detection is one of the most popular computer vision tasks and also a very important one object detection entails correctly classifying objects which are in an image and also saying exactly where these objects are located in the image so if we have this image we see clearly that we have an aeroplane an aeroplane a person a person and then we could say car so this too is some sort of car now an object detector not only classifies these images but also localizes exactly their positions in the image so this aeroplane for example has this bounding box now a bounding box basically surrounds also square a rectangular box which surrounds the object and then for this person we have this bounding box for this this bounding box and this other aeroplane we have this bounding box which is in this other bounding box so unlike a classification problem where when given this kind of input we have as an output or a onehot vector for example which represents the number of classes we're dealing with so supposing we have five classes then when given an input we have to correctly say whether that input belongs to one of the five classes now with object detection we not only have this but we also have the positions in the image so like this we have this position like this and we have several conventions for these positions one of the most popular conventions is the center convention where we have the x center y center width and then height now what does this mean this means that based on this referential right here so we could define a referential right here where we have this origin now recall we used to having we're used to working with this kind of referential now the referential for image data is considered to be this so our origin stands from here we move in the x direction and then you're in the y direction so this is our reference and then it's based from on this point on this origin that we actually define positions so if we have this airplane let's consider this bigger airplane so we could have this bigger airplane we could define its bounding box by its center and then its width so once we have a center obviously and if we're given its width we can obviously see that it is in this bounding box so this is the first convention the center that's the x center and the y center the x center is basically the distance from this to the center so suppose the center is right here so if our center is here then the distance from here to this is x center and the distance from up to this is y center so basically that's what we have so if we want to link this up like this we could see clearly how we obtain x center the distance from here to here and then y center distance from this origin to this which now gives us the center so that's how we get a center now we are given the width and then the height if we're given the width and the height obviously to to get this points or to get because we have actually four points right here to get all these four points which make up the bounding box we could start from say this point for this point to obtain the x and the y coordinates right here we simply take this x coordinate and subtract from the width divided by two because from this to this is width divided by two on the diagram it doesn't clearly that this points at the center but normally this should be at the center so we could rearrange this bounding box now so as we said to get this we have this x center minus the width divided by two to obtain this and then to get the y we have the y center plus because this is actually the positive direction and then this is the negative direction for the x so for y this is positive this is negative for y this is positive for x and then this is negative for x now to obtain the y as we said we have this y center plus the width of this or rather plus the height of this divided by two that's how we obtain this now to obtain this since we're going from this we're going the x center minus the width divided by two we obtain the x coordinate here to obtain the y coordinate we have the y center minus because this is the negative direction so we have the y center minus the height divided by two and this is very similar to obtain the x here there's a x coordinate right here we have the x center here plus the width divided by two to obtain the same x coordinate right here it's x center plus the width divided by two obviously these two have the same x coordinates but different y coordinates to obtain the y coordinate we have this y center minus the height divided by two to obtain the y coordinate here we have the y center plus the height divided by two either ways once we have this two coordinates that's this point here then this we could always obtain this on this automatically. Now another convention is the X mean Y mean X max Y max convention where we just given this coordinates. If we're given this coordinates and then this coordinates we could obviously obtain all this because when once given this and then given this we could just get the whole box automatically. So here are the two main conventions we use to actually locate an object in the image. This yellow paper published several years ago was one of the first to come up with a single neural network which predicts bounding box and class probabilities directly from full images in one evaluation. Now back then models or to be precise object detection models follow this kind of pipeline where we would have a region proposal generator, feature extractor and then a classification unit. So you could look at this with a simple RCNN model where we had the input image, the extract regions that's regions where the model things we could have images where the model proposes the locations of objects and then from here each proposed location is passed into this feature extractor which obviously extracts features from this warped regions as you could see here for example this region and then we have the classifier which tells us whether this is a person, an airplane or say a TV monitor. Now with the yellow as we're saying we have a single network so let's get here you see we do not have all that different stages in our pipelines here we just have our input image a single neural network and then we have the outputs see that there's also this additional nonmax operation here but we'll look at this shortly. Now the performance of the yellow is quite impressive or was quite impressive in terms of speed is especially as now we can obtain speeds of up to 45 frames per second and with a yellow the smaller version of the yellow we obtain up to 155 frames per second while achieving double the mean average precision of other real time detectors who look at the mean average precision subsequently this is the metric used generally in object detection and so a high mean average precision means the object detection model is performing better. Another advantage of the yellow to take note of is the fact that it reasons globally about an image so unlike the sliding windows and the region proposal based techniques we've looked at the regional the region proposed proposal based techniques like the RCNN here for the sliding windows the way it works is we have let's get this way we have this image right here and then this window you see you can look at this as a window so this window is slided through the whole image all I hear what we just simply pass in the image as an input with a sliding window you would have to take each window and pass into our neural network and you do that while you're sliding through the full image so as we're saying getting back to what we're saying here as compared to the sliding window and the RCNN kind of models the yellow performs better in the sense that it sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as the appearance so you'll find that models like the RCNN or mistake background patches in an image for objects because it can't see the larger context okay another point is the YOLO learns generalizable representation of objects so you see that here the YOLO which was which wasn't trained on this kind of paintings performs quite well you see the YOLO was trained on or this particular version of YOLO was trained on the Pascal VOC dataset but when we test this on this image painting you see that it does well so the YOLO models compared to others learns more generalizable representations of objects let's now go in depth and see how the YOLO algorithm works first is first remember that if you have this kind of model see the YOLO model which you could have here there we go you see this YOLO model has several conf layers and then completes with this connected layer so we have the feature extractor and the classifier unit right here anyways we are not going to get into this now let's just consider that we have this model and then we have inputs like this one see this our input and then we have some output right here now this output obviously is meant to be a bounding box so here we will we could draw this bounding box for this woman here so this shows that or this bounding box is for this this person and then let's take this off at the stroke let's change that color so here we have this here this this person detected and then we also have this other person right here so we'll have something like this see that so we have now these two detections one for this woman and and this other one for this other person and so we're gonna build a model which is this one which takes in these kinds of input output pairs and then learns to get this inputs and predict the outputs correctly such that when given a new input it could tell us where every object is located and what type of object it is precisely now that said the actual outputs wouldn't be this image with this bounding box the actual output will be different from what we are seeing here now the way this outputs are created is using some sort of encoding system where we have this input which is breaking up into some grid cells so right here let's take the pen so right here you see we have let's suppose that we have this 224 by 224 input image that's this image here 24 by 224 this one year and then we break this up into several grid cells so this is a single grid cell another one and so on and so forth now each grid cell here given that we have seven grid cells so we have seven by seven output see that we have one two three four five six seven one two three four five six seven so that's it and now each of this is going to be 32 by 32 so it's like 32 by 32 patches so we take each patch here combine them to form 224 by 224 or better still we take the 224 image and break it up into 32 by 32 grid cells see that now once we have this ready or once we once we have somehow broken up our image into this grid cells we are then going to encode the outputs based on the locations of the center of our bounding boxes right here so you see we have we're gonna redraw this bounding boxes so that you see that clearly let's increase this to say five okay so here we are having this bounding box right here see this bounding box and then it has a center at about this position here see that position if we want to locate this year it falls about this around this year so you see is about this now for this other person we have another bounding box like this see that this other bounding box and the center is about this about around the child's nose so it's around here see that so what we're gonna do now is we're gonna have each and every one of this let's change this color we're gonna have each and every one of this sorry we're gonna have every one of this here having certain values now in the case where the cell like this one let's this make this bit more transparent the case where cell like this one let's do trainee so you could see that better so here we're gonna have as we're saying in the case where a cell like this one which doesn't contain an object then we'll say okay its first value will be a zero see that this first value will be zero skip back and here our first value is zero so for this cell the cellular office value is zero now for this other cell here where the there is an object there see this value will be a one see that so each and every cellular each and every cellular has or takes certain values based on whether there is an object or not now as you can see this cell you have a zero the zero in fact all the zero and accept this tool which we put in red which will take values of one and this is simply because it happens that the centers of the bounding boxes fall in those cells so that's the first step now once we have this first step the next thing we want to do is we want to locate the exact position of our images so the first thing is we want to know whether there's first of all an image that's by encoding it like this the next thing we want to know is the exact position now this exact position obviously depends on the kind of the way we want to present our bounding boxes now we could represent our bounding boxes by specifying X mean X mean Y mean and X max Y max with this kind of representation an object like this baby right here or let's say person right here will have this values or will make use of these values to locate this person so we make use of this point here which is X mean Y mean and this other point here which is X max Y max with respect to the origin which is at the top left corner so this our origin right here you see that so we go X steps and Y steps downward then here X steps to the right Y steps downward to locate this person now once we once we've done this we could we could just put this out here so we could say okay we are creating our outputs remember our aim here is to create our outputs so we are creating our outputs we know for every cell or for every grid cell where objects are located that's it then now to get the bounding boxes we could make use of this but what's important to note here is the notation used by the authors of the yellow v1 paper was instead X center Y center then the width and the height see that of the bounding box obviously none of the image so here instead of having making use of X mean Y mean X max Y max we make use of X center which is the center Y center so we'll go X Y and then we look for the width of this box the width and also the height of the box so let's get back so that's that basically how we do that and then there's another special encoding which is done and what they actually do is for the width and the height of the bounding box they're gonna divide this by big W where big W is the over is a width of the whole image so if our image is 224 by 224 will take this width let's suppose that this width is say 160 so we'll take 160 divided by 224 and get our width and then for the height we'll do the same thing so you know if we have the height of say 200 will take 200 by 224 and we get that so it's h divided by the height of the whole image see that now once we have that we here for this XC YC we're gonna do something similar but not divided by the whole width and also not divided by the height of the image what we are gonna do here is we are gonna have this XC with respect to its specific width cell now let's explain let's pull this way so you could see that clearer so we're supposing that we have this here and we've already seen how to get for the width and the height we take that divided by the total width and the height divided by the total height total width and total height now example is 224 now for the XC and YC here for example we have this our XC YC is for this other objects our XC YC let's consider the example of let's consider this example here so what we're saying is we are not gonna take this with respect to the full image instead we're gonna say oh we're gonna take this grid cell and suppose that this distance is 1 and we take this and suppose that this distance towards 1 now our origin here is this see this points our origin now if we're for this cell this will be our origin and this will be our distance 1 and here our distance 1 now if this are distance 1 so distance 1 then this point here let's say this point here will be a fraction of 1 basically so be a value between 0 and 1 now if we take this distance we could approximate this to be about 0.5 6 and then this distance is about 0.7 so in this case XC will be this distance and YC will be this distance in that case we'll have 0.5 6 and 0.7 so that's it now once we have that the next thing we'll do is we'll just simply put that out here so we would have that XC with respect to the grid cell right at G and then we'll have YC G that's it so here we now know how to obtain those values let's change that color back so here we would have say 0.7 0.5 6 0.7 so that's how we have this now once we once we get this remember we already have one year so the next one we have is XC G for the next part if we have a data set where we have say 20 classes like the Pascal VOC data set or with a cocode data set where we have 80 classes what we would have from here see we find whether the object is there or not if the object is there we want to get this location now from the location want to know what object exactly is found there so here we suppose now we have 20 classes see now initially every one year is zero so we have 20 we just gonna align 20 zeros so basically we have this this 20 zeros here now if the person if in the you know in our classes we decide that the person occupies the city third position in a list of classes so we could we could have a class or let's just let's just check out the list of Pascal VOC classes whereas consider we have this 20 different classes and there if we count this we have 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 notice how this person set a 15th position 15 16 17 18 19 20 ok so we have the person at that 15th position and so this means that when encoding our image here or when encoding our output would have to take that into consideration will simply do 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 so here at this position will change this and put a 1 instead of the zero which all the other classes get so we take this off and then we have a 1 right here see that so we have 15 16 17 18 19 20 now if you have another data set with fewer classes say eight classes this will be of length eight now we've seen how for each and every one of these grid cells which actually several 49 of them because we have seven by seven we have this encodence that's so what our model sees or what our model gets as output isn't this but instead this kind of output so now we are going to prepare our data set such that we have an input image which is this and then an output level which is going to be this level right here and then the model is going to struggle to be able to correctly produce these kinds of outputs and so as we've just discussed our output will be of shape this output here will be of shape 7 by 7 by 20 see number of classes 20 plus 5 or could you say 5 plus 20 which is in fact 7 by 7 by 25 so this is what our level will look like so when we get our input and our output I won't get an input and the bounding boxes with the specific classes we're gonna create this kind of outputs from the labels we get depending on our data set nonetheless when we zoom into this model right here we notice that we taken some input and then at a level of the output we have 7 by 7 by 30 instead of 7 by 7 by 25 now the reason why we have this is simple we have the first position let's reduce this we have the first position which tells us whether there is an object or not so that's one and then the next position gives us the location with the X C Y C W and H so here we have the position here we have the position so we have X Y W H now the next ones give us the classes or tells us what class our object belongs to so you would have a series of zeros some point we will have a one and then we have your zeros but the length of this year is 20 now 20 plus 5 is 25 obviously which is less than 30 now to understand why this is actually 30 oops to understand why this is actually 30 what we'll do is let's take this off what we'll do is we'll suppose that we have for every cell two boxes responsible for locating the object so instead of having this year only this particular box so we suppose that this is a box let's take the pen we suppose not this year this year is a box this one is a box instead of having only this box we're gonna make two boxes responsible for locating the object remember this is this permits us to locate the object because first of all it tells us whether the object is found in the cell or not and here it tells us it gives us the exact coordinates of the objects this one is for the classes separate so the next thing we'll do is we'll take this and multiply by two so let's shift this again we have this and then we paste that out see now if we have the number of boxes to be three though this will multiply it twice then the in the yellow view on paper they use they use B to be they considered B to be equal to you could check that out your B equal to so that said we see we now have two boxes responsible for locating the object in that image so when designing the levels that we could design with just one because we know that this does the correct answer but what the model will predict will be two values you see we'll predict this to repeat this and predict this now the reason why we're doing this is because we want that one of these boxes or any one of these boxes should be more specialized depending on the size of the object now as you've seen here we're dealing with relatively large objects with respect to the to the image actually but what if we have an image of a person where the person is say just a very say we have a very very small person to image ratio like this where we will have a bounding box which will be small compared to the whole image in that case what we want is for the model or for these boxes to be specialized such that maybe this first box will will learn to detect the smaller objects while this one learns to detect the larger objects so that's it that's how we construct this output so we understand how to construct the levels and then how to construct the model outputs remember we have to update this weight such that the difference between the levels and the model output let's say all is minimized getting back to the paper we see the exact structure of the yellow model so right here we start with some conf layer conf layer max pool layer conf layer max pool we have several conf layers then a max pool then we have this other conf layers this year times for the max pool conf layers then conf layers then finally we have the fully connected layers for classification so this is for feature extraction this year feature extraction and then this is for classification now for the training what they do is the pretrained this model on image net with 1000 classes but note that this pretraining is for the problem of classification so it's a usual classification problem and then the pretrained this model for over a week year and achieve a top 5% accuracy of 88% and then from this model they add four convolutional layers and two fully connected layers we randomly initialized weights so while going going doing the training for the object detection we have weights from the previous training that's from the pretrained weights from the image net and then the add some the add the four the actually spoke of four so they should be this and this one this one this last two year so they add up four conf layers and two connected fully connected layers which have been randomly initialized as a set year now following the example okay does it detection often requires finegrained visual information so we increase the input resolution of the network from 2 to 2 24 by 2 24 to 4 48 by 4 48 so what they did was they trained on 224 by 224 images and then at detection time we used 4 48 by 4 48 images now they also use a linear activation function for the final layer and all layers use the following leaky relu so they use the relu for the final and then the leaky relu for the all other layers as the activation function now as a recall for that we have our relu which is simply this what the relu does is all values all values it takes which are less less than 0 are sent to 0 and all values greater than 0 are maintained so if you if you pass in a value like 3 the other to the relu you get back 3 but if you pass say negative 0.5 negative 0.5 what you get will be 0 because all negative values are sent to 0 and all positive values are maintained this guy is you remain X if X is greater than or equal 0 and you go to 0 if X is less than 0 now for the final layer or rather for all the other this is for the final layer this is what I use for the final layer now for all the other layers they use the leaky relu for the leaky relu what we have is not this year not the straight horizontal line but inside something like this see something a bit slanted and then here is still maintain the positive still maintain so we still have it's still X it still remains X for X greater than or equal 0 but for this one it goes to 0.1 X for X less than 0 so for all negative values would we would have 0.1 X so it means our gradient here this gradient is going to be 0.1 so the bottom border too much if you don't understand the national gradient anyway we have that we have this it means that if we if we send a value like say negative 0.5 now what we will get will be negative 0.5 times 0.1 and not more 0 so that is the difference and this is activation function used everywhere in the model except for the last layer so that's what is the that was defined right here now the the loss function that we hear the uses the sum square error so that's the simple loss function they use and that's why at the beginning the speak of we frame object detection as a regression problem so that's it so it's like a simple regression problem actually that is the sum square of the difference between the model output and the expected output which is the levels now that we understand globally how the models build and the training process let's get to look indepth into this loss function so right here we have this loss function and then we're supposing that we have this levels here so here are levels and then here's what the model predicts remember we have 7 by 7 7 by 7 cells with cells by 30 for the models predictions whereas the levels are 7 by 7 by 25 see that so we have this first 5 for the location this location this location we have to add this one year oops let's change the color back to red so you see that Clara so we have this location here there we go we have this location here that's it we have this 5 this 5 and we have the classes here we also have the classes and then we have this 5 right here now the way we obtain the loss is we break it up into several parts we'll look at this part first we'll start with this part now this part you see just basically add in the path of the first part the next part this part this part and this part for this first part here this it punishes the model when it makes errors with respect to whether there is an object in a particular grid cell or not so if we have for this grid cell one as a level we expect in the model to predict a one year or end a one right here so this means that what will happen here is we are gonna go through you see this sum from i equals 0 to s square s square here in our case s is 7 so s square is 7 square which is 49 so we go through each and every grid cell here which is logical we go through each and every one of this so we go through each and every one of this 49 different grid cells and then we'll calculate the difference see this you have c i c i shuffle that's c i minus c i shuffle square so we have this here minus this square and then we add all those up now also notice that there's a double sum in your the reason why we have this double sum is because we actually gonna take this minus this plus this minus this see that take this minus this plus this minus this if we had say five or three boxes then we'll have three of this we'll have one two and add another one before the classes so in that case we will go three times so hopefully that's clear but what thing you notice is this this notation here one one of OBJ I OBJ is actually object one of object I so you notice this notation right here now or this notation let's get back and just circle that out here so it's clear so this is this is it right here you'll notice this and what to say in the paper is this one OBJ I notation denotes if an object appears in cell I and this one OBJ IJ because this is actually IJ not I IJ denotes that the jth bounding box that's here we have two bounding boxes so either this bounding box or this bounding box predict predict or in cell I so I is the cell clearly see that I goes to 49 and then J goes to 2 if we if we have this then it should be I equal 1 to 49 and this should be J equal 1 2 2 this means that there's a slight error notation right here anyways we understand that we're going from 1 to 49 and then we're going from 1 to 2 because we're basically going through each cell here and we're also going through each and every one of these boxes now getting back to our one IJ or one OBJ IJ notation we're saying that oh we've already seen that this denotes that the jth bounding box predictor in cell I in the given cell is responsible for that prediction you see that now what does this mean it means that if a particular like here if a particular box if this box is not responsible for the prediction then we are not going to include it when computing this error now how do we know whether this box or this box is responsible for the prediction the way we get this is simple let's pull this to the right or let's reduce that so we could get more space so what happens is let's suppose we have this image and then we have one object here and we have another object here then we have some bounding boxes so we have this bounding box and we have this other bounding box now for this for this year we have a particular cell let's suppose our we're breaking this up and then we have a given grid cell like this one which is responsible for predicts in this object now this the first box you see this first box will predict maybe this bounding box and then the next box will predict maybe this bounding box now what we say we predict this one this will predict this bounding box and the other predict this is actually because they have different values for X C Y C W H see this quadruplet here is different from this other quadruplet and because they are different it means that obviously the bounding box that you get will be different and because those these two bounding boxes you get will be different it means that you you can now compare which of these two is closest to the actual bounding box which is this so this is the actual and this is what the model predicts so we comparing these two they're competing for which of them is closest to the actual so let's suppose that the actual is is something like this suppose that actually something like this here so we have something like this okay so in that case it's clear that this one let's look for a neutral color now it's clear that this because we remember we are having this one this box competing with this black box competing with this black box so this is B 1 and B 2 competing but the blue box is the actual one B we just call that B so we're gonna compare the difference between B 1 and B and the difference between B 2 and B now the one which is which resembles B the most that's the one which has the least the smaller difference will be the one responsible as I said here for that prediction you see that so in our case here is clear that B 1 is responsible so for this particular case because for a different grid cell you may have B 2 responsible for whatever grid cell you may have again B 2 or B 1 it just depends on on what on this difference between the the the bounding box by that box specific box and the actual bounding box now that said another question you may ask yourself is how do we compare this bounding boxes now the way we compare this bounding boxes is by using the IOU score so if we have two bounding boxes like this we have this two bounding boxes and then we have let's see this one let's say we have this other box here and then we also have this one something like this if we had to compare the how close this one this pair this pair of boxes is compared to how close other pairs you see clearly that this pair is closer or simply put get more closer to each other as compared to this other pair right here now the way we look at this is we compute the area between the two boxes so that's it you do for this area of the intersection so this is their intersection and then so we have here we call this IOU IOU actually let's just put it right here IOU actually stands for intersection over union intersection divided which is equal the intersection we'll call it intersection divided by the union so if you take for example this intersection here and divide by the area this area plus this area then that's actually including the this intersection is basically this is this is the intersection and then let's change this so you can see Clara and this is the union see this this is our union that's it so that's our union and this is our intersection so we take this area divided by all this area here and we get the IOU score we're gonna repeat the same process for this one where this is our union and you could see clearly that this one will have a higher IOU compared to this one and so this is how we compute or we know which of the boxes is responsible for that prediction now getting back here let's get back to our loss function as we're saying we're gonna have that if this box for example if it happens that this box be one year this box here is responsible for the prediction then would have this difference times one see times one now if this box is not responsible then we have times zero so this this is not going to be considered when we computing this loss see that that's it so it's true we summing through the two boxes but actually we're gonna take only or consider only one when calculating this difference for this one we're not gonna we're gonna omit it now we move on to the next this other one year computes or permits us to have grid cells or permits us to correctly predict when there is no object notice here this is no object here is object so what we have here is for the cells where we have an object like you see well where the level we have the cell and the cell we're gonna use this year where there is no object we're gonna use instead this one year and here basically when there is no object we're just gonna take the the output or the value we have here minus the value we have here then plus the value we have here minus the value we have here now the next I also know that we have this lambda no object now in the paper the talk a little bit more about this here we have to remedy this first of all let's understand this here to say they use the sum square error because it is easy to optimize however it does not perfectly align with our goal of maximizing the average position it weighs localization error equally with classification error which may not be ideal also in every image many grid cells do not contain any object so this pushes the confidence cards of those cells towards zero often overpowering the gradients from cells that do not contain objects now this can lead to model instability concentrating to diverge early on so to remedy this they increase the loss from the bound and box coordinate predictions and decrease the loss from the confidence predictions for boxes that don't contain objects we use a parameter we used two parameters lambda coordinates this photo the positioning and lambda no objects for when we have no objects to accomplish this so we set lambda coordinates for five and lambda no objects to 0.5 so as we're saying this lambda no object here 0.5 and lambda coordinate is five as they've given us right here now all can deduce from this from this looking at this formulas here is that the model will be punished more severely if it has if a particular grid cell was meant to predict an object and it didn't predict that as compared to when it doesn't have an object and it didn't predict that correctly so for the object we have more punishment as compared to this one because this lambda no object is 0.5 now for the coordinates is it receives highest punishment year because the as we as we have lambda coordinate equal five for the classes it's still equal one so here we have one zero point five one and five five now getting back here we have this one for the classes basically what we have here is we have this condition so this is one object or OBJ of I this is the one was OB or one OBJ IJ so notice that this is now I now here basically what we having is we calculating this difference only when that grid cell has an object so if like we in the level year if we have a one like for this two grid cells then we'll go ahead and compute this difference but in the case where we have no object like in the cell in the cell the cell in the cell or all other cells have this too then we wouldn't get into this so we'll just keep that not also we have this oh sorry we have we actually here wouldn't get into this so here we have just one now this this what we do here is similar to what we're gonna do for the coordinates with the coordinates we have the same process where if there is no object wouldn't go ahead to compute this difference so like here if this is zero then we'll skip this now if it is one then we'll go ahead and compute the difference between this and this and that's what we have here now this is the X minus X of bar square plus the X Y minus Y bar square is basically this X minus this square plus this minus this square and then here and also notice that you know this this two squared and then it's added up and then here we have the square root of the width minus the square root of the order width or the predicted width all of that square plus the square root of the height minus the square root of the predicted height all of that square and then they add this up and multiply by lambda coordinates but it should be noted that we are only going to compute this if we happen to fall in a box which is responsible for that prediction so if this box is responsible for the prediction it means that we are not going to make we're not going to compute this for this box right here we're not going to compare this and this we're going to do this and this because this box is responsible and we've seen already what it means by a box being responsible for the prediction getting back to the paper let's get it back up here now what you have also is that the sum square error or equally waste errors in large boxes and small boxes so the error metric should reflect that small deviations in large boxes matter less than in small boxes to partially address this we predict the square root of the bounding box width and height instead of the width and height directly so what you're saying is if we have this couple here these two boxes and then we have this deviation or we have the difference which we're trying to compute for the loss and then we have also this smaller boxes here with this similar difference so let's let's try to have something similar to that we have something like this hope it's similar enough so we have something like this with the initial method we will have W minus W bar or Chapeau or T over the square plus the height minus the height bar square where this is W here is what the level expects and the W by is what the model predicts so the same for the H and the H bar now what the one is that if we have this difference here or better so let's say that what is going on with this year is that if we have this year these two boxes this difference because the equal difference you see this difference is the same if we have this year then let's say the differences is five let's suppose that this difference is five year difference five that's the width and the height difference is five then we'll have five square plus five squared as 50 now for this small box here this is five and then here is five then we also have 50 but this is not what we want the reason why we don't want this is because this kind of difference for smaller boxes is more important than this difference for this bigger boxes so it's just like you you suppose in that you have say say you have a loaf of bread like this suppose we have a loaf of bread and then you have this part here which you cut the control now compared to a case where you have this smaller loaf of bread you will find that cutting off this part here is like cutting off practically a third of my loaf whereas oh for this case this was like cutting out say one tenth of my loaf so so clearly from here the loss is less felt as compared to this other one and so to as I say in the paper to remedy the situation to add the square root here now let's add the square root and see this difference now the square root of let's say let's say this was 30 and this was 25 so we had five and then there was 30 and 25 the square root of 30 now is let's say this is 30 width 30 and for this other one so this one has width height 30 and so that one has width height 25 so this gave us the difference which was 5 and that's how we had this difference here now when we when we take now the square root of 30 let's take the square root of 30 the square root of 30 that's 5.47 okay so 5.47 now minus the square root of 25 will be 0.47 see 0.47 which when you square let's compute that directly which when you are gonna square it will give you approximately 0.22 see so you have 0.22 plus 0.22 now that will give us 0.44 or 0.44 for this two bigger boxes now for the smaller boxes let's suppose that this one was say because we want to have a difference of 5 we could say 10 and 5 so here we had 10 by 10 and here we have 5 by 5 in that case we will have square root of 10 minus square root of 5 and now when you compute that you would have 0.84 so what I mean here is when you take the square root of 10 minus square root of 5 and square it gives you 0.84 0.84 plus 0.84 we give you about 1.6 let's say 1.6 anyway that's already much greater than 0.44 it means that the model is penalized more now for making this error as compared to making this error so that now solve the problem of where the the model would have penalized them the other model would have been penalized in the same way for this same difference even when the size the size difference between these two boxes is quite considerable now that said from here this the train for over 135 epochs on training and validation data sets from Pascal VOC in 2007 and 2012 we test with testing on 2012 we also include the VOC 2007 testing for training test data for training then try training we use a batch height of 64 momentum of 0.9 and decay that's with decay of 0.000 5 then you ratio was as follows for the first epochs will slowly raise the learning rate from 10 to the negative 3 to 10 to the negative 2 so started with a relatively lower learning rate slowly increasing it because if we start at a high learning rate the model often diverges due to unstable gradients so we continue training with 10 to the negative 2 that's after going from this was to slowly increase to 10 to the negative 2 and then to continue to training with this 10 to the negative 2 for 75 epochs then 10 to the negative 3 for 30 epochs so after this 75 epochs to drop to 10 to the negative 3 and then finally drop against 10 to the negative 4 for 30 epochs that makes now 75 plus 30 that's 105 plus 30 135 epochs okay so to avoid over fitting we use drop out and extensive data augmentation drop out layer with rate 0.5 after the first connected layer prevents caught up adaptation between the layers for data augmentation they introduce random scaling and translations of up to 20% of the original image size we also randomly adjust the exposure and saturation of the image up by up to a factor of 1.5 in the HSV color space now given that after the detection has been made let's get back to the top that's that's after or let's say after the model has been trained we would get detections like this increase this would get detections like this so we might have many more detections than expected so we're gonna apply the non max suppression algorithm to remove those cells or rather to remove those bounding boxes which are repeated around a certain region and focus only on bounding boxes which have the highest probability scores so like this one you see the thickness here signifies the probability score the probability of an object being that location see that so that's why we left with this after the non max suppression now the way this non max suppression algorithm works is as such we have after the person after the model has been trained we pass this input image and we may get predictions like this now let's let's suppose that the this one this this two whites have the highest probabilities and then we also have some other predictions we have maybe see this other prediction here oh that's it we have see another prediction around here something like this now what we're gonna do is we are gonna consider that for a particular object let's say this this bounding box right here for a particular bounding box we look at its probability and compare with that of the bounding box around it and obviously to know what a bounding box surrounds or is very close to this bounding box we look at the IOU so if we fix the IOU to a threshold of 0.5 it means that if we're taking this box for example considering this box then any box with an IOU that's any box is close enough to this here sorry that is IOU is greater than 0.5 meaning that they are very close then we are gonna remove that bounding box so it means I was gonna take this off because they are already very close so you see that now you could you could play around with this value meaning that you could take say 0.2 or even 0.7 depending on the data set you're working with now so what we're saying is because these two are very close to each other we take that off now obviously they must be present the same object now if we have another box like this one if we had another box like this one see suppose we had another box like this one then this box will not be taken off because the IOU is less than 0.5 so when the IOU is greater than 0.5 we know that they are very close to each other we compared your probabilities the one with the highest probability is gonna win the other one is gonna be taken off and the term known max suppression so if you are not in you're not a max we're gonna suppress you so we suppress all of that and you see this one is left now for this year we're gonna say okay this one has the highest probability we're not we're not gonna compare this with this because obviously the the IOU will be less than 0.5 now we're gonna take this one and this one we're gonna compare this to the IOU is gonna be greater than 0.5 and so here we're gonna remove this other one so this one here will be taken off you see this one is still left so that will be it will be left now with this three predictions anyway generally when training our YOLO model we aim to even be able to avoid the known max suppression as a whole and other YOLO variants have been developed to try to reduce that dependence on the known max suppression other YOLO variants are like the YOLO 9000, YOLO V2, YOLO V3, YOLO V4 you also have YOLO V5, YOLO X, YOLO R which perform even better than this YOLO V1 we're discussing right here so here you have some tables which compare to other methods see you could always look review this here fast R CNN you have the YOLO see that it performs better than the than the YOLO but this one is faster than the fast R CNN we also have this comparison table for different objects see for different objects we see the the precision for this different objects and we compare with this different methods here you have the YOLO then you could get down here see the recall see that we've already had a tutorial on the precision and recall so you should be able to understand this now if you're new to that you could check out our previous videos here this is some quantitative results in the VOC 2007 because when people aren't data sets so that performs best on the on the this VOC 2007 it performs best on the Picasso it runs best on the people art data set let's get back here oh okay this is fast R CNN while you're there comparing with the with a with a simple R CNN okay so that's it we see that with a Picasso and the people art it performs in outperforms other methods like our CNN mean that it's as marginalization capacities as compared to other methods techniques here we have some limitations of the YOLO YOLO imposes strong spatial constraints and bounding boxes so since each grid cell only predicts two boxes and can only have one class so this means that let's get back here this means that if we have if we had a person who was say starting just behind rambien sort of person here it would have been difficult to predict this person and this other person and also in the case where we have images where the objects are quite small so we have some very small objects and act up like this this YOLO more algorithm or this YOLO model will find it difficult in detecting each and every one of them getting back to the paper our model struggles with small objects that appear in groups such as flux off of birds since our model learns to predict bounding boxes from data it struggles to generalize to objects in the new on usual aspect ratio or configuration so this has been trained on the Pascal VOC dataset where the objects have certain aspect ratios I'm a turn it out you have a different data set where the aspect ratio is different by aspect ratio with simply meaning the the width to height ratio so it's this ratio right here this this aspect ratio can be say two by five if we taking height by width then you're gonna be like say three by two so what what goes on here is you have trained this on Pascal VOC dataset where you have a specific as or a general kind of aspect ratio or aspect ratios but when this is taken to different images where the aspect ratios aren't similar to that of the Pascal VOC then past the YOLO model finds it difficult or struggles to generalize in such situations finally we went wild train while we trained on a loss function that opposomates detection performance our loss function trees arrows the same small bounding boxes versus large bounding boxes and they say that the main source of error is incorrect localizations that said we're done with this review of the YOLO paper in the next section we are going to build this YOLO from scratch hello everyone and welcome to this new and exciting session in which we shall focus on preparing our Pascal VOC dataset using the TensorFlow dataset pipeline so here on Kego we have this Pascal VOC dataset which is made available by Wang Hang China and it's made of this five different directories that is annotations image sets JPEG images segmentation class and segmentation object nonetheless we shall be making use of the JPEG images and annotations for our object detection problem and now get into the code we are going to start by installing Kaggle we are going to copy this Kaggle.json file into this directory which we just created now note that this Kaggle.json file as we've seen already is gotten from our Kaggle account so you get this from your Kaggle account and you copy out here and then now after this copy you change the access mode of the file and then start with the dataset downloading now to download this or to have that command you just simply scroll like this get here copy API command and you paste it out here so what we have here is simply what we've copied so that's it that's how we download this dataset so we run that and we download the dataset now once this we download it we'll go ahead to unzip the content of that dataset into our dataset directory right now we'll have our dataset there we go we have our dataset we pick out this one here and you see we have the annotations image sets JPEGs segmentation class and segmentation object open up this and this okay so that's it now what we'll do is we are going to define some variables here we have our train images which is simply this path to this JPEG images we have our train maps which is simply the path to the annotations so that's it and then we have our classes so Pascal VOC dataset takes in 20 classes from airplane right up to TV monitor then we have B set to 2 now to understand the significance of this B remember from the paper that we had seen that the image is divided into an S by S grid and for each grid cell and each grid cell predicts B bound in boxes now the other define S to be 7 and then B to be 2 so that's the this number of bounding boxes which we are considering to be B and that's exactly what we have here so that said we have number of classes which is simply from year 20 we have the image height and width which is considered to be 224 we have the split size S which is 224 divided by 32 that's equals 7 we've just seen that this is actually S in the paper we have number of epochs 100 learning rate defined although we have some sort of learning rate scheduling so we could take this off and here we could say 135 then we have a batch size of 32 so that's it we define all this and then we move to the preprocessing of annotations now given that this annotations are essentially this XML files we have here what we are going to be using is this element tree from this XML package which will use to parse this XML data right here so diving into the code you could see here we have the file name which is passed into this parse method from which we obtain a tree then from the tree we could get its root once we obtain the roots we cannot get the tree size see we have root that fine and we specify size and then once we have this tree size we could get the height of a specific image and its width here we have width width and height that is how we obtain this tool from the size tag so we could also obtain the depth from here let's copy this and paste out here and then let's say we want to get the depth here we specify depth and then you see we obtain a text so let's run that and then get the depth we run that and then let's have preprocess XML and then the file name its train maps actually it's actually a fire path train maps plus we have this file 207 207 oh oh oh 33 dot XML okay so it's actually this exact file here see this if you look up here or if you look at this here file name exact same file name so let's run this and then see what we get there we go we should have okay so we have 366 for the height see here we have 500 for the width and then we have three for the depth we've converted all this into floats okay so now we've done with obtaining the images width and height which are all in this size tag let's now move on to obtaining the different bounding boxes of the different objects so you see here we had root dot find size that's because we have a single size for that image now here you see we have rooted find all objects and that's because we will have many or we could have many objects for a single image here again single size but you're great no or a possibility of having more than one object so that's why you always specify find all so we are going to find all objects meaning that we are going to get into each and every object tag we have here you see this is an object this is another object if we scroll down we'll see we have another object so essentially in this image we have one two three objects okay so we have this three different objects and now for each and every bounding box in this objects like this is our object tree here if we pick out a specific object so here we pick out this object for example we have which we go through each and every bounding box in this object and we take its mean that's x mean y mean x max and y max so that's exactly what we do here you see we have bounding box that find now x mean and then we convert that to text we have y mean text x max and y max and at the end of this we now convert this into a float so that's how we obtain x mean y mean x max and y max now you'll notice that we have a break here and the reason why we want to have this is because for a particular object we just need a single bounding box so if we have other bounding boxes we are not going to take those into consideration okay so that said now what we'll do is we're gonna print out x mean y mean x max y max and yeah x max y max okay so let's print this out for each and every object now let's run this and see what we get as we expect you see we have 9 107 499 263 that is that what we have here we have 421 200 482 226 and then finally we have this so yeah what three different objects now what if we try out another different image so let's change this to 32 we run that and see what we get yes it's on XML 5 series 32 instead of 33 you see now we have actually four different objects and so we have this four different bounding boxes which you could see here we have object object object and object now if you consider this image here you see we put this cursor on this point we have could read from here we have 24 188 so matches are with this year see this 26 189 and then here we have 46 240 matches are with 44 238 now in order for us to make it easier when working with a yellow encodings what we are gonna do is we are gonna get the center of this bounding box so the center should be around this point here now that center is about 35 215 so here we have 30 oops let's get a pan so here is about 35 215 and then now we've gotten the center we could also get the width the width is about 18 and then the height the height is about 238 minus 189 that's 49 so we we now have the center which is this we have the width and then we have the height and then what we'll do is we'll divide all this by the total width and total height of the image so for 35 would take 35 divided by the total width the total width of this is 500 so we have 35 divided by 500 and then we'll have 215 divided by well this 215 is divided by the height that is 281 because this this year is xcoordinate and then this is our ycoordinate so this is respect to the width and then this respect to the height so this is divided by 281 okay so we take this divided by 500 and then this divided by 281 then we have the width which is 18 divided by 500 and then we have the height 49 divided by 281 and so instead of having X min Y min X max Y max we have the center which is divided by or which is normalized and then we have the width and the height which are also normalized now putting this in form of code once we're done with getting a specific bounding box we could go ahead and obtain the class name so you see we have as usual make use of our object tree and then we find the name see here here we have name and here is airplane here's a plane here's person here is person we're not gonna be interested in the pose or what else truncated or not or what else difficult or not just interested in the name and this bounding box just as we have seen already that said we have our class name from this class name we could create this class dictionary which will use to convert the different class names into a specific integer so what this simply means is we're gonna convert airplane to zero bicycle to one bird to two both to three and so on and so forth so this is 0 1 2 3 4 5 6 7 8 9 10 this should be 10 yeah 11 12 13 14 okay so we have airplane which is 0 and person which is 14 okay let's take note of that now getting back here you see we're gonna we make use of this dictionary where we simply have a class and that's converted into an integer so that's quite straightforward and then now we have our bounding box which is essentially x min plus x max divided by 2 that is we get the center this is the center and then we divide by the width so that's it we have x min plus x max divided by 2 times width is essentially the center divided by the width and then we have the y center that's y min plus y max divided by 2 that's the center then divided by the height and then for the width we want x max minus x min because to obtain the width to obtain this width you simply take this minus this to obtain the height we take this minus this and that's it so that's how we obtain the width and the height so here we have x max minus x min then divided by the width and then we have x y max minus y min divided by the height and then we have our class which is gonna be an integer instead of say person or airplane then once we are done with this bounding box we now store this in this bounding boxes list so let's create your bounding boxes list there we go we have the bounding boxes list and then now we will return bounding boxes so that's it let's return that and then there we go so let's run this and then see what we get now first thing you can notice is that we have our four bounding boxes now take note of the fact that we have the classes 0 0 and 14 meaning that we have airplane and person which matches exactly what we expect and then when we get back to this image you see 35 divided by 500 and 215 divided by 281 should have 35 divided by 500 and then 215 divided by 281 okay so you see we have 0.07 and 0.76 okay so does it make sense and then for the width for the width we had 18 divided by 500 and 49 divided by 281 so here we have 18 divided by 500 and then 200 divided by 281 0.04 or 0.036 and 0.17 0.71 this should be 71 oh let's get back here it's actually 49 divided by 281 and not 200 so this will be 49 because the height is 49 so we divide that and then see we should have 0.17 okay so that makes sense so that is it we have encoded bounding boxes and now we are ready to produce our outputs based on what was described in the paper so in the paper we saw that our output will be this 7 by 7 by 30 tensor where each and every cell we have here does each and every one of this 49 different cells because we have 7 times 7.49 will take values depending on whether they have an object or not now for a cell like this one that's actually matching up with this one where there is no object will take values like would have a value of 0 for the objectness meaning that there is no object and then for the positioning we will have this year that's four zeros and then for the class because there is no object we will have all zeros now we have 20 classes so we'll go from 0 or we'll have 20 of this zeros and there we go so you see we have 20 of this zeros now we're gonna have the same for each and every grid cell where we do not have an object now you should be noted that a grid cell like this one for example this one here let's change your color this grid cell here stick this well let's get back this grid cell here has actually no object because we consider a grid cell to have an object if the center of that object is in the grid cell now although we have the wing of the plane in this cell given that the center of this plane isn't in this cell we do not consider that this cell has an object so here would have exact same values we have here so there is no object like this is 0 1 2 3 this is 0 1 so we'll go 0 1 2 3 0 1 so here would have this exact same values all zeros and with this with this with this this and all this other cells now what we left with will be this cell which contains the center of this object this cell which contains the center of this object this cell which contains the center and this cell so we have four cells which contain the center or the centers of this four different objects while the other cells contain no object now we shall focus only on this one here so you understand how this outputs are generated based on the bounding boxes and this length this length here is essentially the number of bounding boxes so it's got it could be gotten from this bounding boxes so here we have bounding boxes which is in this case here for this object is actually what we have here that is we've normalize this value so that we have the center x center normalized y center normalized we have the width normalized the height normalized and we have this class now as we saw in the paper this isn't exactly what we want so what we want is a value which tells us the position of the object we respect to that specific grid cell so if we take off this year let's take this off and then we specify the center the center here is around this position here your so center will find that based now on the paper this position here has to be encoded such that we have the this value based off this origin so it's based off this origin because we have a grid cell here which is this one so we have this grid cell it's actually the same grid cell we have here so it's based off this and not based off this origin of the whole image remember the image has its origin and this grid cell has its own origin now let's shift this isn't very clear let's shift this so that we have this full okay so you see clearly now the origin of the image which is this and the origin of this accurate this point here and the origin of the grid cell which is this point and then we have the center of the image which is around this year so centers around this so the idea now is to obtain this distance from this year from well from this origin to this point and that's it so let's get this distance and this distance so that's it we need to get the distance from year to year this is from year to year as of now we have our center normalized we respect to the whole image and if we want to normalize this now with respect to a specific grid cell we need to take this value and multiply by the number of grid cells we have so given that we here we have 0.07 we'll take that and multiply by 7 so we have 0.07 times 7 which will give us 0.49 and then we'll take 0.75 0.75 times 7 which will give us about 5.25 now what this means is that the distance from year to this that's in this horizontal direction is 0.49 that's approximately 0.5 and this makes sense because the distance from year that is origin to where we have the center year of this object is approximately half of the distance from year to the full cell and then the distance from year this origin going the horizontal in the vertical direction to this center is approximately 0.25 so we go about 0.5 and then year about 0.25 nonetheless after multiplying year that's 0.07 times 7 we have 0.49 that's 0.5 that makes sense we have we also multiply 0.75 times 7 that gives us 5.25 but this distance actually only 0.25 so what we'll do is we'll take 5.25 modulo 1 and we'll obtain 0.25 because the distance from this origin to this center the center this is the center of the object is 0.5 in this direction horizontal and 0.25 in the vertical direction now let's make this bigger so you could see that even clearer what we're saying is we have a center which is around this and then we have this origin here the distance from year to year is about 0.5 of our cell and then the distance from year downward up to the center this distance is about 0.25 of our cell and we've seen that to compute this automatically what we need to do is get this already normalized values from year we already have this normalized values we multiply them by seven and then we compute the modulo of those where we find the output from year modulo 1 to obtain these distances so let's do 0.49 modulo 1 it should give you 0.49 and then 5.25 modulo 1 it gives you 0.25 so you see that now we have this center with respect to this grid cells origin and that's exactly what we had in the paper now diving into the code you see that we are going to create this output level which is essentially going to be a 7 by 7 by number of classes plus 5 that's 25 the number of classes 20 by 25 tensor and then we are gonna go through each and every bounding box we have here and then put in the values corresponding to the specific cells so again we have seen that for all these different cells here we have all zeros but for this cell this cell this cell and this cell we have nonzero values not all values are zeros so let's concentrate on this one as we see already so we have here for being range length that's for being the range of the number of bounding boxes we say that length is the number of bounding boxes we have the bounding box that specific bounding box this zero year is simply this so this is X center the center we multiply by the split size multiply by seven so you just like taking 0.07 times seven that will give you 0.49 so this is actually 0.49 if we are dealing with this bounding box here and then for this next one is essentially 5.25 so this is this times seven will give you 5.25 now or one other thing we need to do is we need to pick that specific or the specific grid cell to pick the specific grid cell out of all the 49 grid cells because we have seven by seven year this one two three four five six seven one two three four five six seven so we have 49 options we need to pick only one option and that's what we are doing right here so to do that what we need to do here is we take this 0.49 and then we convert it into an integer always simply round this down so running 0.49 down will give you zero and they're on in 5.25 down we give you five so it simply means that we are going in the X direction our is in the direction we had the zero position but in the y direction we go to the feet position so this is 0 1 2 3 4 5 and then the x direction we still at zero so that's how we we get this here and then so we have we have said 0 5 and then the output level 0 5 just like remember we had this output which is 7 by 7 by 25 this is 1 2 3 4 5 6 so at this other one 7 and then here we have 7 so we have 7 by 7 by 25 let's add this here we have this this this 1 2 3 4 5 6 and then we have 7 so essentially what we're saying here is 0 5 that's 0 and then 1 0 1 2 3 4 5 at this position right here for the first five values for its first five values would have 1 1 signifies that we have an object then for the positions you see we have 0.49 modulo 1 which is going to give us 0.49 then we'll have 5.25 modulo 1 which will give us 0.25 so now we're finding its position with respect to this cell and then we have the width we have the width here which is this bottom box 2 because it's 0 1 2 so this is the width but remember from the paper it's of what we need to have here was a width of this bottom box with respect to the whole image so what we have as value here will stay unmodified so that's it 0.036 0.036 and then for the next value is 0.17 0.17 so that's how we obtain the first five values you see we assign this values we have right here now for the classes we have from 5 right up to 25 so we have from last 5 plus this we assign the value of 1 now to understand why we what we are doing here remember that this bottom boxes or this bottom box of value or index value 4 takes in this value 14 so what we're saying is at the position 5 plus 14 we want to assign a value 1 now remember that we have this first five values which tell us the objectness or which gives us the object score which gives us a position and then the remaining 20 values will tell us the class of the object so after listing this we also have the class but we have 20 zeros see so again we have this objectness right here we have objectness we have bounding boxes and then we have the 20 zeros now when we say 5 plus 14 it means that we'll get to the 19th position this is 5 plus 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 so at this position here you're gonna take this off and then replace that with a 1 so here we have a 1 meaning that this year has an object this is bounding box and that object is of class person so you see here we assign this value 1 and that's it we output or we return the output level recall that we've just done this for this object but we have four different objects that's for in fact four different bounding boxes so we'll do for this we'll do for this we'll do for this and we do for this that's it at this point we could take all this off and then carry out some testing so we have this preprocess XML which outputs this year so we could do generate generates output which takes in pre well let's copy this here let's copy this paste out here the output of this is this bounding boxes and then scroll so you can see that clearly and then we have the length of that output okay so that's it we have this output and we have the length of the output let's run this and see what we get this isn't defined let's run this and there we go so we should have something reasonable from this you see we have this output and if you check this out you see 7 by 7 by 25 and then now what we could do is we could say okay want to get 0 that's 0 0 so 0 0 let's run that so we get 0 0 that's this cell right here you see all its values all the values of this cell let's take all this off all the values of this cell here are zeros which makes sense now let's do 0 5 0 5 we run that you see we have exactly what we expect here we have 0.49 we have 0.31 meaning that the distance from here to the center is about 0.31 here we have 0.036 we have 0.174 and then we have a 1 at this position in that as a person now if we go 0 1 2 well this is 0 1 2 and that was in the direction and 0 1 2 in the vertical direction so this is 2 2 let's do 2 2 and see what we have we have 2 let's get back 2 2 we should have an object there see we have an object so we have here 1 we have his bounding box and then notice how this is his class remember the aeroplane the aeroplane as we had here was the very first class so it makes sense that we have a 1 at this very first position so that makes sense so that's it we see how we could generate our output from this data set we've we've been given all this XML files we have for each and every image before we move on we are going to do some slight modifications on the code so here we no longer need this length we have the bounding boxes then we're gonna create this numpy array instead of the tensor flow variable we had before so it's exact same shape as before still our output level and then we'll get the length directly from this bounding boxes so we have for being range length of the bounding boxes then to account for the fact that we are gonna have this computations bashed we are gonna add this three dots right here so we have now our grid X computed similar to all of your scene and then we have our grid Y computed still our i j garden and then here again we add the three dots to account for the batch computations and so that's it this exact same code and then now we're gonna convert this numpy array into a tensor so use the convert to tensor method in tensor flow so that's it we run this and we still have our same output now to ensure that our validation set doesn't get mixed up with the training we are going to define this set of 64 images which will be our validation set or which will make up our validation set there we go we have this vowel list and then the next thing we'll do is we are gonna copy as you see here we're gonna copy or rather we're gonna move this files into this two directories so here if you open this up now you'll see we have we've created this directory vowel JPEG images which is this and we've created a directory vowel annotations which is this other one which contain the validation set images and annotations and so that's it so now we're gonna create this different lists here we have the image paths and the XML paths we have the validation image paths and the validation XML paths let's run this you see we have 17,061 files for the training and then 64 for the validation and that's it again you should note that this training images has already been defined here we'll define training images right here we've also defined vowel images and vowel maps so that's where we getting all this from that's how we obtain all those different paths now from here we're gonna create the tensorflow data sets that's the train data set and the validation data set which is essentially going to be made of these different paths which we've just created so we make use of this from tensor slices method and we put the image paths and the XML paths and the validation paths and the validation XML paths so let's run this and then we could visualize our validation data so here you see we have this path here that's our image path and then you see we have this on the path which is our XML file so we have the image and its corresponding XML file then now that we have our image paths and our XML paths already making up our data set from this tool we could obtain the image and the bounding boxes for the image all we need to do is pass in the image path in this read file method then decode that read file and then we go ahead and resize and cast the image so let's take this off we actually do the casting here so no need doing that before resizing so as we say we resize and then we carry out the casting and we obtain our image from this image path now for the bounding boxes remember we looked at this preprocess XML method which was already explained here we looked at this method which takes in our XML path our file or file name and then outputs the bounding boxes so that's exactly what we're doing here and so that's it now given that this method does a preprocess XML method isn't made of only tensorflow operations will need to make use of this numpy function method where we are going to pass in our function here we specify the input which is the path that's our XML path and then we also specify the data type of our output tensor which in this case is float 32 so this let's let's say this is XML path and here we have XML path okay so now we have the path and we have the method we can now obtain the boxes and then now we have our train data set which is going to be redesigned such that we have this year we have this paths the image path and the XML path which gets in here and then outputs the images and the boxes so you see train data set we map and then we specify the method which is a get image and bounding boxes this we get the images and the bounding boxes from the image path and the XML path so now our train data set is no longer going to give us the the image path and the XML path but it's going to give us the image itself and the bounding boxes so let's run this and then see what we get there we go you could see we have this image and then we have its corresponding bounding box in this case we have just a single bounding box let's go ahead and write this then we check this out here there we go we have this output you could see so you see we have this output here and it's showing that we have an airplane so if you check this out you see we have the bounding box here and then we have the class the class is zero so if you scroll to the top you would find that the class airplane was here so this is the zero class which makes sense now let's try out with some others well most of those ones here have only one object but there's this one at this 18th position let's keep let's keep that and then a break there we run this again and then see what we get okay so this this particular image has several objects so we could check that out run this okay so you see here we have now many more objects here we have many many people actually you have 14 and this happens to be person so if you if you have here yeah we have this well okay so if you do classes well the list is classes so let's just do classes 14 and we get the exact class we have the classes 14 and you should see you should have person okay so you see we have 1 2 3 4 5 6 7 8 persons and if you get here you see we should have 1 2 3 4 5 6 7 8 exactly what we expect and then we have 10 8 8 well this 8 this 8 here and then after we check out 10 it is cheer so maybe is this cheer okay it is twice so we have a cheer year and we also have another cheer year and then 10 is let's check that out should be dining table okay see we have the dining table we have dining table we have chair chair and then 8 persons so now we have the image and as button boxes the next thing we want to do is have the image and its output levels remember with our generate output method which we had seen already here it takes in the output boxes and outputs the levels so let's get right here and then you see we have this preprocess method which takes in again this image and this button boxes and then right here we are going to output the image here just simply output an image but for the button boxes we need to convert this into output levels so we make this of generate output method which itself takes in the button boxes and then we specify the data type of the output tensor again we're using this numpy function method right here because this generate output isn't made of only tensor flow operations so that's it we'll run this and then there we go our final steps now will be to batch our data set and then implement prefetching so let's run that and then we have our training and validation data sets which have been prepared hello everyone and welcome to this new and exciting session in which we are going to build our own YOLO like model so from the paper we had already seen that we have this initial conf layers which are pretrained on the image net data set such that they could be used to extract very useful features from our input images and then this conf layers were followed by two fully connected layers which were designed in order to adapt to our problem of object detection now given that we do not want to train this backbone here from scratch on the image net data set we'll use an already pretrained backbone which is the ResNet 50 again we have the output dimension defined as number of classes plus 5 times B from the paper B is given to be 2 we've seen this already and then 5 because we have the probability of obtaining an object and then for the remaining 4 we have the bounding box so we have two of this bounding box predictions that's why here we have 2 times 5 10 plus the number of classes which in our case is equal 20 now we define this number of filters to be 512 so that's it from here we have our full complete model you could take this off you see that we have our pretrained ResNet which is our backbone our base model and then we follow this up with several conf layers similar to what we have here in the paper and then we have the global average pooling which is what is given here in the paper now one thing you should note about the average pooling is the fact that when we have inputs let's say we have this 7 by 7 then by let's say 5 so we have 1 channel 2 3 4 and then 5 so let's suppose we have 7 by 7 by 5 input now after going to the global average pooling what we will have here is the averaging of each and every value or let's say pixel in each and every channel so for this channel for example we'll have one representative value for this channel will have one representative value which is the average so for this who would have the average this would have the average and this would have the average average. So we average all those values here. And the problem with this is information about object position is lost. And so instead of using this average pooling is preferable to use the flattening. And so what we'll do is we'll take this off from here, and then we'll have flatten. Okay, so once we have that, just with the paper, you see here, we have the fully connected layer, which is this dense layer right here, and then this other fully connected layer. So we have that, and then we reshape. Now this should be actually split. Let's take all this here, copy and paste. So split by split by split, or split by split by output dimension. So it's 7 by 7 by 30. So this is now our model. You can see we have a total of 53 million parameters, 30 trainable and 23 non trainable. That's from our ResNet. We are now going to get into the YOLO loss. So here we have this YOLO loss method, which takes in Y true and Y pred, where this is our target Y, and then this is our predictions. Now the YOLO loss we have here will be an implementation of what we already discussed from the paper. And so it's made of four different parts. The first part is for the coordinates. The next part is for the object. The next new object. And then finally we have the classification. So we have this four different parts. Now we're going to start with this first part here. We're going to start with the object. So in this case, we shall penalize the model for not detecting an object at a particular cell. And that's why you see here we have this one OBGIJ, which denotes if the object appears in cell I and this J right here. Notice that here we don't have a J. Here we have this J. It denotes the fact that the J bound in box predictor in cell I is responsible for that prediction. Now if we take a look at this figure right here, where we have split this image, which is actually 224 by 224 into seven different parts. So it's basically now seven by seven grid cell image. And each and every grid cell, as we have seen, has its own predictions. Now if we specifically pick this grid cell right here. So let's pick this one. As you see, we've picked this. Now we have two outputs or two possible outputs. We have the Y true, Y true. And then we have the Y predicted by the model. Now you'll notice that if you count this, it's going to be a total of 13. And this is going to be a total of 18. As we've seen already, this first year represents whether we have an object or not. The next four, which is all in green, is the position of the object in the image. And then this other eight specify the class of that object. Now before we move on, you should note that when we first recorded this section on the yellow loss, we were working with a data set with just eight classes. But since we now dealing with a Pascal VOC data set with 20 classes, our Y true will look instead like this. So now we're going to have this additional 12 values here and this additional 12 values here. So instead of having 13, 18 as with eight classes, now we are going to have 25 and 30. So it's like 13 plus 12 and then 18 plus 12. Nonetheless, this doesn't change much on our yellow loss method. So wherever you have eight classes, you should consider in our specific case of the Pascal VOC data set, we're dealing with 20 classes. Now we had also seen previously that although we have this Y true, which has this 13 different values for each cell, this Y pred has actually 18 values because we have two possibilities or two possible positions of the object. We have this first possible position. Well, let's call this B1. And then we have this other possible bounding box, which we'll call B2. So we had already seen previously that we said B to two because we have two possible bounding boxes. And we also saw that even though our model or even though our data was designed such that the outputs were 7 by 7 by 13, the model outputs 7 by 7 by 18 outputs as you could see here. And so when we have a loss function like this, where we are given the Y true and the Y pred, where the Y true is in fact 7 by 7 by 13 and the Y pred is 7 by 7 by 18. Since we want to start with the object part of our loss function, which is this part right here, our focus should only be on these two grid cells, this grid cell and this other grid cell right here. So we want to focus only on this grid cell and this because these are the only two grid cells where we have an object located. Now the way we shall select this programmatically is by gathering all those grid cells where the target is equal to one. Now our target here is simply Y true where we've selected only this object score. So if you look at Y true here and if we pick this object score that coincides with this one we have here. So in the case where we have an object, we will obviously have a one for the Y true. And so when we say we are going to pick all the different cells of Y pred where we have a one at this Y true level, that is this first for this first value of Y true, it means that we'll now have Y pred, which is going to be all these different cells where we have actual objects. And then we're also going to do the same for Y target or Y true. So here we have Y pred extract and then we have Y target extract, which is simply taking or getting all the positions where we have the objects, but this time around for Y true. So now we have Y pred where there are objects and we have Y true where there are objects. And with this, we can now focus on only this two cells, this cell and this cell. Now let's take this off and run this so that you could see what this produces. So we run that. Let's, let's print this out. So let's print out Y pred extract and let's print out Y target extract. Well, we could also print out target. So well, let's just print out this, this way right here. So let's print TF. Let's copy this. We'll get this. So let's paste this out here. There we go. And that should be fine. So let's run this and then now we'll test with some inputs. Now the inputs we are going to be using will simply be the exact same coordinates we got from the dataset. So here you have this and then you have this where this year corresponds to this vehicle right here. And then this one corresponds to this other vehicle. Now we had seen already how we could use generate output method to produce our Y true or our dataset value for Y or the output. So once we have Y true, we are going to add an extra dimension. Now we have Y true ready. Then for Y pred, we'll just generate this random values. And then what we'll do now is for this specific values or for the specific cells, that's one four and three two, we are going to put its own values. You'll notice that here we have 18 different values where here we have the probability of having an object. We have the position 1234. There we go. We have the position, the probability of having an object. We have its position 1234. This is 1234 here. And then the photos rest here, we have the class or the different class probabilities. Now we could do the same for this other one. We have this and then we have 1234. Okay, so there we go. So this is what we have as our Y pred. So we're supposing that the model predicts this and then this is our Y true. Remember we had already seen that this Y true here would produce our datasets output. So now let's run this. Let's run this and then run this and then see what we get. Okay, so there we go. You could see from here, we have all the different positions where this target value is equal one. So that's what we printed out here. And you could see clearly that we have one four and three two. Now the zero year is simply because this is the first batch value. Now that's it. We have this 1432. For the next we have Y pred extract. So want to extract only those values where we fall in this grid cells. Now you'll notice one thing that you have 0.9, 0.47, 0.31, which coincides exactly with this. And then we have the sort of one 0.3, 0.01, which coincides exactly with this. And so clearly we are only focusing on the models outputs where we have actual objects from our dataset. So we've, we've picked this year and we've picked this and then now we're ready to compare what the model produces at this positions and what was expected, which is this Y true now. So this is Y pred and then this is Y true. We're comparing only at this two cells. You see here we have two, two. Now what if we suppose that we have only one object? So if we have only one object, what we'll do is we'll take this one off. Let's run this again. And you see now that we only have one, which is picked. So let's get back and run that. And there we go. So you see, we've, we've picked this, we've picked this cell and this other cell. And we already now as described in the paper to compare the different probably discourse with one another. So we just simply have to subtract this and that would be good for this part of our loss. Now, if we only had a single bounding box prediction, like here, if we didn't have this, so let's suppose we had only this and the Y true only this, then what we'll do is we'll take this one minus whatever value we have here and then we'll subtract. So if Y pred is one and Y true is one, we'll just have one minus one. We'll subtract that and then we square it exactly as we have in the paper right here. You see, we have the object score minus the target object score. And then that squared. And from here, we add this to all the other positions or all the other objects at different grid cells. That's why we have this summation. Anyway, we had seen this already. So getting back here, now that we actually have two bounding box predictions, as you could see, we have B1 and B2. What we'll do is we'll take whatever value we have here and then for this, let's wipe this off. Let's say this is a dash and then this is a dash or some value. Let's call this lambda and we'll call this lambda. What we'll do is we'll take this one minus one of this, that's either lambda one, if this is lambda one and this is lambda two. So we would have one minus lambda one or we would have one minus lambda two. And the way we shall pick between lambda one or lambda two is by looking at this coordinates right here, this positions. So if this bounding box we have here is closer to this other bounding box, that's a true bounding box, then we'll pick lambda one. But if lambda two's bounding box that this is closer to this true bounding box, then we'll pick lambda two. So that's how we do the peaking. And the way we would compare this bounding box with this and then this other bounding box with this is by making use of the IOU's course. So let's take this off and suppose that we have this input image. Let's take all this off too. We have this input image and then we have this bounding box. So let's say this is the true bounding box here. This is our true bounding box. And then we have this bounding box B one, which is something like this. So let's say here we have a lambda one and we have its coordinates. So we produce something like this. Let's change this color. Let's say we have something like this. See, so we have something like this. So this is what we're getting from this one year. And then we have this other bounding box where we have maybe say something like this. Now, in this case, because this year let's get back. So we differentiate between the two bounding boxes. Let's redraw this year. And then we suppose we had this. Okay. So because this green bounding box is closer to the red bounding box, we are going to pick lambda one. And simply we are, we are going to take this into consideration. We're not going to take B two into consideration when computing objectness part of our loss. And so getting back to the code at this point, we have to subtract 0.9 with one that we have here, all would subtract. This is 1234 0.8 and one. So we have to, we actually have this or this and the choice between this and this will depend on this coordinates and this coordinates. But then our IOU method is designed such that it actually takes in pixel values. So what that means is we could design our IOU method, which will take the center and the width and the height of these two boxes. Let's say we have this, this box and this other box or this rectangle and so the rectangle, we take the center of this, it's width, it's height. We think sort of once center is width, it's height. And based on that, we compute the IOU score. But the point that we have in here is that what we get in here is not the pixel values. What we have here is this preprocessed values, which we had seen already. And so if we have the center here, you could read that from here. If we had the center, which is at position, let's say 43, let's fix this to this. We have 46, 140. So we have this center, which is at around 46, 140. Now we have, well, 46, take this off. We have 46, 140. If we focus on only this first coordinate here, then we would have 46, modulo 32, which will give us 12. All right, a 14. Let's get back. This will give us 14. And then now this 14. Yeah, we move to the next step. 14 divided by 32 would give us, 14 divided by 32 would give us 0.44. So we would have 0.44. And this is the percentage occupied in this grid cell. Remember, to obtain this coordinate or to obtain this value from the original center value of X would have to make sure that we get this position of the center with respect to this grid cells center, which is right here. So this distance from year to year is about 0.44 as we've just calculated. And that's exactly what YOLO takes in. But since now we need to get back to this original values from these values we used to train, because we need to be able to compute the IOU scores between a given box and some other box, then what we'll do is now reverse this process. So we are going to go from what we get here. Like for example, this value back to this value 46. Now, if you're wondering why we picked 32 right here, you should know that this image is 224 by 224. And if we divide 224 by 7, you would have 32. So each cell, though this is not from year to year is 32, from year to year is 32 and so on and so forth. So if from year to this point here is 44 or rather 46, then if you want to get this remaining distance, you just need to do 46 modulo 32 to have this remaining distance in year. And then you divide this distance by 32 to obtain the fraction of occupied by this distance in this full grid cell. So with that, we are going to also repeat the same process for this height coordinate. So this is a center X and center Y, we will take 140 modulo 32, we obtain this value divided by 32. We now get this fraction occupied from this year to the center. We've got in this already, this is 0.44. While we could also get this Y and so that's how we obtain this fraction as we have here. Anyway, we've seen this already. But now what we're trying to do is get back from this to a value like 46 or from this to a value like 140. Now we should be noted that for the width and the height is going to be quite easy because remember when designing this or when obtaining this, what we had simply done was we took the distance or we took the width of this and divided by the image width. So if we take this and divide by the image width, then to obtain the original width, all we need to do is multiply by the image width. So if we had originally, let's say this width was originally 20, for example, then to obtain what we have here, this width and this height would take 20 divided by 224. We get some value. Let's say approximately, this is like 111, something like that. So it's like 111. Let's say approximately 0.11. Okay, so if you have 0.11, then now to obtain 20 from this, we just take 0.11 times 224. So that will give us back 20. So that's how we reverse this. It's easier compared to this where we need to go through all these two steps before getting back this original values. And again, the reason why I want to get this original values is simply because we want to be able to compute the IOU between a given bounding box and this two bounding boxes here. So we make the choice of whether to say 1 minus lambda 1 or 1 minus lambda 2. Now let's dive into how we move from 0.44, 0.375. It's actually 0.375 to 46 and 140. So the first thing we'll do is we'll simply multiply this 1, 4 by 32. Now 1 will become 32 and 4 will become 128. And this will take us from this origin right up to this position. So this will be found at this point here, at this point you have from year to year 32. We've already seen that each grid cell here has a size of 32. So from year to year 32, from year to year is 128 because we have 32, 64, 96, and then 128. So you see we're already getting close to the center of our object. And so that's why we have this rescaler right here. We have this rescaler, which is gotten by multiplying those values where the target equal 1 by 32. So all the positions where target equal 1 will multiply by 32. And so now let's run this. And you see that we have 32, 128, and for 32 we have 96, 64. So for this other object here, for this object we have 1, 2, 3, that's 96, and then 64 is 1, 2. So we add this position. We're now left to get to the center. Center should be around this year. Anyway, we've taken this first step. Now the next thing we want to do is get rid of this batch here, the batch dimension. We don't need that. So we'll just have 32, 128, 96, 64, and then we'll add two zeros for the width and the height. So here is width and height. Well, this is X center and Y center. Now to do this, we are going to simply take off the batch. So you see, we start from one, we take off the batch dimension, which not consider the zeroth index. We take that off and then we add two zeros. So here we just add this two zeros. Now the number of lines we would have would depend on the length of our rescaler. If our rescaler is the length of two, as in this case, then we'll have two by two metrics, which we are going to concatenate with this rescaled output. So essentially we had this year, we had this 32. Let's rewrite that. When we take only this first index or from the first index to the end, we have 32, 128, and then we have 96, 64, and then we concatenate that with a metrics, which is two by two, where we filled all those with zeros. And so now we have this year, which represents X, Y, and this height or width and height. The next thing we'll do is we are going to take this target coordinates. Note that we take from one to five. So it's essentially X, Y, W, and H, and they will multiply by 32, 32, 224, 224. Now let's explain why we choose to multiply by all this. So let's suppose that you have, for example, 0.44, you know that this is 0.44 right here, and then you have 0.375 for Y for the center. Now taking 0.44 and multiplying by 32 will give you this distance from year to year in pixels. So if you have 0.44 times 32, you should have 14. Now remember that to have 0.44, you had 14 divided by 32. And so now to get back 14, you simply have to do 0.44 times 32. And that is exactly what we do here. You have this 32 times 0.44, and then you have 32 times 0.375. So once we have this first two, now you have the height and the width, where you have the values multiplied by 224 and 224, which makes sense because as we've seen before, to obtain the width and the height, we divide it by 224. And so now to get back to the original width and height, we need to multiply by 224. So we essentially have in this year multiplied by the coordinates or bound in box coordinates. Now the reason why we repeat in year is simply because we could have several objects. Now in case we have two objects, so we just repeat this twice and carry out the same calculation for the two different objects. And this repeats here where we specify the length of the rescaler. The length of rescaler is equal to 2. So this permits us to repeat twice. So that is it for the target where we've taken this value, because this is 1, this is 0, 1, 2, 3, 4. So we go, when we say 1 to 5, we're taking 1, 2, 3, 4. So we're taking this, this, this, and this, multiplied by 32 for this, multiplied by 32 for this, multiplied by 224 for this, and this by 224. And that's how we obtain this distance from year to year in terms of pixels. So from year to year, we get the distance. And then now, we could also repeat the same process for the other two predictions we have for the prediction one, and then we have the prediction two. For prediction one, we simply do the same. You see here, but the difference is why prep, and year is why prep, unlike year, which is why target. We still have the same calculation. And note how year we go from 6 to 10, because this is 1 to 5, and then this is 6 to 10 for bound in box two. So that's the only difference we have here. And then we would get the pred one and pred two. So at this point, for this object year, we note the distance from the origin right up to the nearest cell, which is this distance. In this case, it's 32. We had seen that already. And then we also notice distance from year to the center of the object in terms of pixels. And that's what we just calculated. And that gives us about 15.21. So we have now 32, and we have 15.21. So it means we have the center or the distance from year to the center in terms of pixels. Now for this 128, you see, we're going to go from year to year. We know that this is 128. And we also have this distance from year to year, which is 10. So now we could add this up for the width and the height, we have simply taken what we had and then multiplied by 224. And that's what we've had. So we don't need to do any modifications here. Now you will notice now that because we have this, we could add this with this, this with this, zero with this, zero with this. And then we could get our output now, which will be this width and the, or rather this center, respect to this origin and the width and the height respect to the full image. Now to make it very clear, let's, let's consider we had only one year. So let's take off this prediction and then take off the sort of prediction. Suppose that we had only one object. So you could see on that very clearly, you see here we have 96, 64. Well, this should be on the order because we took this one off. We should instead take this year off. Oops, this should be the one taken off. Okay. So let's run this again and there we go. So you see here we have 32, 128, 0, 0. We have 15, 10, 28, 52, where we will take now 32 plus 15 and then we'll take 128 plus 10. We'll take 0 plus this, 0 plus that. And that gives us the, the width and the height and also the center. And then for the predictions, we will take this plus this, and then we'll take this plus this, take this plus this, this plus this, and that will be it. So finally we'll be able to obtain the bounding boxes in terms of pixels for the target and for the two predictions. So finally now we are going to take what we had here and then add with this other one target this of scalar one and target of scalar two. So let's run this so you could see the outputs. There we go. You see we have now 47. We've taken the 32 plus that 15 and we've had 47, 128, this, this, we've had this, this, this, this, and then this. So we now have those bounding boxes in terms of pixels. So the next step one we follow is actually compare this box with this and then compare this box with this and then see which of these two are actually closer to this. Now, one thing we did while designing this, why Prez was to ensure that the first one, this one was going to be the closer one because you could see here 47, 47, they are actually almost the same values so that when testing, you'll see that this one is closer. So to compare this, we'll need the IOU score and to compute this IOU, let's consider this simple example where we have two boxes, this B1 and B2. Now the IOU, as we saw already, is simply the intersection over the union. So we have to compute this, it's going to be this here intersection divided by the union. So it's essentially this region, as we've seen already, divided by all this, including this intersection region. So it is this divided by this. Now starting with this intersection, what we'll need would be this coordinate here and also this position and this here. So the way we're going to get this point is by starting by getting this point, this point here, this point, this point and this point. But remember that we're actually dealing with the center and the width and the height. So we are not having the X min, Y min, X max, Y max and X min, Y min, X max, Y max values by pass in here. What we pass in here is, what we actually receive there is the center, the width and the height. Now the first thing we have to do is to convert these coordinates given to us in this form of center, width and height to one in which we have this X min, Y min and this X max, Y max. And to do that, we have this piece of code right here. Now first thing you have to note is the fact that we have 0, 1, 2, 3, where this represents our X center, Y center, width and height. Now if I'm given this here, X center, Y center, to obtain this position X min, Y min, it suffices to take for example this XC, which is this distance from year to year and then subtract it or take away half of the width from it because from year to year is the width and then from year to year is half of the width. And if we take XC minus half of the width, then we get back to this position. And then if we take YC minus half of the height, then we get this position here. So at the end of the day, we get back to the origin, obviously of the specific box. So if I want to get back to the origin of the specific box, which happens to be X min, Y min, I need to take XC minus half of the width and YC minus half of the height. And so you'll see here we have this is XC, this is X, we've seen this already, minus half of the width. The width is two, see the width, half of the width, that you see width divided by two and then XC, YC, which is this minus half of the height, see three year. So that's what we do to have X min, Y min. And now if we want to have X max, Y max, then we have to take XC plus half of the width and then YC plus half of the height. And that's what we do here, just simply replace and put the plus and that's fine. So that's how this box is now. This box one is converted to box one to you where we now have X min, Y min, X max, Y max. And we'll do the same for this second box. Now, once we have this, the next step will be to get this coordinates here. Now it happens that if we want to get this X min, because remember, this is the intersection. Now this intersection forms another box. Now to get this year, it suffices to compare this X min, Y min and this other X min, Y min. So we take the X min, Y min of B1 and the X min, Y min of B2 and look for the maximum between the two. And is this maximum or the one which is right most because our origin already is at the top left corner. So increasing is this direction, increasing is this direction, increasing is in this direction and this direction. So when we say maximum of this and this, then we're looking for the one which is in the right most direction. So when we compare in here, you see, we're taking the first two and the first two year. So we're comparing the Xs and X min, Y min actually. So when we compare the X min, Y min, we get the one which is maximum and that's the one which will play the role of the X min, Y min of this intersection rectangle, which will form here. And then for the X max, Y max, we need instead of minimum. So we need the one between these two, between this and this, we need a one which is to the, which is left most. So that's why we need to take the minimum and we compare the last two indices, that's X max, Y max, as you could see. And then that's how we obtain this position and this position for this intersection rectangle. Now, once we've gotten that, that's X min, Y min, X max, Y max for this intersection. The next thing we want to do is actually compute the width and the height so that we could multiply and get the area of this year. Now to get the width and the height is quite simple. We just need to take, we need to subtract this simple because here you have, let's write that again, we have X min, Y min, X max, Y max. So you just take X M that's X max minus X and then you multiply by Y max minus Y. That's all. So this is the width and then this is the height. Take this minus this times this minus this. That's all. So that's what you see is, which is actually done here. You see we've, we've done the subtraction from here. We've done the subtraction and then we now multiply this two and that's how we get the area, the intersection area. Now, once we have the intersection area, the next thing we want to do is obtain the union area. So we take the box one, you see two is the width and box one again, the height, multiply the width times height. We have this area. Take for the box two, width times height, we have this area. Now we add these two areas up. You see here we add these two areas up. We remove the intersection because we want to get only this and not, we do not want to add this twice because when you calculate this area, you already have this. And when you see calculate the area, you still have this. So you want to take off the intersection because it's going to be completed twice. And then we now have the full union. And so now we have intersection divided by union and that's it. So once we have intersection divided by union, now we will be able to compare this with this and this. And so now in order to compare this different boxes, we would have the target box compared with the second prediction. And we also have the target box compared with the first prediction box. Well, before doing this, we could actually print this two out separately. So let's print this one out. Let's do TF print. Second, second box that's comparing the target with a second box. Let's have that there. And then we would compare the target with the first box. So let's print now first box. There we go. We have now print one. Okay. So that's it. We'll run this before having this. So you better understand what we're doing here. So let's run this. Let's take this off. We could take this off and then run this. You see that with the second box, we have 0.16 and with the first box we have 0.93, which makes much sense because when you look at those predictions, the first box looks much more similar to the target as compared to the second box, which is this one. So it makes more sense that the first box has a higher IOU score. Now up line the tensor flow math greater method, we'll be able to have this Boolean output, which tells us whether this second prediction is greater than the first prediction or the IOU between the target and the second position is greater than the IOU between the first and the target. So given that in this case, for example, this IOU between the first and the target is greater than the IOU between the pred two and the target, then this will output false. And since this will output false, casting this to an integer will produce an output of zero. And the zero will simply mean that between the two options, that's the first output and the second output, this is actually the first output. And then this is the second output or second bounding box between the first and the second bounding box. This is the one which has, or which is closer to the target. And that is exactly what we want to have here. So there we go. We have this output zero. Now let's include this other example. So now we'll suppose that we have two objects, this object and this object by adding this we've added this other one year, which happens to be this object. Then we'll uncomment this part and then see what we have for the mask. So there we go. You see, we have zero one, meaning that for the first object, that is this object here, it is this second box, which is this one year, which is closer to the object or to the target. And then for the second box that for the second object here, it is this first, which is closer. Now let's, let's interchange this. Let's take this off here and paste out here and then take this from here and then paste out here and see what we get. Take this off and add a comma right here. So now because this one year will be closer, we should have this one being picked. So we should have one, one. Now let's run this and see what we get. See, we have one, and you could see that from here. The second boxes have higher IOU scores compared to the first boxes. Okay. So now we have the bounding boxes, which are closer to the target. Like in this case, we know that B1, B1 is closer to the target as compared to B2. So the next thing we need to do now is get Lambda one and Lambda two. And then from this Lambda one, Lambda two, choose Lambda one. Now let's start by getting Lambda one, Lambda two. It's going to be quite easy. All we need to do is pick this zero value and this fit value. Remember this is zero, one, two, three, four, and then five. So this is a fit. So that's why you see, we picked this. So this is the probability of having an object for B2. And this is that for B1. In fact, this is Lambda two and then this is Lambda one. So we concatenate this and then we rearrange this by transposing. So let's run this and see what we get. There we go. As you could see, we have 0.9 and 0.8. That's it. And then we have 0.3 and 0.98. And so what we'll do now is based on the mask, we'll say, okay, for the first value, which is zero, it means that we are going to take this one instead of this. So we're going to take this probability instead and then move into the next position or move into the next object. We are going to select the value number, this first index or this first or the second value. So we'll skip zero and then we'll pick one. So for the first one, this is our Lambda one we picked. And for this other one, Lambda two will be picked. Remember that if we change these positions, then we'll pick Lambda two for this one and Lambda two for this one, because this will be one one. So since this is zero one, we'll pick Lambda one for this first object. And then since this is one year, we'll pick Lambda two for this other object. So now to do that programmatically, we are going to gather all this Lambda values that this probability is here based on the mask. So doing that, you'll see that we'll be able to pick those values of Lambda corresponding to those bounding boxes with the higher IOS cards with respect to the targets. So we've seen how when given this, let's take a single example. So let's comment this again, comment this and this, run that. We're seeing how we're able to pick these two bounding boxes, that's this bounding box and this other bounding box. And then from this, pick the one with the higher IOS card, and hence pick the probability which we are going to use to subtract from one. So now we know that we will have one minus 0.9. Now let's reverse this. Let's reverse this again, reverse this, take this off, and then we paste this out here. Then we take this off. Then we paste this here. Okay, so let's run this and then see what we get. You see now we have 0.8 that we've picked this other one instead because this bounding box is having a higher IOS card with respect to the target as compared to this other bounding box. So this is how we are going to pick the bounding box, which we are going to use for computing the objectness part of our loss. Okay, so finally now we just need to compute a difference between what we have here. That's in this case 0.8. For example, let's get back. So we have what we had originally, which is going to be 0.9. So different between 0.9 and 1. So you see here we have 1s, which you could see the length is based on the rescaler. So since we have only one object, this is going to be 1. So 1 minus 0.9, and that will be it. Now this difference method we have here has been defined. It's basically the square of Y minus X. So we take two subtract, find the square as defined in the paper, and then we have this reduce some method to get a single value. So that's it. We run all this and we could now print out our object loss. So we have print object loss. There we go. Let's run that. And you see what we have 0.01. You see that that's quite small because 0.9 is close to 0.1. Now let's just modify this and say, okay, let's say 0.09. See that it's going to be having a higher loss. See that we have higher loss. Now that's it. So that's what we do. Now you see that no matter what we do here, even if we put here 1.0, we'll never have a loss of 0 here. And that's because it's this one, which is used to compute that loss. We are now going to move to the new objectness part of our loss, which is this year. And it's kind of similar to this calculation. But what the difference is that now we focus in on those regions where we do not have objects. So unlike here where we focused on this year, this cell where there's this object and this other cell, what we'll do now is we'll be focusing on all the other cells except for this one and this one. So as you could see, we get all the white prints. We gather all these predictions where the target is 0. That is simply where there is no object. So let's print this out. Let's print out these different positions where we have no object and the corresponding predictions. Here we have TF prints, Y, print, extract. So this is seven by seven meaning that we have 49 different cells right here. Now two cells have objects and the remaining 47 do not have objects. So let's run this and see what we get. And there we go. Here's what we have. So you could see clearly from here that we have all these different cells and let's do print so we could get its shape. There we go. Let us run that. You could see this is actually 48. Now that's because, okay, so that was because we had only one object. So if we have two objects, two objects, let's get back. We have two objects. So let's run this again and see what we get. Okay. So you see now we have 47 cells where there is no object. So that tells you that out of the 49 we have two where there's an object and then this Y, print, extract here now contains all this score and bounding boxes. You could see from here that this is 47 by 10. So let's get to the bottom. You see 47 by 10 because for each and every one of this, we could take this off from here. We'll make this connection from a cell where there is no object. So this cell, for example, here we have this 10 different predictions and then now we will make use of this to compute the loss. So obviously the no objectness part of our loss will be computed using this lambda and this lambda. So we'll take this one or rather zero because we expect it to be zero. So this time around our Y true will be zero because when there is no object like here is zero. So this is going to be zero minus lambda here minus lambda one square plus zero minus lambda two square. So the idea here is to ensure that this probability is equal to zero when there is no object and equal one when there is an object. So getting back to the code, you see that we break this up into two parts. This is like getting the lambda one or competing with the lambda one and this is like competing with a lambda two. So unlike with the object part of the loss where we're trying to pick which of this was responsible for the prediction, here we just simply take this minus this plus this minus this. So our target is obviously zeros unlike here where our target was once. So now we have zeros and then we compute the difference between our Y target here and the Y pred. Now we pick the zero for this one and then we pick this five for this other box right here. So that's it. We simply find the difference or compute the difference using this method which we've defined already right here. And then we sum those two up to obtain our no object loss. So that's it. Let's print out or let's take off this here and then print out our no object loss. We have no object loss. There we go. You see we have 110. Now we'll move to this next part. That's for the classification where we'll focus on the objects class. You'll see that we are going to only compute this or get this loss for cells where we have an object here only where we have an object. And we do not care about which of the bounding boxes is responsible because we actually focus in on only the classes. So instead of i j here we have just i because we are not focused on choosing or we are not interested in choosing any specific bounding box given that it doesn't really matter since we're focusing on classes. So getting back here we have for object class loss we have the predictions and the target. Now if you check from here you remember that the target will start from five because this is zero one two three four and then five. But for the predictions it will start from ten. So you see we have zero one two three four five six seven eight nine and then ten. So we start from ten right here. So this starts from five and this starts from ten and we go from ten to the last and then we go from five to the last and I will simply compute the difference between this and this. So that's it. We also make sure that's where there's an object. So now you have this difference and you obtain your class loss. You see that we obtain a class loss of five point four seven. Now we get to the last part of our loss which is that involving the coordinates which itself is broken up into two soft parts. This first part is just for the center and then this other is for the width and height. Now this part is more similar compared to this object as part of the loss simply because here we are going to focus only on cells where we have an object and only on those bounding boxes which are responsible for the prediction. So again we are going to gather all our predictions here. We gather all our predictions where the target is equal to one. So simply where we have objects then now we combine the centers. So we see we go from one to three and then from six to eight reason being that this year because this is zero this is one this is two. So this year represent our centers and then when you have here this is actually six seven and this is representing the other centers. So this one is the center this two center. So that's why you see we take this and this and stack them up to form our center joint. And similar to what we had done already with the objectness loss we are going to only pick a given center based on whether it's that bounding box which is responsible or not for the prediction. And again we're going to use this mask. Remember we had seen this already previously with the objectness loss. So we had this already seen. So the exact same process we following year. We just want to make sure we pick in the bounding box which is responsible for the prediction. And since we've completed this mask already that's what we're going to do. Now let's let's print out our center joint and then let's print out the center print. So we see that we actually pick out only some bounding boxes from the two choices we have center right. You see here that for the first object which is this we have this option and we have this option. Now because for the first object is the first or the zeroth index that's responsible you see here. And then for the second object we have this option and this option. But because it's this one that's responsible or the second bounding box or the other first index that's responsible we actually pick this. So you see that this one is discarded and this one too is discarded. So we focus only on this. Now for the target we simply just pick out this one and two. So that's it. So we pick this obviously going from one to three is simply taking one and then taking two. So we pick this and then now we compare with whichever one of this is responsible for the prediction and comparing that is simply applying our difference method. So now that we're done with the center we finish with the center that's actually this part here we're now going to move to the width and the height. So here is exact same thing with just the difference that we pick in the width and the height. So now instead of one two we're going to pick three four. So that way you see we go from three to five and then here we pick eight ten. Well okay this is the prediction. So instead of picking this this we pick three four. So we have this let's pick this we have this three four and then eight ten. So that's what we do here. We stack them up we carry out the selection and then we also get the target. So we take this three four see that and then we compute a difference. Now remember that when computing this difference we have to make take the square root. So you see here we have square root and since the square root takes in only positive numbers we make sure to compute the square root of the absolute values. So that's it. Bread and size target and from here we've gotten the center loss. We've also gotten the size loss. This now forms our box loss and that's it for all those different loss functions. We're not simply going to add them up. Now before adding up from the paper we had seen that lambda coordinate is going to be five and lambda null object is going to be 0.5. We have seen this already from here. We have this lambda coordinate and this lambda null object. So that's it. We have this. We make sure when adding this up we take this into consideration. So let's run this and then well let's print out the loss. So let's print out the loss. There we go and that should be fine. We are then going to define our model checkpoint where our file path is this year. Then we're going to save only the weights. We're going to monitor the validation loss. We're going to obviously save the model which produces the minimum or the smallest validation loss and that's it. We save the best weights only. So we run that and then now we move to the scheduling. Here if the number of epochs is less than 40 so the first 40 epochs we use a learning rate of 1 times 10 to the negative 3. Between 40 and 80 we use a learning rate of 5 times 10 to the negative 4 and then after that we use a learning rate of 1 times 10 to the negative 4. So that's it. We compile our model and then we start with the training. Now after training for epochs you'll notice that the model starts to overfeed. And so in the next section we are going to use several techniques to help solve this or resolve the problem of overfeeding. Now we've been training for over 20 epochs and you could see clearly from the loss and the validation or the training loss and the validation loss that our model starts performing well and at some point starts overfeeding. As you could see here we have the training loss which keeps dropping right here and then the validation loss drops and then at some point starts increasing. So clearly our model is overfeeding. Hi there and welcome to this new and exciting session in which we shall be looking at different strategies to reduce overfeeding. And in the yellow V1 paper some strategies were underlined. To avoid overfeeding they use dropout and extensive data augmentation. Now a dropout layer with rate 0.5 after the first connected layer prevents coadaptation between layers. And so here you see that after this fully connected layer we're going to have the dropout and we're going to give it parameter 0.5. Then for the data augmentation the authors introduce random scaling and translations of up to 20% of the original image size. Then they also randomly adjust the exposure and saturation of the image by a factor of 1.5 in the HSV color space. So that said we are going to break up our data augmentation strategies into two main categories. The very first category will entail modifying the pixel values without modifying the positions of the different objects. So we could have something like this. Let's click on edit right here and then let's try to say brighten up the image. You see we could modify this like this. So we go from this initial image to this by playing around the brightness, playing around with colorization and so on and so forth. So this first category as we've said already entails just modifying the different pixel values without any changing position of any object we have here. And then for the second category we could go from this image to this one where you see that this flipping has made this object position to go from here to this position right here. And now in the first case where we just modify for example the image brightness there is little or no updates made to our existing code base. But when we have to modify the image such that the bounding boxes have to be changed or the positions of the bounding boxes have to be changed like this one here or this one which will go from here to here, this one here which goes from here to this other one. It means that we are now updating this bounding boxes and so we would have to write some extra code for all these different modifications. Now nonetheless it turns out that when we work with a library like albumentations, let's take this off, when we work with albumentations all changes made in the positions of the bounding boxes are carried out automatically. So you could see here we have this input image with this dog, this tennis ball, and this cat. And then after going through some transformation like here we see we have some transformation on this image where first of all the image is flipped so you see the dog moves to this other position and then the image also appears zoomed in. So you see that this cat for example is now not as complete or we don't have the complete cat as we had in this original image. And so now we have completely different bounding boxes. This one for example becomes this, tennis ball becomes this, this dog's bounding box becomes this. So you can see that it becomes larger compared to the input. And so with albumentations as we're saying you have this input bounding boxes like you could see for the dog at this position 23, 74, and 295, 388 is automatically converted to 149, 69, I could see here 295, 381. So here you just need to define your transformations and then albumentation make sure you have the right bounding boxes as output. So now diving into the code as you might have seen in some previous sessions we're going to import albumentations. So that's the import of albumentation. We now move on to integrate our transform. You see right here we have the different transforms. The first thing we'll do is to resize our images to 24 by 224 and then we'll apply a random crop. Now this random crop is applied such that the output image will have a height or rather a width line between 200 and 224 and a height line between 200 and 224. So we could just have this height or let's say height minus 20 or we could just say 0.9% of the height. So we have that we go from that to and then here we also have 90% of the width. So we have 0.9 of the width. Take that off and there we go. Here we have the width. Okay, so what we're seeing here is as we've had already, we want to randomly crop the image. So we have the image now our new image will have a width, which will fall in this range and the height which will fall in this range. And then we want to have a probability of 0.5 of applying this transformation. So if you want this transformation to always be applied, then you could always set always apply to true. If not, you just have P or the probability of applying to set to 0.5 or maybe even 0.2 or say 0.8. It just depends on you. So that's it. Now for the next we have this random scaling. Here we specify the scaling limit. We have the interpolation type and then again we have this probability. Then here we have the horizontal flip. So we're going to apply horizontal flip and probability set to 0.5. Now finally, because after doing this random crop, we'll have an image which is not 224 by 224. We actually resize this back to 224 by 224. So that's it. That's all you need. Those are all our transformations here, which we pass in this compose method. So it's basically a list made of this transform, this transform, this transform, this and this. Now one additional term we're going to pass in this compose method is this bounding box parameters. And the reason why we need to pass this is simply because unlike with the image classification with the object detection, we have bounding boxes which are going to be modified. So here we specify this bounding box parameters to take into consideration the kinds of boxes we're dealing with. And here you notice that we specify a format YOLO. Now getting back to the documentation, we actually have three formats. We have the Pascal VOC format, our implementation format, COCO format, YOLO format, Pascal VOC, our implementations. Now it turns out that in our specific case, we're actually dealing with the YOLO format. Not just because we're building a YOLO model, but because the way we've normalized our inputs or process our inputs is such that we have our X center, Y center, width and height representing bounding boxes. So if we had instead X mean, Y mean, width, height would have picked this. So it's not, it's not, it's not because of the name, although it actually coincides with the fact that we're building a YOLO model. But then as we said, we have an X center, Y center, width, height. And again, this is normalized. Notice here it's normalized. So this here is divided by the width, this divided by the height, this is divided by the width, and then this divided by the height. Remember, this is the width and the height of the specific bounding box, which happens to be exactly what we have seen when we're doing this here. Remember we took the X mean, Y mean, we obtained the X center and then we divided by the width to the Y mean, Y max, obtain the Y center divided by the height to the width divided by the total width to the height divided by the total height. So is the YOLO format we actually using right here. Now again, gets into the code, you see, we have the format YOLO specified. Now here we have this mean area set to 25 and this mean visibility set to 0.1. Now to understand the concept of the mean area, consider you have this input right here. And then after carrying out the transforms, what you have is say this in this output here. So we have this output where the area of the, this box is 4,344 pixels. This is actually the COCO format. So you will find that the bounding boxes will be different from the kind of bounding boxes we have. Nonetheless, the area as we said, after transformation is 4,344 pixels. So clearly it's quite small compared to the 23,892 pixels we had already from this 132 times 181 computation we had here. So after transforming this, we obtain this right here. But if we set the mean area to 4,500, it means that any box less than 4,500 is going to be omitted. And so that's why you see that when we specify this mean area to be 4,500, this box here disappears. So if you don't set anything, the box remains. But if you set this, then the box is going to disappear. And then we also have the mean visibility. So let's take this here. If we set the mean visibility to say 0.3, then if the output box, which is this box divided by the initial box, which is this is less than or gives us a ratio less than 0.3, it means that box is going to be omitted. And so right here, if you take this, this area, which is 6,888 divided by 24,108 from this original box here, 24,108, you would have 0.286, which is less than 0.3. And so when you see, you see when you say 0.3, this box disappears. So that's it. That's the idea behind the mean area and the mean visibility. Now we can actually leave those out. So let's just take this off. And that should be fine. So this is it. We have our transforms. We go around this. There we go. We have our org album and method, which takes in the image takes in the bounding boxes. And then all it does is it passes the image and the bounding boxes into our transforms here. And then we obtain the transformed image and the transformed bounding boxes. So as we are seeing in the schematic here, we go from this image bounding box pair to this transformed image, transformed bounding box pair. So let's get back to the code. We could run this. And then we have our process data method, which makes use of this TensorFlow NumPy function, because actually here, those are known TensorFlow operators, which we are calling, especially here when you use make use of implementations, it's made of computations in NumPy. So we make use of this method right here in order to integrate that in our data pipeline. So yeah, we specify the function or album and the inputs image and bounding boxes, the output tensors. Here, this one's this two year floats, actually float 32. So that's it. So we run this and we create our train data set. So we could visualize that. You see, for example, here, we have this image, let's write that image out so we could see it, we had output one and output two, let's check that out. See here, we have this, oh, no modification was made. So in order to be sure that we make OC some change, what we could do is we could make sure for example, let's let's comment this random scaling. And then let's carry out let's make sure the flipping is always done. So we have always true set, always apply set to true. There we go. So we run that run that again. And we have output one and then output two, which hasn't flipped. Now, one thing you would notice is also the fact that this bounding boxes are changed. So let's copy this from here. And then let's pass it out just here. So we could see that. Okay, so this is what we have before. And this is what we have after. Now you see that this actually makes sense because when you have this original image, when you have this original image where you have something like this for the bounding box, oops, where's all of that, we have something like this. And then when you flip it, you see when you flip it, what actually changes here will only be this X center. So if your center was around this, let's say centers around this, then flipping the X center changes position slightly. And that's what we will notice here, you notice how this X center changes position just a little bit. But for the Y center, it doesn't really change the width and the height remains the same. Now let's play around with this year. So let's have this random crop now. Let's do this. Let's set this to always apply. Let's set always apply to true. So we'll have that. Oops, we would have always apply. We set it to true. Run this and check out our output. So yeah, we would have, okay, we should have, let's run this year to have out two. So we'll have out one and then we have out two. Okay. Now what we have is this and then this, you see appears somehow zoomed in. Anyways, this example now shows us that we have, let's take this off. We have this image which has this bottom box here, something like this and this, and maybe the center around here. And then now it's modified and you see the height gets modified and although not too much, actually the width doesn't change. Well change is just a little bit. Now let's change this example so that we could see this clearly. This example isn't very demonstrative of this transformation process. So we'll take maybe the second. What is it? Do a skip. So here we have skip. Yeah, then we break. Hopefully the second has maybe many more objects or it's a better example actually. Let's check this out. Okay. So after flipping, we expect to have something which is looking different from this. Um, okay. So that's it. Let's take this off for work with this example. Now let's run this again. We have this year around this. There we go. We have that. We have our output which we obtained from skipping and then we get back here and then we also skip. So we skip and there we go too. And then from here we break and run that and then see what we get. Now you could see from here we have our output one. Let's also check out our output two. There we go. We have our one and then our two exactly as we expect. So that now this, this example is much different from what we had before and should be a better way to demonstrate what goes on in our limitations. So right here we have this input, um, boxes. Let's copy that. We have the input boxes. We're going to paste it just here. Our input boxes. Let's take that off. There we go. We have this output boxes. Copy that and then paste right here. Now before we move on, notice the fact that those classes are the same. So class 18 and here's class 18. And if we get right to the top, you, well, we use classes. So let's, let's say classes. Um, let's get 18 and see what we get. It should be trained. Um, from here we have, well, yeah, we have classes, classes 18. There we go. We see we have trained. Okay. So that is it. Let's take this off. Now let's check this out. We have our art one. You can see from here, it makes sense that the center is about, um, 27% of the full width. So this distance is about 27% of all this distance. So that's it. Then this distance too, it's about 36% of all this distance. And then we have the width, the width, which is about 0.54 or 54% of the total image width. Let's change the color. This is about 54% of the total width. And then this is about 71% of the total height. Now after flipping, you see that this has to change. Let's drag this now, drag this this way. And we have someone like this. Let's take this off. You would see that this now the center of this distance here, this distance from year to year is about 73% of the full image. See that? Excenter changes. And then the photo for the height, it doesn't really change much or changes very little. See that almost the same. The width remains practically the same. And then the height remains also almost the same. But at least the idea here is to show that after going through this augmentations transforms, augmentations permits us to obtain this output bound in boxes, which match up with a transformed image. Now don't forget to make sure you change this back from always apply to probabilities of 0.5. So that's it. The next set of transformations we'll make will be with TensorFlow. And we'll make use of TensorFlow image. So here we have this random brightness, random contrast. We're going to leave out the random crop for obvious reasons. Remember, if you had to do this random crop, it means you would have to write the code which permits you also modify the bound in boxes because when you carry out a random crop, the bound in boxes actually change. So that's why we're making use of augmentation since it makes life, it makes life much more easy. And then we use this, we use this, we carry out, no, we're not carrying this out. We carry out, no, not this, we carry out random U random saturation. Okay, so this we're going to make use of, you can see that here in the code, we have brightness, saturation, contrast U. And then we finally carry out this clip in by value to make sure all the values lie between zero and 255. Now you could always feel free to comment on comment any one of this right here. So that said, we again going to carry out this pre processing. See, we have that. And then remember, this is for the training, and then this is for the validation. So that's it, we carry out this mapping, we batch, prefetch, and then you could check out your outputs right here. So let's say out one, out two, let's check out out three. Here we go, we have out three. Well, this should be let's go to skip, let's keep this skip to and break. Okay, so let's run that again and see what we get out one out two and out three out one out two and out three. Well, since this was already bashed, we will maintain the tick. So we'll take the first we'll maintain this and then you would pick out on one, run that and there we go. We instead have this so this should be two. Let's run that. We have out one out two and then out three out one out two and out three. Okay, so you see that this now appears much darker as compared to this one. So that's it. We have seen how to carry out this different transformations or augmentations. But before we go on with the training again, one slight modification will make is we'll replace this ResNet 50 with the EfficientNet B1. Then we'll go ahead and compile the model and restart the training. There we go, training has begun. And after training for several epochs, here's what we obtained. You can see here that the training loss and the validation loss both keep dropping. See that they all keep dropping up to where we have this 123 around this year. So up to around this, our loss keeps dropping and then somehow increases slightly and stabilizes around 128. So that's why at this point, we have to stop the training and get the weight which produced the lowest validation loss. And that's it for this section. In the next section, we are going to test out this model. Hi there and welcome to this new session in which we are going to dive into testing out our YOLO model which we trained in the previous section. And to carry out this testing, we are going to make use of the COCO dataset. And so now we'll pick around some images from this dataset and test them out with our model. Now the first thing we'll do is load the model. So there we go. We load the model. We're going to create this outputs directory and then we'll specify the path to our test images. Now let's dive into this test method. Here we're going to take the file path, test path. And then we have this image on which we are going to put the bound and boxes and the classes. Then given that we are not, or we did not use OpenCV to load the image previously, we are going to go with the exact same process we had already. That is we read the file, we decode and then we resize. So this is what we had. And once we have this image, we pass this into our model. Now the output of our model will be something like this. So we'll have this 7 by 7 by 30 tensor. Now remember that for a cell like this one where there is no object, we suppose that this person here is an object. A cell like this one where there's no object will have a zero for the other first position. And then for the next position, we will have this for bound and box positions or bound and box values. And then we'll have another zero and then we'll have four and then we'll have now the 20 values for the class. Now, given that there is no object here, all this wouldn't really matter. And so that's why we are only going to take the boxes where this two values, this value and this value is going to be greater than or equal to the threshold, which will define to be 0.25. So that said, a cell like this one, which is a center of our object, remember we have this object here and we have this center. So you would have this cell here in the center, meaning that if we take this, let's take its values. If we take this, you would have our let's say 0.75. And then you have four values for presenting this position or its bound and box. And then we'll have maybe another say 0.9. Then we have four values and then we have different values here for the class. And so the idea here is to get all the different positions where we have values greater than 0.25. Now, you should note that we could pick a threshold of say 0.5 or 0.7 or 0.2 as we have done. And it really depends on how this threshold affects the model performance. So we picked 0.25 because it performs better than picking 0.5. As with 0.5, many objects were missed out. So that is it. We move to the next. We simply just gather all these different outputs. That is we have the object positions from here. And then to obtain this different outputs here, we'll take the output itself, which is all this. And then based off the positions, we'll get this output. So if we do here, from here we print out the object positions and then below we print out the selected output. Here we have this exception. Okay, so let's run this. You find that for this image, for example, we have these positions. That is, let's get back to this. So it's telling us that this is under position 4, 3. We have an object. So we go 0, 1, 2, 3, 4, and then 0, 1, 2, 3. So an object is found here for this image we have. Now the reason why we have this duplicate is simply because it happens that for this first position, that's for this first score, there's an object. And for this other score, there's also an object. You could see that from here that we actually compared the 0 position and this feed position, which is this 0.75 and this 0.9 respectively. And now we could take a closer look at this selected output. You see that this is 0.96. So here is 0.96. Well it's for this position, so we could just take all this off. So if we take all this off, all this here off, this is dirty. You find that this, which is 4, 3, the object for this specific image is for 4 at this cell 4, 3, and outputs this year. And you see the first position 0.96. Then we have the 4 for the bounding box. And then the next 0.98. So it shows clearly that the model is sure that there's an object there. And then from there we have this 4 again. And then now we followed with this 20 different classes. Now just by looking at this, just by looking at this here, 0.3, 0.3, try to look for the one with the highest value. Okay. It shows clearly that this is the class with the highest value. And so from this we know that there's an object at this position and that object belongs to this class. And obviously we have the bounding box surrounding the object. So now that we've had this different values right here, the next thing to do would be to convert this bounding boxes as this into the X mean, Y mean, X max, Y max format, which we are then going to use OpenCV to draw this bounding boxes on the image. Now we are going to go through each and every object position, which we've had already from here. You can see here, we have this object positions, 043, 043. Well, it's a duplicate. So let's focus on just a single or this single one. So we have 043. That's essentially the position for three as we've seen already. And to obtain the output box, which is this value, this value and this value, what we'll do is we have the output and then we'll say output position zero position is from the object positions. Remember the object position in this case is 043. So when you say position zero, you're taking zero. So here you have zero, position one is four, position two is three. And that's how you select this specific output here. Now, once you select the specific output, the next selection you want to make is that of the bounding boxes. Now when J is equal to zero, here you have zero times five is zero. So you go from one right up to zero plus five, that's five. So we go from one up to five, obviously one up to five minus one. So we have one. So we see this position one year, position two, position three, and then position four. So that's how we select this year from our output. And then notice that given that we have two different bounding box predictions, you have this year. So for the first one, you have one to five. And then for the next time we get into this loop, we have, since this is one, we'll have one times five, which is five, five plus one is six. So we go from six to 10, which now is this year, this is six, seven, eight, nine, well, six, seven, eight, nine, okay, yeah, six, seven, eight, nine, go again from six to 10 minus one. So that's it. So this is how we obtain the output boxes. That's how we obtain this year, this bounding boxes. And then given that, as we said already, we need to convert this into this X min Y min X max Y max format. The first thing we'll do is convert it into the X center Y center format or X center Y center with height format, which is what we do here. Now to obtain the X center from this year, from this 0.53, for example, and so let's suppose for example, that this 0.53 is at the position 043 as this, well it's 43. So we go 0, 1, 2, 3, 4, 0, 1, 2, 3. So we have this year around the center or 0.53, 0.17, so it's around here, we have this. And the idea is to obtain its value with respect to this full image height and image width. So first things first, we know that the distance from year to this position year is simply four divided by seven times 224 times 224. And that's simply because all this is 1, 2, 3, 4, 5, 6, 7. So because the full image width is 224, it means getting right up to this position year is 4 and 7 times 224, which is in fact 4 times 32 because 224 by 7 is 32. So you take this year and multiply by 32 and you get this distance from year right up to this year. Now to account for the fact that we have this 0.53 year, 0.53, we'll take 0.53 times 32 because this full cell is 32. So 0.53 times 32 plus 4 times 32. So to obtain this distance, we have 4 times 32 plus 0.53 times 32. Now for the height, because for the Y center, because this is X center, we'll still go 0, 1, this is 1, 2, 3, so we still have this year. Again divided by 7 times 224, this is going to be 3 times 32 plus 0.17 times 32. So that is it. To obtain the Y center, we have this position, that's 3 times 32 plus 0.17 times 32 to find this distance year. So that's exactly what we do right here. You will notice we have this post one. This post one is actually from year. This is post. So post one is 4, which is multiplied by 32 because this is this post one plus this output zero times all of this times 32. So you have this times 32 plus this output box zero. Output box zero is 0.53, 0.53 times 32. So all this is just like saying we want to have 4 plus 0.53 and then all of this times 32. So that's what we do here. For the Y center, it's the same. We have post two. Now this is year three times 32 plus output box one. Output box one is this 0.17 times 32. Remember, we got output box from year and it coincides with 0.17. So that's it. We obtain X center and we obtain Y center. And the next thing we want to do is obtain the width and the height. For the width and the height, it's going to be easier because when encoding this, we simply divided by the complete width and the complete height. So now we'll simply multiply by the height and then multiply by the width to obtain the width and the height. So that's how we obtain this. From here, we could now leave from X center, Y center, X width, Y width to X min, Y min, X max, Y max. Now if we have a bounding box like this, let's say we have the center and we know the width and the height. To obtain the X min, we could simply take this center minus half of the width because this distance, let's say this is the origin, this distance here, here is X center. If we subtract half of this width, then we would have this distance which will take us to the X min. And then we'll do the same for the Y. That is we take this distance right up to the center and then we subtract the Y, the Y height, that's the height. We subtract half of the height, not the height, but half of the height. Then we'll get to this position. That's why I mean, that's what we do here. We have X center minus half of the width. And then we have Y center minus half of the height. And then for the max, we have X center plus half of the width. So if we want to get this position here, we'll take this plus half of this width, which would take us to this point here. And then if we want to have this for the Y, then we'll take this distance, this distance plus half of the height, which will add up to this point right here. So we have X min, Y min, X max, Y max. Now be careful. In case where the X min happens to be less than zero, we want to fix this to zero. If the Y min is less than zero, we fix that to zero. So we don't have negative values. If this is greater than the width, we fix that to the width. If the Y max is greater than the height, we fix that to the height. And so once we have this now, we obtain our final boxes. That's X min, Y min, X max, Y max. And then not to forget the fact that we have some classes. So we are going to simply get the class with the highest probability score. So we just do this arc max and we make use of the selected output. Remember the selected output is what we had already seen here. So based on this, we are going to take the last 20 values. This year we get the arc max, which happens to be this year. And then we get a class which corresponds to this position. So that is essentially what we do right here. We have that position and then we have its corresponding class. And we make sure that this is string. And then we add that to our final box. So that's it for our final box. We also need our final scores. We will understand why we need this final scores later. For now, just take all the final scores. We make sure that we have again, you see, if J is equal to zero, then here we have zero. So we have the selected output, I, and we pick zero, meaning that we pick in this. And then if J is equal, if J is equal to one, then here we would have one times five, that's five. So that's a fit position, which is going to be this probability score. So essentially we're getting the probability scores for the two, um, predictions. Remember we had actually, we actually have two predictions. So we get the, uh, probability scores and then we, um, in them out. So we see what this looked like. There we go. As you could see, we have 0.965, which makes sense. This is 0.98. Uh, yeah, we have 0.965 and we have 0.985. Well, this is because there are some duplicates here. So that's it. Then we also see the final boxes. You see the class person, see person, person, person. Now, uh, we're going to see how to eliminate this duplicate shortly. And, uh, that's it. So for now we have understood how we could get from this models outputs to then be able to obtain this final boxes and the final scores. And now the next step will be to get into this norm max suppression. So we have, uh, maybe seen already the, we looked at the norm max suppression already in theory. Now we'll see that with tensor flows actually very easy to implement this, but before implementing, let's, um, take a look at what it's all about. Let's suppose we have an image like this and then we have, um, this object. Let's say we have this object here and then we have some bound in box. We have this bound in box. Remember for each, um, cell, we have two predictions. So let's suppose that our cell predicts this and that same cell predicts again, another bound in box like this, all this for this same object. Now what we'll do is with a norm max suppression algorithm, we are going to compare these two probabilities and say, okay, which one has the highest probability? If it turns out that is this one with the highest probability. So let's say if this is 0.98, oops, is let's say if this is 0.98 and then this one year is 0.96 and then these two are predicting the same object, then we are going to discard this box. So hence the term known max suppression. So we'll suppress this box and we'll be left only with this. So that's how we are going to also, um, discard those duplications. Now, um, going back to the implementation, all we need here is just this, um, nonmax suppression method we have here. So we have this no max suppression from tensorflow image. We specify the boxes. So we have this boxes here. Now note that our boxes from year included the classes, but we do not need that year. So we just, as you see, we pick the first four elements as essentially X min, Y min, X max, Y max. We pick this first four boxes. Then we also make sure we send in the scores. Remember in the normal suppression algorithm, we need the scores to be able to discard certain boxes which have, uh, which are not the max, uh, scores, which we do not have the max scores and which are predicting, uh, an object, which has already been predicted by another box of higher score. So that's why we need to pass in the score year. So, um, essentially we pass in the boxes passing the scores. We want to specify the total, the maximum, um, output size. Yeah. We just pick a hundred. We, we don't expect to have more than a hundred, but depending on your, on your task, like you could have a task where you generally have maybe say 150 objects to be detected at once. In that case, then you will need to increase this max output size to maybe say a thousand. Now we have this IOU threshold right here to understand this concept of the IOU threshold. Let's take back our example we had here. If we have this year, if you have this example, in order for those algorithm to know that these two boxes are trying to predict the same object and we want to actually discard this one, what we'll make use of is this IOU, um, threshold. So remember we have seen the IOU already. So if you have two boxes like this, these two boxes will compute the IOU score. That's essentially we'll look for the intersection between these two boxes, which is this area, and then divided by this total area occupied by these two boxes. So in this case, it's all this area right here. So let's, let's, let's have it back. We have this year is the intersection and then this year totally is the union. So we take that intersection divided by the union to obtain the IOU score. Now, if that IOU score is greater than the IOU threshold, like in this case, let's make, let's specify an IOU threshold of 0.5. It is greater than 0.5. Then we are going to discard this box. So we're going to discard this if that IOU score happens to be greater than 0.5. Now, if it is less than 0.5, many of that, if we have a box like this, um, let's say we have a box like this where this area, this area here is divided by all this area is less than 0.5. Then we are not going to discard this box. So we consider that this box is trying to is, is for a different object and not this other object. So this IOU threshold year permits also determine whether two boxes are trying to predict, um, the same object or not essentially. So that's it. And then here we have this call threshold, which is set to negative infinity. Now the documentation is said that the score threshold actually is already float tensor, representing the threshold for deciding when to remove boxes based on score. So if you have a score threshold of 0.4, for example, one, then what you're seeing is all boxes, which are, uh, which have a score of less than 0.1 are going to be discarded straightaway, mindless of, or regardless of whether, um, they overlap with a box of higher score or not. So that's it. We get back here and then now we could print out, let's print out, um, our norm max suppression output output. Let's run that. Now, one thing you'll notice here in this output is the fact that we have a single element. Now what the single element actually means is between all this four options we have, that is here, we have this person, we have this person, we have this person and this person only this one year at this position one is going to be left. All the rest will be discarded. And to understand why they're discarded, you could look at this course. This is 0.96. This is 0.85. This is 0.96. This is 0.98. Oh, this is 0.985, not 85. So what we're saying here is because this one has the highest probability and because it overlaps with the others, like you see here, uh, this one and this one will overlap because it's actually represent the same person. Then, um, this others will be discarded. Now in the case like this, where we have this exact same box with exact same probability, one is going to be left out and the other one left. So, um, that's it. We have our output now. We know that we only have a single box instead of all this four boxes. So we have our normal expression output. The next step will be to, um, show visually what our, um, predictions look like. Now, yeah, you know, the fact that we are going to write this in this image. So we will draw the bounding boxes and put the text on our image only for I in, uh, no max operation output. So in this example, we're going to do that only once, unlike the case where if we do not have no max operation, I would have had to do that four times. So now we're doing this only once because after no max operation, we're left with only a single box. Now take a look at what we have here. We have the X mean, we have the X max. Remember from the final boxes here, what we had here, X mean, um, Y mean, like you see, we have the X mean, we have, um, the Y mean, sorry, not the X max. We have the X max, we have the Y max, and then we have the color for the box and that's it. So that's it. We now put the text. Now we're putting this text based on, um, certain position. So, um, the text itself is going to contain the class from the final box. You see, we take this last element. Remember from here, we had the class and then, uh, once we obtain that class, we write that as the text we're going to put to the text and then the position of that text will be based on the X mean Y mean values. So you see, we go, we go to X mean, but for Y mean we step 15 pixels, um, downwards. So that's it. We define the font and the color and that's it. So we've put out this text and then now we're ready to write this out in our, um, new image. So we create this new image and then we resize it. Obviously the content of this image is this year, this image, which on which we have written the, or we on which we've drawn the rectangle or that's the bounding boxes and the texts. So let's run this now completely. And then you see again, we have one open this up. Okay. So we should be able to have our output and there we go. You see, we have a person notice from your, um, from your, we had decided to go 15 steps. Oops. Let's take this off. We had decided to go 15 steps. Let's go downward here. We decided to go 15 step. That way you see the text comes slightly down. So if you, if you, if you don't have this and then you do this, see it goes up and it's not very visible. So let's have that back. You could obviously change the color. So let's say the 225 and you could play around with all those different parameters. So that's it. Well, let's get back to the color because it's actually better. Let's say we want to have one. So now we could run this for all the different files and see what we get. Well, before you've been checking on that, let's suppose that we do not have this normal expression output. So let's, um, leave out the normal expression algorithm and see what our outputs will look like. Let's have your, uh, for I in range, the length of the final boxes, the final boxes here had a length of four. We had four outputs. Um, let's take this off. There we go. And then run this again and see what we have. So we have outputs. Oops. Well, we already had several different predictions. Um, like, okay, let's, let's take this one. For example, you see here, we have this one. Uh, no, let's take, let's, let's not have that. Let's say we want to have your 40, let's get back to 40, um, 40. Okay. So one thing you can notice here is the fact that this, you see, we have this two predictions for this person and, uh, that's not what we want. So you see that the fact that we add this in a, uh, not my suppression year permits us to remove some of the boxes. So this threshold is what you play around with to ensure that you have a single box for a single object. So yeah, you can see that this model does well and predicts in the location of this train and knowing that it is actually a train. We have this year predicts that this is a person. We have this produce this person, but unfortunately doesn't get this other people year. Um, guess they're playing this person, this person well doesn't get this person, um, does quite well here. See, this is the dining table. Um, this person, this person and this person. So that was great. Um, yeah, we have the TV monitor, um, though it doesn't get the other monitors. Okay. So from here, we also have this bus. Unfortunately, yeah, we, we, it predicts two buses, um, a car also predicts two cars, first, maybe due to this auto car being here. Then we have this dining table and this person. We have this person and a dog. We have this person, this person, well, stills a dog here, but that's not right. Um, person, person, but doesn't see this other person, um, sees this two people here. Um, here says this person's though the bottom box isn't, um, quite well put out. Then we have the cut. We have this person. Um, we have this car and then we have this person doesn't see the dog. We have here sees this cows. Well, this, this particular image was gotten from the paper. So just basically crop this from the paper to test it out. So it sees a person, but doesn't see that this to a dog store actually locates them quite well. So that's it. Um, yeah, it sees a cat, but doesn't sit as dog. Um, she's a person. She's a cow. She's the person person. She's two people sees this person. And this person though the bottom boxes aren't quite well put out. Unfortunately I see cars, um, not quite correct. Then here we have motorbike and person, but this should be two motorbikes actually. Okay. So that's it. We've just tested out our model. We'll see how, um, it does or how it works with our images on which it has never actually seen. Hello everyone and welcome to this new and exciting session in which we shall dive into image generation. So as you could see right here, this image was AI generated, but back in 2014, 2015, 2016, images like this weren't yet to be generated using AI with advances in AI, like the original auto encoders, the GANS and even more sophisticated GANS like the W GANS, pro GANS, SR GANS, and cycle GANS. AI algorithms have been trained to produce high quality images. And today we even get much better results with a diffusion models. That said, in this section, we shall treat the variational auto encoders and the DC GANS. And so at the end of the section, you should be able to produce images like this. Back in 2014, one of the best performing image generation models was this model you have right in front of you. That is the variational auto encoder. And the way this model worked was quite simple. We had an encoder block, which took in some input image and then generated this embeddings. And now this embeddings having encoded information about the inputs could be used by the decoder to generate output images. Nonetheless, by 2014, Ian Goodfellow came up with this idea of the GANS. And the GANS signifies generative adversarial neural networks. So here we have two neural networks here, the generator G and the discriminator, where this G and this D that's the discriminator are both in some context where the generator is learning how to produce images which look like those from the real data set or the training set. And on the other hand, the discriminator is learning how to differentiate between real data like this one and fake data produced by the generator. If we consider this simple example here, you can see that we pass in some input noise, we get this output. And because this output doesn't look like the real data, the discriminator considers this as fake. Whereas now for this other example, the output from the generator looks like the real data. And so the discriminator sees this or the discriminator is tricked by the generator to think that this is real data. So after updating the parameters of the generator and discriminator, such that we get to that point where the discriminator no longer knows the difference between what is coming from the generator and what's coming from the training set, we now have this generator block, which is able to take in random noise and generate outputs, which are similar to those from our training set. And although this architecture was groundbreaking in 2014, 2015, today we have more advanced better models like the StyleGAN. Hi there and welcome to this new session in which we shall be trading the variational encoder. And we shall see how it could be used in image generation. In this first part, we shall dive deep into understanding the theory behind the variational encoder. And then the subsequent sections, we shall practically implement a working variational autoencoder. That said, we shall start with explaining or understanding this autoencoder and we'll make use of this blog post by Jeremy Jordan. Now to understand the autoencoder, we can break this word into two parts. That's auto and we have encode. So essentially we have a system which self encodes itself. Now, supposing you have an image like this one right here. When we pass this into some encoder block, let's have something like this. We have some encoder block and then we could obtain this output vector. Now this output vector is six dimensional. So yes, six different positions and each of the positions represent a specific characteristic of this image. You could see your smile 0.99, skin tone 0.85, gender negative 0.73, beard 0.85, glasses 0.002, hair color 0.68. So all these six values here, the six values we have here are characterizing our image. So they encode information or information about this image is encoded in this vector, or this vector right here. And then on the other hand, when we want to retrieve this encoded information, what we could now do is we get a decoder which takes this encoded information and then reproduces this original image. And so that's globally how we produce this kind of system which could be used in image compression where we could take this image, encode it so that we have just this vector. Then we could pass this vector via some network. And then on the other side of the network, we decode this vector such that we have the original image. Apart from compression, another field where we could apply this kind of auto encoder network is in image search. So let's suppose that we have this image right here and we have this vector. Now if we have another image of this same person here, so we have another image of the same person. We call this image B and here we have image A which produces a vector which we'll call VA. Then it means that in this 6D vector space, six because we have six different values here, it could be 128D or whatever dimension we make it to be. So as we're saying, we have this image B which is the same image here, the image of this person but not necessarily the same image, maybe some other image of the same person. Then after encoding, after passing through an encoder, you have our 6D vector VB. But because it's a similar person or because it's the same person, we would expect these values to be similar. And so VA will be close to VB. And if we have another person, let's say another person C, this is here and we generate that person's VC, that's this encoded vector, then we would expect VA to be much different from VC. And so this means that in an image search scenario, we'll just pass this input, we obtain this vector and then we'll compare the two vectors to see whether it belongs to the same person or not. Now it should also be noted that when training an autoencoder model where we have an image A and we have a reconstructed image A', then our aim here would be to minimize the difference between A and A'. So we could minimize AA'. Now it turns out that in image generation to get better results, instead of dealing with discrete values like what we had here, let's get back to the top. You see here we had the given value, let's take this off, a given value for smile, for skin tone and so on and so forth. So as we're saying, instead of having a fixed value for each and every one of these features, what we'll do is we'll make use of a probability distribution. So here, instead of having a value, let's say this is negative 0.6, we would have a probability distribution whose mean is at negative 0.6. Well, this looks more like negative 0.5. But here we suppose now we're going from 0.6 to this probability distribution which means a negative 0.6 with a given variance. Now for those of you who don't have a map background, what this essential means is instead of picking a value or picking the value negative 0.6, what we'll do is we'll pick some random value within this range. So instead of having negative 0.6, as we said, we're going to have a random value in this range and values closest to negative 0.6 have a higher probability of being picked. So instead of having this, we could now have negative 0.3 or negative 0.55 or negative 0.75 and so on and so forth. So we have values which we can pick in this range. Now if we see this example here where we have 0 now turned to this probability distribution, we pick values in this range, negative 1 to 1. So here the variance or the range of values which are in which we are allowed to pick a value is larger than this other one. But still values around this zero, that's the mean, around this zero have a higher probability of being picked. So here you would have a higher chance of picking 0.1 instead of picking 0.9. To see that clearly here, let's say this is 0.1 at this map and then this is 0.9 around here. You'll find that if you link this up here, this is 0.1, you see that this has a higher score and a higher chance of being picked as compared to this one which has much lower chance of being picked. So in a nutshell, instead of having this 0.5, we now have a mean value which is 0.5 and a variance which shows us or better still gives us the range of values for which we can pick the specific value for a given feature. And so as you could see, this one here has a smaller variance as compared to this and as compared to this one. And from this point, we'll define this mean as mu and the variance which is some distance from here, this distance as sigma square. This probabilistic approach to generating a latent vector which previously was this vector we had here. Scroll back up. Previously, it was this vector. It's now what leads us to the variational autoencoder. So you see that here we have our input image. It gets into the encoder which produces mu and sigma square or let's just say sigma. Sigma is a standard deviation, sigma square is a variance. So it produces mu and sigma. And then using mu and sigma with our decoder, we are able to obtain our reconstructed output image. Now note that in this case, we would have 1, 2, 3, 4, 5, 6 positions. So mu would be this 6D vector, sigma would be another 6D vector where mu1, this first position here, mu1 and sigma1 represent the mean and the standard deviation for this distribution. Now it should be noted that the main benefit of a variational autoencoder is that they are capable of learning smooth latent state representations of the input data. Now to better understand that statement, let's consider this output generated by an autoencoder and this other output generated by a variational autoencoder. You will notice that as we go from one digit to another, like let's say we're going from six to eight year, you see here we have six. Well, it looks very well like six. Let's take this one which looks already very well like six. This is six but here it's really confusing because we don't know exactly what this is. Now this looks more like eight but not really very clear and then here we start getting eight and eight and well this too doesn't look very clear. But when you look at the output generated by the variational encoder or the variational autoencoder, as we go from one digit to another, we can see here that we have an even much smoother transition. And this is thanks to the fact that instead of working with discrete values at the level of our latent vectors, we're going for a probabilistic approach with the variational autoencoder. Because we're going in for this probabilistic approach, the training of our variational autoencoder is no longer evident. And this is simply because during the training, we need to compute partial derivatives with respect to z here with respect to mu and partial derivative with respect to mu and with respect to sigma. But because the z we have here is drawn from a normal distribution, with mean mu and standard deviation sigma, we won't be able to compute this partial derivative. And so the idea now will be to convert this node that's here to one that is deterministic. You could see here we have this key random node and then deterministic nodes. So this one is deterministic, that's fine, this one fine. Now, well, this is fine. Now the idea will be to convert this into one which is deterministic such that we could compute this partial derivatives and hence train the model such that we could update the encoder and the decoder parameters. And so now this idea of converting this node from one which is random to one which is deterministic is known as the reparameterization trick. So instead of having this where we have, well, let's take this off, let's make it simple. So we have this where we have the mean mu and the standard deviation. Well, we'll pick any value at random in this range. We are instead going to define this epsilon which is drawn from a normal distribution with mean zero. So our mean now will always be zero and then the standard deviation will be one. So we have negative one, one. So this epsilon here as we've said is drawn from this probability distribution and then to obtain z, unlike here where we obtain z randomly from values surrounding the mean, here we'll do the mean plus the standard deviation times a random value which lies between or which surrounds zero. And so now we could compute this partial derivative here, this respect to z, and hence train our variational autoencoder model. The next and final point we'll look at in the section is the variational autoencoder slots. Now from the autoencoder or the variational autoencoder paper, the authors break up this loss into two main parts. The first part, let's take this off, the first part is the reconstruction loss and this other part acts as a regularizer. For the reconstruction loss, we try to minimize the difference between x and x prime or xshapo. So we want that the input and the reconstructed input or the reconstructed output should be similar. Here is denoted as this, we're trying to minimize this and then for the reconstruction loss, we're computing the KL divergence between this distribution and this other distribution. Now to understand what these distributions actually signify, we can take a look at this figure. So here we have this KL or this distribution KL of z given x which happens to be a learned distribution meaning that when we'll be training this encoder model right here, this is our encoder model, we'll be training this encoder model, we shall in fact be getting this distribution which as we said already is a learned distribution. Nonetheless, we do not want this learned distribution to be very much different from the distribution P of z and so that's why we are going to minimize the distance between this distribution and this all this learned distribution and the true prior distribution P of z. Now it should be noted that this KL divergence here is a tool which permits us measure the distance between two distributions and so if we could minimize this, if we could minimize this, then we'll reduce the distance between this distribution and this distribution P of z and getting back to the original paper, it should be noted that the reconstruction loss can be taken as the mean square error while this here will be our regularizer and so this is what we obtain after computing the KL divergence between those two distributions. Hello everyone and welcome to this new section in which we are going to be building our own variational autoencoder models from scratch. Previously, we saw how variational autoencoders could be used in helping to generate new images where we build out this encoder decoder structure such that we could produce outputs which are similar to the inputs while being entirely new images. In this session, what we'll be doing will be to build our own variational autoencoders and generate our own images. The data we'll be using and training our variational autoencoder will be the MNIST dataset which you could get from TensorFlow datasets. So right here we load this dataset and then we concatenate both the training and the test datasets. So generally, we usually have a dataset made of say xtrain and ytrain, xtest, ytest but since here we are not going to be making use of those outputs as the ytrain and ytest we just get this tool and then we concatenate both since we would not be having a test set. So basically, we have that and then one other modification we make is we or one other reprocess instead we take is we divide these values by 255 so we normalize our dataset. So let's run this and then once our dataset has been downloaded what we do now is we convert this dataset into the TensorFlow data format. So we have our dataset which is tf.dataset or rather tf.dataset from TensorSlices. So you see that we take that and then we pass in our MNIST digits which we've already downloaded. So we run that and that should be fine. Now we could check out the length of this dataset and you see you should have 70,000. So we have 70,000 different data points which make up our dataset. From here we're going to define the batch size. So we would have a batch size of 128. That's it and then we'll go to the usual steps of shuffling our dataset. We have our dataset, we shuffle, we batch and then we prefetch. Now if you're new to this you could check out the previous sections in this course. Anyways we have this three as we've said and then now we could run this. So that's it. You could see train dataset. There we go. You see that we have this train dataset here and we could see its shape. So it's all 28 by 28 by 1 images we have in our dataset and there are 70,000 of them. So that's fine. Now getting to the modeling we're going to start with the encoder. You can recall that what we had seen so far was this model or this encoder model which takes in an input image right here and then outputs the mean and the variance. So we have the mean and the variance and then this two have been combined via the reparameterization technique where we have mu plus sigma times a random value drawn from a normal distribution and then this z is passed into a decoder here. So it passes into a decoder and then we get an output image such that the difference between this two is minimized. Now that said let's get back to the code and we design our encoder. So our encoder here is going to be a very simple conf net. We'll start by defining the latent dimension. There we go. Let's just put this right here. So latent dimension will be two. We have that and then we get back here. For encoder we're going to start as we said with this encoder input. So we have an encoder input and then this has a shape which we're going to give to be 28 by 28 by 1 just as we just the same as that of our images now data set. So that's the input. We've seen this already and then from here we'll define a conf 2D a conf 2D which has 32 filters 3 by 3 activation relu. So supposing that you already have some background knowledge and conf nets activation relu number of strides equal to the pattern same. So we're going to build this very basic conf net. Now this takes in the encoder inputs. Remember or recall we're using the Keras functional API right here. So we have encoder inputs. We then create another conf layer. We just basically copy this and paste it out and then what we would have here is an increased number of channels. So we have 64 here and from here we'll go ahead and flatten our outputs. Now note that here we have X so we should change this to X from here we move on to flatten. So we have flattened and this takes in X. So now the output is flattened. We are now going to output both the mean and the standard deviation which we are going to use in sampling. But before that we'll pass this into another dense layer. So here we have this dense layer let's say 16 outputs activation, activation relu and then we take in X. Okay so we have that and now we're ready to let's just copy from here. We're ready to get the mean and the standard deviation. Copy that paste it out here and this. Okay so here we have the mean and we'll have the standard deviation. So we have dense activation relu standard deviation and then here since remember we have this output to be the latent dimension. So here we have instead of 16 we now have latent dimension which we've already fixed right here to be 2. Now one very important reason why we we have those activations to be relu here would be simply the fact that the the mean and the standard deviation are all positive numbers. So because we may output or we may get negative numbers here we want to always make sure that the values we get are positive. Now the problem with the standard deviation particularly is the fact that it's usually a very small number between 0 and 1 where the number is very far away from 1 meaning that the number is very instead closer to 0. So we have a number very close to 0 like this but then the problem with working with the relu is that having to find derivatives around this 0 here will lead to numerical instability during training and so what we want to do instead is to map this range of values or this possible range of values that the standard deviation can take to a larger range. Now to carry out this mapping we have to use a function which is both continuous in this range and monotonous that is either increasing or decreasing. Now one great function for this task will be the log function as this log function will map values of x in the range 0 to 1 two values in the range let's open it up here in the range negative infinity log or the limit as we go to a 0 the log is negative infinity if you plot out a log you would have something like this let's have something like this so you find that as you go towards 0 log goes towards negative infinity so that's why we have negative infinity here and the log of 1 is 0 so we go from this range to this larger range hence we can have a much more stable training process and so what we'll do now is instead of relying on this ReLU activation to ensure that our standard deviation is always positive what we'll do is we'll instead compute the log of the standard deviation square which happens to be the log of the variance so we're going to take this off for both the standard deviation and the mean stick that off and then right here we have log bar that's a log of the standard deviation square log bar and then we'll move on to the sampling process where we're going to obtain z remember z is equal to mu which is the mean plus the standard deviation times epsilon where epsilon here is a random number drawn from a normal distribution we are now going to create the sampling layer which takes in the mean and the log of the variance and then outputs the z so here we have z equals sampling there we go we now taking as inputs the mean and the log of the variance and that's it so from now we're going to create the sampling layer so let's go ahead and create the sampling layer so we have your sampling sampling there we go and this is a layer okay so we have that then we just we just have this call method which takes in our inputs let's let's just have our inputs and then from those inputs what we would have is the mean and the variance so basically we have we extract the mean of the variance from this input remember we have the mean and a log bar let's say mean and the log bar actually so we have that mean and log bar which we extract from the inputs and then if you remember what we have here so the way we obtain z is mu plus sigma times this random number right here now let's let's see how when given mu when we when we get mu and we get the log of the variance we are able to obtain this sigma because we already have mu but now we need to get sigma now note that sigma is a standard deviation and the standard deviation or let's say let's just write sigma and sigma itself is equal the square root of the variance you see that it's equal to square root of the variance so it means that and then we know that the variance written like this can be written as e that's exponential to the power of log of the variance so generally we know that x equal e to the log of x you see that so here we have e to the log of the variance and then we also have the square root so let's have this here now this is equal e to the log of the variance let's just leave that as v and all of this to the power of a half we also know that e to the power of e to the power of say x to the power of all of this to the power of a is equal e to the power of a x see that so here this is going to be equal e to the power of half times log v so it's going to be half log variance so now we now that we have the log of the variance to obtain the sigma this basically what we need to do so let's get back and then we have the mean plus sigma which is exponential and then we have half so 0.5 times the log of the variance and then we need to multiply this here the sigma by a random value so we have tf the random normal and then we specify a shape which is simply the batch size and the latent dimension now that we have this we can now go ahead and define our encoder model call this encoder model which is a tensorflow model and we have encoder inputs for the input there we go that's it here and then for the outputs we have this list made of z the mean and the log variance so we have mean and log var okay so let's give it a name we'll call this encoder now once we have this we could get a summary of this so we have encoder model summary which we could visualize right here see that from here now we move on to defining our decoder the decoder as we've seen already we'll take in the z here and then output the images so we'll go ahead here and create the inputs call this latent inputs and then there we have a tensorflow input and shape is gonna be the same as that of z so right here we have latent zim and that's fine okay so now we have this we're gonna take our z let's get back here so now we have to up sample this z here which has a shape to now an image of shape 28 by 28 by 1 but generally what we've been doing is we've been used to down sampling so in down sampling we have an input we have some conf net layers we stack them up and then when we get towards the end we could flatten and then we have some dense layers with a specific output which matches the type of output one again but in our case we're now doing some sort of the opposite of this so what we'll do is we'll go through some dense layer here we'll go through some dense layer and then from this dense layer we would pass this into a transpose convolution layer so we've been used to the convolution layer but here we use the conf to the transpose layer essentially what we have is this conf to the layer with its weights which up samples inputs now getting back to the code we have this input which has shape batch size by the latent dimension let's say two and then from here we want to make use of a conf to the transpose layer which takes inputs batch size by some x by some y by some z that's similar to the conf layer so what we will have to do is we're gonna uh reshape this actually so we have to reshape this such that we have something like this now if we want to have uh x y z such that x is say for example seven y is seven and z is say 64 let's take that example then we have to ensure that what we're getting after this has shape b by um this seven by seven by 64 seven times seven times 64 you see getting back here we have this let's let's maybe redraw this again so it's clear so what we're saying is we have this input and then what we intend to have is something like batch size by seven by seven by 64 now the reason why we pick in seven is because the output is 28 by 28 so one to be able to up sample sorry that we could say seven by seven up sampled to 14 by 14 and then the 14 by 14 of sample to 28 by 28 so that's why we pick in seven year now uh this uh year you see it doesn't match with this there's no way we could reshape two to become this so what we'll do is i will pass this through a dense layer which has outputs batch by seven times seven times 64 and now after reshaping we could obtain this so let's get into the code and what we'll have is uh dense layer dense oops we have uh dense layer um seven times seven times 64 and then we have the activation activation relu and this simply takes in the latent inputs so we have here latent inputs there we go now from from here now we we do the reshaping so we have x equal reshape and then we specify the shape so we have seven by seven by 64 so that's it now we have that we have x there we go we now start with our conf to the transpose so you see we'll reshape this into this now could make it so far come to the transpose come to the transpose which takes in uh number of filters is very similar to the conf layer so we have a number of filters let's say 64 filters and then the the the kennel size 3 the activation relu so we're going to use the relu activation number of strides um two the padding is going to be the same see so it's quite similar to the conf layer but the difference is that now we up sampling instead of down sampling so we have that then from here we are going to change this to 32 so for the encoder what we did was we increased this year this number of channels and then here we reduce the number of channels now in our final output layer we're going to have this decoder output which is going to have just one channel so here we have an output which is 28 by 28 by 1 where the values lie between zero and one so what we're going to do now is we're going to have channel numbers equal one um the activation instead of relu will be sigmoid so here we have sigmoid and then we're not going to use any strides since we're not up sampling so that's it now the reason why we're using sigmoid here is quite simple since one of our values fall between zero and one we want that each and every time we have an input we have something like this so no matter the input we have the sigmoid will always put the the value or make that input turn into a value which lies between zero and one and that's basically what we want to do in this last layer right here so that's why you see we're making use of the sigmoid now once that's done we create our decoder model which is tensorflow model and then we have latent inputs there we go we have our decoder output the name is decoder and then we could get a summary of this model so that's basically it let's run that and there we go see we have a decoder model now for the training we are going to make use of the atom optimizer with a learning rate of 0.001 and we're going to train for over 20 epochs now as we've seen already our loss we made of two parts that's the reconstruction and the regularization part now for the reconstruction part aim is to minimize the difference between the output image and the input image so we'll go ahead and start with the reconstruction loss we have our custom loss there we go it takes in y pred takes in y let's start with y true takes in y true y pred and then the reconstruction loss itself is defined such that we have recounts let's just say loss reconstruction is equal tf keras losses binary cross entropy loss okay so our outputs remember outputs range between zero and one so we could use the binary cross entropy loss here feel free to test all our different losses so we have that and then we pass in y true and y pred now once we make use of this we are going to now sum all the values because we have in let's say we have something like this let's suppose that it was our five by five outputs we are having so we would have something like this one two three four five that's fine five there we go you see we have this five by five here so with this binary cross entropy we'll be able to get the the difference for each and every position here now we need to sum all this here so what we would have is um let's take this off we'll take this off and we'll have the reduced sum so this reduced sum now we'll sum all this different um difference all these different loss values we get here so now we we we sum for each and every position obviously we have 28 so it's 28 by 28 positions and we also need to specify the axis here so the axis we're going to work with is one and two one and two now to understand why we have in this let's take a look at the shape of the output is b by 28 by 28 by one but where we're actually carrying out this where we're actually computing the loss is in this axis here so uh we specify one two because this is zero one two three so this is where we want to compute our loss so it's on this two axis now said we have that and then we now look for the mean so we could after summing up we could look for the mean let's average the values and that should be it so that's it for our loss reconstruction the next step let's take all this off the next step will be this loss regularization getting back here you see we have this sum see the sum of the the the the variance plus the mean square of the mean minus one minus the log of the variance so let's get back here and we have this negative half here if you take this negative and multiply by each and every one of this would have log var plus one minus the mean square minus the var before we continue remember again that we could get sigma j as e to the log of uh sigma j see that so oh so yeah yeah no let's let's let's let's write this better so we have sigma j equal e to the log of sigma j okay so that's it um let's get back here we have uh loss uh we'll still have this mean and sum so we'll sit uh average sum and then find the average uh let's have that and then we'll have negative 0.5 times the log var let's get here we'll need a log var oh let's say log mean and then let's let's get exactly what we had here remember in this um encoder model we outputted a mean and a log var so let's have mean there we go we have mean and log var okay so we have this set now we could make use of it right here we have as we've said already log var plus one so we just have plus one minus the mean square so tf mat mean uh rather the square of the mean and then minus tf mat e to the power of log var see that and now we return the loss reconstruction and the loss regularization so that's it so this is our custom loss we run this uh we get in this error let's add this and run that again okay so that's fine and we now set to start with the training but before going on we have to um also specify the axis for this sum so right here we have this axis which is equal one and we explain why we we specify this axis to be one now the shape of the log var and the mean is this matched by two and so the shape of all this sum will still be this and so if you're comparing this all right if you're computing this here you'll get this kind of output and so since for the loss we need a single value we need to sum all the um values we get in this axis so that's why you see we specify the axis to be one we have this uh input which we define then we have the encoder model which outputs z the mean and the variance which you're not going to make use of and then we have the decoder which takes in z and then produces this output so here we have this uh vae model which contains both the encoder and the decoder now we're going to go ahead and build our custom training block we suppose that you already have an idea of how this works so we have that training block which takes in our x batch and then we're going to make use of tensorflow's gradient tip so we will have this width tf gradient tape gradient tape there we go as a recorder we're going to pass this batch into the encoder so we have our encoder model which takes in x batch and then what it outputs is z the mean and the log variance here we have this okay so we have that set and then now once we get this we we we get the z from here and pass this into our decoder so we have our decoder model which takes in z and then what it outputs is our y predicted see that and now from here we could obtain our loss by simply calling on our custom loss method which we'll define it takes in the y true it takes in the y pred y pred it takes in um as we'll define here the mean and it takes in the log variance now the this y true y true happens to be the x batch remember we are having this here this encoder takes in that so we we have our input image here let's let's take this off android a bit here a bit clearer so here we have this and we have this so we have our encoder and we have our decoder we have our input image which is what we expect to have here so our y true is what we pass as input here which is this x batch so that's why you see we specify y trials x batch then y pred is what a decoder produces so we're going to compare the y pred and then the y true which is as we've said already the x batch that's fine we get back to the code we have that said and then now we have our partial derivatives our partial derivatives which we'll get by making use of the recorder so we have recorder gradient it takes in the loss it takes in the overall models trainable weights so we have trainable weights okay so talking about the overall model which we yet to define we have it right here it takes in this input 28 by 28 by 1 it takes the input pass into the encoder model gets the output from the encoder model and then into the decoder model and then from there we create our model which takes in v ae input and this output right here so from here let's get the summary and we see that we have exactly what we expect so we have this model which takes as we've said already this input the encoder outputs z the mean and log bar and then the decoder outputs this image right here so let's get back to our training as we're saying where we we have this partial derivatives from the loss and the trainable weights so that's it we then go ahead with the gradient descent step we apply the optimizer so we have optimizer that apply gradients there we go which takes in our partial derivatives and our trainable weights so there we go we have z partial derivatives which we just calculated right here derivatives and then the trainable weights trainable weights there we go okay so that's it we could from here just simply return the loss now we're gonna run this and let's we haven't run this yet okay so we have that already now we could define our neural learn method so we have your neural learn which will take in the number of epochs and then from here for the epoch in range epochs there we go we're going to start by printing out the training starts for epoch number whatever epoch we add so we format that to take in the epoch plus one since we're going to step from zero or we could just take this one from here and then there we go now we have to add plus one here okay so we have that and then now we're going to do for step x batch we're going to enumerate in enumerate um the train data set which we've defined already let's get back to the top and we see we have our train data set so we're going to go through this train data set let's get back here there we go so we go through our train data set we take a specific batch and then we compute the loss so we have that training block which we've defined here our training block has been defined here so not only we compute the loss but we also apply the gradient descent step for that specific batch so we're doing this for each and every batch of our training data set now what this takes in is our x batch simple as that now once we have this the next thing we'll do is we'll print out our loss so the training loss um is um there we go we have loss and then once the training is complete we could simply print out training complete okay so we have that set now let's run this and then we have neural learn let's train for ebooks um here we get in this error let's take that off that's fine let's run this again so training is now complete and here's what we get you see that our loss drops and then we can now get straight into testing out our VAE model now before testing let's recall that our VAE is comprised of two units that's our encoder and then the decoder right here now we've trained this VAE model end to end to make sure that the inputs look very similar to this outputs produced now if we want to generate new outputs what we'll do is we are going to cut off or we are going to take off this region here and focus only on the decoder now to generate an image at random or to generate a digit in our case at random we'll just have to pass in a z in here that's a random value of z remember z is mu plus sigma epsilon so we have to pass this value of z in here and then get a random output now remember z is twodimensional so z is this vector made of two values and so that said we will define the first values here which we will call greed x and we use the lean space method to get values from a scale let's add a cell above this so we would have a scale which will take a value of one and then we will have n equals to say 16 different values so here we'll go from negative one to one and then we'll have 16 values in between we'll pick this for grid y now so this is the first element and this is the next element so here we would have different elements so that we could generate different images there we go let's run this and then print out greed x and greed y now that we have this done the next thing we'll do is we'll plot out our different images which we shall generate using our decoder so here we have this figure we define the fixed size let's say five anyway five by five and then for i and greed x and for j in greed y we define the different subplots subplot um five by five by k plus one let's define k right here so k equals zero okay so that's it this is greed not grad we have greed that way and then now we're ready to use or to use this greed x values that's the values of i and j to generate new images okay so what we'll do now is this is plt the subplot so what we'll do now is we'll have our input which is tensorflow constant and then we have i j so these two values and then from here we have the output which is our va e but notice how we pick out uh layers too so uh to better understand this you should get back here where we defined our va e model so here you see this is uh the va e our original auto encoder which is made of different layers so uh this is layer zero layer one and then layer two um if you do va e dot layers um let's say zero see you have this input layer right here now if you change this and say anyway let's let's say for i in uh range three there we go we're going to print this out so we're going to print our layers i let's run that and we see that we have this input layer we have this uh functional model this other functional model but you should note that's basically our encoder and our decoder okay so that said we're going to make use of our va e layers here so we have the layers two this is to say that we're using the decoder actually so our decoder is going to take in the input see that it takes in the input there we go and then we'll have to select um our first axis here so that's why we have this and then we have to um select this other axis right here so sorry that our output now you see if we if we do this if we have zero here we do this see this is not going to be transformed to 28 by 28 so basically that's why we do this now once we have that we do the aim show and then we have our output our map is uh we there we go plot axis um off and then k plus equal one okay so we basically increase the value of one of k sorry sorry that uh we could have this as different subplots so let's run this now and then see what we get getting this error here so this is this is actually because we have in many more values as compared to what we're defining here in this subplots so what we should have here should be n and here should be n let's set let's run this again and see what we get now as you can see we are able to generate these digits making use of just this z vector right here which is composed of two numbers now one thing you can notice here in this latent space is that as we go from values of negative one to one in this twodimensional latent space the outputs are created or generated such that in each line as you could see we have one digit which is being slowly morphed into another see here we start with a nine but as we change values in this latent space see slowly we get to eight and then here you see eights and at this point you start with a nine and then you slowly get into fives and so on and so forth you can look at this horizontally as well as vertically see that we get to six and that and then you could also look at this diagonal you see nine eight three two six and then you get to zeros so for this first line or this first lines you can look at this first lines as going from nine to eight then here it's like from nine to five passing through three see and so on and so forth so basically this is how we generate images using variational auto encoders at this point one question you may ask yourself is we are having this outputs which have been generated from the making use of this z vector right here but how does the z vector this z vector here of the different images vary with the particular digits in those images so how does the z vector in say let's take this for example in six vary with that of zero with two and all other digits to experiment with this we'll copy out this code we had already and then get back here and now try to pass in the different digits into our encoder and then plot out the positions of our z vectors and then see whether there is some correlation with the z vector position and the input we have your right your other specific input digit that said we're gonna not make use of this so we have that and then here we're going to have our y train so this time around we'll be needing the y train because we need to know exactly what digit we're dealing with unlike here where we just needed to have the input images okay so that's it we have x train y train we're going to reload this and then we do the usual preprocessing from here we run this cell and then we move straight away to pass in those images in our encoder now similar to what we had here now we're going to be making use of our VAE layers and then picking out the first index so here we have output we could call this or basically we have z then we have the mean and the variance and then this is equal let's copy this here um there we go we have that then this is now one see that and then from here we pass in this x train so that's it we pass in the x train and then we could go ahead to start with a plot in uh plot figure specify the figure size figure size equals 12 by 12 there we go and then we do a scatter plot now the scatter plot will show us the different positions of the values of z that's the z points in our twodimensional space let's take this off this different z points and then also we're going to color this such that digits belonging to the same level have the same color there we go we're going to plot this as our x and then our y so we have that and then i will specify this y train as our levels so that's it and then we can now go ahead to plot out this color bar and show let's run that and here's what we get as you could see the levels here go from zero right up to nine and then you could notice these different clusters we have in here see these different clusters you see that this shows that the encoder path of our variational auto encoder has been trained such that now it's able to generate values of z where this point is outputted such that two inputs belonging to the same digit will be closer to each other as compared to two inputs belonging to two different digits so what we've been doing so far is we've been training the variational auto encoder model with our own custom training block you see right here we made use of this gradient tape and we carry out the gradient descent manually as compared to when you just have say model feet and then you pass in your training data set and basically this takes care of doing the training now the advantage of working with this feet method is that it's quite simple to use but compared to this custom training loops it doesn't give you that much freedom and that's why we're making use of this gradient tape here to train our model now with tensorflow it's possible to actually get the best of both worlds now this means that we can be able to train or get this custom training right here and then still make use of the feet method the way this is done is by overriding the train step method of our model which we shall define so that's set let's get right here and define this VAE model then we go ahead and define this model as we would do with a usual Keras model but just as we had done right here that's in this VAE model we had built here we need to take into consideration or take this argument here the encoder model and the decoder model so that's it now once we have these two models we have here our encoder which is our encoder model and we do the same for the decoder now once we're done with this decoder let's type this out we have your decoder as we're saying once we're done with this decoder the next thing we want to do is define some loss so here we have our loss it's actually a loss tracker now we'll get to understand better why we call it this a loss tracker so here we have the average or the mean of the different losses we shall get and then from here we'll go ahead to the main section where we shall override the train step method so just like let's add this here just like we used to working with methods like model like the compile method like the feet method here we have this train step method which is called each time we call on this feet method right here but since we are not going to make use of the default feet or the default train step method we'll have to override this and write out our own training block which is basically this block we have here so what we could do here is copy this out so we could simply copy this out and then paste it out here now we let's just copy all this there we go and that's it we paste this out so here we have our training block we'll call this train step so we override this train step method which is a Keras model method so here we have our train step we're taking in our batch there we go and then it's going to be similar to what we've had already in the training block so you see here we have z mean log bar which is gotten from the encoder model and from here we pass z and then we obtain y print we have our y true we obtain our loss we've already defined our custom loss method and that's it then from here we're going to update our loss metric so here we have loss tracker which we defined already here we're going to update its state so we update our loss state with the value of this loss so that's it now we are going to return the result so we have this dictionary we have loss and then we have our loss tracker dot result see that we have that so there we go we have updated our loss we output a loss but one important method we need to call again is the metrics method so here we have metrics you could define several metrics but here we just have this loss and then we return our loss tracker so we have loss tracker returned now notice how this is actually a list so you see this is a list because we could have say different losses which we could return and then we have the property decorator which we place right here so once we have this set you see we do not need any more to make use of this or to write this as as it was written and so this means that we we are not going to be writing out for example training start for epoch this the training loss is this and all of that now all we need to do is just call on our model dot fit method so right here let's do let's run this first let's run this right here there we go and then we go ahead we're getting an error now from here we we define our VAE so it's quite similar to to what we had already seen here see that now we have this VAE which is now this VAE we should create a year so let's have that VAE equal VAE and then it takes in the encoder model and the decoder model that's it so once we have this now we could do just as we were used to doing let's let's even say model let's just say model and then here we have model compile as usual and then here we're going to pass in our optimizer so here we have optimizer i guess we defined this optimizer already so let's check here yeah we have defined optimizer so here we just have optimizer equal optimizer we should define already and then we do model dot fit see that so that's all you just need to now make or call this fit method and then we have our train data set which we pass in the number of epochs we'll define this to be 20 and then the batch size equal say 128 so that's it so here unlike before where we needed for example to specify the optimizer in this way and then actually get in depth to understand that this gradient descent process needs for example the partial derivatives and the trainable weights now you'll see that unlike before where if you were for example needing to make use of some callbacks you would have to write that or include that in here with some custom code but now all you need to do is just have callbacks so we we we both will now to write our own custom training loops while still taking advantage of everything that comes with the fit method so right here you could have some callbacks and i'll be it anyway let's take this off and then let's get back here where there's this slight error you see here we have this VAE VAE dot trainable weights and that now this was in the case where we had defined our VAE this way we had this model and then get into our custom training loop right here let's get down here we had to actually uh we the way we got the trainable weights was from our VAE model which we had defined now given that we we have this VAE model right here the way we access the trainable weights is by using self so we have self dot trainable weights and then here we have self dot trainable weights and this is simply possible because we are inheriting from a Keras model which already has these different attributes which have been defined now that said let's run this that should be fine trainer complete you could see that we get in our usual outputs when we making use of the fit method and then we could go ahead and start with a testing so we run the cells and then in here we have some modifications to make so right here or previously we had this layers and predict and we took in the inputs and we got some output now here we have our model model that predicts you see that but this model is the is a whole VAE model so since we want to make use of our decoder we're going to say model dot decoder model there we go model dot decoder actually the decoder because here you define this decoder attribute so we have model dot decoder there we go dot predict and that should be fine okay so let's run this and then see what we get there we go you see we have exact same kind of outputs as when we were having our own custom training loop right here and that's it for the section hope you enjoyed it see you in the next section hi there and welcome to this new session in which we are going to delve deep into generative adversarial neural networks in the previous session we have seen how we made use of the variational auto encoders to generate these kinds of outputs which are in fact MNIST digits from just an input noise so what we had was this kind of encoder decoder structure where we had some input images here like this one for example some inputs and then we train the model such that the outputs look like this input and then once training is done we could take off this encoder and then just pass in a noise signal in here and then generate new outputs like what we have here which look like those from the original data set and in this section we'll be seeing how to build a new set of or a new category of generative models known as the GANs and we'll use them to generate images like this one which we have here you should note that all those images are images of people who do not actually exist but before we dive into practice and see how to build models which can build this kind of realistic looking images we should start by understanding how this generative adversarial neural networks that's GANs work now this GANs were first introduced in this paper by Goodfellow et al where the GAN architecture was first proposed and to understand how the GANs work let's make use of this figure from Louise Bouchard's post now let's suppose we have the bank who produces real money you see here we have this real $100 bill and then on this other end we have the thief who produces fake money you see here this $100 bill has this man with a mustache which is in the case of the real dollar bill and because differences like this and say for example this compared to this are very clear or can be easily seen this police officer is able to detect that this money is fake but when this police officer detects that the the the bank notice fake he or she says that it's fake because we have a mustache for example because we have this word fake written on it because this $100 has a fake around it and stuff like that so it tells the the forger or the thief what needs to be ameliorated in order to make sure that the next time this thief presents this fake money to the police officer the officer thinks it is maybe this real money and so if we suppose that real money takes a value of one and fake money takes a value of zero let's change the color fake money takes a value of zero then let's say in the first years of production of this fake dollar bills the police officer correctly says okay this is a zero and this is a one that's it's very evident at the beginning but then with time this thief gains experience and now produces fake money which looks just like the real money and this now pushes the police officer not to be able to distinguish between the real and the fake any longer now although we do not advocate for these kinds of malpractices it turns out that this is the way the gains actually work and in our case we'll replace this generator or we replace this thief by a generator model which is a neural network so this is going to be a neural network and we replace this police officer by a discriminator which also is a neural network specifically a binary classifier which takes in some input and says whether it is real or not whereas our generator here takes in some random inputs and then learns to output this bank notes such that the discriminator thinks that it is real now we shall head on to the GAN lab which is a project by Minsukang et al and we'll consider some very simple example so here we'll suppose all that data distribution is this year see that and then notice how we have a generator we have a discriminator and then the generator takes in some random noise and then outputs the sample and then the discriminator takes in the sample and says whether it is real or it's fake now apart from the fake samples it also takes in the real samples and then also says whether it's real or it's fake now now the weights of the generator and the discriminator have been updated such that after some time this samples which are going to be produced will look very much like this right here so let's go ahead and click on run and we see what we get you see how we start with the training let's take all this off see we start with the training for now the fix let's let's pause and start over let's restart that okay you see initially we're getting these kinds of outputs you see this output you could look at this here so this is this kind of sample is generated and this is the data of our real data right here and one thing we notice is that at times as time goes on you have here both the green and the purples which are considered to be real and then here's only the purples considered to be fake so this discriminator now starts making errors when it comes to saying whether a given sample is real or not whereas on the other hand the generator is now producing samples which look much like the real samples and it's because of this competition between the generator and the discriminator that they actually called the GANS generative adversarial this adversary comes from this competition between the generator and the discriminator and it now leads to the generator producing samples which look very much like the real samples one other point you should notice that as we carrying out this training overall we have two main parts we have this block right here oh let's take this block we have this block which consists of wearing when the discriminator takes in this real and then outputs some value the the output from this is used to update the parameters of the discriminator and then when the discriminator takes in this and then when the discriminator takes in this fake samples this output here is used now to update the generator so we have this block wherein we update the discriminator and we have this other block wherein we update our generator so with that now we could pass this and you could change the data distribution and so again in comparison to with the VAEs where we had this encoder and then the decoder well if we have a distribution like this one so we have some some input we want to have or we want to be able to output or get outputs which will look similar to this input distribution and then after training this encoder decoder we now break this up and then make use of only this decoder to now generate output which are similar our distribution is similar to that of this real inputs right here again another important point to note is the fact that after training after a certain number of epochs at some point where when the reals and the fakes look very similar the discriminator now becomes somehow confused as if you notice here you find some green patches you see you have some green patches purple patches here we have some green patches and purple patches so it's no longer able to distinguish between the reals and the fakes and so instead of as before or as at the beginning where I was able to say this is a one and this was a zero now it sees this as a 0.5 and this as a 0.5 it becomes confused and so in fact the aim of our GAN training process will be to ensure that the generator wins the fight it should be noted that most of very cool applications of GANs are in the domain of image generation and so we'll look at some of these applications in this article by Junotano Hoi GANs can be used in creating anime characters you see here we have this anime characters which have been generated automatically using GANs and so this means that similar to what we had here we are going to have a real data set which produces images similar to what we have right here let's get back here similar to what we have right here and then we're going to have a generator which will learn over time to generate images which will look like the real images and that's how we get to have images like this one now from here we also have pose guided person image generation here for example you'll notice that we have this input image and then if we want to have this same person but with a different pose then we could pass in this pose and then get this output right here so you see that we have this input that's this image with this different pose and then this is being generated another application is in crossdomain translations so you see here we have this input where we have this two or say three zebras and then we are able to transform this input image automatically into this other domain where we instead have horses and so this image or this input or this output rather has been generated from this input and this could be done in the reverse direction as we would see here we see you could go get from zebra to horse as we had here and then from horse to zebra then we have another example this is a star gun which permits us carry out translations or transformations where we modify specific high level features of an image so right here you see we have this input and then we add for example blonde hair so it changes the gender modified so that this this male becomes a female aged and talking about aged you could build an application which tells you or which shows you what you would look like after say 2050 years then here we have pale skin another awesome other modifications we could have here is he angry happy fearful and that so that's it you you see making use of this star again you could carry out these kinds of transformations on an input image the next we have this pixel dt again which creates clothing images and styles from an input image so you see we have the source image and we have this different images which have been generated from this source we have other examples right here it's good see and then we have super resolution now for super resolution gains have been used to increase the resolution of an input image while making this higher resolution images more realistic so you see this image for example this by cubic this one right here which has been gotten using the by cubic method and then we have this other image which is using res net and then now you'll notice that there is some difference with the kind of output we get using again notice how this image here looks more realistic or looks much more like the original as compared to what this other two known gain methods produce to even get a much clearer difference notice this part here where we have this water pouring you'll notice how this looks much more realistic as compared to using a classical neural network like the res net then from here move on to the next application which is that of generating faces by this time around very high definition faces so here you see you have 1024 by 1024 images generated you see this images of people who do not actually exist which have been gotten using a pro gain that's a progressive gain now we move to the next we have style gain which even comes with much better resolution and with some styling so from here we go on to high resolution image synthesis where we could get you see this semantic map and then from here we generate this output we have right here then the next go gains which as we've seen already takes these kinds of semantic maps and then produces this output you see this is the the ground truth this is the exact output and this is what the gain produces see that see from here we were to produce this from this we able to produce this now this kind of technology could be applied in video compression in a sense that during a video call where we have this input so supposing that we have this this is a sender and then this is a receiver we separate it by this dotted lines right here so we have this input right here and then we carry out key point extraction where we get this key points as you could see here and then this is what's been transmitted via the network so instead of transmitting this input we transmit this key points and then making use of some key frame which has been passed initially we combine those key points with the key frame to produce now this output right here which looks like this original input which we wanted to pass and so now at the receiving end we are able to get this here at a much lower bandwidth since we are taking in only the key points and not this whole input image then we also have applications in text to image where we could pass in a text like this flower has long thin yellow petals and a lot of yellow enters in the center and this generates this kind of output now talking about this kind of applications we could check out rayon.com which is in fact a del E mini model where we'll be able to create much more realistic output so let's click on this and while this is loading we could check out on the other applications the next application text to image we've seen this already face synthesis you see right here we get with a single input image we create faces in different viewing angles so for example we can use this to transform images that will be easier for face recognition so if we suppose that we get in this kind of input let's say we get this kind of input we could generate or we could convert this for example into this one here such that it will be easier for a face recognition model to do its job now from here we go to the next one image in painting okay so right here we have this inputs let's increase this take this off so as we're saying we have let's let's pick this one for example we have this input right here but we have this patch which has been taken off and now making use of again we are able to generate an output which will be like this input without the patch so just like the gain takes this patch off and as you can see the gain does its job quite well let's reduce this there we go before we move on let's get back to this output create it or i create on the com and you can see that this dial e mini model produces even much more realistic images then we move on to the disco gains where we could create outputs which match the style of a given input so supposing you want to go out on a little trip you want to say take this back and you wanting to get some ideas of the kind of shoe you could put on then you could make a call on the disco gain and you'll get this kind of output based on your input so this is a model which is similar to the cycle gain which we had already seen right here and where we were able to leave from one domain to another one other fun project will be to generate emojis from input images see here we have this input image for example and we have this emoji which is generated from the input then another very interesting application will be in deblurring so right here we suppose that we have this input image which is clearly blurred and then we want to de blur this image you see that we're able to produce this kind of images making use of gains and you can see from those images here that gains do this job quite well another application is in photo editing and so now you do not need to be some expert in photo editing to carry out some of this photo edits all you need now is some gain and you're good to go apart from image generation gains too can be used in music generation though in this course we shall focus on image generation then the medical domain gains could be used in anomaly detection and that's it for this section in the next section we are going to look at how these gains are actually trained in practice and the type of loss functions we use when training those gains hi there and welcome to this session in this session we're going to look at the gain loss function and also how gains have been trained then finally we'll look at common gain training problems so for a brief summary of what we've seen already we'll consider that we have this two distributions right here this distribution or this one this black dotted lines represents the real data that's similar to what we had here so similar to this real data right here and then this other distribution in green represents the fake data which is going to be generated by our generator so getting back here we have this you see this here which is our discriminator so right here we have the discriminator d then we have as we've said already the real and the fake distributions now once we start training we have this discriminator which sees that most most of the real data is getting a score of one or it classifies with the probability of one that this data is real and then as we approach this generated data as we get samples from generated distribution we find that the discriminator now outputs zero now as we keep on training the generation gets better and then the generated samples now look a bit more like the real data you see they come they get closer this distance here becomes smaller as compared to what we had before so we've trained and we've gotten to this point where now this discriminator still sees the real data to be one but now confuses or sometimes classifies this fake data to to be one so you see there around here see this samples we consider to be ones are classified as one though we'll still have some samples classified as zero and then once we get to convergence we have this half here so we have this classifier now which is unable to differentiate between the real and the generated of fake data because the distributions look very much alike and so that's it for the recap of the previous session now we're going to dive into the gain loss function so right here we have this training algorithm which we could see here expand this we have this training algorithm and if you notice we have this two loss functions here one for the discriminator and the other for the generator but this could be combined into one equation right here let's take this year and now we have our two player min max game with this value function v now when we talk about this min max game right here it makes allusion to what goes on between the generator and the discriminator so our two players in this case are our generator and the discriminator now one thing you could also do with this gain lab is you could put out your own distribution so let's say for example this distribution we apply and we start to train and the the generator now together with the discriminator start to play this game where at the end or at convergence we expect the generator to produce outputs which are similar to that of the discriminator now coming back to this equations you'll notice that we have min and g and max and d so to understand this notation you can consider that we are minimizing this expression right here by updating the parameters of g and then we maximizing this expression by updating the parameters of d which is our discriminator where g is our generator and then if we try to separate this two that's to to get for the to to start with minimizing for example for the generator you'd find that given that in this first expression you see this is this expression right here let's have that so in this expression there is actually no g so we could make use of only this when we're trying to minimize this whole expression with respect to parameters of g so that's why if you if you take in the algorithm given to us right here you find that for loss for g you have only that second expression now for the d let's get back here for the d d is in the both sides or this both this expression and this sort of expression so that's why we have the combination of the two in this our algorithm right here now to better explain or better understand this in depth let's consider this here so we've extracted for the d and for the g you should also note that let's get back here while we talk about updating the discriminator or updating the generator is basically our gradient descent step remember if we have for example tethered all right if we have let's take this off if we have tethered at a particular step let's say step i to get tethered at i i plus one step then we would have tethered i that's a previous step or let's say tethered i minus one let's take this off so we want to get tethered i which equal to the i minus one minus the learning rate times the partial derivative of the loss with respect to our tethered i minus one so this is basically what we have in here and this expression here where we have this reversed triangle that's this right here is actually this section of our gradient descent and so this means that we're finding the partial derivative of our loss this is the loss with respect to the parameters tethered then we have the same for the generator now let's get back here where we're going to understand in depth what all these different expressions actually mean right here we have this real sample and this fake sample so this two here we have real and fake and then with this real sample we pass it into the discriminator which we hope or which we will train to see that this is a one and then for the fake sample because this is a person who doesn't exist whereas this person actually exists and we'll train again which we'll see later on in this course to produce this type of output so let's get back here we have our discriminator then we from here let's take let's instead shift this let's move this this way so let's take this right here yeah okay so we have that and then here we have this right here now this is our fake sample and then we have our generator let's change the color for the generator so our generator produces this and then we have some random noise which we send in here because we just we just want to be able to produce this from some random noise so we we have this random noise right here which produces this our g which produces that fake sample and then we have our discriminator who is able to classify or say whether an input is fake or not okay so getting back here let's take this off getting back here when we want to train our discriminator we're gonna have an input x right here see this input x now this input x is this real sample right here so those are x i which is which is this here and these are discriminator which takes in this input then once the discriminator takes this input it is expected to output a one and if you get back here let's get back here you find you're told update the discriminator by ascending its stochastic gradient and then you update the generator by descending the stochastic gradient now generally when we're trying to ascend or ascending or descending actually is is different like what we've seen already where we had theta equal theta minus linear rate partial derivative of the loss with respect to theta is gradient descent which is actually what we're doing for the generator but when we talk about gradient ascent it's actually this instead we have a plus instead of a minus see that so if we have for let's plot our loss function here respect to theta for our gradient descent that's a classical gradient descent what we want to do is to minimize this loss but for our gradient ascent we want to instead maximize this loss and then getting back here before we move on we'll consider this plot of the log function so here if we have x and we have log of x then we have a plot which looks like this let's have this there we go and this is one so when x equal one the log is zero and then well as x takes smaller values approaching zero the log goes to us negative infinity so as we as x goes to us zero obviously from the right in this direction going in this direction then the log of x goes towards negative infinity so that's it now let's get back here for the discriminator as we've seen we're having a gradient ascent so we're trying to maximize this remember from our min and max loss function we we we're trying to maximize our discriminator so when we when the discriminator takes in a real sample it gives us an output of one and with an output of one the log of one so see here it's going to give us zero so the log of one is zero so this will give us a zero now you should note that all the outputs of our discriminator range between zero and one so our discriminator is our usual classifier so it's going to output values between zero and one and so getting a value of zero is the highest um you could get as an output after passing through the log so let me reexplain we we have this discriminator which outputs value between zero and one oh let's plot it out let's reply it again here so the discriminator outputs values between zero and one see that so this is what the discriminator outputs and so when you when you have the log of this values between zero and one then the maximum value for the log will be a zero see that the maximum value for all these values between zero and one will be a zero so when you have a zero it means you've maximized this and that falls in line with what you expect because we want to maximize this expression we have right here okay so we get back here for the reals we want to output a one and the log of one will give us the maximum possible value which is in this case zero now uh for this year we have a z remember this x is this real image here the zer is our random noise which we have seen already here so when this random noise passes through our generator we output a fake sample see that the generator takes in the random noise and then this generator let's have this here the generator outputs this fake sample and once the outputs this are fake sample we now take this fake sample and pass it into our discriminator and we expect our discriminator to produce instead of a one this time around a zero see that and in our case uh the log the log of one minus a zero is a log of one and the log of one is zero that's the highest possible value we could get when we're dealing with logs in the range zero to one so we maximizing this you see that now it's less understood we could move on to the generator for the generator we want to instead minimize this expression so since we're trying to minimize this expression we would expect the output from this to be negative infinity as the lowest possible value but uh let's get let's put in these values and see how we'll obtain this negative infinity now we have z which is our random noise when the random noise passes through g it outputs this fake sample again and now this time around we expect z to consider this fake sample this time around to be like a real sample so what we're saying here is at one instance we want our discriminator to see this as real another instance we want the discriminator to see this as fake and depending on the instance we're going to get or we're going to update parameters of the corresponding network so in the case where we want to see this as a fake we want to update the parameters of the discriminator such that it sees this as fake and then the case where we want this to be seen as a real by the discriminator want to be updating the parameters of the generator so in fact what's going to be happening here is let's change the scholar when training the discriminator we're going to freeze the generator so we're not going to update these parameters we're going to update the parameters of the discriminator now when training when training let's get back when training our generator we're going to freeze this year oops when training the generator we're going to freeze this and then update its parameters solita is able to fool the discriminator to think that this is a real sample and when he thinks it's a real sample he's going to output a one now log of one minus one is log of zero and log of zero is negative infinity see that and this is the minimum possible value so we're minimizing this expression so getting back here we have for a number of training iterations that basically for the number of ebooks uh we're going to do k steps we're going to update the discriminator we are k steps see four k steps do this we take we sample a mini batch of noise we get the noise uh mini batch of real samples then from here we obtain uh we get the generator outputs and then we obtain the output loss which we use to update the discriminator's parameters and then uh for these k steps i think in here they say we use k equals one the number of steps are applied say use k equal one so you could modify this although in practice k equals one is fine so um getting back here after going through k steps for this we're going to update now the generator so sample mini batch of m noise again so just like this then uh we obtain this output from the generator this time i only expect it to fold the discriminator so we're going to update the generator by descending its stochastic gradient such that it folds the discriminator and that's it for this section in the next section we're going to uh get into some practice and see how to get this kind of results we had here so these are the real samples and then uh what we'll obtain will be something like this see we're able to generate this um fake outputs of these people who do not actually exist and they look pretty realistic then one point we have to note before we move on is that the type of neural network used in this original GAN paper is a classic artificial neural network and these were kinds of outputs they got but in the practice we're going to be making use of the DC GAN so instead of the the simple GAN we'll make use of the DC GAN the DC actually stands for deep convolutional um and that's it DC deep convolutional so DC GAN means deep convolutional generative adversarial neural networks okay so here we're going to be looking at this um neural networks which are convolution based and so with that see you in the next section hi guys and welcome to the session in which we are going to write the code for generating images like this in TensorFlow from the previous session we had looked at the GAN loss function and now we'll see how to adapt this loss such that we could instead use the binary cross entropy loss and then from there we'll go ahead to look at different methods to make our GAN training much more stable you can see from here that this GAN training is in a very stable process since we have two adversaries where one is trying to maximize the loss and the other is trying to minimize the loss so by nature GANs are much more difficult to train as compared to other classical neural networks then after looking at how or looking at different methods to make this GAN training process more stable we'll go ahead to train our GAN from our previous session we had the discriminator loss and the generator loss now we want to introduce the binary cross entropy loss you'll notice that it looks quite similar to what we have here now let's start first with the discriminator for the discriminator it takes in this real image and then we expect it to output one see that we expect to have a wonder now for the binary cross if we have to compare this with binary cross entropy then this yshuffle this y here is going to be equal d of xi which is practically what we have here so this is our yshuffle and then y this yishuffle is going to be this here now when oh since we want our y to be equal one it means that one minus y will be zero so we will not consider this but we consider only this part right here now if we consider only this part we are left with this expression since y is equal one and then when y equals zero the case where we have this sorry this yshuffle equals zero then we'll have log of zero which is log of negative infinity minus that with this minus which we didn't have here is now in this year because we're dealing with a binary cross entropy the negative of the negative infinity gives us positive infinity so it means that for yshuffle equals zero we instead have positive infinity so here since we want to have yshuffle to be equal one then our aim will be to minimize the binary cross entropy loss since the values we could get will range between zero and positive infinity unlike here where the values were ranging from negative infinity to zero so here we was maximization problem we had gradient ascent but here we're trying to minimize this instead so we minimize this because we want to obtain a zero since it's when our output is zero that or is when our y shuffle is equal one that we get this output of zero so that said we have it for this first part for the next part when we're dealing with our data our fake data which has been generated by our generator we expect d to be equal to zero and since our y in this case is equal to zero since we have y to be equal to zero then this term is taken off and we're left with only this term now for this term we would have one minus the expected value of d which is zero that's one log one minus yshuffle where yshuffle is what the model predicts so in the case where we have this yshuffle to be equal zero as expected then we would have log of zero and that will give us zero so we would have a zero here now in another case or in the other case where our model instead predicts one year if the model instead predicts one then we'll have log of one minus one which is going to be zero log of zero is negative infinity this negative here turns out to positive infinity and so now we have positive infinity and since our aim is to obtain this yshuffle to be zero it means that we'll minimize this expression here such that it takes the value of zero over the with the aim of having a value of zero and again unlike this original expression where we went from negative infinity to zero now we're going from zero to positive infinity and so this means that when working with the discriminator and making use of the binary cross entropy we would go through a simple gradient descent where we will minimize the loss from here we will move on to the generator for the generator well we expect d to produce a one right here so we expect them to have one year and this means that this expression is taken off since we have one minus one which is zero so here we left only with this now we left with one bit is one so it's just log log of yshuffle which is what the model predict or what this d the discriminator would predict and so we have negative anyway let's just say negative let's take up meet the one on n and the sum so we have this expression right here and then in the case where the model actually predicts a one so the model is predicted to predict a one and it actually predicts a one in that case we would have a zero in the case where the model predicts a zero when when it's supposed to predict a one in that case we will have a log of zero that is negative infinity negative negative infinity is positive infinity and again here our aim is to obtain uh this zero right here so we will again be minimizing our bce loss now given that we'll be implementing the model or the architecture in this paper that is this again paper we should note some of these guidelines here we told replace any pulling layers with strided convolutions so instead of using pulling we use strided convolutions and fractional strided convolutions for the generator um the next use batch norm in both the generator and the discriminator then remove fully connected hidden layers for deeper uh architectures basically here we're using the convolutional layers instead of the fully connected hidden layers uh use relu activations in generator for all layers except for the output which uses the tanh activation then use leaky relu activation in the discriminator for all layers but before we move on to look at some of these details we should note that there is this github repo here by one of the authors authors of the paper that's um there we go let's get up here by Sumit Shintala and what he proposes here is this um list of tips and tricks used to make GANs work now as we said already it's it's not that evident to make GANs work so uh taking advantage of the experience from one of the authors of the original DC or of the DC GAN paper not the original GAN paper will be very interesting for us since we will not uh get to make the same mistakes which maybe he had made uh before discovering all those different tricks now that said we have here the very first one you should note that this list is no longer maintained i'm not sure how relevant it is in 2020 so this is this has been for a while uh six years six years anyways we have uh normalized the first one normalized the input so normalized the images between negative one and one then use tanh as the last layer of the generator again this was already in the paper the next tip will be to modify the loss function now it should be noted that we've already explained this but maybe the transition from this previous loss function to this other loss function wasn't made very clear now let's get back to that call for our loss function we had um we had a log of one minus d of g of z this for the generator remember so we had only this year only this expression in the original GAN paper they expected the discriminator to produce a zero although previously we had mentioned that we expect the discriminator to produce a one right from the beginning we've been speaking of this but in the original paper what they actually wanted was the discriminator to produce a zero for the generator before we go on to look at how to compute this BCE loss we are going to take into consideration this modification on this original loss right here and this modification comes in because of the following problem when the training just starts it's difficult for this discriminator to output a one unlike here where it's easier for it to output a zero when it sees fake data because remember let's get back to this and uh let's restart here let's restart this and let's pick another distribution oh let's let's pick this one which is slightly more complicated so when you just start with the training the the the generated outputs you see uh the generated outputs do not look very much like the real data you see this year it's very different from this and so because of this great difference at the start of the training it's difficult for us to make the generator fool the discriminator to output a one right here and also because of the fact that classifying whether an input is real or not is easier than generating new inputs the generator here will experience vanishing gradients and so instead of as we've seen already trying to minimize this we can instead maximize the sum of log dg of z so instead of log one minus dg of z we're going to have log dg of z the reason why is preferable for us to use this expression where we maximizing this year instead of minimizing this other expression is simply because when we make use of this expression we are being more lenient on the generator at the beginning of the training especially so when we were minimizing the log of one minus dg of z we had levels or if we had to make use of the binary cross entropy loss we would have levels which are equal zero and so if our level equals zero obviously this expression is left out and we left only with this part now we left with y equals zero and then we have log of one minus y just similar to what we have here but then the fact that at the very beginning we are expecting our discriminator to output y equals zero when it sees fake data from the generator is a problem because it's a very easy task for the generator for the discriminator especially because right here or at the very beginning this year is obviously not going to look like the real data and so the discriminator will find it very easy in predicting that this is fake data and because of this ease the generator's weights wouldn't be updated any further to ensure that we could produce even more realistic looking fake images and so what we do is we flip the levels and flipping the levels matches with this expression that is let's take this off we are having now y equal one so instead of y equals zero we now have y equal one so we're expecting the discriminator to output one when it sees fake data from the generator now doing this we have y equal one so obviously this is left out you see that this now matches with maximizing this expression and so when y equal one now we are telling the discriminator to output a one year when it sees fake data and now this will permit the generator to be able to update its weights and so make the training much more stable as compared to when we're dealing with or working with labels yi which are equal to zero you should also note that when we're talking about levels here we're talking about this yi's and when we're talking about what the model predicts we're talking about this y hats or this y shappels our next tip is one which we've discussed already so we told in the GAN paper is the last function to optimize g is the minimization of log one minus d but in practice folks practically use max of log d this uh what we have seen already where we had the minimization of one minus log d g of z so we had um log sorry log of one minus d g of z right here so we minimize this expression and then now instead of this we maximizing um the log of d g of z and the reason why we prefer to work this way as we've said already is because if you are expecting the discriminator to take in some fake data from here at the beginning and say that it is fake then this is going to be a very easy task especially at the beginning since the fake data here is going to look very much different from the real data and so because we our aim here is to make this discriminator output zero is going to be difficult for the generator to update its parameters such that the discriminator can start uh getting fooled and so instead of this as we've said already we flip the levels and uh instead aim for the discriminator to output once for the next tip we said uh they say don't sample from a uniform distribution so for the noise here for a generator noise we're going to sample from a Gaussian distribution or normal distribution okay so from there we're encouraged to use batch normalization avoid sparse gradients and unlike in the paper if we get back to the DCGAN paper and we check let's get to this we check here we we told use leaky relu activation in the discriminator for all layers and then use relu activation in the generator for all layers but what the say is uh the leaky relu is good in both the g and the d that's both the generator and the discriminator then one point you should notice the fact that the stability of the GAN game suffers if you have space sparse gradients now what does it mean by this uh if you have a relu activation if you have some input all negative inputs are sent to zero and all positives remain the same then for the leaky relu we have all negatives we should take up some value depending on what we pick the value to be if the value is 0.2 for example then uh an input of negative one will give us an output of negative 0.2 uh and an input of negative two will give us for example uh negative 0.4 an input of say whatever value stems negative 0.2 to get output now that said the this the positive section remains the same but now when we talk about those sparse gradients here it comes due to the fact that if we have relus in our network then we'll tend to have many zeros and the zeros uh or will cause this sparsity in the gradient and so because we do not want to have this and because we want to train the GANs in the most stable manner then we'll make use of the leaky relu then for down sampling we're told to use average pooling or conv2d and strides then for up sampling you speak the shuffle conv transpose convolutional transpose 2d with strides okay then from here we're told to use soft and noisy levels so what does this mean this means that if we have a discriminator and let's say we're training our discriminator where we have an input from a generator we train now we update the parameters of the generator where we pass in our fake data from this output of the generator into our discriminator and the discriminator has to compare the the output from the model let's let's put the output from the model in this color let's say the output from the model is 0.4 for example so what we'll be comparing will be this correct level with the model's prediction you see that now what the sayers apply level smoothing so instead of taking one we could take a random value around one so instead of this we could take for example say 1.2 or 0.8 or 0.9 or whatever value just around one so instead of having hard levels just like some strict levels either we have zero or we have one would take values around so around exchange the color will take values around zero and then values around one you see that so we use smooth leveling instead of some hard leveling then we also total make the level make the levels the noisy make the levels noisy for the discriminator that's occasionally flip the levels when training the discriminator okay so that's fine now next use this again when you can it works so if you if you can use this again and the model table use hybrid model like for example the VAE and GAN now here we have been told that if we are to generate images we should not use the original GANs that we should not use the simple neural networks that are fully connected neural networks we should go in for convolutional neural networks okay the next is to use stability tricks from reinforcement learning now we we get to treat your reinforcement learning so we're going to skip out this now from here use the Adam optimizer optimum Adam rules so the after Adam optimizer rules and then track failures early so if you want to be able to train your GANs without maybe upgrading at the end that you you haven't or you you can get the kind of results we expected you should try to ensure that you make sure your GAN isn't doing any one of this right here so if you're training and you and your loss the loss of the discriminator goes to zero then it's failure mode because your discriminator is proving to be too good at doing its job and then if the norms of the gradients are over a hundred things are screwing up when things are working the loss has low variance and goes down over time versus having huge variance and spiking so what the same year is if we have this the loss for the discriminator remember we have the discriminator on the generator what we expect to have is something like this which should go down slowly over time instead of having this kind of high variance and spiking now if the loss of the generator is steadily decreases then it's fooling the D that's a discriminator with garbage and so this means that we do not expect the generator to be so good that during training its loss just drops steadily and so as we said already you should track all those failures early on now the next we have don't balance loss via statistics unless you have a good reason to now oh they say they've tried it's hard and they've tried it all um let's take this off so what they're saying is when try to balance the training of the generator and the discriminator based on some loss value so if you are to try this you should have a principled approach to it rather than just intuition now if you have levels use them now talking about levels this means that if you have say we have this our discriminator and then we have our real data and then you have maybe some data set of fake data then you could train your discriminator like the usual classifier in supervised learning then the next point is to add noise to the inputs and then decay over time from here we have this tips where they're actually not sure so well we may just keep them actually uh this is for conditional gains we'll not take this into consideration use dropouts and g in both training test phase so provide noise in the form of dropout as this generally leads to better results much thanks to the authors smith emily martin and michelle then apart from the vanishing gradient uh problem another very common problem would be that of mode collapse where the generator produces output or produces the same outputs even after training for several epochs and so now we're going to start with building all this again while taking into consideration the tips and tricks which we've just seen hi there and welcome to the session in which we shall practically train again to produce images like this one here we'll start with the imports and then we'll move on to prepare our data the data set we shall be using will be a celeb a data set which signifies celeb faces attributes data set now this is over 200 000 images of celebrities with 40 binary attribute annotations let's uh open up some of this here you could have some of these images from this file up and there we go you see we have this faces right here and so what we'll be doing will be to train our discriminator alongside with our generator such that our generator can generate image of faces which can be able or which can be realistic enough to be able to fool the discriminator to think that they are actually real faces this notebook is provided by Jessica Lee on Kegel and can be downloaded so let's uh go straight away to download this data set and then start with uh uh DCGAN modeling in order to download a data set from Kegel we'll be needing this Kegel.json file right here now this Kegel.json file can be gotten from Kegel by getting to your account and then creating a new API token so once you have that you'll get right here and then click on copy API command which when you paste out here you see you have Kegel data set download and you have the the the user name of the person who uploaded this data set to Kegel platform and then you have the data set name right here but before carrying out this data set download we'll start by installing Kegel we'll make this directory we'll copy this Kegel.json into this directory then we can now go ahead and download the data set from this command API command which we downloaded or which we copied rather and then we can now unzip this into some data set folder or directory which we specify so that said let's simply run the cell and everything should move on well let's take this off as you can see the data set has been downloaded and now we're extracting the files into this data set folder right here now that we have this successfully extracted into our data set folder as we could see we specify the batch size the image shape and the learning rate now from here we let's run the cell and then we move on to create our tensorflow data data set so here we have let's call this data set and then we specify this path now to get this path you could click open right here and then you see if you click open this it's going to take a while since we have 200 000 of different images here so let's just let that and then we copy this path there's a path that you specify in here and once you specify that you have you have a labeling mode which is known we have the image size which was specified already we have the batch size anyway let's let we could have your batch size but it doesn't matter as we'll see shortly anyway we have that and then from here we run this cell you see you could have data set we're getting an error tensorflow not defined let's run this oops um that's fine um next this we've run this already and then we run this now this should be fine let's check out our data set it should use that so as we can see we have 202 599 files belonging to one class and our data set has been bashed so we have a batch data set you can see the shape right here we have 64 by 64 by 3 images now the default is um 256 by 256 so if we if we do not specify this let's see what we get let's take this off take that off and run this again and check out on the image size you see here when we don't specify anything you have 256 by 256 okay let's get back and run this again then we're moving to process our data so right here what we're going to do is we're going to make sure this data lies between negative one and one and so that's why we have in here the image divided by 127.5 minus one and so this means that any value we get between zero and 255 let's say for example we have the value 255 we'll take 255 divided by 127.5 which is two then minus one which gives us one so that's how we we preprocess these images and then after preprocessing we're going to unbatch because we need to reshuffle or because we need to drop the remainder so we unbatch and then we use the batching of our tensorflow data i then from here we carry out some prefetching for a more efficient way of loading the data now from here you can visualize a single element in our data set there we go we have 4d in our train data set let's take a single element we could print out its shape the shape and there we go now we have this shape which is 128 by 64 by 64 by 3 as expected and we could go ahead and visualize some elements here so let's visualize four elements we could increase this definitely so we should visualize four elements of this for now now here we have the subplot um plot image and we could take off the axis so let's run that and then see what we get see here we have this um four different images let's reduce this a little now uh one thing we could do too is modify this here this um value of our array now the reason we want to modify this is because this value ranges between negative one and one whereas this plot that image takes in values of range zero to one so we're going to modify this so we move from negative one one to the range zero one and to do that we need to take whatever value we have in this range add one to it and then divide by two so let's take this off let me get back here we just have plus one then divided by two there we go let's run that again and there we go you see you have now the images are much clearer and you you do not get the messages which we're getting previously now we'll go ahead with the modeling and we're going to use this same architecture presented in the dis again paper so right here we have this 100 dimensional latent vector and then this is projected and reshaped into this four by four by 1024 tensor and then from here we apply the up sampling that's actually the conf 2d transpose to then get this other vector right here notice how we're getting from four by four to eight by eight and then from here again repeat the same process 16 by 16 32 by 32 and then finally 64 by 64 also notice that while the size of the outputs keep increasing from eight six from eight to 16 32 64 the depth is reduced and so we'll go from 1024 to 512 to 256 to 128 and finally we have three so we get back to the code and we specify our latent dimension which is equal to 100 let's rerun this cell right here there we go and then that should be fine and then we go ahead and build our generator so here we have our generator or which we'll build with a sequential model we have tf keras sequential model and then we start to pass in our different layers so here we have our input layer input which has a shape of the latent dime or latent dimension so here we have latent dimension there we go and that's fine so that's our first layer and then the next is our dense layer so this is our projection so we project this such that the output is having four times four times the latent deem number of out units so just as we had seen in the paper now once we have this we move to the next layer which is going to be the reshape layer so we go ahead and reshape such that because at this point we're having latent dim is 100 so we're having 16 times 100 at 16 100 outputs now we reshape this such that it is a threedimensional tensor so here we have four by four by latent latent deem and that's it there we go so this is our next layer reshape from the reshape we go ahead to do the conv2d or the up sampling with the conv2d transpose as is the paper we have conv2d transpose and then we have 512 number of filters represent the number of output channels then the kernel size kernel size equal four now if we get back to the paper here let's get back to the paper you'll see that the kernel size isn't necessarily exactly equal four but one very important rule to follow when picking out the kernel size is that the kernel size has to be divisible by the number of strides so when we pick kernel size equal four we could have the strides to be equal to and the reason why we generally want that the kernel size is divisible by the number of strides is simply because of the quality of outputs will get generated by the generator when this isn't the case so always ensure that we have the kernel size divisible by number of strides now from here on we go on to apply batch normalization as suggested in the paper and also in the tips and tricks github repo so here we have batch norm and then from after the batch norm we have our leaky relu now for the leaky relu we have it takes in value of 0.2 so here we have 0.2 and that's it for this first part so we have this first block here which you could see in the paper this very first conv layer now once we have this here it could be repeated again so we just um copy this and then paste it out but modifying this depth so here we have 256 and then again oops here get back um and then here we paste this out and then here finally we have 128 okay so we have that uh for now we're not going to apply any dropout you could always feel free to apply that and see um the kind of results you will get so here we have this and then uh now we have that we we already set that's we have the let's get to the paper we have the first the second and the third conv layer now this final conv layer is to get an output which is like an image so we have 64 by 64 by three and so let's get back here um uh we will have no leaky relu or whatever like that just copy this out and this is out here okay so we have that and now we just come to the transpose we have um activation which is a tang so after the strikes here we specify the activation activation and this activation is uh um tang activation so just have tang and that's it pattern um equal same we'll also copy this out here so here we have pattern same and right here we have pattern same okay so that's it uh that should be for the generator i guess we've respected what we had in the tip and tricks and also um right here let's see we're told use relu activation and generator for all layers except for the output which is the tang we'll call this model the generator so we have our generator model and then let's run that and the next thing we want to do is summarize this so let's get a summary we check this out here and you see how we get in this year instead of three so let's go ahead and modify this right here uh this should be three let's run that again and get the summary okay so that's what we have then now we can move ahead to our discriminator so instead of generator right here we have this screaminator and the input we're gonna have here is gonna be 64 by 64 by three so it's actually in shape so in shape um the index in shape there we go by three and then instead of the conf 2d uh transpose layers we'll be using the conf 2d layer so here we have conf 2d and then the the depth increases instead here instead of decreasing as we had with the generator so here we go from 64 so we start with 64 and then we move on to 128 and so on and so forth for now let's take this off since we're dealing with a conf 2d and then again we have the kernel size which is divisible by the number of strides we have the leaky relu as we've seen already in the tips and tricks um take this off here but we'll make use of the batch num still so let's let's place this out and then we have batch batch normalization we have the batch normalization there we go um here is conf 2d but we increase the depth so we have 128 now that we have this depth increased we just simply copy this out and paste it over the next layers so uh for the next blocks because we consider this would be a block and this a block and this a block um now we move on to 256 batch norm leaky relu still and that's it now for the final or for the last conf layer let's just paste this out here let's take this off uh we have this last conf layer right here we'll give it a depth of one and then given that our discriminator call that our discriminator is a usual classifier which takes in the 64 by 64 by 3 input and then outputs a single value whether one or zero or a value between zero and one actually so you output a single value here so at this point we should be thinking of using some dense layer and then specifying that its output is going to have only one unit okay so let's take this up and then now from here we could um flatten so we flatten um what we get is output from the conf 2d layer and then after flattening we could uh have our dense layer one see here we have just one output and then uh the activation activation is sigmoid so that's it recall with a sigmoid with a sigmoid we have values or our inputs from negative infinity to positive infinity which have been mapped in the range zero zero to one actually and that's exactly what we need right here so we have that um that's fine let's take this off now now before we move on it should be noted that unlike the sigmoid which maps values between zero and one the tense function maps values between negative one and zero so this is from zero to one and then the tense maps values between negative one and one and that's what we use in the final layer for the for the discriminate for the generator and now for the discriminator we're using the sigmoid okay so we have that understood now we have here our discriminator okay so we have that let's run this cell and then finally we're gonna have our summary discriminator summary let's run this and then see what we get there we go we have our summary um everything looks fine and now we could go ahead start with our training and just like we had done previously we're gonna overwrite the training step so here we have a VAE model which we had built previously where we override at this train step right here and with this we're able to make use of methods like the model.fit so here instead of the VAE we have again let's get back we have again model this again model is made of a discriminator discriminator and a generator so let's replace the encoder and decoder by the discriminator and the generator respectively then here we have our discriminator discriminator and our generator self generator and self discriminator we can modify the compile method let's modify this compile method the compile method actually will take in the optimizer for the discriminator the optimizer for the generator and then the loss function so we have uh the optimizer let's say the optimizer g optimizer and then the loss function so that's our compiler method and then we also go ahead and define our discriminator loss metric and our generator loss metric we've taken all these three and then we've put this out here okay so that's it now let's have our d loss metric and our g loss metric and then from here we move on now to the training step for the training let's recall that we have a discriminator we have a generator and then this generator takes in a fake data here takes in a vector fake this uh this noise here and then generates fake data so takes noise our g takes noise generates fake data and then this fake data is then passed onto the discriminator which says whether it's a one or zero or gives the value between zero and one okay uh we also have our real data right here which is also going to be passed into our discriminator and it's also going to give a value between zero and one now we should be noted that we'll start with training the discriminator and when you're training the discriminator we're going to freeze the generator that's we do not update its parameters so we're going to just update the weights of the discriminator just like we had seen previously so the first thing we want to do here is to get our noise the noise is tf random normal so we have normal and then we specify since we have a normal distribution we want to specify its shape now the shape of this here would be um the shape will be our latent deem now let's let's have our latent deem we should have defined already so here we'll have latent dimension now given that we'll be working in batches we need to add the batch dimension so here we have batch size by latent deem now to obtain the batch size all we need to do here batch size is equal tf dot shape of our x batch x batch and then we get the zero value now from here we have the batch size we have the noise and then we're ready to fit this into a generator and then obtain the fake data then also make use of the real data which is basically this year because this is our real data remember our data set is made of 200 000 different images or faces of celebrities and what we've done is we've broken this up into batches of 128 so for every batch we're going to take this x batch here which is basically the real data and then we're also going to use make use of this noise generated fake data and then train our discriminator and so with that we have our fake data or let's say fake images fake images equals self generator which takes in the noise or let's just say some random noise here we have random noise random noise vector actually okay so we have this now we have our fake images that we have this year now the next and we also have the real so we can now dive into training a discriminator now as you may know that the discriminator's last function will take in the output from here that's output from the real data and compare it with one and then taking the output from those fakes and compare it with zero so we'll take the output from the real let's call this r and compare with one and then we take the output from the fakes let's call it f and then compare it with zero so getting back into the code right here let's change this let's call this real images and here we have real images real images there we go so here we want to have the predicted output or better still let's say real predictions real predictions so you have our real predictions here now the real predictions are gotten from taking in our discriminator discriminator and then this discriminator actually takes in real images so we're presenting what's going on right here so we have this real which gets into our discriminator and then we output the real predictions which would then compare with the value one so that's it we can now go ahead and compute the loss so we'll have discriminator loss for real is equal our last function which we're going to pass in and then this last function takes in the real predictions and uh once so oh yeah we will have both the real predictions and the real levels uh the real levels as we've said already are the ones so it's basically this one year since we we haven't batches of images we we have several ones of size the batch size so here we have real um we there we go we have real um levels see tf ones and then we specify its size or better still its shape so here we have the the shape which is batch size by one see that batch size by one and the reason why we have batch size by one is simply because we have an output which takes in just one single value while this output will all for the real level will be equal one and we have the batch size okay so we have that now the next thing we want to have here are the fake levels now this fake levels is going to be this zeros right here so we have uh sorry we have zeros and then we have batch size and one okay so that's it now we we have our real levels and we have our fake levels and we will take the real levels we'll take the real predictions that is we take we we get what the model thinks about a particular input that's all the classification and then compare it with the real levels the real levels is once because we expect that the model should take in a real input and then know that it is a real input and that and that means that it should output a one when it takes in a real data and if it outputs a value different from one then the loss is going to be greater than zero whereas if it's exactly equal one the loss is going to be zero and our m years minimizes loss now we have that and the next thing we want to do is repeat this but this time around for the fake predictions so here we have fake predictions now the the first step is we have real data getting into the discriminator and the other we have fake data getting into the discriminator so here we have fake predictions and this time around it doesn't just take the real images but it takes the generated images so we have self generator and it takes in noise actually so we should take in a random noise which is this one right here so it takes some random noise and then it outputs fake images but since we'll define this already here which we could just make use of it here so let here we have fake images and there we go so here we have the discriminator which takes in a fake image and then gives us a fake prediction and we're going to compare this fake prediction with zero so we expect that the the fake predictions should be zero if not the loss isn't going to be equal to zero so here instead of real levels we have fake levels so we comparing zero with what the model is going to predict or the output of the discriminator oh yeah we have discriminator fake we have here fake that's it i think that's okay we the same loss function actually is a binary cross entropy loss and once we have this now we could have could define the loss to be equal the loss real plus the loss fake since that's basically a combination of this two losses right here now before you move on you could recall in the tips and tricks we saw the level smoothing now here we have our levels our real and our fake levels let's separate this and now what we're going to do is instead of taking a one we'll take values around one so we take we add plus 0.25 times some random value between negative one and one so basically what we're saying here is we want to take this one and then add it plus a value in the range of negative 0.25 and 25 on 0.25 so this means that now instead of having the level to be fixed at one we would have the level to be between because one minus 0.25 is 0.75 so we between 0.75 and 1.25 instead of just one so that's how what our level will be now and then for the zeros since we we don't want to have negative values we'll take the zero plus some random value between zero and 0.25 so instead of zero we'll have some random value between zero and 0.25 okay so that said what we'll do now is we'll get right here we have tf random uniform and then we specify the mean vowel which is negative one and then the max vowel which is one now specifying this means we're going from negative one to one and then multiplying by 0.25 means we're going from negative 0.25 to 0.25 so that's basically it and then also we specify its shape so we have um the batch size by one there we go now we're just going to copy this out and then paste this right here so we don't want to have negative numbers so we start from zero instead so here we have zero to one but by default the values are already from zero to one so we could take this off okay so that's it everything looks fine i think everything is done for our discriminator now we have in this let's take this off we have our loss and that's it okay so that's basically it we now move on to our partial derivatives here we take in our d loss and then we are going to update um the discriminator so here we have this screen this screen meter dot trainable weights then for the optimizer is the optimizer which we have specified already right here so here we have instead of this optimizer we have our d optimizer which we're going to specify so we have the optimizer uh takes in the partial derivatives and our trainable weights again here we are training only the discriminator so we have discrete meter that's it so here we go we have our model our again the discriminator the trainable weights and we repeat the same process here now uh this is self okay so that's it uh this should be fine now and then next step we do is we're going to do this same but for the generator so what we'll do now is we're going to again sample some noise random noise here we copy this code out and then paste it out after this okay there we go so we have this random noise right here random noise and then from this random noise we have our fake images um cell generator and it takes in the random noise okay now we're gonna have the same again so we're gonna uh make use of the gradient tape as we've done already with the discriminator so paste that out here and then here we have we'll be working instead with a generator um this is discriminator discriminate let's go back discriminator okay so as we've said this is let's let's just comment this let's find out generator and then right here we have the discrete discriminator okay so that's a discriminator and now for the generator so as we were saying we have those fake images we have our random noise and we make use of the random noise to generate the fake image but this time around we want to follow the discriminator so instead of expecting our fake levels to be zeros as we had here this time around our fake levels will be ones and we'll obviously not have anything to do with the real data so let's take this off and then get back here now we have the fake levels which which is equal one and it happens that they have actually been flipped so let's let's um get down here and then paste this out here so here we have flipped flipped fake levels which are actually ones instead of zeros remember we've seen this already so we have that and we're not going to do any level smoothing right here so we have that and the next thing we want to do is start with our recording of the gradients now yeah let's take this off we have our fake predictions we have um yeah we go discriminator takes in the fake images and that's it now here we have the flipped flipped fake levels and then we have our fake predictions fake predictions i guess if we we should have had we should have made an error here this is actually fake fake predictions we're comparing the fake predictions with the fake levels and then here we're comparing the fake predictions with the flipped fake levels so that's it um that should be fine here we have g so this is our g loss g loss that's not like g loss fake or g loss real we just have g loss and that should be it partial derivatives self generator that's it radar and then here we have updated our generator so we're not making we're not updating the parameters of the discriminator here now here we have g optimizer that's it okay so um if that's okay we have now to update the different states so we have d loss metric we update the state and we pass in the d loss and then we repeat the same for the g loss that's the generator loss so here we have g and then here we have g then for loss we have g loss g loss metric result metric that's it and then here we have d loss self d loss metric and then result okay let's now run the cell and normally everything should work fine now we move on to define the number of epochs so we're gonna work for 20 epochs and then let's get back here we define our gain which is those gain we've just defined and then takes in the discriminator and the generator which we defined already uh from here we go ahead and compile the model so we have again compile and then we specify the optimizer the d optimizer optimizer which is uh the atom optimizer optimizers and then atom now this atom optimizer will be or with a learning rate learning rate which was specified already at the beginning learning rate equal two times standard negative four which is specified at the beginning so we have your learning rate and then beta one beta one equal zero point five now we'll put the same for the generator let's get back here copy that and then paste it out here so this is for our generator now now you notice that there's no there's no major difference actually there's no difference we just use the same optimizer now we have that the next thing we want to do is pass in our loss function so we have your loss function which is equal our binary cross entropy loss so we have losses dot or binary cross entropy and that's it so we have this set we could run this now and then go ahead to train our model by calling on gain dot fit method or just model of fit method so we have history equal gain dot fit and then we pass in our train data set we'll start with say 10 or let's let's take just 100 elements first and then here the number of epochs equal epochs we should define already and then we have some callback so will you make use of this callback and you already see the advantage of overriding the train step method as now we could just define our callback and then pass it in here and the job is done so we will have this callback which is going to show us the generated images at the end of an epoch so let's call it show image and it's going to take in latent deem okay so that's it everything looks fine now the next thing we want to do is define this show image callback right here now here let's let's define this callback just above here so we have our show image callback and then we get the latent dimension and then we specify on epoch and so at the end of every epoch we are going to run this code right here now what's going on here is simple we have our model and then we have the generator we take in some random noise we pass it into our generator and then we try to see what the model is generating so this means that when we're training or initially we have some output or some fake data generated by the generator and then after an epoch we want to see what the model is generating as we keep on training our whole complete model so this that this is very important as we could already be able to debug and understand what's going on so this means that in a case where the model is say for example generating the same kinds of output or generating some outputs which are clearly not the type of output which we expect to get or whose distribution is very far away from that of the real data then we will have to take some measures so it's very important to work with these kinds of callbacks as they already permit us to debug our whole model training process so that said we just have to specify the figure size here and then what we're doing is we we have in this different subplots because we see we have 64 we could you could reduce this or it could increase just depends on you you could just pick whatever you want to pick here so you could generate a certain number of images here n is equal to 6 so we're generating 6 by 6 that's 36 images so we could change this to 36 and then now for each and every subplot we're going to show the image you see this out this outcome from here so it's from the generator and that's it now we're going to save this figure in some in some directory which we could visualize so let's modify this let's take here to be 36 um that's fine let's run this we have that show image that's fine um everything looks fine now let's go ahead and uh start with the train so that we could even reduce this to just stand so that we could be able to notice any errors quickly and then now train on the full data set we're getting this error unexpected keyword argument mean vowel this actually mean let's get back here without the underscore so this is here you have this main vowel and here we have max vowel now you could feel free to check out the documentation and you should find the exact syntax so let's run this again and then start with the training oh getting these arrows well we are told that no no gradients are provided for any variable and when you look at this you will notice that we have come to the transposes this means that most probably this arrow is coming from the generator so let's get back here and make sure everything is okay uh one thing we notice here already is that this should be g so here should be g and what do we have again here um yeah it looks fine so let's run this again and then see what we get we still get this arrow which again shows that it's coming from the generator we get back here and what do we notice we notice here that we we get these fake images out of the scope of our gradient tape so what we have to do is instead directly call this in here so we we we want to update the parameters of the generator so it has to be in this gradient tape scope and not outside so let's take this off take that off and one question you may be asking yourself is why is it possible to just do this here let's have this out but not in the generator and the simple answer is for the discriminator this generator isn't updated so it doesn't matter if it's in here or not whereas for the generator it actually matters so we we have to make sure it's in here so that's it let's run this again and then see what we get so that's it training started we told no such file directory generated and all of that anyway let's let's make this directory generated and then we run this again there we go didn't seem to be working fine you see that struggling to generate some images let's open up our generated year you see we have the different files let's check out on say the 18th output you see this already so you see it's struggling already to produce you see this image here this one year looks already like a human face though it's still struggling a lot so we have that let's let's check out on say the 19th there we go okay so that's it let's close this hopefully there's no issue with connection let's get back up and then if those images aren't very visible you can retrain and go for or take many more samples so here we're dealing with 100 we take 100 and then after let's check this out after six epochs you see the kind of results we get it's the images we get already look like humans now let's modify this code let's stop the training and then let's modify the code such that we do not flip the levels there we go we could get back to the flip thick levels and then instead of ones we just take the thick levels so let's let's take zeros so if we have this we're going to compare it with when we actually do the flip leveling so let's run this um let's uh run the cells and then we start with the train after nine epochs we can now check out those generated images let's let's open up the zero open up the third the feet and then say the eight um let's see what we get here see see this our generators experiencing vanishing ingredients and that's why we're getting these kinds of horrible outputs so let's um again stop the training let's stop the training and then let's get back to our models what we'll do now is instead of using the leaky relu we just use the relu so let's let's change this activation and set it to relu there we go let's um simply places out everywhere and then take off the leaky relu and then see the kind of output we would get in case we we we used the the the relu itself instead of the leaky relu um let's take that off everything looks fine now let's get back into the generator and then repeat the same so space is out here relu activation take off leaky relu take off leaky relu and then rerun this again um this should be fine now let's run that and then see what we get okay now that train has been going on for a while now we could open up the zero let's say open up the second the feet the seventh um and then the tent so we could uh look at this now what do you notice you see we're getting practically nothing as output right here and that's simply because we using the relu instead of the leaky relu and as you can see sparsity isn't great for generating images so you have to be very careful with that now let's stop the straining um stop the training and then uh get back to what we had before um we have this yeah this should be fine uh no discriminator then right here get back again and oops let's get back then we restart the train again and then see what we get now that train has been going on for a while let's go ahead and open this up one there's zero take out two for example five um seven and say eight so let's check out what the model is outputting see um there we go we can see that with the normal relu it isn't doing that bad either so in this specific example using the leaky relu isn't maybe oh that necessary okay so now we've looked at the effect of not using this leaky relu um you could also or take off the batch norm and see how that affects the kinds of outputs you get from the generator um here we get an error let's run this again okay that's fine now let's go ahead and uh restart the training by this time around with a full data set so let's take this off and then start with the training after training for over 20 epochs here are the kinds of outputs we shall be getting in our deep learning for image generation course we delve deep into how to create even much higher quality outputs like this one for example which was created with a diffusion model or this other one which we created with a program that's it for this section on image generation with the variational auto encoders and the generative adversarial neural networks so we've come to the end of this course and if there's one thing we suggest you're doing is working on as many projects as possible on your own so you could make use of tools like tensorflow hugging face 1b onyx just to name a few to build and deploy image segmentation models object content models text detection text recognition depth estimation image search engines pose estimation face recognition drawsness detection license plate recognition object tracking and video classification and if you want to take a deep dive into natural language processing image generation or object detection you could check out those different courses on the neural learn.ai platform with that said we should very best as you move forward in your career
