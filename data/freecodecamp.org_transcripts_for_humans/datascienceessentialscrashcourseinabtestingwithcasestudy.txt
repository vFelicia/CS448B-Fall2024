With timestamps:

00:00 - in this applied data science crash
00:02 - course you learn all about AB testing
00:05 - from the concepts to the Practical
00:07 - details they can apply in business AB
00:09 - testing is commonly used in data science
00:12 - it's an experiment on two variants to
00:15 - see which performs better based on a
00:17 - given metric this course merges in-depth
00:20 - statistical analysis with the kind of
00:22 - data science theories big Tech firms
00:24 - rely on T from L Tech developed this
00:27 - course she is a very experienced a
00:30 - scientist and teacher welcome to the
00:32 - handsone ab Testing crash course where
00:35 - we will do some refreshment when it
00:37 - comes to AB testing if you're looking
00:39 - for that one course where you can learn
00:41 - and quickly refresh your memory for AB
00:44 - testing and how to actually do an AB
00:46 - testing case study hands on in Python
00:49 - then you are in the right place in this
00:52 - crash course we are going to refresh our
00:54 - memory for the a test design including
00:57 - the power analysis and defining those
00:59 - different PR such as minimum detectable
01:01 - effect statistical significance level
01:04 - and also the uh type two probability so
01:06 - the power of the test and then we are
01:08 - going to do Hands-On case study project
01:11 - where we will be conducting an AB
01:13 - testing results analysis in Python at
01:16 - the end of this course you can expect to
01:18 - know everything about designing an AB
01:20 - test what it means as to design a proper
01:23 - AB test and how to do a Ab test results
01:26 - analysis in Python in a proper way I'm
01:29 - dat Vas and co-founder at Lun Tech and I
01:32 - have been in data science for the last 5
01:34 - years I have learned AB testing end to
01:37 - end after following numerous blogs and
01:39 - numerous research papers and courses and
01:42 - I've noticed that there is not a one
01:44 - place one course that will cover all the
01:47 - fundamentals and necessary stuff both
01:50 - the theory and implementation in Python
01:52 - in one place and that's about to change
01:55 - as we have this crash course that will
01:57 - help you to do exactly that to learn how
02:00 - to design an AB test in a proper way as
02:03 - a good and solidated scientist and to
02:05 - Showcase your skills by doing python AB
02:09 - testing results and asset don't forget
02:11 - to subscribe like and comment to help
02:14 - the algorithm to make this content more
02:16 - accessible to everyone across the world
02:19 - and if you want to get free resources
02:22 - make sure to check the free resources
02:24 - section at lunch. and if you want to
02:28 - become a job ready data scientist and
02:30 - you are looking for this accessible boot
02:32 - camp that will help you to make your job
02:35 - ready data scientist consider enrolling
02:38 - to the data science boot camp so whether
02:40 - you are a product scientist whether you
02:42 - are a data analyst data scientist or a
02:46 - product manager who wants to learn about
02:47 - AB testing at high level and how it can
02:50 - be done in Python then you are in the
02:53 - right place because in this crash course
02:55 - we're going to refresh our memory what
02:57 - it means to properly design an a test
02:59 - test which means doing power analysis
03:02 - and also calculating the sample size by
03:05 - hand by following the statistical
03:07 - guidelines and ensuring that everything
03:10 - is done properly and then as the second
03:13 - part of this Crush course we are also
03:15 - going to do an handson case study in
03:18 - Python when it comes to performing AB
03:20 - testing results analysis so we are going
03:23 - to cover all these important Concepts
03:25 - such as P values sample size and also uh
03:29 - interpreting the ab test results using
03:31 - standard error calculating those uh
03:33 - estimates pulled variance and then
03:36 - evaluating the ab test results including
03:38 - confidence interal generalizability of
03:41 - the results reproducibility of the
03:43 - results so without further Ado let's get
03:49 - started AB testing is an important topic
03:52 - for data scientists to know because it's
03:54 - a powerful method for evaluating changes
03:57 - or improvements to the products or
03:59 - services
04:00 - it allows us to make data driven
04:02 - Decisions by comparing the performance
04:04 - of two different versions of a product
04:06 - or a service usually referred as
04:08 - treatment or control for example a
04:11 - testing allows data scientists to
04:13 - measure the effectiveness of changes to
04:15 - your product or a service which is
04:17 - important as it enables data scientists
04:19 - to make data driven decisions rather
04:21 - they're relying on Intuition or
04:24 - assumptions secondly AB testing helps
04:26 - data Sciences to identify the most
04:29 - effective change changes to a product or
04:31 - a service which is really important
04:33 - because it allows us to optimize the
04:35 - performance of a product or a service
04:37 - which can then lead to increased
04:39 - customer satisfaction and
04:42 - sales AB testing helps us also to
04:45 - validate certain hypothesis about what
04:47 - changes will improve a product or
04:49 - service this is important because it
04:52 - helps us to build a deeper understanding
04:54 - of the customers and the factors that
04:56 - influence customers
04:57 - Behavior finally AB testing is a common
05:01 - practice in many Industries such as
05:02 - e-commerce digital marketing website
05:05 - optimization and many others so data
05:08 - scientists who have knowledge and
05:09 - experience in a testing will be more
05:12 - valuable to these
05:14 - companies no matter in which industry
05:16 - you want to enter as a data scientist
05:18 - and what kind of job you will be
05:20 - interviewed for and even if you believe
05:22 - more technical data scien is your cup of
05:24 - tea be prepared to know at least higher
05:26 - level understanding and the details
05:28 - behind this method will definitely help
05:30 - you to know about this topic when you
05:31 - are speaking with product owners
05:33 - stakeholders product scientists and
05:35 - other people involved in the
05:38 - business let's briefly discuss the
05:40 - perfect audience for the section of the
05:42 - course and prerequisites there are no
05:44 - prerequisites of the section in terms of
05:46 - AB testing Concepts that you should know
05:48 - already but knowing the basics and
05:51 - statistics which you can find in the
05:53 - fundamentals to statistics section is
05:55 - highly recommended this section will be
05:58 - great if you have no priority AB testing
06:00 - knowledge and you want to identify and
06:02 - learn the essential AB testing Concepts
06:04 - from scratch so this will help you to
06:06 - prepare for your job interviews it will
06:09 - also be a good refresher for anyone who
06:11 - does have AB testing knowledge but who
06:13 - wants to refresh their memory or want to
06:15 - fill in the gaps in their knowledge in
06:18 - this lecture we will start off the topic
06:20 - about AB testing where we will formally
06:22 - Define what AB testing is and we will
06:25 - look at the high level overview of AB
06:27 - testing process step by step
06:32 - by definition AB testing or split
06:34 - testing is originated from the
06:36 - statistical randomized control trials
06:38 - and is one of the most popular ways for
06:41 - businesses to test new ux features new
06:43 - versions of a product or an algorithm to
06:46 - decide whether your business should
06:47 - launch that new ux feature or should
06:49 - productional IE that new recommender
06:51 - system create that new product that new
06:54 - button or that new
06:56 - algorithm the idea behind a testing is
06:59 - that you should show the variated or the
07:01 - new version of the product to sample of
07:03 - customers often referred as experimental
07:05 - group and the existing version of the
07:07 - product to another sample of customers
07:09 - referred as control group then the
07:11 - difference in the product performance in
07:13 - experimental versus control group is
07:15 - tracked to identify the effect of these
07:18 - new versions of the product on the
07:20 - performance of the product so the goal
07:22 - is then to track the metric during the
07:24 - test period and find out whe there is a
07:27 - difference in the performance of the
07:28 - product and and what type of difference
07:30 - is it the motivation behind this test is
07:33 - to test new product variants that will
07:35 - improve the performance of the existing
07:37 - product and will make this product more
07:39 - successful and optimal showing a
07:41 - positive treatment effect what makes
07:44 - this testing great is that businesses
07:46 - are getting direct feedback from their
07:48 - actual users by presenting them the
07:50 - existing versus the variated product
07:52 - version and in this way they can quickly
07:54 - Test new ideas in case of ab Test shows
07:58 - that the variated version is not
08:00 - effective at least businesses can learn
08:02 - from this and can decide whether they
08:03 - need to improve it or need to look for
08:05 - other ideas let us go through the steps
08:08 - included in the AB testing process which
08:11 - will give you a higher level overview
08:13 - into the
08:14 - process the first step in conducting AB
08:16 - testing is stating the hypothesis of the
08:18 - ab test this is a process that includes
08:21 - coming up with business and statistical
08:23 - hypothesis that you would like to test
08:25 - with this test including how you
08:27 - measured the success which will
08:29 - primary
08:31 - metric next step in AB testing is to
08:34 - perform what we call power analysis and
08:37 - design the entire test which includes
08:39 - making assumptions about the most
08:40 - important parameters of the test and
08:42 - calculate the minimum sample size
08:44 - required to claim statistical
08:47 - significance the third step in AB
08:49 - testing is to run the actual AB test
08:51 - which in practical sense for the data
08:53 - scientist means making sure that the
08:55 - test runs smoothly and correctly
08:57 - collaborate with engineers and product
08:59 - managers to ensure that all the
09:01 - requirements are satisfied this also
09:04 - includes collecting the data of control
09:06 - and experimental groups which will be
09:07 - used in The Next
09:10 - Step next step in AB testing is choosing
09:13 - the right statistical test whether it is
09:15 - z test T Test Ki Square test Etc to test
09:19 - the hypothesis from the step one by
09:21 - using the data collected from the
09:23 - previous step and to determine whether
09:25 - there is a statistically significant
09:27 - difference between the control versus
09:29 - experimental
09:32 - group The Fifth and the final step in AB
09:34 - testing is continuing to analyze the
09:36 - results and find out whether besides
09:38 - statistical significance there is also
09:40 - practical significance in this step we
09:43 - use the second step's power analysis so
09:45 - the assumptions that we made about model
09:47 - parameters and the simple siiz and the
09:50 - four steps results to determine whether
09:52 - there is a practical significance beside
09:54 - of the statistical significance this
09:57 - summarizes the AB testing process at a
09:59 - high level in next couple of lectures
10:01 - we'll go through the steps one at a time
10:04 - so buckle up and let's learn about AB
10:07 - testing in this lecture lecture number
10:10 - two we will discuss the first step in a
10:12 - testing process so let's bring our
10:15 - diagram back as you can recall from the
10:18 - previous lecture when we were discussing
10:20 - the entire process of AB testing at a
10:22 - high level we saw that in the first step
10:24 - in conducting AB testing is stating the
10:26 - hypothesis of ab test this process
10:29 - includes coming up with a business and
10:31 - statistical hypothesis that you would
10:32 - like to test with this test including
10:35 - how you measured the success which we
10:37 - call a primary metric so what is the
10:39 - metric that we can use to say that that
10:42 - the product that we are testing performs
10:45 - well first we need to State the business
10:47 - hypothesis for our AB test from a
10:50 - business perspective so formally
10:52 - business hypothesis describes what the
10:54 - two products are that being compared and
10:56 - what is the desired impact or the
10:58 - difference for the businesses so how to
11:00 - fix a potential issue in the product
11:02 - where a solution of these two problems
11:04 - will influence what we call a key
11:06 - performance indicator or the kpi of the
11:09 - interest business hypothesis is usually
11:12 - set as a result of brainstorming and
11:14 - collaboration of relevant people on the
11:16 - product team and data science team the
11:19 - idea behind this hypothesis is to decide
11:21 - how to fix a potential issue in the
11:23 - product where a solution of these
11:25 - problems will improve the target kpi one
11:28 - example of business hypothesis is that
11:30 - changing the color of learn more button
11:33 - for instance to Green will increase the
11:36 - engagement of the web
11:41 - page next we need to select what we call
11:43 - primary metric for our av testing there
11:46 - should be only one primary metric in
11:48 - your ab test choosing this metric is one
11:51 - of the most important parts of ab test
11:53 - since this metric will be used to
11:55 - measure the performance of the product
11:57 - or feature for the experiment Al and
11:59 - control groups and they will be used to
12:02 - identify whether there is a difference
12:03 - or what we call statistically
12:05 - significant difference between these two
12:06 - groups by definition primary metric is a
12:09 - way to measure the performance of the
12:11 - product being tested in the ab test for
12:14 - the experimental and control groups it
12:16 - will be used to identify whether there
12:18 - is a statistically significant
12:19 - difference between these two groups the
12:21 - choice of the success metric depends on
12:23 - the underlying hypothesis that is being
12:25 - tested with this AB test this is if not
12:28 - the most one of the most important parts
12:30 - of the ab test because it determines how
12:32 - the test will be designed and also how
12:34 - will the proposed ideas perform choosing
12:37 - poor metrics might disqualify a large
12:39 - amount of work or might result in wrong
12:41 - conclusions for instance the revenue is
12:44 - not always the end goal therefore in AB
12:47 - testing we need to tie up the primary
12:49 - metric to the direct and the higher
12:51 - level goals of the
12:55 - product the expectation is that if the
12:57 - product makes more money then this
12:59 - suggests the content is great but in
13:01 - achieving that goal instead of improving
13:03 - the overall content of the material and
13:06 - writing one can just optimize the
13:08 - conversion funless one way to test the
13:10 - accuracy of the metric you have chosen
13:12 - as your primary metric for your ab test
13:14 - could be to go back to the exact problem
13:17 - you want to solve you can ask yourself
13:19 - the following question what I tend to
13:21 - call the metric validity
13:23 - question so if the Chen metric were to
13:26 - increase significantly while everything
13:28 - else T constant would we achieve our
13:31 - goal and would we address our business
13:32 - problem is it higher revenue is it
13:35 - higher customer engagement or is it high
13:37 - views that we are chasing in the
13:39 - business so the choice of the metric
13:42 - will then answer this question though
13:44 - you need to have a single primary metric
13:46 - for your ab test you still need to keep
13:48 - an eye on the remaining metrics to make
13:50 - sure that all the metrics are showing a
13:52 - change and not only the target one
13:55 - having multiple metrics in your ab test
13:57 - will lead to false positives since you
13:59 - will identify many significant
14:01 - differences well there is no effect
14:03 - which is something you want to avoid so
14:05 - it's always a good idea to pick just a
14:07 - single primary metric but to keep an eye
14:10 - and monitor all the remaining
14:15 - metrics so if the answer to the metric
14:18 - validity question is higher Revenue
14:20 - which means that you are saying that the
14:22 - higher revenue is what you are chasing
14:25 - and better performance means higher
14:26 - revenue for your product then you can
14:28 - use your primary metric what we call a
14:30 - conversion rate conversion rate is a
14:33 - metric that is used to measure the
14:34 - effectiveness of a website a product or
14:37 - a marketing campaign it is typically
14:39 - used to determine the percentage of
14:40 - visitors or customers who take a desired
14:43 - action such as making a purchase filling
14:45 - out a form or signing up for a service
14:48 - the formula for conversion rate is
14:51 - conversion rate is equal to number of
14:53 - conversions divided to number of total
14:55 - visitors multiplied by 100% for example
14:59 - if a website has thousand visitors and
15:01 - 50 of them make a purchase the
15:04 - conversion rate would be equal to 50
15:06 - divide 2,000 multiply by 100% which
15:09 - gives us 5% this means that our
15:11 - conversion rate in this case is equal to
15:14 - 5% conversion rate is an important
15:17 - metric because it allows us and
15:19 - businesses to measure the effectiveness
15:21 - of their website a product or a
15:23 - marketing campaign it can help
15:25 - businesses to identify areas for
15:27 - improvement such as increasing the
15:28 - number of conversions or improving the
15:30 - user experience conversion rate can be
15:33 - used for different purposes for example
15:35 - if a company wants to measure the
15:37 - effectiveness of an online store the
15:39 - conversion rate would be the percentage
15:40 - of visitors who make a purchase and on
15:43 - the other hand if a company wants to
15:45 - measure the effectiveness of landing
15:47 - page the conversion rate would be the
15:49 - percentage of visitors who fill out a
15:51 - form or sign up for a service so if the
15:53 - answer to the metric validity question
15:55 - is higher engagement then you can use
15:57 - the clickr rate or CTR as your primary
16:01 - metric this is by the way a common
16:03 - metric used in a testing whenever we are
16:05 - dealing with e-commerce product search
16:07 - engine recommander system clickr rate or
16:11 - CTR is a metric that measures the
16:13 - effectiveness of a digital marketing
16:15 - campaign or the user engagement or some
16:17 - feature on your web page or your website
16:20 - and it's typically used to determine the
16:21 - percentage of users who click on a
16:24 - specific link or button or call to
16:26 - action CTA out of the total to number of
16:29 - users who view it the formula for the
16:31 - clickr rate can be represented as
16:33 - follows so the CTR is equal to number of
16:36 - clicks divided to number of Impressions
16:38 - multiply by 100% not to be confused with
16:41 - click through probability because there
16:43 - is a difference between the click
16:44 - through rate and click through
16:46 - probability for example if an online
16:48 - advertisement receives thousand of
16:50 - Impressions which means that we are
16:51 - showing it to the customers for a
16:53 - thousand times and there were 25 clicks
16:56 - which means 25 out of all this
16:58 - impression resulted in clicks this means
17:00 - that the clickr rate for this specific
17:02 - example would be equal to 25 divide
17:05 - 2,000 multiply by 100% which gives us
17:09 - 2.5% this means that for this particular
17:11 - example our clickr rate is equal to
17:14 - 2.5% cure rate is an important metric
17:17 - because it allows businesses to measure
17:19 - the effectiveness of their digital
17:20 - marketing campaigns and the user
17:22 - engagement with their website or web
17:24 - pages High click through rate indicates
17:27 - that a campaign or the web page or
17:29 - feature is relevant and appealing to the
17:31 - target audience because they are
17:33 - clicking on it while low clickthrough
17:35 - rate indicates that a campaign or the
17:37 - web page needs an improvement click
17:39 - through rate can be used to measure the
17:41 - performance of different digital
17:42 - marketing channels such as PID search
17:45 - display advertising email marketing and
17:48 - social media it can also be used to
17:50 - measure the performance of different ad
17:52 - formats such as text advertisements
17:54 - Banner advertisement video
17:56 - advertisements Etc
18:00 - next and the final task in this first
18:01 - step in the process of AP testing is to
18:04 - State the statistical hypothesis based
18:06 - on business hypothesis and the chosen
18:08 - primary
18:09 - metric next and in the final task in
18:12 - this first step of the AB testing
18:13 - process we need to State the statistical
18:15 - hypothesis based on the business
18:17 - hypothesis we stated and the chosen
18:19 - primary metric in the section of
18:22 - fundamentals through statistics of this
18:23 - course in lecture number seven we went
18:26 - into details about statistical
18:27 - hypothesis testing included what n
18:29 - hypothesis is and what alternative
18:32 - hypothesis is so do have a look to get
18:34 - all the insight about this topic AB
18:37 - testing should always be based on a
18:39 - hypothesis that needs to be tested this
18:42 - hypothesis is usually set as a result of
18:44 - brainstorming and collaboration of
18:46 - relevant people on the product team and
18:49 - data science team the idea behind this
18:51 - hypothesis is to decide how to fix a
18:54 - potential issue in a product where a
18:56 - solution of these problems will
18:58 - influence the key performance indicators
19:00 - or the kpi of interest it's also highly
19:03 - important to make prioritization out of
19:06 - a range of product problems and ideas to
19:08 - test while you want to P that fixing
19:10 - this problem would result in the biggest
19:13 - impact for the
19:15 - product we can put the hypothesis that
19:18 - is subject to rejection so that we want
19:19 - to reject in the ideal World Under The N
19:22 - hypothesis what we Define by AG zero
19:25 - well we can put the hypothesis subject
19:27 - to acceptance so the desire hypothesis
19:29 - that we would like to have as a result
19:31 - of AB testing under the alternative
19:33 - hypothesis defined by
19:35 - H1 for example if the kpi of the product
19:38 - is to increase the customer engagement
19:40 - by changing the color of the read more
19:42 - button from blue to green then under the
19:45 - N hypothesis we can state that clickr
19:48 - rate of learn more button with blue
19:50 - color is equal to the click through rate
19:51 - of green button under the alternative we
19:54 - can then state that the click true rate
19:56 - of the learn more button with green
19:57 - color is Lar larger than the click
19:59 - through of the blue
20:03 - button so ideally want to reject this no
20:06 - hypothesis and we want to accept the
20:08 - alternative hypothesis which will mean
20:10 - that we can improve the clickr rate so
20:13 - the engagement of our product by simply
20:15 - changing the color of the button from
20:17 - blue to green once we have set up the
20:20 - business hypothesis selected the primary
20:22 - metrics and stated the statistical
20:24 - hypothesis we are ready to proceed to
20:26 - the next stage in the ab testing
20:30 - process in this lecture we will discuss
20:33 - the next Second Step In AB testing
20:35 - process which is designing the ab test
20:37 - including the power analysis and
20:40 - calculating the minimum sample sizes for
20:42 - the control and experimental groups stay
20:45 - tuned as this is a very important part
20:47 - of AB testing process commonly appearing
20:49 - during the data science interviews some
20:52 - argue that AB testing is an art and
20:55 - others say that it's a business adjusted
20:57 - common statistical test but the
20:59 - borderline is that to properly Design
21:01 - This experiment you need to be
21:03 - disciplined and intentional while
21:04 - keeping in mind that it's not really
21:06 - about testing but it's about learning
21:09 - following AR steps you need to take to
21:11 - have a solid design for your ab test so
21:14 - let's bring the diagram back so in this
21:16 - step we need to perform the power
21:18 - analysis for our AB test and calculate
21:20 - the minimum sample size in order to
21:22 - design our AB
21:24 - test AB test design includes three steps
21:27 - the first step is power analysis which
21:30 - includes making assumptions about model
21:32 - parameters including the power of the
21:34 - test the significance level Etc the
21:37 - second step is to use these parameters
21:39 - from Power analysis to calculate the
21:41 - minimum sample size for the control and
21:43 - experimental groups and then the final
21:46 - third step is to decide on the test
21:48 - duration depending on several factors so
21:51 - let's discuss each of these topics one
21:53 - by
21:54 - one power analysis for AB testing
21:57 - includes this tree specific specific
21:58 - steps the first one is determining the
22:01 - power of the test this is our first
22:03 - parameter the power of the statistical
22:05 - test is a probability of correctly
22:07 - rejecting the N hypothesis power is the
22:10 - probability of making a correct decision
22:12 - so to reject the N hypothesis when the N
22:15 - hypothesis is false if you're wondering
22:18 - what is the power of the test what is
22:20 - this different concepts that we just
22:21 - talk about what is this null hypothesis
22:23 - and what does it mean to reject the null
22:25 - hypothesis then head towards the
22:27 - fundamental statistic section of this
22:28 - course as we discuss this topic in
22:31 - detail as part of that
22:35 - section the power is often defined by 1
22:38 - minus beta which is equal to the
22:40 - probability of not making a type two
22:42 - error where type two error is a
22:44 - probability of not rejecting the null
22:46 - hypothesis while the null is actually
22:48 - false it's common practice to pick 80%
22:51 - as the power of the ab test which means
22:53 - that we allow 20% of type to error and
22:56 - this means that we are fine with not
22:58 - detecting so failing to reject n
23:00 - hypothesis 20% of the time which means
23:03 - that we are fine with not detecting a
23:05 - true treatment effect while there is an
23:07 - effect which means that we are failing
23:09 - to reject the N however the choice of
23:12 - value of this parameter depends on
23:14 - nature of the test and the business
23:17 - constraints secondly we need to
23:19 - determine a significance level for our
23:21 - AB test the significance level which is
23:24 - also the probability of type one error
23:26 - is the likelihood of rejecting the no
23:28 - hence detecting a treatment effect while
23:30 - the know is actually true and there is
23:32 - no statistically significant impact this
23:35 - value often defined by a Greek letter
23:36 - Alpha is a probability of making a false
23:39 - Discovery often referred to as a false
23:41 - positive rate generally we use the
23:43 - significance level of 5% which indicates
23:46 - that we have 5% risk of concluding that
23:48 - there exists a statistically significant
23:50 - difference between the experimental and
23:52 - control variant performances when there
23:54 - is no actual difference so we are fine
23:56 - by having five out of 100 cas Cas is
23:58 - detecting a treatment effect well there
24:00 - is no effect it also means that you have
24:02 - a significant result difference between
24:04 - the control and the experimental groups
24:06 - within 95% confidence like in the case
24:10 - of the power of the test the choice of
24:12 - the alpha is dependent on the nature of
24:14 - the test and the business constraints
24:15 - that you have for instance if running
24:18 - this a test is related to high
24:20 - engineering course then the business
24:22 - might decide to pick a high offer such
24:24 - that it would be easier to detect a
24:26 - treatment effect on the other hand the
24:28 - implementation costs of the proposed
24:30 - version in production are high you can
24:33 - then pick a lower significance level
24:35 - since this proposed feature should
24:37 - really have a big impact to justify the
24:39 - high implementation cost so it should be
24:41 - harder to reject n
24:44 - hypothesis finally as the last tyep of
24:47 - power analysis we need to determine a
24:49 - minimum detectable effect for the
24:51 - test last parameter as part of the power
24:54 - analysis we need to make assumptions
24:55 - about is what we call minimum detectable
24:58 - effect or Delta from the business point
25:00 - of view so what is the substantive to
25:02 - the statistical significance that the
25:04 - business wants to see as a minimum
25:06 - impact of the new version to find this
25:08 - variant investment
25:11 - worthy the answer to this question is
25:13 - what is the amount of change we aim to
25:15 - observe in a new versions metric
25:17 - compared to the existing one to make
25:19 - recommendations to the business that
25:21 - this feature should be launched in the
25:23 - production that it's investment worthy
25:26 - an estimate of this parameter is what is
25:27 - known as as a minimum detectable effect
25:30 - often defined by a Greek letter Delta
25:32 - which is also related to the Practical
25:34 - significance of the test so this mde or
25:36 - the minimum detectable effect is a proxy
25:39 - that relates to the smallest effect that
25:40 - would matter in practice for the
25:42 - business and it's usually set by
25:44 - stakeholders as this parameter is highly
25:46 - dependent on the business there is no
25:48 - common level of it instead so this
25:51 - minimum detectable effect is basically
25:53 - the translation from statistical
25:55 - significance to the Practical
25:56 - significance and here we want to see and
25:59 - we want to answer the question what is
26:00 - this percentage increase in the
26:02 - performance of the product that we want
26:04 - to experiment with that will tell to the
26:06 - business that this is good enough to
26:08 - invest in this new feature or in this
26:10 - new product and this can be for instance
26:12 - 1% for one product it can be 5% for
26:15 - another one and it really depends on the
26:17 - business and what is the underlying
26:22 - kpi a popular reference to the
26:24 - parameters involved in the power
26:25 - analysis for AB testing is like this so
26:28 - 1 minus beta for the power of the test
26:31 - Alpha for the significance level Delta
26:33 - for the minimum detectable effect to
26:36 - make sure that our results are
26:37 - repeatable robust and can be generalized
26:39 - to the entire population we need to
26:41 - avoid P hacking to ensure real
26:44 - statistical significance and to avoid
26:46 - biased results so we want to make sure
26:48 - that we collect enough amount of
26:50 - observations and we run the test for a
26:52 - minimum predetermined amount of time
26:55 - therefore before running the test we
26:57 - need to determine the samp size of the
26:58 - control and experimental groups as well
27:00 - as later on in this lecture we will see
27:02 - also how long we need to run the test so
27:05 - this is another important part of AB
27:07 - testing which needs to be done using the
27:10 - defined power of the test which was the
27:12 - one minus beta the significance level
27:14 - and a minimum detectable effect so all
27:16 - the parameters that we decided upon when
27:19 - conducting the power
27:21 - analysis calculation of the sample size
27:24 - depends on the underlying primary metric
27:26 - as well that you have chosen for
27:28 - tracking the progress of the control and
27:30 - experimental versions of the product so
27:32 - we need to distinguish here two
27:34 - cases so when discussing the primary
27:36 - metric we saw that there are different
27:38 - ways that we can measure the performance
27:40 - of different type of products if we are
27:42 - interested in engagement then we are
27:44 - looking at a metric such as click
27:46 - through rate which is in the form of
27:48 - averages so the case one will be where
27:50 - the primary metric of AB testing is in
27:52 - the form of a binary variable it can be
27:55 - for instance conversion or no conversion
27:58 - click or no click and in case two where
28:01 - the primary metric of the test is in the
28:03 - form of proportions or averages which
28:05 - means mean order amount or mean click
28:08 - through
28:09 - rate for today we will be covering only
28:12 - one of these cases but you can find more
28:14 - details on the second case in my blog
28:17 - which I posted also as part of the
28:18 - resources section this blog post
28:21 - contains all the details that you need
28:23 - to know about AB testing including the
28:25 - statistical test and their corresponding
28:27 - hypothesis the descriptions of different
28:29 - primary metrics that go beyond what we
28:31 - have covered as part of this section as
28:33 - well as many more details that you need
28:35 - to know about a
28:38 - testing so let's look at a case two
28:40 - where the primary metric of the test is
28:42 - in the form of proportions or averages
28:44 - so let's say we want to test whether the
28:46 - average click to rate of control is
28:48 - equal to the average click to rate of
28:49 - experimental group and under HD we have
28:53 - that the m control is equal to M
28:54 - experimental and under H1 we have that
28:57 - the m control is not to Mu experimental
28:59 - so here the MU control and mu
29:01 - experimental are simply the average of
29:03 - the primary metric for control group and
29:05 - for the experimental group respectively
29:08 - so this the formal hypothesis we want to
29:10 - test with our AB test and we can assume
29:13 - that this new control is for instance
29:15 - the clickr rate of the control group and
29:17 - the MU experimental is the clickr rate
29:19 - of the experimental
29:21 - group so this is the formal statistical
29:24 - hypothesis we want to test with our AB
29:26 - test if you haven't done so I would
29:28 - highly suggest you to head towards the
29:30 - fundamental statistic section of this
29:32 - course where in lecture number seven and
29:34 - eight of the statistical part of this
29:36 - course I go in detail about statistical
29:38 - hypothesis testing the means averages
29:41 - significance level Etc this also holds
29:44 - for the theorem that the some prise
29:46 - calculation is based upon called Central
29:48 - limit theorem so check out the last
29:51 - lecture about inferential statistics
29:53 - where I covered the central limit
29:55 - theorem which we will also use in this
29:57 - section and finally also check the
30:00 - lecture number five in that section
30:02 - where we cover the normal distribution
30:04 - another thing that we will use as part
30:07 - of this section so the central limit
30:09 - theorem states that given a sufficiently
30:11 - large sample size from an arbitrary
30:13 - distribution the sample mean will be
30:15 - approximately normally distributed
30:17 - regardless of the shape of the original
30:18 - population distribution this means that
30:20 - the distribution of the sample means
30:22 - will be approximately normal if we take
30:24 - a large enough sample even if the
30:26 - distribution of the orig sample is not
30:29 - normal so when we are dealing with a
30:31 - primary performance tracking metric that
30:33 - is in the form of average such as this
30:35 - one that we are covering today which is
30:36 - a clickr rate we intend to compare the
30:39 - means of the control and experimental
30:40 - groups then we can use the central limit
30:43 - theorem as state that the mean sampling
30:45 - distribution of both controlling
30:46 - experimental groups follow normal
30:49 - distribution consequently the sampling
30:51 - distribution of the difference of the
30:52 - means of these two groups also will be
30:55 - normally distributed
30:59 - so this can be expressed like this where
31:01 - we see that the mean of the control
31:02 - group and mean of the experimental group
31:04 - follows normal distribution with mean mu
31:07 - control and mu experimental respectively
31:09 - and then with the variance of Sigma
31:11 - control squared and sigma experimental
31:13 - squared respectively though derivation
31:16 - of this Pro is out of the scope of this
31:17 - course we can state that the difference
31:19 - between the means of the true group so
31:21 - xar control minus xar experimental also
31:25 - follows normal distribution with a mean
31:26 - new control minus new experimental and
31:29 - with a variance of Sigma control squ /
31:31 - to n Control Plus Sigma experimental
31:33 - Square / to n experimental so the sample
31:36 - size of the experimental group and the
31:38 - sample size of the control group hence
31:41 - the sample size needed to compare the me
31:43 - of the two normally distributed samples
31:45 - using a two-sided test which prespecify
31:47 - significance of alpha power level and
31:50 - minimum detectable effect can be
31:52 - calculated as
31:54 - follows so here you can see the
31:56 - mathematical representation of the
31:58 - minimum sample size so the N which
32:00 - stands for the minimum sample size is
32:02 - equal to and in denominator we have Sig
32:04 - control S Plus Sigma experimental squar
32:07 - multip by z 1us alpha / to 2 + z 1us
32:11 - beta squ / to the Delta squ and here the
32:14 - Alpha and the beta and the Delta we have
32:17 - made assumptions about as part of the
32:18 - power analysis and the sigma control
32:21 - squar and a sigma experimental squared
32:23 - are the uh estimates of the variance
32:25 - that we can come up with using the
32:27 - So-Cal A8 testing I would say you do not
32:30 - necessarily need to know this derivation
32:32 - as there are many online calculators
32:34 - that will ask you for the alpha the beta
32:36 - and the Delta values as well as the
32:38 - sample estimates for the sigma squ
32:40 - control and experimental and then these
32:42 - calculators will automatically calculate
32:44 - the minimum S size for you if you're
32:46 - wondering what this AA testing is and
32:48 - how we can come up with the sigma
32:49 - control squared and sigma experimenting
32:51 - squared as well as all the other values
32:53 - then make sure to to check out the blog
32:55 - that I posted before and that I
32:57 - mentioned before as I explained in
32:59 - detail all these values as well as check
33:01 - out the resource section where I've
33:02 - included many resources regarding this
33:05 - but for now just keep in mind that the
33:07 - Z1 minus Alpha / to two and Z1 minus
33:10 - beta are just two constants and come
33:12 - from the normal distributed and standard
33:14 - normal distributed tables I would say
33:16 - you do not necessarily need to know this
33:18 - derivation as there are many online
33:20 - calculators that will ask you for this
33:22 - Alpha Beta And Delta values as well as
33:24 - the sule estimates for the sigma squ
33:26 - controling Sigma experimental control
33:29 - and then we'll calculate automatically
33:30 - the sample size for you for the control
33:32 - and experimental group
33:36 - effectively one example of such
33:38 - calculator is this AB testy online
33:40 - calculator but if you Google it you will
33:42 - find many others that will ask you for
33:44 - the minimum detectable effect for the
33:46 - statistical significance or the
33:48 - statistical power and then it will
33:50 - automatically calculate for you the
33:51 - minimum sample size that you should have
33:53 - in order to have a statistical
33:55 - significance and in order to have a
33:56 - valid AB test
33:58 - one thing to keep in mind is that you
33:59 - will notice that the statistical
34:01 - significance level is set to 95% in here
34:04 - which is not what we have seen when we
34:06 - were discussing the alpha significance
34:08 - level so sometimes these online
34:10 - calculators will confuse or will
34:12 - interchangeably use the significance
34:14 - level versus the confidence level which
34:16 - are the opposite so the significance
34:18 - level is usually at the level of 5% or
34:21 - 1% confidence level is around 95% so
34:24 - which is basically 100% minus the alpha
34:27 - therefore whenever whenever you see this
34:28 - 95% know that this means that your Alpha
34:31 - should be 5% so it's really important to
34:33 - understand how to use this calculator
34:35 - not to end up with the wrong minimum
34:36 - sample size conduct an entire AB test
34:39 - and then at the end realize that you
34:41 - have used the wrong uh significance
34:46 - level the final step is to calculate the
34:48 - test duration this question needs to be
34:50 - answered before you run your experiment
34:53 - and not during the experiment sometimes
34:55 - people stop the test when they detect
34:57 - statistical significance which is what
34:58 - we call P hacking and that's absolutely
35:01 - not what you want to do to to determine
35:03 - the Baseline of the duration time a
35:05 - common approach is to use this formula
35:07 - as you can see duration is equal to n
35:10 - ided to the number of visitors per day
35:12 - where n is your minimum sample size that
35:14 - we just calculated in the previous step
35:16 - and the number of visitors per day is
35:18 - the average number of visitors that you
35:20 - expect to see as part of your
35:23 - experiment for instance if this formula
35:26 - results in 14 days or 14 this suggest
35:28 - that running the test for two weeks is a
35:30 - good idea however it's highly important
35:33 - to take many business specific aspect
35:35 - into account when choosing the time to
35:38 - run the test and for how long you need
35:40 - to run it and simply using this formula
35:42 - is not enough for example if you want to
35:45 - run an experiment at the end of the
35:46 - month December with Christmas breaks
35:49 - when higher than expected or lower than
35:50 - expected number of people are usually
35:52 - checking your web page then this
35:54 - external and uncertain event had an
35:56 - impact on the page page to search for
35:58 - some businesses this
36:02 - means for example if you want to run an
36:05 - experiment at the end of the month of
36:06 - December with Christmas breaks when
36:08 - higher than expected or in some cases
36:11 - lower than expected number of people are
36:13 - usually checking the web page so
36:15 - depending on the nature of your business
36:16 - or the product then this external and
36:19 - uncertain event can have an impact on
36:21 - the page usage for some businesses which
36:24 - means that for some businesses a high
36:26 - increasing the page usage can be the
36:28 - result and for some a huge decrease in
36:30 - usability in this case running AB test
36:33 - without taking into account this
36:35 - external Factor would result in
36:37 - inaccurate results since the activity
36:39 - period would not be true representation
36:41 - of a common page usage and we no longer
36:44 - have this Randomness which is a crucial
36:46 - part of AB
36:47 - testing beside this When selecting a
36:50 - specific test duration there are few
36:52 - other things to be aware of firstly two
36:54 - small test duration might result in what
36:56 - we call novelty effects users tend to
36:59 - react quickly and positively to all
37:01 - types of changes independent of their
37:03 - nature so it's referred as a novelty
37:06 - effect and it vares of in time and it is
37:08 - considered illusionary so it would be
37:11 - wrong to describe this effect to the
37:12 - experimental version itself and to
37:14 - expect that it will continue to persist
37:16 - after the noble T effect wears off hence
37:19 - when picking a test duration we need to
37:21 - make sure that we do not run the test
37:23 - for too short amount of time period
37:25 - otherwise we can have a novelty effect
37:27 - novelty effect can be a major threat to
37:29 - the external validity of an AV test so
37:32 - it's important to avoid it as much as
37:35 - possible secondly if the test duration
37:38 - is too large then we can have what we
37:40 - call maturation effects when planning an
37:42 - AB test it's usually useful to consider
37:44 - a longer test duration for allowing
37:46 - users to get used to a new feature or
37:49 - product in this way one will be able to
37:52 - absorve the real treatment effect by
37:54 - giving more time to returning users to
37:56 - cool down from an initial positive
37:58 - reaction or a spike of Interest due to a
38:01 - change that was introduced as part of a
38:03 - treatment this should help to avoid
38:05 - novelty effect and is better predictive
38:07 - value for the test outcome however the
38:10 - longer the test period the larger is the
38:13 - likelihood of external effect impacting
38:15 - the reaction of the users and possibly
38:18 - contaminating the test results this is
38:21 - what we call maturation effect and
38:23 - therefore running the AP test for too
38:25 - short amount of time or too long amount
38:27 - of time is not recommended as it's a
38:30 - very involved topic we can talk for
38:32 - hours about this part of the ab test and
38:35 - also a topic that is asked a lot during
38:37 - the data science and product scientist
38:39 - interviews therefore I highly suggest
38:41 - you to check out this book about AB
38:43 - testing which is a Hands-On tutorial
38:45 - about everything you need to know about
38:47 - AB testing as well as check out the
38:49 - interview preparation guide in this
38:51 - section that contains 30 most popular AB
38:54 - testing related questions you can expect
38:56 - during your data science interviews so
38:58 - stay tuned and in the next couple of
39:00 - lectures we will cover the next stages
39:02 - of AB testing process if you are looking
39:05 - for one place to learn everything about
39:08 - AB testing without unnecessary
39:10 - difficulties but also with a good
39:13 - statistical and da Science Background
39:16 - then make sure to check out the AB
39:18 - testing course at lunch. a so if you
39:21 - want to learn all this background
39:23 - information including what is
39:24 - statistical significance what is AB
39:26 - testing how can AB testing be done and
39:29 - you want to have this endtoend AB
39:31 - testing course then make sure to check
39:34 - the AB testing for data science course
39:37 - at
39:38 - l. that's the only course that is
39:40 - available at the moment on the internet
39:43 - that covers the most fundamental concept
39:46 - of a testing including the theory and
39:48 - the implementation in Python without
39:51 - know the extra details and right going
39:54 - straight to the point in order to help
39:56 - you to Kickstart your Journey with AB
39:58 - [Music]
40:00 - Tes the resource that I would suggest
40:02 - you to keep by the hand is the blog
40:05 - called complete guide toab testing
40:06 - design implementation and pitfalls which
40:09 - is part of the Hands-On tutorials of the
40:11 - towards data science so in here and
40:13 - specifically this part where we are
40:15 - discussing the two sample that test I
40:17 - would suggest you to go through it as we
40:19 - are going to conduct this two sample Z
40:22 - test as part of our Python and we are
40:24 - going to learn how to implement this in
40:26 - Python in this book you can learn
40:28 - everything out there that you need to
40:30 - know about AV testing including
40:32 - different uh pits include of Av testing
40:35 - the process behind it how you can
40:37 - conduct the ab test end to end how you
40:39 - can calculate a sample size how you can
40:41 - choose a test the primary metric
40:43 - definitions different statistical test
40:46 - that you can use including the Ki Square
40:47 - test the two sample Z test and two
40:49 - sample T Test so given that as part of
40:52 - the lectures of the um AB testing and
40:55 - specifically lecture number five we have
40:57 - already discussed the two sample T Test
41:00 - and how to implement it I thought it
41:02 - would be more useful for you to know how
41:03 - to implement the two sample Z test such
41:05 - that you know both of them and you know
41:07 - their theory behind it and also how to
41:10 - implement them and finally if you are
41:12 - wondering how you can Implement them in
41:14 - Python then head towards my uh blog uh
41:17 - in the medium as well as my GitHub
41:19 - repository that I will post in the
41:20 - resource section where you can find all
41:22 - the different statistical tests you can
41:24 - use for analyzing your ab test results
41:27 - including the two sample T Test two
41:29 - sample Z test K Square test and much
41:31 - more so without further Ado let's get
41:33 - started with our
41:35 - demo so uh as you can see here I'm
41:38 - generating the data myself assuming that
41:41 - uh the uh primary metric follows bomal
41:43 - distribution so the output is in the
41:46 - form of zeros and ones because we are
41:48 - looking into the click event and click
41:50 - can be either zero or one and then I'm
41:53 - using here the binomial distribution to
41:55 - randomly sample from it and in case of
41:57 - the experimental version I'm using a
41:59 - probability of success equal to 0.4 and
42:02 - in case of the control version I'm using
42:03 - a probability of success equal to 0.2
42:06 - because I want to have a quiet
42:08 - difference between the two groups and
42:10 - then later on we can also adjust this
42:12 - and we can change the difference to see
42:14 - how our it has behaves so um I'll assume
42:18 - that um the uh at the end of the uh data
42:21 - generation process we have a data that
42:24 - is similar to the form that you will get
42:26 - from the uh engine engers once they uh
42:28 - finish up collecting all the data from
42:30 - your customers and I will also assume
42:32 - that the Integrity of theab test is held
42:36 - which means that the observations who
42:38 - were in the control group they only saw
42:40 - the control version of the product and
42:42 - observations who were in the
42:43 - experimental group they only saw the
42:44 - experimental version of the product and
42:47 - let's actually go ahead and see how the
42:49 - data looks
42:51 - like so so as you can see here we are
42:53 - generating our data so the data is in
42:56 - this format so you can see that we have
42:57 - an observation in total we have 20K
43:00 - observations because we have two
43:01 - different groups each with 10K
43:03 - observations and then the first col
43:05 - describes the click event so we will
43:07 - either have a click or we will have no
43:09 - click and the primary metric is in the
43:11 - form of a click so we are measuring the
43:13 - performance of the product both control
43:15 - and the experimental with the same
43:17 - metric which is whether there is a click
43:18 - event or no click event and the primary
43:20 - metric is in the form of a binary
43:22 - variable so we have either zeros or we
43:24 - have ones Whenever there is a click then
43:27 - the corresponding value is one whenever
43:28 - there is no click then the corresponding
43:30 - value is zero and then we have the
43:32 - corresponding Group which helps us to
43:34 - understand whether the observation
43:36 - belongs to the experimental group so X
43:38 - or the control group which is a uh Co so
43:42 - uh this is how the data looks like and
43:43 - this also what you can uh expect from uh
43:46 - data Engineers uh once the uh AB test is
43:49 - conducted so you have run your ab test
43:51 - and Engineers have collected data
43:53 - assuming that the data Integrity has
43:55 - been kept and also that there was no
43:57 - systematic error when collecting and
43:59 - measuring the performance of the uh
44:01 - control and the experimental versions of
44:03 - the product first thing that we are
44:05 - going to do is to estimate the P hat
44:08 - control and a p hat experimental and for
44:11 - that what we need to do first is to
44:13 - count the number of clicks per group so
44:16 - we saw earlier that we have this data
44:18 - that we generate ourselves consisting of
44:20 - 20 K rows where 10 belongs to the uh
44:24 - control group and the 10K belongs to the
44:26 - experimental group and each consists of
44:28 - this click variable and the group The
44:30 - Click variable is an indicator uh that
44:33 - says that the observation clicked on the
44:36 - uh page versus uh not clicked on the
44:38 - page so whenever there was a click we
44:40 - have here one whenever there was no
44:42 - click we have here zero and then we have
44:44 - the corresponding uh group such that we
44:46 - can use to group this data based on the
44:48 - control versus experimental group and
44:51 - that's exactly what we are going to do
44:53 - as the first step in our process so we
44:55 - are going to calculate the number of
44:58 - total clicks for control group and for
45:00 - the experimental
45:02 - group so here we are making use of the
45:04 - function Group by in order to group this
45:07 - data frame so this data frame based on
45:09 - the group and then we want to click uh
45:12 - the we want to get the uh click variable
45:15 - and we want to sum this variable because
45:18 - the variable is of a binary nature so we
45:20 - have ones and zeros if we do the sum we
45:23 - are basically counting the number of
45:25 - times we have the uh observation click
45:29 - equal to one so by summing a binary
45:32 - variable we are simply getting the
45:34 - number of ones in that variable and
45:36 - that's exactly what we are doing in this
45:38 - part and then what is remaining is to
45:41 - get the uh number of clicks from control
45:44 - group and number of clicks from the
45:46 - experimental group by using this
45:47 - function code look so we saw earlier
45:50 - when we were discussing the um accessing
45:52 - of observations in a pendis data frame
45:55 - that there is a difference between iog
45:57 - and loog and the reason why we are using
45:59 - here the loog is because the uh group uh
46:02 - data that we are getting in here it will
46:05 - provide us an output where the index is
46:08 - in the format of a string so let's
46:10 - actually go ahead and print that part
46:12 - because I think it's an important part
46:14 - to see how the data looks
46:16 - like and it also will make sense why I'm
46:19 - using here the look function to access
46:21 - the uh control groups number of clicks
46:23 - and the experimental groups number of
46:25 - clicks
46:32 - so this is the uh group data frame that
46:35 - we are getting as you can see we are
46:36 - getting here the group and here we are
46:38 - getting for the control uh index the
46:41 - number of clicks is equal
46:44 - 2,924 and for the experimental group
46:46 - it's equal to
46:49 - 5,7 so then the next thing what we need
46:52 - to do is actually access this value and
46:54 - for that we need to specify that we want
46:56 - to access the value corresponding to the
46:59 - index equal to control and this can be
47:02 - done by using this log function so you
47:05 - cannot use ilog or any other way of
47:07 - accessing this because the index is of
47:09 - string type and therefore we are using
47:11 - the log so let's actually also add some
47:14 - print statements to make our code more
47:18 - readable so this will then print the
47:20 - number of clicks per control group and
47:23 - per experimental group
47:27 - here we go so as you can see we are
47:29 - nicely accessing the correct values then
47:33 - the next step is to calculate the P had
47:36 - control and the P experimental so
47:39 - basically the estimate of the click
47:40 - probabilities of the control group and
47:42 - the experimental group respectively and
47:45 - for that we just need to take the uh
47:47 - number of clicks and we need to divide
47:49 - it to the number of observations for
47:51 - that
47:53 - group so it is this part let's go ahead
47:57 - and calculate those values so as you can
47:59 - see I'm taking the number of clicks that
48:01 - we just obtained and I'm dividing it to
48:03 - the number of observations that we have
48:05 - defined in the very
48:13 - beginning here we go so as you can see
48:16 - for the control group the uh click
48:18 - probability is equal to
48:20 - 020 and in case of the experimental
48:23 - group is equal to 0.5 so we see that
48:26 - there is a large difference between the
48:28 - quick probability for these two groups
48:31 - which is um a reflection of what we saw
48:34 - here because we generated the data such
48:37 - that the uh success probability for the
48:40 - experimental group is equal to
48:42 - 0.5 and the um for the control group is
48:45 - equal to 0.2 so we see these numbers
48:47 - reflecting also in here and the reason
48:50 - for that is because we have sampled our
48:52 - data large enough and we see that the um
48:55 - probability so the the mean of our
48:58 - sample um converges in a probability to
49:02 - the mean that we use and this is also
49:05 - the idea behind the low of large num
49:07 - something that we have also discussed as
49:09 - part of the fundamental to statistic
49:11 - section of this
49:12 - course so the next thing what we need to
49:15 - do is to compute the P poed hat or the
49:19 - uh estimate of the pulled success
49:22 - probability and we saw uh when we were
49:25 - discussing the theory behind it that
49:26 - it's equal to the sum of the clicks for
49:29 - both control and experimental group
49:31 - divided to the total number of
49:32 - observations in both control and the
49:34 - experimental group so
49:37 - basically the P pulled head is equal to
49:42 - xcore control plus xcore experimental
49:46 - ided to the ncore Control Plus ncore
49:54 - experimental then the next thing we need
49:56 - to do is to compute a pulled variance
49:59 - and we just so that the pulled variance
50:01 - can be calculated by taking the pulled
50:04 - uh estimate for the click probability so
50:06 - this p p and then multiply by one minus
50:10 - p p head and then multiply by the
50:12 - inverses of the uh observ number of
50:15 - observations in each of the groups and
50:17 - there sum so 1 / 2 N Control Plus 1/ to
50:20 - an experimental so it can be calculated
50:24 - as follows
50:27 - so pulled variance then is equal to P
50:29 - ped head multip by 1 minus P ped head
50:33 - multip by 1 / to n control + 1 / to n
50:38 - experimental let's also add some print
50:46 - statements here we
50:49 - go and then the next step is to
50:51 - calculate the standard error so the
50:53 - standard error is the square root of the
50:55 - pulled variance so quite straightforward
50:58 - and here we are going to make use of the
51:00 - npy function so the SE is equal to npy
51:04 - Dot and a square roof is simply uh
51:07 - calculated by using the function sqrt
51:10 - which stands for square roof and then
51:12 - here we need to mention the pulled
51:14 - variance let's also add the print
51:17 - statement explaining the uh the code and
51:21 - this really can help your reviewer the
51:22 - code reviewer to understand what you are
51:24 - doing
51:27 - okay so now we have also the standard
51:28 - eror and now we are ready to calculate
51:31 - our test statistics so we saw that the
51:33 - test statistics is equal to the P
51:36 - control head minus P experimental head
51:38 - divided to the standard error and that's
51:41 - exactly what we are going to implement
51:43 - in
51:44 - here so as you can see the test
51:47 - statistics is equal to P control head
51:49 - minus P experimental head divided 2D SC
51:52 - so standard
51:53 - error and then finally what we need to
51:56 - do is to compute the Z critical value
51:58 - the P value and the confidence interval
52:01 - but for doing that we need to assume the
52:04 - significance level so usually this is
52:06 - done before conducting the test but here
52:08 - I'm assuming that before conducting the
52:10 - test there was a power analysis and as
52:12 - part of that we have decided that the
52:14 - statistical significance level is equal
52:16 - to
52:17 - 5% so let's add that here so Alpha is
52:19 - equal to 0.05 therefore we are going to
52:22 - use this specific Alpha so 5% in order
52:25 - to calculate our critical value coming
52:27 - from the normal table and to do this
52:29 - there are uh various options so one way
52:32 - of doing that is to hard code the value
52:34 - which I would not recommend but it is
52:35 - definitely uh an easy way to go if you
52:38 - um haven't used uh the python libraries
52:41 - to automize this process but here I will
52:44 - provide you the code and I will also
52:45 - tell you how you can use the scipi norm
52:49 - um function in order to calculate the
52:51 - critical volume and I think keeping the
52:53 - code as general as possible will help
52:55 - you in the term to because it can be
52:58 - that this time you're calculating the
52:59 - critical value corresponding to Alpha is
53:01 - equal to 0.05 but maybe next time you
53:03 - want to calculate the critical value
53:06 - when your Alpha is equal to 1% so you're
53:09 - interested in the uh case when your type
53:11 - one probability is equal to 1% so for
53:14 - those cases uh you want to keep your
53:17 - code as general as possible such that by
53:19 - changing your uh variable let's say
53:21 - Alpha you don't need to go each time and
53:24 - then in the chat GPT look for the a
53:26 - corresponding uh value coming from the
53:28 - standard normal table so for this what
53:32 - we are going to use is the norm function
53:35 - so the norm function come from the CPI
53:37 - stats library for that we need to import
53:39 - from ci. stats the norm function which
53:42 - stand for the normal distribution so in
53:44 - here what we need to use is the function
53:47 - called ppf which is the uh percentage
53:49 - Point
53:52 - function so the norm done ppf function
53:54 - stands for the present Point function
53:56 - and it's usually known as the inverse
53:58 - cumulative distribution function or the
54:00 - CDF of the standard normal distribution
54:03 - and it takes as an input the probability
54:05 - value and it Returns the corresponding
54:07 - value on the xaxis of the CDF once you
54:10 - provide a p so here we are providing the
54:11 - P which is equal to 1 minus Alpha / to 2
54:14 - then this function calculates the X so
54:16 - the xaxis such that the probability of
54:18 - observing a volue less than or equal to
54:21 - two or 2 x in a standard normal
54:23 - distribution is equal to P so we have
54:26 - this inverse CDF and we have the xaxis
54:28 - and we have the y-axis on the y- axis we
54:30 - have the probabilities and on the x-axis
54:32 - we have the X values so here what we are
54:34 - basically doing is that we are providing
54:36 - the probability that we have which is
54:38 - equal to 1 minus Alpha / to 2 and we
54:40 - want to know the corresponding X valume
54:43 - therefore it's also called inverse
54:44 - humity distribution function and in this
54:47 - way we can calculate Z critical value
54:49 - which can help us to identify the place
54:51 - where we need to have our rejection
54:53 - region and so here is the uh rejection
54:56 - region of this test and as you can see
54:58 - we have two-sided test therefore we have
55:01 - also a two regions and whenever the um
55:04 - test statistics is larger than the
55:06 - critical value in the right hand side
55:08 - and it is smaller than the critical
55:09 - value from the left hand side then we
55:11 - are saying that we can reject the N
55:14 - hypothesis therefore it's also called
55:15 - the rejection
55:17 - region so uh once we calculate this set
55:20 - critical value we are ready to go to the
55:22 - next step but before that let's also add
55:24 - some statement print statement for
55:25 - readability
55:30 - here so the next step is to calculate a
55:33 - p volume and a p volue can be calculated
55:35 - by using the norm. SF function so the
55:38 - norm function comes once again from the
55:40 - scipi library and the SF stands for
55:43 - survival
55:47 - function the norm. SF function stands
55:50 - for a survival function and it stands
55:52 - for the complement of the CDF function
55:55 - so the cumulative distribution fun
55:56 - function of the standard normal
55:57 - distribution it calculates the
55:59 - probability of observing a value greater
56:01 - than a given
56:02 - threshold so in this case we want to
56:05 - calculate the uh probability that our
56:08 - test statistics will be smaller than
56:09 - equal the critical volume and as we saw
56:12 - that the standard normal distribution
56:13 - was symmetric here we are multiplying
56:16 - just one side of that probability by two
56:18 - in order to obtain our final value so
56:21 - here once we run this test we will
56:24 - finally get our P value and as you can
56:27 - see here the P value of the two sample
56:29 - that test that we got is equal to zero
56:32 - well now once we have the P value and
56:34 - also we know what is our Alpha we are
56:36 - ready to test for the statistical
56:37 - significance of our results so given
56:40 - that our P value is equal to zero and
56:42 - it's smaller than 0.05 so our Alpha we
56:45 - can state that the null hypothesis can
56:47 - be rejected and we can state that there
56:49 - is a statistically significant
56:51 - difference between our experimental
56:53 - version of the product and the control
56:55 - version of the product
56:57 - so this will help us to test for the
57:00 - statistical significance of our AB test
57:04 - however if you were for instance to have
57:06 - a different samples so let's say we
57:08 - would compute uh we would randomly
57:10 - sample from the binomial
57:14 - distribution so as you can see once we
57:16 - are getting the uh probability of the
57:18 - success the same for the two groups then
57:21 - the P value becomes large at least much
57:23 - larger than the alpha which means that
57:25 - we can no longer reject the ne
57:27 - hypothesis and we can no longer State
57:29 - there is a statistical evidence at the
57:30 - 5% statistical level that the control
57:33 - version is statistically significantly
57:35 - different from the experimental version
57:37 - and this uh verifies that everything
57:39 - that we have done here is correct so the
57:42 - ab test results analysis is accurate now
57:45 - the question is whether we um also have
57:48 - a practical significance once we pass
57:50 - the statistical significance test so
57:52 - let's move this back to what we had
57:54 - before so this is your
57:57 - .5 and once again the P value is just
58:00 - zero and let's go ahead and calculate
58:03 - our confidence interval such that we can
58:05 - test for the Practical significance and
58:07 - we can comment on the accuracy of the
58:09 - test and the general ability of our AB
58:12 - test so we saw that the confidence
58:14 - interval can be calculated as
58:16 - follows so we have the difference
58:19 - between the P had experimental and the P
58:22 - had control and then for the lower bound
58:24 - we need to uh subtract from this
58:26 - standard or multiply by Z critical value
58:29 - and then for the upper bound we need to
58:30 - do the same only with summing the
58:32 - standard multiply by Z critical volume
58:35 - so the difference here you might notice
58:37 - is this round function and the reason
58:39 - why I'm adding this is because I want to
58:41 - have nice numbers that will be rounded
58:43 - uh just three numbers after to decimal
58:45 - instead of having long uh floating
58:48 - numbers so once we go ahead and print
58:51 - this confidence interval we can also see
58:54 - the lower bound and upper bound in
58:57 - numbers here we go so as you can see we
59:00 - are getting a confidence info which is
59:02 - quite narrow so this is a suggestion
59:05 - that our AB test results are most likely
59:07 - accurate and that the Precision of our
59:10 - AB test is high and this is a good sign
59:13 - because then we can say that the ab test
59:15 - we have conducted in here is most likely
59:17 - generalizable to the entire
59:19 - population then the next question is
59:22 - okay do we have a practical significance
59:24 - or not and for that we do need the final
59:27 - assumption regarding the minimum
59:29 - detectable effect so let's say during
59:32 - the power analysis before conducting our
59:34 - AB test we got an mde which or let's
59:37 - actually call it Delta let's keep the
59:39 - Greek letters uh and the Delta let's say
59:41 - is equal to 3% so
59:45 - 0.03 well in this case we can notice
59:48 - that the Delta 0 03 so 3% is much lower
59:53 - than the lower bound of our confidence
59:55 - interval which is equal to 30% so
59:58 - 29.7% this means that in that case we
60:01 - would have said that there is a
60:02 - practical significance also but if the
60:05 - uh Delta would have been for instance
60:07 - the uh 0.31 so we have a 31% Delta then
60:12 - in that case the Delta is no longer
60:14 - smaller than the lower bound of our
60:16 - confidence interval and in that case we
60:18 - cannot say that our results are also
60:20 - practically significant so depending on
60:22 - the business and depending on the
60:24 - Assumption regarding the Del or the
60:26 - minum detectable effect we can then
60:28 - compare this to the lower bound of the
60:30 - confidence interval and we can State
60:32 - whether there is a practical
60:33 - significance or not in case there is a
60:35 - practical significance then we are good
60:38 - to go so we can say that we have a
60:39 - statistical significance we have a
60:41 - practical significance and we also have
60:43 - a narrow confidence interval which is a
60:45 - suggestion that our results are also
60:47 - generalizable and accurate so uh this
60:51 - completes our uh AB test results
60:53 - analysis and this is all that you need
60:55 - to do in order to have a valid and uh
60:58 - good quality AB test looking to elevate
61:01 - your data science or data analytics
61:03 - portfolio then you are in the right
61:05 - place with this AB testing and Trend
61:08 - case study you can showcase your AB
61:10 - testing and coding skills in one place
61:13 - I'm T Vasan data scientist and AI
61:16 - professional and I'm the co-founder of
61:18 - lunar Tech where we are making data
61:20 - science and AI accessible to everyone
61:23 - individuals businesses and
61:27 - institutions in this case study we are
61:29 - going to complete an endtoend case study
61:32 - with AB testing where we are going to
61:34 - test in a datadriven way whether it's
61:36 - worth to change one of our features in
61:39 - our ux design in the lunar text landing
61:41 - page this is a real life data science
61:45 - case study that you can conduct and you
61:47 - can put it on your resume in order to
61:50 - Showcase your experience in datadriven
61:52 - decision making where you will showcase
61:55 - your statistic skills experimentation
61:57 - skills with AB testing and your coding
62:00 - skills in Python using Library such as T
62:03 - models but also the pendas npy also metp
62:07 - lip and caburn we are going to start
62:10 - with the business objective of this case
62:13 - study then we are going to translate the
62:16 - business objective into a data science
62:18 - problem then we are going to start with
62:21 - the actual coding we are going to load
62:23 - libraries we are going to look into Data
62:25 - visual the data The Click data we are
62:28 - going to look into the motivation behind
62:30 - choosing that specific primary metric
62:32 - which is the clickr rate then we are
62:34 - going to talk about the statistical
62:36 - hypothesis for our AB testing I will
62:40 - also teach you step by step all the
62:42 - calculation starting from the
62:43 - calculation of the pulled estimate from
62:45 - the clickr rate and then a computation
62:48 - of the uh pulled variance the standard
62:51 - error but also the motivation behind
62:54 - choosing the searches statistical test
62:56 - that I will be using such as the two
62:58 - sample Z test and then how you can
63:01 - calculate the test statistics how you
63:04 - can calculate the P value of the test
63:06 - statistics and then use that with the
63:08 - statistical significance to test the uh
63:11 - statistical significance of your ab test
63:14 - after this we will also then compute the
63:16 - confidence interval comment on the
63:18 - general ability of the ab test and then
63:22 - at the end we will also test for the
63:24 - Practical significance of the ab test
63:28 - then we will conclude and we will wrap
63:29 - up and we will make a decision based on
63:32 - our data driven approach using the ab
63:35 - test to check whether it's worth it to
63:37 - change a feature in our ux design in the
63:40 - lunar text landing page so without
63:43 - further Ado let's get started so let's
63:46 - now start our case study in here I have
63:49 - in the left hand side this uh version of
63:52 - our landing page so which is our control
63:55 - vers version so to say the existing
63:57 - version where you can see that here we
63:59 - have start freet trial and here we got
64:02 - us our button secure free trial in the
64:06 - right hand side we got this new
64:07 - experimental version that we would like
64:09 - to have which is the Andro Now button so
64:12 - as we saw in the introduction what we
64:14 - are trying to understand is that whether
64:17 - our customers click more on the new
64:20 - version the experimental version versus
64:22 - the existing version the control version
64:25 - so uh um as of the day of uh loading
64:28 - this and uh conducting this case study
64:30 - our landing page uh has a secure free
64:33 - trial but what we wanted to test with
64:35 - our data is whether the uh enroll now is
64:40 - more engaging such that we can go from
64:42 - the secure free trial version to the
64:44 - enroll now version and uh here um for
64:49 - this specific case not only but also in
64:52 - general as we know from a testing is
64:55 - that when ever we got an existing
64:58 - algorithm or existing feature existing
65:01 - button then we are referring this group
65:04 - that we will um where we will expose
65:06 - this existing version of the product we
65:09 - are referring this as a control group so
65:11 - all the users to whom we will show the
65:14 - existing version of our landing page we
65:17 - will refer them as the uh control group
65:20 - participants and then we have the the
65:22 - right hand side our experimental version
65:25 - and our experimental users so the users
65:28 - our existing customers that are selected
65:30 - to be taken part um in our experimental
65:33 - group and in our experiment they will be
65:36 - then uh exposed to this new version of
65:39 - our landing page which contains this
65:41 - androll now button so our end goal in
65:44 - terms of the business as we saw in the
65:47 - introduction is to understand whether we
65:50 - should release the new button which will
65:53 - end up being high High engaging which
65:57 - means that we will have higher CTR or
65:59 - higher uh more uh clicks that will come
66:03 - from our user site which uh
66:05 - automatically means better business
66:07 - because we want to have highly engaging
66:10 - users if they are clicking on this
66:12 - button it means that it interests them
66:15 - more compared to the control version and
66:18 - uh if something on our landing page in
66:20 - this case our call to action is more
66:22 - interesting and highly engaging it means
66:25 - means that we are doing something right
66:27 - and our users might uh either make use
66:31 - of our free products or uh purchase our
66:33 - products or um just stay engag with us
66:37 - to keep real Tech in mind and whenever
66:40 - there is someone who uh is interested in
66:43 - data science or AI um Solutions or
66:46 - products then they can at least refer
66:48 - their friends if they are just clicking
66:50 - to understand and to learn more about
66:52 - our products that's also possibility so
66:56 - from a business perspective we therefore
66:59 - are using here as our primary
67:03 - metric uh our click through rate the CTR
67:06 - of this specific button which in our
67:08 - control version is the secure free trial
67:11 - and in our experimental version is the
67:13 - enroll now and what we want to
67:15 - understand is that whether this new
67:17 - button will end up having higher CTR or
67:20 - not because higher CTR from the
67:23 - technical perspective will translate to
67:26 - higher engagement from the business
67:28 - perspective so here we are making this
67:30 - translation from business versus
67:33 - technical um when it comes to AB testing
67:36 - we can have different sorts of primary
67:37 - metrics we can have a clickr rate as a
67:40 - primary metric we can have a conversion
67:42 - rate as a primary metric or any other
67:46 - primary metric what we want to have as
67:49 - our metric that will work as the single
67:53 - measure that will will compare our dra
67:55 - an experimental group to understand
67:57 - which version performs better is first
68:00 - to understand what this definition of
68:01 - better is and how that translates back
68:03 - to the business because if the
68:06 - engagement is what we are referring as
68:09 - Better Business for some reason and I
68:13 - will explain you in a bit why we think
68:14 - the engagement in this case is what we
68:16 - what matter for us at ler Tech then it
68:19 - means that clickr rate can be used as a
68:21 - primary metric this is just a universal
68:24 - metric that has been used across um
68:27 - different web applications search
68:29 - engines recommender systems and many
68:31 - other digital products to understand
68:33 - whether the engagement of that specific
68:35 - algorithm feature web design whether
68:38 - that is better or not and in this case
68:41 - in this specific case study we are also
68:43 - going to use the CTR because we are
68:45 - interested in the engagement so at luner
68:48 - Tech we really care about the engagement
68:51 - um with our users and we want our users
68:56 - to make use of our products but uh
68:58 - ultimately to engage with us because if
69:00 - they engage with us it means that our
69:03 - products are being seen our uh landing
69:06 - page is being visited and the user is
69:08 - actually interested to click on that
69:10 - button and then the action point and
69:13 - then to start either free trial or to
69:16 - enroll to see what is going on because
69:18 - all these are signs of Interest coming
69:20 - from the user side and in the control
69:24 - version as our click to action is to
69:27 - secure a free trial which directly uh
69:30 - lends the user to our free trial to our
69:34 - ultimate data science boot camp but
69:36 - given that we are expanding which means
69:38 - that we are now offering more courses we
69:40 - are offering free products and also we
69:42 - have uh Enterprise clients uh we have
69:45 - businesses as clients who want data
69:47 - science and AI Solutions and who want
69:49 - corporate training therefore we want to
69:51 - go from this Niche uh version of a
69:54 - landing page so secure free trial to
69:56 - enroll now because we already have a lot
69:59 - of Engagement in terms of the free trial
70:01 - we want to make it more General so
70:03 - that's the business perspective and on
70:05 - the other hand we also want to change
70:09 - beside of changing this um main um call
70:13 - Action we want to make it generalized
70:16 - and at the same time we want to see
70:18 - whether this generalized version will
70:20 - end up leading us um a higher engagement
70:24 - not only in terms of of the other
70:25 - products but also for the tree trial
70:27 - free trial itself because we always are
70:30 - looking for educating people and
70:33 - providing this free trial such that they
70:34 - can make use of our Flagship product
70:37 - which is the the ultimate data science
70:38 - boot camp so now when we understand why
70:41 - we care about the engagement here at ler
70:43 - Tech and we understand why we want to
70:46 - check whether this new button in our ux
70:48 - design will end up increasing the
70:51 - engagement or not we can now make this
70:54 - translation back to the data science
70:56 - terms because we know now from the
70:58 - business perspective All We Care is to
71:00 - understand whether this experimental
71:02 - version of the product is performing
71:04 - better or not but then this means that
71:07 - we need to conduct an AV test and we
71:09 - need to understand whether the ideas
71:12 - that we got and the speculation that the
71:15 - enroll now more General Button as so
71:17 - call you action will be better than the
71:19 - secure free trial version whether this
71:22 - is actually true or not from the uh
71:24 - customer perspective because if we want
71:26 - to call us a datadriven company we
71:29 - cannot just base our conclusions and our
71:32 - decisions for our products or for just
71:34 - in general for our product road map
71:36 - based on Intuition or logic we want this
71:39 - to be data driven which means that the
71:41 - customers are at the first place we are
71:44 - customer driven and our customers need
71:46 - to tell us whether the new um button is
71:50 - better or not and here we have conducted
71:53 - conducted an Navy test and um here I
71:57 - won't be using the real data I will be
71:59 - using the uh proxy data or simulated
72:02 - data that I generated myself and um this
72:06 - one contains the similar structure and
72:09 - this uh the same um idea of the data
72:12 - that we got when we were conducting our
72:15 - IB test and collecting this data and
72:17 - what is our business uh hypothesis in
72:20 - our business hypothesis we can say that
72:23 - we have at least temp % increase in our
72:26 - click through rate so 10% higher
72:28 - engagement when we have our enroll Now
72:31 - versus the secure free trial version of
72:34 - the product so this is our business
72:36 - hypothesis which means that our enroll
72:39 - now CTR so click through rate of the
72:41 - enroll Now button will result in at
72:43 - least 10% higher
72:45 - CTR than the secure free trial so there
72:48 - exists uh 10% at least 10% difference in
72:52 - terms of the engagement when we compare
72:53 - this new version of the product versus
72:56 - the old version of this new uh
72:59 - button and when we translate this back
73:01 - to statistical hypothesis we can say
73:03 - that under the N hypothesis we are
73:05 - saying that there is no statistically
73:07 - significant difference between the um
73:10 - control p and then P experimental which
73:13 - means the um um probability clickr
73:17 - probability clickr rate for control
73:20 - group versus experimental group so under
73:22 - AG n the null hypothesis we are stating
73:25 - what we ideally want to reject we are
73:27 - saying there is no difference between
73:29 - the experimental and control group CTR
73:32 - and under the alternative hypothesis so
73:34 - the H1 we are saying no uh we do have a
73:38 - difference which means that the uh
73:40 - control
73:42 - groups CTR is different from the control
73:45 - experimentals group CTR and one key part
73:50 - here is to mention that they are not
73:52 - just different but they are
73:54 - statistically significantly different so
73:57 - uh when it comes to starting the case
73:59 - study first things first is to load the
74:02 - libraries in this case study we are
74:04 - going to use a numpy we are going to use
74:06 - a pendas as usual for any sort of data
74:09 - analytics data size um case studies you
74:12 - always need those two usually pendis
74:15 - will be needed for our data wrangling to
74:17 - load data process the data visualize it
74:21 - nonp will be used to uh work with
74:24 - different arrays and part of the data
74:26 - then we are going to use the ci. stat uh
74:29 - model and from that we will import the
74:31 - norm function later on um we will see
74:34 - that we are using this in order to
74:37 - visualize this um uh rejection region
74:40 - that we get from for our test to
74:42 - understand whether we need to reject our
74:44 - null hypothesis or not then in this case
74:47 - study we also want to visualize our
74:49 - results and visualize our data for which
74:52 - we are going to need our visualization
74:54 - Library from python which are the curn
74:56 - and the med plot L let's look into our
74:59 - data so what we have in our data we have
75:02 - four different columns and of course
75:03 - this is a filter data that contains the
75:07 - information that we need but in general
75:10 - you can have a larger database you can
75:12 - have more sorts of um um matric matrices
75:16 - and uh different other Matrix but for
75:19 - conducting your ab test the pure AB test
75:22 - you actually need only the following
75:24 - information
75:25 - so you need your user ID to understand
75:29 - uh what are the user you are dealing
75:31 - with so it's the user one user two user
75:34 - 10 it can be that you have other way of
75:36 - referring to your users and uh those can
75:39 - be for instance this long strings that
75:41 - we use to refer to our user but given
75:44 - that our case is a simple one our case
75:48 - study we have just a user ID and this
75:51 - user ID is just a integers that go from
75:55 - one till uh until the end of our uh data
75:59 - and here we got in total 20,000 users
76:03 - therefore this number user ID goes to um
76:07 - 20,000 and those 20,000 um are all part
76:12 - of the user group which means that they
76:14 - are all users and they contain both the
76:17 - experimental and control users then we
76:21 - have our uh click variable and this
76:24 - click variable it's a binary variable
76:27 - which can be uh either one or zero where
76:30 - one refers that the user has clicked on
76:34 - the button and zero means the user
76:38 - didn't click on the button this is our
76:40 - primary metric for our AB test then we
76:44 - have the group reference which is this
76:46 - um string variable and this string
76:49 - variable helps us to understand whether
76:51 - the user comes from the experimental
76:53 - group or from the control group so this
76:55 - can contain only two different values
76:57 - two strings and it is X referring to the
77:01 - experimental and control referring to
77:03 - the uh control group if you can see here
77:06 - we got just this three letters X
77:08 - referring to the experimental group and
77:11 - then if we go in here because we have
77:13 - first the experimental and then the
77:15 - control ones you can see that here we
77:17 - got the uh control group now we have
77:20 - also some time stamp which is not
77:22 - something relevant so we'll be skipping
77:24 - that for now um given that this uh data
77:28 - that we have here it's not the actual
77:31 - data our data but it's a synthetic one
77:34 - but similar in terms of its structures
77:36 - in terms of the uh nature of variables
77:40 - and you can Implement exactly the same
77:42 - steps when you have your data and you
77:44 - are getting it from your ab test and
77:46 - then you are conducting your ab test uh
77:49 - case study so in here what we are going
77:52 - to make use of the most is our click
77:54 - click variable and the group variable
77:56 - because we want to find out per group
77:59 - what are the users that have clicked on
78:02 - the uh button and to be more specific we
78:05 - are looking for these averages so we are
78:08 - not so much interested that that
78:10 - specific user from that specific group
78:12 - has clicked on the product or not that's
78:14 - something that we can explore later but
78:17 - for now we are interested on the more
78:19 - high level so what is this uh
78:21 - percentages what is the click
78:23 - probability or click the true rate perir
78:25 - group and here we got groups of
78:27 - experimental and control as it should be
78:30 - in any source of ab test so once we have
78:34 - conducted our AB test then I will also
78:36 - provide you more insights on what you
78:38 - can do with your data especially with
78:40 - this user ID to learn more about uh the
78:44 - idea behind these different decisions or
78:48 - whether your ab test is different per
78:51 - group but the idea is that this AB test
78:54 - that we are conducting by following all
78:57 - the steps and by ensuring that the uh
79:00 - pitfalls are avoided that we are making
79:02 - a decision that um represents the entire
79:06 - population so we are using a sample that
79:09 - is large enough for us to make a
79:11 - decision for our product and for our
79:14 - business that will be generalized and
79:16 - will be a representation and
79:18 - representative when we apply this
79:20 - decision on our
79:22 - population so let me close this part
79:26 - because we no longer need this and let's
79:29 - go ahead and load this data so here I'm
79:31 - using the pendus library and the common
79:35 - uh approvation of PD and I'm saying pd.
79:39 - read CSV and then I'm here referring to
79:42 - the name of the data that contains my
79:46 - click data and here you can see that dat
79:48 - that that data is here so abore testore
79:53 - click uncore data that's here PV and I
79:56 - will be providing you this data because
79:58 - you won't have this in your own Google
80:00 - collab you will have the link to this
80:02 - Google clab and I'll provide you also
80:04 - the data such that you can put that data
80:07 - you can download it first from my source
80:09 - and then load it in here by using this
80:14 - specific button in here and by doing
80:17 - that you can then go to that specific
80:19 - folder where you downloaded the data and
80:21 - then you will have also this uh
80:23 - corresponding CSV file in your folders
80:27 - so once you have that then you will uh
80:29 - smoothly run this code and uh here I'm
80:34 - loading that data and putting under the
80:36 - name of DF abore test so basically the
80:39 - data frame containing my ab test click
80:41 - data what I want you to do is to
80:44 - Showcase you how the data looks like so
80:47 - here you will see the header given that
80:49 - here I haven't provided any argument it
80:51 - just looks at the top five elements so
80:54 - the top five rows and here I got only
80:58 - the first five users from the
81:00 - experimental group I see that some of
81:02 - them have clicked some of them didn't
81:03 - click and the corresponding user ID and
81:06 - the Tim stamp uh that they um done the
81:09 - click
81:11 - action then um when we look at
81:16 - the describe function you can see here
81:20 - that this gives us more general idea uh
81:23 - of uh what the contains not so much what
81:26 - the top five rows just look like which
81:28 - is great in terms of to understand what
81:30 - kind of data you are dealing with with
81:32 - what kind of variable you have now you
81:35 - can see more the uh total uh picture so
81:38 - high level picture what kind of um data
81:40 - what amount of data you got so the
81:42 - descriptive statistics so here we can
81:45 - see that in total we got 20,000 of users
81:48 - included in this data so 20,000
81:51 - observations 20K rows and we have the
81:55 - mean for the user ID of course it it's
81:58 - not relevant the mean is
82:00 - 10,000 and um this is an interesting
82:03 - number so we see that um average click
82:06 - uh when we look at both user and control
82:09 - the experimental and control groups it
82:11 - is 40% so
82:14 - 0.40 uh 52 so 40.
82:18 - 52% however this is not what we are too
82:21 - much interested so this is not to be
82:25 - confused with the click through rate
82:27 - perir group what we are interested is
82:30 - the click through rate or the mean
82:32 - clickr um when it comes to the
82:35 - experimental group and the control group
82:39 - so then we have our standard deviation
82:42 - we see a high standard deviation which
82:45 - is understandable given that we have
82:47 - this uh large variation in our data we
82:51 - got a control group and experimental
82:53 - group and this Vari shows that we have a
82:55 - huge difference in the these different
82:58 - values uh when it comes to the click
83:00 - event and then we have the mean and the
83:03 - maxim which doesn't give us too much
83:04 - information because the click event so
83:07 - the click variable is a binary variable
83:09 - it contains the zeros and one so
83:11 - naturally the minimum will be the value
83:14 - zero because the click can take value
83:16 - zero and one and the largest one is of
83:18 - course one which means the maximum will
83:19 - be one and then for the rest the 25% so
83:23 - the first quantile the second quantile
83:24 - the 15% which SS the median or the third
83:27 - quantile the 75th percentile is not that
83:30 - much relevant so when it comes to the
83:32 - descriptive statistics for this kind of
83:34 - data especially if it's filtered is not
83:36 - super relevant but if you would have a
83:38 - larger data more matrices beside of
83:41 - Click which is your primary matric but
83:43 - you all have also measured some other
83:45 - Matrix which is
83:46 - recommendable then you would see more um
83:49 - values which would be interesting to
83:51 - look at so not only to look at the click
83:54 - rate but also to look at for instance
83:56 - the mean or maybe the median of
83:58 - conversion rate or the uh mean uh amount
84:02 - of time the average amount of time the
84:04 - user has spent on your landing page or
84:06 - how much time did that user end up
84:08 - spending before making that decision of
84:10 - a click those can be all very
84:12 - interesting Matrix to look into from the
84:14 - product uh data science perspective to
84:17 - understand the decision process and the
84:19 - channel and The Funnel of these clicks
84:22 - but for now for our study what we are
84:25 - purely interested is in our primary
84:28 - metric which is the click event so what
84:32 - we can also see in here is that we got
84:35 - um uh in our group um when it comes to
84:38 - the control group we got uh
84:42 - 1,989 users out of all uh control users
84:47 - that end up clicking versus the
84:49 - experimental group where we have
84:52 - 6,6 users who did click so do not
84:56 - confuse this with the total amount of
84:59 - users per group this amount is the um
85:04 - grouping of the uh data so using the
85:08 - group by and then group so we are
85:10 - grouping that data per group and we want
85:12 - to see per group what is the sum of this
85:15 - variable sum of the clicks and given
85:17 - that the click is a biner variable we
85:19 - know from basics of python that we are
85:22 - basically accounting the number of click
85:24 - events because if you got a binary
85:26 - variable containing zeros and ones if
85:29 - you do the sum of the clicks adding the
85:32 - zeros do not doesn't have any impact
85:34 - which means that um you end up just
85:37 - summing up all the ones to each other
85:40 - and then you end up getting the number
85:43 - of or the total amount of uh cases when
85:46 - this click variable is equal to one so
85:48 - in this case when there is a click event
85:50 - therefore we can see that per
85:52 - experimental group we got
85:56 - 6,16 uh users out of all the
85:58 - experimental users that end up clicking
86:01 - and then out of control group this
86:02 - amount is much lower so we end up having
86:05 - uh only
86:07 - 989 users clicking so let's now go ahead
86:10 - and visualize this data I want to
86:12 - showcase in a bar chart using this
86:15 - clicks what is the total number of
86:16 - clicks so I want to show the
86:18 - distribution of the clicks when it comes
86:21 - to um the uh click event pair group and
86:24 - here I want to uh see next to each other
86:30 - the experimental group and control group
86:33 - and as you can see here here we are
86:36 - getting our bar charts and the yellow
86:40 - corresponds to the no which means that
86:43 - there was no click versus the uh black
86:47 - corresponds to the yes which means there
86:48 - was a click so whenever you see this
86:51 - amount it means that that amount
86:53 - corresponds to no click no engagement
86:56 - from the user side and this is per group
86:59 - so this is what we are referring as a
87:01 - click distribution in our uh data in our
87:04 - experimental uh and control groups and
87:07 - the way that I generated this bar chart
87:10 - is by first creating this um uh list
87:13 - that will contain the colors that I want
87:16 - to assign to each of my groups and I'm
87:18 - saying zero corresponds to the yellow
87:20 - and one corresponds to Black which means
87:22 - that if my variable contains amount of
87:24 - zero in this case my click is equal to
87:26 - zero it means that I don't have a click
87:30 - so it's a no and this I want to
87:32 - visualize by yellow otherwise I have a
87:35 - black which means that um the um one
87:39 - corresponds to the case when we have um
87:42 - click and in this case we will get a
87:44 - black as you can see here the uh yes
87:48 - which means a click is um visualized by
87:51 - this black color and then what I'm doing
87:53 - is that I'm initializing this uh figure
87:56 - size by saying that I I want to have a
87:59 - figure size of 10 and six you you can
88:01 - also skip it but I I think it's always
88:03 - great to put the size of a figure to
88:05 - ensure that you are getting the size
88:07 - like you want it to be such lat you can
88:09 - also download or take a screenshot then
88:12 - we have this uh here I'm using as you
88:15 - can see a combination of the Met plot
88:17 - leap. pip plot Library as well as the uh
88:20 - cabbo because CBO has much nicer colors
88:23 - and here here I'm saying uh we are going
88:26 - to uh make use of the curn to um create
88:30 - um count plot because we are going to
88:34 - count and we are going to showcase the
88:36 - counts per group uh what is the number
88:40 - or the count of the clicks versus no
88:43 - clicks for a group called experimental
88:47 - and what is the number of um or the
88:49 - percentage of clicks versus no clicks
88:51 - when it comes to the group control and
88:54 - then here I'm specifying that the Hue
88:56 - should be on the click which means that
88:59 - we are looking at the click variable and
89:01 - we are going to use the data dfab test
89:04 - which means that we are going to look in
89:05 - this data from here we are going to
89:07 - select this specific variable called
89:09 - click and we are going to use this in
89:11 - order to group our data based on this
89:14 - group so you can see that we are doing
89:16 - the grouping on the variable called
89:18 - group so the argument is called x x is
89:21 - equal to group we're grouping our dfab
89:24 - has data on this group and we are going
89:25 - to do the count in our count plot based
89:28 - on this variable click basically what
89:30 - I'm saying here is that go and group our
89:34 - data dfap test based on Group which
89:37 - means that we will group based on
89:39 - experimental versus control and then I'm
89:41 - saying go and count the click events
89:45 - count pair group so pair experimental
89:48 - perir control group what is the number
89:50 - of times when we have a no so we have a
89:53 - zero and and what is the number of times
89:55 - when we have a yes or we have a one as a
89:58 - value for click variable and then as a
90:00 - pet I'm using my custom pet that I just
90:02 - created which should be in the form of a
90:04 - list as you can see in here if I would
90:07 - have here also my third group or fourth
90:09 - group then I of course need to extend
90:11 - this color palette because I need to
90:14 - have the same amount of colors as the
90:16 - number of groups pair my target variable
90:19 - in this case The Click has only two
90:21 - possible values 0 and one which means
90:23 - that I'm only or only specifying the two
90:25 - colors in my list so then we have the
90:29 - title of our plot always nice to add by
90:32 - and then we have our labels which means
90:35 - that I want to emphasize uh as my X
90:38 - label so here I want to have my group
90:40 - you can see here is my group because I
90:42 - will either have group experiment or
90:44 - control that's my variable on my xaxis
90:48 - and on my y AIS of course I have the the
90:51 - count so I'm counting the number of
90:54 - times I got uh the uh no click versus
90:58 - click event so here note that the um y
91:02 - AIS is in terms of this count so here
91:04 - you can see it's uh 8,000 here sa 7,000
91:09 - or 6,000 5,000 which means that we are
91:12 - talking about the numbers and the counts
91:15 - rather than
91:16 - percentages and this is important
91:18 - because um another thing that I'm also
91:21 - doing is that I'm going the extra Mile
91:24 - and I'm also adding beside of this
91:27 - counts on the top of each bar I I want
91:30 - to visualize and clarify what are the
91:32 - corresponding percentages it's always
91:35 - great to enhance your data visualization
91:39 - with some percentages percentages is
91:42 - easier for the uh person who follows
91:44 - your presentation to understand for
91:47 - instance if you got an experimental
91:48 - group and the the users is here 6,000
91:52 - and um 4,000 they might not quickly
91:55 - understand that you got for instance in
91:57 - total 10,000 of users and then 6,000 has
92:01 - then uh clicked and then 4,000 didn't
92:04 - click so um then the idea is that by
92:09 - adding these percentages we can then see
92:12 - that
92:14 - 61.2% has clicked in this experimental
92:18 - group and
92:20 - 38.8% has not clicked of course this a
92:23 - simulated data I specifically pick the
92:25 - extreme in such way that we can clearly
92:28 - see this difference in the clickr rates
92:31 - but um in the reality you can have a
92:33 - click-through rate of 10% up to 14%
92:35 - which is usually a good number if you
92:37 - have a clickthrough rate of 40% is great
92:39 - but it's really depend on underlying
92:41 - user base what kind of product you got
92:43 - how large is your user base because if
92:45 - you have very large user base then 10%
92:48 - can be a good clickr rate versus if you
92:51 - have a very small user base Maybe 61% is
92:55 - considered uh good or
92:58 - average so uh in here we have just a
93:02 - simulated data of course and I've have
93:04 - added these percentages uh by using the
93:08 - following code so I won't go too much
93:11 - into detail in here um uh feel free to
93:14 - check and see uh and if something
93:16 - doesn't make sense go back to our python
93:19 - for data science course that contains
93:21 - lot of information on the basics in
93:23 - Python
93:24 - but here just quickly what I'm doing is
93:26 - that I am uh calculating the percentages
93:29 - and I'm annotating the bars so I want to
93:32 - know what are these percentages which
93:33 - means that per group I want to take the
93:36 - total amount of clicks I want to
93:38 - understand what is number of Click event
93:40 - when the click variable is equal to one
93:42 - and what are the number of cases when
93:44 - there was no click from the user s which
93:46 - is what are the number of cases when the
93:47 - click variable is equal to zero and then
93:50 - I'm counting those amounts and then
93:52 - using the total amount to calculate the
93:54 - percentage for instance in this specific
93:57 - case I'm filtering the data for
93:58 - experimental group I'm looking at the
94:00 - total number of users for this group
94:02 - which is 10K and then I'm counting the
94:05 - number of times when out of this 10,000
94:07 - users the amount of users that end up
94:09 - clicking on that button which is the
94:12 - click is equal to one case and then I'm
94:15 - taking that number dividing it to the
94:17 - total number of users for this
94:19 - experimental group multiplying by 100 in
94:22 - order to get that in percentage
94:24 - and this is the calculation that you can
94:26 - see in here one thing that is important
94:29 - here is that here I'm using this um uh
94:34 - percentage um so for the current bar I'm
94:37 - saying U as a way to identify whether we
94:40 - are dealing with experimental or control
94:42 - group is by getting by looking into this
94:45 - uh p and uh this p in here is the
94:49 - basically the patches so in this case
94:52 - I'm basically saying if if I'm dealing
94:54 - with experimental group then go ahead
94:56 - and calculate what is this uh total
94:59 - amount of observations and then take
95:01 - what is the uh number of clicks and then
95:04 - divide the two numbers uh C multiply
95:07 - this with 100 and this will then give us
95:10 - the percentage and then I'm doing this
95:12 - for each of those groups so I'm doing it
95:15 - for this group I'm doing for this group
95:17 - and for this one and for this one so I
95:18 - got two groups but then within each
95:20 - group I got clicks and no clicks and I'm
95:23 - calculating these four different
95:24 - percentages and then I'm adding these
95:27 - percentages on the top of those bars so
95:29 - I not only want to have numbers repr
95:32 - presented in my visualizations but I
95:33 - also want to add this corresponding
95:36 - percentages at the top just for
95:38 - visualization purposes I wanted to put
95:40 - this out there because this can help
95:43 - your uh data visualization toolkit and
95:46 - it also will um make your audience from
95:49 - your presentations be more thankful to
95:51 - you when you are telling the story of
95:54 - your
95:55 - data so uh this is about the data that
95:58 - we have we see that uh 38.8% of our
96:01 - experimental group users have not
96:03 - clicked on the button versus the
96:06 - 61.2% have clicked on the button based
96:09 - on the simulated data and then uh in the
96:12 - control group we have a quite the
96:14 - opposite situation we got the majority
96:17 - of the users
96:19 - 80.1% not clicking on the button versus
96:22 - the remaining 19 .9% have actually
96:25 - clicked on that button so we got a huge
96:28 - difference a dis anounce when it comes
96:30 - to the experimental group and uh control
96:32 - group this kind of gives us an
96:34 - indication hey something is going on
96:37 - here we kind of uh have already um
96:41 - higher level intuition what the
96:44 - remaining analysis will look like um
96:47 - which is that there most likely will be
96:50 - a difference in their ctrs when it comes
96:53 - to the uh the um uh control versus
96:57 - experimental group and the corresponding
97:01 - buttons but uh hey let's continue that's
97:04 - the entire goal behind a testing is to
97:07 - ensure that our intuition our
97:09 - conclusions are all based on the data
97:12 - rather than on our intuition so what are
97:15 - the parameters that I'm using here for
97:18 - conducting our AB test when I was
97:21 - designing this AB test uh the first step
97:23 - step was to of course do all these
97:25 - different translations that we learn as
97:26 - part of our AB test course um conducting
97:29 - it properly which means coming up with
97:32 - this three different parameters when
97:35 - doing our power analysis and usually
97:38 - this should be done when you are
97:40 - collaborating also with your colleagues
97:42 - and uh with your product managers or
97:44 - your product people domain experts
97:47 - because they have um a lot of
97:49 - information on what it means to have um
97:53 - threshold that you need to pass in order
97:56 - to say that for instance this new
97:59 - version of your feature is different and
98:02 - is uh considerably uh different from the
98:05 - existing one and here um in order to for
98:09 - us to understand this uh and make these
98:11 - conclusions we need to come up with the
98:14 - three different parameters that can help
98:16 - us to properly conduct an AB test as we
98:18 - learned when we were looking into
98:20 - designing a proper AB test so so first
98:23 - we have our significance level the
98:26 - significance level or the alpha the
98:28 - Greek letter that we are using to refer
98:30 - to the significance level which is also
98:32 - the probability of the type one error
98:35 - and that amount we have chosen following
98:38 - the industry standard which is 5% given
98:41 - that we didn't have any uh previous
98:44 - information or specific reason to choose
98:46 - a different significance level so lower
98:49 - or higher we decided to go with the
98:51 - industry standard which is the 5%
98:54 - this means that we want to have um we
98:57 - want to compare our P value of our uh
99:01 - statistical test to this 5% and then say
99:04 - whether we have a statistically
99:06 - significant difference between the
99:08 - control and experimental group based on
99:10 - this 5% significance level and let's
99:12 - refresh our memory on this Alpha this
99:15 - Alpha uh or significance level is also
99:18 - the probability of type one error so
99:20 - this is the amount of error that we are
99:22 - comfortable making
99:23 - when we um reject the N hypothesis well
99:27 - the null hypothesis is actually uh true
99:32 - which means that we are detecting a
99:34 - difference between the experimental and
99:36 - control version while there is no
99:38 - difference and we are making that
99:40 - mistake and here we are saying that we
99:42 - are fine and we are comfortable with
99:45 - making this mistake at a maximum of 5%
99:48 - but higher than that it's not allowed we
99:50 - are not comfortable making uh error um
99:54 - higher than 5% then the next variable uh
99:57 - in this case the B uh or beta the
99:59 - probability of type two error which is
100:01 - the opposite of the type 1 error which
100:03 - is a false negative rate or the amount
100:05 - of time the um um proportion of time
100:10 - when we end up failing to reject the
100:14 - null hypothesis while null hypothesis is
100:16 - false and it should have been rejected
100:19 - then the 1 minus beta is actually power
100:21 - of the test so what is the amount of
100:23 - time we are correctly rejecting our null
100:26 - hypothesis and correctly stating that
100:28 - there is indeed a statistically
100:30 - significant difference between our
100:32 - experimental group and our control group
100:35 - so we have chosen for this the uh
100:37 - industry standard as well which is the
100:39 - 80% but given that for your results
100:42 - analysis in this case for conducting
100:44 - this case study that part of the power
100:46 - analysis is not relevant we use that
100:49 - when calculating our minimum sample size
100:51 - but we don't need that when conduct our
100:53 - results analysis therefore I'm not
100:56 - initializing that as part of this code
100:59 - so here I'm only providing to my program
101:02 - the values for my significance level
101:04 - which is 0.05 or this is the same as
101:08 - 5% and then the Delta which is the third
101:12 - parameter and this Delta is our minimum
101:15 - detectable effect so this a Greek letter
101:18 - Delta which is the minimum detectable
101:20 - effect helps us to understand whether
101:24 - beside of having this statistically
101:26 - significant difference whether this
101:28 - difference is large enough for us to say
101:31 - that we are comfortable making that
101:33 - business decision to launch this new
101:36 - button so it can be that when we are
101:39 - conducting an AB test we are finding out
101:42 - that the experimental group has indeed
101:45 - higher engagement than the uh control
101:48 - group and we are uh getting a small P or
101:52 - at least is smaller than the alpha and
101:54 - we are seeing that P is small than the
101:56 - alpha level which means that we can
101:57 - reject the null hypothesis and we can
101:59 - say that the CTR or the clickr rate of
102:03 - the experimental group is statistically
102:05 - significantly different from the control
102:07 - group at 5% significance level but we
102:09 - know from the theory of ab test that
102:12 - only that is not enough only statistical
102:15 - significance is not enough for the
102:16 - business to make that important decision
102:19 - to launch an algorithm or to launch a
102:21 - feature in this case to change change
102:23 - our landing page the button from the
102:26 - start free trial to the enroll now which
102:30 - means that we want to have enough users
102:33 - and we want to have enough difference
102:35 - large difference in our click through
102:37 - rate or enough users saying that we are
102:39 - more happy with this uh new version of
102:42 - the landing page for us to go and change
102:45 - our feature and what is the definition
102:48 - of enough what is the difference in the
102:50 - click through rate that we need to
102:52 - detect
102:54 - after we have detected the statistical
102:55 - significance in order for us to say that
102:58 - we also have a practical significant so
103:01 - practically we are also comfortable
103:04 - making that business decision and then
103:07 - launching this new feature and changing
103:09 - our landing page button and that is
103:11 - exactly what we have under our Delta
103:13 - this minimum detectable effect in this
103:16 - case we have chosen for Delta of
103:20 - 10% so you can see here 0.5
103:23 - this is 10% this means that our Delta or
103:26 - the MD the Min detectable effect is 10%
103:30 - this means that we are saying not only
103:33 - we should have a statistically
103:34 - significant difference between the
103:35 - experimental group and control group but
103:38 - also we need to have this difference to
103:41 - be at least 10% which means that we need
103:45 - to
103:46 - have detected that the experimental
103:50 - version of the landing page results in a
103:53 - at least 10% higher quick rate compared
103:56 - to the control version for us to go
103:59 - ahead and to launch this new version and
104:03 - deploy this new uh ux uh feature so this
104:08 - is really important because many people
104:11 - go and check for statistical
104:12 - significance so they do their Alpha and
104:15 - then check uh whether the P value is
104:17 - more the alpha and then say hey we have
104:19 - a statistically significant difference
104:21 - and then they are done with that but
104:23 - that's not correct after you have
104:26 - conducted your uh statistical
104:28 - significant analysis and you have
104:30 - detected that your uh experimental
104:32 - version has a statistically significant
104:35 - different um CTR than the control
104:39 - version at your Alpha significance level
104:43 - the next thing you need to do is to
104:45 - ensure that you also have a practical
104:47 - significance beside of the statistical
104:49 - significance and this practical
104:51 - significance you can detect
104:53 - and you can check when you use your mde
104:56 - or your Delta and you compare it to your
104:59 - confidence interval that you have
105:01 - calculated something that we have also
105:03 - learned as part of the theory of
105:05 - conducting a proper AB test but once we
105:08 - come to that point so after we check for
105:10 - our statistical significance I will also
105:12 - explain how exactly uh we will need to
105:15 - do this check and at the same time we
105:17 - will also be refreshing our theory on
105:19 - the Practical significance so let's now
105:22 - goad head and calculate the total number
105:25 - of clicks per group by summing up these
105:27 - clicks and I also want to calculate and
105:31 - group by this amounts just to Showcase
105:34 - how you can do that on your own so here
105:36 - what I'm doing is that I'm taking my ab
105:38 - test data I'm grouping by by group group
105:43 - is the uh variable that contains the
105:45 - reference where we are dealing with
105:47 - experimental group or control group and
105:49 - as you know from our uh python series
105:52 - and demos python for data science course
105:55 - that uh whenever we want to group that
105:57 - data a pend this data frame first we
105:59 - need to say pendas data frame name.
106:02 - Group by Within parenthesis the variable
106:04 - that we are using to do the grouping
106:06 - which is in this case group and then
106:08 - within Square braces I want to emphasize
106:11 - and put the name of a variable that I
106:13 - want to um apply operations on so I want
106:17 - to group my data on the group variable
106:20 - and I want to count the number of times
106:24 - I have a click in my control group and
106:26 - in my experimental group this will be my
106:29 - X control and X experimental variables
106:32 - so X control will then compute contain
106:36 - information about number of Clicks in my
106:38 - control group and then each experimental
106:40 - will contain the number of Clicks in my
106:42 - experimental group and given that um I
106:45 - want to refer to the name of that uh
106:50 - Group after I did my
106:53 - grouping so I am getting this kind of
106:57 - this shape of data frame of course I
107:00 - then need to uh use my do log function
107:04 - in order to properly call that amount so
107:07 - to understand what is this amount
107:09 - corresponding to this index and what is
107:10 - this amount corresponding to this index
107:12 - and given that my index is in strings
107:14 - I'm then using here mylog function
107:17 - something that we also learned as part
107:18 - of our python for data science course so
107:22 - here is basically the printing just
107:25 - writing nicely what are the results
107:28 - which means that we are counting that
107:30 - the let me count again that the uh
107:33 - number of uh clicks for my control group
107:37 - is
107:41 - 1,989 so you can see that it
107:45 - is want to double check and see what we
107:48 - got yes so we got same number so we are
107:50 - dealing with the same data set just to
107:52 - to make sure and here the number of
107:54 - clicks for experimental group is equal
107:56 - to 6K and 116 so
108:00 - 6,116
108:01 - clicks so then we are calculating the uh
108:04 - pulled estimates for the clicks per
108:07 - group let me quickly fix this typo so
108:11 - calculating the uh pulled estimate for
108:14 - the clicks perir Group which means the P
108:17 - estimate for the experimental group and
108:20 - for the uh control group so let me
108:22 - quickly at here how I can calculate the
108:26 - uh total cases when we got uh
108:30 - experimental group users so what is the
108:32 - number of users in the experimental
108:33 - group and what is the number of users in
108:35 - the uh control group so here what I want
108:38 - to do is that I want to say that the
108:41 - group The DF test group should be equal
108:45 - to
108:47 - expermental and this of
108:50 - course should be my filter and I want to
108:54 - count
108:55 - this and let me quickly copy this I saw
108:59 - that is already under the control so
109:01 - here I'm changing to the control and
109:04 - this will need to give me the number of
109:09 - users in each of these groups too so
109:11 - number of users in control and number of
109:15 - users in Click and here I will simply
109:18 - check
109:19 - this so I will print then the number of
109:22 - user users per group and at the same
109:25 - time I will also click the number of
109:26 - clicks per
109:28 - group there we go so now when we have
109:32 - done this what we are ready to do is to
109:35 - go ahead and calculate the F estimate
109:38 - for clicks perir Group which means pair
109:41 - control group and perir experimental
109:43 - group for that what we need to do is to
109:46 - take the number of clicks of the control
109:49 - group divide to the number of all users
109:51 - for control group as you can see in here
109:53 - x control / to n control and we are
109:56 - referring to this variable as P control
109:59 - head because we know that the estimate
110:02 - of this click probability um is always
110:05 - with a hat it's just the way that we
110:07 - reference it in um statistics and in ay
110:11 - testing so this is the estimate
110:13 - something that we are estimating
110:15 - therefore we are saying hat and then we
110:18 - have the same for experimental group
110:20 - which means that the estimate of the
110:21 - experimental groups uh click probability
110:24 - is equal to X exp and then divide it to
110:26 - n x then um in order to calculate the uh
110:31 - pulled estimate or uh pulled click
110:34 - probability which means the value that
110:37 - will describe of the uh control group
110:39 - and experimental group we need to follow
110:42 - this formula which means that we are
110:44 - taking the ex control we are adding to
110:47 - that X control the X experimental this
110:49 - is our nominator of our
110:53 - uh value and then we are dividing this
110:56 - to the uh sum of the sizes of each of
111:00 - those groups which is n control and N
111:04 - experimental so this is the common
111:06 - formula of the pulled estimate uh when
111:10 - it comes to this type of experimentation
111:12 - when you are dealing with um primary uh
111:15 - metric that is in the form of zeros and
111:17 - ones and if you want to refresh your
111:20 - memory on this type of formulas then
111:22 - make sure to also check our AB testing
111:25 - course because in there we go in detail
111:28 - in this uh specific lesson of the uh AB
111:32 - test result results analysis we are
111:34 - looking into this uh all this formulas
111:37 - on how we can calculate the pulled
111:39 - estimate of this uh click
111:42 - probability so click probability but
111:46 - then we are calling
111:47 - it
111:51 - P click for
111:53 - probability and then what we
111:57 - got is this
112:00 - volum so that amount is then
112:05 - 040 this number should look familiar
112:07 - because this is then the mean that we
112:09 - saw when we were looking at the um uh
112:13 - descriptive statistics table if you can
112:16 - recall this
112:19 - table let me see this number
112:23 - so now basically we are then calculating
112:25 - this manually because we need a variable
112:28 - that will hold this uh value so it is
112:32 - simply summing up all the
112:34 - clicks for control group and
112:36 - experimental group to get the total
112:38 - number of clicks and we're dividing it
112:40 - to the total number of users so n
112:42 - Control Plus n experiment so now when we
112:46 - have this we are ready to also calculate
112:48 - what we are referring as a pulled
112:49 - variance also something that we have
112:51 - learned as part of this theory for AB
112:53 - testing so the pulled variance is equal
112:55 - to the pulled estimate of the clicks so
112:59 - P had something that we just
113:01 - calculated multiplied by one minus P
113:05 - head so the uh click event the estimate
113:08 - of the click probability multiplied by
113:10 - the estimate of no click and we know
113:13 - already this IDE of berola distribution
113:16 - that the variable that uh describes this
113:18 - process of clicks and no clicks follows
113:21 - kind of this idea of B distribution when
113:23 - we have a click and no click so we have
113:26 - probability of click and then we have
113:28 - probability of no click which is the one
113:30 - minus that click probability so that's
113:32 - the idea or the part of the formula that
113:35 - we are following as kind of an intuition
113:38 - and then this multiplied by 1 / to n
113:42 - control+ 1 / to n experimental so here
113:46 - I'm purely following the formula for the
113:48 - pulled variance if you want more details
113:50 - and explanations and sure to check the
113:52 - corresponding Theory lecture because we
113:55 - are going into details of each of those
113:58 - formulas and understanding why we
114:00 - calculate this um P variance and P
114:03 - estimates uh in this specific way and
114:05 - using these specific formulas so here by
114:09 - just follow following the uh formula I'm
114:12 - getting that the uh pull uh variance is
114:16 - this
114:17 - amount so this is in nutshell how I
114:20 - calculated my uh pull click probability
114:23 - and a pulled variance of that click
114:26 - event and we are going to need that in
114:29 - the next very important step which is
114:32 - calculating the standard error and
114:35 - calculating the test statistics because
114:38 - in this case what we are doing is that
114:41 - we are dealing with a case when the
114:44 - primary metric is in the form of zeros
114:47 - and one so we let's Now quickly talk
114:49 - about the uh choice of a statistical
114:53 - test be uh before conducting the actual
114:56 - calculation of standard error in the
114:58 - test statistics so here I went for the
115:00 - two samples at test and let me explain
115:03 - you why and what is the motivation
115:05 - because as we learned as part of the
115:07 - theory um whenever we have a primary
115:10 - metric that is in the form of an
115:12 - averages like we have now because we are
115:15 - using the P control head and P
115:17 - experimental head head so we have a
115:20 - primary metric that is the uh click
115:22 - through rate which is the average clicks
115:26 - per group so we have calculated the
115:28 - average click per experimental group and
115:31 - control group then the primary metric
115:35 - the form of it already dictates given
115:37 - that it's in averages that we need to
115:40 - look at uh either parametric test
115:43 - corresponding to this averages or
115:45 - non-parametric test corresponding to the
115:47 - um averages in this case I went for the
115:50 - parametri case because uh it has Better
115:52 - Properties if I have this information
115:55 - about the distribution of my data and
115:59 - why do I have this information and then
116:01 - this also dictates the uh choice of my
116:05 - um statistical test well I have a size
116:09 - of my sample which is over 100 and
116:12 - actually over 30 that's the threshold
116:14 - that we tend to use in statistics and in
116:16 - a testing in order to say whether we
116:19 - have a large size or large data or not
116:22 - if our sample is not large so it
116:25 - contains less than 30 users per group
116:28 - which happens as well then we say that
116:30 - we need to go for um statistical
116:34 - test uh that will be specific for this
116:38 - kind of cases because we can no longer
116:40 - make use of the uh statistical theorems
116:43 - like the central limit theorem which
116:45 - helps us to um uh to take the uh to the
116:49 - inference so to make use of the
116:50 - inferential statistics and make
116:52 - conclusions regarding the distribution
116:54 - of our population just having the sample
116:57 - and what do I mean by that so if my
116:59 - sample is larger than 30 like in this
117:02 - specific case I got 10,000 users per
117:04 - group so it is definitely larger than 30
117:08 - uh
117:09 - users then in that case I can say that
117:12 - by making use of the central limit
117:14 - theorem I can say that my sampling
117:18 - distribution is normally distributed and
117:21 - this is simply making use of the central
117:24 - limit theorem something that we have
117:27 - also learned when we were looking into
117:28 - this concept of inferential statistics
117:31 - as part of the fundamental statistics
117:33 - course uh course um in lunar Tech so
117:36 - this is a powerful theorem that we use
117:39 - in AB testing in order to make our life
117:42 - easier because when we have a sample
117:45 - that is larger than
117:48 - 30 for each of these groups then we can
117:51 - say that even if we don't know the
117:54 - actual distribution or the name of the
117:56 - distribution that our uh sample follows
118:00 - where it comes to the click um event so
118:03 - the random variable that describes this
118:05 - number of clicks or the average click
118:08 - true rate what is that um distribution
118:11 - exactly but given that we have that this
118:14 - uh size is large enough it's large than
118:16 - 30 users we can say that by making use
118:20 - of the central limit theorem we can say
118:22 - that the uh the uh sample distribution
118:27 - follows a normal distribution if given
118:30 - that the sample size is large enough and
118:34 - this helps us to say that well in that
118:36 - case it doesn't matter whether we make
118:38 - use of the two sample Z test or two
118:40 - sample T Test we can make use of either
118:44 - of these test in order to conduct our
118:47 - analysis and we had this specific
118:50 - template to make this choice easier uh
118:53 - in our AB test course at the loer tech
118:57 - where we were making all this decisions
119:00 - and saying if the SLE size is this we
119:02 - need to do this if the SLE size is this
119:04 - we need to do this and in this specific
119:06 - case following that exact structured and
119:08 - organized approach I ended up seeing
119:11 - that my sample size is large so it's
119:14 - larger than 30 so I can then make use of
119:17 - the central limit theorem I then know
119:19 - what is the random um what my random
119:21 - variable describing this click through
119:23 - rate um follows the kind of distribution
119:27 - in this case a normal distribution and
119:29 - then this means that whether I use a t
119:31 - test or Z test doesn't really matter I'm
119:33 - going to end up with the same
119:35 - conclusions therefore I will just go
119:37 - with the two sample set test simply
119:39 - because um it is just easier for me to
119:43 - do for
119:44 - example you can also go with the two
119:46 - sample T Test and you can even change
119:49 - this case study and tweak it and then
119:51 - make make it your own and put it on your
119:53 - resume in that way by making it more
119:55 - unique and that will be totally fine
119:57 - because you will see that you are going
119:59 - to end up with exactly the same
120:01 - conclusions as we do in this specific
120:03 - case study because if you have a large
120:05 - enough sample it won't matter whether
120:07 - you have a two sample Z test as your
120:09 - parametric test or the two sample T Test
120:13 - and um if you want to know why why this
120:16 - matters and all the different details
120:18 - statistical insights make sure to check
120:20 - the actual uh course dedicated to AB
120:23 - testing because there will be cover this
120:26 - all and you will then become a master in
120:28 - the field of AB testing now we know this
120:32 - uh decisions and the motivation behind
120:34 - choosing the uh two samples that test
120:37 - let's now go ahead and do the actual
120:39 - calculations so here we have a standard
120:41 - error which we calculate by taking the
120:44 - pulled variance and taking the square
120:46 - root of it and this is again using the
120:49 - idea of this formulas that we learned as
120:51 - part of the ab test so we are using this
120:54 - P variance taking the square root of
120:56 - this which gives us the standard error
120:59 - and the standard error as you can see in
121:01 - here is then equal to
121:05 - 0.69
121:07 - 29499 this
121:09 - amount there we calculate our test
121:11 - statistic for our two sample at test so
121:14 - the test statistic is equal to P control
121:17 - head minus P experimental heads divided
121:19 - to standard error so here uh you can now
121:22 - see the motivation behind not only
121:25 - Computing the P pulled head but really
121:27 - also the p uh control head and P
121:30 - experimental head and then I take the P
121:33 - control head and subtract the P
121:35 - experimental head and I divide it to the
121:37 - standard error to compute my test
121:40 - statistics once I did this as you can
121:43 - see this is this amount so test
121:45 - statistics for our two sample that test
121:48 - is this amount minus 5956
121:52 - around it then um we can also compute
121:56 - the critical value of our Z test which
121:59 - is uh by using this Norm function that
122:02 - we uh loaded in here from the
122:05 - S and this will help us to understand
122:08 - what is this value from our normal
122:11 - distribution table the standard normal
122:13 - distribution table uh where by making
122:16 - use of this table we
122:19 - identify what is this critical value
122:22 - that we need to have to uh create our
122:25 - rejection regions and to say whether we
122:27 - can uh reject our n hypothesis or not so
122:31 - to conduct our test we need to have a
122:34 - critical
122:35 - value for uh to which we will compare
122:38 - our test statistics and this critical
122:41 - value will be based simply on the
122:43 - standard normal distribution so this is
122:45 - this norm.
122:46 - ppf and then uh probability um uh
122:51 - function base basically uh the the
122:53 - probability function that comes from the
122:56 - normal distribution standard normal
122:57 - distribution and as you can see it is
122:59 - correspond specifically to this percent
123:02 - Point function which is the inverse of
123:04 - the cumulative distribution function so
123:07 - this based on the alpha / 2 so 1us Alpha
123:12 - / 2 is the argument that we need to put
123:14 - for our percent Point uh probability
123:17 - function and why divide it to two
123:21 - because we have a two sample test so
123:25 - because we have a two-sided two sample
123:27 - test sorry so if you want to understand
123:30 - this difference between uh two sample um
123:33 - test two-sided Test please check out the
123:36 - uh fundamentals to statistics course at
123:39 - ler Tech because we cover this uh Topic
123:42 - in detail and it's a very involved topic
123:44 - it contains many complex stst U from
123:47 - statistical point of view so I won't be
123:49 - spending in this case study too much
123:51 - time on that here I'm assuming that you
123:53 - know this formula already but if you
123:56 - don't and if you quickly need to do your
123:58 - case study May testing feel free just
124:00 - just to copy this line which basically
124:03 - is a value that we need based on the
124:06 - corresponding chosen statistical
124:08 - significance level that we need to
124:11 - compute to compare our test statistics
124:14 - so our test statistics is this value and
124:16 - the value that we need to compare it to
124:19 - is the Z critical volue so we can see
124:22 - that this critical value is then equal
124:25 - to
124:27 - 1.96 this is actually a very common
124:30 - value that we know even without looking
124:32 - at a standard normal table when you make
124:35 - use of this test enough often then you
124:38 - know that the uh critical value
124:41 - corresponding to a two-sided test when
124:43 - it comes to normal table is equal to uh
124:46 - 1.96 this is just the value that we know
124:49 - and in here by even without calculating
124:53 - the next step which is a P value we can
124:55 - even say already what is the decision we
124:58 - need to make in terms of statistical
124:59 - significance because we know that one
125:02 - way we can test our hypothesis
125:04 - statistical hypothesis is by Computing
125:06 - the test statistics and checking with
125:08 - the test statistics the absolute value
125:11 - of it is larger than the critical value
125:14 - and we see that the test statistics is
125:16 - equal to minus
125:18 - 5956 the absolute value of that is 59
125:21 - .56 and that value is much larger than
125:24 - our critical value which is equal to
125:28 - 1.96 this already gives us an idea that
125:30 - we can reject our null hypothesis at 5%
125:34 - statistical significance level but I
125:37 - want to uh go on to the next step
125:39 - actually because that's um more
125:42 - structured more organized way to doing
125:45 - and conducting experimentations as in
125:47 - the industry we tend to make use of the
125:51 - p values instead of making use of this
125:53 - econometrical approach and statistical
125:56 - approach of um testing the statistical
125:59 - test so once we have calculated our test
126:02 - statistics the next thing we need to do
126:04 - is to calculate our P value and then use
126:09 - that P value compare to the significance
126:11 - level Alpha and then make a decision
126:14 - whether we need to reject our n
126:16 - hypothesis and say that we have a
126:18 - statistical significance or we cannot
126:20 - reject our n hyp hypthesis and then we
126:22 - need to say that we don't have a
126:25 - statistical significance so we don't
126:26 - have enough evidence to reject anal
126:28 - hypothesis so the idea here is that we
126:33 - need to make use of our uh normal
126:36 - function and specifically the norm. SF
126:39 - so making use of exactly the same
126:41 - Library the norm from CI do DOS and then
126:45 - this time we're using the survival
126:47 - function which is the one minus the
126:49 - cumulative distribution function of
126:51 - normal distribtion this comes again from
126:54 - statistics and then using the absolute
126:57 - value of our test statistics multiplying
126:59 - it by two given that we have a two-sided
127:01 - test I'm calculating my P value this is
127:04 - simply by making use of the same formula
127:08 - that we saw when we were uh studying
127:10 - theab test from a technical point of
127:13 - view because we learned that the P value
127:15 - is then the probability that Z will be
127:18 - smaller than equal the minus test
127:20 - statistics or that the test statistic is
127:22 - smaller than equal to Z so uh we
127:25 - basically want to calculate what is this
127:28 - probability the P value which is equal
127:31 - to the probability that our test
127:33 - statistics will be smaller than the
127:35 - critical value or
127:37 - our negative of the test statistics will
127:40 - be larger than equal of the critical
127:43 - value and we want to know this
127:45 - probability because what this
127:47 - probability represents is that what is
127:50 - the chance that we will get a large test
127:53 - statistics well this is due to a random
127:56 - chance and not because we have a uh
127:59 - actual statistical difference between
128:02 - the clickr rate of the experimental
128:03 - group versus control group so this is
128:06 - the idea behind P value so what is this
128:09 - chance that we are uh mistaking this
128:12 - random
128:13 - mistake this random observation that we
128:17 - got a large test statistic and saying
128:20 - that there is a statis significance well
128:22 - there is no such thing and we are purely
128:26 - getting this large test statistics um
128:29 - because of the random chance if the
128:32 - probability of getting a large test
128:34 - statistics by random chance is small so
128:36 - if this P value is small then we can say
128:39 - that we have a statistical significance
128:41 - that's the idea behind it and this P
128:43 - value when we calculate uh we are
128:46 - storing it in this variable called pcore
128:49 - volume and then the next thing what I'm
128:51 - doing is that I'm writing this function
128:53 - quote is statistically significant which
128:55 - takes argument as P value in Alpha so I
128:59 - just need the P value that I just
129:01 - calculated for my test Set uh test uh
129:03 - statistics and then I want the
129:06 - statistical significance level that I
129:07 - want to use for my test and then this is
129:11 - the value that comes from my power
129:13 - analysis as I mentioned before that's
129:15 - the 5% this P value I'm calculating for
129:19 - my test statistics so in here and then
129:22 - I'm taking the two and I want to compare
129:25 - them so I want to assess whether I have
129:28 - a statistical significance by comparing
129:30 - my P value to my statistical
129:32 - significance level Alpha and what is
129:35 - this comparison well we know uh from the
129:38 - theory that um if we have a low P value
129:41 - and specifically in the P that we are
129:44 - getting the P value is small than equal
129:46 - the 5% or 0.05 which is the significance
129:50 - level then this indicates that we have a
129:53 - strong statistical uh evidence that uh
129:56 - the N hypothesis is false and we need to
129:58 - reject it so we have a strong evidence
130:01 - against the null
130:02 - hypothesis and otherwise if the P value
130:07 - is larger than
130:08 - 0.05 so it's larger than 5% that we have
130:12 - chosen as the maximum threshold of that
130:15 - mistake so the significance level is uh
130:18 - uh no longer the largest element but the
130:21 - P value is larger than your significance
130:23 - level then this indicates that you don't
130:25 - have enough
130:27 - evidence against the N hypothesis so
130:29 - your evidence is weak this means that
130:32 - you fail to reject the N
130:35 - hypothesis so this is what I'm doing in
130:38 - here with this code so I'm saying print
130:41 - the P value first and we are rounding it
130:44 - up with this round function I'm rounding
130:46 - it to the three decimal and then I want
130:50 - to check and Det determine whether I
130:52 - have a statistically significant or not
130:54 - and the way that I'm doing that is I'm
130:56 - saying if my P value is more than my
130:58 - alpha or actually let at smaller than
131:01 - equal than Alpha then we can print that
131:03 - there is a statistical significance
131:05 - which indicates that the observed
131:07 - differences between the experimental and
131:10 - control groups are un unlikely to occur
131:13 - due to random
131:14 - chance which means that this is not
131:17 - random chance and uh we have a strong
131:20 - evidence that there is a statistical
131:22 - significance and this suggests that this
131:25 - new feature that we got this new version
131:28 - of our landing page with this um uh call
131:33 - to action um as the and now is better
131:37 - and result in higher statistically
131:40 - significantly higher clickr rate than
131:42 - the existing version of the control uh
131:45 - group so there is a real effect then
131:48 - otherwise if this is not the case which
131:50 - means that my P value is larger than my
131:52 - Alpha then I'm saying print that there
131:55 - is no seral significance and that the
131:57 - observed difference that we see in the
132:00 - clickr rate is not because uh of the
132:03 - real difference in the performance but
132:05 - TR truly this is just the random chance
132:08 - so here we can see that once we run our
132:11 - we call the function in here which is
132:14 - simply the function name and the
132:15 - argument so P value and alpha alpha
132:18 - comes from the initialized value that we
132:20 - had from our power analis so
132:24 - from here we initialize this value
132:28 - 0.05 and then here we got the P value
132:32 - that we just calculated then what we are
132:35 - getting in here is that our P value is
132:37 - actually so small that it's um rounded
132:41 - to the zero so what this means is that
132:44 - that there is evidence that suggest that
132:46 - at 5% statistical level significance
132:49 - level that the uh clickr rate of the
132:52 - experimental group is different from the
132:54 - clickr rate of the control group note
132:57 - that I'm not saying higher or lower
132:59 - because our statistical test was
133:02 - two-sided so under new hypothesis we had
133:05 - that the uh P control so in here as you
133:09 - can see our P control was equal to P
133:12 - experimental and under the alternative
133:14 - we had that the P control is not equal
133:16 - to P experimental this means that we um
133:20 - have now how rejected the null
133:22 - hypothesis we have found evidence that
133:25 - suggests that the null hypothesis can be
133:28 - rejected since our P value is zero and
133:31 - it's smaller than the statistical
133:33 - significance level
133:35 - 5% and this means that we can reject the
133:37 - hle and we can say that uh there is
133:41 - enough evidence to say that P control is
133:43 - not equal to P
133:45 - experiment and given that that we saw
133:49 - from the uh visualization and from our
133:51 - calculations that the um clickr rate for
133:55 - our experimental group is much
133:58 - higher then the click rate of the uh
134:02 - control group we can also say that we
134:04 - have found evidence that at 5%
134:07 - significance level we have found out
134:11 - that there is a statistically
134:12 - significant difference between the
134:15 - experimental and control groups clickr
134:17 - rate and that the experimental groups
134:19 - clickr rate is actually higher so
134:21 - statistically significantly higher than
134:24 - the control versions clickr rate so this
134:28 - is really important because this
134:30 - suggests that this difference in their
134:32 - click to rate is not due to random
134:34 - chance Alan but truly that there is
134:36 - evidence statistical evidence that can
134:38 - support this hypothesis that there is a
134:41 - true difference between the performance
134:44 - of the experimental version of the
134:46 - product so in this case in our case the
134:49 - landing page that has enrolled now
134:51 - button versus the control version of the
134:54 - product which had the uh start free
134:57 - trial version of the landing page the
134:59 - existing version so beside of
135:01 - calculating this P value it's always a
135:03 - great practice to also visualize your
135:06 - results and this is great for your
135:08 - audience who are technically sound and
135:11 - who know uh these different concepts and
135:14 - you want to visualize uh the results
135:16 - that you got not only by showing some
135:18 - number that is the P value and say hey I
135:21 - have a statistical significance but you
135:23 - also want to showcase the actual picture
135:26 - of what you got what is your test
135:28 - statistic what is the significance level
135:30 - that you use to kind of tell a story
135:32 - around your numbers and that's the uh
135:35 - art behind the data science I would say
135:38 - so let's go ahead and do some art so
135:40 - what I'm doing here is that I am making
135:43 - use of my standard normal distribution
135:45 - or the gaan distribution the way that we
135:48 - are referring to the standard normal
135:49 - distribution in statistics I'm saying
135:51 - that my mean or the MU is equal to zero
135:54 - my Sigma is equal to one which is my
135:55 - standard deviation and I'm saying that
135:58 - my uh I want to now plot my uh standard
136:01 - normal distribution by getting my uh X
136:05 - values which are the uh number of uh X
136:09 - elements that I want to have my xaxis
136:11 - and then taking the PDF or the
136:14 - probability distribution function for
136:16 - the normal distribution by using the CP
136:18 - Library I'm then providing my my X
136:22 - values for which I want to get
136:25 - my uh
136:27 - corresponding uh values of Y so
136:29 - basically here are all the values
136:31 - between let's say minus something minus
136:34 - three and then so between - 3 and three
136:38 - and I want to find all the Y's
136:40 - corresponding to this which basically
136:42 - plus the probability distribution
136:44 - function of the goian distribution or
136:45 - the standard normal distribution and
136:47 - then I want to add to this graph all
136:50 - also the uh corresponding rejection
136:53 - region and as you can see it is here so
136:57 - then what I'm adding here by using this
137:00 - part of the plot is that I want to fill
137:03 - in the rejection regions so I'm saying
137:06 - for all the values in this figure
137:09 - whenever the uh value is lower than that
137:13 - threshold in this case the threshold is
137:15 - z critical
137:17 - 1.96 so whenever my threshold is smaller
137:21 - than minus this uh
137:24 - 1.96 and larger than this
137:27 - 1.96 then we are in the rejection region
137:31 - we are saying then if my test statistics
137:34 - is falling in the rejection region in
137:36 - this case you can see that we are in the
137:38 - far left so the test statistic is minus
137:43 - 59.4 and it's much lower than this
137:46 - threshold as you can see in here this is
137:49 - this left Blue Line in here
137:51 - then in this case it falls in this
137:53 - rejection region so actually this entire
137:56 - thing is the rejection region it starts
137:59 - from here and it goes all the way to
138:01 - here anything anything in this region
138:04 - means that we need to we have a test
138:07 - statistic fully in the rejection region
138:10 - which means that we can reject to n
138:12 - hypothesis if we were to get a test
138:15 - stais that is very large and very
138:17 - positive it means we would be in this
138:19 - part of the figure and again in the
138:21 - rejection region anything above this
138:24 - line is then uh going under this
138:26 - category of rejection region and also
138:29 - anything in here so for anything in here
138:33 - we are in the rejection region being in
138:36 - the rejection region it means that we
138:38 - can reject the hypothesis and we can say
138:41 - that we have a statistically significant
138:44 - results so now when we have our
138:46 - statistical significance it's always a
138:49 - great idea to go on to the next step and
138:51 - it's actually mandatory to do this
138:54 - because not only a statistical
138:56 - significance is important but also the
138:58 - Practical significance as I mentioned in
139:00 - the beginning of this case study so for
139:03 - that what we are going to do is first we
139:05 - are going to calculate a confidence
139:06 - interval of the test and this confidence
139:09 - interval will help us to first of all
139:12 - make um comments regarding the quality
139:15 - of our test and its
139:17 - generalizability uh at our entire
139:20 - population
139:21 - and the accuracy of our results and then
139:24 - we will use this confidence interval to
139:26 - make a comments and to test for the
139:29 - Practical significance in our AB test so
139:33 - let's go ahead and calculate the
139:35 - confidence interval so as we learn as
139:38 - part of our lectures the confidence
139:39 - interval can be calculated by first
139:42 - taking the uh P experimental head and P
139:45 - control head and the standard error and
139:47 - the Z critical so here we need the two
139:49 - different estimat of the experimental
139:51 - groups click through rate and the
139:53 - control groups click through rate we
139:55 - also need the standard error of our two
139:57 - sample Z test as well as the critical
140:00 - value and then we need to First
140:02 - calculate the lower bound of our
140:04 - confidence interval and then we need to
140:06 - calculate the upper bound of our
140:07 - confidence interval and in this case uh
140:10 - given that the um statistical
140:13 - significance level we are using is
140:15 - Alpha uh the uh Z critical is based on
140:19 - that therefore for we are also saying
140:21 - that we are calculating the 95%
140:24 - confidence interval so in here the way
140:28 - we will calculate the lower bound is by
140:30 - taking the P experimental head
140:32 - subtracting from that the P control head
140:34 - and then once we have done that we then
140:36 - substract from that the standard error
140:38 - multiply by Z critical volume and we are
140:41 - just rounding this up up to the three
140:43 - decimal behind the zero then we are
140:46 - doing the same thing only with a plus
140:48 - sign in here for the opp upper bound
140:51 - calculation of the confidence interval
140:53 - so this is just pure following the
140:55 - formula of the confidence interval that
140:57 - I will set you
140:58 - here and let's go ahead and print this
141:03 - value which is this interval so what we
141:06 - are seeing here is that we have a
141:08 - confidence interval that is from
141:10 - 0 399 so
141:13 - 0.4 to 0 uh
141:16 - 43 so quite a narrow confidence interval
141:19 - I would say which is actually a good
141:21 - sign because this confidence interal
141:24 - that provides this range of values
141:26 - within which the true difference between
141:28 - this control and experimental groups
141:30 - proportions or the clickr rate is likely
141:33 - to lie within a certain level of
141:35 - confidence in this case 95% confidence
141:39 - this is very narrow and if it's a narrow
141:41 - confidence interval it means that the uh
141:45 - accuracy of our results is higher and it
141:48 - means that the results we are getting
141:50 - BAS B on our smaller sample it will most
141:53 - likely generalize well when we apply
141:55 - these changes and deploy these changes
141:58 - and we put this new product in front of
142:00 - the entire population of users because
142:02 - now we are doing all this experiment for
142:04 - a small group for the sample and this
142:08 - confidence info that is narrow it's not
142:10 - wide it's narrow it means that the
142:12 - results that we are getting is are
142:15 - accurate more or less accurate and this
142:17 - means that we the results that we are
142:20 - getting based on a sample are most
142:22 - likely a true representation of the
142:25 - entire population that we got this is
142:27 - the idea behind the width of the
142:29 - confidence interval the narrower it is
142:32 - the higher uh is the quality of your
142:34 - results which means that the uh more
142:37 - generalizable are your results so let's
142:41 - now go on to the final stage of our case
142:44 - study which is to test the Practical
142:46 - significance of our results so now when
142:49 - we know that the statistical
142:51 - significance is there the experimental
142:54 - version of our feature is statistically
142:56 - significantly different from the control
142:58 - version in terms of the clickr rate and
143:01 - we have seen that the competence
143:02 - interval is narrow which means that our
143:04 - results are accurate quite uh with quite
143:08 - high accuracy then we can now comment on
143:11 - the Practical significance of our
143:13 - results this means we want to see
143:15 - whether the significant difference that
143:18 - we obtained whether this difference is
143:20 - actually large enough from the business
143:23 - perspective to say that it's worth to
143:25 - put our engineering resources and our
143:28 - money and our uh uh product into uh to
143:34 - put through through this change and to
143:36 - uh say that it's wor from the business
143:39 - perspective to change this button and to
143:42 - put this into um the production and in
143:45 - front of our users and of course here we
143:48 - are not only talking about the engineer
143:50 - ing resources that it will take from us
143:52 - to change this and the deployment and
143:55 - the monitoring but also in terms of the
143:58 - quality of the product we are providing
144:00 - to our users because whenever we are
144:01 - making a change to our product it is a
144:04 - risk because we are changing what our
144:06 - user is used to see and this can always
144:09 - be scary uh when it comes uh to the
144:13 - business because we don't want to uh
144:15 - make our customers scared so therefore
144:19 - we need to also check for this practical
144:21 - significance so for that what I'm doing
144:23 - is that I'm creating this python
144:25 - function that will take two arguments so
144:28 - two values that is the minimum
144:29 - detectable effect and then the 95%
144:32 - confidence interval that I just
144:34 - calculated those will be the two
144:36 - arguments for my function and I'm
144:37 - calling this function is practically
144:40 - significant and this function will go
144:43 - and check whether the uh practical
144:46 - significance is there or not and it will
144:48 - then return through true or false and
144:51 - then it will also print whether we have
144:53 - a practical significance or not and we
144:56 - learned from the theory and we know from
144:58 - this AB testing concept that whenever
145:01 - the uh MD or the Delta that we got the
145:04 - minimum detectable effect is larger than
145:07 - the lower bound of our confidence
145:09 - interval it means that the lowest
145:12 - possible value that we can get based on
145:15 - the results that we obtain in our
145:17 - sample that that amount is smaller than
145:20 - and the minimum detectable effect that
145:23 - we assumed before even conducting our AB
145:26 - test this suggests that we have a
145:29 - practical significance and the
145:30 - difference the minimum difference that
145:32 - we will obtain is large enough for us to
145:35 - have a motivation to make this change in
145:38 - our product for that what I'm doing is
145:42 - that first I'm taking my 95% confidence
145:44 - interval and I'm taking the first
145:47 - element because we know that a
145:48 - confidence interval is actually ranged
145:50 - so two pull of two numbers the lower
145:52 - bound and upper bound I need the lower
145:54 - bound because all I care for this
145:56 - practical significance is to compare the
145:58 - lower bound of the 95% confidence
146:01 - interval to this minimum detectable
146:04 - effect which is my Delta so therefore
146:06 - I'm taking this lower bound of
146:08 - confidence interval putting that into a
146:11 - variable and then I'm using this
146:14 - variable this lower uncore bound uncore
146:17 - CI confidence interval and I'm comparing
146:20 - this to my Delta I'm saying if my lower
146:23 - Bond of the confidence
146:25 - interval actually I'm noticing that here
146:28 - I got a mistake it should be the other
146:29 - way around we need to say that if our
146:33 - Delta is larger or
146:36 - equal the uh lower bound of the
146:39 - confidence interval which is the same as
146:42 - if
146:42 - our lower bound of the confidence
146:46 - interval is smaller than equal our Delta
146:50 - so if our we can also write this the
146:52 - other way around so if our
146:55 - Delta
146:56 - is larger than equal than our lower
147:00 - underscore bounde uncore CI then we can
147:05 - say that we have a practical
147:06 - significance so we the MDA of in this
147:10 - case so I want to use my initial Delta
147:13 - therefore I won't be initializing this
147:16 - so you might recall here a Delta of 10%
147:19 - I want to still make use of that Delta
147:22 - so therefore I will just go ahead and
147:25 - then in here what I want to do is to
147:27 - call this function by using that
147:30 - specific Delta so I want to have a 10%
147:33 - as my MD and whenever this Delta will be
147:36 - larger than the lower bound of my
147:38 - confidence interval that I just obtained
147:41 - I will then say that we have a practical
147:45 - significance and with an MDA of 10% the
147:49 - differ between control and experimental
147:52 - group is also practically significant so
147:55 - you can see that the lower bound is 0.04
147:58 - something that we obtain in here and
148:00 - that amount is then compared to this
148:04 - Delta and here you can see that we have
148:08 - concluded that we also have a practical
148:12 - significance so amazing we have come to
148:15 - the end of this case study and in this
148:17 - involved case study we have conducted an
148:19 - entire app AB has results in ases so
148:22 - this case study a has and to end going
148:25 - from the point of loading the data and
148:28 - then understanding this business concept
148:30 - or business objective of ab test where
148:32 - we were testing whether the um enroll
148:36 - Now button which is the new version the
148:38 - experimental version should replace the
148:40 - existing button which is the secure
148:43 - vraal and based on this case study what
148:45 - we found out is that we have a
148:47 - statistical significance at 5%
148:50 - significance level suggesting that we
148:51 - can reject the N hypothesis and we can
148:53 - say that indeed there exists a
148:56 - statistical significant difference
148:58 - between the click through rate in the
149:00 - experimental group versus control group
149:03 - uh and specifically that the enroll now
149:05 - experimental button results in
149:07 - statistically significantly higher click
149:09 - through rate than the uh secure free
149:12 - trial button and beside this we also
149:15 - checked the um accuracy of our results
149:18 - by looking at a confidence interval and
149:20 - saw that the confidence interval was
149:21 - quite narrow suggesting that the results
149:24 - we obtained were quite uh accurate and
149:27 - this means that the results that we got
149:29 - for the sample will generalize to our
149:31 - population of users and finally we have
149:34 - also checked the Practical significance
149:36 - of our results by using the 95%
149:39 - confidence interval and comparing the
149:41 - lower bound of that interval with our
149:44 - minimum detectable effect Delta and we
149:47 - saw that we will have at least 10%
149:50 - significant difference between the
149:52 - control groups CTR and the control uh
149:55 - the experimental group CTR and the
149:57 - experimental group CTR will be at least
149:59 - 10% higher than the uh control groups
150:03 - and this suggests that uh from the
150:05 - business perspective we also have a
150:08 - motivation uh beside of this statistical
150:11 - significance we also have practical
150:12 - significance suggesting that we also
150:14 - have enough motivation and reason from
150:17 - the business perspective to put this new
150:20 - button into production so we can
150:22 - conclude that uh based on this
150:24 - datadriven approach and conducting an AB
150:27 - testing we uh can see a clear motivation
150:31 - of deploying this new button and draw
150:34 - now and replace the existing one secure
150:38 - free trial version and we will then
150:40 - expect to see more users clicking on
150:43 - this and engaging with our product and
150:46 - for now this will be all for this case
150:48 - study if you want to learn more about AB
150:50 - testing make sure to check our AB
150:53 - testing course as well as the ultimate
150:55 - data science boot camp don't forget to
150:57 - try our free trial this time using our
151:00 - enroll Now button and if you want to see
151:03 - more case studies like this make sure to
151:06 - check our tic case studies we have many
151:08 - case studies also included as part of
151:10 - our ultimate data science boot camp
151:12 - where we go in detail of these different
151:15 - steps and we conduct different sorts of
151:17 - case studies to put our data science
151:19 - theory in to practice including from the
151:21 - field of NLP machine learning
151:23 - recommended systems Advanced analytics
151:26 - and also AB testing and soon also from
151:29 - AI so for now thank you for staying with
151:32 - me and conducting this case study happy
151:38 - learning thank you for watching this
151:40 - video If you like this content make sure
151:42 - to check all the other videos available
151:44 - on this channel and don't forget to
151:46 - subscribe like and comment to help the
151:49 - algorithm to make this content more
151:51 - accessible to everyone across the world
151:54 - and if you want to get free resources
151:57 - make sure to check the free resources
151:59 - section at lunch. and if you want to
152:02 - become a job rate data scientist and you
152:05 - are looking for this accessible boot
152:07 - camp that will help you to make a job
152:09 - ready data scientist consider enrolling
152:12 - to the data science boot camp the
152:14 - ultimate data science boot camp at
152:17 - l. you will learn all the things the
152:20 - fundamentals to become a jbre data
152:22 - scientist he will also implement the
152:25 - learn theory into a real world multiple
152:29 - data science projects beside this after
152:32 - learning the theory and practicing it
152:34 - with the real world case studies you
152:36 - will also prepare for your data science
152:38 - interviews and if you want to stay up to
152:41 - date with the recent developments in
152:42 - Tech what are the headlines that you
152:44 - have missed in the last week what are
152:47 - the open positions currently in the
152:49 - market across the globe and what are the
152:51 - tech startups that are making waves in
152:53 - the tech and sure to subscribe to the
152:56 - data science Nai newsletter from lunar
152:58 - Tech
153:03 - [Music]

Cleaned transcript:

in this applied data science crash course you learn all about AB testing from the concepts to the Practical details they can apply in business AB testing is commonly used in data science it's an experiment on two variants to see which performs better based on a given metric this course merges indepth statistical analysis with the kind of data science theories big Tech firms rely on T from L Tech developed this course she is a very experienced a scientist and teacher welcome to the handsone ab Testing crash course where we will do some refreshment when it comes to AB testing if you're looking for that one course where you can learn and quickly refresh your memory for AB testing and how to actually do an AB testing case study hands on in Python then you are in the right place in this crash course we are going to refresh our memory for the a test design including the power analysis and defining those different PR such as minimum detectable effect statistical significance level and also the uh type two probability so the power of the test and then we are going to do HandsOn case study project where we will be conducting an AB testing results analysis in Python at the end of this course you can expect to know everything about designing an AB test what it means as to design a proper AB test and how to do a Ab test results analysis in Python in a proper way I'm dat Vas and cofounder at Lun Tech and I have been in data science for the last 5 years I have learned AB testing end to end after following numerous blogs and numerous research papers and courses and I've noticed that there is not a one place one course that will cover all the fundamentals and necessary stuff both the theory and implementation in Python in one place and that's about to change as we have this crash course that will help you to do exactly that to learn how to design an AB test in a proper way as a good and solidated scientist and to Showcase your skills by doing python AB testing results and asset don't forget to subscribe like and comment to help the algorithm to make this content more accessible to everyone across the world and if you want to get free resources make sure to check the free resources section at lunch. and if you want to become a job ready data scientist and you are looking for this accessible boot camp that will help you to make your job ready data scientist consider enrolling to the data science boot camp so whether you are a product scientist whether you are a data analyst data scientist or a product manager who wants to learn about AB testing at high level and how it can be done in Python then you are in the right place because in this crash course we're going to refresh our memory what it means to properly design an a test test which means doing power analysis and also calculating the sample size by hand by following the statistical guidelines and ensuring that everything is done properly and then as the second part of this Crush course we are also going to do an handson case study in Python when it comes to performing AB testing results analysis so we are going to cover all these important Concepts such as P values sample size and also uh interpreting the ab test results using standard error calculating those uh estimates pulled variance and then evaluating the ab test results including confidence interal generalizability of the results reproducibility of the results so without further Ado let's get started AB testing is an important topic for data scientists to know because it's a powerful method for evaluating changes or improvements to the products or services it allows us to make data driven Decisions by comparing the performance of two different versions of a product or a service usually referred as treatment or control for example a testing allows data scientists to measure the effectiveness of changes to your product or a service which is important as it enables data scientists to make data driven decisions rather they're relying on Intuition or assumptions secondly AB testing helps data Sciences to identify the most effective change changes to a product or a service which is really important because it allows us to optimize the performance of a product or a service which can then lead to increased customer satisfaction and sales AB testing helps us also to validate certain hypothesis about what changes will improve a product or service this is important because it helps us to build a deeper understanding of the customers and the factors that influence customers Behavior finally AB testing is a common practice in many Industries such as ecommerce digital marketing website optimization and many others so data scientists who have knowledge and experience in a testing will be more valuable to these companies no matter in which industry you want to enter as a data scientist and what kind of job you will be interviewed for and even if you believe more technical data scien is your cup of tea be prepared to know at least higher level understanding and the details behind this method will definitely help you to know about this topic when you are speaking with product owners stakeholders product scientists and other people involved in the business let's briefly discuss the perfect audience for the section of the course and prerequisites there are no prerequisites of the section in terms of AB testing Concepts that you should know already but knowing the basics and statistics which you can find in the fundamentals to statistics section is highly recommended this section will be great if you have no priority AB testing knowledge and you want to identify and learn the essential AB testing Concepts from scratch so this will help you to prepare for your job interviews it will also be a good refresher for anyone who does have AB testing knowledge but who wants to refresh their memory or want to fill in the gaps in their knowledge in this lecture we will start off the topic about AB testing where we will formally Define what AB testing is and we will look at the high level overview of AB testing process step by step by definition AB testing or split testing is originated from the statistical randomized control trials and is one of the most popular ways for businesses to test new ux features new versions of a product or an algorithm to decide whether your business should launch that new ux feature or should productional IE that new recommender system create that new product that new button or that new algorithm the idea behind a testing is that you should show the variated or the new version of the product to sample of customers often referred as experimental group and the existing version of the product to another sample of customers referred as control group then the difference in the product performance in experimental versus control group is tracked to identify the effect of these new versions of the product on the performance of the product so the goal is then to track the metric during the test period and find out whe there is a difference in the performance of the product and and what type of difference is it the motivation behind this test is to test new product variants that will improve the performance of the existing product and will make this product more successful and optimal showing a positive treatment effect what makes this testing great is that businesses are getting direct feedback from their actual users by presenting them the existing versus the variated product version and in this way they can quickly Test new ideas in case of ab Test shows that the variated version is not effective at least businesses can learn from this and can decide whether they need to improve it or need to look for other ideas let us go through the steps included in the AB testing process which will give you a higher level overview into the process the first step in conducting AB testing is stating the hypothesis of the ab test this is a process that includes coming up with business and statistical hypothesis that you would like to test with this test including how you measured the success which will primary metric next step in AB testing is to perform what we call power analysis and design the entire test which includes making assumptions about the most important parameters of the test and calculate the minimum sample size required to claim statistical significance the third step in AB testing is to run the actual AB test which in practical sense for the data scientist means making sure that the test runs smoothly and correctly collaborate with engineers and product managers to ensure that all the requirements are satisfied this also includes collecting the data of control and experimental groups which will be used in The Next Step next step in AB testing is choosing the right statistical test whether it is z test T Test Ki Square test Etc to test the hypothesis from the step one by using the data collected from the previous step and to determine whether there is a statistically significant difference between the control versus experimental group The Fifth and the final step in AB testing is continuing to analyze the results and find out whether besides statistical significance there is also practical significance in this step we use the second step's power analysis so the assumptions that we made about model parameters and the simple siiz and the four steps results to determine whether there is a practical significance beside of the statistical significance this summarizes the AB testing process at a high level in next couple of lectures we'll go through the steps one at a time so buckle up and let's learn about AB testing in this lecture lecture number two we will discuss the first step in a testing process so let's bring our diagram back as you can recall from the previous lecture when we were discussing the entire process of AB testing at a high level we saw that in the first step in conducting AB testing is stating the hypothesis of ab test this process includes coming up with a business and statistical hypothesis that you would like to test with this test including how you measured the success which we call a primary metric so what is the metric that we can use to say that that the product that we are testing performs well first we need to State the business hypothesis for our AB test from a business perspective so formally business hypothesis describes what the two products are that being compared and what is the desired impact or the difference for the businesses so how to fix a potential issue in the product where a solution of these two problems will influence what we call a key performance indicator or the kpi of the interest business hypothesis is usually set as a result of brainstorming and collaboration of relevant people on the product team and data science team the idea behind this hypothesis is to decide how to fix a potential issue in the product where a solution of these problems will improve the target kpi one example of business hypothesis is that changing the color of learn more button for instance to Green will increase the engagement of the web page next we need to select what we call primary metric for our av testing there should be only one primary metric in your ab test choosing this metric is one of the most important parts of ab test since this metric will be used to measure the performance of the product or feature for the experiment Al and control groups and they will be used to identify whether there is a difference or what we call statistically significant difference between these two groups by definition primary metric is a way to measure the performance of the product being tested in the ab test for the experimental and control groups it will be used to identify whether there is a statistically significant difference between these two groups the choice of the success metric depends on the underlying hypothesis that is being tested with this AB test this is if not the most one of the most important parts of the ab test because it determines how the test will be designed and also how will the proposed ideas perform choosing poor metrics might disqualify a large amount of work or might result in wrong conclusions for instance the revenue is not always the end goal therefore in AB testing we need to tie up the primary metric to the direct and the higher level goals of the product the expectation is that if the product makes more money then this suggests the content is great but in achieving that goal instead of improving the overall content of the material and writing one can just optimize the conversion funless one way to test the accuracy of the metric you have chosen as your primary metric for your ab test could be to go back to the exact problem you want to solve you can ask yourself the following question what I tend to call the metric validity question so if the Chen metric were to increase significantly while everything else T constant would we achieve our goal and would we address our business problem is it higher revenue is it higher customer engagement or is it high views that we are chasing in the business so the choice of the metric will then answer this question though you need to have a single primary metric for your ab test you still need to keep an eye on the remaining metrics to make sure that all the metrics are showing a change and not only the target one having multiple metrics in your ab test will lead to false positives since you will identify many significant differences well there is no effect which is something you want to avoid so it's always a good idea to pick just a single primary metric but to keep an eye and monitor all the remaining metrics so if the answer to the metric validity question is higher Revenue which means that you are saying that the higher revenue is what you are chasing and better performance means higher revenue for your product then you can use your primary metric what we call a conversion rate conversion rate is a metric that is used to measure the effectiveness of a website a product or a marketing campaign it is typically used to determine the percentage of visitors or customers who take a desired action such as making a purchase filling out a form or signing up for a service the formula for conversion rate is conversion rate is equal to number of conversions divided to number of total visitors multiplied by 100% for example if a website has thousand visitors and 50 of them make a purchase the conversion rate would be equal to 50 divide 2,000 multiply by 100% which gives us 5% this means that our conversion rate in this case is equal to 5% conversion rate is an important metric because it allows us and businesses to measure the effectiveness of their website a product or a marketing campaign it can help businesses to identify areas for improvement such as increasing the number of conversions or improving the user experience conversion rate can be used for different purposes for example if a company wants to measure the effectiveness of an online store the conversion rate would be the percentage of visitors who make a purchase and on the other hand if a company wants to measure the effectiveness of landing page the conversion rate would be the percentage of visitors who fill out a form or sign up for a service so if the answer to the metric validity question is higher engagement then you can use the clickr rate or CTR as your primary metric this is by the way a common metric used in a testing whenever we are dealing with ecommerce product search engine recommander system clickr rate or CTR is a metric that measures the effectiveness of a digital marketing campaign or the user engagement or some feature on your web page or your website and it's typically used to determine the percentage of users who click on a specific link or button or call to action CTA out of the total to number of users who view it the formula for the clickr rate can be represented as follows so the CTR is equal to number of clicks divided to number of Impressions multiply by 100% not to be confused with click through probability because there is a difference between the click through rate and click through probability for example if an online advertisement receives thousand of Impressions which means that we are showing it to the customers for a thousand times and there were 25 clicks which means 25 out of all this impression resulted in clicks this means that the clickr rate for this specific example would be equal to 25 divide 2,000 multiply by 100% which gives us 2.5% this means that for this particular example our clickr rate is equal to 2.5% cure rate is an important metric because it allows businesses to measure the effectiveness of their digital marketing campaigns and the user engagement with their website or web pages High click through rate indicates that a campaign or the web page or feature is relevant and appealing to the target audience because they are clicking on it while low clickthrough rate indicates that a campaign or the web page needs an improvement click through rate can be used to measure the performance of different digital marketing channels such as PID search display advertising email marketing and social media it can also be used to measure the performance of different ad formats such as text advertisements Banner advertisement video advertisements Etc next and the final task in this first step in the process of AP testing is to State the statistical hypothesis based on business hypothesis and the chosen primary metric next and in the final task in this first step of the AB testing process we need to State the statistical hypothesis based on the business hypothesis we stated and the chosen primary metric in the section of fundamentals through statistics of this course in lecture number seven we went into details about statistical hypothesis testing included what n hypothesis is and what alternative hypothesis is so do have a look to get all the insight about this topic AB testing should always be based on a hypothesis that needs to be tested this hypothesis is usually set as a result of brainstorming and collaboration of relevant people on the product team and data science team the idea behind this hypothesis is to decide how to fix a potential issue in a product where a solution of these problems will influence the key performance indicators or the kpi of interest it's also highly important to make prioritization out of a range of product problems and ideas to test while you want to P that fixing this problem would result in the biggest impact for the product we can put the hypothesis that is subject to rejection so that we want to reject in the ideal World Under The N hypothesis what we Define by AG zero well we can put the hypothesis subject to acceptance so the desire hypothesis that we would like to have as a result of AB testing under the alternative hypothesis defined by H1 for example if the kpi of the product is to increase the customer engagement by changing the color of the read more button from blue to green then under the N hypothesis we can state that clickr rate of learn more button with blue color is equal to the click through rate of green button under the alternative we can then state that the click true rate of the learn more button with green color is Lar larger than the click through of the blue button so ideally want to reject this no hypothesis and we want to accept the alternative hypothesis which will mean that we can improve the clickr rate so the engagement of our product by simply changing the color of the button from blue to green once we have set up the business hypothesis selected the primary metrics and stated the statistical hypothesis we are ready to proceed to the next stage in the ab testing process in this lecture we will discuss the next Second Step In AB testing process which is designing the ab test including the power analysis and calculating the minimum sample sizes for the control and experimental groups stay tuned as this is a very important part of AB testing process commonly appearing during the data science interviews some argue that AB testing is an art and others say that it's a business adjusted common statistical test but the borderline is that to properly Design This experiment you need to be disciplined and intentional while keeping in mind that it's not really about testing but it's about learning following AR steps you need to take to have a solid design for your ab test so let's bring the diagram back so in this step we need to perform the power analysis for our AB test and calculate the minimum sample size in order to design our AB test AB test design includes three steps the first step is power analysis which includes making assumptions about model parameters including the power of the test the significance level Etc the second step is to use these parameters from Power analysis to calculate the minimum sample size for the control and experimental groups and then the final third step is to decide on the test duration depending on several factors so let's discuss each of these topics one by one power analysis for AB testing includes this tree specific specific steps the first one is determining the power of the test this is our first parameter the power of the statistical test is a probability of correctly rejecting the N hypothesis power is the probability of making a correct decision so to reject the N hypothesis when the N hypothesis is false if you're wondering what is the power of the test what is this different concepts that we just talk about what is this null hypothesis and what does it mean to reject the null hypothesis then head towards the fundamental statistic section of this course as we discuss this topic in detail as part of that section the power is often defined by 1 minus beta which is equal to the probability of not making a type two error where type two error is a probability of not rejecting the null hypothesis while the null is actually false it's common practice to pick 80% as the power of the ab test which means that we allow 20% of type to error and this means that we are fine with not detecting so failing to reject n hypothesis 20% of the time which means that we are fine with not detecting a true treatment effect while there is an effect which means that we are failing to reject the N however the choice of value of this parameter depends on nature of the test and the business constraints secondly we need to determine a significance level for our AB test the significance level which is also the probability of type one error is the likelihood of rejecting the no hence detecting a treatment effect while the know is actually true and there is no statistically significant impact this value often defined by a Greek letter Alpha is a probability of making a false Discovery often referred to as a false positive rate generally we use the significance level of 5% which indicates that we have 5% risk of concluding that there exists a statistically significant difference between the experimental and control variant performances when there is no actual difference so we are fine by having five out of 100 cas Cas is detecting a treatment effect well there is no effect it also means that you have a significant result difference between the control and the experimental groups within 95% confidence like in the case of the power of the test the choice of the alpha is dependent on the nature of the test and the business constraints that you have for instance if running this a test is related to high engineering course then the business might decide to pick a high offer such that it would be easier to detect a treatment effect on the other hand the implementation costs of the proposed version in production are high you can then pick a lower significance level since this proposed feature should really have a big impact to justify the high implementation cost so it should be harder to reject n hypothesis finally as the last tyep of power analysis we need to determine a minimum detectable effect for the test last parameter as part of the power analysis we need to make assumptions about is what we call minimum detectable effect or Delta from the business point of view so what is the substantive to the statistical significance that the business wants to see as a minimum impact of the new version to find this variant investment worthy the answer to this question is what is the amount of change we aim to observe in a new versions metric compared to the existing one to make recommendations to the business that this feature should be launched in the production that it's investment worthy an estimate of this parameter is what is known as as a minimum detectable effect often defined by a Greek letter Delta which is also related to the Practical significance of the test so this mde or the minimum detectable effect is a proxy that relates to the smallest effect that would matter in practice for the business and it's usually set by stakeholders as this parameter is highly dependent on the business there is no common level of it instead so this minimum detectable effect is basically the translation from statistical significance to the Practical significance and here we want to see and we want to answer the question what is this percentage increase in the performance of the product that we want to experiment with that will tell to the business that this is good enough to invest in this new feature or in this new product and this can be for instance 1% for one product it can be 5% for another one and it really depends on the business and what is the underlying kpi a popular reference to the parameters involved in the power analysis for AB testing is like this so 1 minus beta for the power of the test Alpha for the significance level Delta for the minimum detectable effect to make sure that our results are repeatable robust and can be generalized to the entire population we need to avoid P hacking to ensure real statistical significance and to avoid biased results so we want to make sure that we collect enough amount of observations and we run the test for a minimum predetermined amount of time therefore before running the test we need to determine the samp size of the control and experimental groups as well as later on in this lecture we will see also how long we need to run the test so this is another important part of AB testing which needs to be done using the defined power of the test which was the one minus beta the significance level and a minimum detectable effect so all the parameters that we decided upon when conducting the power analysis calculation of the sample size depends on the underlying primary metric as well that you have chosen for tracking the progress of the control and experimental versions of the product so we need to distinguish here two cases so when discussing the primary metric we saw that there are different ways that we can measure the performance of different type of products if we are interested in engagement then we are looking at a metric such as click through rate which is in the form of averages so the case one will be where the primary metric of AB testing is in the form of a binary variable it can be for instance conversion or no conversion click or no click and in case two where the primary metric of the test is in the form of proportions or averages which means mean order amount or mean click through rate for today we will be covering only one of these cases but you can find more details on the second case in my blog which I posted also as part of the resources section this blog post contains all the details that you need to know about AB testing including the statistical test and their corresponding hypothesis the descriptions of different primary metrics that go beyond what we have covered as part of this section as well as many more details that you need to know about a testing so let's look at a case two where the primary metric of the test is in the form of proportions or averages so let's say we want to test whether the average click to rate of control is equal to the average click to rate of experimental group and under HD we have that the m control is equal to M experimental and under H1 we have that the m control is not to Mu experimental so here the MU control and mu experimental are simply the average of the primary metric for control group and for the experimental group respectively so this the formal hypothesis we want to test with our AB test and we can assume that this new control is for instance the clickr rate of the control group and the MU experimental is the clickr rate of the experimental group so this is the formal statistical hypothesis we want to test with our AB test if you haven't done so I would highly suggest you to head towards the fundamental statistic section of this course where in lecture number seven and eight of the statistical part of this course I go in detail about statistical hypothesis testing the means averages significance level Etc this also holds for the theorem that the some prise calculation is based upon called Central limit theorem so check out the last lecture about inferential statistics where I covered the central limit theorem which we will also use in this section and finally also check the lecture number five in that section where we cover the normal distribution another thing that we will use as part of this section so the central limit theorem states that given a sufficiently large sample size from an arbitrary distribution the sample mean will be approximately normally distributed regardless of the shape of the original population distribution this means that the distribution of the sample means will be approximately normal if we take a large enough sample even if the distribution of the orig sample is not normal so when we are dealing with a primary performance tracking metric that is in the form of average such as this one that we are covering today which is a clickr rate we intend to compare the means of the control and experimental groups then we can use the central limit theorem as state that the mean sampling distribution of both controlling experimental groups follow normal distribution consequently the sampling distribution of the difference of the means of these two groups also will be normally distributed so this can be expressed like this where we see that the mean of the control group and mean of the experimental group follows normal distribution with mean mu control and mu experimental respectively and then with the variance of Sigma control squared and sigma experimental squared respectively though derivation of this Pro is out of the scope of this course we can state that the difference between the means of the true group so xar control minus xar experimental also follows normal distribution with a mean new control minus new experimental and with a variance of Sigma control squ / to n Control Plus Sigma experimental Square / to n experimental so the sample size of the experimental group and the sample size of the control group hence the sample size needed to compare the me of the two normally distributed samples using a twosided test which prespecify significance of alpha power level and minimum detectable effect can be calculated as follows so here you can see the mathematical representation of the minimum sample size so the N which stands for the minimum sample size is equal to and in denominator we have Sig control S Plus Sigma experimental squar multip by z 1us alpha / to 2 + z 1us beta squ / to the Delta squ and here the Alpha and the beta and the Delta we have made assumptions about as part of the power analysis and the sigma control squar and a sigma experimental squared are the uh estimates of the variance that we can come up with using the SoCal A8 testing I would say you do not necessarily need to know this derivation as there are many online calculators that will ask you for the alpha the beta and the Delta values as well as the sample estimates for the sigma squ control and experimental and then these calculators will automatically calculate the minimum S size for you if you're wondering what this AA testing is and how we can come up with the sigma control squared and sigma experimenting squared as well as all the other values then make sure to to check out the blog that I posted before and that I mentioned before as I explained in detail all these values as well as check out the resource section where I've included many resources regarding this but for now just keep in mind that the Z1 minus Alpha / to two and Z1 minus beta are just two constants and come from the normal distributed and standard normal distributed tables I would say you do not necessarily need to know this derivation as there are many online calculators that will ask you for this Alpha Beta And Delta values as well as the sule estimates for the sigma squ controling Sigma experimental control and then we'll calculate automatically the sample size for you for the control and experimental group effectively one example of such calculator is this AB testy online calculator but if you Google it you will find many others that will ask you for the minimum detectable effect for the statistical significance or the statistical power and then it will automatically calculate for you the minimum sample size that you should have in order to have a statistical significance and in order to have a valid AB test one thing to keep in mind is that you will notice that the statistical significance level is set to 95% in here which is not what we have seen when we were discussing the alpha significance level so sometimes these online calculators will confuse or will interchangeably use the significance level versus the confidence level which are the opposite so the significance level is usually at the level of 5% or 1% confidence level is around 95% so which is basically 100% minus the alpha therefore whenever whenever you see this 95% know that this means that your Alpha should be 5% so it's really important to understand how to use this calculator not to end up with the wrong minimum sample size conduct an entire AB test and then at the end realize that you have used the wrong uh significance level the final step is to calculate the test duration this question needs to be answered before you run your experiment and not during the experiment sometimes people stop the test when they detect statistical significance which is what we call P hacking and that's absolutely not what you want to do to to determine the Baseline of the duration time a common approach is to use this formula as you can see duration is equal to n ided to the number of visitors per day where n is your minimum sample size that we just calculated in the previous step and the number of visitors per day is the average number of visitors that you expect to see as part of your experiment for instance if this formula results in 14 days or 14 this suggest that running the test for two weeks is a good idea however it's highly important to take many business specific aspect into account when choosing the time to run the test and for how long you need to run it and simply using this formula is not enough for example if you want to run an experiment at the end of the month December with Christmas breaks when higher than expected or lower than expected number of people are usually checking your web page then this external and uncertain event had an impact on the page page to search for some businesses this means for example if you want to run an experiment at the end of the month of December with Christmas breaks when higher than expected or in some cases lower than expected number of people are usually checking the web page so depending on the nature of your business or the product then this external and uncertain event can have an impact on the page usage for some businesses which means that for some businesses a high increasing the page usage can be the result and for some a huge decrease in usability in this case running AB test without taking into account this external Factor would result in inaccurate results since the activity period would not be true representation of a common page usage and we no longer have this Randomness which is a crucial part of AB testing beside this When selecting a specific test duration there are few other things to be aware of firstly two small test duration might result in what we call novelty effects users tend to react quickly and positively to all types of changes independent of their nature so it's referred as a novelty effect and it vares of in time and it is considered illusionary so it would be wrong to describe this effect to the experimental version itself and to expect that it will continue to persist after the noble T effect wears off hence when picking a test duration we need to make sure that we do not run the test for too short amount of time period otherwise we can have a novelty effect novelty effect can be a major threat to the external validity of an AV test so it's important to avoid it as much as possible secondly if the test duration is too large then we can have what we call maturation effects when planning an AB test it's usually useful to consider a longer test duration for allowing users to get used to a new feature or product in this way one will be able to absorve the real treatment effect by giving more time to returning users to cool down from an initial positive reaction or a spike of Interest due to a change that was introduced as part of a treatment this should help to avoid novelty effect and is better predictive value for the test outcome however the longer the test period the larger is the likelihood of external effect impacting the reaction of the users and possibly contaminating the test results this is what we call maturation effect and therefore running the AP test for too short amount of time or too long amount of time is not recommended as it's a very involved topic we can talk for hours about this part of the ab test and also a topic that is asked a lot during the data science and product scientist interviews therefore I highly suggest you to check out this book about AB testing which is a HandsOn tutorial about everything you need to know about AB testing as well as check out the interview preparation guide in this section that contains 30 most popular AB testing related questions you can expect during your data science interviews so stay tuned and in the next couple of lectures we will cover the next stages of AB testing process if you are looking for one place to learn everything about AB testing without unnecessary difficulties but also with a good statistical and da Science Background then make sure to check out the AB testing course at lunch. a so if you want to learn all this background information including what is statistical significance what is AB testing how can AB testing be done and you want to have this endtoend AB testing course then make sure to check the AB testing for data science course at l. that's the only course that is available at the moment on the internet that covers the most fundamental concept of a testing including the theory and the implementation in Python without know the extra details and right going straight to the point in order to help you to Kickstart your Journey with AB Tes the resource that I would suggest you to keep by the hand is the blog called complete guide toab testing design implementation and pitfalls which is part of the HandsOn tutorials of the towards data science so in here and specifically this part where we are discussing the two sample that test I would suggest you to go through it as we are going to conduct this two sample Z test as part of our Python and we are going to learn how to implement this in Python in this book you can learn everything out there that you need to know about AV testing including different uh pits include of Av testing the process behind it how you can conduct the ab test end to end how you can calculate a sample size how you can choose a test the primary metric definitions different statistical test that you can use including the Ki Square test the two sample Z test and two sample T Test so given that as part of the lectures of the um AB testing and specifically lecture number five we have already discussed the two sample T Test and how to implement it I thought it would be more useful for you to know how to implement the two sample Z test such that you know both of them and you know their theory behind it and also how to implement them and finally if you are wondering how you can Implement them in Python then head towards my uh blog uh in the medium as well as my GitHub repository that I will post in the resource section where you can find all the different statistical tests you can use for analyzing your ab test results including the two sample T Test two sample Z test K Square test and much more so without further Ado let's get started with our demo so uh as you can see here I'm generating the data myself assuming that uh the uh primary metric follows bomal distribution so the output is in the form of zeros and ones because we are looking into the click event and click can be either zero or one and then I'm using here the binomial distribution to randomly sample from it and in case of the experimental version I'm using a probability of success equal to 0.4 and in case of the control version I'm using a probability of success equal to 0.2 because I want to have a quiet difference between the two groups and then later on we can also adjust this and we can change the difference to see how our it has behaves so um I'll assume that um the uh at the end of the uh data generation process we have a data that is similar to the form that you will get from the uh engine engers once they uh finish up collecting all the data from your customers and I will also assume that the Integrity of theab test is held which means that the observations who were in the control group they only saw the control version of the product and observations who were in the experimental group they only saw the experimental version of the product and let's actually go ahead and see how the data looks like so so as you can see here we are generating our data so the data is in this format so you can see that we have an observation in total we have 20K observations because we have two different groups each with 10K observations and then the first col describes the click event so we will either have a click or we will have no click and the primary metric is in the form of a click so we are measuring the performance of the product both control and the experimental with the same metric which is whether there is a click event or no click event and the primary metric is in the form of a binary variable so we have either zeros or we have ones Whenever there is a click then the corresponding value is one whenever there is no click then the corresponding value is zero and then we have the corresponding Group which helps us to understand whether the observation belongs to the experimental group so X or the control group which is a uh Co so uh this is how the data looks like and this also what you can uh expect from uh data Engineers uh once the uh AB test is conducted so you have run your ab test and Engineers have collected data assuming that the data Integrity has been kept and also that there was no systematic error when collecting and measuring the performance of the uh control and the experimental versions of the product first thing that we are going to do is to estimate the P hat control and a p hat experimental and for that what we need to do first is to count the number of clicks per group so we saw earlier that we have this data that we generate ourselves consisting of 20 K rows where 10 belongs to the uh control group and the 10K belongs to the experimental group and each consists of this click variable and the group The Click variable is an indicator uh that says that the observation clicked on the uh page versus uh not clicked on the page so whenever there was a click we have here one whenever there was no click we have here zero and then we have the corresponding uh group such that we can use to group this data based on the control versus experimental group and that's exactly what we are going to do as the first step in our process so we are going to calculate the number of total clicks for control group and for the experimental group so here we are making use of the function Group by in order to group this data frame so this data frame based on the group and then we want to click uh the we want to get the uh click variable and we want to sum this variable because the variable is of a binary nature so we have ones and zeros if we do the sum we are basically counting the number of times we have the uh observation click equal to one so by summing a binary variable we are simply getting the number of ones in that variable and that's exactly what we are doing in this part and then what is remaining is to get the uh number of clicks from control group and number of clicks from the experimental group by using this function code look so we saw earlier when we were discussing the um accessing of observations in a pendis data frame that there is a difference between iog and loog and the reason why we are using here the loog is because the uh group uh data that we are getting in here it will provide us an output where the index is in the format of a string so let's actually go ahead and print that part because I think it's an important part to see how the data looks like and it also will make sense why I'm using here the look function to access the uh control groups number of clicks and the experimental groups number of clicks so this is the uh group data frame that we are getting as you can see we are getting here the group and here we are getting for the control uh index the number of clicks is equal 2,924 and for the experimental group it's equal to 5,7 so then the next thing what we need to do is actually access this value and for that we need to specify that we want to access the value corresponding to the index equal to control and this can be done by using this log function so you cannot use ilog or any other way of accessing this because the index is of string type and therefore we are using the log so let's actually also add some print statements to make our code more readable so this will then print the number of clicks per control group and per experimental group here we go so as you can see we are nicely accessing the correct values then the next step is to calculate the P had control and the P experimental so basically the estimate of the click probabilities of the control group and the experimental group respectively and for that we just need to take the uh number of clicks and we need to divide it to the number of observations for that group so it is this part let's go ahead and calculate those values so as you can see I'm taking the number of clicks that we just obtained and I'm dividing it to the number of observations that we have defined in the very beginning here we go so as you can see for the control group the uh click probability is equal to 020 and in case of the experimental group is equal to 0.5 so we see that there is a large difference between the quick probability for these two groups which is um a reflection of what we saw here because we generated the data such that the uh success probability for the experimental group is equal to 0.5 and the um for the control group is equal to 0.2 so we see these numbers reflecting also in here and the reason for that is because we have sampled our data large enough and we see that the um probability so the the mean of our sample um converges in a probability to the mean that we use and this is also the idea behind the low of large num something that we have also discussed as part of the fundamental to statistic section of this course so the next thing what we need to do is to compute the P poed hat or the uh estimate of the pulled success probability and we saw uh when we were discussing the theory behind it that it's equal to the sum of the clicks for both control and experimental group divided to the total number of observations in both control and the experimental group so basically the P pulled head is equal to xcore control plus xcore experimental ided to the ncore Control Plus ncore experimental then the next thing we need to do is to compute a pulled variance and we just so that the pulled variance can be calculated by taking the pulled uh estimate for the click probability so this p p and then multiply by one minus p p head and then multiply by the inverses of the uh observ number of observations in each of the groups and there sum so 1 / 2 N Control Plus 1/ to an experimental so it can be calculated as follows so pulled variance then is equal to P ped head multip by 1 minus P ped head multip by 1 / to n control + 1 / to n experimental let's also add some print statements here we go and then the next step is to calculate the standard error so the standard error is the square root of the pulled variance so quite straightforward and here we are going to make use of the npy function so the SE is equal to npy Dot and a square roof is simply uh calculated by using the function sqrt which stands for square roof and then here we need to mention the pulled variance let's also add the print statement explaining the uh the code and this really can help your reviewer the code reviewer to understand what you are doing okay so now we have also the standard eror and now we are ready to calculate our test statistics so we saw that the test statistics is equal to the P control head minus P experimental head divided to the standard error and that's exactly what we are going to implement in here so as you can see the test statistics is equal to P control head minus P experimental head divided 2D SC so standard error and then finally what we need to do is to compute the Z critical value the P value and the confidence interval but for doing that we need to assume the significance level so usually this is done before conducting the test but here I'm assuming that before conducting the test there was a power analysis and as part of that we have decided that the statistical significance level is equal to 5% so let's add that here so Alpha is equal to 0.05 therefore we are going to use this specific Alpha so 5% in order to calculate our critical value coming from the normal table and to do this there are uh various options so one way of doing that is to hard code the value which I would not recommend but it is definitely uh an easy way to go if you um haven't used uh the python libraries to automize this process but here I will provide you the code and I will also tell you how you can use the scipi norm um function in order to calculate the critical volume and I think keeping the code as general as possible will help you in the term to because it can be that this time you're calculating the critical value corresponding to Alpha is equal to 0.05 but maybe next time you want to calculate the critical value when your Alpha is equal to 1% so you're interested in the uh case when your type one probability is equal to 1% so for those cases uh you want to keep your code as general as possible such that by changing your uh variable let's say Alpha you don't need to go each time and then in the chat GPT look for the a corresponding uh value coming from the standard normal table so for this what we are going to use is the norm function so the norm function come from the CPI stats library for that we need to import from ci. stats the norm function which stand for the normal distribution so in here what we need to use is the function called ppf which is the uh percentage Point function so the norm done ppf function stands for the present Point function and it's usually known as the inverse cumulative distribution function or the CDF of the standard normal distribution and it takes as an input the probability value and it Returns the corresponding value on the xaxis of the CDF once you provide a p so here we are providing the P which is equal to 1 minus Alpha / to 2 then this function calculates the X so the xaxis such that the probability of observing a volue less than or equal to two or 2 x in a standard normal distribution is equal to P so we have this inverse CDF and we have the xaxis and we have the yaxis on the y axis we have the probabilities and on the xaxis we have the X values so here what we are basically doing is that we are providing the probability that we have which is equal to 1 minus Alpha / to 2 and we want to know the corresponding X valume therefore it's also called inverse humity distribution function and in this way we can calculate Z critical value which can help us to identify the place where we need to have our rejection region and so here is the uh rejection region of this test and as you can see we have twosided test therefore we have also a two regions and whenever the um test statistics is larger than the critical value in the right hand side and it is smaller than the critical value from the left hand side then we are saying that we can reject the N hypothesis therefore it's also called the rejection region so uh once we calculate this set critical value we are ready to go to the next step but before that let's also add some statement print statement for readability here so the next step is to calculate a p volume and a p volue can be calculated by using the norm. SF function so the norm function comes once again from the scipi library and the SF stands for survival function the norm. SF function stands for a survival function and it stands for the complement of the CDF function so the cumulative distribution fun function of the standard normal distribution it calculates the probability of observing a value greater than a given threshold so in this case we want to calculate the uh probability that our test statistics will be smaller than equal the critical volume and as we saw that the standard normal distribution was symmetric here we are multiplying just one side of that probability by two in order to obtain our final value so here once we run this test we will finally get our P value and as you can see here the P value of the two sample that test that we got is equal to zero well now once we have the P value and also we know what is our Alpha we are ready to test for the statistical significance of our results so given that our P value is equal to zero and it's smaller than 0.05 so our Alpha we can state that the null hypothesis can be rejected and we can state that there is a statistically significant difference between our experimental version of the product and the control version of the product so this will help us to test for the statistical significance of our AB test however if you were for instance to have a different samples so let's say we would compute uh we would randomly sample from the binomial distribution so as you can see once we are getting the uh probability of the success the same for the two groups then the P value becomes large at least much larger than the alpha which means that we can no longer reject the ne hypothesis and we can no longer State there is a statistical evidence at the 5% statistical level that the control version is statistically significantly different from the experimental version and this uh verifies that everything that we have done here is correct so the ab test results analysis is accurate now the question is whether we um also have a practical significance once we pass the statistical significance test so let's move this back to what we had before so this is your .5 and once again the P value is just zero and let's go ahead and calculate our confidence interval such that we can test for the Practical significance and we can comment on the accuracy of the test and the general ability of our AB test so we saw that the confidence interval can be calculated as follows so we have the difference between the P had experimental and the P had control and then for the lower bound we need to uh subtract from this standard or multiply by Z critical value and then for the upper bound we need to do the same only with summing the standard multiply by Z critical volume so the difference here you might notice is this round function and the reason why I'm adding this is because I want to have nice numbers that will be rounded uh just three numbers after to decimal instead of having long uh floating numbers so once we go ahead and print this confidence interval we can also see the lower bound and upper bound in numbers here we go so as you can see we are getting a confidence info which is quite narrow so this is a suggestion that our AB test results are most likely accurate and that the Precision of our AB test is high and this is a good sign because then we can say that the ab test we have conducted in here is most likely generalizable to the entire population then the next question is okay do we have a practical significance or not and for that we do need the final assumption regarding the minimum detectable effect so let's say during the power analysis before conducting our AB test we got an mde which or let's actually call it Delta let's keep the Greek letters uh and the Delta let's say is equal to 3% so 0.03 well in this case we can notice that the Delta 0 03 so 3% is much lower than the lower bound of our confidence interval which is equal to 30% so 29.7% this means that in that case we would have said that there is a practical significance also but if the uh Delta would have been for instance the uh 0.31 so we have a 31% Delta then in that case the Delta is no longer smaller than the lower bound of our confidence interval and in that case we cannot say that our results are also practically significant so depending on the business and depending on the Assumption regarding the Del or the minum detectable effect we can then compare this to the lower bound of the confidence interval and we can State whether there is a practical significance or not in case there is a practical significance then we are good to go so we can say that we have a statistical significance we have a practical significance and we also have a narrow confidence interval which is a suggestion that our results are also generalizable and accurate so uh this completes our uh AB test results analysis and this is all that you need to do in order to have a valid and uh good quality AB test looking to elevate your data science or data analytics portfolio then you are in the right place with this AB testing and Trend case study you can showcase your AB testing and coding skills in one place I'm T Vasan data scientist and AI professional and I'm the cofounder of lunar Tech where we are making data science and AI accessible to everyone individuals businesses and institutions in this case study we are going to complete an endtoend case study with AB testing where we are going to test in a datadriven way whether it's worth to change one of our features in our ux design in the lunar text landing page this is a real life data science case study that you can conduct and you can put it on your resume in order to Showcase your experience in datadriven decision making where you will showcase your statistic skills experimentation skills with AB testing and your coding skills in Python using Library such as T models but also the pendas npy also metp lip and caburn we are going to start with the business objective of this case study then we are going to translate the business objective into a data science problem then we are going to start with the actual coding we are going to load libraries we are going to look into Data visual the data The Click data we are going to look into the motivation behind choosing that specific primary metric which is the clickr rate then we are going to talk about the statistical hypothesis for our AB testing I will also teach you step by step all the calculation starting from the calculation of the pulled estimate from the clickr rate and then a computation of the uh pulled variance the standard error but also the motivation behind choosing the searches statistical test that I will be using such as the two sample Z test and then how you can calculate the test statistics how you can calculate the P value of the test statistics and then use that with the statistical significance to test the uh statistical significance of your ab test after this we will also then compute the confidence interval comment on the general ability of the ab test and then at the end we will also test for the Practical significance of the ab test then we will conclude and we will wrap up and we will make a decision based on our data driven approach using the ab test to check whether it's worth it to change a feature in our ux design in the lunar text landing page so without further Ado let's get started so let's now start our case study in here I have in the left hand side this uh version of our landing page so which is our control vers version so to say the existing version where you can see that here we have start freet trial and here we got us our button secure free trial in the right hand side we got this new experimental version that we would like to have which is the Andro Now button so as we saw in the introduction what we are trying to understand is that whether our customers click more on the new version the experimental version versus the existing version the control version so uh um as of the day of uh loading this and uh conducting this case study our landing page uh has a secure free trial but what we wanted to test with our data is whether the uh enroll now is more engaging such that we can go from the secure free trial version to the enroll now version and uh here um for this specific case not only but also in general as we know from a testing is that when ever we got an existing algorithm or existing feature existing button then we are referring this group that we will um where we will expose this existing version of the product we are referring this as a control group so all the users to whom we will show the existing version of our landing page we will refer them as the uh control group participants and then we have the the right hand side our experimental version and our experimental users so the users our existing customers that are selected to be taken part um in our experimental group and in our experiment they will be then uh exposed to this new version of our landing page which contains this androll now button so our end goal in terms of the business as we saw in the introduction is to understand whether we should release the new button which will end up being high High engaging which means that we will have higher CTR or higher uh more uh clicks that will come from our user site which uh automatically means better business because we want to have highly engaging users if they are clicking on this button it means that it interests them more compared to the control version and uh if something on our landing page in this case our call to action is more interesting and highly engaging it means means that we are doing something right and our users might uh either make use of our free products or uh purchase our products or um just stay engag with us to keep real Tech in mind and whenever there is someone who uh is interested in data science or AI um Solutions or products then they can at least refer their friends if they are just clicking to understand and to learn more about our products that's also possibility so from a business perspective we therefore are using here as our primary metric uh our click through rate the CTR of this specific button which in our control version is the secure free trial and in our experimental version is the enroll now and what we want to understand is that whether this new button will end up having higher CTR or not because higher CTR from the technical perspective will translate to higher engagement from the business perspective so here we are making this translation from business versus technical um when it comes to AB testing we can have different sorts of primary metrics we can have a clickr rate as a primary metric we can have a conversion rate as a primary metric or any other primary metric what we want to have as our metric that will work as the single measure that will will compare our dra an experimental group to understand which version performs better is first to understand what this definition of better is and how that translates back to the business because if the engagement is what we are referring as Better Business for some reason and I will explain you in a bit why we think the engagement in this case is what we what matter for us at ler Tech then it means that clickr rate can be used as a primary metric this is just a universal metric that has been used across um different web applications search engines recommender systems and many other digital products to understand whether the engagement of that specific algorithm feature web design whether that is better or not and in this case in this specific case study we are also going to use the CTR because we are interested in the engagement so at luner Tech we really care about the engagement um with our users and we want our users to make use of our products but uh ultimately to engage with us because if they engage with us it means that our products are being seen our uh landing page is being visited and the user is actually interested to click on that button and then the action point and then to start either free trial or to enroll to see what is going on because all these are signs of Interest coming from the user side and in the control version as our click to action is to secure a free trial which directly uh lends the user to our free trial to our ultimate data science boot camp but given that we are expanding which means that we are now offering more courses we are offering free products and also we have uh Enterprise clients uh we have businesses as clients who want data science and AI Solutions and who want corporate training therefore we want to go from this Niche uh version of a landing page so secure free trial to enroll now because we already have a lot of Engagement in terms of the free trial we want to make it more General so that's the business perspective and on the other hand we also want to change beside of changing this um main um call Action we want to make it generalized and at the same time we want to see whether this generalized version will end up leading us um a higher engagement not only in terms of of the other products but also for the tree trial free trial itself because we always are looking for educating people and providing this free trial such that they can make use of our Flagship product which is the the ultimate data science boot camp so now when we understand why we care about the engagement here at ler Tech and we understand why we want to check whether this new button in our ux design will end up increasing the engagement or not we can now make this translation back to the data science terms because we know now from the business perspective All We Care is to understand whether this experimental version of the product is performing better or not but then this means that we need to conduct an AV test and we need to understand whether the ideas that we got and the speculation that the enroll now more General Button as so call you action will be better than the secure free trial version whether this is actually true or not from the uh customer perspective because if we want to call us a datadriven company we cannot just base our conclusions and our decisions for our products or for just in general for our product road map based on Intuition or logic we want this to be data driven which means that the customers are at the first place we are customer driven and our customers need to tell us whether the new um button is better or not and here we have conducted conducted an Navy test and um here I won't be using the real data I will be using the uh proxy data or simulated data that I generated myself and um this one contains the similar structure and this uh the same um idea of the data that we got when we were conducting our IB test and collecting this data and what is our business uh hypothesis in our business hypothesis we can say that we have at least temp % increase in our click through rate so 10% higher engagement when we have our enroll Now versus the secure free trial version of the product so this is our business hypothesis which means that our enroll now CTR so click through rate of the enroll Now button will result in at least 10% higher CTR than the secure free trial so there exists uh 10% at least 10% difference in terms of the engagement when we compare this new version of the product versus the old version of this new uh button and when we translate this back to statistical hypothesis we can say that under the N hypothesis we are saying that there is no statistically significant difference between the um control p and then P experimental which means the um um probability clickr probability clickr rate for control group versus experimental group so under AG n the null hypothesis we are stating what we ideally want to reject we are saying there is no difference between the experimental and control group CTR and under the alternative hypothesis so the H1 we are saying no uh we do have a difference which means that the uh control groups CTR is different from the control experimentals group CTR and one key part here is to mention that they are not just different but they are statistically significantly different so uh when it comes to starting the case study first things first is to load the libraries in this case study we are going to use a numpy we are going to use a pendas as usual for any sort of data analytics data size um case studies you always need those two usually pendis will be needed for our data wrangling to load data process the data visualize it nonp will be used to uh work with different arrays and part of the data then we are going to use the ci. stat uh model and from that we will import the norm function later on um we will see that we are using this in order to visualize this um uh rejection region that we get from for our test to understand whether we need to reject our null hypothesis or not then in this case study we also want to visualize our results and visualize our data for which we are going to need our visualization Library from python which are the curn and the med plot L let's look into our data so what we have in our data we have four different columns and of course this is a filter data that contains the information that we need but in general you can have a larger database you can have more sorts of um um matric matrices and uh different other Matrix but for conducting your ab test the pure AB test you actually need only the following information so you need your user ID to understand uh what are the user you are dealing with so it's the user one user two user 10 it can be that you have other way of referring to your users and uh those can be for instance this long strings that we use to refer to our user but given that our case is a simple one our case study we have just a user ID and this user ID is just a integers that go from one till uh until the end of our uh data and here we got in total 20,000 users therefore this number user ID goes to um 20,000 and those 20,000 um are all part of the user group which means that they are all users and they contain both the experimental and control users then we have our uh click variable and this click variable it's a binary variable which can be uh either one or zero where one refers that the user has clicked on the button and zero means the user didn't click on the button this is our primary metric for our AB test then we have the group reference which is this um string variable and this string variable helps us to understand whether the user comes from the experimental group or from the control group so this can contain only two different values two strings and it is X referring to the experimental and control referring to the uh control group if you can see here we got just this three letters X referring to the experimental group and then if we go in here because we have first the experimental and then the control ones you can see that here we got the uh control group now we have also some time stamp which is not something relevant so we'll be skipping that for now um given that this uh data that we have here it's not the actual data our data but it's a synthetic one but similar in terms of its structures in terms of the uh nature of variables and you can Implement exactly the same steps when you have your data and you are getting it from your ab test and then you are conducting your ab test uh case study so in here what we are going to make use of the most is our click click variable and the group variable because we want to find out per group what are the users that have clicked on the uh button and to be more specific we are looking for these averages so we are not so much interested that that specific user from that specific group has clicked on the product or not that's something that we can explore later but for now we are interested on the more high level so what is this uh percentages what is the click probability or click the true rate perir group and here we got groups of experimental and control as it should be in any source of ab test so once we have conducted our AB test then I will also provide you more insights on what you can do with your data especially with this user ID to learn more about uh the idea behind these different decisions or whether your ab test is different per group but the idea is that this AB test that we are conducting by following all the steps and by ensuring that the uh pitfalls are avoided that we are making a decision that um represents the entire population so we are using a sample that is large enough for us to make a decision for our product and for our business that will be generalized and will be a representation and representative when we apply this decision on our population so let me close this part because we no longer need this and let's go ahead and load this data so here I'm using the pendus library and the common uh approvation of PD and I'm saying pd. read CSV and then I'm here referring to the name of the data that contains my click data and here you can see that dat that that data is here so abore testore click uncore data that's here PV and I will be providing you this data because you won't have this in your own Google collab you will have the link to this Google clab and I'll provide you also the data such that you can put that data you can download it first from my source and then load it in here by using this specific button in here and by doing that you can then go to that specific folder where you downloaded the data and then you will have also this uh corresponding CSV file in your folders so once you have that then you will uh smoothly run this code and uh here I'm loading that data and putting under the name of DF abore test so basically the data frame containing my ab test click data what I want you to do is to Showcase you how the data looks like so here you will see the header given that here I haven't provided any argument it just looks at the top five elements so the top five rows and here I got only the first five users from the experimental group I see that some of them have clicked some of them didn't click and the corresponding user ID and the Tim stamp uh that they um done the click action then um when we look at the describe function you can see here that this gives us more general idea uh of uh what the contains not so much what the top five rows just look like which is great in terms of to understand what kind of data you are dealing with with what kind of variable you have now you can see more the uh total uh picture so high level picture what kind of um data what amount of data you got so the descriptive statistics so here we can see that in total we got 20,000 of users included in this data so 20,000 observations 20K rows and we have the mean for the user ID of course it it's not relevant the mean is 10,000 and um this is an interesting number so we see that um average click uh when we look at both user and control the experimental and control groups it is 40% so 0.40 uh 52 so 40. 52% however this is not what we are too much interested so this is not to be confused with the click through rate perir group what we are interested is the click through rate or the mean clickr um when it comes to the experimental group and the control group so then we have our standard deviation we see a high standard deviation which is understandable given that we have this uh large variation in our data we got a control group and experimental group and this Vari shows that we have a huge difference in the these different values uh when it comes to the click event and then we have the mean and the maxim which doesn't give us too much information because the click event so the click variable is a binary variable it contains the zeros and one so naturally the minimum will be the value zero because the click can take value zero and one and the largest one is of course one which means the maximum will be one and then for the rest the 25% so the first quantile the second quantile the 15% which SS the median or the third quantile the 75th percentile is not that much relevant so when it comes to the descriptive statistics for this kind of data especially if it's filtered is not super relevant but if you would have a larger data more matrices beside of Click which is your primary matric but you all have also measured some other Matrix which is recommendable then you would see more um values which would be interesting to look at so not only to look at the click rate but also to look at for instance the mean or maybe the median of conversion rate or the uh mean uh amount of time the average amount of time the user has spent on your landing page or how much time did that user end up spending before making that decision of a click those can be all very interesting Matrix to look into from the product uh data science perspective to understand the decision process and the channel and The Funnel of these clicks but for now for our study what we are purely interested is in our primary metric which is the click event so what we can also see in here is that we got um uh in our group um when it comes to the control group we got uh 1,989 users out of all uh control users that end up clicking versus the experimental group where we have 6,6 users who did click so do not confuse this with the total amount of users per group this amount is the um grouping of the uh data so using the group by and then group so we are grouping that data per group and we want to see per group what is the sum of this variable sum of the clicks and given that the click is a biner variable we know from basics of python that we are basically accounting the number of click events because if you got a binary variable containing zeros and ones if you do the sum of the clicks adding the zeros do not doesn't have any impact which means that um you end up just summing up all the ones to each other and then you end up getting the number of or the total amount of uh cases when this click variable is equal to one so in this case when there is a click event therefore we can see that per experimental group we got 6,16 uh users out of all the experimental users that end up clicking and then out of control group this amount is much lower so we end up having uh only 989 users clicking so let's now go ahead and visualize this data I want to showcase in a bar chart using this clicks what is the total number of clicks so I want to show the distribution of the clicks when it comes to um the uh click event pair group and here I want to uh see next to each other the experimental group and control group and as you can see here here we are getting our bar charts and the yellow corresponds to the no which means that there was no click versus the uh black corresponds to the yes which means there was a click so whenever you see this amount it means that that amount corresponds to no click no engagement from the user side and this is per group so this is what we are referring as a click distribution in our uh data in our experimental uh and control groups and the way that I generated this bar chart is by first creating this um uh list that will contain the colors that I want to assign to each of my groups and I'm saying zero corresponds to the yellow and one corresponds to Black which means that if my variable contains amount of zero in this case my click is equal to zero it means that I don't have a click so it's a no and this I want to visualize by yellow otherwise I have a black which means that um the um one corresponds to the case when we have um click and in this case we will get a black as you can see here the uh yes which means a click is um visualized by this black color and then what I'm doing is that I'm initializing this uh figure size by saying that I I want to have a figure size of 10 and six you you can also skip it but I I think it's always great to put the size of a figure to ensure that you are getting the size like you want it to be such lat you can also download or take a screenshot then we have this uh here I'm using as you can see a combination of the Met plot leap. pip plot Library as well as the uh cabbo because CBO has much nicer colors and here here I'm saying uh we are going to uh make use of the curn to um create um count plot because we are going to count and we are going to showcase the counts per group uh what is the number or the count of the clicks versus no clicks for a group called experimental and what is the number of um or the percentage of clicks versus no clicks when it comes to the group control and then here I'm specifying that the Hue should be on the click which means that we are looking at the click variable and we are going to use the data dfab test which means that we are going to look in this data from here we are going to select this specific variable called click and we are going to use this in order to group our data based on this group so you can see that we are doing the grouping on the variable called group so the argument is called x x is equal to group we're grouping our dfab has data on this group and we are going to do the count in our count plot based on this variable click basically what I'm saying here is that go and group our data dfap test based on Group which means that we will group based on experimental versus control and then I'm saying go and count the click events count pair group so pair experimental perir control group what is the number of times when we have a no so we have a zero and and what is the number of times when we have a yes or we have a one as a value for click variable and then as a pet I'm using my custom pet that I just created which should be in the form of a list as you can see in here if I would have here also my third group or fourth group then I of course need to extend this color palette because I need to have the same amount of colors as the number of groups pair my target variable in this case The Click has only two possible values 0 and one which means that I'm only or only specifying the two colors in my list so then we have the title of our plot always nice to add by and then we have our labels which means that I want to emphasize uh as my X label so here I want to have my group you can see here is my group because I will either have group experiment or control that's my variable on my xaxis and on my y AIS of course I have the the count so I'm counting the number of times I got uh the uh no click versus click event so here note that the um y AIS is in terms of this count so here you can see it's uh 8,000 here sa 7,000 or 6,000 5,000 which means that we are talking about the numbers and the counts rather than percentages and this is important because um another thing that I'm also doing is that I'm going the extra Mile and I'm also adding beside of this counts on the top of each bar I I want to visualize and clarify what are the corresponding percentages it's always great to enhance your data visualization with some percentages percentages is easier for the uh person who follows your presentation to understand for instance if you got an experimental group and the the users is here 6,000 and um 4,000 they might not quickly understand that you got for instance in total 10,000 of users and then 6,000 has then uh clicked and then 4,000 didn't click so um then the idea is that by adding these percentages we can then see that 61.2% has clicked in this experimental group and 38.8% has not clicked of course this a simulated data I specifically pick the extreme in such way that we can clearly see this difference in the clickr rates but um in the reality you can have a clickthrough rate of 10% up to 14% which is usually a good number if you have a clickthrough rate of 40% is great but it's really depend on underlying user base what kind of product you got how large is your user base because if you have very large user base then 10% can be a good clickr rate versus if you have a very small user base Maybe 61% is considered uh good or average so uh in here we have just a simulated data of course and I've have added these percentages uh by using the following code so I won't go too much into detail in here um uh feel free to check and see uh and if something doesn't make sense go back to our python for data science course that contains lot of information on the basics in Python but here just quickly what I'm doing is that I am uh calculating the percentages and I'm annotating the bars so I want to know what are these percentages which means that per group I want to take the total amount of clicks I want to understand what is number of Click event when the click variable is equal to one and what are the number of cases when there was no click from the user s which is what are the number of cases when the click variable is equal to zero and then I'm counting those amounts and then using the total amount to calculate the percentage for instance in this specific case I'm filtering the data for experimental group I'm looking at the total number of users for this group which is 10K and then I'm counting the number of times when out of this 10,000 users the amount of users that end up clicking on that button which is the click is equal to one case and then I'm taking that number dividing it to the total number of users for this experimental group multiplying by 100 in order to get that in percentage and this is the calculation that you can see in here one thing that is important here is that here I'm using this um uh percentage um so for the current bar I'm saying U as a way to identify whether we are dealing with experimental or control group is by getting by looking into this uh p and uh this p in here is the basically the patches so in this case I'm basically saying if if I'm dealing with experimental group then go ahead and calculate what is this uh total amount of observations and then take what is the uh number of clicks and then divide the two numbers uh C multiply this with 100 and this will then give us the percentage and then I'm doing this for each of those groups so I'm doing it for this group I'm doing for this group and for this one and for this one so I got two groups but then within each group I got clicks and no clicks and I'm calculating these four different percentages and then I'm adding these percentages on the top of those bars so I not only want to have numbers repr presented in my visualizations but I also want to add this corresponding percentages at the top just for visualization purposes I wanted to put this out there because this can help your uh data visualization toolkit and it also will um make your audience from your presentations be more thankful to you when you are telling the story of your data so uh this is about the data that we have we see that uh 38.8% of our experimental group users have not clicked on the button versus the 61.2% have clicked on the button based on the simulated data and then uh in the control group we have a quite the opposite situation we got the majority of the users 80.1% not clicking on the button versus the remaining 19 .9% have actually clicked on that button so we got a huge difference a dis anounce when it comes to the experimental group and uh control group this kind of gives us an indication hey something is going on here we kind of uh have already um higher level intuition what the remaining analysis will look like um which is that there most likely will be a difference in their ctrs when it comes to the uh the um uh control versus experimental group and the corresponding buttons but uh hey let's continue that's the entire goal behind a testing is to ensure that our intuition our conclusions are all based on the data rather than on our intuition so what are the parameters that I'm using here for conducting our AB test when I was designing this AB test uh the first step step was to of course do all these different translations that we learn as part of our AB test course um conducting it properly which means coming up with this three different parameters when doing our power analysis and usually this should be done when you are collaborating also with your colleagues and uh with your product managers or your product people domain experts because they have um a lot of information on what it means to have um threshold that you need to pass in order to say that for instance this new version of your feature is different and is uh considerably uh different from the existing one and here um in order to for us to understand this uh and make these conclusions we need to come up with the three different parameters that can help us to properly conduct an AB test as we learned when we were looking into designing a proper AB test so so first we have our significance level the significance level or the alpha the Greek letter that we are using to refer to the significance level which is also the probability of the type one error and that amount we have chosen following the industry standard which is 5% given that we didn't have any uh previous information or specific reason to choose a different significance level so lower or higher we decided to go with the industry standard which is the 5% this means that we want to have um we want to compare our P value of our uh statistical test to this 5% and then say whether we have a statistically significant difference between the control and experimental group based on this 5% significance level and let's refresh our memory on this Alpha this Alpha uh or significance level is also the probability of type one error so this is the amount of error that we are comfortable making when we um reject the N hypothesis well the null hypothesis is actually uh true which means that we are detecting a difference between the experimental and control version while there is no difference and we are making that mistake and here we are saying that we are fine and we are comfortable with making this mistake at a maximum of 5% but higher than that it's not allowed we are not comfortable making uh error um higher than 5% then the next variable uh in this case the B uh or beta the probability of type two error which is the opposite of the type 1 error which is a false negative rate or the amount of time the um um proportion of time when we end up failing to reject the null hypothesis while null hypothesis is false and it should have been rejected then the 1 minus beta is actually power of the test so what is the amount of time we are correctly rejecting our null hypothesis and correctly stating that there is indeed a statistically significant difference between our experimental group and our control group so we have chosen for this the uh industry standard as well which is the 80% but given that for your results analysis in this case for conducting this case study that part of the power analysis is not relevant we use that when calculating our minimum sample size but we don't need that when conduct our results analysis therefore I'm not initializing that as part of this code so here I'm only providing to my program the values for my significance level which is 0.05 or this is the same as 5% and then the Delta which is the third parameter and this Delta is our minimum detectable effect so this a Greek letter Delta which is the minimum detectable effect helps us to understand whether beside of having this statistically significant difference whether this difference is large enough for us to say that we are comfortable making that business decision to launch this new button so it can be that when we are conducting an AB test we are finding out that the experimental group has indeed higher engagement than the uh control group and we are uh getting a small P or at least is smaller than the alpha and we are seeing that P is small than the alpha level which means that we can reject the null hypothesis and we can say that the CTR or the clickr rate of the experimental group is statistically significantly different from the control group at 5% significance level but we know from the theory of ab test that only that is not enough only statistical significance is not enough for the business to make that important decision to launch an algorithm or to launch a feature in this case to change change our landing page the button from the start free trial to the enroll now which means that we want to have enough users and we want to have enough difference large difference in our click through rate or enough users saying that we are more happy with this uh new version of the landing page for us to go and change our feature and what is the definition of enough what is the difference in the click through rate that we need to detect after we have detected the statistical significance in order for us to say that we also have a practical significant so practically we are also comfortable making that business decision and then launching this new feature and changing our landing page button and that is exactly what we have under our Delta this minimum detectable effect in this case we have chosen for Delta of 10% so you can see here 0.5 this is 10% this means that our Delta or the MD the Min detectable effect is 10% this means that we are saying not only we should have a statistically significant difference between the experimental group and control group but also we need to have this difference to be at least 10% which means that we need to have detected that the experimental version of the landing page results in a at least 10% higher quick rate compared to the control version for us to go ahead and to launch this new version and deploy this new uh ux uh feature so this is really important because many people go and check for statistical significance so they do their Alpha and then check uh whether the P value is more the alpha and then say hey we have a statistically significant difference and then they are done with that but that's not correct after you have conducted your uh statistical significant analysis and you have detected that your uh experimental version has a statistically significant different um CTR than the control version at your Alpha significance level the next thing you need to do is to ensure that you also have a practical significance beside of the statistical significance and this practical significance you can detect and you can check when you use your mde or your Delta and you compare it to your confidence interval that you have calculated something that we have also learned as part of the theory of conducting a proper AB test but once we come to that point so after we check for our statistical significance I will also explain how exactly uh we will need to do this check and at the same time we will also be refreshing our theory on the Practical significance so let's now goad head and calculate the total number of clicks per group by summing up these clicks and I also want to calculate and group by this amounts just to Showcase how you can do that on your own so here what I'm doing is that I'm taking my ab test data I'm grouping by by group group is the uh variable that contains the reference where we are dealing with experimental group or control group and as you know from our uh python series and demos python for data science course that uh whenever we want to group that data a pend this data frame first we need to say pendas data frame name. Group by Within parenthesis the variable that we are using to do the grouping which is in this case group and then within Square braces I want to emphasize and put the name of a variable that I want to um apply operations on so I want to group my data on the group variable and I want to count the number of times I have a click in my control group and in my experimental group this will be my X control and X experimental variables so X control will then compute contain information about number of Clicks in my control group and then each experimental will contain the number of Clicks in my experimental group and given that um I want to refer to the name of that uh Group after I did my grouping so I am getting this kind of this shape of data frame of course I then need to uh use my do log function in order to properly call that amount so to understand what is this amount corresponding to this index and what is this amount corresponding to this index and given that my index is in strings I'm then using here mylog function something that we also learned as part of our python for data science course so here is basically the printing just writing nicely what are the results which means that we are counting that the let me count again that the uh number of uh clicks for my control group is 1,989 so you can see that it is want to double check and see what we got yes so we got same number so we are dealing with the same data set just to to make sure and here the number of clicks for experimental group is equal to 6K and 116 so 6,116 clicks so then we are calculating the uh pulled estimates for the clicks per group let me quickly fix this typo so calculating the uh pulled estimate for the clicks perir Group which means the P estimate for the experimental group and for the uh control group so let me quickly at here how I can calculate the uh total cases when we got uh experimental group users so what is the number of users in the experimental group and what is the number of users in the uh control group so here what I want to do is that I want to say that the group The DF test group should be equal to expermental and this of course should be my filter and I want to count this and let me quickly copy this I saw that is already under the control so here I'm changing to the control and this will need to give me the number of users in each of these groups too so number of users in control and number of users in Click and here I will simply check this so I will print then the number of user users per group and at the same time I will also click the number of clicks per group there we go so now when we have done this what we are ready to do is to go ahead and calculate the F estimate for clicks perir Group which means pair control group and perir experimental group for that what we need to do is to take the number of clicks of the control group divide to the number of all users for control group as you can see in here x control / to n control and we are referring to this variable as P control head because we know that the estimate of this click probability um is always with a hat it's just the way that we reference it in um statistics and in ay testing so this is the estimate something that we are estimating therefore we are saying hat and then we have the same for experimental group which means that the estimate of the experimental groups uh click probability is equal to X exp and then divide it to n x then um in order to calculate the uh pulled estimate or uh pulled click probability which means the value that will describe of the uh control group and experimental group we need to follow this formula which means that we are taking the ex control we are adding to that X control the X experimental this is our nominator of our uh value and then we are dividing this to the uh sum of the sizes of each of those groups which is n control and N experimental so this is the common formula of the pulled estimate uh when it comes to this type of experimentation when you are dealing with um primary uh metric that is in the form of zeros and ones and if you want to refresh your memory on this type of formulas then make sure to also check our AB testing course because in there we go in detail in this uh specific lesson of the uh AB test result results analysis we are looking into this uh all this formulas on how we can calculate the pulled estimate of this uh click probability so click probability but then we are calling it P click for probability and then what we got is this volum so that amount is then 040 this number should look familiar because this is then the mean that we saw when we were looking at the um uh descriptive statistics table if you can recall this table let me see this number so now basically we are then calculating this manually because we need a variable that will hold this uh value so it is simply summing up all the clicks for control group and experimental group to get the total number of clicks and we're dividing it to the total number of users so n Control Plus n experiment so now when we have this we are ready to also calculate what we are referring as a pulled variance also something that we have learned as part of this theory for AB testing so the pulled variance is equal to the pulled estimate of the clicks so P had something that we just calculated multiplied by one minus P head so the uh click event the estimate of the click probability multiplied by the estimate of no click and we know already this IDE of berola distribution that the variable that uh describes this process of clicks and no clicks follows kind of this idea of B distribution when we have a click and no click so we have probability of click and then we have probability of no click which is the one minus that click probability so that's the idea or the part of the formula that we are following as kind of an intuition and then this multiplied by 1 / to n control+ 1 / to n experimental so here I'm purely following the formula for the pulled variance if you want more details and explanations and sure to check the corresponding Theory lecture because we are going into details of each of those formulas and understanding why we calculate this um P variance and P estimates uh in this specific way and using these specific formulas so here by just follow following the uh formula I'm getting that the uh pull uh variance is this amount so this is in nutshell how I calculated my uh pull click probability and a pulled variance of that click event and we are going to need that in the next very important step which is calculating the standard error and calculating the test statistics because in this case what we are doing is that we are dealing with a case when the primary metric is in the form of zeros and one so we let's Now quickly talk about the uh choice of a statistical test be uh before conducting the actual calculation of standard error in the test statistics so here I went for the two samples at test and let me explain you why and what is the motivation because as we learned as part of the theory um whenever we have a primary metric that is in the form of an averages like we have now because we are using the P control head and P experimental head head so we have a primary metric that is the uh click through rate which is the average clicks per group so we have calculated the average click per experimental group and control group then the primary metric the form of it already dictates given that it's in averages that we need to look at uh either parametric test corresponding to this averages or nonparametric test corresponding to the um averages in this case I went for the parametri case because uh it has Better Properties if I have this information about the distribution of my data and why do I have this information and then this also dictates the uh choice of my um statistical test well I have a size of my sample which is over 100 and actually over 30 that's the threshold that we tend to use in statistics and in a testing in order to say whether we have a large size or large data or not if our sample is not large so it contains less than 30 users per group which happens as well then we say that we need to go for um statistical test uh that will be specific for this kind of cases because we can no longer make use of the uh statistical theorems like the central limit theorem which helps us to um uh to take the uh to the inference so to make use of the inferential statistics and make conclusions regarding the distribution of our population just having the sample and what do I mean by that so if my sample is larger than 30 like in this specific case I got 10,000 users per group so it is definitely larger than 30 uh users then in that case I can say that by making use of the central limit theorem I can say that my sampling distribution is normally distributed and this is simply making use of the central limit theorem something that we have also learned when we were looking into this concept of inferential statistics as part of the fundamental statistics course uh course um in lunar Tech so this is a powerful theorem that we use in AB testing in order to make our life easier because when we have a sample that is larger than 30 for each of these groups then we can say that even if we don't know the actual distribution or the name of the distribution that our uh sample follows where it comes to the click um event so the random variable that describes this number of clicks or the average click true rate what is that um distribution exactly but given that we have that this uh size is large enough it's large than 30 users we can say that by making use of the central limit theorem we can say that the uh the uh sample distribution follows a normal distribution if given that the sample size is large enough and this helps us to say that well in that case it doesn't matter whether we make use of the two sample Z test or two sample T Test we can make use of either of these test in order to conduct our analysis and we had this specific template to make this choice easier uh in our AB test course at the loer tech where we were making all this decisions and saying if the SLE size is this we need to do this if the SLE size is this we need to do this and in this specific case following that exact structured and organized approach I ended up seeing that my sample size is large so it's larger than 30 so I can then make use of the central limit theorem I then know what is the random um what my random variable describing this click through rate um follows the kind of distribution in this case a normal distribution and then this means that whether I use a t test or Z test doesn't really matter I'm going to end up with the same conclusions therefore I will just go with the two sample set test simply because um it is just easier for me to do for example you can also go with the two sample T Test and you can even change this case study and tweak it and then make make it your own and put it on your resume in that way by making it more unique and that will be totally fine because you will see that you are going to end up with exactly the same conclusions as we do in this specific case study because if you have a large enough sample it won't matter whether you have a two sample Z test as your parametric test or the two sample T Test and um if you want to know why why this matters and all the different details statistical insights make sure to check the actual uh course dedicated to AB testing because there will be cover this all and you will then become a master in the field of AB testing now we know this uh decisions and the motivation behind choosing the uh two samples that test let's now go ahead and do the actual calculations so here we have a standard error which we calculate by taking the pulled variance and taking the square root of it and this is again using the idea of this formulas that we learned as part of the ab test so we are using this P variance taking the square root of this which gives us the standard error and the standard error as you can see in here is then equal to 0.69 29499 this amount there we calculate our test statistic for our two sample at test so the test statistic is equal to P control head minus P experimental heads divided to standard error so here uh you can now see the motivation behind not only Computing the P pulled head but really also the p uh control head and P experimental head and then I take the P control head and subtract the P experimental head and I divide it to the standard error to compute my test statistics once I did this as you can see this is this amount so test statistics for our two sample that test is this amount minus 5956 around it then um we can also compute the critical value of our Z test which is uh by using this Norm function that we uh loaded in here from the S and this will help us to understand what is this value from our normal distribution table the standard normal distribution table uh where by making use of this table we identify what is this critical value that we need to have to uh create our rejection regions and to say whether we can uh reject our n hypothesis or not so to conduct our test we need to have a critical value for uh to which we will compare our test statistics and this critical value will be based simply on the standard normal distribution so this is this norm. ppf and then uh probability um uh function base basically uh the the probability function that comes from the normal distribution standard normal distribution and as you can see it is correspond specifically to this percent Point function which is the inverse of the cumulative distribution function so this based on the alpha / 2 so 1us Alpha / 2 is the argument that we need to put for our percent Point uh probability function and why divide it to two because we have a two sample test so because we have a twosided two sample test sorry so if you want to understand this difference between uh two sample um test twosided Test please check out the uh fundamentals to statistics course at ler Tech because we cover this uh Topic in detail and it's a very involved topic it contains many complex stst U from statistical point of view so I won't be spending in this case study too much time on that here I'm assuming that you know this formula already but if you don't and if you quickly need to do your case study May testing feel free just just to copy this line which basically is a value that we need based on the corresponding chosen statistical significance level that we need to compute to compare our test statistics so our test statistics is this value and the value that we need to compare it to is the Z critical volue so we can see that this critical value is then equal to 1.96 this is actually a very common value that we know even without looking at a standard normal table when you make use of this test enough often then you know that the uh critical value corresponding to a twosided test when it comes to normal table is equal to uh 1.96 this is just the value that we know and in here by even without calculating the next step which is a P value we can even say already what is the decision we need to make in terms of statistical significance because we know that one way we can test our hypothesis statistical hypothesis is by Computing the test statistics and checking with the test statistics the absolute value of it is larger than the critical value and we see that the test statistics is equal to minus 5956 the absolute value of that is 59 .56 and that value is much larger than our critical value which is equal to 1.96 this already gives us an idea that we can reject our null hypothesis at 5% statistical significance level but I want to uh go on to the next step actually because that's um more structured more organized way to doing and conducting experimentations as in the industry we tend to make use of the p values instead of making use of this econometrical approach and statistical approach of um testing the statistical test so once we have calculated our test statistics the next thing we need to do is to calculate our P value and then use that P value compare to the significance level Alpha and then make a decision whether we need to reject our n hypothesis and say that we have a statistical significance or we cannot reject our n hyp hypthesis and then we need to say that we don't have a statistical significance so we don't have enough evidence to reject anal hypothesis so the idea here is that we need to make use of our uh normal function and specifically the norm. SF so making use of exactly the same Library the norm from CI do DOS and then this time we're using the survival function which is the one minus the cumulative distribution function of normal distribtion this comes again from statistics and then using the absolute value of our test statistics multiplying it by two given that we have a twosided test I'm calculating my P value this is simply by making use of the same formula that we saw when we were uh studying theab test from a technical point of view because we learned that the P value is then the probability that Z will be smaller than equal the minus test statistics or that the test statistic is smaller than equal to Z so uh we basically want to calculate what is this probability the P value which is equal to the probability that our test statistics will be smaller than the critical value or our negative of the test statistics will be larger than equal of the critical value and we want to know this probability because what this probability represents is that what is the chance that we will get a large test statistics well this is due to a random chance and not because we have a uh actual statistical difference between the clickr rate of the experimental group versus control group so this is the idea behind P value so what is this chance that we are uh mistaking this random mistake this random observation that we got a large test statistic and saying that there is a statis significance well there is no such thing and we are purely getting this large test statistics um because of the random chance if the probability of getting a large test statistics by random chance is small so if this P value is small then we can say that we have a statistical significance that's the idea behind it and this P value when we calculate uh we are storing it in this variable called pcore volume and then the next thing what I'm doing is that I'm writing this function quote is statistically significant which takes argument as P value in Alpha so I just need the P value that I just calculated for my test Set uh test uh statistics and then I want the statistical significance level that I want to use for my test and then this is the value that comes from my power analysis as I mentioned before that's the 5% this P value I'm calculating for my test statistics so in here and then I'm taking the two and I want to compare them so I want to assess whether I have a statistical significance by comparing my P value to my statistical significance level Alpha and what is this comparison well we know uh from the theory that um if we have a low P value and specifically in the P that we are getting the P value is small than equal the 5% or 0.05 which is the significance level then this indicates that we have a strong statistical uh evidence that uh the N hypothesis is false and we need to reject it so we have a strong evidence against the null hypothesis and otherwise if the P value is larger than 0.05 so it's larger than 5% that we have chosen as the maximum threshold of that mistake so the significance level is uh uh no longer the largest element but the P value is larger than your significance level then this indicates that you don't have enough evidence against the N hypothesis so your evidence is weak this means that you fail to reject the N hypothesis so this is what I'm doing in here with this code so I'm saying print the P value first and we are rounding it up with this round function I'm rounding it to the three decimal and then I want to check and Det determine whether I have a statistically significant or not and the way that I'm doing that is I'm saying if my P value is more than my alpha or actually let at smaller than equal than Alpha then we can print that there is a statistical significance which indicates that the observed differences between the experimental and control groups are un unlikely to occur due to random chance which means that this is not random chance and uh we have a strong evidence that there is a statistical significance and this suggests that this new feature that we got this new version of our landing page with this um uh call to action um as the and now is better and result in higher statistically significantly higher clickr rate than the existing version of the control uh group so there is a real effect then otherwise if this is not the case which means that my P value is larger than my Alpha then I'm saying print that there is no seral significance and that the observed difference that we see in the clickr rate is not because uh of the real difference in the performance but TR truly this is just the random chance so here we can see that once we run our we call the function in here which is simply the function name and the argument so P value and alpha alpha comes from the initialized value that we had from our power analis so from here we initialize this value 0.05 and then here we got the P value that we just calculated then what we are getting in here is that our P value is actually so small that it's um rounded to the zero so what this means is that that there is evidence that suggest that at 5% statistical level significance level that the uh clickr rate of the experimental group is different from the clickr rate of the control group note that I'm not saying higher or lower because our statistical test was twosided so under new hypothesis we had that the uh P control so in here as you can see our P control was equal to P experimental and under the alternative we had that the P control is not equal to P experimental this means that we um have now how rejected the null hypothesis we have found evidence that suggests that the null hypothesis can be rejected since our P value is zero and it's smaller than the statistical significance level 5% and this means that we can reject the hle and we can say that uh there is enough evidence to say that P control is not equal to P experiment and given that that we saw from the uh visualization and from our calculations that the um clickr rate for our experimental group is much higher then the click rate of the uh control group we can also say that we have found evidence that at 5% significance level we have found out that there is a statistically significant difference between the experimental and control groups clickr rate and that the experimental groups clickr rate is actually higher so statistically significantly higher than the control versions clickr rate so this is really important because this suggests that this difference in their click to rate is not due to random chance Alan but truly that there is evidence statistical evidence that can support this hypothesis that there is a true difference between the performance of the experimental version of the product so in this case in our case the landing page that has enrolled now button versus the control version of the product which had the uh start free trial version of the landing page the existing version so beside of calculating this P value it's always a great practice to also visualize your results and this is great for your audience who are technically sound and who know uh these different concepts and you want to visualize uh the results that you got not only by showing some number that is the P value and say hey I have a statistical significance but you also want to showcase the actual picture of what you got what is your test statistic what is the significance level that you use to kind of tell a story around your numbers and that's the uh art behind the data science I would say so let's go ahead and do some art so what I'm doing here is that I am making use of my standard normal distribution or the gaan distribution the way that we are referring to the standard normal distribution in statistics I'm saying that my mean or the MU is equal to zero my Sigma is equal to one which is my standard deviation and I'm saying that my uh I want to now plot my uh standard normal distribution by getting my uh X values which are the uh number of uh X elements that I want to have my xaxis and then taking the PDF or the probability distribution function for the normal distribution by using the CP Library I'm then providing my my X values for which I want to get my uh corresponding uh values of Y so basically here are all the values between let's say minus something minus three and then so between 3 and three and I want to find all the Y's corresponding to this which basically plus the probability distribution function of the goian distribution or the standard normal distribution and then I want to add to this graph all also the uh corresponding rejection region and as you can see it is here so then what I'm adding here by using this part of the plot is that I want to fill in the rejection regions so I'm saying for all the values in this figure whenever the uh value is lower than that threshold in this case the threshold is z critical 1.96 so whenever my threshold is smaller than minus this uh 1.96 and larger than this 1.96 then we are in the rejection region we are saying then if my test statistics is falling in the rejection region in this case you can see that we are in the far left so the test statistic is minus 59.4 and it's much lower than this threshold as you can see in here this is this left Blue Line in here then in this case it falls in this rejection region so actually this entire thing is the rejection region it starts from here and it goes all the way to here anything anything in this region means that we need to we have a test statistic fully in the rejection region which means that we can reject to n hypothesis if we were to get a test stais that is very large and very positive it means we would be in this part of the figure and again in the rejection region anything above this line is then uh going under this category of rejection region and also anything in here so for anything in here we are in the rejection region being in the rejection region it means that we can reject the hypothesis and we can say that we have a statistically significant results so now when we have our statistical significance it's always a great idea to go on to the next step and it's actually mandatory to do this because not only a statistical significance is important but also the Practical significance as I mentioned in the beginning of this case study so for that what we are going to do is first we are going to calculate a confidence interval of the test and this confidence interval will help us to first of all make um comments regarding the quality of our test and its generalizability uh at our entire population and the accuracy of our results and then we will use this confidence interval to make a comments and to test for the Practical significance in our AB test so let's go ahead and calculate the confidence interval so as we learn as part of our lectures the confidence interval can be calculated by first taking the uh P experimental head and P control head and the standard error and the Z critical so here we need the two different estimat of the experimental groups click through rate and the control groups click through rate we also need the standard error of our two sample Z test as well as the critical value and then we need to First calculate the lower bound of our confidence interval and then we need to calculate the upper bound of our confidence interval and in this case uh given that the um statistical significance level we are using is Alpha uh the uh Z critical is based on that therefore for we are also saying that we are calculating the 95% confidence interval so in here the way we will calculate the lower bound is by taking the P experimental head subtracting from that the P control head and then once we have done that we then substract from that the standard error multiply by Z critical volume and we are just rounding this up up to the three decimal behind the zero then we are doing the same thing only with a plus sign in here for the opp upper bound calculation of the confidence interval so this is just pure following the formula of the confidence interval that I will set you here and let's go ahead and print this value which is this interval so what we are seeing here is that we have a confidence interval that is from 0 399 so 0.4 to 0 uh 43 so quite a narrow confidence interval I would say which is actually a good sign because this confidence interal that provides this range of values within which the true difference between this control and experimental groups proportions or the clickr rate is likely to lie within a certain level of confidence in this case 95% confidence this is very narrow and if it's a narrow confidence interval it means that the uh accuracy of our results is higher and it means that the results we are getting BAS B on our smaller sample it will most likely generalize well when we apply these changes and deploy these changes and we put this new product in front of the entire population of users because now we are doing all this experiment for a small group for the sample and this confidence info that is narrow it's not wide it's narrow it means that the results that we are getting is are accurate more or less accurate and this means that we the results that we are getting based on a sample are most likely a true representation of the entire population that we got this is the idea behind the width of the confidence interval the narrower it is the higher uh is the quality of your results which means that the uh more generalizable are your results so let's now go on to the final stage of our case study which is to test the Practical significance of our results so now when we know that the statistical significance is there the experimental version of our feature is statistically significantly different from the control version in terms of the clickr rate and we have seen that the competence interval is narrow which means that our results are accurate quite uh with quite high accuracy then we can now comment on the Practical significance of our results this means we want to see whether the significant difference that we obtained whether this difference is actually large enough from the business perspective to say that it's worth to put our engineering resources and our money and our uh uh product into uh to put through through this change and to uh say that it's wor from the business perspective to change this button and to put this into um the production and in front of our users and of course here we are not only talking about the engineer ing resources that it will take from us to change this and the deployment and the monitoring but also in terms of the quality of the product we are providing to our users because whenever we are making a change to our product it is a risk because we are changing what our user is used to see and this can always be scary uh when it comes uh to the business because we don't want to uh make our customers scared so therefore we need to also check for this practical significance so for that what I'm doing is that I'm creating this python function that will take two arguments so two values that is the minimum detectable effect and then the 95% confidence interval that I just calculated those will be the two arguments for my function and I'm calling this function is practically significant and this function will go and check whether the uh practical significance is there or not and it will then return through true or false and then it will also print whether we have a practical significance or not and we learned from the theory and we know from this AB testing concept that whenever the uh MD or the Delta that we got the minimum detectable effect is larger than the lower bound of our confidence interval it means that the lowest possible value that we can get based on the results that we obtain in our sample that that amount is smaller than and the minimum detectable effect that we assumed before even conducting our AB test this suggests that we have a practical significance and the difference the minimum difference that we will obtain is large enough for us to have a motivation to make this change in our product for that what I'm doing is that first I'm taking my 95% confidence interval and I'm taking the first element because we know that a confidence interval is actually ranged so two pull of two numbers the lower bound and upper bound I need the lower bound because all I care for this practical significance is to compare the lower bound of the 95% confidence interval to this minimum detectable effect which is my Delta so therefore I'm taking this lower bound of confidence interval putting that into a variable and then I'm using this variable this lower uncore bound uncore CI confidence interval and I'm comparing this to my Delta I'm saying if my lower Bond of the confidence interval actually I'm noticing that here I got a mistake it should be the other way around we need to say that if our Delta is larger or equal the uh lower bound of the confidence interval which is the same as if our lower bound of the confidence interval is smaller than equal our Delta so if our we can also write this the other way around so if our Delta is larger than equal than our lower underscore bounde uncore CI then we can say that we have a practical significance so we the MDA of in this case so I want to use my initial Delta therefore I won't be initializing this so you might recall here a Delta of 10% I want to still make use of that Delta so therefore I will just go ahead and then in here what I want to do is to call this function by using that specific Delta so I want to have a 10% as my MD and whenever this Delta will be larger than the lower bound of my confidence interval that I just obtained I will then say that we have a practical significance and with an MDA of 10% the differ between control and experimental group is also practically significant so you can see that the lower bound is 0.04 something that we obtain in here and that amount is then compared to this Delta and here you can see that we have concluded that we also have a practical significance so amazing we have come to the end of this case study and in this involved case study we have conducted an entire app AB has results in ases so this case study a has and to end going from the point of loading the data and then understanding this business concept or business objective of ab test where we were testing whether the um enroll Now button which is the new version the experimental version should replace the existing button which is the secure vraal and based on this case study what we found out is that we have a statistical significance at 5% significance level suggesting that we can reject the N hypothesis and we can say that indeed there exists a statistical significant difference between the click through rate in the experimental group versus control group uh and specifically that the enroll now experimental button results in statistically significantly higher click through rate than the uh secure free trial button and beside this we also checked the um accuracy of our results by looking at a confidence interval and saw that the confidence interval was quite narrow suggesting that the results we obtained were quite uh accurate and this means that the results that we got for the sample will generalize to our population of users and finally we have also checked the Practical significance of our results by using the 95% confidence interval and comparing the lower bound of that interval with our minimum detectable effect Delta and we saw that we will have at least 10% significant difference between the control groups CTR and the control uh the experimental group CTR and the experimental group CTR will be at least 10% higher than the uh control groups and this suggests that uh from the business perspective we also have a motivation uh beside of this statistical significance we also have practical significance suggesting that we also have enough motivation and reason from the business perspective to put this new button into production so we can conclude that uh based on this datadriven approach and conducting an AB testing we uh can see a clear motivation of deploying this new button and draw now and replace the existing one secure free trial version and we will then expect to see more users clicking on this and engaging with our product and for now this will be all for this case study if you want to learn more about AB testing make sure to check our AB testing course as well as the ultimate data science boot camp don't forget to try our free trial this time using our enroll Now button and if you want to see more case studies like this make sure to check our tic case studies we have many case studies also included as part of our ultimate data science boot camp where we go in detail of these different steps and we conduct different sorts of case studies to put our data science theory in to practice including from the field of NLP machine learning recommended systems Advanced analytics and also AB testing and soon also from AI so for now thank you for staying with me and conducting this case study happy learning thank you for watching this video If you like this content make sure to check all the other videos available on this channel and don't forget to subscribe like and comment to help the algorithm to make this content more accessible to everyone across the world and if you want to get free resources make sure to check the free resources section at lunch. and if you want to become a job rate data scientist and you are looking for this accessible boot camp that will help you to make a job ready data scientist consider enrolling to the data science boot camp the ultimate data science boot camp at l. you will learn all the things the fundamentals to become a jbre data scientist he will also implement the learn theory into a real world multiple data science projects beside this after learning the theory and practicing it with the real world case studies you will also prepare for your data science interviews and if you want to stay up to date with the recent developments in Tech what are the headlines that you have missed in the last week what are the open positions currently in the market across the globe and what are the tech startups that are making waves in the tech and sure to subscribe to the data science Nai newsletter from lunar Tech
