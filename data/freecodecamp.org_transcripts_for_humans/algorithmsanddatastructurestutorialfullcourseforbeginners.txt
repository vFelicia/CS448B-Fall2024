With timestamps:

00:00 - this is a full-length course from
00:01 - treehouse we at free code camp are
00:03 - longtime fans of their learning platform
00:06 - they were kind enough to let our
00:07 - non-profit make this course freely
00:09 - available on our youtube channel if you
00:11 - like this course treehouse has a lot
00:13 - more courses like this one the link is
00:15 - in the description
00:16 - along with time codes to the different
00:18 - sections in this course
00:21 - [Music]
00:29 - hi my name is passan i'm an instructor
00:31 - here at treehouse and welcome to
00:33 - introduction to algorithms
00:34 - whether you are a high school or college
00:36 - student a developer in the industry or
00:39 - someone who is learning to code you have
00:41 - undoubtedly run into the term algorithm
00:44 - for many people this word is kind of
00:46 - scary it represents this body of
00:48 - knowledge that seems just out of reach
00:51 - only people with computer science
00:52 - degrees know about algorithms now to
00:55 - others this brings up feelings of
00:56 - imposter syndrome
00:58 - you might already know how to code but
01:00 - you're not a real developer because you
01:03 - don't know anything about algorithms
01:05 - personally it made me frame certain jobs
01:08 - as above my skill level because the
01:10 - interview contained algorithm questions
01:13 - well whatever your reasons are in this
01:16 - course our goal is to dispel all those
01:18 - feelings and get you comfortable with
01:20 - the basics of algorithms
01:22 - like any other subject i like to start
01:24 - my courses with what the course is and
01:27 - is not
01:28 - in this course we're going to cover the
01:30 - very basic set of knowledge that you
01:32 - need as a foundation for learning about
01:34 - algorithms
01:36 - this course is less about specific
01:38 - algorithms and more about the tools you
01:40 - will need to evaluate algorithms
01:43 - understand how they perform
01:44 - compare them to each other and make a
01:46 - statement about the utility of an
01:48 - algorithm in a given context
01:50 - now don't worry none of this will be
01:52 - theoretical and we will learn these
01:54 - concepts by using well-known algorithms
01:57 - in this course we will also be writing
01:59 - code so i do expect you to have some
02:01 - programming experience if you intend to
02:03 - continue with this topic
02:05 - you can definitely stick around even if
02:07 - you don't know how to code but you might
02:09 - want to learn the basics of programming
02:11 - in the meantime
02:13 - in this course we will be using the
02:14 - python programming language python reads
02:17 - a lot like regular english and is the
02:19 - language you will most likely encounter
02:21 - when learning about algorithms these
02:23 - days
02:24 - if you don't know how to code or if you
02:26 - know how to code in a different language
02:28 - check out the notes section of this
02:30 - video for links to other content that
02:32 - might be useful to you
02:34 - as long as you understand the
02:36 - fundamentals of programming you should
02:38 - be able to follow along pretty well
02:41 - if you're a javascript developer or a
02:43 - student who's learning javascript for
02:45 - example chances are good that you'll
02:47 - still be able to understand the code we
02:48 - write later i'll be sure to provide
02:51 - links along the way if you need anything
02:53 - to follow up on
02:55 - let's start with something simple
02:57 - what is an algorithm
02:59 - an algorithm is a set of steps or
03:01 - instructions for completing a task
03:04 - this might sound like an over
03:06 - simplification but really that's
03:08 - precisely what an algorithm is
03:11 - a recipe is an algorithm your morning
03:13 - routine when you wake up is an algorithm
03:16 - and the driving directions you follow to
03:18 - get to a destination is also an
03:19 - algorithm
03:20 - in computer science the term algorithm
03:23 - more specifically means the set of steps
03:25 - a program takes to finish a task
03:28 - if you've written code before any code
03:30 - really generally speaking you have
03:32 - written an algorithm
03:34 - given that much of the code we write can
03:36 - be considered an algorithm what do
03:38 - people mean when they say you should
03:40 - know about algorithms
03:42 - now consider this
03:44 - let's say i'm a teacher in a classroom
03:46 - and i tell everyone i have an assignment
03:47 - for them on their desks they have a
03:49 - picture of a maze and their task is to
03:52 - come up with a way to find the quickest
03:54 - way out of the maze
03:56 - everyone does their thing and comes up
03:58 - with a solution
03:59 - every single one of these solutions is a
04:01 - viable solution and is a valid example
04:04 - of an algorithm the steps one needs to
04:06 - take to get out of the maze but from
04:08 - being in classrooms or any group of any
04:11 - sort you know that some people will have
04:13 - better ideas than others we all have a
04:15 - diverse array of skill sets
04:18 - over time our class picks the best of
04:20 - these solutions and any time we want to
04:22 - solve a maze we go with one of these
04:24 - solutions this is what the field of
04:27 - algorithms is about
04:29 - there are many problems in computer
04:30 - science but some of them are pretty
04:32 - common regardless of what project you're
04:35 - working on
04:36 - different people have come up with
04:37 - different solutions to these common
04:39 - problems and over time the field of
04:42 - computer science has identified several
04:44 - that do the job well for a given task
04:47 - when we talk of algorithms we're
04:49 - referring to two points
04:51 - we're primarily saying there's an
04:53 - established body of knowledge on how to
04:55 - solve particular problems well and it's
04:58 - important to know what the solutions are
05:01 - now why is it important
05:03 - if you're unaware that a solution exists
05:05 - you might try to come up with one
05:07 - yourself and there's a likelihood that
05:09 - your solution won't be as good or
05:11 - efficient whatever that means compared
05:14 - to those that have been thoroughly
05:15 - reviewed
05:16 - but there's a second component to it as
05:18 - well
05:19 - part of understanding algorithms is not
05:22 - just knowing that an algorithm exists
05:24 - but understanding when to apply it
05:27 - understanding when to apply an algorithm
05:29 - requires properly understanding the
05:32 - problem at hand and this arguably is the
05:34 - most important part of learning about
05:36 - algorithms and data structures
05:39 - as you progress through this content you
05:41 - should be able to look at a problem and
05:43 - break it down into distinct steps
05:46 - when you have a set of steps you should
05:47 - then be able to identify which algorithm
05:50 - or data structure is best for the task
05:52 - at hand
05:53 - this concept is called algorithmic
05:55 - thinking and it's something we're going
05:57 - to try and cultivate together as we work
05:59 - through our content
06:01 - lastly learning about algorithms gives
06:03 - you a deeper understanding about
06:05 - complexity and efficiency in programming
06:08 - having a better sense of how your code
06:10 - will perform in different situations is
06:12 - something that you'll always want to
06:14 - develop in hone
06:16 - algorithmic thinking is why algorithms
06:18 - also come up in big tech interviews
06:21 - interviewers don't care as much that you
06:23 - are able to write a specific algorithm
06:25 - in code but more about the fact that you
06:28 - can break a seemingly insurmountable
06:30 - problem into distinct components and
06:32 - identify the right tools to solve each
06:35 - distinct component
06:37 - and that is what we plan on doing in
06:39 - this course though we're going to focus
06:41 - on some of the tools and concepts you'll
06:43 - need to be aware of before we can dive
06:46 - into the topic of algorithms if you're
06:48 - ready let's get started
06:51 - hey again in this video we're going to
06:52 - do something unusual we're going to play
06:54 - a game and by we i mean me and my two
06:57 - friends here brittany and john
06:59 - this game is really simple and you may
07:01 - have played it before it goes something
07:02 - like this i'm going to think of a number
07:04 - between 1 and 10 and they have to guess
07:07 - what the number is easy right
07:09 - when they guess a number i'll tell them
07:11 - if their guess is too high or too low
07:13 - the winner is the one with the fewest
07:15 - tries all right john let's start with
07:17 - you i'm thinking of a number between one
07:19 - and ten what is it
07:21 - between you and me the answer is three
07:23 - uh quick question does the range include
07:25 - one and ten
07:27 - that is a really good question so what
07:29 - john did right there was to establish
07:30 - the bounds of our problem
07:32 - no solution works on every problem and
07:34 - an important part of algorithmic
07:36 - thinking is to clearly define what the
07:38 - problem set is and clarify what values
07:40 - count as inputs
07:42 - yeah 1 and ten are both included is it
07:45 - one too low is it two too low is it
07:47 - three correct
07:49 - okay so that was an easy one it took
07:51 - john three tries to get the answer let's
07:53 - switch over to brittany and play another
07:55 - round using the same number as the
07:57 - answer okay brittany i'm thinking of a
07:59 - number between 1 and 10 inclusive so
08:01 - both 1 and 10 are in the range what
08:02 - number am i thinking of
08:04 - is it 5 too high
08:06 - 2 too low
08:08 - is it 3 correct all right so what we had
08:10 - there was two very different ways of
08:12 - playing the same game
08:14 - somehow with even such a simple game we
08:16 - saw different approaches to figuring out
08:18 - a solution
08:19 - to go back to algorithmic thinking for a
08:21 - second this means that with any given
08:24 - problem there's no one best solution
08:27 - instead what we should try and figure
08:29 - out is what solution works better for
08:31 - the current problem
08:32 - in this first pass at the game they both
08:35 - took the same amount of turns to find
08:36 - the answer so it's not obvious who has
08:39 - the better approach and that's mostly
08:40 - because the game was easy
08:42 - let's try this one more time now this
08:44 - time the answer is 10.
08:46 - all right john you first is it one too
08:48 - low is it two still too low is it three
08:51 - too low is it four too low is it five
08:54 - still too low is it six too low is it
08:56 - seven too low is it eight low is it nine
09:00 - do low is it ten correct you got it okay
09:03 - so now same thing but with britney this
09:05 - time
09:06 - is it five too low
09:08 - eight too low is it nine still too low
09:11 - it's ten
09:13 - all right so here we start to see a
09:14 - difference between their strategies when
09:16 - the answer was three they both took the
09:18 - same number of turns this is important
09:21 - when the number was larger but not that
09:23 - much larger 10 in this case we start to
09:26 - see that britney strategy did better she
09:28 - took four tries while john took 10.
09:31 - we've played two rounds so far and we've
09:33 - seen a different set of results based on
09:34 - the number they were looking for
09:36 - if you look at john's way of doing
09:38 - things then the answer being 10 the
09:40 - round we just played is his worst case
09:42 - scenario he will take the maximum number
09:45 - of turns 10 to guess it
09:48 - when we picked a random number like
09:49 - three it was hard to differentiate which
09:52 - strategy was better because they both
09:54 - performed exactly the same
09:56 - but in john's worst case scenario a
09:58 - clear winner in terms of strategy
10:00 - emerges
10:01 - in terms of algorithmic thinking we're
10:03 - starting to get a sense that the
10:05 - specific value they're searching for may
10:07 - not matter as much as where that value
10:10 - lies in the range that they've been
10:11 - given
10:12 - identifying this helps us understand our
10:15 - problem better
10:16 - let's do this again for a range of
10:18 - numbers from one to one hundred we'll
10:20 - start by picking five as an answer to
10:22 - trick them
10:23 - okay so this time we're going to run
10:25 - through the exercise again this time
10:26 - from one to one hundred and both one and
10:28 - one hundred are included is it one at
10:30 - this point without even having to run
10:32 - through it we can guess how many tries
10:33 - john is going to take since he starts at
10:35 - one and keeps going he's going to take
10:37 - five tries as we're about to see is it
10:39 - five cool correct
10:41 - okay now for brittany's turn
10:43 - is it 50 too high is it 25 still too
10:46 - high
10:47 - is it 13 too high is it seven
10:50 - too high
10:51 - is it four too low
10:53 - is it six too high is it five correct
10:57 - let's evaluate john took five tries
11:00 - brittany on the other hand takes seven
11:01 - tries so john wins this round but again
11:04 - in determining whose strategy is
11:06 - preferred there's no clear winner right
11:08 - now
11:09 - what this tells us is that it's not
11:11 - particularly useful to look at the easy
11:13 - answers where we arrive at the number
11:15 - fairly quickly because it's at the start
11:16 - of the range
11:18 - instead let's try one where we know john
11:20 - is going to do poorly let's look at his
11:22 - worst case scenario where the answer is
11:24 - 100 and see how britney performs in such
11:27 - a scenario
11:28 - okay john let's do this one more time
11:30 - one through 100 again
11:32 - is it one we can fast forward this scene
11:34 - because well we know what happens john
11:36 - takes 100 tries
11:38 - hi brittany you're up
11:40 - is it 50 too low is it 75 too low 88 too
11:44 - low
11:45 - 94 too low is it 97 too low
11:49 - 99 too low
11:51 - 100.
11:53 - okay so that took brittney seven turns
11:55 - again and this time she is the clear
11:57 - winner if you compare their individual
11:59 - performances for the same number set
12:01 - you'll see that britney's approach
12:03 - leaves john's in the dust
12:05 - when the answer was five so right around
12:06 - the start of the range john took five
12:08 - turns but when the answer was 100 right
12:11 - at the end of the range he took 100
12:13 - tries it took him 20 times the amount of
12:15 - tries to get that answer compared to
12:17 - britney
12:19 - on the other hand if you compare
12:20 - britney's efforts when the number was 5
12:22 - she took seven tries but when the number
12:24 - was 100 she took the same amount of
12:26 - tries this is pretty impressive if we
12:29 - pretend that the number of tries is the
12:31 - number of seconds it takes britney and
12:33 - john to run through their attempts this
12:35 - is a good estimate for how fast their
12:38 - solutions are
12:39 - ok we've done this a couple times and
12:41 - brittany and john are getting tired
12:42 - let's take a break in the next video
12:44 - we'll talk about the point of this
12:46 - exercise
12:47 - in the last video we ran through an
12:49 - exercise where i had some of my
12:50 - co-workers guess what number i was
12:52 - thinking so was the point of that
12:54 - exercise you might be thinking hey i
12:56 - thought i was here to learn about
12:57 - algorithms
12:59 - the exercise we just did was an example
13:01 - of a real life situation you will run
13:04 - into when building websites apps and
13:06 - writing code
13:08 - both approaches taken by john and
13:10 - brittany to find the number i was
13:11 - thinking of are examples of searching
13:14 - for a value
13:15 - it might be weird to think that there's
13:16 - more than one way to search but as you
13:19 - saw in the game the speed at which the
13:21 - result was obtained differed between
13:23 - john and brittany think about this
13:25 - problem from the perspective of a
13:26 - company like facebook
13:28 - at the time of this recording facebook
13:30 - has 2.19 billion active users
13:33 - let's say you're traveling in a
13:34 - different country and meet someone you
13:36 - want to add on facebook
13:38 - you go into the search bar and type out
13:39 - this person's name
13:41 - if we simplify how the facebook app
13:44 - works it has to search across these 2.19
13:47 - billion records and find the person you
13:50 - are looking for
13:51 - the speed at which you find this person
13:53 - really matters imagine what kind of
13:55 - experience it would be if when you
13:57 - search for a friend facebook put up a
14:00 - spinning activity indicator and said
14:02 - come back in a couple hours
14:04 - i don't think we'd use facebook as much
14:06 - if that was the case
14:07 - from the company's perspective working
14:09 - on making search as fast as possible
14:12 - using different strategies really
14:14 - matters
14:15 - now i said that the two strategies
14:17 - britney and john used were examples of
14:19 - search
14:20 - more specifically these are search
14:22 - algorithms
14:24 - the strategy john took where he started
14:26 - at the beginning of the range and just
14:28 - counted one number after the other is a
14:31 - type of search called linear search
14:33 - it is also called sequential search
14:36 - which is a better description of how it
14:37 - works or even simple search since it
14:40 - really is quite simple
14:42 - but what makes his approach an algorithm
14:45 - as opposed to just looking for something
14:47 - remember we said that an algorithm is a
14:49 - set of steps or instructions to complete
14:51 - a task
14:53 - linear search is a search algorithm and
14:55 - we can define it like this
14:57 - we start at the beginning of the list or
14:59 - the range of values
15:01 - then we compare the current value to the
15:03 - target
15:04 - if the current value is the target value
15:06 - that we're looking for we're done
15:09 - if it's not we'll move on sequentially
15:11 - to the next value in the list and then
15:13 - repeat step 2.
15:15 - if we reach the end of the list then the
15:17 - target value is not in the list
15:20 - this definition has nothing to do with
15:22 - programming and in fact you can use it
15:24 - in the real world for example
15:26 - i could tell you to walk into a
15:27 - bookstore and find me a particular book
15:30 - and one of the ways you could do it is
15:32 - using the linear search algorithm
15:34 - you could start at the front of the
15:36 - bookstore and read the cover or the
15:38 - spine of every book to check that it
15:40 - matches the book that you're looking for
15:42 - if it doesn't you go to the next book
15:44 - and repeat until you find it or run out
15:46 - of books
15:48 - what makes this an algorithm is the
15:50 - specificity of how it is defined
15:53 - in contrast to just jumping into a
15:55 - problem and solving it as we go along an
15:58 - algorithm follows a certain set of
15:59 - guidelines and we use the same steps to
16:02 - solve the problem each time we face it
16:05 - an important first step to defining the
16:07 - algorithm isn't the algorithm itself but
16:09 - the problem we're trying to solve
16:12 - our first guideline is that an algorithm
16:14 - must have a clear problem statement
16:17 - it's pretty hard to define an
16:18 - instruction set when you don't have a
16:20 - clear idea of what problem you're trying
16:22 - to solve
16:23 - in defining the problem we need to
16:25 - specify how the input is defined and
16:28 - what the output looks like when the
16:30 - algorithm has done its job
16:32 - for linear search the input can be
16:34 - generally described as a series of
16:36 - values and the output is a value
16:38 - matching the one we're looking for
16:41 - right now we're trying to stay away from
16:42 - anything code related so this problem
16:44 - statement definition is pretty generic
16:46 - but once we get to code we can actually
16:48 - tighten this up
16:49 - once we have a problem an algorithm is a
16:51 - set of steps that solves this problem
16:54 - given that the next guideline is that an
16:57 - algorithm definition must contain a
16:59 - specific set of instructions in a
17:01 - particular order
17:03 - we really need to be clear about the
17:05 - order in which these instructions are
17:07 - executed
17:08 - taking our simple definition of linear
17:11 - search if i switched up the order and
17:13 - said move sequentially to the next value
17:16 - before specifying that first comparison
17:18 - step if the first value were the target
17:20 - one our algorithm wouldn't find it
17:22 - because we moved to the second value
17:24 - before comparing
17:26 - now you might think okay that's just an
17:28 - avoidable mistake and kind of common
17:30 - sense
17:30 - the thing is computers don't know any of
17:32 - that and just do exactly as we tell them
17:35 - so specific order is really important
17:38 - the third guideline is that each step in
17:40 - our algorithm definition must not be a
17:42 - complex one and needs to be explicitly
17:44 - clear what i mean by that is that you
17:47 - shouldn't be able to break down any of
17:49 - the steps into further into additional
17:51 - subtasks
17:53 - each step needs to be a distinct one we
17:55 - can't define linear search as search
17:57 - until you find this value because that
17:59 - can be interpreted in many ways and
18:02 - further broken down into many more steps
18:04 - it's not clear
18:06 - next and this one might seem obvious but
18:08 - algorithms should produce a result
18:11 - if it didn't how would we know whether
18:13 - the algorithm works or not
18:16 - to be able to verify that our algorithm
18:18 - works correctly we need a result
18:21 - now when using a search algorithm the
18:23 - end result can actually be nothing which
18:25 - indicates that the value wasn't found
18:27 - but that's perfectly fine
18:29 - there are several ways to represent
18:31 - nothing in code and as long as the
18:33 - algorithm can produce some results we
18:35 - can understand its behavior
18:37 - the last guideline is that the algorithm
18:40 - should actually complete and cannot take
18:41 - an infinite amount of time
18:43 - if we let john loose in the world's
18:45 - largest library and asked him to find a
18:48 - novel we have no way of knowing whether
18:50 - he succeeded or not unless he came back
18:52 - to us with a result
18:54 - okay so quick recap what makes an
18:56 - algorithm an algorithm and not just
18:58 - something you do
19:00 - one it needs to have a clearly defined
19:02 - problem statement input and output
19:05 - when using linear search the input needs
19:07 - to be just a series of values but to
19:10 - actually use brittany's strategy there's
19:12 - one additional precondition so to speak
19:15 - if you think about her strategy it
19:17 - required that the numbers be sorted in
19:19 - ascending order
19:21 - this means that where the input for john
19:22 - is just a series of values to solve the
19:25 - problem the input to brittany's
19:27 - algorithm needs to be a sorted series of
19:29 - values
19:30 - so clearly defined problem statement
19:32 - clearly defined input and clearly
19:34 - defined output
19:36 - second the steps in the algorithm need
19:38 - to be in a very specific order
19:41 - the steps also need to be distinct you
19:44 - should not be able to break it down into
19:46 - further subtasks
19:48 - next the algorithm should produce a
19:50 - result
19:51 - and finally the algorithm should
19:53 - complete in a finite amount of time
19:56 - these guidelines not only help us define
19:58 - what an algorithm is but also helps us
20:01 - verify that the algorithm is correct
20:04 - executing the steps in an algorithm for
20:06 - a given input must result in the same
20:09 - output every time
20:12 - if in the game i played the answer was
20:14 - 50 every time then every single time
20:17 - john must take 50 turns to find out that
20:20 - the answer is 50. if somehow he takes 50
20:23 - turns in one round then 30 the next and
20:26 - we technically don't have a correct
20:27 - algorithm
20:29 - consistent results for the same set of
20:31 - values is how we know that the algorithm
20:33 - is correct
20:35 - i should stress that we're not going to
20:36 - be designing any algorithms on our own
20:39 - and we'll start off and spend most of
20:41 - our time learning the tried and true
20:43 - algorithms that are known to efficiently
20:45 - solve problems
20:46 - the reason for talking about what makes
20:48 - for a good algorithm though is that the
20:50 - same set of guidelines makes for good
20:53 - algorithmic thinking which is one of the
20:55 - most important skills we want to
20:56 - cultivate
20:58 - when we encounter a problem before
21:00 - rushing in and thinking about solutions
21:02 - what we want to do is work through the
21:04 - guidelines
21:05 - first we break down the problem into any
21:07 - possible number of smaller problems
21:09 - where each problem can be clearly
21:11 - defined in terms of an input and an
21:13 - output
21:15 - now that we know how to generally define
21:17 - an algorithm let's talk about what it
21:18 - means to have a good algorithm
21:20 - an important thing to keep in mind is
21:22 - that there's no one single way to
21:25 - measure whether an algorithm is the
21:27 - right solution because it is all about
21:29 - context
21:30 - earlier we touched on two concepts
21:33 - correctness and efficiency
21:35 - let's define correctness more clearly
21:37 - because before we can evaluate an
21:39 - algorithm on efficiency we need to
21:41 - ensure its correctness
21:43 - before we define our algorithms we start
21:45 - by defining our problem
21:48 - in the definition of that problem we
21:50 - have a clearly defined input satisfying
21:52 - any preconditions and a clearly defined
21:55 - output
21:56 - an algorithm is deemed correct if on
21:59 - every run of the algorithm against all
22:01 - possible values in the input data we
22:03 - always get the output we expect
22:06 - part of correctness means that for any
22:09 - possible input the algorithm should
22:11 - always terminate or end
22:14 - if these two are not true then our
22:16 - algorithm isn't correct
22:19 - if you were to pick up an algorithm's
22:20 - textbook and look up correctness you
22:22 - will run into a bunch of mathematical
22:25 - theory this is because traditionally
22:27 - algorithm correctness is proved by
22:29 - mathematical induction which is a form
22:32 - of reasoning used in mathematics to
22:34 - verify that a statement is correct
22:37 - this approach involves writing what is
22:38 - called a specification and a correctness
22:41 - proof
22:42 - we won't be going into that in this
22:44 - course
22:45 - proof through induction is an important
22:47 - part of designing algorithms but we're
22:49 - confident that you can understand
22:51 - algorithms both in terms of how and when
22:53 - to use them without getting into the
22:55 - math so if you pick up a textbook and
22:58 - feel daunted don't worry i do too but we
23:00 - can still figure things out without it
23:02 - all right so once we have a correct
23:04 - algorithm we can start to talk about how
23:07 - efficient an algorithm is
23:09 - remember that this efficiency ultimately
23:12 - matters because they help us solve
23:13 - problems faster and deliver a better end
23:16 - user experience in a variety of fields
23:19 - for example algorithms are used in the
23:22 - sequencing of dna and more efficient
23:24 - sequencing algorithms allow us to
23:27 - research and understand diseases better
23:29 - and faster but let's not get ahead of
23:31 - ourselves we'll start simple by
23:34 - evaluating john's linear search
23:36 - algorithm in terms of its efficiency
23:39 - first what do we mean by efficiency
23:41 - there are two measures of efficiency
23:43 - when it comes to algorithms time and
23:46 - space sounds really cool and very sci-fi
23:48 - huh
23:49 - efficiency measured by time something
23:52 - you'll hear called time complexity is a
23:54 - measure of how long it takes the
23:56 - algorithm to run
23:58 - time complexity can be understood
24:00 - generally outside the context of code
24:02 - and computers because how long it takes
24:04 - to complete a job is a universal measure
24:06 - of efficiency the less time you take the
24:09 - more efficient you are
24:10 - the second measure of efficiency is
24:12 - called space complexity and this is
24:15 - pretty computer specific
24:16 - it deals with the amount of memory taken
24:19 - up on the computer
24:21 - good algorithms need to balance between
24:23 - these two measures to be useful
24:25 - for example you can have a blazingly
24:27 - fast algorithm but it might not matter
24:29 - if the algorithm consumes more memory
24:31 - than you have available
24:33 - both of these concepts time and space
24:35 - complexity are measured using the same
24:38 - metric but it is a very technical
24:40 - sounding metric so let's build up to it
24:42 - slowly and start simple
24:45 - a few videos ago i played a game with
24:47 - brittany and john where they tried to
24:49 - guess the number i was thinking of
24:51 - effectively they were searching for a
24:52 - value
24:53 - so how do we figure out how efficient
24:56 - each algorithm is and which algorithm
24:58 - was more suited to our purposes
25:01 - if we consider the number of tries they
25:03 - took to guess or search for the value as
25:06 - an indicator of the time they take to
25:09 - run through the exercise this is a good
25:11 - indicator of how long the algorithm runs
25:13 - for a given set of values
25:16 - this measurement is called the running
25:17 - time of an algorithm and we'll use it to
25:19 - define time complexity
25:21 - in the game we play it four rounds let's
25:24 - recap those here focusing on john's
25:26 - performance
25:27 - in round one we had 10 values the target
25:30 - was 3 and john took 3 turns
25:33 - in round 2 we had 10 values the target
25:35 - was 10 and john took 10 turns
25:38 - in round 3 we had 100 values the target
25:40 - was
25:41 - john took five tries and finally in
25:43 - round four when the target was 100 given
25:46 - 100 values john took 100 tries
25:50 - on paper it's hard to gauge anything
25:52 - about this performance
25:53 - when it comes to anything with numbers
25:55 - though i like to put it up on a graph
25:56 - and compare visually
25:58 - on the vertical or y-axis let's measure
26:01 - the number of tries it took john to
26:03 - guess the answer or the running time of
26:05 - the algorithm on the horizontal or
26:08 - x-axis what do we put
26:10 - for each turn we have a number of values
26:13 - as well as a target value
26:16 - we could plot the target value on the
26:18 - horizontal axis but that leaves some
26:20 - context and meaning behind it's far more
26:23 - impressive that john took five tries
26:25 - when the range went up to 100 then when
26:28 - he took three tries for a maximum of 10
26:30 - values
26:32 - we could plot the maximum range of
26:34 - values but then we're leaving out the
26:35 - other half of the picture
26:37 - there are data points however that
26:39 - satisfy both requirements
26:41 - if we only plot the values where the
26:44 - target the number john was looking for
26:46 - was the same as the maximum range of
26:49 - values we have a data point that
26:51 - includes both the size of the data set
26:53 - as well as his effort
26:55 - there's an additional benefit to this
26:57 - approach as well
26:58 - there are three ways we can measure how
27:00 - well john does or in general how well
27:02 - any algorithm does
27:04 - first we can check how well john does in
27:06 - the best case or good scenarios from the
27:09 - perspective of his strategy
27:11 - in the range of 100 values the answer
27:13 - being a low number like three at the
27:15 - start of the range is a good scenario he
27:18 - can guess it fairly quickly one is his
27:21 - best case scenario
27:22 - or we could check how well he does on
27:24 - average we could run this game a bunch
27:26 - of times and average out the running
27:28 - time
27:29 - this would give us a much better picture
27:30 - of john's performance over time but our
27:33 - estimates would be too high if the value
27:35 - he was searching for was at the start of
27:37 - the range or far too low if it was at
27:39 - the end of the range
27:41 - let's imagine a scenario where facebook
27:43 - naively implements linear search when
27:44 - finding friends
27:46 - they looked at the latest u.s census saw
27:48 - that 50 of names start with the letters
27:51 - a through j which is the first 40 of the
27:54 - alphabet and thought okay on average
27:57 - linear search serves us well
27:59 - but what about the rest of those whose
28:01 - names start with the letter after j in
28:03 - the alphabet
28:04 - searching for my name would take longer
28:06 - than the average and much longer for
28:08 - someone whose name starts with the
28:09 - letter z
28:10 - so while measuring the run time of an
28:12 - algorithm on average might seem like a
28:14 - good strategy it won't necessarily
28:16 - provide an accurate picture
28:19 - by picking the maximum in the range
28:21 - we're measuring how our algorithm does
28:23 - in the worst case scenario
28:26 - analyzing the worst case scenario is
28:27 - quite useful because it indicates that
28:30 - the algorithm will never perform worse
28:32 - than we expect there's no room for
28:34 - surprises
28:36 - back to our graph we're going to plot
28:38 - the number of tries a proxy for running
28:40 - time of the algorithm against the number
28:42 - of values in the range which will
28:44 - shorten to n
28:46 - n here also represents john's worst case
28:48 - scenario when n is 10 he takes 10 turns
28:52 - when n is 100 he takes 100 turns
28:55 - but these two values alone are
28:57 - insufficient to really get any sort of
28:59 - visual understanding moreover it's not
29:02 - realistic
29:03 - john may take a long time to work
29:05 - through 100 numbers but a computer can
29:07 - do that in no time
29:09 - to evaluate the performance of linear
29:11 - search in the context of a computer we
29:13 - should probably throw some harder and
29:15 - larger ranges of values at it
29:18 - the nice thing is by evaluating a worst
29:20 - case scenario we don't actually have to
29:22 - do that work
29:24 - we know what the result will be for a
29:26 - given value of n using linear search it
29:28 - will take n tries to find the value in
29:31 - the worst case scenario so let's add a
29:33 - few values in here to build out this
29:35 - graph
29:36 - okay so we have a good picture of what
29:38 - this is starting to look like as the
29:40 - values get really large the running time
29:42 - of the algorithm gets large as well
29:45 - we sort of already knew that
29:47 - before we dig into this runtime any
29:48 - deeper let's switch tracks and evaluate
29:50 - brittany's work
29:52 - by having something to compare against
29:54 - it should become easier to build a
29:55 - mental model around time complexity
29:58 - the algorithm john used linear search
30:01 - seemed familiar to us and you could
30:03 - understand it because it's how most of
30:04 - us search for things in real life anyway
30:07 - brittany's approach on the other hand
30:09 - got results quickly but it was a bit
30:11 - harder to understand so let's break it
30:13 - down
30:14 - just like john's approach britney
30:16 - started with a series of values or a
30:17 - list of numbers as her input
30:20 - where john just started at the beginning
30:22 - of the list and searched sequentially
30:24 - brittany's strategy is to always start
30:26 - in the middle of the range
30:28 - from there she asks a comparison
30:30 - question
30:31 - is the number in the middle of the range
30:33 - equal to the answer she's looking for
30:35 - and if it's not is it greater than or
30:37 - less than the answer
30:39 - if it's greater than she can eliminate
30:41 - all the values less than the one she's
30:43 - currently evaluating if it's lesser than
30:46 - the answer she can eliminate all the
30:48 - values greater than the one she's
30:49 - currently evaluating
30:51 - with the range of values that she's left
30:53 - over with she repeats this process until
30:56 - she arrives at the answer
30:58 - let's visualize how she did this by
31:00 - looking at round three
31:02 - in round three the number of values in
31:04 - the range was 100 the answer was 5.
31:07 - the bar here represents the range of
31:08 - values one of the left 100 at the right
31:11 - and this pointer represents the value
31:13 - britney chooses to evaluate
31:15 - so she starts in the middle at 50. she
31:18 - asks is it equal to the answer i say
31:20 - it's too high so this tells her that the
31:23 - value she is evaluating is greater than
31:25 - our target value which means there's no
31:28 - point in searching any of the values to
31:30 - the right of 50 that is values greater
31:33 - than 50 in this range so she can discard
31:35 - those values altogether
31:37 - she only has to consider values from 1
31:40 - to 50 now
31:41 - the beauty of this strategy and the
31:43 - reason why britney was able to find the
31:45 - answer in such few turns is that with
31:47 - every value she evaluates she can
31:50 - discard half of the current range
31:53 - on her second turn she picks the value
31:55 - in the middle of the current range which
31:56 - is 25. she asks the same question i say
32:00 - that the value is too high again and
32:02 - this tells her that she can discard
32:04 - everything greater than 25 and the range
32:06 - of values drops from 1 to 25.
32:10 - again she evaluates the number in the
32:11 - middle roughly so that'd be 13 here i
32:14 - tell her this is still too high she
32:16 - discards the values greater moves to
32:18 - value at 7 which is still too high
32:21 - then she moves to 4 which is now too low
32:24 - she can discard everything less than 4
32:26 - which leaves the numbers 4 through 7.
32:28 - here she picked 6 which was too high
32:31 - which only leaves one value 5.
32:34 - this seems like a lot of work but being
32:36 - able to get rid of half the values with
32:38 - each turn is what makes this algorithm
32:41 - much more efficient
32:42 - now there's one subtlety to using binary
32:45 - search and you might have caught on to
32:46 - this
32:47 - for this search method to work as we've
32:49 - mentioned the values need to be sorted
32:51 - with linear search it doesn't matter if
32:53 - the values are sorted since a linear
32:56 - search algorithm just progresses
32:58 - sequentially checking every element in
33:00 - the list if the target value exists in
33:02 - the list it will be fouled but let's say
33:05 - this range of values 100 was unsorted
33:08 - britney would start at the middle with
33:10 - something like 14 and ask if this value
33:12 - was too low or too high i say it's too
33:15 - high so she discards everything less
33:17 - than 14.
33:18 - now this example starts to fall apart
33:20 - here because well britney knows what
33:22 - numbers are less than 14 and greater
33:24 - than one she doesn't need an actual
33:26 - range of values to solve this a computer
33:29 - however does need that
33:31 - remember search algorithms are run
33:33 - against lists containing all sorts of
33:35 - data it's not always just a range of
33:37 - values containing numbers
33:39 - in a real use case of binary search
33:41 - which we're going to implement in a bit
33:43 - the algorithm wouldn't return the target
33:45 - value because we already know that it's
33:48 - a search algorithm so we're providing
33:50 - something to search for instead what it
33:52 - returns is the position in the list that
33:54 - the target occupies without the list
33:56 - being sorted a binary search algorithm
33:59 - would discard all the values to the left
34:01 - of 14 which over here could include the
34:03 - position where our target value is
34:06 - eventually we'd get a result back saying
34:07 - the target value doesn't exist in the
34:09 - list which is inaccurate
34:11 - earlier when defining linear simple
34:14 - search i said that the input was a list
34:16 - of values and the output was the target
34:18 - value or more specifically the position
34:21 - of the target value in the list
34:23 - so with binary search there's also that
34:25 - precondition the input list must be
34:27 - sorted so let's formally define binary
34:30 - search
34:31 - first the input a sorted list of values
34:34 - the output the position in the list of
34:36 - the target value we're searching for or
34:39 - some sort of values indicate that the
34:41 - target does not exist in the list
34:44 - remember our guidelines for defining an
34:45 - algorithm let me put those up again
34:47 - really quick
34:48 - the steps in the algorithm need to be in
34:50 - a specific order the steps also need to
34:52 - be very distinct
34:54 - the algorithms should produce a result
34:56 - and finally the algorithm should
34:58 - complete in a finite amount of time
35:00 - let's use those to define this algorithm
35:03 - step one we determine the middle
35:05 - position of the sorted list
35:07 - step two we compare the element in the
35:09 - middle position to the target element
35:12 - step three if the elements match we
35:14 - return the middle position and end
35:17 - if they don't match in step 4 we check
35:20 - whether the element in the middle
35:21 - position is smaller than the target
35:23 - element
35:24 - if it is then we go back to step 2 with
35:27 - a new list that goes from the middle
35:29 - position of the current list to the end
35:31 - of the current list
35:32 - in step five if the element in the
35:34 - middle position is greater than the
35:36 - target element then again we go back to
35:38 - step two with a new list that goes from
35:40 - the start of the current list to the
35:42 - middle position of the current list
35:44 - we repeat this process until the target
35:47 - element is found or until a sub list
35:50 - contains only one element
35:52 - if that single element sublist does not
35:55 - match the target element then we end the
35:57 - algorithm indicating that the element
35:59 - does not exist in the list
36:02 - okay so that is the magic behind how
36:04 - britney managed to solve the round much
36:06 - faster
36:07 - in the next video let's talk about the
36:09 - efficiency of binary search
36:12 - [Music]
36:16 - we have a vague understanding that
36:18 - britney's approach is better in most
36:19 - cases but just like with linear search
36:22 - it helps to visualize this
36:23 - much like we did with linear search when
36:26 - determining the efficiency of an
36:27 - algorithm and remember we're still only
36:29 - looking at efficiency in terms of time
36:32 - time complexity as it's called we always
36:35 - want to evaluate how the algorithm
36:37 - performs in the worst case scenario now
36:39 - you might be thinking well that doesn't
36:41 - seem fair because given a series of data
36:44 - if the target value we're searching for
36:46 - is somewhere near the front of the list
36:48 - then linear search may perform just as
36:50 - well if not slightly better than binary
36:52 - search and that is totally true
36:55 - remember a crucial part of learning
36:57 - algorithms is understanding what works
36:59 - better in a given context
37:01 - when measuring efficiency though we
37:03 - always use the worst case scenarios as a
37:06 - benchmark because remember it can never
37:08 - perform worse than the worst case
37:10 - let's plot these values on the graph we
37:12 - started earlier with the number of tris
37:15 - or the runtime of the algorithm on the y
37:17 - axis and the maximum number of values in
37:20 - the series or n on the horizontal axis
37:23 - to represent the worst case scenario we
37:25 - have two data points when n equals 10
37:28 - britney took four tries using binary
37:30 - search and when n equals 100 it took
37:33 - seven tries
37:34 - but even side by side these data points
37:36 - are sort of meaningless
37:38 - remember that while there is quite a
37:40 - difference between the run time of
37:41 - linear search and binary search at an n
37:44 - value of 100 for a computer that
37:46 - shouldn't matter
37:47 - what we should check out is how the
37:49 - algorithm performs at levels of n that
37:52 - might actually slow a computer down
37:54 - as n grows larger and larger how do
37:57 - these algorithms compare to one another
37:59 - let's add that to the graph
38:01 - okay now a picture starts to emerge
38:04 - as n gets really large the performance
38:07 - of these two algorithms differs
38:09 - significantly
38:10 - the difference is kind of staggering
38:12 - actually
38:13 - even with the simple game we saw that
38:14 - binary search was better but now we have
38:17 - a much more complete idea of how much
38:19 - better
38:20 - for example when n is 1000 the runtime
38:24 - of linear search measured by the number
38:26 - of operations or turns is also 1000.
38:29 - for binary search it takes just 10
38:32 - operations
38:33 - now let's look at what happens when we
38:35 - increase n by factor of 10
38:37 - at 10 000 linear search takes 10 000
38:40 - operations while binary search takes 14
38:43 - operations
38:44 - and increased by a factor of 10 in
38:46 - binary search only needs four more
38:48 - operations to find a value
38:51 - if we increase it again by a factor of
38:53 - 10 once more to an n value of 100 000
38:57 - binary search takes only 17 operations
39:00 - it is blazing fast
39:02 - what we've done here is plotted on a
39:04 - graph how the algorithm performs as the
39:07 - input set it is working on increases
39:10 - in other words we've plotted the growth
39:12 - rate of the algorithm also known as the
39:15 - order of growth
39:17 - different algorithms grow at different
39:19 - rates and by evaluating their growth
39:21 - rates we get a much better picture of
39:23 - their performance because we know how
39:26 - the algorithm will hold up as n grows
39:28 - larger
39:29 - this is so important in fact it is the
39:32 - standard way of evaluating an algorithm
39:34 - and brings us to a concept called big o
39:38 - you might have heard this word thrown
39:39 - about and if you found it confusing
39:41 - don't worry we've already built up a
39:43 - definition in the past few videos we
39:46 - just need to bring it all together
39:48 - let's start with a common statement
39:50 - you'll see in studies on algorithms
39:52 - big o is a theoretical definition of the
39:55 - complexity of an algorithm as a function
39:57 - of the size
39:59 - wow what a mouthful this sounds really
40:01 - intimidating but it's really not let's
40:03 - break it down
40:04 - big o is a notation used to describe
40:08 - complexity and what i mean by notation
40:10 - is that it simplifies everything we've
40:12 - talked about down into a single variable
40:16 - an example of complexity written in
40:18 - terms of big o looks like this
40:21 - as you can see it starts with an
40:22 - uppercase letter o that's why we call it
40:25 - big o it's literally a big o
40:28 - the o comes from order of magnitude of
40:31 - complexity so that's where we get the
40:32 - big o from now complexity here refers to
40:35 - the exercise we've been carrying out in
40:37 - measuring efficiency
40:39 - if it takes brittany 4 tries when n is
40:42 - 10
40:42 - how long does the algorithm take when n
40:45 - is 10 million
40:46 - when we use big o for this the variable
40:49 - used which we'll get to distills that
40:51 - information down so that by reading the
40:53 - variable you get a big picture view
40:55 - without having to run through data
40:56 - points and graphs just like we did
40:59 - it's important to remember that
41:01 - complexity is relative
41:03 - when we evaluate the complexity of the
41:05 - binary search algorithm we're doing it
41:07 - relative to other search algorithms not
41:10 - all algorithms
41:12 - bigo is a useful notation for
41:14 - understanding both time and space
41:15 - complexity but only when comparing
41:18 - amongst algorithms that solve the same
41:20 - problem
41:21 - the last bit in that definition of big o
41:23 - is a function of the size and all this
41:26 - means is that big o measures complexity
41:29 - as the input size grows because it's not
41:32 - important to understand how an algorithm
41:33 - performs in a single data set but in all
41:36 - possible data sets
41:38 - you will also see big o referred to as
41:40 - the upper bound of the algorithm and
41:43 - what that means is that big o measures
41:45 - how the algorithm performs in the worst
41:47 - case scenario
41:49 - so that's all big o is
41:51 - nothing special it's just a notation
41:53 - that condenses the data points and
41:55 - graphs that we've built up down to one
41:57 - variable okay so what do these variables
41:59 - look like
42:00 - for john's strategy linear search we say
42:03 - that it has a time complexity of big o
42:06 - and then n so that's again big o with an
42:08 - n inside parentheses
42:10 - for britney strategy binary search we
42:13 - say that it has a time complexity of big
42:15 - o of log n that's big o with something
42:17 - called a log and an n inside parentheses
42:20 - now don't worry if you don't understand
42:22 - that we'll go into that in more detail
42:23 - later on in the course
42:26 - each of these has a special meaning but
42:28 - it helps to work through all of them to
42:29 - get a big picture view so over the next
42:32 - few videos let's examine what are called
42:34 - common complexities or common values of
42:36 - big o that you will run into and should
42:38 - internalize
42:40 - in our discussions of complexity we made
42:42 - one assumption that the algorithm as a
42:44 - whole had a single measure of complexity
42:47 - that isn't true and we'll get at how we
42:49 - arrive at these measures for the entire
42:51 - algorithm at the end of this exercise
42:54 - but each step in the algorithm has its
42:56 - own space and time complexity
42:59 - in linear search for example there are
43:01 - multiple steps and the algorithm goes
43:03 - like this
43:04 - start at the beginning of the list or
43:06 - range of values compare the current
43:08 - value to the target if the current value
43:10 - is the target value that we're looking
43:12 - for we're done
43:13 - if it's not we'll move on sequentially
43:15 - to the next value in the list and repeat
43:17 - step two
43:19 - if we reach the end of the list then the
43:21 - target value is not in the list
43:23 - let's go back to step two for a second
43:25 - comparing the current value to the
43:27 - target
43:28 - does the size of the data set matter for
43:31 - this step
43:32 - when we're at step two we're already at
43:34 - that position in the list and all we're
43:36 - doing is reading the value to make a
43:38 - comparison reading the value is a single
43:41 - operation and if we were to plot it on a
43:44 - graph of runtime per operations against
43:46 - n it looks like this a straight line
43:49 - that takes constant time regardless of
43:52 - the size of n since this takes the same
43:55 - amount of time in any given case we say
43:58 - that the run time is constant time it
44:00 - doesn't change
44:02 - in big o notation we represent this as
44:04 - big o with a 1 inside parentheses now
44:08 - when i first started learning all this i
44:10 - was really confused as to how to read
44:12 - this even if it was in my own head
44:14 - should i say big o of one
44:16 - when you see this written you're going
44:18 - to read this as constant time so reading
44:20 - a value in a list is a constant time
44:23 - operation
44:24 - this is the most ideal case when it
44:26 - comes to run times because input size
44:28 - does not matter and we know that
44:30 - regardless of the size of n the
44:32 - algorithm runtime will remain the same
44:35 - the next step up in complexity so to
44:37 - speak is the situation we encountered
44:39 - with the binary search algorithm
44:42 - traditionally explaining the time
44:44 - complexity of binary search involves
44:46 - math i'm going to try to do it both with
44:48 - and without
44:50 - when we played the game using binary
44:52 - search we notice that with every turn we
44:55 - were able to discard half of the data
44:58 - but there's another pattern that emerges
45:00 - that we didn't explore
45:02 - let's say n equals 10. how long does it
45:04 - take to find an item at the 10th
45:06 - position of the list we can write this
45:08 - out so we go from 10 to 5 to 8 to 9 and
45:12 - then down to 10.
45:14 - here it takes us four tries to cut down
45:16 - the list to just one element and find
45:18 - the value we're looking for
45:20 - let's double the value of n to 20 and
45:23 - see how long it takes for us to find an
45:25 - item at the 20th position so we start at
45:27 - 20 and then we pick 10 from there we go
45:29 - to 15 17 19 and finally 20.
45:33 - so here it takes us five tries
45:36 - okay let's double it again so that n is
45:38 - 40 and we try to find the item in the
45:40 - 40th position
45:42 - so when we start at 40 the first
45:44 - midpoint we're going to pick is 20 from
45:46 - there we go to 30 then 35 37 39 and then
45:50 - 40.
45:51 - notice that every time we double the
45:54 - value of n the number of operations it
45:57 - takes to reduce the list down to a
45:59 - single element only increases by 1.
46:02 - there's a mathematical relationship to
46:04 - this pattern and it's called a logarithm
46:07 - of n
46:08 - you don't really have to know what
46:09 - logarithms truly are but i know that
46:11 - some of you like underlying explainers
46:13 - so i'll give you a quick one
46:15 - if you've taken algebra classes you may
46:18 - have learned about exponents here's a
46:20 - quick refresher
46:22 - 2 times 1 equals 2. now this can be
46:24 - written as 2 raised to the first power
46:27 - because it is our base case two times
46:30 - one is two
46:31 - now two times two is four this can be
46:33 - written as two raised to the second
46:35 - power because we're multiplying two
46:37 - twice first we multiply two times one
46:40 - then the result of that times 2.
46:43 - 2 times 2 times 2 is 8 and we can write
46:46 - this as 2 raised to the 3rd power
46:48 - because we're multiplying 2 3 times
46:51 - in 2 raised to 2 and 2 raised to 3 the 2
46:54 - and 3 there are called exponents and
46:57 - they define how the number grows
47:00 - with 2 raised to 3 we start with the
47:02 - base value and multiply itself 3 times
47:06 - the inverse of an exponent is called a
47:08 - logarithm so if i say log to the base 2
47:12 - of 8 equals 3 i'm basically saying the
47:15 - opposite of an exponent
47:16 - instead of saying how many times do i
47:18 - have to multiply this value i'm asking
47:20 - how many times do i have to divide 8 by
47:23 - two to get the value one
47:25 - this takes three operations
47:27 - what about the result of log to the base
47:29 - two of sixteen that evaluates to four
47:33 - so why does any of this matter
47:35 - notice that this is sort of how binary
47:37 - search works
47:38 - log to the base 2 of 16 is 4.
47:42 - if n was 16 how many triads does it take
47:45 - to get to that last element
47:47 - well we start in the middle at 8 that's
47:49 - too low so we move to 12 then we move to
47:51 - 14 then to 15 and then to 16 which is 5
47:55 - tries or log to the base 2 of 16 plus 1.
48:00 - in general for a given value of n the
48:03 - number of tries it takes to find the
48:05 - worst case scenario
48:06 - is log of n plus one
48:09 - and because this pattern is overall a
48:12 - logarithmic pattern we say that the
48:14 - runtime of such algorithms is
48:16 - logarithmic
48:17 - if we plot these data points on our
48:19 - graph a logarithmic runtime looks like
48:22 - this
48:23 - in big o notation we represent a
48:25 - logarithmic runtime as big o of log n
48:28 - which is written as big o with log n
48:31 - inside parentheses or even sometimes as
48:34 - l n n inside parentheses
48:37 - when you see this read it as logarithmic
48:39 - time
48:40 - as you can see on the graph as n grows
48:43 - really large the number of operations
48:46 - grows very slowly and eventually
48:48 - flattens out
48:50 - since this line is below the line for a
48:52 - linear runtime which we'll look at in a
48:54 - second you might often hear algorithms
48:57 - with logarithmic runtimes being called
48:59 - sublinear
49:00 - logarithmic or sub-linear runtimes are
49:03 - preferred to linear because they're more
49:05 - efficient but in practice linear search
49:08 - has its own set of advantages which
49:09 - we'll take a look at in the next video
49:13 - next up let's look at the situation we
49:15 - encountered with the linear search
49:16 - algorithm
49:17 - we saw that in the worst case scenario
49:20 - whatever the value of n was john took
49:22 - exactly that many tries to find the
49:24 - answer
49:25 - as in linear search when the number of
49:27 - operations to determine the result in
49:29 - the worst case scenario is at most the
49:32 - same as n
49:33 - we say that the algorithm runs in linear
49:36 - time
49:37 - we represent this as big o of n now you
49:40 - can read that as big o of n like i just
49:41 - said or you can say linear time which is
49:44 - more common
49:45 - when we put that up on a graph against
49:47 - constant time and logarithmic time we
49:50 - get a line that looks like this
49:52 - any algorithm that sequentially reads
49:55 - the input will have linear time
49:57 - so remember anytime you know a problem
50:00 - involves reading every item in a list
50:02 - that means a linear run time as you saw
50:05 - from the game we played brittany's
50:07 - strategy using binary search was clearly
50:09 - better and we can see that on the graph
50:12 - so if we had the option why would we use
50:14 - linear search which runs in linear time
50:17 - remember that binary search had a
50:19 - precondition the input set had to be
50:21 - sorted
50:22 - while we won't be looking at sorting
50:24 - algorithms in this course as you learn
50:26 - more about algorithms you'll find that
50:28 - sorting algorithms have varying
50:30 - complexities themselves just like search
50:32 - does so we have to do additional work
50:34 - prior to using binary search
50:37 - for this reason in practice linear
50:40 - search ends up being more performant up
50:42 - to a certain value of n because the
50:44 - combination of sorting first and then
50:47 - searching using binary search adds up
50:50 - the next common complexity you will hear
50:52 - about is when an algorithm runs in
50:54 - quadratic time if the word quadratic
50:56 - sounds familiar to you it's because you
50:59 - might have heard about it in math class
51:01 - quadratic is a word that means an
51:03 - operation raised to the second power or
51:06 - when something is squared
51:08 - let's say you and your friends are
51:10 - playing a tower defense game and to
51:12 - start it off you're going to draw a map
51:14 - of the terrain
51:16 - this map is going to be a grid and you
51:18 - pick a random number to determine how
51:20 - large this grid is let's set n the size
51:23 - of the grid to four
51:25 - next you need to come up with a list of
51:26 - coordinates so you can place towers and
51:28 - enemies and stuff on this map so how
51:31 - would we do this
51:32 - if we start out horizontally we'd have
51:34 - coordinate points that go 1 1 1 2 1 3
51:38 - and 1 4.
51:39 - then you go up one level vertically and
51:41 - we have points 2 1 2 2 2 3 and 2 4.
51:46 - go up one more and you have the points 3
51:48 - 1 3 2 3 3 and 3 4 and on that last row
51:52 - you have the points 4 1 4 2 4 3 and 4 4.
51:57 - notice that we have a pattern here
51:59 - for each row we take the value and then
52:01 - create a point by adding to that every
52:03 - column value
52:05 - the range of values go from 1 to the
52:07 - value of n
52:08 - so we can generally think of it this way
52:11 - for the range of values from 1 to n for
52:13 - each value in that range we create a
52:16 - point by combining that value with the
52:18 - range of values from 1 to n again
52:21 - doing it this way for each value in the
52:23 - range of 1 to n we create an n number of
52:27 - values and we end up with 16 points
52:30 - which is also n times n or n squared
52:34 - this is an algorithm with a quadratic
52:36 - runtime because for any given value of n
52:39 - we carry out n squared number of
52:41 - operations
52:42 - now i picked a relatively easy so to
52:45 - speak example here because in english at
52:47 - least we often denote map sizes by
52:50 - height times width so we would call this
52:52 - a 4 by 4 grid which is just another way
52:55 - of saying 4 squared or n squared
52:58 - in big o notation we would write this as
53:00 - big o of n squared or say that this is
53:03 - an algorithm with a quadratic runtime
53:06 - many search algorithms have a worst case
53:09 - quadratic runtime which you'll learn
53:10 - about soon
53:12 - now in addition to quadratic runtimes
53:14 - you may also run into cubic runtimes as
53:16 - you encounter different algorithms in
53:19 - such an algorithm for a given value of n
53:21 - the algorithm executes n raised to the
53:23 - third power number of operations
53:26 - these aren't as common as quadratic
53:28 - algorithms though so we won't look at
53:30 - any examples but i think it's worth
53:31 - mentioning
53:32 - thrown up on our graph quadratic and
53:35 - cubic runtimes look like this
53:37 - so this is starting to look pretty
53:39 - expensive computationally as they say we
53:42 - can see here that for small changes in n
53:44 - there's a pretty significant change in
53:46 - the number of operations that we need to
53:48 - carry out
53:50 - the next worst case runtime we're going
53:52 - to look at is one that's called
53:53 - quasi-linear and a sort of easier to
53:55 - understand for lack of better word by
53:58 - starting with the big o notation
54:00 - quasi-linear runtimes are written out as
54:03 - big o of n times log n
54:06 - we learned what log n was right a
54:08 - logarithmic runtime whereas n grew the
54:11 - number of operations only increased by a
54:13 - small factor with a quasi-linear runtime
54:16 - what we're saying is that for every
54:18 - value of n we're going to execute a log
54:21 - n number of operations hence the run
54:23 - time of n times log n
54:26 - so you saw earlier with the quadratic
54:28 - runtime that for each value of n we
54:30 - conducted n operations it's sort of the
54:32 - same in that as we go through the range
54:35 - of values in n we're executing login
54:37 - operations
54:38 - in comparison to other runtimes a
54:40 - quasi-linear algorithm has a runtime
54:43 - that lies somewhere between a linear
54:45 - runtime and a quadratic runtime
54:47 - so where would we expect to see this
54:50 - kind of runtime in practical use
54:52 - well sorting algorithms is one place you
54:54 - will definitely see it
54:56 - merge sort for example is a sorting
54:58 - algorithm that has a worst case runtime
55:00 - of big o of n log n
55:02 - let's take a look at a quick example
55:05 - let's say we start off with a list of
55:07 - numbers that looks like this and we need
55:08 - to sort it
55:10 - merge sort starts by splitting this list
55:12 - into two lists down the middle
55:15 - it then takes each sub list and splits
55:17 - that in half down the middle again
55:19 - it keeps doing this until we end up with
55:22 - a list of just a single number
55:24 - when we're down to single numbers we can
55:26 - do one sort operation and merge these
55:29 - sub-lists back in the opposite direction
55:32 - the first part of merge sort cuts those
55:34 - lists into sub-lists with half the
55:36 - numbers
55:38 - this is similar to binary search where
55:40 - each comparison operation cuts down the
55:43 - range to half the values
55:45 - you know the worst case runtime in
55:47 - binary search is log n so these
55:49 - splitting operations have the same
55:51 - runtime big o of log n or logarithmic
55:54 - but splitting into half isn't the only
55:56 - thing we need to do with merge sort we
55:58 - also need to carry out comparison
56:00 - operations so we can sort those values
56:02 - and if you look at each step of this
56:04 - algorithm we carry out an n number of
56:07 - comparison operations and that brings
56:09 - the worst case runtime of this algorithm
56:12 - to n times log n also known as quasi
56:15 - linear don't worry if you didn't
56:17 - understand how merge sort works that
56:19 - wasn't the point of this demonstration
56:21 - we will be covering merge sorts soon in
56:23 - a future course
56:25 - the run times we've looked at so far are
56:27 - all called polynomial runtimes an
56:30 - algorithm is considered to have a
56:31 - polynomial runtime if for a given value
56:34 - of n its worst case runtime is in the
56:37 - form of n raised to the k power where k
56:40 - just means some value so it could be n
56:42 - squared where k equals 2 for a quadratic
56:44 - runtime n cubed for a cubic runtime and
56:47 - so on
56:48 - all of those are in the form of n raised
56:50 - to some power
56:52 - anything that is bounded by this and
56:54 - what i mean by that is if we had a
56:56 - hypothetical line on our graph of n
56:58 - raised to the k power anything that
57:00 - falls under this graph is considered to
57:03 - have a polynomial runtime
57:05 - algorithms with an upper bound or a
57:07 - runtime with a big o value that is
57:09 - polynomial are considered efficient
57:12 - algorithms and are likely to be used in
57:14 - practice
57:15 - now the next class of runtimes that
57:17 - we're going to look at are a runtimes
57:19 - that we don't consider efficient and
57:22 - these are called exponential runtimes
57:25 - with these runtimes as n increases
57:27 - slightly the number of operations
57:29 - increases exponentially and as we'll see
57:32 - in a second these algorithms are far too
57:34 - expensive to be used
57:36 - an exponential runtime is an algorithm
57:39 - with a big o value of some number raised
57:41 - to the nth power
57:43 - imagine that you wanted to break into a
57:45 - locker that had a padlock on it let's
57:47 - assume you forgot your code
57:49 - this lock takes a two digit code and the
57:52 - digit for the code ranges from zero to
57:54 - nine
57:55 - you start by setting the dials to zero
57:57 - and then with the first dial remaining
57:59 - on zero you change the second dial to
58:02 - one and try and open it if it doesn't
58:04 - work you set it to two then try again
58:06 - you would keep doing this and if you
58:08 - still haven't succeeded with the second
58:10 - dial set to 9 then you go back to that
58:12 - first dial set it to 1 and start the
58:15 - second dial over
58:16 - the range of values you'd have to go
58:18 - through is 0 0 to 9 9 which is 100
58:22 - values
58:23 - this can be generalized as 10 to the
58:26 - second power since there are 10 values
58:28 - on each dial raised to two dials
58:31 - searching through each individual value
58:33 - until you stumble on the right one is a
58:35 - strategy called brute force and brute
58:38 - force algorithms have exponential run
58:40 - times
58:41 - here there are two dials so n is 2 and
58:44 - each dial has 10 values so again we can
58:46 - generalize this algorithm as 10 raised
58:48 - to n where n represents the number of
58:51 - dials
58:52 - the reason that this algorithm is so
58:54 - inefficient is because with just one
58:56 - more dial on the lock the number of
58:58 - operations increases significantly
59:01 - with three dials the number of
59:03 - combinations in the worst case scenario
59:05 - where the correct code is the last digit
59:07 - in the range is 10 raised to 3 or 1 000
59:10 - values
59:11 - with an additional wheel it becomes 10
59:13 - raised to 4 or 10 000 values
59:16 - as n increases the number of operations
59:18 - increases exponentially to a point where
59:21 - it's unsolvable in a realistic amount of
59:23 - time
59:24 - now you might think well any computer
59:27 - can crack a four digit numerical lock
59:29 - and that's true because n here is
59:31 - sufficiently small but this is the same
59:34 - principle that we use for passwords
59:36 - in a typical password field implemented
59:39 - well users are allowed to use letters of
59:41 - the english alphabet so up to 26
59:43 - characters numbers from 0 to 9 and a set
59:46 - of special characters of which there can
59:48 - be around 33
59:50 - so typically that means each character
59:52 - in a password can be one out of 69
59:55 - values
59:56 - this means that for a one character
59:58 - password it takes 69 to the nth power so
60:02 - 1 which equals 69 operations in the
60:05 - worst case scenario to figure out the
60:07 - password
60:08 - just increasing n to 2 increases the
60:11 - number of operations needed to guess the
60:13 - password to 69 squared or
60:17 - 4761 operations
60:19 - now usually on a secure website there
60:22 - isn't really a limit but in general
60:24 - passwords are limited to around 20
60:26 - characters in length
60:28 - with each character being a possible 69
60:30 - values and there being 20 characters the
60:33 - number of operations needed to guess the
60:35 - password in the worst case scenario is
60:38 - 69 raised to the 20th power or
60:41 - approximately 6 followed by 36 zeros
60:44 - number of operations
60:46 - an intel cpu with five cores can carry
60:50 - out roughly about 65 000 million
60:52 - instructions per second that's a funny
60:54 - number i know to crack our 20-digit
60:57 - passcode in this very simplistic model
61:00 - it would take this intel cpu
61:02 - to race to 20th power years to brute
61:06 - force the password
61:08 - so while this algorithm would eventually
61:10 - produce a result it is so inefficient
61:12 - that it's pointless
61:14 - this is one of the reasons why people
61:15 - recommend you have longer passwords
61:18 - since brute forcing is exponential in
61:20 - the worst case each character you add
61:23 - increases the number of combinations by
61:24 - an exponent
61:26 - the next class of exponential algorithms
61:29 - is best highlighted by a popular problem
61:31 - known as the traveling salesman
61:34 - the problem statement goes like this
61:36 - given a list of cities and the distance
61:38 - between each pair of cities what is the
61:41 - shortest possible route that visits each
61:43 - city and then returns to the origin city
61:46 - this seems like a simple question but
61:48 - let's start with a simple case three
61:50 - cities a b and c
61:53 - to figure out what the shortest route is
61:55 - we need to come up with all the possible
61:57 - routes
61:58 - with three cities we have six routes in
62:00 - theory at least some of these routes can
62:03 - be discarded because abc is the same as
62:05 - c b a but in the opposite direction
62:08 - but as we do know sometimes going from a
62:11 - to c through b may go through a
62:12 - different route than c to a through b so
62:15 - we'll stick to the six routes and from
62:17 - there we could determine the shortest no
62:19 - big deal
62:20 - now if we increase this to four cities
62:22 - we jump to 24 combinations
62:24 - the mathematical relationship that
62:26 - defines this is called a factorial and
62:29 - is written out as n followed by an
62:31 - exclamation point
62:33 - factorials are basically n times n minus
62:36 - one repeated until you reach the number
62:39 - one so for example the factorial of
62:41 - three is three times two times one which
62:44 - is six which is the number of
62:46 - combinations we came up with for three
62:47 - cities
62:49 - the factorial of four is four times
62:51 - three times two times one or 24 which is
62:54 - the number of combinations we arrived at
62:56 - with four cities
62:58 - in solving the traveling salesman
63:00 - problem the most efficient algorithm
63:03 - will have a factorial runtime or a
63:05 - combinatorial runtime as it's also
63:07 - called
63:09 - at low values of n algorithms with a
63:11 - factorial runtime may be used but with
63:13 - an n value of say 200 it would take
63:16 - longer than humans have been alive to
63:18 - solve the problem
63:19 - for sake of completeness let's plot a
63:22 - combinatorial runtime on our graph so
63:24 - that we can compare
63:26 - an algorithm such as one that solves the
63:28 - traveling salesman problem as a worst
63:30 - case run time of big o of n factorial
63:34 - studying exponential runtimes like this
63:36 - are useful for two reasons
63:38 - first in studying how to make such
63:40 - algorithms efficient we develop
63:42 - strategies that are useful across the
63:44 - board and can potentially be used to
63:46 - make existing algorithms even more
63:48 - efficient
63:50 - second it's important to be aware of
63:51 - problems that take a long time to solve
63:54 - knowing right off the bat that a problem
63:56 - is somewhat unsolvable in a realistic
63:58 - time means you can focus your efforts on
64:01 - other aspects of the problem
64:03 - as beginners though we're going to steer
64:05 - clear of all this and focus our efforts
64:07 - on algorithms with polynomial runtimes
64:09 - since we're much more likely to work
64:11 - with and learn about such algorithms
64:14 - now that we know some of the common
64:15 - complexities in the next video let's
64:18 - talk about how we determine the
64:19 - complexity of an algorithm because there
64:22 - are some nuances
64:24 - over the last few videos we took a look
64:26 - at common complexities that we would
64:27 - encounter in studying algorithms but the
64:30 - question remains how do we determine
64:32 - what the worst case complexity of an
64:34 - algorithm is
64:36 - earlier i mentioned that even though we
64:38 - say that an algorithm has a particular
64:40 - upper bound or worst case runtime each
64:43 - step in a given algorithm can have
64:45 - different run times
64:47 - let's bring up the steps for binary
64:48 - search again
64:50 - assuming the list is sorted the first
64:52 - step is to determine the middle position
64:54 - of the list
64:56 - in general this is going to be a
64:57 - constant time operation
64:59 - many programming languages hold on to
65:02 - information about the size of the list
65:04 - so we don't actually need to walk
65:06 - through the list to determine the size
65:08 - now if we didn't have information about
65:10 - the size of the list we would need to
65:12 - walk through counting each item one by
65:15 - one until we reached the end of the list
65:18 - and this is a linear time operation but
65:21 - realistically this is a big o of 1 or
65:24 - constant time
65:26 - step 2 is to compare the element in the
65:28 - middle position to the target element
65:31 - we can assume that in most modern
65:32 - programming languages this is also a
65:34 - constant time operation because the
65:37 - documentation for the language tells us
65:39 - it is
65:40 - step 3 is our success case and the
65:42 - algorithm ends
65:44 - this is our best case and so far we have
65:47 - only incurred two constant time
65:49 - operations
65:50 - so we would say that the best case run
65:52 - time of binary search is constant time
65:55 - which is actually true
65:56 - but remember that best case is not a
65:58 - useful metric
66:00 - step 4 if we don't match is splitting
66:02 - the list into sub-lists
66:04 - assuming the worst case scenario the
66:07 - algorithm would keep splitting into
66:08 - sub-lists until a single element list is
66:11 - reached with the value that we're
66:13 - searching for
66:14 - the run time for this step is
66:16 - logarithmic since we discard half the
66:18 - values each time
66:20 - so in our algorithm we have a couple
66:22 - steps that are constant time and one
66:25 - step that is logarithmic overall
66:28 - when evaluating the run time for an
66:29 - algorithm we say that the algorithm has
66:32 - as its upper bound the same runtime as
66:35 - the least efficient step in the
66:37 - algorithm
66:38 - think of it this way let's say you're
66:40 - participating in a triathlon which is a
66:43 - race that has a swimming running and a
66:45 - cycling component
66:47 - you could be a phenomenal swimmer and a
66:49 - really good cyclist but you're a pretty
66:51 - terrible runner
66:52 - no matter how fast you are at swimming
66:54 - or cycling your overall race time is
66:57 - going to be impacted the most by your
66:59 - running race time because that's the
67:01 - part that takes you the longest
67:03 - if you take an hour 30 to finish the
67:06 - running component 55 minutes to swim and
67:08 - 38 minutes to bike it won't matter if
67:11 - you can fine tune your swimming
67:13 - technique down to finish in 48 minutes
67:15 - and your cycle time to 35 because you're
67:18 - still bounded at the top by your running
67:20 - time which is close to almost double
67:22 - your bike time
67:24 - similarly with the binary search
67:26 - algorithm it doesn't matter how fast we
67:28 - make the other steps they're already as
67:30 - fast as they can be
67:32 - in the worst case scenario the splitting
67:34 - of the list down to a single element
67:36 - list is what will impact the overall
67:38 - running time of your algorithm
67:40 - this is why we say that the time
67:42 - complexity or run time of the algorithm
67:44 - in the worst case is big o of log n or
67:47 - logarithmic
67:48 - as i alluded to though your algorithm
67:50 - may hit a best case runtime and in
67:53 - between the two best and worst case have
67:55 - an average run time as well
67:57 - this is important to understand because
67:59 - algorithms don't always hit their worst
68:01 - case but this is getting a bit too
68:02 - complex for us for now we can safely
68:05 - ignore average case performances and
68:07 - focus only on the worst case in the
68:10 - future if you decide to stick around
68:11 - we'll circle back and talk about this
68:13 - more
68:15 - now that you know about algorithms
68:16 - complexities and big o let's take a
68:19 - break from all of that and write code in
68:21 - the next video
68:23 - [Music]
68:28 - so far we've spent a lot of time in
68:29 - theory and while these things are all
68:31 - important things to know you get a much
68:33 - better understanding of how algorithms
68:35 - work when you start writing some code as
68:38 - i mentioned earlier we're going to be
68:39 - writing python code in this and all
68:42 - subsequent algorithm courses
68:44 - if you do have programming experience
68:46 - but in another language check the notes
68:48 - section of this video for an
68:50 - implementation in your language
68:52 - if you don't have any experience i'll
68:54 - try my best explain as we go along
68:57 - on the video you're watching right now
68:59 - you should see a launch workspaces
69:01 - button
69:02 - we're going to use a treehouse coding
69:04 - environment call workspaces to write all
69:06 - of our code
69:08 - if you're familiar with using python in
69:10 - a local environment then feel free to
69:12 - keep doing so workspaces is an
69:14 - in-browser coding environment and will
69:16 - take care of all the setup and
69:18 - installation so you can focus on just
69:21 - writing and evaluating code workspaces
69:24 - is quite straightforward to use on the
69:26 - left here we have a file navigator pane
69:29 - which is currently empty since we
69:31 - haven't created a new file
69:33 - on the top we have an editor where we
69:35 - write all our code and then below that
69:37 - we have a terminal or a command line
69:39 - prompt where we can execute the scripts
69:41 - that we write let's add a new file here
69:43 - so at the top in the editor area we're
69:45 - going to go to file new file and we'll
69:48 - name this linear
69:50 - underscore search
69:52 - dot pi
69:54 - in here we're going to define our linear
69:57 - search algorithm as a standalone
69:59 - function
70:00 - we start with the keyword def which
70:03 - defines a function or a block of code
70:06 - and then we give it the name linear
70:08 - underscore search
70:10 - this function will accept two arguments
70:13 - first the list we're searching through
70:15 - and then the target value we're looking
70:18 - for both of these arguments are enclosed
70:20 - in a set of parentheses and there's no
70:22 - space between the name of the function
70:24 - and the arguments
70:26 - after that we have a colon
70:28 - now there might be a bit of confusion
70:30 - here since we already have this target
70:32 - value what are we searching for unlike
70:35 - the game we played at the beginning
70:37 - where john's job was to find the value
70:39 - in a true implementation of linear
70:42 - search we're looking for the position in
70:44 - the list where the value exists
70:46 - if the target is in the list then we
70:48 - return its position
70:50 - and since this is a list that position
70:52 - is going to be denoted by an index value
70:55 - now if the target is not found we're
70:57 - going to return none the choice of what
70:59 - to return in the failure case may be
71:01 - different in other implementations of
71:03 - linear search
71:04 - you can return -1 since that isn't
71:07 - typically an index value
71:09 - you can also raise an exception which is
71:11 - python speak for indicating an error
71:13 - occurred
71:14 - now i think for us the most
71:15 - straightforward value we can return here
71:17 - is none now let's add a comment to
71:19 - clarify this so hit enter to go to the
71:21 - next line
71:22 - and then we're going to add
71:24 - three
71:26 - single quotes
71:27 - and then below that on the next line
71:29 - we'll say returns
71:31 - the position or the index
71:33 - position
71:35 - of the target
71:36 - if found
71:38 - else returns none
71:40 - and then on the next line we'll close
71:42 - off those three quotes
71:43 - this is called a doc string and is a
71:46 - python convention for documenting your
71:48 - code the linear search algorithm is a
71:50 - sequential algorithm that compares each
71:53 - item in the list until the target is
71:55 - found
71:56 - to iterate or loop or walk through our
71:59 - list sequentially we're going to use a
72:02 - for loop
72:03 - now typically when iterating over a list
72:05 - in python we would use a loop like this
72:08 - we'd say for item in list
72:11 - this assigns the value at each index
72:13 - position to that local variable item
72:16 - we don't want this though since we
72:18 - primarily care about the index position
72:21 - instead we're going to use the range
72:23 - function in python to create a range of
72:26 - values that start at 0 and end at the
72:29 - number of items in the list
72:31 - so we'll say 4 i i stands for index here
72:35 - in range
72:36 - starting at 0 and going all the way up
72:38 - to the length of the list
72:42 - we can get the number of items in the
72:44 - list using the len function
72:46 - now going back to our talk on complexity
72:48 - and how individual steps in an algorithm
72:51 - can have its own run times this is a
72:53 - line of code that we would have to be
72:55 - careful about
72:56 - python keeps track of the length of a
72:58 - list so this function call here len list
73:02 - is a constant time operation now if this
73:05 - were a naive implementation let's say we
73:07 - wrote the implementation of the list
73:10 - and we iterate over the list every time
73:12 - we call this length function then we've
73:15 - already incurred a linear cost
73:17 - okay so once we have a range of values
73:19 - that represent index positions in this
73:21 - list we're going to iterate over that
73:23 - using the for loop and assign each index
73:26 - value to this local variable i using
73:29 - this index value we can obtain the item
73:31 - at that position using subscript
73:33 - notation on the list
73:35 - now this is also a constant time
73:37 - operation because the language says so
73:40 - so we'll do if list so once we have this
73:43 - value which we'll get by using subscript
73:45 - notation so we'll say list i
73:47 - once we have this value we'll check if
73:49 - it matches the target so if the value at
73:52 - i
73:53 - equals target
73:55 - well if it does then we'll return that
73:57 - index value because we want the position
74:00 - and once we hit this return statement
74:02 - we're going to terminate our function if
74:04 - the entire for loop is executed and we
74:06 - don't hit this return statement then the
74:08 - target does not exist in the list so at
74:11 - the bottom here we'll say return none
74:14 - even though all the individual
74:16 - operations in our algorithm run in
74:18 - constant time
74:19 - in the worst case scenario this for loop
74:22 - here will have to go through the entire
74:24 - range of values and read every single
74:26 - element in the list
74:28 - therefore giving the algorithm a big o
74:30 - value of n or running in linear time now
74:34 - if you've written code before you've
74:35 - definitely written code like this a
74:37 - number of times and i bet you didn't
74:38 - know but all along you are implementing
74:40 - what is essentially a well-known
74:42 - algorithm
74:43 - so i hope this goes to show you that
74:45 - algorithms are pretty approachable topic
74:48 - like everything else this does get
74:49 - advanced but as long as you take things
74:51 - slow there's no reason for it to be
74:53 - impossible remember that not any block
74:55 - of code counts as an algorithm to be a
74:58 - proper implementation of linear search
75:01 - this block of code must return a value
75:04 - must complete execution in a finite
75:06 - amount of time and must output the same
75:09 - result every time for a given input set
75:11 - so let's verify this with a small test
75:15 - let's write a function called verify
75:18 - that accepts an index value
75:20 - if the value is not none it prints the
75:23 - index position if it is none it informs
75:25 - us that the target was not found in the
75:27 - list so def verify
75:30 - and this is going to take an index value
75:33 - and we'll say if index is not none
75:37 - then we'll print
75:40 - target
75:42 - found at index
75:47 - oops that's a colon here
75:50 - index else
75:54 - that needs to go back
75:56 - there we go
75:57 - else we'll say target
76:01 - not found in list
76:04 - okay using this function let's define a
76:06 - range of numbers now so this will be a
76:09 - list numbers
76:11 - and we'll just go from 1 to
76:14 - let's say 10.
76:20 - now if you've written python code before
76:22 - you know that i can use a list
76:24 - comprehension to make this easier but
76:26 - we'll keep things simple
76:28 - we can now use our linear search
76:29 - function to search for the position of a
76:32 - target value in this list so we can say
76:34 - result
76:35 - equal
76:36 - linear underscore search
76:39 - and we're going to pass in the numbers
76:40 - list that's the one we're searching
76:42 - through and we want to look for the
76:44 - position where the value 12 exists
76:47 - and then we'll verify
76:48 - this result
76:50 - if our algorithm works correctly the
76:53 - verify function should inform us that
76:55 - the target did not exist so make sure
76:57 - you save the file which you can do by
76:58 - going up to file and save or hitting
77:00 - command s
77:02 - and then below in the terminal
77:06 - you're going to type out python
77:08 - linear search or you can hit tab and it
77:11 - should auto complete linear search dot
77:13 - pi
77:14 - as you can see correct the target was
77:16 - not found in the list so the output of
77:18 - our script is what we expect
77:20 - for our second test let's search for the
77:22 - value 6 in the list so you can copy this
77:26 - command c to copy and then paste it
77:28 - again and we'll just change 12 here to 6
77:31 - and then come back down to the terminal
77:33 - hit the up arrow to execute the same
77:35 - command again and hit enter you'll
77:37 - notice that i forgot to hit save so it
77:39 - did not account for that new change
77:41 - we'll try that again
77:42 - and there you'll see that if it works
77:45 - correctly which it did the index should
77:48 - be number five run the program on your
77:50 - end and make sure everything works as
77:52 - expected
77:53 - our algorithm returned a result in each
77:55 - case it executed in a finite time and
77:58 - the results were the ones we expect in
78:00 - the next video let's tackle binary
78:02 - search
78:03 - in the last video we left off with an
78:05 - implementation of linear search
78:08 - let's do the same for binary search so
78:09 - that we get an understanding of how this
78:11 - is represented in code
78:13 - so we'll do this in a new file back to
78:15 - file new file
78:17 - and we'll name this one binary
78:20 - search
78:22 - dot
78:23 - py
78:24 - like before we're going to start with a
78:25 - function named binary search so we'll
78:27 - say def
78:28 - binary underscore search
78:31 - that takes a list and a target
78:34 - if you remember binary search works by
78:36 - breaking the array or list down into
78:39 - smaller sets until we find the value
78:41 - we're looking for
78:43 - we need a way to keep track of the
78:45 - position of the list that we're working
78:47 - with so let's create two variables first
78:50 - and last to point to the beginning and
78:52 - end of the array so first equal
78:56 - zero now if you're new to programming
78:59 - list positions are represented by index
79:01 - values that start at zero instead of one
79:04 - so here we're setting first to zero to
79:07 - point to the first element in the list
79:09 - last is going to point to the last
79:11 - element in the list so we'll say last
79:14 - equal
79:15 - len list
79:17 - minus one now this may be confusing to
79:20 - you so a quick sidebar to explain what's
79:22 - going on
79:23 - let's say we have a list containing 5
79:25 - elements if we called len on that list
79:28 - we should get 5 back because there are 5
79:30 - elements
79:31 - but remember that because the position
79:33 - numbers start at 0 the last value is not
79:36 - at position 5 but at 4. in nearly all
79:39 - programming languages getting the
79:41 - position of the last element in the list
79:43 - is obtained by determining the length of
79:46 - the list and deducting 1 which is what
79:48 - we're doing
79:49 - okay so we know what the first and last
79:52 - positions are when we start the
79:53 - algorithm
79:54 - for our next line of code we're going to
79:56 - create a while loop
79:58 - a while loop takes a condition and keeps
80:00 - executing the code inside the loop until
80:03 - the condition evaluates to false
80:06 - for our condition we're going to say to
80:08 - keep executing this loop until the value
80:11 - of first is less than or equal to the
80:14 - value of last
80:16 - so while first less than or equal to
80:19 - last
80:21 - well why you ask why is this our
80:22 - condition well let's work through this
80:24 - implementation and then a visualization
80:27 - should help
80:28 - inside the while loop we're going to
80:30 - calculate the midpoint of our list since
80:32 - that's the first step of binary search
80:35 - midpoint equal
80:37 - so we'll say first
80:39 - plus last
80:41 - and then we'll use the floor division
80:43 - double slash here
80:44 - divided by two
80:46 - now the two forward slashes here are
80:48 - what python calls a floor division
80:50 - operator what it does is it rounds down
80:52 - to the nearest whole number so if we
80:55 - have an eight element array first is
80:57 - zero last is 7 if we divided 0 plus 7
81:02 - which is 7 by 2 we would get 3.5 now 3.5
81:06 - is not a valid index position so we
81:08 - round that down to 3 using the floor
81:10 - division operator okay so now we have a
81:13 - midpoint the next step of binary search
81:15 - is to evaluate whether the value at this
81:18 - midpoint is the same as the target we're
81:20 - looking for so say if list
81:23 - value at midpoint
81:25 - equals the target
81:28 - well if it is then we'll go ahead and
81:30 - return the midpoint
81:32 - so we'll say return
81:33 - midpoint
81:34 - the return statement terminates our
81:36 - algorithm and over here we're done this
81:39 - is our best case scenario
81:42 - next we'll say else if
81:44 - list at midpoint
81:48 - or value at midpoint is less than the
81:50 - target now here if the value is less the
81:53 - value at midpoint is less than the
81:55 - target then we don't care about any of
81:57 - the values lower than the midpoint so we
82:00 - redefine first
82:02 - to point to the value after the midpoint
82:05 - so we'll say midpoint plus 1.
82:07 - now if the value at the midpoint is
82:10 - greater than the target then we can
82:12 - discard the values after the midpoint
82:14 - and redefine last to point to the value
82:17 - prior to the midpoint so we'll say else
82:22 - last equal midpoint
82:25 - minus 1.
82:26 - let's visualize this we're going to
82:28 - start with a list of nine integers
82:31 - to make this easier to understand let's
82:33 - specify these integers to be of the same
82:35 - value as its index position so we have a
82:38 - range of values from 0 to 8.
82:40 - our target is the worst case scenario
82:42 - we're looking for the position of the
82:44 - value 8. at the start our algorithm sets
82:47 - first to point to the index 0 and last
82:51 - to point to the length of the list minus
82:53 - 1 which is 8.
82:55 - next we hit our while loop the logic of
82:57 - this loop is going to be executed as
82:59 - long as the value of first is not
83:01 - greater than the value of last or as
83:04 - we've defined it we're going to keep
83:06 - executing the contents of the loop as
83:08 - long as first is less than or equal to
83:10 - last
83:12 - on the first pass this is true so we
83:14 - enter the body of the loop
83:16 - the midpoint is first plus last divided
83:18 - by two and rounded down so we get a nice
83:21 - even four the value at this position is
83:24 - four now this is not equal to the target
83:26 - so we move to the first else if
83:29 - four is less than eight so now we
83:32 - redefine first to point to midpoint plus
83:34 - one which is five
83:37 - first is still less than last so we run
83:40 - through the body of the loop again the
83:42 - midpoint is now six
83:44 - six is less than eight so we move first
83:47 - to point to seven
83:49 - seven is still less than or equal to
83:51 - eight so we go for another iteration of
83:53 - the loop
83:54 - the midpoint is seven oddly enough and
83:57 - seven is still less than the target so
83:59 - we move first to point to eight first is
84:02 - equal to last now but our condition says
84:05 - keep the loop going as long as first is
84:08 - less than or equal to last so this is
84:11 - our final time through the loop
84:13 - the midpoint is now 8 which makes the
84:16 - value at the midpoint equal to the
84:18 - target and we finally exit our algorithm
84:21 - and return the position of the target
84:23 - now what if we had executed all this
84:25 - code and never hit a case where midpoint
84:28 - equal the target well that would mean
84:30 - the list did not contain the target
84:32 - value so after the while loop at the
84:35 - bottom
84:36 - will return
84:37 - none
84:38 - we have several operations that make up
84:41 - our binary search algorithm so let's
84:43 - look at the runtime of each step we
84:45 - start by assigning values to first and
84:48 - last
84:49 - the value assigned to last involves a
84:51 - call to the len function to get the size
84:54 - of the list but we already know this is
84:56 - a constant time operation in python so
84:59 - both of these operations run in constant
85:01 - time
85:02 - inside the loop we have another value
85:05 - assignment and this is a simple division
85:07 - operation so again the runtime is
85:09 - constant
85:10 - in the next line of code we're reading a
85:12 - value from the list and comparing the
85:14 - midpoint to the target both of these
85:17 - again are constant time operations the
85:20 - remainder of the code is just a series
85:22 - of comparisons and value assignments and
85:25 - we know that these are all constant time
85:27 - operations as well
85:28 - so if all we have are a series of
85:30 - constant time operations why does this
85:33 - algorithm have in the worst case a
85:35 - logarithmic runtime
85:37 - it's hard to evaluate by just looking at
85:39 - the code but the while loop is what
85:41 - causes the run time to grow
85:44 - even though all we're doing is a
85:45 - comparison operation by redefining first
85:48 - and last
85:50 - over here or rather in the last two
85:52 - steps over here we're asking the
85:54 - algorithm to run as many times as it
85:57 - needs until first is equal or greater
86:00 - than last
86:01 - now each time the loop does this the
86:03 - size of the data set the size of the
86:05 - list grows smaller by a certain factor
86:08 - until it approaches a single element
86:11 - which is what results in the logarithmic
86:13 - runtime
86:14 - okay just like with linear search let's
86:17 - test that our algorithm works so we'll
86:19 - go back to linear search.hi
86:21 - and we're going to copy paste
86:23 - so command c to copy if you're on a mac
86:26 - then go back to binary search and at the
86:28 - bottom
86:30 - oops
86:31 - we're going to paste in that verify
86:33 - function
86:34 - okay we'll also go back and grab this
86:36 - numbers
86:38 - you know what let's go ahead and copy
86:39 - all all of these things so numbers and
86:41 - the two verify cases we'll paste that in
86:45 - as well
86:46 - and the only thing we need to change
86:47 - here is instead of calling linear search
86:49 - this is going to call binary search
86:54 - okay we'll hit command s to save the
86:56 - file and then i'm going to drag up my
86:59 - console and we'll run python binary
87:02 - search dot
87:03 - and hit enter and you'll see like just
87:05 - like before we get the same results back
87:08 - now note that an extremely important
87:10 - distinction needs to be made here
87:12 - the numbers list that we've defined
87:15 - for our test cases
87:17 - right here
87:18 - has to be sorted the basic logic of
87:21 - binary search relies on the fact that if
87:23 - the target is greater than the midpoint
87:26 - then our potential values lie to the
87:28 - left or vice versa since the values are
87:31 - sorted in ascending order if the values
87:34 - are unsorted our implementation of
87:36 - binary search may return none even if
87:39 - the value exists in the list
87:42 - and just like that you've written code
87:44 - to implement two search algorithms how
87:46 - fun was that
87:47 - hopefully this course has shown you that
87:49 - it isn't a topic to be afraid of and
87:51 - that algorithms like any other topic
87:54 - with code can be broken down and
87:55 - understood piece by piece
87:58 - now we have a working implementation of
88:00 - binary search but there's actually more
88:02 - than one way to write it so in the next
88:04 - video let's write a second version
88:07 - i'm going to create a new file
88:09 - as always file new file
88:12 - and we'll name this recursive
88:15 - underscore binary underscore search dot
88:18 - p
88:20 - y
88:21 - okay so we're going to add our new
88:23 - implementation here so that we don't get
88:25 - rid of that first implementation we
88:27 - wrote let's call this new function
88:29 - recursive binary search
88:31 - unlike our previous implementation this
88:33 - version is going to behave slightly
88:35 - differently in that it won't return the
88:38 - index value of the target element if it
88:40 - exists
88:41 - instead it will just return a true value
88:43 - if it exists and a false if it doesn't
88:46 - so recursive
88:49 - underscore binary underscore search
88:52 - and like before this is going to take a
88:54 - list it accepts a list and a target to
88:57 - look for in that list
88:59 - we'll start the body of the function by
89:01 - considering what happens if an empty
89:04 - list is passed in in that case we would
89:06 - return false so i would say if the
89:08 - length of the list which is one way to
89:10 - figure out if it's empty if it's equal
89:12 - to zero
89:14 - then we'll return false
89:16 - now you might be thinking that in the
89:18 - previous version of binary search we
89:20 - didn't care if the list was empty well
89:22 - we actually did but in a roundabout sort
89:25 - of way so in the previous version of
89:27 - binary search our function had a loop
89:30 - and that loop condition was true when
89:33 - first was less than or equal to last so
89:36 - as long as it's less than or equal to
89:38 - last we continue the loop
89:40 - now if we have an empty list then first
89:42 - is greater than last and the loop would
89:45 - never execute and we return none at the
89:47 - bottom
89:48 - so this is the same logic we're
89:50 - implementing here we're just doing it in
89:51 - a slightly different way if the list is
89:54 - not empty we'll implement an else clause
89:57 - now here we'll calculate the midpoint
90:01 - by dividing the length of the list by 2
90:04 - and rounding down
90:05 - again there's no use of first and last
90:08 - here so we'll say length of list
90:11 - and then using the floor division
90:13 - operator we'll divide that by 2.
90:15 - if the value at the midpoint which we'll
90:18 - check by saying if list
90:20 - using
90:21 - subscript notation we'll say midpoint as
90:24 - the index now if this value at the
90:27 - midpoint is the same as the target
90:30 - then we'll go ahead and return true
90:34 - so far this is more or less the same
90:37 - except for the value that we're
90:39 - returning
90:40 - let me actually get rid of all that
90:45 - okay
90:46 - all right so if this isn't the case
90:48 - let's implement an else clause now here
90:50 - we have two situations so first if the
90:53 - value at the midpoint is less than the
90:55 - target so if
90:58 - value at midpoint
91:01 - is less than the target
91:04 - then we're going to do something new
91:06 - we're going to call this function again
91:09 - this recursive binary search function
91:12 - that we're in the process of defining
91:14 - we're going to call that again and we're
91:16 - going to give it the portion of the list
91:18 - that we want to focus on in the previous
91:20 - version of binary search we moved the
91:23 - first value to point to the value after
91:26 - the midpoint
91:27 - now here we're going to create a new
91:29 - list using what is called a slice
91:31 - operation and create a sub list that
91:34 - starts at midpoint plus 1 and goes all
91:37 - the way to the end
91:39 - we're going to specify the same target
91:41 - as a search target and when this
91:43 - function call is done we'll return the
91:45 - value so we'll say return the return is
91:48 - important
91:50 - then we'll call this function again
91:51 - recursive
91:53 - binary search
91:55 - and this function takes a list and here
91:57 - we're going to use that subscript
91:59 - notation to perform a slice operation by
92:02 - using two indexes a start and an end so
92:05 - we'll say our new list that we're
92:06 - passing in needs to start at midpoint
92:08 - plus one
92:10 - and then we'll go all the way to the end
92:12 - and this is a
92:13 - python syntactic sugar so to speak if i
92:16 - don't specify an end index python knows
92:19 - to just go all the way to the end all
92:21 - right so this is our new list that we're
92:22 - working with
92:24 - and we need a target we'll just pass it
92:26 - through if you're confused bear with me
92:28 - just like before we'll visualize this at
92:30 - the end
92:31 - okay we have another else case here
92:34 - and this is a scenario where the value
92:36 - at the midpoint is greater than the
92:38 - target
92:39 - which means we only care about the
92:40 - values in the list from the start going
92:43 - up to the midpoint now in this case as
92:45 - well we're going to call the binary
92:47 - search function again and specify a new
92:49 - list to work with this time the list is
92:52 - going to start at the beginning and then
92:53 - go all the way up to the midpoint so it
92:56 - looks the same we'll say return
92:58 - recursive
93:01 - binary search
93:03 - we're going to pass in a list here so if
93:05 - we just put a colon here
93:07 - without a start index python knows to
93:10 - start at the beginning and we're going
93:11 - to go all the way up to the midpoint
93:14 - the target here is the same
93:16 - and this is our new binary search
93:18 - function so let's see if this works
93:22 - actually
93:23 - yes
93:24 - down here we'll make some space
93:26 - and we'll define a verify function
93:29 - we're not going to copy paste the
93:30 - previous one
93:31 - because we're not returning none or an
93:34 - integer here so we'll verify the result
93:36 - that we pass in and we'll say print
93:39 - target found
93:41 - and this is just going to say true or
93:43 - false whether we found it
93:45 - okay so like before we need a numbers
93:47 - list
93:48 - and we'll do something one two three
93:50 - four all the way up to eight
93:54 - okay and now let's test this out so
93:55 - we'll call
93:57 - our recursive
94:00 - binary search function
94:02 - and we'll pass in the numbers list
94:05 - and the target here is 12.
94:08 - we're going to verify this
94:11 - verify the result make sure it works and
94:13 - then we'll call it again this time
94:15 - making sure that we give it a target
94:16 - that is actually in the list so here
94:18 - we'll say 6
94:19 - and we'll verify this again
94:23 - make sure you hit command s to save
94:26 - and then in the console below we're
94:28 - going to type out
94:30 - python recursive binarysearch.pi
94:34 - run it and you'll see that we've
94:35 - verified that search works
94:38 - while we can't verify the index position
94:39 - of the target value which is a
94:41 - modification to how our algorithm works
94:44 - we can guarantee by running across all
94:46 - valid inputs that search works as
94:48 - intended
94:49 - so why write a different search
94:52 - algorithm here a different binary search
94:54 - algorithm and what's the difference
94:56 - between these two implementations anyway
94:58 - the difference lies in these last four
95:02 - lines of code that you see here
95:05 - we did something unusual here now before
95:08 - we get into this a small word of advice
95:11 - this is a confusing topic and people get
95:13 - confused by it all the time
95:15 - don't worry that doesn't make you any
95:18 - less of a programmer in fact i have
95:20 - trouble with it often and always look it
95:22 - up including when i made this video
95:24 - this version of binary search is a
95:26 - recursive binary search
95:29 - a recursive function is one that calls
95:31 - itself
95:33 - this is hard for people to grasp
95:35 - sometimes because there's few easy
95:37 - analogies that make sense but you can
95:39 - think of it and sort this way so let's
95:41 - say you have this book that contains
95:43 - answers to multiplication problems
95:46 - you're working on a problem and you look
95:48 - up an answer
95:49 - in the book the answer for your problem
95:51 - says add 10 to the answer for problem 52
95:56 - okay so you look up problem 52 and there
95:59 - it says add 12 to the answer for problem
96:02 - 85
96:04 - well then you go and look up the answer
96:05 - to problem 85 and finally instead of
96:08 - redirecting you somewhere else that
96:10 - answer says 10. so you take that 10 and
96:13 - then you go back to problem 52 because
96:15 - remember the answer for problem 52 was
96:18 - to add 12 to the answer for problem 85
96:22 - so you take that 10 and then you now
96:24 - have the answer to problem 85 so you add
96:27 - 10 to 12 to get 22.
96:29 - then you go back to your original
96:31 - problem where it said to add 10 to the
96:33 - answer for problem 52 so you add 10 to
96:36 - 22 and you get 32 to end up with your
96:38 - final answer so that's a weird way of
96:41 - doing it but this is an example of
96:42 - recursion
96:44 - the solution to your first lookup in the
96:46 - book was the value obtained by another
96:49 - lookup in the same book which was
96:51 - followed by yet another lookup in the
96:53 - same book the book told you to check the
96:55 - book until you arrived at some base
96:57 - value
96:59 - our function works in a similar manner
97:01 - so let's visualize this with an example
97:03 - of list
97:04 - like before we have a nine element list
97:07 - here with values zero through eight
97:09 - the target we're searching for is the
97:12 - value eight
97:13 - we'll check if the list is empty by
97:15 - calling len on it this list is not empty
97:18 - so we go to the else clause next we
97:20 - calculate the midpoint 9 divided by 2 is
97:22 - 4.5 rounded down is 4 so our first
97:26 - midpoint value is 4.
97:28 - we'll perform our first check is the
97:30 - value at the midpoint equal to the
97:32 - target
97:33 - not true so we go to our else clause
97:35 - we'll perform another check here is the
97:38 - value at the midpoint less than the
97:39 - target now in our case this is true
97:42 - earlier when we evaluated this condition
97:44 - we simply change the value of first
97:47 - here we're going to call the recursive
97:49 - binary search function again and give it
97:51 - a new list to work with
97:53 - the list starts at midpoint plus 1 so at
97:57 - index position 5 all the way to the end
98:00 - notice that this call to recursive
98:02 - binary search inside of recursive binary
98:05 - search includes a return statement
98:08 - this is important and we'll come back to
98:10 - that in a second
98:12 - so now we're back at the top
98:14 - of a new call to recursive binary search
98:17 - with effectively a new list although
98:20 - technically just a sub list of the first
98:22 - one
98:23 - the list here contains the numbers 6 7
98:26 - and 8.
98:27 - starting with the first check the list
98:29 - is not empty so we move to the else
98:32 - the midpoint in this case length of the
98:35 - list 3 divided by 2 rounded down is 1.
98:39 - is the value of the midpoint equal to
98:40 - the target well the value at that
98:42 - position is 7 so no in the else we
98:46 - perform the first check is the value at
98:49 - the midpoint less than the target indeed
98:51 - it is so we call recursive binary search
98:54 - again and provided a new list
98:56 - this list starts at midpoint plus 1 and
98:59 - goes to the end so in this case that's a
99:01 - single element list
99:03 - since this is a new call to recursive
99:05 - binary search we start back up at the
99:08 - top
99:09 - is the list empty no the midpoint is
99:12 - zero
99:13 - is the value at the midpoint the same as
99:15 - the target it is so now we can return
99:18 - true
99:19 - remember a minute ago i pointed out that
99:22 - when we call recursive binary search
99:24 - from inside the function itself it's
99:26 - preceded by a return statement
99:29 - that plays a pretty important role here
99:31 - so back to our visualization
99:34 - we start at the top and recall binary
99:36 - search with a new list but because
99:38 - that's got a return statement before it
99:40 - what we're saying is hey when you run
99:42 - binary search on this whatever value you
99:45 - get back return it to the function that
99:48 - called you
99:49 - then at the second level we call binary
99:51 - search again along with another return
99:53 - statement like with the first call we're
99:56 - instructing the function to return a
99:58 - value back to the code that called it
100:01 - at this level we find the target so the
100:03 - function returns true back to the caller
100:06 - but since this inner function was also
100:08 - called by a function with instructions
100:10 - to return it keeps returning that true
100:12 - value back up until we reach the very
100:15 - first function that called it going back
100:17 - to our book of answers recursive binary
100:20 - search instructs itself to keep working
100:22 - on the problem until it has a concrete
100:24 - answer
100:25 - once it does it works its way backwards
100:28 - giving the answer to every function that
100:30 - called it until the original caller has
100:33 - an answer
100:34 - now like i said at the beginning this is
100:36 - pretty complicated so you should not be
100:39 - concerned if this doesn't click honestly
100:41 - this is not one thing that you're going
100:42 - to walk away with knowing fully how to
100:44 - understand recursion after your first
100:46 - try i'm really not lying when i say i
100:48 - have a pretty hard time with recursion
100:51 - now before we move on i do want to point
100:52 - out one thing
100:54 - even though the implementation of
100:56 - recursion is harder to understand
100:58 - it is easier in this case to understand
101:01 - how we arrive at the logarithmic run
101:03 - time since we keep calling the function
101:06 - with smaller lists let's take a break
101:08 - here in the next video let's talk a bit
101:11 - more about recursion and why it matters
101:14 - [Music]
101:19 - in the last video we wrote a version of
101:21 - binary search that uses a concept called
101:23 - recursion
101:25 - recursion might be a new concept for you
101:27 - so let's formalize how we use it
101:30 - a recursive function is one that calls
101:33 - itself
101:34 - in our example the recursive binary
101:36 - search function called itself inside the
101:40 - body of the function
101:42 - when writing a recursive function you
101:44 - always need a stopping condition
101:46 - and typically we start the body of the
101:48 - recursive function with this stopping
101:50 - condition it's common to call this
101:53 - stopping condition the base case
101:55 - in our recursive binary search function
101:58 - we had two stopping conditions
102:01 - the first was what the function should
102:03 - return if an empty list is passed in
102:07 - it seems weird to evaluate an empty list
102:09 - because you wouldn't expect to run
102:11 - search on an empty list but if you look
102:14 - at how our function works recursive
102:16 - binary search keeps calling itself and
102:19 - with each call to itself the size of the
102:21 - list is cut in half
102:23 - if we searched for a target that didn't
102:25 - exist in the list then the function
102:28 - would keep halving itself until it got
102:30 - to an empty list
102:32 - consider a three element list with
102:34 - numbers one two three where we're
102:36 - searching for a target of four
102:39 - on the first pass the midpoint is 2 so
102:42 - the function would call itself with the
102:44 - list 3.
102:45 - on the next pass the midpoint is 0 and
102:48 - the target is still greater so the
102:50 - function would call itself this time
102:52 - passing in an empty list because an
102:55 - index of 0 plus 1 in a single element
102:58 - list doesn't exist
102:59 - when we have an empty list this means
103:02 - that after searching through the list
103:04 - the value wasn't found
103:06 - this is why we define an empty list as a
103:08 - stopping condition or a base case that
103:10 - returns false if it's not an empty list
103:13 - then we have an entirely different set
103:15 - of instructions we want to execute
103:17 - first we obtain the midpoint of the list
103:20 - once we have the midpoint we can
103:22 - introduce our next base case or stopping
103:24 - condition
103:25 - if the value at the midpoint is the same
103:28 - as the target then we return true
103:31 - with these two stopping conditions we've
103:33 - covered all possible paths of logic
103:36 - through the search algorithm you can
103:38 - either find the value or you don't
103:41 - once you have the base cases the rest of
103:43 - the implementation of the recursive
103:45 - function is to call the function on
103:47 - smaller sub-lists until we hit one of
103:50 - these base cases going back to our
103:52 - visualization for a second we see that
103:54 - recursive binary search calls itself a
103:57 - first time which then calls itself again
104:00 - for the initial list we started with the
104:03 - function only calls itself a few times
104:05 - before a stopping condition is reached
104:08 - the number of times a recursive function
104:10 - calls itself is called recursive depth
104:13 - now the reason i bring all of this up is
104:16 - because if after you start learning
104:18 - about algorithms you decide you want to
104:20 - go off and do your own research you may
104:22 - start to see a lot of algorithms
104:24 - implemented using recursion
104:27 - the way we implemented binary search the
104:29 - first time is called an iterative
104:32 - solution
104:33 - now when you see the word iterative it
104:35 - generally means the solution was
104:36 - implemented using a loop structure of
104:38 - some kind
104:40 - a recursive solution on the other hand
104:42 - is one that involves a set of stopping
104:44 - conditions and a function that calls
104:46 - itself computer scientists and computer
104:49 - science textbooks particularly from back
104:51 - in the day
104:52 - favor and are written in what are called
104:55 - functional languages
104:57 - in functional languages we try to avoid
104:59 - changing data that is given to a
105:01 - function
105:02 - in our first version of binary search we
105:04 - created first and last variables using
105:07 - the list and then modified first and
105:09 - last as we needed to arrive at a
105:11 - solution
105:12 - functional languages don't like to do
105:14 - this all this modification of variables
105:16 - and prefer a solution using recursion
105:19 - a language like python which is what
105:21 - we're using is the opposite and doesn't
105:24 - like recursion in fact python has a
105:27 - maximum recursion depth after which our
105:30 - function will halt execution python
105:32 - prefers an iterative solution now i
105:35 - mentioned all of this for two reasons
105:38 - if you decide that you want to learn how
105:40 - to implement the algorithm in a language
105:42 - of your choice that's not python then
105:45 - you might see a recursive solution as
105:47 - the best implementation in that
105:49 - particular language
105:51 - i'm an ios developer for example and i
105:53 - work with a language called swift
105:55 - swift is different from python in that
105:58 - it doesn't care about recursion depth
106:00 - and does some neat tricks where it
106:01 - doesn't even matter how many times your
106:03 - function calls itself
106:05 - so if you want to see this in swift code
106:07 - then you need to know how recursion
106:09 - works
106:10 - well and now you have some idea now the
106:12 - second reason i bring it up is actually
106:14 - way more important and to find out on to
106:16 - the next video
106:17 - at the beginning of this series i
106:19 - mentioned that there were two ways of
106:20 - measuring the efficiency of an algorithm
106:23 - the first was time complexity or how the
106:25 - run time of an algorithm grows as n
106:27 - grows larger
106:29 - the second is space complexity
106:31 - we took a pretty long route to build up
106:33 - this example but now we're in a good
106:35 - place to discuss space complexity
106:38 - space complexity is a measure of how
106:40 - much working storage or extra storage is
106:43 - needed as a particular algorithm grows
106:47 - we don't think about it much these days
106:49 - but every single thing we do on a
106:51 - computer takes up space in memory in the
106:54 - early days of computing considering
106:56 - memory usage was of paramount importance
106:58 - because memory was limited and really
107:01 - expensive
107:02 - these days were spoiled our devices are
107:04 - rich with memory this is okay when we
107:07 - write everyday code because most of us
107:10 - aren't dealing with enormously large
107:11 - data sets
107:13 - when we write algorithms however we need
107:15 - to think about this because we want to
107:17 - design our algorithms to perform as
107:19 - efficiently as it can as the size of the
107:22 - data set n grows really large
107:25 - like time complexity space complexity is
107:28 - measured in the worst case scenario
107:30 - using big-o notation
107:32 - since you are familiar with the
107:33 - different kinds of complexities let's
107:35 - dive right into an example
107:38 - in our iterative implementation of
107:40 - binary search the first one we wrote
107:43 - that uses a while loop let's look at
107:45 - what happens to our memory usage as n
107:47 - gets large
107:49 - let's bring up that function
107:52 - let's say we start off with a list of 10
107:54 - elements now inspecting the code we see
107:57 - that our solution relies heavily on
107:58 - these two variables first and last
108:02 - first points to the start of the list
108:03 - and last to the end
108:05 - when we eliminate a set of values we
108:08 - don't actually create a sub list instead
108:10 - we just redefine first
108:12 - and last as you see here
108:15 - to point to a different section of the
108:17 - list
108:18 - since the algorithm only considers the
108:20 - values between first and last when
108:22 - determining the midpoint
108:25 - by redefining first and last as the
108:27 - algorithm proceeds we can find a
108:29 - solution using just the original list
108:32 - this means that for any value of n
108:34 - the space complexity of the iterative
108:37 - version of binary search is constant or
108:40 - that the iterative version of binary
108:42 - search takes constant space
108:45 - remember that we would write this as big
108:47 - o of one
108:48 - this might seem confusing because as n
108:51 - grows we need more storage to account
108:53 - for that larger list size
108:55 - now this is true but that storage is not
108:58 - what space complexity cares about
109:00 - measuring
109:01 - we care about what additional storage is
109:03 - needed as the algorithm runs and tries
109:06 - to find a solution
109:08 - if we assume something simple say that
109:10 - for a given size of a list represented
109:12 - by a value n it takes n amount of space
109:16 - to store it whatever that means
109:18 - then for the iterative version of binary
109:20 - search regardless of how large the list
109:23 - is at the start middle and end of the
109:26 - algorithm process the amount of storage
109:29 - required does not get larger than n
109:32 - and this is why we consider it to run in
109:34 - constant space
109:36 - now this is an entirely different story
109:38 - with the recursive version however in
109:40 - the recursive version of binary search
109:42 - we don't make use of variables to keep
109:44 - track of which portion of the list we're
109:46 - working with
109:47 - instead we create new lists every time
109:50 - with a subset of values or sub-lists
109:53 - with every recursive function call
109:56 - let's assume we have a list of size n
109:58 - and in the worst case scenario the
110:00 - target element is the last in the list
110:03 - calling the recursive implementation of
110:05 - binary search on this list and target
110:08 - would lead to a scenario like this
110:10 - the function would call itself and
110:12 - create a new list that goes from the
110:14 - midpoint to the end of the list
110:17 - since we're discarding half the values
110:19 - the size of the sub list is n by 2.
110:22 - this function will keep calling itself
110:24 - creating a new sub list that's half the
110:26 - size of the current one until it arrives
110:29 - at a single element list and a stopping
110:31 - condition
110:32 - this pattern that you see here where the
110:35 - size of the sublist is reduced by a
110:37 - factor on each execution of the
110:39 - algorithmic logic well we've seen that
110:41 - pattern before do you remember where
110:44 - this is exactly how binary search works
110:46 - it discards half the values every time
110:49 - until it finds a solution now we know
110:51 - that because of this pattern the running
110:53 - time of binary search is logarithmic
110:56 - in fact the space complexity of the
110:58 - recursive version of binary search is
111:00 - the same
111:01 - if we start out with a memory allocation
111:04 - of size n that matches the list
111:06 - on each function call of recursive
111:09 - binary search we need to allocate
111:11 - additional memory of size n by 2 n by 4
111:14 - and so on until we have a sub list that
111:17 - is either empty or contains a single
111:19 - value because of this we say that the
111:22 - recursive version of the binary search
111:24 - algorithm runs in logarithmic time with
111:27 - a big o of log n
111:29 - now there's an important caveat here
111:31 - this totally depends on the language
111:34 - remember how i said that a programming
111:36 - language like swift can do some tricks
111:38 - to where recursion depth doesn't matter
111:40 - the same concept applies here if you
111:43 - care to read more about this concept
111:45 - it's called tail
111:46 - optimization it's called tail
111:48 - optimization because if you think of a
111:51 - function as having a head and a tail
111:54 - if the recursive function call is the
111:56 - last line of code in the function as it
111:59 - is in our case
112:01 - we call this tail recursion since it's
112:04 - the last part of the function that calls
112:06 - itself
112:07 - now the trick that swift does to reduce
112:09 - the amount of space and therefore
112:11 - computing overhead to keep track of this
112:14 - recursive calls is called tail call
112:16 - optimization or tail call elimination
112:20 - it's one of those things that you'll see
112:21 - thrown around a loss in algorithm
112:23 - discussions but may not always be
112:25 - relevant to you
112:27 - now what if any of this is relevant to
112:29 - us well python does not implement tail
112:32 - call optimization so the recursive
112:34 - version of binary search takes
112:36 - logarithmic space
112:38 - if we had to choose between the two
112:40 - implementations given that time
112:42 - complexity or run time of both versions
112:45 - the iterative and the recursive version
112:47 - are the same we should definitely go
112:49 - with the iterative implementation in
112:51 - python since it runs in constant space
112:55 - okay that was a lot but all of this with
112:57 - all of this we've now established two
112:59 - important ways to distinguish between
113:02 - algorithms that handle the same task and
113:04 - determine which one we should use
113:07 - we've arrived at what i think is a good
113:09 - spot to take a long break and let all of
113:11 - these new concepts sink in but before
113:14 - you go off to the next course let's take
113:16 - a few minutes to recap everything we've
113:17 - learned so far
113:19 - while we did implement two algorithms in
113:22 - this course in actual code much of what
113:24 - we learned here was conceptual and will
113:26 - serve as building blocks for everything
113:28 - we're going to learn in the future so
113:30 - let's list all of it out
113:32 - the first thing we learned about and
113:33 - arguably the most important was
113:35 - algorithmic thinking
113:37 - algorithmic thinking is an approach to
113:39 - problem solving that involves breaking a
113:41 - problem down into a clearly defined
113:43 - input and output along with a distinct
113:46 - set of steps that solves the problem by
113:48 - going from input to output
113:51 - algorithmic thinking is not something
113:53 - you develop overnight by taking one
113:55 - course so don't worry if you're thinking
113:57 - i still don't truly know how to apply
113:58 - what i learned here
114:00 - algorithmic thinking sinks in after you
114:02 - go through several examples in a similar
114:04 - fashion to what we did today
114:07 - it also helps to apply these concepts in
114:09 - the context of a real example which is
114:12 - another thing we will strive to do
114:13 - moving forward
114:15 - regardless it is important to keep in
114:16 - mind that the main goal here is not to
114:18 - learn how to implement a specific data
114:21 - structure or algorithm off the top of
114:23 - your head i'll be honest i had to look
114:25 - up a couple code snippets for a few of
114:27 - the algorithms myself in writing this
114:29 - course
114:30 - but in going through this you now know
114:32 - that binary search exists and can apply
114:35 - to a problem where you need a faster
114:37 - search algorithm
114:39 - unlike most courses where you can
114:40 - immediately apply what you have learned
114:42 - to build something cool learning about
114:44 - algorithms and data structures will pay
114:46 - off more in the long run
114:48 - the second thing we learned about is how
114:51 - to define and implement algorithms we've
114:53 - gone over these guidelines several times
114:55 - i won't bore you here again at the end
114:58 - but i will remind you that if you're
115:00 - often confused about how to effectively
115:02 - break down a problem in code to
115:04 - something more manageable following
115:06 - those algorithm guidelines is a good
115:08 - place to start
115:09 - next we learned about big o and
115:12 - measuring the time complexity of
115:13 - algorithms this is a mildly complicated
115:16 - topic but once you've abstracted the
115:18 - math away it isn't as hazy a topic as it
115:20 - seems
115:21 - now don't get me wrong the math is
115:23 - pretty important but only for those
115:25 - designing and analyzing algorithms
115:27 - our goal is more about how to understand
115:30 - and evaluate algorithms
115:32 - we learned about common run times like
115:35 - constant linear logarithmic and
115:37 - quadratic runtimes these are all fairly
115:40 - new concepts but in time you will
115:42 - immediately be able to distinguish the
115:44 - runtime of an algorithm based on the
115:46 - code you write and have an understanding
115:48 - of where it sits on an efficiency scale
115:51 - you will also in due time internalize
115:53 - runtimes of popular algorithms like the
115:56 - fact that binary search runs in
115:58 - logarithmic time and constant space
116:00 - and be able to recommend alternative
116:02 - algorithms for a given problem
116:05 - all in all over time the number of tools
116:07 - in your tool belt will increase
116:10 - next we learned about two important
116:12 - search algorithms and the situations in
116:14 - which we select one over the other
116:16 - we also implemented these algorithms in
116:18 - code so that you got a chance to see
116:20 - them work
116:21 - we did this in python but if you are
116:23 - more familiar with a different language
116:25 - and haven't gotten the chance to check
116:27 - out the code snippets we've provided you
116:29 - should try your hand at implementing it
116:31 - yourself it's a really good exercise to
116:34 - go through
116:35 - finally we learned about an important
116:37 - concept and a way of writing algorithmic
116:39 - code through recursion recursion is a
116:42 - tricky thing and depending on the
116:44 - language you write code with you may run
116:46 - into it more than others
116:48 - it is also good to be aware of because
116:50 - as we saw in our implementation of
116:52 - binary search
116:53 - whether recursion was used or not
116:55 - affected the amount of space we used
116:58 - don't worry if you don't fully
117:00 - understand how to write recursive
117:02 - functions i don't truly know either the
117:04 - good part is you can always look these
117:06 - things up and understand how other
117:08 - people do it
117:09 - anytime you encounter recursion in our
117:12 - courses moving forward you'll get a full
117:14 - explanation of how and why the function
117:16 - is doing what it's doing
117:18 - and that brings us to the end of this
117:20 - course i'll stress again that the goal
117:22 - of this course was to get you prepared
117:24 - for learning about more specific
117:26 - algorithms by introducing you to some of
117:29 - the tools and concepts you will need
117:31 - moving forward
117:32 - so if you're sitting there thinking i
117:34 - still don't know how to write many
117:35 - algorithms or how to use algorithmic
117:37 - thinking that's okay we'll get there
117:40 - just stick with it
117:41 - as always have fun and happy coding
117:46 - [Music]
117:53 - hi my name is passant i'm an instructor
117:55 - at treehouse and welcome to the
117:57 - introduction to data structures course
117:59 - in this course we're going to answer one
118:01 - fundamental question why do we need more
118:04 - data structures than a programming
118:05 - language provides
118:07 - before we answer that question some
118:09 - housekeeping if you will
118:11 - in this course we're going to rely on
118:12 - concepts we learned in the introduction
118:14 - to algorithms course
118:16 - namely big-o notation space and time
118:19 - complexity and recursion
118:21 - if you're unfamiliar with those concepts
118:23 - or just need a refresher check out the
118:25 - prerequisites courses listed
118:27 - in addition this course does assume that
118:29 - you have some programming experience
118:32 - we're going to use data structures that
118:34 - come built into nearly all programming
118:36 - languages as our point of reference
118:38 - while we will explain the basics of how
118:40 - these structures work we won't be going
118:42 - over how to use them in practice
118:45 - if you're looking to learn how to
118:46 - program before digging into this content
118:49 - check the notes section of this video
118:50 - for helpful links
118:52 - if you're good to go then awesome let's
118:54 - start with an overview of this course
118:56 - the first thing we're going to do is to
118:58 - explore a data structure we are somewhat
119:00 - already familiar with arrays
119:02 - if you've written code before there's a
119:04 - high chance you have used an array
119:06 - in this course we're going to spend some
119:08 - time understanding how arrays work what
119:10 - are the common operations on an array
119:13 - and what are the run times associated
119:15 - with those operations
119:17 - once we've done that we're going to
119:18 - build a data type of our own called a
119:20 - linked list
119:22 - in doing so we're going to learn that
119:23 - there's more than one way to store data
119:26 - in fact there's way more than just one
119:28 - way
119:29 - we're also going to explore what
119:30 - motivates us to build specific kinds of
119:32 - structures and look at the pros and cons
119:34 - of these structures
119:36 - we'll do that by exploring four common
119:38 - operations accessing a value searching
119:41 - for a value inserting a value and
119:43 - deleting a value
119:45 - after that we're actually going to
119:46 - circle back to algorithms and implement
119:48 - a new one a sorting algorithm
119:51 - in the introductions to algorithms
119:52 - course we implemented a binary search
119:55 - algorithm a precondition to binary
119:57 - search was that the list needed to be
119:59 - sorted
120:00 - we're going to try our hand at sorting a
120:02 - list and open the door to an entirely
120:04 - new category of algorithms
120:07 - we're going to implement our sorting
120:08 - algorithm on two different data
120:10 - structures and explore how the
120:11 - implementation of one algorithm can
120:14 - differ based on the data structure being
120:16 - used
120:17 - we'll also look at how the choice of
120:18 - data structure potentially influences
120:21 - the run time of the algorithm
120:23 - in learning about sorting we're also
120:24 - going to encounter another general
120:27 - concept of algorithmic thinking called
120:29 - divide and conquer
120:30 - along with recursion dividing conquer
120:32 - will be a fundamental tool that we will
120:34 - use to solve complex problems all in due
120:37 - time in the next video let's talk about
120:39 - arrays
120:41 - a common data structure built into
120:43 - nearly every programming language is the
120:45 - array
120:46 - arrays are a fundamental data structure
120:48 - and can be used to represent a
120:49 - collection of values but it is much more
120:52 - than that arrays are also used as
120:54 - building blocks to create even more
120:56 - custom data types and structures
120:58 - in fact in most programming languages
121:00 - text is represented using the string
121:02 - type and under the hood strings are just
121:05 - a bunch of characters stored in a
121:06 - particular order in an array
121:09 - before we go further and dig into arrays
121:12 - what exactly is a data structure
121:15 - a data structure is a way of storing
121:16 - data when programming it's not just a
121:19 - collection of values and the format
121:21 - they're stored in but the relationship
121:23 - between the values in the collection as
121:25 - well as the operations applied on the
121:27 - data stored in the structure
121:29 - an array is one of very many data
121:32 - structures in general an array is a data
121:35 - structure that stores a collection of
121:37 - values where each value is referenced
121:39 - using an index or a key
121:42 - a common analogy for thinking about
121:44 - arrays is as a set of train cars
121:47 - each car has a number and these cars are
121:49 - ordered sequentially
121:51 - inside each car the array or the train
121:54 - in this analogy stores some data
121:57 - while this is the general representation
121:59 - of an array it can differ slightly from
122:01 - one language to another but for the most
122:03 - part all these fundamentals remain the
122:05 - same in a language like swift or java
122:08 - arrays are homogeneous containers which
122:11 - means they can only contain values of
122:13 - the same type
122:14 - if you use an array to store integers in
122:17 - java it can only store integers
122:20 - in other languages arrays are
122:22 - heterogeneous structures that can store
122:24 - any kind of value in python for example
122:27 - you can mix numbers and text with no
122:29 - issues
122:30 - now regardless of this nuance the
122:32 - fundamental concept of an array is the
122:34 - index
122:35 - this index value is used for every
122:38 - operation on the array from accessing
122:40 - values to inserting updating and
122:42 - deleting
122:44 - in python the language we're going to be
122:46 - using for this course it's a tiny bit
122:48 - confusing
122:49 - the type that we generally refer to as
122:51 - an array in most languages is best
122:54 - represented by the list type in python
122:57 - python does have a type called array as
123:00 - well but it's something different so
123:01 - we're not going to use it
123:03 - while python calls it a list when we use
123:06 - a list in this course we'll be talking
123:08 - about concepts that apply to arrays as
123:10 - well in other languages so definitely
123:12 - don't skip any of this there's one more
123:14 - thing
123:15 - in computer science a list is actually a
123:17 - different data structure than an array
123:20 - and in fact we're going to build a list
123:22 - later on in this course
123:24 - generally though this structure is
123:25 - called a linked list as opposed to just
123:28 - list so hopefully the terminology isn't
123:30 - too confusing
123:32 - to properly understand how arrays work
123:35 - let's take a peek at how arrays are
123:37 - stored under the hood
123:39 - an array is a contiguous data structure
123:42 - this means that the array is stored in
123:44 - blocks of memory that are right beside
123:46 - each other with no gaps
123:48 - the advantage of doing this is that
123:50 - retrieving values is very easy
123:52 - in a non-contiguous data structure we're
123:55 - going to build one soon the structure
123:57 - stores a value as well as a reference to
123:59 - where the next value is
124:01 - to retrieve that next value the language
124:03 - has to follow that reference also called
124:05 - a pointer to the next block of memory
124:08 - this adds some overhead which as you
124:10 - will see increases the runtime of common
124:13 - operations a second ago i mentioned that
124:16 - depending on the language arrays can
124:18 - either be homogeneous containing the
124:20 - same type of value or heterogeneous
124:22 - where any kind of value can be mixed
124:25 - this choice also affects the memory
124:27 - layout of the array
124:28 - for example in a language like c swift
124:31 - or java where arrays are homogeneous
124:34 - when an array is created since the kind
124:36 - of value is known to the language
124:38 - compiler and you can think of the
124:40 - compiler as the brains behind the
124:42 - language
124:43 - it can choose a contiguous block of
124:45 - memory that fits the array size and
124:47 - values created
124:49 - if the values were integers assuming an
124:51 - integer took up space represented by one
124:54 - of these blocks then for a five item
124:56 - array the compiler can allocate five
124:59 - blocks of equally sized memory
125:02 - in python however this is not the case
125:04 - we can put any value in a python list
125:07 - there's no restriction
125:08 - the way this works is a combination of
125:11 - contiguous memory and the pointers or
125:13 - references i mentioned earlier
125:16 - when we create a list in python there is
125:18 - no information about what will go into
125:21 - that array which makes it hard to
125:23 - allocate contiguous memory of the same
125:25 - size
125:26 - there are several advantages to having
125:28 - contiguous memory
125:29 - since the values are stored beside each
125:31 - other accessing the values happens in
125:33 - almost constant time so this is a
125:36 - characteristic we want to preserve
125:38 - the way python gets around this is by
125:40 - allocating contiguous memory and storing
125:43 - init not the value we want to store but
125:46 - a reference or a pointer to the value
125:49 - that's stored somewhere else in memory
125:51 - by doing this it can allocate equally
125:54 - sized contiguous memory since regardless
125:56 - of the value size the size of the
125:58 - pointer to that value is always going to
126:00 - be equal this incurs an additional cost
126:03 - in that when a value is accessed we need
126:06 - to follow the pointer to the block of
126:08 - memory where the value is actually
126:09 - stored but python has ways of dealing
126:12 - with these costs that are outside the
126:13 - scope of this course
126:15 - now that we know how an array stores its
126:17 - values let's look at common operations
126:19 - that we execute on an array
126:21 - regardless of the kind of data structure
126:23 - you work with all data structures are
126:25 - expected to carry out four kinds of
126:27 - operations at minimum
126:29 - we need to be able to access and read
126:31 - values stored in the structure we need
126:33 - to be able to search for an arbitrary
126:35 - value
126:36 - we also need to be able to insert a
126:38 - value at any point into the structure
126:40 - and finally we need to be able to delete
126:42 - structures
126:43 - let's look at how these operations are
126:45 - implemented on the array structure in
126:47 - some detail starting with access
126:50 - elements in an array are identified
126:52 - using a value known as an index and we
126:55 - use this index to access and read the
126:57 - value
126:58 - most programming languages follow a
127:00 - zero-based numbering system when it
127:02 - comes to arrays and all this means is
127:04 - that the first index value is equal to
127:07 - zero not one
127:08 - generally speaking when an array is
127:10 - declared a base amount of contiguous
127:13 - memory is allocated as the array storage
127:16 - computers refer to memory through the
127:18 - use of an address but instead of keeping
127:20 - a reference to all the memory allocated
127:22 - for an array the array only has to store
127:25 - the address of the first location
127:27 - because the memory is contiguous using
127:30 - the base address the array can calculate
127:32 - the address of any value by using the
127:34 - index position of that value as an
127:37 - offset
127:38 - if you want to be more specific think of
127:40 - it this way
127:41 - let's say we want to create an array of
127:42 - integers and then each integer takes up
127:45 - a certain amount of space in memory that
127:47 - we'll call m
127:48 - let's also assume that we know how many
127:50 - elements we're going to create so the
127:52 - size of the array is some number of
127:54 - elements we'll call n
127:56 - the total amount of space that we need
127:58 - to allocate is n times the space per
128:00 - item m
128:01 - if the array keeps track of the location
128:04 - in memory where the first value is held
128:06 - so let's label that m0 then it has all
128:09 - the information it needs to find any
128:11 - other element in the list
128:13 - when accessing a value in an array we
128:15 - use the index
128:16 - to get the first element in the list we
128:18 - use the zeroth index to get the second
128:21 - we use the index value 1 and so on
128:24 - given that the array knows how much
128:25 - storage is needed for each element it
128:28 - can get the address of any element by
128:30 - starting off with the address for the
128:32 - first element and adding to that the
128:34 - index value times the amount of storage
128:37 - per element
128:38 - for example to access the second value
128:41 - we can start with m0 and to that add m
128:44 - times the index value 1 giving us m1 as
128:47 - the location in memory for the second
128:49 - address
128:50 - this is a very simplified model but
128:52 - that's more or less how it works
128:54 - this is only possible because we know
128:56 - that array memory is contiguous with no
128:59 - gaps
129:01 - let's switch over to some code
129:03 - as i mentioned earlier we're going to be
129:05 - using python in this course
129:07 - if you don't know how to code or you're
129:09 - interested in this content but know a
129:11 - language other than python check the
129:13 - notes section of this video for more
129:15 - information while the code will be in
129:17 - python the concepts are universal and
129:20 - more importantly simple enough that you
129:22 - should have no issue following along in
129:24 - your favorite programming language
129:26 - and to get started click on the launch
129:28 - workspaces button on the video page that
129:31 - you're watching right now
129:33 - this should spin up an instance of a
129:35 - treehouse workspace an in-browser coding
129:38 - environment right now your workspace
129:40 - should be empty and that's expected so
129:42 - let's add a new file in here i'm going
129:44 - to go to file new file
129:46 - and we'll call this arrays
129:49 - dot py pi
129:51 - creating a list in python is quite
129:54 - simple so we'll call this new underscore
129:56 - list
129:57 - we use a set of square brackets around a
130:00 - set of values to create a list so one
130:02 - and we comma separate them so space two
130:06 - and space three this allocates a base
130:09 - amount of memory for the array to use or
130:12 - when i say array know that in python i
130:14 - mean a list
130:15 - since this is python the values aren't
130:18 - stored in memory
130:19 - instead the values 1 2 and 3 are stored
130:23 - elsewhere in memory and the array stores
130:25 - references to each of those objects
130:28 - to access a value we use a subscript
130:32 - along with an index value so to get the
130:34 - first value we use the index 0 and if we
130:38 - were to assign this to another variable
130:40 - we would say result
130:42 - equal new list
130:44 - we write out new lists since this is the
130:46 - array that we're accessing the value
130:48 - from and then a subscript notation which
130:50 - is a square bracket
130:51 - and then the index value
130:53 - as we saw since the array has a
130:56 - reference to the base location in memory
130:59 - the position of any element can be
131:01 - determined pretty easily
131:03 - we don't have to iterate over the entire
131:05 - list
131:06 - all we need to do is a simple
131:08 - calculation of an offset from the base
131:11 - memory since we're guaranteed that the
131:13 - memory is contiguous
131:15 - for this reason access is a constant
131:18 - time operation on an array or a python
131:21 - list
131:22 - this is also why an array crashes if you
131:24 - try to access a value using an index
131:27 - that is out of bounds of what the array
131:29 - stores
131:30 - if you've used an array before you've
131:32 - undoubtedly run into an error or a crash
131:35 - where you try to access a value using an
131:37 - index that was larger than the number of
131:39 - elements in the array since the array
131:42 - calculates the memory address on the fly
131:45 - when you access a value with an out of
131:47 - bounds index as it's called the memory
131:49 - address returned is not one that's part
131:52 - of the array structure and therefore
131:54 - cannot be read by the array now in
131:56 - python this is represented by an index
131:58 - error and we can make this happen by
132:00 - using an index we know our array won't
132:03 - contain
132:04 - now i'm writing out my code here inside
132:06 - of a text editor which obviously doesn't
132:08 - run the code so let's drag up this
132:10 - console area here
132:12 - and i'm going to write python
132:15 - to bring up the python interpreter
132:18 - and in here we can do the same thing so
132:19 - i can say new
132:21 - list equal one
132:23 - comma two comma three and now this is an
132:26 - interpreter so it's actually going to
132:27 - evaluate our code
132:29 - all right so now we have a new list if i
132:30 - type out new list it gets printed out
132:33 - into the console
132:34 - okay i can also do new list square
132:37 - bracket 0 and you'll see that i get the
132:39 - value 1 which is the value stored at the
132:41 - zeroth index
132:43 - now to highlight that index error we can
132:45 - do new list
132:47 - and inside the square brackets we can
132:48 - provide an index that we know our array
132:51 - doesn't contain so here i'll say index
132:54 - 10
132:55 - and if i hit enter you'll see it say
132:57 - index error list index out of range
133:00 - and those are the basics of how we
133:02 - create and read values from an array in
133:04 - the next video let's take a look at
133:06 - searching
133:07 - in the last video we learned what
133:09 - happens under the hood when we create an
133:11 - array and read a value using an index
133:13 - in this video we're going to look at how
133:15 - the remaining data structure operations
133:17 - work on arrays
133:19 - if you took the introduction to
133:20 - algorithms course we spent time learning
133:22 - about two search algorithms linear
133:25 - search and binary search
133:27 - while arrays are really fast at
133:28 - accessing values they're pretty bad at
133:30 - searching
133:32 - taking an array as is the best we can do
133:34 - is use linear search for a worst case
133:37 - linear runtime linear search works by
133:40 - accessing and reading each value in the
133:42 - list until the element in concern is
133:44 - found
133:45 - if the element we're looking for is at
133:47 - the end of the list then every single
133:49 - element in the list will have been
133:50 - accessed and compared
133:52 - even though accessing and comparing our
133:54 - constant time operations having to do
133:56 - this for every element results in an
133:58 - overall linear time
134:00 - let's look at how search works in code
134:03 - in python we can search for an item in
134:06 - an array in one of two ways
134:08 - we can use the in operator to check
134:11 - whether a list contains an item so i can
134:13 - say if
134:15 - one in new underscore
134:18 - list
134:19 - then print
134:21 - true
134:23 - the in operator actually calls a
134:25 - contains method that is defined on the
134:28 - list type which runs a linear search
134:31 - operation
134:32 - in addition to this we can also use a
134:34 - for loop to iterate over the list
134:36 - manually and perform a comparison
134:39 - operation
134:40 - so i can say
134:42 - for
134:43 - n in new list
134:48 - if n equals one then print
134:52 - true
134:53 - and then after that break out of the
134:55 - loop
134:56 - this is more or less the implementation
134:58 - of linear search
134:59 - if the array were sorted however we
135:01 - could use binary search but because sort
135:04 - operations incur a cost of their own
135:07 - languages usually stay away from sorting
135:09 - the list and running binary search since
135:11 - for smaller arrays linear search on its
135:13 - own may be faster
135:15 - now again remember that
135:17 - since this is an editor this is just a
135:19 - text file none of these lines of code
135:21 - are evaluated so you can try that out in
135:23 - here so we'll copy that we can come down
135:26 - here and say python and hit enter and
135:29 - then when it starts up we can paste in
135:31 - our list
135:32 - and now we can try what we just did so
135:34 - if one in new list
135:38 - print
135:39 - true
135:41 - and there you go it prints true now
135:43 - because we've already learned about
135:44 - linear and binary search in a previous
135:47 - course there's nothing new going on here
135:50 - what's more interesting to look at in my
135:51 - opinion is inserting and deleting values
135:54 - in an array let's start with inserting
135:57 - in general most array implementations
136:00 - support three types of insert operations
136:03 - the first is a true insert using an
136:06 - index value where we can insert an
136:08 - element anywhere in the list this
136:10 - operation has a linear runtime imagine
136:13 - you wanted to insert an item at the
136:15 - start of the list when we insert into
136:18 - the first position what happens to the
136:20 - item that is currently in that spot
136:22 - well it has to move to the next spot at
136:25 - index value one what happens to the
136:27 - second item at index position one
136:29 - that one moves to the next spot at index
136:32 - position two
136:33 - this keeps happening until all elements
136:36 - have been shifted forward one index
136:38 - position
136:39 - so in the worst case scenario inserting
136:41 - at the zeroth position of an array every
136:44 - single item in the array has to be
136:47 - shifted forward and we know that any
136:49 - operation that involves iterating
136:51 - through every single value means a
136:53 - linear runtime
136:55 - now the second way we can insert an item
136:57 - into an array is by appending appending
137:00 - although technically an insert operation
137:03 - in that it inserts an item into an
137:05 - existing array doesn't incur the same
137:07 - runtime cost because appends simply add
137:10 - the item to the end of the list
137:13 - we can simplify and say that this is
137:15 - constant time this is a constant time
137:18 - operation but it depends on the language
137:20 - implementation of array
137:22 - to highlight why that matters let's
137:24 - consider how lists in python work in
137:28 - python when we create a list the list
137:30 - doesn't know anything about the size of
137:32 - the list and how many elements we're
137:34 - going to store
137:35 - creating a new empty list like so so
137:38 - numbers equal and two empty brackets
137:42 - so this creates a list and allocates a
137:45 - space of size n plus one
137:48 - since n here is zero there are no
137:50 - elements in this array in this list
137:53 - space is allocated for a one element
137:56 - list to start off
137:57 - because the space allocated for the list
137:59 - and the space used by the list are not
138:02 - the same
138:03 - what do you think happens when we ask
138:05 - python for the length of this list so i
138:07 - can say len numbers
138:10 - we correctly get 0 back
138:13 - this means that the list doesn't use the
138:15 - memory allocation as an indicator of its
138:17 - size because as i mentioned it has
138:20 - allocated space for a one element list
138:22 - but it returns zero so it determines it
138:25 - in other ways
138:26 - okay so numbers this list currently has
138:29 - space for one element
138:31 - let's use the append method defined on
138:33 - the type to insert a number at the end
138:36 - of the list so you can say numbers dot
138:39 - append and i'll pass in 2.
138:42 - now the memory allocation and the size
138:44 - of the list are the same since the list
138:47 - contains one element
138:49 - now what if i were to do something like
138:50 - this numbers.append
138:53 - there needs to be a dot
138:56 - and i'll add another value 200.
138:59 - now since the list only has an
139:00 - allocation for one item at this point
139:03 - before it can add the new element to the
139:06 - list it needs to increase the memory
139:08 - allocation and thereby the size of the
139:10 - list it does this by calling a list
139:13 - resize operation list resizing is quite
139:16 - interesting because it shows the
139:18 - ingenuity in solving problems like this
139:22 - python doesn't resize the list to
139:24 - accommodate just the element we want to
139:27 - add
139:28 - instead in this case it would allocate
139:30 - four blocks of memory to increase the
139:32 - size to a total of four contiguous
139:35 - blocks of memory
139:36 - it does this so that it doesn't have to
139:38 - resize the list every single time we add
139:40 - an element but at very specific points
139:44 - the growth pattern of the list type in
139:47 - python is 0 4 8 16 25 35 46 and so on
139:54 - this means that as the list size
139:56 - approaches these specific values
139:59 - resize is called again if you look at
140:02 - when the size of the list is four
140:04 - this means that when appending four more
140:06 - values until the size of eight
140:09 - each of those append operations do not
140:11 - increase the amount of space taken
140:14 - at specific points however when resizing
140:17 - is triggered space required increases as
140:20 - memory allocation increases
140:23 - this might signify that the append
140:25 - method has a non-constant space
140:27 - complexity but it turns out that because
140:30 - some operations don't increase space and
140:32 - others do
140:34 - when you average all of them out append
140:36 - operations take constant space
140:39 - we say that it has an amortized constant
140:42 - space complexity this also happens with
140:45 - insert operations
140:46 - if we had a four element array we would
140:48 - have four elements and a memory
140:50 - allocation of four
140:51 - an insert operation at that point
140:54 - doesn't matter where it happens on the
140:55 - list but at that point it would trigger
140:57 - a resize
140:59 - inserting is still more expensive though
141:01 - because after the resize every element
141:03 - needs to be shifted over one
141:06 - the last insert operation that is
141:08 - supported in most languages is the
141:10 - ability to add one list to another
141:13 - in python this is called an extend and
141:15 - looks like this
141:16 - so i'll say numbers now if you let me
141:19 - actually clear out the console
141:21 - oh actually you will let's exit python
141:25 - we'll clear this out so we're back at
141:26 - the top and we'll start again
141:28 - so i'll say numbers
141:31 - and we'll set it to an empty list and
141:33 - now we can say numbers dot extend
141:37 - and as an argument we're going to pass
141:39 - in a new list entirely so here we'll say
141:42 - 4 comma 5 comma 6
141:45 - and then once i hit enter if i were to
141:48 - print out numbers you'll see that it now
141:50 - contains the values 4 5 and 6.
141:52 - so extend takes another list to add
141:55 - extend effectively makes a series of
141:58 - append calls on each of the elements in
142:00 - the new list until all of them have been
142:03 - appended to the original list this
142:05 - operation has a run time of big o of k
142:09 - where k represents the number of
142:11 - elements in the list that we're adding
142:12 - to our existing list
142:14 - the last type of operation we need to
142:16 - consider are delete operations deletes
142:19 - are similar to inserts in that when a
142:22 - delete operation occurs the list needs
142:24 - to maintain correct index values so
142:27 - where an insert shifts every element to
142:29 - the right a delete operation shifts
142:32 - every element to the left
142:34 - just like an insert as well if we delete
142:36 - the first element in the list every
142:38 - single element in the list needs to be
142:40 - shifted to the left
142:42 - delete operations have an upper bound of
142:45 - big o of n also known as a linear
142:47 - runtime now that we've seen how common
142:50 - operations work on a data structure that
142:52 - we're quite familiar with let's switch
142:54 - tracks and build our own data structure
142:58 - [Music]
143:02 - over the next few videos we're going to
143:04 - build a data structure that you may have
143:06 - worked with before a linked list
143:09 - before we get into what a linked list is
143:11 - let's talk about why we build data
143:13 - structures instead of just using the
143:14 - ones that come built into our languages
143:17 - each data structure solves a particular
143:19 - problem
143:20 - we just went over the basics of the
143:22 - array data structure and looked at the
143:24 - cost of common operations that we carry
143:26 - out on arrays
143:27 - we found that arrays were particularly
143:29 - good at accessing reading values happens
143:32 - in constant time but arrays are pretty
143:34 - bad at inserting and deleting both of
143:36 - which run in linear time
143:38 - linked lists on the other hand are
143:40 - somewhat better at this although there
143:42 - are some caveats and if we're trying to
143:44 - solve a problem that involves far more
143:46 - inserts and deletes than accessing a
143:49 - linked list can be a better tool than an
143:51 - array
143:52 - so what is a linked list
143:54 - a linked list is a linear data structure
143:57 - where each element in the list is
143:59 - contained in a separate object called a
144:02 - node a node models two pieces of
144:04 - information an individual item of the
144:06 - data we want to store and a reference to
144:09 - the next node in the list
144:11 - the first node in the linked list is
144:13 - called the head of the list while the
144:15 - last node is called the tail
144:17 - the head and the tail nodes are special
144:20 - the list only maintains a reference to
144:22 - the head although in some
144:23 - implementations it keeps a reference to
144:25 - the tail as well
144:27 - this aspect of linked lists is very
144:29 - important and as you'll see most of the
144:31 - operations on the list need to be
144:33 - implemented quite differently compared
144:34 - to an array
144:36 - the opposite of the head the tail
144:38 - denotes the end of the list
144:40 - every node other than the tail points to
144:42 - the next node in the list but tail
144:45 - doesn't point to anything this is
144:47 - basically how we know it's the end of
144:49 - the list nodes are what are called
144:51 - self-referential objects the definition
144:54 - of a node includes a link to another
144:57 - node and self-referential here means the
144:59 - definition of node includes the node
145:01 - itself linked lists often come in two
145:04 - forms a singly linked list where each
145:06 - node stores a reference to the next node
145:08 - in the list or a doubly linked list
145:11 - where each node stores a reference to
145:13 - both the node before and after if an
145:16 - array is a train with a bunch of cars in
145:19 - order then a linked list is like a
145:21 - treasure hunt
145:22 - when you start the hunt you have a piece
145:24 - of paper with the location of the first
145:26 - treasure you go to that location and you
145:29 - find an item along with a location to
145:31 - the next item of treasure
145:33 - when you finally find an item that
145:35 - doesn't also include a location you know
145:37 - that the hunt has ended
145:39 - now that we have a high level view of
145:40 - what a linked list is let's jump into
145:42 - code and build one together we'll focus
145:45 - on building a singly linked list for
145:46 - this course there are advantages to
145:49 - having a doubly linked list but we don't
145:51 - want to get ahead of ourselves
145:54 - let's start here by creating a new file
145:59 - we're going to put all our code for our
146:00 - linked list so we'll call this linked
146:03 - underscore list
146:04 - dot pi and first we're going to create a
146:08 - class to represent a node
146:12 - say class
146:13 - node
146:15 - now node is a simple object in that it
146:18 - won't model much so first we'll add a
146:21 - data variable
146:23 - it's an instance variable here called
146:25 - data and we'll assign the value none
146:27 - initially
146:28 - and then we'll add one more we'll call
146:30 - this next node and to this we'll assign
146:33 - none as well so we've created two
146:35 - instance variables data to hold on to
146:38 - the data that we're storing and next
146:41 - node to point to the next node in the
146:43 - list
146:44 - now we need to add a constructor to make
146:46 - this class easy to create so we'll add
146:49 - an init
146:50 - method here that takes self and some
146:53 - data to start off
146:55 - and all we're going to do is assign
146:57 - data to that instance variable we
146:59 - created so that's all we need to model
147:02 - node
147:03 - before we do anything else though let's
147:05 - document this so right after the class
147:07 - definition let's create a docs string so
147:10 - three quotes
147:12 - next line and we'll say an object
147:15 - for storing
147:16 - a single
147:18 - node of a linked list
147:22 - and then on the next line we'll say
147:23 - models two attributes
147:28 - data
147:29 - and
147:30 - the link to the next
147:34 - node in the list
147:36 - and then we'll close this doc string off
147:39 - with three more quotation marks okay
147:41 - using the node class is fairly
147:43 - straightforward so we can create a new
147:46 - instance of node with some data to store
147:48 - now the way we're going to do this is
147:49 - we're going to bring up the console
147:51 - and we're going to type out like we've
147:52 - been typing out before python followed
147:55 - by the name of the script that we wrote
147:57 - which is linked list linked underscore
147:59 - list.pi but before we do that we're
148:01 - going to pass an argument to the python
148:03 - command we're going to say dash or
148:05 - python i and then the name of the script
148:09 - linked underscore list dot pi so what
148:11 - this does is this is going to run the
148:14 - python repl
148:16 - the read evaluate print loop in the
148:18 - console but it's going to load the
148:19 - contents of our file into that so that
148:22 - we can use it
148:23 - so i'll hit enter and we have a new
148:25 - instance going and now we can use the
148:27 - node in here so we can say n1
148:29 - equal node
148:31 - and since we defined that constructor we
148:33 - can pass it some data so we'll say 10
148:35 - here
148:36 - now if we try to inspect this object the
148:39 - representation returned isn't very
148:41 - useful
148:42 - which will make things really hard to
148:44 - debug as our code grows so for example
148:46 - if i type out n1 you'll see that
148:49 - we have a valid instance here but it's
148:50 - not very helpful the way it's printed
148:52 - out
148:53 - so we can customize this by adding a
148:56 - representation of the object using the
148:58 - wrapper function now in the terminal
149:00 - still we'll type out exit
149:02 - like that hit enter to exit the console
149:06 - and then down here
149:08 - let's add in some room
149:12 - okay and here we'll say def
149:14 - double underscore
149:16 - wrapper another set of double
149:17 - underscores
149:18 - and then this function takes the
149:20 - argument self
149:21 - and in here we can provide a string
149:23 - representation of what we want printed
149:26 - to the console when we inspect that
149:29 - object inside of it inside of a console
149:32 - so here we'll say return
149:34 - again
149:35 - this is a string representation so
149:36 - inside quotes we'll say
149:38 - node so this represents a node instance
149:41 - and the data it contains here we'll say
149:44 - percent s
149:46 - which is a python way of substituting
149:48 - something into a string string
149:51 - interpolation and outside of the string
149:53 - we can say percent again
149:55 - and here we're saying we want to replace
149:57 - this percent s with
150:00 - self.data okay
150:02 - let's hit save and before we move on
150:04 - let's verify that this works so i'm
150:06 - going to come in here
150:08 - type clear to get rid of everything
150:11 - and then we'll do what we did again and
150:13 - you can just hit the up arrow a couple
150:15 - times to get that command
150:17 - all right so hit enter and now just so
150:19 - you know every time you run this you
150:21 - start off you know from scratch so n1
150:23 - that we created earlier not there
150:24 - anymore so let's go ahead and create it
150:26 - n1 equal node
150:29 - 10
150:30 - and we can type n1 again and hit enter
150:32 - and you have a much better
150:33 - representation now so we can see that we
150:35 - have a node and it contains the data 10.
150:38 - we can also create another one n2 equal
150:40 - node that contains the data 20 and now
150:43 - we can say n1.next n1.nextnode
150:46 - equal n2 so n1 now points to n2 and if
150:51 - we say n1.nextnode
150:53 - you'll see that it points to that node
150:55 - the node containing 20.
150:58 - nodes are the building blocks for a list
151:00 - and now that we have a node object we
151:02 - can use it to create a singly linked
151:05 - list so again i'm going to exit out of
151:06 - this
151:09 - and then go back to the text editor
151:12 - and here we'll create a new class so
151:14 - class
151:15 - linked
151:16 - list
151:17 - the linked list class is going to define
151:20 - a head and this attribute models the
151:23 - only node that the list is going to have
151:25 - a reference to so here we'll say head
151:28 - and we'll assign none initially and then
151:30 - like we did earlier let's create a
151:32 - constructor
151:33 - so double underscore init double
151:35 - underscore this takes self
151:39 - and then inside like before we'll say
151:41 - self dot head
151:43 - equal none this is the same
151:46 - as doing this so we can actually get rid
151:48 - of that and just use the constructor
151:51 - okay so again this head attribute models
151:54 - the only node that the list will have a
151:56 - reference to since every node points to
151:59 - the next node to find a particular node
152:02 - we can go from one node to the next in a
152:04 - process called list traversal
152:07 - so in the class constructor here we've
152:08 - set the default value of head to none so
152:11 - that new lists created are always empty
152:14 - again you'll notice here that i didn't
152:16 - explicitly declare the head attribute at
152:18 - the top of the class definition
152:20 - and don't worry that's not an oversight
152:22 - the self.head in the initializer means
152:25 - that it's still created okay so that's
152:27 - all there is to modeling a linked list
152:30 - now we can add methods that make it
152:32 - easier to use this data structure
152:34 - first a really simple docstring to
152:36 - provide some information
152:38 - so here we'll to create a docstring
152:40 - three quotation marks
152:41 - and then we'll say singly linked list
152:44 - and then close it off
152:46 - a common operation carried out on data
152:49 - structures is checking whether it
152:51 - contains any data or whether it's empty
152:54 - at the moment to check if a list is
152:56 - empty we would need to query these
152:58 - instance variables head and so on every
153:01 - time
153:02 - ideally we would like to not expose the
153:04 - inner workings of our data structure to
153:06 - code that uses it
153:08 - instead let's make this operation more
153:10 - explicit by defining a method so we'll
153:13 - say def is empty
153:16 - and this method takes self as an
153:18 - argument and here we'll say return
153:20 - self.head double equal none
153:24 - all we're doing here is checking to see
153:26 - if head is none
153:27 - if it is this condition evaluates to
153:29 - true which indicates the list is empty
153:32 - now before we end this video let's add
153:34 - one more convenience method to calculate
153:37 - the size of our list the name
153:39 - convenience method indicates that what
153:41 - this method is doing is not providing
153:43 - any additional functionality that our
153:45 - data structure can't handle right now
153:48 - but instead making existing
153:49 - functionality easier to use
153:52 - we could calculate the size of our
153:53 - linked list by traversing it every time
153:56 - using a loop until we hit a tail node
153:59 - but doing that every time is a hassle
154:01 - okay so we'll call this method
154:04 - size and as always it takes self
154:07 - unlike calling len on a python list not
154:10 - to be confused with a linked list which
154:12 - is a constant time operation our size
154:16 - operation is going to run in linear time
154:18 - the only way we can count how many items
154:20 - we have is to visit each node and call
154:23 - next until we hit the tail node
154:26 - so we'll start by getting a reference to
154:28 - the head we'll say current
154:30 - equal self.head let's also define a
154:33 - local variable named count with an
154:36 - initial value of 0 that will increment
154:39 - every time we visit a node once we hit
154:42 - the tail count will reflect the size of
154:44 - that list
154:46 - next we'll define a while loop that will
154:48 - keep going until there are no more nodes
154:51 - so say while current
154:53 - while current is the same as writing out
154:56 - while current does not equal none but
154:59 - it's more succinct so we'll go with this
155:01 - former
155:02 - if the ladder is more precise for you
155:03 - you can go with that
155:05 - now inside this loop we'll increment the
155:07 - count value so count plus equal one
155:10 - plus equal if you haven't encountered it
155:12 - before is the same as writing count
155:13 - equal count plus one so if count is zero
155:16 - initially so it's zero plus one is one
155:19 - and then we'll assign that back to count
155:21 - okay so count plus equal one
155:24 - next we're going to assign the next node
155:26 - in the list to current so current equal
155:30 - current dot next
155:33 - node
155:34 - this way once we get to the tail and
155:36 - call next node current will equal none
155:39 - and the while loop terminates so the end
155:41 - we can return
155:43 - count
155:44 - as you can see we need to visit every
155:46 - node to determine the size meaning our
155:49 - algorithm runs in linear time so let's
155:51 - document this
155:52 - up in our docs string which we'll add
155:54 - now to size
155:56 - we'll say
155:57 - returns
155:58 - the number of nodes in the list
156:02 - takes
156:04 - linear time
156:06 - let's take a break here we can now
156:08 - create lists check if they're empty and
156:10 - check the size
156:12 - in the next video let's start
156:13 - implementing some common operations
156:16 - at the moment we can create an empty
156:18 - list but nothing else let's define a
156:20 - method to add data to our list
156:23 - technically speaking there are three
156:24 - ways we can add data to a list
156:26 - we can add nodes at the head of the list
156:28 - which means that the most recent node we
156:30 - created will be the head and the first
156:32 - node we created will be the tail
156:34 - or we could flip that around most recent
156:36 - nodes are the tail of the list and the
156:38 - first node to be added is the head i
156:41 - mentioned that one of the advantages of
156:42 - linked lists over arrays is that
156:44 - inserting data into the list is much
156:46 - more efficient than to the array
156:48 - this is only true if we're inserting at
156:50 - the head or the tail
156:52 - technically speaking this isn't an
156:54 - insert and you'll often see this method
156:56 - called add prepend if the data is added
156:59 - to the head or append if it's added to
157:01 - the tail
157:02 - a true insert is where you can insert
157:04 - the data at any point in the list which
157:07 - is our third way of adding data we're
157:09 - going to circle back on that if we
157:11 - wanted to insert at the tail then the
157:13 - list needs a reference to the tail node
157:15 - otherwise we would have to start at the
157:17 - head and walk down the length of the
157:19 - list or traverse it to find the tail
157:22 - since our list only keeps a reference to
157:24 - the head we're going to add new items at
157:26 - the head of the list
157:29 - now before we add our new method i
157:32 - forgot that i didn't show you in the
157:33 - last video how to actually use the code
157:36 - we just added and how to check every
157:38 - time you know when we add new code that
157:39 - it works correctly
157:41 - so like before we're gonna bring up the
157:43 - console and here we're gonna say python
157:45 - dash i
157:47 - linked underscore list dot pi which
157:50 - should load it
157:51 - load the contents of our file
157:53 - and now we'll start here by creating a
157:55 - linked list so l equal linked list
157:59 - and then we'll use a node so n1 equal
158:03 - node
158:04 - with the value 10
158:06 - and now we can assign n1 to the nodes or
158:09 - to the linked lists head attribute so l1
158:12 - dot head equal n1
158:14 - and then
158:15 - we can see if size works correctly so if
158:18 - we call l1 dot size and since this is a
158:21 - method we need a set of parentheses at
158:22 - the end
158:23 - and enter you'll see that we get back
158:25 - one correctly okay so it works
158:29 - now let's add our new method which we're
158:31 - going to call add
158:33 - add is going to accept some data to add
158:36 - to the list inside of a node
158:40 - so we'll say def
158:41 - add
158:42 - and every python method takes self as an
158:45 - argument and then we want to add some
158:47 - data to this node so we're going to say
158:49 - data for the second argument
158:51 - inside the method first we'll create a
158:53 - new node to hold on to the data so new
158:56 - underscore node equal
158:59 - node with the data
159:00 - before we set the new node as the head
159:02 - of the list we need to point the new
159:05 - node's next property at whatever node is
159:08 - currently at head this way when we set
159:10 - the new node as the head of the list we
159:12 - don't lose a reference to the old head
159:15 - so new underscore node dot next node
159:20 - equal self.head
159:22 - now if there was no node at head this
159:25 - correctly sets next node to none
159:29 - now we can set the new node as the head
159:31 - of the node so say self.head equal
159:35 - new underscore node because the insert
159:39 - operation is simply a reassignment of
159:41 - the head and next node properties this
159:44 - is a constant time operation so let's
159:46 - add that in as a docs string
159:50 - first what the method does so it adds a
159:52 - new node
159:55 - containing data
159:57 - at the head of the list
160:02 - this operation takes
160:04 - constant time which is our best case
160:07 - scenario
160:08 - okay let's test this out so i'm going to
160:10 - bring the console back up we'll exit out
160:13 - of
160:14 - our current reply
160:16 - and we'll load
160:18 - the contents of the file again
160:20 - and now we don't need to create a node
160:22 - like we did earlier so we can say l
160:25 - equal linked
160:26 - list
160:28 - l.add one
160:30 - okay let's see if this works we'll call
160:32 - size
160:33 - and if it worked
160:34 - the linked list should now have a size
160:37 - of one there we go you can also do
160:39 - l.add2
160:40 - l.add three
160:44 - and l dot size should now be three there
160:47 - we go now if we i were to type l and
160:49 - just hit print
160:51 - again what we get in the repel is
160:53 - nothing useful
160:54 - so like before we'll implement the
160:57 - wrapper function for our linked list
161:00 - now
161:01 - i'm just going to copy paste this in and
161:03 - we'll walk through it
161:05 - okay so this is what our implementation
161:08 - of wrapper looks like for the linked
161:09 - list object you can grab this code from
161:12 - the notes section of this video
161:15 - okay so at the top you'll see a docs
161:16 - string where it says it returns a string
161:18 - representation of the list and like
161:20 - everything we need to do with a linked
161:22 - list we need to traverse it so this is
161:24 - going to take linear time we start by
161:27 - creating an empty list now i need to
161:29 - distinguish this is a python list not a
161:31 - linked list so we create an empty list
161:34 - called nodes and two nodes we're going
161:36 - to add strings that have a description
161:38 - that provide a description of each node
161:41 - but we're not going to use the
161:42 - description that we implemented in the
161:44 - node class because we're going to
161:46 - customize it a bit here
161:48 - next we start by assigning self.head to
161:51 - current so we sort of have a pointer to
161:53 - the head node as long as current does
161:56 - not equal none which means we're not at
161:58 - the tail we're going to implement some
162:00 - logic
162:00 - so in the first scenario if the node
162:03 - assigned to current is the same as the
162:05 - head
162:06 - then we're going to append this string
162:08 - to our nodes list
162:11 - and the string is simply
162:13 - going to say that hey this is a head
162:14 - node and it contains some data which
162:17 - will extract using current.data
162:19 - next scenario is if the node assigned to
162:23 - current's next node is none meaning
162:26 - we're at the tail node then we'll assign
162:28 - a different kind of string so it's the
162:29 - same as earlier except we're saying tail
162:31 - here and then finally in any other
162:33 - scenario which means we're not at the
162:35 - head or not of the tail we'll simply
162:37 - print the node's value inside and again
162:40 - we'll extract it using current.data with
162:42 - every iteration of the loop we'll move
162:44 - current forward by calling
162:46 - current.nextnode and reassigning it
162:48 - and then at the very end when we're done
162:50 - we'll join all the strings that are
162:52 - inside the nodes list together using the
162:55 - python join method and we'll say that
162:59 - with every join so when you join these
163:01 - two strings together to make one string
163:03 - you need to put this set of characters
163:05 - in between all right so let's see what
163:07 - this looks like
163:08 - so i'm going to come down here exit out
163:10 - of the console again
163:11 - clear it out
163:13 - load the contents of the file again and
163:16 - let's try that so we'll say l equal
163:18 - linked list
163:20 - all right so l dot add one l dot add two
163:25 - l dot add three that seems enough
163:28 - and then now if i type out l and hit
163:29 - enter we get a nice string
163:32 - representation of the list so you can
163:34 - see that we add every new node to the
163:36 - head so we added one first one ends up
163:39 - being the tail because it keeps getting
163:40 - pushed out
163:42 - then two and then finally three so three
163:44 - is at the head
163:46 - so far we've only implemented a single
163:48 - method which functions much like the
163:50 - append method on a python list or an
163:53 - array
163:54 - except it adds it to the start of the
163:57 - linked list it pre-pens it
163:59 - like append this happens in constant
164:02 - time in the next video let's add the
164:04 - ability to search through our list for
164:07 - the search operation we're going to
164:08 - define a method that takes a value to
164:10 - search for and returns either the node
164:13 - containing the value if the value is
164:15 - found or none if it isn't
164:17 - so right after actually you know what
164:19 - we'll make sure wrapper is the last
164:21 - function our last method
164:24 - in our class so we'll add it above it so
164:26 - here we'll say def search
164:28 - self
164:30 - and then key
164:32 - in the last video we implemented the
164:34 - wrapper method to provide a string
164:36 - representation of the list
164:38 - so we're going to use similar logic here
164:40 - to implement the search function we'll
164:43 - start by setting a local variable
164:45 - current to point to the head of the list
164:49 - while the value assigned to current is a
164:51 - valid node that is it isn't none
164:54 - we'll check if the data on that node
164:57 - matches the key that we're searching for
164:59 - so while current
165:02 - we'll say if
165:03 - current.data
165:05 - is the key
165:07 - then we'll return current
165:09 - if it does match we'll go ahead and
165:11 - return it like we've done here but if it
165:14 - doesn't we'll assign the next node in
165:16 - the list to current and check again so
165:19 - say else
165:21 - current equal current dot next node
165:26 - once we hit the tail node and haven't
165:29 - found the key
165:30 - current gets set to none and the while
165:32 - loop exits
165:34 - at this point we know the list doesn't
165:36 - contain the key so we can return
165:39 - none
165:40 - okay that completes the body of our
165:42 - method
165:43 - let's add a docs string to document this
165:46 - so up at the top we'll say search
165:49 - for the first node
165:52 - containing data that matches
165:57 - the key
165:58 - now this is important because if our
166:00 - linked list contains more than one node
166:02 - with the same value it doesn't matter
166:04 - we're going to return the first one with
166:06 - this implementation
166:08 - we'll also say here that it returns the
166:10 - node or none
166:13 - if not found
166:16 - in the worst case scenario we'll need to
166:18 - check every single node in the list
166:20 - before we find the key or fail and as a
166:23 - result this operation runs in linear
166:25 - time so i'll say takes
166:28 - o of n or linear time
166:33 - so far we haven't seen anything that
166:35 - indicates this data structure has any
166:37 - advantage over an array or a python list
166:40 - but we knew that
166:41 - i mentioned the strength of linked lists
166:44 - comes in inserts and deletes at specific
166:47 - positions we'll check that out in the
166:49 - next video but as always before we end
166:52 - this one let's make sure everything
166:54 - works
166:55 - so we'll load the contents of the file
166:57 - again
167:01 - l equal linked
167:02 - list
167:05 - and then we'll say l.add 10
167:08 - l dot add
167:09 - 20 2 doesn't matter l dot add
167:13 - 45 and one more metal dot add
167:16 - 15.
167:17 - now we can say
167:18 - l.search and we need to give it a value
167:20 - so we'll say 45 and this returns a node
167:24 - or none so we'll say n equal
167:27 - and then we'll hit enter if this works
167:30 - n should be a node
167:32 - okay weirdly n
167:35 - does not work here
167:36 - at least it says it's not a node which
167:38 - means i made a mistake in typing out our
167:40 - code
167:41 - and looking at it immediately it's
167:42 - fairly obvious so this return none needs
167:44 - to be outside of the while loop okay so
167:48 - i'm going to hit save now so make sure
167:49 - it's on the same indentation here which
167:52 - means it's outside the while loop
167:54 - and then we'll run through this again
167:58 - okay so l is linked list
168:02 - l.add 10
168:04 - l dot add 2
168:06 - l.add
168:08 - 45 and what was the last one we did i
168:10 - believe it was 15
168:12 - and now we should be able to say
168:14 - l.search remember we're assigning this
168:17 - to a node to a variable so l.search
168:21 - 45
168:24 - and there you go we get that node back
168:26 - and we can hit l
168:27 - and we'll see a representation of our
168:29 - list
168:30 - okay so again in the next video inserts
168:32 - and deletes at specific positions
168:35 - insert operations on linked lists are
168:37 - quite interesting
168:39 - unlike arrays where when you insert an
168:41 - element into the array all elements
168:43 - after the particular index need to be
168:45 - shifted with a linked list we just need
168:48 - to change the references to next on a
168:50 - few nodes and we're good to go
168:52 - since each node points to the next one
168:54 - by swapping out these references we can
168:56 - insert a node at any point in the list
168:59 - in constant time
169:00 - much like binary search though there's a
169:02 - catch
169:03 - to find the node at that position we
169:05 - want to insert we need to traverse the
169:08 - list and get to that point
169:10 - we just implemented our search algorithm
169:12 - for the linked list type and we know
169:14 - that this runs in linear time so while
169:17 - actually inserting is fast finding the
169:19 - position in the list you want to insert
169:21 - it is not
169:23 - this is why i mentioned that there were
169:24 - some caveats to inserting
169:26 - anyway let's see what this looks like in
169:28 - code
169:29 - we'll define a method named insert that
169:32 - takes data to insert along with an index
169:34 - position so we'll do this after search
169:37 - right here
169:39 - say def
169:40 - insert
169:43 - and this takes some data to insert and a
169:46 - position to insert it at
169:50 - you may be thinking wait a minute linked
169:52 - lists don't have index positions right
169:54 - and you're correct but we can mimic that
169:56 - behavior by just counting the number of
169:59 - times we access next node
170:01 - if the index value passed into this
170:03 - argument is 0 that means we want to
170:06 - insert the new node at the head of the
170:08 - list this is effectively the same
170:11 - behavior as calling add which means the
170:13 - logic is the same so we don't need to
170:15 - repeat it we can call the add method we
170:18 - wrote earlier so we'll say if
170:20 - index if index equals 0 or if index is 0
170:25 - then self dot add
170:28 - data
170:29 - if the index is greater than 0 then we
170:32 - need to traverse the list to find the
170:35 - current node at that index
170:37 - so if index is greater than zero
170:40 - now before we do that we need to create
170:43 - a new node containing the data we want
170:45 - to insert so we'll say new equal node
170:48 - with some data
170:50 - i'm going to assign index the argument
170:52 - passed to our function to a local
170:55 - variable named position and the head of
170:57 - the list to a variable named current
171:00 - position
171:01 - equal index
171:04 - current equal self.head
171:07 - every time we call
171:09 - current.nextnode meaning we're moving to
171:11 - the next node in the list we'll decrease
171:14 - the value of position by 1.
171:16 - when position is zero we'll have arrived
171:19 - at the node that's currently at the
171:21 - position we want to insert in
171:23 - in reality though we don't want to
171:24 - decrease it all the way to zero
171:27 - imagine we have a list with five nodes
171:29 - and we want to insert a node at position
171:32 - 3. to insert a node at position 3 we
171:35 - need to modify the nodes at positions 2
171:37 - and 3.
171:39 - node 2's next node attribute is going to
171:41 - point to the new node and the new node's
171:44 - next node attribute will point to node
171:46 - 3.
171:47 - in this way an insert is a constant time
171:49 - operation we don't need to shift every
171:52 - single element we just modify a few next
171:54 - node references
171:56 - in a doubly linked list we can use node
171:59 - 3 to carry out both of these operations
172:02 - node 3 in a doubly linked list would
172:04 - have a reference to node 2 and we can
172:07 - use this reference to modify all the
172:09 - unnecessary links
172:10 - and a singly linked list though which is
172:12 - what we have if we kept decreasing
172:15 - position until we're at 0 we arrive at
172:18 - node 3.
172:19 - we can then set the new node's next node
172:22 - property to point to node 3 but we have
172:24 - no way of getting a reference to node 2
172:27 - which we also need
172:29 - for this reason it's easier to decrease
172:31 - position to just 1 when it equals 1 and
172:34 - stop at node 2. so in here we'll say
172:39 - while
172:40 - position
172:41 - is greater than one
172:43 - now while the position is greater than
172:45 - one we'll keep calling next node and
172:47 - reassigning the current node so current
172:51 - equal node.next
172:53 - node and at the same time we'll
172:55 - decrement position so position
172:58 - equal to position
173:01 - minus one which you can also succinctly
173:04 - write as minus equal
173:06 - one
173:07 - this way when the position equals one
173:10 - the loop exits and current will refer to
173:13 - the node at the position before the
173:15 - insert point so outside the while loop
173:18 - we'll say previous equal current
173:22 - and next equal current dot next
173:25 - node
173:26 - to make things more clear what i've done
173:29 - here is name the node before the new one
173:31 - previous and the node after the new one
173:34 - next
173:35 - all that's left to do now is to insert
173:37 - the new node between previous and next
173:39 - so we'll say previous dot next
173:42 - node
173:44 - equal
173:45 - new
173:46 - and then new dot next node
173:49 - equal next
173:52 - now it seems like there's an issue with
173:53 - variable naming here and i'm most
173:56 - probably conflicting with some globally
173:58 - named next variable so actually go ahead
174:00 - and call this next node
174:02 - and
174:04 - previous node so that we don't mess
174:06 - things up here
174:08 - previous node
174:12 - so the dot next node is obviously the
174:14 - attribute on a node but this is just a
174:16 - local variable let's document this
174:18 - method so up at the top
174:21 - we'll add a docs string and it will say
174:23 - inserts a new node
174:26 - containing
174:27 - data at index
174:30 - position
174:33 - insertion takes
174:35 - constant time
174:39 - but finding the node
174:43 - at the insertion point
174:46 - takes linear
174:50 - time
174:52 - let's add this to the next line
174:54 - there we go
174:55 - and then we'll say therefore it takes an
174:58 - overall
175:00 - linear time
175:03 - this is why even though we can easily
175:05 - insert a new node without having to
175:08 - shift the rest ultimately adding to
175:10 - either the head or the tail if you have
175:12 - a reference is much more efficient
175:15 - we have one more operation to add to our
175:17 - linked list that will make it a robust
175:19 - data structure
175:20 - much like inserts removing a node is
175:22 - actually quite fast and occurs in
175:24 - constant time but to actually get to the
175:26 - node that we want to remove and modify
175:29 - the next connections we need to traverse
175:31 - the entire list in our worst case so in
175:33 - the worst case this takes linear time
175:36 - let's add this operation to our data
175:37 - structure
175:39 - there are two ways we can define the
175:41 - remove method one where we provide a key
175:44 - to remove as an argument and one where
175:46 - we provide an index now in the former
175:49 - the key refers to the data the node
175:52 - stores so in order to remove that node
175:54 - we would first need to search for data
175:56 - that matches the key i'm going to
175:58 - implement that first method which we'll
176:00 - call remove and i'll leave it up to you
176:02 - to get some practice in and implement a
176:05 - remove at index method to complete our
176:07 - data structure so we'll add this after
176:09 - the insert method right here
176:14 - remove
176:15 - is going to accept a key which we'll
176:17 - need to search for before we can remove
176:20 - a node earlier we defined a search
176:22 - method that found a node containing data
176:24 - that matches a key but we can't use that
176:27 - method as is for the implementation of
176:29 - remove when we remove a node much like
176:32 - the insert operation we need to modify
176:35 - the next node references
176:37 - the node before the match needs to point
176:39 - to the node after the match
176:41 - if we use the search method we defined
176:43 - earlier we get the node we want to
176:45 - remove as a return value but because
176:48 - this is a singly linked list we can't
176:51 - obtain a reference to the previous node
176:53 - like i said earlier if this was a doubly
176:55 - linked list we could use the search
176:57 - method since we would have a reference
176:59 - to that previous node
177:01 - we'll start here by setting a local
177:03 - variable named current to point to the
177:06 - head let's also define a variable named
177:09 - previous
177:10 - that will set to none to keep track of
177:13 - the previous node as we traverse the
177:15 - list
177:16 - finally let's declare a variable named
177:18 - found that we'll set to false
177:21 - found is going to serve as a stopping
177:23 - condition for the loop that we'll define
177:26 - we'll use the loop to keep traversing
177:28 - the linked list as long as found is
177:31 - false meaning we haven't found the key
177:33 - that we're looking for once we've found
177:35 - it we'll set found to true and the loop
177:37 - terminates so let's set up our loop so
177:39 - we'll say while current
177:41 - and
177:42 - not
177:43 - found
177:46 - here we're defining a while loop that
177:48 - contains two conditions
177:50 - first we tell the loop to keep iterating
177:53 - as long as current does not equal none
177:57 - when current equals none this means
177:59 - we've gone past the tail node and the
178:01 - key doesn't exist
178:03 - the second condition asks the loop to
178:05 - keep evaluating as long as not found
178:08 - equals true now this might be tricky
178:11 - because it involves a negation here
178:13 - right now found is set to false so not
178:16 - found not false equals true this not
178:20 - operator flips the value
178:22 - when we find the key and we set found to
178:25 - true
178:26 - not true not found we'll equal false
178:29 - then and the loop will stop
178:31 - the end in the while loop means that
178:34 - both conditions current being a valid
178:36 - node and not found equalling true both
178:39 - have to be true
178:40 - if either one of them evaluates to false
178:42 - then the loop will terminate
178:44 - now inside the loop there are three
178:45 - situations that we can run into
178:48 - first the key matches the current node's
178:50 - data and current is still at the head of
178:53 - the list
178:54 - this is a special case because the head
178:56 - doesn't have a previous node and it's
178:58 - the only node being referenced by the
179:00 - list let's handle this case so we'll say
179:03 - if current.data
179:05 - double equals the key and current
179:09 - is self.head which you can write out as
179:12 - current equal self.head or current is
179:14 - self.head
179:15 - now if we hit this case
179:18 - we'll indicate that we found the key by
179:20 - setting found to true
179:23 - and then this means that on the next
179:24 - pass
179:25 - this is going to evaluate to false
179:27 - because not true will be false
179:31 - and then the loop terminates once we do
179:33 - that we want to remove the current node
179:35 - and since it's the head node all we need
179:38 - to do is point head to the second node
179:41 - in the list which we can get by
179:43 - referencing the next node attribute on
179:45 - current self.head equal current.nextnode
179:50 - so when we do this there's nothing
179:52 - pointing to that first node so it's
179:54 - automatically removed the next scenario
179:57 - is when the key matches data in the node
179:59 - and it's a node that's not the head
180:02 - so here we'll say else if current dot
180:05 - data equal key
180:08 - if the current node contains the key
180:10 - we're looking for we need to remove it
180:13 - to remove the current node we need to go
180:14 - to the previous node and modify its next
180:17 - node reference to point to the node
180:19 - after current
180:21 - but first we'll set found
180:23 - to true
180:24 - and then we'll switch out the references
180:26 - so
180:27 - previous.nextnode
180:29 - equal
180:30 - current.nextnode
180:32 - so far we haven't written any code to
180:34 - keep track of the previous node
180:37 - we'll do that in our else case here
180:40 - so if we hit the else case it means that
180:42 - the current node we're evaluating
180:44 - doesn't contain the data that matches
180:46 - the key so in this case we'll make
180:48 - previous point to the current node and
180:50 - then set current to the next node so
180:52 - previous equal current
180:55 - and current equal current.nextnode
180:59 - and that's it for the implementation of
181:01 - remove
181:02 - now we're not doing anything at the
181:04 - moment with the node we're removing but
181:06 - it's common for remove operations to
181:08 - return the value being removed so at the
181:10 - bottom
181:12 - outside the while loop
181:13 - let's return
181:16 - current
181:17 - and with that we have a minimal
181:18 - implementation of a linked list and your
181:21 - first custom data structure how cool is
181:24 - that
181:24 - there's quite a bit we can do here to
181:26 - improve our data structure particularly
181:29 - in making it easy to use but this is a
181:31 - good place to stop
181:33 - before we move on to the next topic
181:35 - let's document our method so the top
181:38 - another docs string
181:39 - and here we'll say removes node
181:42 - containing data that matches the key
181:46 - also it returns the node or none
181:50 - if the key doesn't exist
181:53 - and finally this takes
181:56 - linear time because in the worst case
181:57 - scenario we need to search the entire
181:59 - list
182:01 - if you'd like to get in some additional
182:02 - practice implementing functionality for
182:04 - linked lists two methods you can work on
182:07 - are remove it index and node at index to
182:11 - allow you to easily delete or read
182:14 - values in a list at a given index
182:17 - now that we have a linked list let's
182:19 - talk about where you can use them the
182:21 - honest answer is not a lot of places
182:24 - linked lists are really useful
182:26 - structures to build for learning
182:28 - purposes because they're relatively
182:29 - simple and are a good place to start to
182:32 - introduce the kinds of operations we
182:34 - need to implement for various data
182:36 - structures it is quite rare however that
182:38 - you will need to implement a linked list
182:40 - on your own
182:42 - there are typically much better and by
182:44 - that i mean much more efficient data
182:46 - structures that you can use
182:48 - in addition many languages like java for
182:50 - example provide an implementation of a
182:52 - linked list already
182:54 - now that we have a custom data structure
182:57 - let's do something with it let's combine
182:59 - the knowledge we have and look at how a
183:01 - sorting algorithm can be implemented
183:04 - across two different data structures
183:07 - [Music]
183:11 - now that we've seen two different data
183:12 - structures let's circle back and apply
183:14 - what we know about algorithms to these
183:16 - new concepts
183:17 - one of the first algorithms you learned
183:19 - about was binary search and we learned
183:21 - that with binary search there was one
183:23 - precondition the data collection needs
183:25 - to be sorted
183:26 - over the next few videos let's implement
183:28 - the merge sort algorithm which is one of
183:30 - many sorting algorithms on both arrays
183:33 - or python lists and the singly linked
183:35 - list we just created
183:37 - this way we can learn a new sorting
183:39 - algorithm that has real world use cases
183:41 - and see how a single algorithm can be
183:44 - implemented on different data structures
183:46 - before we get into code let's take a
183:48 - look at how merge sort works
183:50 - conceptually and we'll use an array to
183:52 - work through this
183:54 - we start with an unsorted array of
183:56 - integers and our goal is to end up with
183:58 - an array sorted in ascending order
184:01 - merge sort works like binary sort by
184:04 - splitting up the problem into sub
184:06 - problems but it takes the process one
184:08 - step further
184:09 - on the first pass we're going to split
184:11 - the array into two smaller arrays now in
184:14 - binary search one of these subarrays
184:16 - would be discarded but that's not what
184:18 - happens here
184:19 - on the second pass we're going to split
184:20 - each of those subarrays into further
184:23 - smaller evenly sized arrays and we're
184:25 - going to keep doing this until we're
184:27 - down to single element arrays
184:30 - after that the merge sort algorithm
184:32 - works backwards repeatedly merging the
184:34 - single element arrays and sorting them
184:37 - at the same time
184:39 - since we start at the bottom by merging
184:41 - to single element arrays we only need to
184:44 - make a single comparison to sort the
184:46 - resulting merge array by starting with
184:49 - smaller arrays that are sorted as they
184:51 - grow
184:52 - merge sort has to execute fewer sort
184:54 - operations than if it sorted the entire
184:57 - array at once
184:58 - solving a problem like this by
185:00 - recursively breaking down the problem
185:02 - into subparts until it is easily solved
185:05 - is an algorithmic strategy known as
185:07 - divide and conquer but instead of
185:09 - talking about all of this in the
185:10 - abstract let's dive into the code this
185:13 - way we can analyze the runtime as we
185:15 - implement it
185:17 - for our first implementation of merge
185:19 - sort we're going to use an array or a
185:21 - python list
185:23 - while the implementation won't be
185:24 - different conceptually for a linked list
185:27 - we will have to write more code because
185:29 - of list traversal and how nodes are
185:31 - arranged so once we have these concepts
185:33 - squared away we'll come back to that
185:36 - let's add a new file here
185:39 - we'll call this merge underscore sort
185:43 - dot pi
185:45 - in our file let's create a new function
185:47 - named merge sort that takes a list and
185:50 - remember when i say list unless i
185:52 - specify linked list i mean a python list
185:55 - which is the equivalent of an array so
185:57 - we'll say def
185:59 - merge underscore sort
186:01 - and takes a list
186:03 - in the introduction to algorithms course
186:06 - we started our study of each algorithm
186:08 - by defining the specific steps that
186:10 - comprise the algorithm
186:12 - let's write that out as a docstring
186:14 - in here the steps of the algorithm so
186:16 - that we can refer to it right in our
186:18 - code
186:20 - this algorithm is going to sort the
186:22 - given list in an ascending order so
186:24 - we'll start by putting that in here as a
186:26 - simple definition
186:28 - sorts a list in ascending
186:32 - order
186:33 - there are many variations of merge sort
186:36 - and in the one we're going to implement
186:38 - we'll create and return a new sorted
186:41 - list other implementations will sort the
186:43 - list we pass in and this is less typical
186:46 - in an operation known as sort in place
186:50 - but i think that returning a new list
186:51 - makes it easier to understand the code
186:54 - now these choices do have implications
186:56 - though and we'll talk about them as we
186:58 - write this code
187:00 - for our next bit of the docs string
187:02 - let's write down the output of this
187:03 - algorithm so returns
187:06 - a new
187:07 - sorted list
187:09 - merge sort has three main steps
187:12 - the first is the divide step where we
187:14 - find the midpoint of the list so i'll
187:17 - say divide
187:19 - find the mid point of the list and
187:22 - divide
187:26 - into sub-lists
187:29 - the second step is the conquer step
187:31 - where we sort the sub-list that we
187:33 - created in the divide step so we'll say
187:35 - recursively
187:37 - sort the sub-lists created in previous
187:42 - step
187:43 - and finally the combine the combined
187:45 - step where we merge these recursively
187:48 - sorted sub-lists back into a single list
187:51 - so merge the sorted sub-lists
187:55 - created in previous
187:58 - step
187:59 - when we learned about algorithms we
188:01 - learned that a recursive function has a
188:04 - basic pattern first we start with a base
188:07 - case that includes a stopping condition
188:10 - after that we have some logic that
188:12 - breaks down the problem and recursively
188:14 - calls itself
188:16 - our stopping condition is our end goal a
188:18 - sorted array
188:20 - now to come up with a stopping condition
188:22 - or a base case we need to come up with
188:25 - the simplest condition that satisfies
188:28 - this end result
188:29 - so there are two possible values that
188:31 - fit a single element list or an empty
188:35 - list
188:36 - now in both of these situations we don't
188:38 - have any work to do
188:40 - if we give the merge sort function an
188:42 - empty list or a list with one element
188:44 - it's technically already sorted we call
188:46 - this naively sorting so let's add that
188:49 - as our stopping condition we'll say if
188:51 - len list if the length of the list is
188:54 - less than or equal to one
188:57 - then we can return the list
188:59 - okay so this is a stopping condition
189:02 - and now that we have a stopping
189:04 - condition we can proceed with the list
189:06 - of steps
189:09 - first we need to divide the list into
189:11 - sub lists
189:13 - to make our functions easier to
189:14 - understand we're going to put our logic
189:16 - in a couple different functions instead
189:18 - of one large one so i'll say it left
189:21 - half
189:23 - comma right half
189:25 - equal
189:27 - split
189:28 - list so here we're calling a split
189:31 - function that splits the list we pass in
189:34 - and returns two lists split at the
189:36 - midpoint because we're returning two
189:38 - lists we can capture them in two
189:40 - variables
189:42 - now you should know that this split
189:44 - function is not something that comes
189:45 - built into python this is a global
189:47 - function that we're about to write
189:50 - next is the conquer step where we sort
189:52 - each sub-list and return a new sorted
189:55 - sub-list
189:56 - so we'll say left equal
189:59 - merge sort
190:01 - left half
190:04 - and right equal merge sort
190:09 - right half
190:11 - this is the recursive portion of our
190:13 - function so here we're calling merge
190:15 - sort on this divided sub list so we
190:18 - divide the list into two here and then
190:19 - we call merge sort on it again
190:22 - this further splits that sublist into
190:25 - two in the next pass through of merge
190:27 - sort this is going to be called again
190:29 - and again and again until we reach our
190:32 - stopping condition where we have single
190:35 - element lists or empty lists
190:37 - when we've subdivided until we cannot
190:39 - divide any more then we'll end up with a
190:42 - left and a right half
190:44 - and we can
190:45 - start merging backwards so we'll say
190:48 - return
190:49 - merge
190:51 - left and right
190:53 - that brings us to the combined step
190:55 - once two sub-lists are sorted and
190:57 - combined we can return it
191:00 - now obviously none of these functions
191:02 - merge merge sort well merge sort is
191:04 - written but merge and split haven't been
191:06 - written so all we're going to do here if
191:08 - we run it is raise an error so in the
191:10 - next video let's implement the split
191:12 - operation
191:14 - the first bit of logic we're going to
191:16 - write is the divide step of the
191:17 - algorithm this step is fairly
191:19 - straightforward and only requires a few
191:21 - lines of code but is essential to get
191:23 - the sorting process going
191:25 - all right so as we saw earlier we're
191:27 - going to call the function for the
191:28 - divide step split so we'll say def split
191:32 - and split is going to take as an
191:34 - argument a list to split up
191:37 - let's document how this function works
191:40 - so we'll say
191:42 - divide the unsorted list at midpoint
191:47 - into sub lists and it's always good to
191:50 - say what we're returning as well so
191:52 - we'll say returns to sub-lists
191:56 - left and right
192:00 - all right so the first step is to
192:01 - determine the midpoint of this list of
192:04 - this array
192:05 - we're going to use the floor division
192:07 - operator for this
192:09 - floor division carries out a division
192:11 - operation and if we get a non-integer
192:13 - value like 2.5 back it just gets rounded
192:16 - down to two we'll define the midpoint to
192:19 - be the length of the list divided by two
192:22 - and then rounded down
192:24 - so
192:25 - lan list
192:27 - and using the
192:28 - two forward slashes for the floor
192:30 - division operator we'll put number two
192:33 - after it
192:34 - okay once we have the midpoint we can
192:36 - use the slicing notation in python to
192:40 - extract portions of the list we want to
192:42 - return
192:43 - for instance we can define left
192:47 - as the left sub-list that goes all the
192:49 - way from the start of the list
192:52 - all the way up to the midpoint without
192:54 - including the midpoint
192:56 - now over here we're using the slicing
192:59 - syntax where it's like using the you
193:02 - know subscript notation to access a
193:04 - value from a list but instead we give
193:06 - two index values as a start and stop
193:09 - if we don't include a start value as
193:11 - i've done here python interprets that as
193:14 - starting from the zeroth index or the
193:16 - start of the list now similarly we can
193:19 - define right
193:20 - [Music]
193:22 - to be values on the right of the
193:24 - midpoint so starting at the midpoint and
193:27 - going all the way up to the end of the
193:29 - list
193:30 - so a couple things to note as i said
193:33 - earlier when you don't include the
193:34 - starting index it interprets it as to
193:36 - start at the very beginning of the list
193:39 - the index you give as the stopping
193:40 - condition that value is not included in
193:43 - the slice so over here we're starting at
193:46 - the very beginning of list and we go all
193:48 - the way up to midpoint but not including
193:50 - midpoint and then right starts at
193:53 - midpoint so it includes that value and
193:55 - then goes all the way to the end of the
193:56 - list
193:57 - now once we have these two sub-lists we
194:00 - can return them
194:02 - so we'll return left and right notice
194:05 - that we're returning two values here and
194:07 - then in the merge sort function when we
194:10 - call that split function
194:12 - we're declaring two variables left half
194:14 - and right half to assign so that we can
194:17 - assign these two sub lists to them
194:20 - okay and that's all there is to the
194:22 - split function in the next video let's
194:24 - implement the crucial portion of the
194:26 - merge sort logic
194:28 - once we run the split function
194:30 - recursively over the array we should end
194:32 - up with several single member or empty
194:34 - arrays
194:36 - at this point we need to merge them all
194:37 - back and sort them in the process which
194:39 - is what our merge function is for
194:42 - the merge function is going to take two
194:44 - arrays or lists as arguments and to
194:47 - match the naming conventions we used in
194:49 - the split function we'll call this left
194:51 - and right as well so we'll say def merge
194:55 - takes a left and a right list
194:58 - now like before let's add some
194:59 - documentation to our function so this
195:02 - function merges to lists or arrays
195:07 - sorting them in the process
195:11 - and then it returns a new merged list
195:15 - since our function is going to return a
195:17 - new list let's start by creating one
195:21 - now in the process of merging we need to
195:24 - sort the values in both lists
195:27 - to sort we need to compare values from
195:29 - each array or each list so next let's
195:33 - create two local variables to keep track
195:35 - of index values that we're using for
195:37 - each list
195:39 - so the convention here is i and j so
195:41 - we'll stick to it
195:42 - so i equals 0 j equals 0.
195:45 - as we inspect each value in either list
195:48 - we'll use the variables to keep track of
195:51 - the indexes of those values so we'll use
195:54 - i to keep track of indexes in the left
195:56 - list and j for indexes in the right list
196:00 - when merging we want to keep sorting the
196:02 - values until we've worked through both
196:05 - lists so for our loop let's set up two
196:08 - conditions with an and operator so we'll
196:10 - say while
196:12 - let's just stay up here while i is less
196:15 - than
196:18 - while i is less than the length of the
196:21 - left list
196:22 - and j
196:24 - is less than the length
196:27 - of the right list then we'll keep
196:30 - executing our loop so here we're
196:32 - ensuring that as long as i is less than
196:34 - the length of the left list
196:36 - and the and is important and j is less
196:39 - than the length of the right list we're
196:41 - going to keep executing the code now i
196:44 - and j are both set to zero initially
196:46 - which means that our first comparison
196:48 - operation will be on the first element
196:51 - of each list respectively so we'll say
196:55 - if
196:56 - left i so i zero so this is going to get
196:59 - the first value out of the left list
197:02 - is less than right
197:04 - j
197:05 - and again here
197:07 - j is zero so we're going to get the
197:08 - first value out of the right list now if
197:11 - the value at index i in the left list is
197:14 - less than the value at index j in the
197:16 - right list what do we do well that means
197:19 - the value being compared in left is less
197:22 - than the value in the right and can be
197:24 - placed at position 0 in the new array l
197:28 - that we created earlier so here we'll
197:30 - say l dot append
197:32 - left
197:33 - i
197:35 - since we've read and done something with
197:38 - the value at position i let's increment
197:41 - that value so we move forward to
197:43 - evaluate the next item in the left list
197:49 - i plus one or we can say i plus equal
197:52 - one okay next is an else statement
197:57 - and here we'll say
197:58 - if the value at index i so i don't have
198:01 - to write out the actual logic because
198:04 - it's implied so here we're saying that
198:07 - left the value at left is less than the
198:09 - value at right now in the else clause if
198:11 - the value at so i equal
198:15 - is greater and i haven't written out
198:16 - that condition because it's implied so
198:18 - here we're saying if the value in the
198:20 - left is less than the value in the right
198:22 - so in the else clause it's going to mean
198:24 - that the value in the left is either
198:25 - greater than or equal to the value in
198:28 - the right but when we hit the else
198:30 - clause if the value at index i in the
198:32 - left list is greater
198:34 - then we place the value at index j from
198:38 - the right list at the start of the new
198:40 - one list l
198:42 - and similarly increment j so here we'll
198:45 - say l dot append
198:47 - right
198:48 - j and then j equal j plus one
198:55 - doing this doesn't necessarily mean that
198:57 - in one step we'll have a completely
198:59 - sorted array but remember that because
199:01 - we start with single element arrays and
199:04 - combine with each merge step we will
199:06 - eventually sort all the values more than
199:09 - one time and by the time the entire
199:11 - process is done all the values are
199:13 - correctly sorted
199:15 - now this isn't all we need to do in the
199:17 - merge step however there are two
199:19 - situations we can run into one where the
199:22 - left array is larger than the right and
199:24 - vice versa so this can occur when an
199:27 - array containing an odd number of
199:29 - elements needs to be split so how do you
199:31 - split a three element array or list well
199:34 - the left can have two elements and the
199:36 - right can have one or the other way
199:38 - around
199:38 - in either case our while loop uses an
199:41 - and condition
199:42 - where the variables used to store the
199:44 - indexes need to be less than the length
199:46 - of the lists if the left list is shorter
199:49 - than the right then the first condition
199:51 - returns false and the entire loop
199:53 - returns false because it's an and
199:55 - condition
199:56 - this means that in such an event when
199:58 - the while loop terminates not all the
200:00 - values in the right list will have been
200:02 - moved over to the new combined list
200:05 - so to account for this let's add two
200:07 - more while loops
200:09 - the first while loop is going to account
200:11 - for a situation where the right list is
200:14 - shorter than the left and the previous
200:16 - loop terminated because we reached the
200:19 - end of the right list first
200:21 - so in this case what we're going to do
200:23 - is simply add the remaining elements in
200:25 - the left to the new list
200:27 - we're not going to compare elements
200:28 - because we're going to assume that
200:30 - within a list the elements are already
200:32 - sorted
200:33 - so while i
200:35 - is less than length of left
200:38 - then
200:39 - it's the same logic l dot append left
200:43 - i
200:44 - and i
200:46 - plus equal one
200:48 - so the while loop is going to have the
200:50 - similar condition keep the loop going
200:52 - until it's at the last index
200:54 - inside the body we're incrementing the
200:56 - index with every iteration of the loop
200:58 - our final loop accounts for the opposite
201:00 - scenario where the left was shorter than
201:03 - the right
201:04 - the only difference here is that we're
201:05 - going to use the variable j along with
201:07 - the right list so we'll say while j
201:10 - is less than length of right
201:14 - l dot append
201:16 - right
201:17 - j
201:20 - and j plus equal one okay let's stop
201:23 - here in the next video let's test out
201:26 - merge sort make sure our code is running
201:28 - correctly and everything is written well
201:30 - and then we'll wrap up this stage by
201:31 - documenting our code and evaluating the
201:34 - run time of our algorithm in the last
201:36 - video we completed our implementation
201:39 - for the merge sort algorithm but we
201:41 - didn't test it in any way let's define a
201:43 - new list at the bottom that contains
201:45 - several numbers
201:47 - you can put whatever you want in there
201:48 - but make sure that the numbers are not
201:50 - in order i'll call mine a list
201:55 - and in here
201:57 - we'll say 54
201:58 - 26 or 62 doesn't matter 93 17
202:04 - 77
202:05 - 31
202:07 - just add enough so that you can make out
202:10 - that it's sorted okay next we're going
202:12 - to call the merge sort algorithm
202:17 - and pass in our list let's assign this
202:19 - to some variables so we'll say l equal
202:21 - merge
202:22 - underscore sort
202:24 - a list
202:27 - and then if it works correctly we should
202:29 - be able to print this list and see what
202:32 - it looks like so i'm going to hit save
202:34 - down here in the console we'll tap out
202:35 - python
202:37 - merge sort dot pi
202:39 - and before i hit enter i actually
202:41 - noticed i made an error in the last
202:42 - video but i'll hit enter anyway and you
202:44 - should see the error pop up okay so what
202:47 - i forgot to do which is a pretty crucial
202:49 - part of our algorithm is in the merge
202:52 - function i forgot to return the list
202:54 - containing the sorted numbers after
202:56 - carrying out all this logic
202:58 - so here at the bottom
203:00 - we'll say return
203:02 - l
203:03 - all right we'll save again
203:05 - and now we'll clear this out and try
203:08 - that one more time
203:10 - and there we go
203:11 - you should see a sorted list printed out
203:14 - we can write out a more robust function
203:16 - to test this because with bigger arrays
203:18 - visually evaluating that printed list
203:20 - won't always be feasible so bring this
203:23 - back down
203:25 - let's get rid of this
203:27 - and we'll call our
203:29 - function verify sorted
203:32 - and this will take a list
203:35 - first we're going to check inside the
203:37 - body of the function we'll check the
203:38 - length of the list
203:40 - if the list is a single element list or
203:44 - an empty list we don't need to do any
203:46 - unnecessary work because remember it is
203:48 - naively sorted so we'll say if n
203:51 - equals 0 or
203:54 - if n equals
203:56 - 1
203:57 - then we'll return true we've verified
203:59 - that it's sorted
204:00 - now to conclude our function we're going
204:02 - to write out one line of code that will
204:04 - actually do quite a bit of work
204:06 - so first we'll say return
204:09 - list
204:10 - zero so we'll take the first element out
204:12 - of the list and we'll compare and see if
204:14 - that's less than the second element in
204:17 - the list okay so first we'll check that
204:20 - the first element in the list is less
204:22 - than the second element in the list
204:24 - this returns either true or false so we
204:26 - can return that directly
204:28 - but this isn't sufficient if it were we
204:31 - could trick the verify function by only
204:33 - sorting the first two elements in the
204:34 - list
204:35 - so to this return statement we're going
204:37 - to use an and operator to add on one
204:41 - more condition for this condition we're
204:43 - going to make a recursive function call
204:46 - back to verify
204:48 - sorted
204:50 - and for the argument we're going to pass
204:52 - in the list
204:53 - going from the second element all the
204:56 - way
204:57 - to the end let's visualize how this
204:59 - would work
205:00 - we'll use a five element list as an
205:02 - example so we'll call verify sorted and
205:05 - pass in the entire list
205:06 - this list is not one or zero elements
205:09 - long so we skip that first if statement
205:12 - there's only one line of code left in
205:14 - the function and first we check that the
205:16 - element at index 0 is less than the
205:18 - element at index 1. if this is false the
205:21 - function returns immediately with a
205:23 - false value
205:25 - an and operator requires both conditions
205:28 - to be true for the entire line of code
205:30 - to return true
205:31 - since the first condition evaluates to
205:33 - false we don't need to bother evaluating
205:35 - the second the second condition is a
205:38 - recursive call with a sub-list
205:41 - containing elements from the original
205:42 - list starting at position 1 and going to
205:45 - the end
205:46 - so on the second call again we can skip
205:48 - that first if statement and proceed to
205:50 - check whether the value at element 0 is
205:53 - less than the value at element 1.
205:55 - remember that because this list is a
205:57 - sub-list of the original starting at the
205:59 - element that was the second element in
206:01 - the original list
206:02 - by comparing the elements at position 0
206:05 - and 1 in the sub list we're effectively
206:08 - comparing the elements at position 1 and
206:10 - 2 in the original list with each
206:13 - recursive call as we create new sub
206:16 - lists that start at index position 1
206:19 - we're able to check the entire list
206:21 - without having to specify any checks
206:23 - other than the first two elements
206:26 - since this is a recursive function it
206:28 - means we need a stopping condition and
206:30 - we have it already it's that first if
206:32 - condition
206:34 - as we keep making sub lists once we
206:36 - reach a single element list that element
206:38 - is already sorted by definition so we
206:41 - can return true since this recursive
206:43 - function call is part of an and
206:45 - condition
206:46 - it means that every single recursive
206:49 - call has to return true all the way back
206:51 - to the beginning for our top level
206:53 - function to return true and for the
206:56 - function to say yes this is sorted
206:58 - now we could have easily done this using
207:00 - an iterative solution and a for loop but
207:02 - this way you get another example of
207:04 - recursion to work through and understand
207:07 - so let's use this function
207:08 - at the bottom we'll say print
207:11 - verify sorted and first we'll pass in a
207:15 - list
207:16 - oops we got rid of that didn't we
207:19 - okay let me write it out again so a list
207:23 - equal
207:24 - and i think i have those original
207:25 - numbers here somewhere so we'll say 54
207:29 - 26 93
207:36 - okay and then we assigned to l the
207:39 - result of calling merge
207:41 - sort
207:42 - on a list
207:44 - okay so now here we're going to use the
207:46 - verify sorted function
207:48 - and we'll check first that a list is
207:51 - sorted that should return false and then
207:53 - we'll check the same call on we'll pass
207:56 - an l and this should return true
207:59 - okay so now at the bottom here in the
208:01 - console
208:03 - we'll call python merge sort dot pi and
208:05 - there we go it returned false for a list
208:08 - meaning it's not sorted but l is sorted
208:11 - cool so our merge sort function works in
208:14 - the next video let's talk about the cost
208:16 - of this algorithm
208:18 - if we go back to the top level the merge
208:21 - sort function what is the run time of
208:23 - this function look like and what about
208:25 - space complexity how does memory usage
208:27 - grow as the algorithm runs
208:29 - to answer those questions let's look at
208:31 - the individual steps starting with the
208:33 - split function in the split function all
208:36 - we're doing is finding the midpoint of
208:38 - the list and splitting the list at the
208:40 - midpoint
208:41 - this seems like a constant time
208:43 - operation but remember that the split
208:45 - function isn't called once it's called
208:47 - as many times as we need it to to go
208:50 - from the initial list down to a single
208:52 - element list
208:53 - now this is a pattern we've seen a
208:55 - couple times now and we know that
208:57 - overall this runs in logarithmic time so
209:00 - let's add that as a comment so here i'll
209:03 - say
209:04 - takes
209:05 - overall
209:06 - big o of log
209:08 - n time now there's a caveat here but
209:11 - we'll come back to that
209:13 - so next up is the merge step in the
209:15 - merge step we've broken the original
209:18 - list down into single element lists and
209:21 - now we need to make comparison
209:22 - operations and merge them back in the
209:24 - reverse order
209:26 - for a list of size n we will always need
209:29 - to make an n number of merge operations
209:31 - to get back from single element lists to
209:34 - a merge list
209:35 - this makes our overall runtime big o of
209:38 - n times log n because that's an n number
209:41 - of merge steps multiplied by log n
209:44 - number of splits of the original list
209:47 - so to our merge step here let's add a
209:49 - comment we'll say it runs
209:51 - in overall
209:53 - oops there we go runs an overall linear
209:56 - time
209:57 - right it takes an n number of steps
210:00 - number of merge steps but now that we
210:02 - have these two so linear here and
210:05 - logarithmic here we can multiply these
210:07 - and say that the merge sort function the
210:10 - top level function we can conclude that
210:12 - the runtime of the overall sorting
210:15 - process is big o of n times log n
210:20 - now what about that caveat i mentioned
210:22 - earlier
210:23 - so if we go back to our split function
210:26 - here
210:28 - right here there we go
210:31 - let's take a look at the way we're
210:32 - actually splitting the list so we're
210:34 - using python's list slicing operation
210:37 - here
210:38 - and passing in two indexes where the
210:40 - split occurs
210:42 - now if you go and poke around the python
210:44 - documentation which i've done it says
210:46 - that a slicing operation is not a
210:48 - constant time operation and in fact has
210:51 - a runtime of big o of k where k
210:54 - represents the slice size
210:57 - this means that in reality our
210:59 - implementation of split this
211:01 - implementation of split does not run in
211:03 - logarithmic time but k times logarithmic
211:06 - time because there is a slice operation
211:09 - for each split
211:10 - this means that our implementation is
211:13 - much more expensive so
211:15 - overall
211:16 - that makes our overall top level merge
211:18 - sort function not n times log n but k n
211:22 - times log n which is much more expensive
211:25 - now let's get rid of all that
211:29 - to fix this we would need to remove this
211:32 - slicing operation now we can do that by
211:34 - using a technique we learned in a
211:36 - previous course
211:38 - in the introduction to algorithms course
211:40 - we looked at two versions of binary
211:42 - search in python a recursive and an
211:45 - iterative version
211:46 - in the recursive one we use list slicing
211:49 - with every recursion call but we achieve
211:51 - the same end result using an iterative
211:53 - approach without using list slicing
211:56 - over there we declared two variables to
211:58 - keep track of the starting and ending
212:00 - positions in the list
212:02 - we could rewrite merge sort to do the
212:04 - same but i'll leave that as an exercise
212:07 - for you if you want some hints if you
212:09 - want any direction i've included a link
212:11 - in the notes with an implementation so
212:13 - that is time complexity now just so we
212:16 - know before moving on for python here
212:19 - our
212:20 - overall run time is not what i've listed
212:22 - here but this is what the actual run
212:25 - time of the merge sort algorithm looks
212:26 - like so the merge step runs in linear
212:29 - time and the split step takes
212:31 - logarithmic time for an overall n times
212:34 - log n and that is how merge sort
212:37 - actually works
212:38 - okay so what about space complexity
212:40 - the merge sort algorithm takes linear
212:42 - space and this is weird to think about
212:44 - it first but as always a visualization
212:47 - helps
212:48 - so if we start at the top again with our
212:50 - full list and carry out the split method
212:53 - until we have single element lists each
212:56 - of these new lists take up a certain
212:58 - amount of space
212:59 - so the second level here we have two
213:01 - lists where each take up an n by two
213:04 - amount of space
213:05 - now this makes it seem that the sum of
213:07 - all this space is the additional space
213:09 - needed for merge sort but that's not
213:11 - actually the case in reality there are
213:14 - two factors that make a difference
213:16 - first not every single one of these sub
213:18 - lists are created simultaneously
213:21 - at step two we create two n by two size
213:24 - sub lists
213:25 - when we move to the next step however we
213:27 - don't hold on to the n by two sub lists
213:30 - and then create four n by four size sub
213:33 - lists for the next split
213:35 - instead after the four n by four size
213:38 - sub lists are created the n by two ones
213:40 - are deleted from memory there's no
213:42 - reason to hold on to them any longer now
213:45 - the second point is that our code
213:47 - doesn't execute every path
213:49 - simultaneously
213:50 - think of it this way when we pass our
213:53 - list to the top level merge sort
213:55 - function
213:56 - our implementation calls split
213:59 - which returns a left half and a right
214:01 - half
214:02 - the next line of code then calls merge
214:05 - sort on the left half again
214:08 - this runs the function the merge sort
214:10 - function again with a new list
214:12 - in that second run of the function split
214:14 - is called again we get a second left and
214:17 - right half and then again like before we
214:20 - call merge sort on this left half as
214:22 - well what this means is that the code
214:24 - walks down the left path all the way
214:27 - down until that initial left half is
214:30 - sorted and merged back into one array
214:33 - then it's going to walk all the way down
214:35 - the right path and sort that until we're
214:37 - back to that first split with two n by
214:40 - two sized sublists
214:42 - essentially we don't run all these paths
214:44 - of code at once so the algorithm doesn't
214:47 - need additional space for every sub-list
214:50 - in fact it is the very last step that
214:52 - matters
214:53 - in the last step the two sub-lists are
214:56 - merged back into the new sorted list and
214:59 - returned
215:00 - that sorted list has an equal number of
215:02 - items as the original unsorted list
215:06 - and since this is a new list it means
215:08 - that at most the additional space the
215:11 - algorithm will require at a given time
215:14 - is n
215:15 - yes at different points in the algorithm
215:17 - we require log n amount of space but log
215:20 - n is smaller than n and so we consider
215:23 - the space complexity of merge sort to be
215:25 - linear because that is the overall
215:27 - factor
215:28 - okay that was a lot so let's stop here
215:31 - don't worry if you've got questions
215:32 - about merge sort because we're not done
215:34 - yet
215:35 - over the next few videos let's wrap up
215:37 - this course by implementing merge sort
215:39 - on a linked list
215:41 - [Music]
215:45 - over the last few videos we implemented
215:47 - the merge sort algorithm on the array or
215:49 - list type in python merge sort is
215:52 - probably the most complicated algorithm
215:54 - we've written so far but in doing so we
215:57 - learned about an important concept
215:59 - divide and conquer
216:00 - we also concluded the last video by
216:02 - figuring out the run time of merge sort
216:04 - based on our implementation
216:07 - over the next few videos we're going to
216:08 - implement merge sort again this time on
216:11 - the linked list type
216:12 - in doing so we're going to get a chance
216:14 - to see how the implementation differs
216:17 - based on the data structure while still
216:19 - keeping the fundamentals of the
216:20 - algorithm the same and we'll also see
216:22 - how the run time may be affected by the
216:24 - kinds of operations we need to implement
216:27 - let's create a new file to put our
216:29 - second implementation of merge sort in
216:32 - so file over here new file
216:35 - and it's going to have a rather long
216:37 - name we'll call this linked
216:39 - list
216:40 - merge sort with underscores everywhere
216:43 - dot pi
216:45 - we're going to need the linked list
216:47 - class that we created earlier so we'll
216:49 - start at the top by importing the linked
216:52 - list class from the linkedlist.pi file
216:56 - the way we do that is we'll say from
216:58 - linked
216:59 - list
217:00 - import linked list
217:03 - right so that imports the class
217:05 - uh let's test if this works really quick
217:09 - we'll just do something like l equal
217:11 - linked
217:12 - list
217:14 - l.add
217:15 - ten
217:16 - or one doesn't matter print l
217:19 - okay and if i hit save
217:22 - and then down here we'll say python
217:25 - linked list
217:26 - merge sword dot pi
217:28 - okay it works so this is how we get some
217:30 - of the code how we reuse the code that
217:32 - we've written in other files into this
217:34 - current file
217:36 - and get rid of this now
217:38 - okay like we did with the first
217:40 - implementation of merge sort we're going
217:42 - to split the code up across three
217:44 - functions
217:45 - the main function merge sort
217:48 - a split function and a merge function
217:52 - now if you were to look up a merge sort
217:54 - implementation in python both for a
217:55 - regular list an array or a linked list
217:58 - you would find much more concise
218:00 - versions out there but they're kind of
218:01 - hard to explain so splitting it up into
218:04 - three will sort of help it you know be
218:06 - easier to understand so we'll call this
218:08 - merge sort at the top level and this
218:11 - time it's going to take a linked list
218:15 - let's add a dog string to document the
218:17 - function
218:18 - so say that this function sorts a linked
218:22 - list in ascending order
218:25 - and like before we'll add the steps in
218:27 - here so we'll say you first recursively
218:30 - divide the linked list into sub lists
218:35 - containing
218:37 - a single
218:38 - node
218:40 - then
218:41 - we repeatedly
218:43 - merge these sub-lists
218:46 - to produce sorted sub-lists
218:49 - until one remains
218:52 - and then finally this function returns a
218:54 - sorted
218:56 - linked list
218:58 - the implementation of this top level
219:01 - merge function is nearly identical to
219:03 - the array or list version we wrote
219:05 - earlier so first we'll provide a
219:07 - stopping condition or two if the size of
219:10 - the list is one or it's an empty list
219:12 - we'll return the linked list since it's
219:15 - naively sorted so if linked
219:18 - list dot size remember that function we
219:20 - run equal one
219:23 - then we'll return
219:24 - linked
219:25 - list
219:27 - else if
219:28 - linked list dot head
219:31 - is none meaning it's an empty list then
219:34 - we'll return linked list as well okay
219:38 - next let's split the linked list into a
219:41 - left and right half
219:43 - conceptually this is no different but in
219:45 - practice we need to actually traverse
219:47 - the list we'll implement a helper method
219:50 - to make this easier
219:51 - but we'll say left half
219:53 - comma right half
219:56 - equal split
219:58 - linked list
220:00 - now once we have two sub lists like
220:01 - before we can call merge sort the top
220:04 - level function on each
220:06 - so left equal merge sort
220:09 - left half
220:14 - and right equal merge sort
220:17 - on the right half
220:20 - finally we'll merge these two top-level
220:22 - sub-lists and return it so merge left
220:26 - and right
220:27 - okay nothing new here but in the next
220:29 - video let's implement the split logic
220:32 - the next step in the merge sort
220:34 - algorithm is the divide step or rather
220:36 - an implementation of the split function
220:39 - so down here
220:40 - we'll call this split like before and
220:42 - this is going to take a linked
220:44 - list
220:47 - documenting things is good and we've
220:48 - been doing it so far so let's add a
220:50 - docstring
220:52 - divide the unsorted list at midpoint
220:58 - into sub-lists
221:00 - now of course when i say sub-lists here
221:02 - i mean sub-linked lists but that's a
221:04 - long word to say
221:06 - now here's where things start to deviate
221:08 - from the previous version
221:10 - with the list type we could rely on the
221:12 - fact that finding the midpoint using an
221:14 - index and list slicing to split into two
221:17 - lists would work even if an empty list
221:20 - was passed in
221:21 - since we have no automatic behavior like
221:23 - that we need to account for this when
221:25 - using a linked list so our first
221:27 - condition is if the linked list is none
221:30 - or if it's empty that is if head is
221:33 - equal to none
221:34 - so we'll say if linked list
221:37 - equal none
221:38 - or you can write is there it doesn't
221:39 - matter or linked list dot head is none
221:45 - well linked list can be none for example
221:47 - if we call split on a linked list
221:49 - containing a single node a split on such
221:51 - a list would mean left would contain the
221:54 - single node while right would be none
221:56 - now in either case we're going to assign
221:59 - the entire list to the left half and
222:01 - assign none to the right so we'll say
222:03 - left half
222:05 - equal linked list
222:08 - and then right half
222:11 - equal none
222:12 - you could also assign the single element
222:15 - list or none to left and then create a
222:17 - new empty linked list assigned to the
222:19 - right half but that's unnecessary work
222:22 - so now that we've done this we can
222:24 - return
222:25 - left half
222:27 - and right half
222:29 - so that's our first condition let's add
222:31 - an else clause to account for non-empty
222:34 - linked lists first we'll calculate the
222:37 - size of the list now this is easy
222:39 - because we've done the work already and
222:41 - we can just call the size method that
222:42 - we've defined we'll say size equal
222:46 - linked underscore list dot size
222:49 - using this size we can determine the
222:50 - midpoint so mid equal size and here
222:53 - we'll use that floor division operator
222:55 - to divide it by two once we have the
222:57 - midpoint we need to get the node at that
223:00 - midpoint
223:01 - now make sure you hit command s to save
223:04 - here and we're going to navigate back to
223:06 - linkedlist.hi
223:09 - in here we're going to add a convenience
223:11 - method at the very bottom right before
223:13 - the wrapper function right here
223:16 - and this convenience method is going to
223:18 - return a node at a given index so i'll
223:21 - call this node
223:24 - at
223:25 - index and it's going to take an index
223:27 - value
223:29 - this way instead of having to traverse
223:31 - the list inside of our split function we
223:34 - can simply call node at index and pass
223:36 - it the midpoint index we calculated to
223:38 - give us the node right there so we can
223:40 - perform the split
223:41 - okay so this method accepts as an
223:44 - argument the index we want to get the
223:46 - node for if this index is zero then
223:48 - we'll return the head of the list so if
223:51 - index double equals zero return
223:55 - self.head
223:56 - the rest of the implementation involves
223:58 - traversing the linked list and counting
224:01 - up to the index as we visit each node
224:05 - the rest of the implementation involves
224:07 - traversing the linked list and counting
224:09 - up to the index as we visit each node so
224:13 - i'll add an else clause here
224:14 - and we'll start at the head so we'll say
224:16 - current equal self.head
224:19 - let's also declare a variable called
224:21 - position
224:23 - to indicate where we are in the list
224:26 - we can use a while loop to walk down the
224:28 - list our condition here is as long as
224:32 - the position is less than the index
224:34 - value
224:36 - so i'll say while position
224:39 - is less than index
224:41 - inside the loop we'll assign the next
224:44 - node to current and increment the value
224:46 - of position by one
224:48 - so current equal current dot next node
224:51 - position
224:54 - plus equal one
224:57 - once the position value equals the index
224:59 - value current refers to the node we're
225:01 - looking for and
225:03 - we can return it we'll say return
225:05 - current
225:08 - let's get rid of all this empty space
225:10 - there we go now back in linked list
225:13 - merge sort
225:14 - dot pi
225:16 - we can use this method to get at the
225:18 - node after we've calculated the midpoint
225:20 - to get the node at the midpoint of the
225:23 - list
225:24 - so we'll say mid node
225:26 - equal
225:27 - linked
225:28 - list
225:29 - dot node at index
225:32 - and here i'm going to do something
225:34 - slightly confusing i'm going to do mid
225:36 - minus 1. remember we're subtracting 1
225:39 - here
225:41 - because we used size
225:43 - to calculate the midpoint and like the
225:46 - len function size will always return a
225:48 - value greater than the maximum index
225:51 - value
225:52 - so think of a linked list with two nodes
225:55 - size would return two
225:57 - the midpoint though and the way we're
225:58 - calculating the index we always start at
226:01 - zero which means size is going to be one
226:03 - greater than that so we're going to
226:04 - deduct one from it to get the value we
226:07 - want but we're using the floor division
226:09 - operator so it's going to round that
226:10 - down even more no big deal with the node
226:13 - at the midpoint now that we have this
226:15 - midnote we can actually split the list
226:18 - so first we're going to assign the
226:19 - entire linked list to a variable named
226:21 - left half so left half
226:24 - equal linked list
226:27 - this seems counterintuitive but make
226:29 - sense in a second
226:30 - for the right half we're going to assign
226:33 - a new instance of linked list so right
226:36 - half equal
226:38 - linked list
226:40 - this newly created list is empty but we
226:42 - can fix that by assigning the node that
226:44 - comes after the midpoint so after the
226:47 - midpoint of the original linked list we
226:49 - can assign the node that comes after
226:51 - that midpoint node as the head of this
226:54 - newly created
226:55 - right linked list
226:57 - so here we'll say right half
226:59 - dot head
227:00 - equal mid node dot
227:03 - node
227:04 - once we do that we can assign none to
227:07 - the next node property on mid node to
227:10 - effectively sever that connection and
227:12 - make what was the mid node now the tail
227:15 - node of the left linked list so i'll say
227:18 - mid node
227:20 - dot next node
227:22 - equal none
227:23 - if that's confusing here's a quick
227:25 - visualization of what just happened
227:28 - we start off with a single linked list
227:30 - and find the midpoint the node that
227:32 - comes after the node at midpoint is
227:34 - assigned to the head of a newly created
227:36 - linked list and the connection between
227:39 - the midpoint node and the one after is
227:41 - removed
227:42 - we now have two distinct linked lists
227:45 - split at the midpoint
227:47 - and with that we can return the two sub
227:49 - lists so we'll return left half
227:52 - and right half
227:54 - in the next video let's tackle our merge
227:56 - function in the last video we defined an
227:59 - implementation for the version of the
228:01 - split function that works on linked
228:03 - lists it contained a tiny bit more code
228:05 - than the array or list version that was
228:07 - expected the merge function is no
228:10 - different because like with the split
228:12 - function after we carry out a comparison
228:14 - operation we also need to swap
228:16 - references to corresponding nodes all
228:19 - right let's add our merge function over
228:21 - here at the bottom below
228:24 - the split functions we'll call this
228:26 - merge and it's going to take a left
228:28 - and right
228:30 - now because this can get complicated
228:32 - we're going to document this function
228:34 - extensively and as always we're going to
228:36 - start with a doc string
228:40 - so we'll say that this function merges
228:43 - two linked lists
228:45 - sorting
228:46 - by data in the nodes
228:50 - and it returns a new
228:53 - merged list
228:56 - remember that in the merge step we're
228:58 - going to compare values across two
229:00 - linked lists and then return a new
229:02 - linked list with nodes where the data is
229:05 - sorted
229:06 - so first we need to create that new
229:08 - linked list let's add a comment in here
229:10 - we'll say create
229:12 - a new linked list
229:15 - that contains nodes from
229:18 - let's add a new line merging left and
229:21 - right
229:22 - okay and then create the list so merged
229:24 - equal
229:25 - new linked list
229:28 - to this list we're going to do something
229:30 - unusual we're going to add a fake head
229:33 - this is so that when adding sorted nodes
229:35 - we can reduce the amount of code we have
229:37 - to write by not worrying about whether
229:39 - we're at the head of the list once we're
229:41 - done we can assign the first sorted node
229:44 - as the head and discard the fake head
229:46 - now this might not make sense at first
229:48 - but not having to worry about whether
229:50 - the new linked list already contains a
229:52 - head or not makes the code simpler we'll
229:55 - add another comment
229:56 - and a fake hand that is discarded
230:00 - later
230:01 - we'll say merged dot add zero
230:04 - like we've been doing so far we'll
230:06 - declare a variable named current to
230:09 - point to the head of the list
230:12 - set current to the head of the linked
230:16 - list and then current equal
230:19 - merged dot head
230:21 - next we'll get a reference to the head
230:23 - on each of the linked lists left and
230:26 - right
230:27 - so we'll say obtain
230:29 - head nodes
230:31 - for left and right linked lists
230:35 - and here's call this left head
230:38 - equal
230:40 - left dot head
230:41 - and right hand equal right dot head
230:48 - okay so with that setup out of the way
230:50 - let's start iterating over both lists
230:54 - so another comment
230:56 - iterate over left and right
230:59 - as long
231:00 - or we'll say until
231:02 - the
231:03 - until we reach the tail node
231:06 - of either
231:08 - and we'll do that by saying while
231:11 - left head
231:12 - or right head
231:15 - so this is a pattern that we've been
231:16 - following all along we're going to
231:18 - iterate until we hit the tail nodes of
231:20 - both lists and we'll move this pointer
231:22 - forward every time so that we traverse
231:24 - the list with every iteration if you
231:26 - remember the logic behind this from the
231:28 - earlier version once we hit the tail
231:30 - note of one list if there are nodes left
231:33 - over in the other linked list we don't
231:35 - need to carry out a comparison operation
231:37 - anymore and we can simply add those
231:39 - nodes to the merged list
231:41 - the first scenario we'll consider is if
231:43 - the head of the left linked list is none
231:46 - this means we're already past the tail
231:48 - of left and we can add all the nodes
231:51 - from the right linked list to the final
231:53 - merge list so here i'll say if
231:56 - the head node of left is none
232:00 - we're past the tail
232:03 - add the node
232:06 - from the right from right to merged
232:09 - linked
232:10 - list so here we'll say if
232:13 - left head
232:15 - is none
232:16 - current dot next node
232:19 - remember current points to the head of
232:21 - the merge list that we're going to
232:23 - return so here we're setting its next
232:25 - node reference to the head node on the
232:28 - right link list so we'll say right head
232:33 - then when we do that we'll move the
232:35 - right head forward to the next node so
232:38 - let's say right
232:40 - head
232:41 - equal right hand
232:43 - dot next node
232:46 - this terminates the loop on the next
232:48 - iteration let's look at a visualization
232:50 - to understand why
232:52 - let's say we start off with a linked
232:54 - list containing four nodes so we keep
232:56 - calling split on it until we have lists
232:58 - with just a single head single node
233:01 - linked lists essentially
233:02 - so let's focus on these two down here
233:04 - that we'll call left and right
233:06 - we haven't implemented the logic for
233:08 - this part yet but here we would compare
233:10 - the data values and see which one is
233:12 - less than the other
233:13 - so we'll assume that left's head is
233:15 - lesser than right's head so we'll set
233:17 - this as the next node in the final merge
233:19 - list
233:20 - left is now an empty length list so left
233:23 - dot head equals none on the next pass
233:26 - through the loop left head is none which
233:29 - is the situation we just implemented
233:31 - here we can go ahead and now assign
233:34 - right head as the next note in the merge
233:36 - link list we know that right is also a
233:38 - singly linked list
233:40 - here's the crucial bit when we move the
233:42 - pointer forward by calling next node on
233:45 - the right node there is no node and the
233:48 - right link the right linked list is also
233:51 - empty now which means that both left
233:54 - head and right head are none and either
233:56 - one of these would cause our loop
233:57 - condition to terminate
233:59 - so what we've done here is encoded a
234:02 - stopping condition for the loop so we
234:04 - need to document this because it can get
234:06 - fuzzy so right above that line of code
234:08 - i'll say call next
234:10 - on right
234:12 - to
234:13 - set loop condition
234:16 - to false
234:17 - okay there's another way we can arrive
234:18 - at this stopping condition and that's in
234:20 - the opposite direction if we start with
234:22 - the right head being none so here we'll
234:24 - say i'm going to add another comment
234:28 - if oops not there there
234:31 - if
234:33 - the head node of right
234:36 - is none
234:38 - we're past the tail
234:40 - then we'll say
234:41 - add the tail node from left
234:45 - to merged linked list
234:48 - and then we'll add that condition we'll
234:49 - say else if right head
234:52 - is none
234:53 - now remember we can enter these even if
234:56 - left head is none we can still go into
234:59 - this condition we can still enter this
235:01 - if statement and execute this logic
235:03 - because the while loop the loop
235:05 - condition here is an or statement so
235:08 - even if left head is false if this
235:10 - returns true because there's a value
235:11 - there there's a node there the loop will
235:13 - keep going
235:15 - okay now in this case we want to set the
235:17 - head of the left linked list as the next
235:20 - node on the merge list so this is simply
235:22 - the opposite of what we did over here
235:25 - we'll set current dot next node
235:28 - equal to left head
235:30 - and then we'll move so after doing that
235:33 - we can move the variable pointing to
235:35 - left head forwards which as we saw
235:37 - earlier is past the tail node and then
235:39 - results in the loop terminating so we'll
235:41 - say left hand
235:43 - equal
235:44 - left
235:45 - head dot next node and we'll add that
235:48 - comment here as well so we'll say call
235:50 - next on left to set loop condition
235:55 - to false
235:56 - because here right head is none and now
235:58 - we make left head none these two
236:00 - conditions we looked at where either the
236:02 - left head or right head
236:05 - were at the tail nodes of our respective
236:07 - lists those are conditions that we run
236:10 - into when we've reached the bottom of
236:12 - our split where we have single element
236:14 - linked lists or empty linked lists let's
236:17 - account for our final condition where
236:20 - we're evaluating a node that is neither
236:22 - the head nor the tail of the list
236:25 - and this condition we need to reach into
236:27 - the nodes and actually compare the data
236:29 - values to one another before we can
236:31 - decide which node to add first to the
236:34 - merged list
236:35 - so here this is an else because we've
236:37 - arrived at our third condition third and
236:39 - final
236:40 - and here we'll say not at either tail
236:43 - node
236:45 - obtain
236:47 - no data to perform
236:49 - comparison operations so let's get each
236:52 - of those data values out of the
236:54 - respective nodes so that we can compare
236:56 - it so we'll say left
236:57 - data equal left head dot data
237:01 - and write data
237:03 - equal right head righthead.data
237:06 - okay what do we do next well we compare
237:08 - but first let's add a comment so we'll
237:10 - say if data
237:12 - on left
237:14 - is less than right
237:16 - set current to left node and then
237:21 - move
237:22 - actually we'll add this in a second so
237:24 - here we'll say if left data
237:26 - is less than write data
237:29 - then current dot next node
237:32 - equal left head
237:34 - and then
237:35 - we'll add a comment and we'll say move
237:38 - left head to next node
237:40 - on that list so we'll say left head
237:43 - equal left head
237:45 - dot next node
237:49 - just as our comment says we'll check if
237:51 - the left data is less than the right
237:54 - data if it is since we want a list in
237:57 - ascending order we'll assign the left
237:59 - node to be the next node in the merged
238:01 - list we'll also move the left head
238:03 - forward to traverse down to the next
238:05 - node in that particular list now if left
238:09 - is larger than right then we want to do
238:11 - the opposite so we'll go back to spaces
238:14 - another comment
238:15 - if data on left is greater than right
238:19 - set current
238:21 - to
238:22 - right node
238:24 - okay so else
238:26 - here we assign the right head to be the
238:29 - next node in the merge list so
238:31 - current.nextnode
238:32 - equal right
238:35 - head
238:36 - and then comment
238:38 - move right head to next node
238:43 - so right
238:44 - head equal
238:46 - right head
238:47 - dot next
238:49 - node okay after doing that we move the
238:52 - right head pointer to reference the next
238:54 - node in the right list
238:56 - and finally at the end of each iteration
239:00 - of the while loop so not here but two
239:02 - spaces back right make sure we're
239:04 - indented at the same level as the while
239:07 - so we got to go yep or not the same
239:10 - level as the wild but the same outer
239:11 - scope
239:12 - and then there we're going to say
239:15 - move current to next node
239:19 - so current equal current dot next node
239:23 - okay don't worry if this is confusing as
239:25 - always we'll look at a visualization in
239:28 - just a bit so we'll wrap up this
239:29 - function by discarding that fake head we
239:31 - set earlier setting the correct node as
239:34 - head and returning the linked list so
239:37 - we'll add a comment
239:38 - discard fake head
239:40 - and set
239:41 - first
239:44 - merged node as head so here we'll say
239:47 - head equal
239:49 - merged dot head dot next
239:52 - node and then merged dot head equal head
239:57 - and finally return
239:59 - merged
240:00 - okay we wrote a lot of code here a lot
240:02 - of it was comments but still it's a
240:04 - bunch let's take a quick break in the
240:06 - next video we'll test this out evaluate
240:09 - our results and determine the runtime of
240:11 - our algorithm
240:12 - okay first things first let's test out
240:15 - our code now we'll keep it simple
240:17 - because writing a robust verify function
240:19 - would actually take up this entire video
240:21 - instead i'll leave that up to you to try
240:23 - as homework
240:25 - okay so
240:26 - at the very end
240:29 - let's create a new linked list
240:33 - let's add a few notes to this so l add
240:35 - i'm going to copy paste this
240:37 - so it makes it easier for me
240:39 - not to have to retype a bunch so i'll
240:41 - add 10
240:42 - uh then set 2
240:44 - 44
240:45 - 15
240:47 - and something like 200. okay then we'll
240:50 - go ahead and print l so that we can
240:52 - inspect this list
240:55 - next
240:56 - let's create a declare variable here so
240:59 - we'll call this sorted linked list
241:02 - and to this we're going to assign the
241:04 - result of calling merge sort on l
241:07 - and then we'll print this
241:09 - so sorted
241:10 - linked
241:12 - list
241:13 - okay since we've taken care of all the
241:15 - logic we know that this gets added in as
241:17 - nodes and then
241:18 - let's see what this looks like all right
241:20 - so hit save
241:21 - and then bring up the console we're
241:23 - going to type out python
241:26 - linked
241:27 - list underscore merge sort dot pi and
241:30 - then enter okay so we see that linked
241:34 - list we first created remember that what
241:36 - we add first right that eventually
241:39 - becomes a tail or right yeah so 10 is
241:41 - the tail 200 is the last one so 200 is
241:43 - the head because i'm calling add it
241:45 - simply adds each one to the head of the
241:46 - list so here we have 10 to 44 15 and 200
241:51 - in the order we added and then the
241:53 - sorted linked list sorts it out so it's
241:56 - 2 10 15 44 and 200. look at that a
242:00 - sorted linked list
242:01 - okay so let's visualize this from the
242:03 - top
242:04 - we have a linked list containing five
242:06 - nodes with integers 10 2 4 15 and 200 as
242:12 - data respectively our merge sort
242:14 - function calls split on this list the
242:17 - split function calls size on the list
242:19 - and gets back 5 which makes our midpoint
242:22 - 2.
242:23 - using this midpoint we can split the
242:25 - list using the node at index method
242:28 - remember that when doing this we deduct
242:30 - 1 from the value of mid so we're going
242:32 - to split here using an index value of 1.
242:35 - effectively this is the same since we're
242:37 - starting with an index value of 0 1
242:39 - means we split after node 2. we assign
242:42 - the entire list to left half
242:44 - then create a new list and assign that
242:46 - to right half
242:48 - we can assign node 3 at index value 2 as
242:51 - the head of the right list and remove
242:53 - the references between node two and node
242:56 - three so far so good right
242:58 - okay so now we're back in the merge sort
243:00 - function after having called split and
243:03 - we have two linked lists
243:05 - let's focus on just the left half
243:07 - because if you go back and look at our
243:08 - code we're going to call merge sort on
243:11 - the left linked list again
243:13 - this means the next thing we'll do is
243:15 - run through that split process since
243:17 - this is a linked list containing two
243:19 - nodes this means that split is going to
243:21 - return a new left and right list each
243:23 - with one node again we're back in the
243:25 - merge sort function which means that we
243:28 - call merge sort on this left list again
243:32 - since this is a single node linked list
243:34 - on calling merge sort on it we
243:36 - immediately return before we split since
243:38 - we hit that stopping condition so we go
243:40 - to the next line of code which is
243:42 - calling merge sort on the right list as
243:45 - well but again we'll get back
243:46 - immediately because we hit that stopping
243:48 - condition now that we have a left and
243:50 - right that we get back from calling
243:52 - merge sort we can call merge on them
243:55 - inside the merge function we start by
243:57 - creating a new linked list and attaching
244:00 - a fake head
244:01 - then we evaluate whether either the left
244:03 - or the right head is none
244:05 - since neither condition is true we go to
244:07 - the final step where we evaluate the
244:08 - data in each node
244:10 - in this case the data in the right node
244:13 - is less than the left node so we assign
244:15 - the right node as the next node in the
244:17 - merge link list and move the right head
244:20 - pointer forward
244:22 - in the merge link list we move our
244:23 - current pointer forward to this new node
244:25 - we've added and that completes one
244:27 - iteration of the loop
244:30 - on the next iteration righthead now
244:32 - points to none since that was a single
244:35 - node list and we can assign the rest of
244:37 - the left linked list which is
244:39 - effectively the single node over to the
244:41 - merge link list here we discard the fake
244:44 - head move the next node up to be the
244:47 - correct head and return the newly merged
244:50 - sorted linked list remember that at this
244:52 - point because right head and left head
244:55 - pointed to none are while loop
244:56 - terminated
244:58 - so in this way we recursively split and
245:00 - repeatedly merge sub-lists until we're
245:03 - back with one sorted linked list
245:05 - the merge sort algorithm is a powerful
245:08 - sorting algorithm but ultimately it
245:10 - doesn't really do anything complicated
245:12 - it just breaks the problem down until
245:14 - it's really simple to solve
245:16 - remember the technique here which we've
245:18 - talked about before is called divide and
245:20 - conquer so i like to think of merge sort
245:22 - in this way there's a teacher at the
245:24 - front of the room and she has a bunch of
245:26 - books that she needs to sort into
245:27 - alphabetical order instead of doing all
245:30 - that work herself she splits that pile
245:32 - into two and hands it to two students at
245:34 - the front
245:35 - each of those students split it into two
245:38 - more and hand it to the four students
245:40 - seated behind them as each student does
245:42 - this eventually a bunch of single
245:45 - students has two books to compare and
245:47 - they can sort it very easily and hand it
245:49 - back to the student who gave it to them
245:51 - in front of them who repeats the process
245:53 - backwards so ultimately it's really
245:55 - simple work is just efficiently
245:57 - delegated
245:59 - now back to our implementation here
246:01 - let's talk about runtime so far other
246:03 - than the node swapping we had to do it
246:05 - seems like most of our implementation is
246:08 - the same right in fact it is including
246:10 - the problems that we ran into in the
246:12 - list version as well so in the first
246:15 - implementation of merge sort we thought
246:17 - we had an algorithm that ran in big o of
246:19 - n log n but turns out we didn't why well
246:23 - the python list slicing operation if you
246:26 - remember actually takes up some amount
246:28 - of time amounting to big o of k
246:31 - a true implementation of merge sort runs
246:33 - in quasi-linear or log linear time that
246:36 - is n times log n so we almost got there
246:40 - but we didn't now in our implementation
246:42 - of merge sort on a linked list we
246:44 - introduce the same problem so if we go
246:46 - back up to
246:48 - the merge or rather the split function
246:50 - this is where it happens now swapping
246:52 - node references that's a constant time
246:54 - operation no big deal comparing values
246:57 - also constant time
246:59 - the bottleneck here like list slicing is
247:03 - in splitting a late list at the midpoint
247:06 - if we go back to our implementation you
247:08 - can see here that we use the node at
247:11 - index method which finds the node we
247:13 - want by traversing the list
247:16 - this means that every split operation
247:19 - incurs a big o of k cost where k here is
247:22 - the midpoint of the list effectively n
247:25 - by 2 because we have to walk down the
247:28 - list counting up the index until we get
247:30 - to that node
247:31 - given that overall splits take
247:34 - logarithmic time our split function just
247:37 - like the one we wrote earlier
247:39 - incurs a cost of
247:41 - big o of
247:43 - k log n so here we'll say it takes
247:46 - big o of k times log n
247:49 - now the merge function also like the one
247:51 - we wrote earlier takes linear time so
247:53 - that one is good that one runs in the
247:55 - expected amount of time so here we'll
247:56 - say runs in linear
248:00 - time
248:01 - and that would bring our overall run
248:03 - time so up at the merge sort function we
248:06 - can say this runs in
248:08 - big o of k n times log
248:12 - n
248:13 - it's okay though this is a good start
248:15 - and one day when we talk about constant
248:17 - factors and look at ways we can reduce
248:19 - the cost of these operations using
248:21 - different strategies we can come back
248:24 - and re-evaluate our code to improve our
248:26 - implementation for now as long as you
248:28 - understand how merge sort works
248:30 - conceptually what the run time and space
248:33 - complexities look like and where the
248:35 - bottlenecks are in your code that's
248:37 - plenty of stuff
248:39 - if you're interested in learning more
248:40 - about how we would solve this problem
248:42 - check out the notes in the teachers
248:44 - video in the next video let's wrap this
248:46 - course up
248:48 - and with that let's wrap up this course
248:50 - in the prerequisite to this course
248:52 - introduction to algorithms we learned
248:54 - about basic algorithms along with some
248:56 - concepts like recursion and big o that
248:59 - set the foundation for learning about
249:01 - implementing and evaluating algorithms
249:04 - in this course we learned what a data
249:06 - structure is and how data structures go
249:08 - hand in hand with algorithms
249:10 - we started off by exploring a data
249:12 - structure that many of us use in our
249:14 - day-to-day programming arrays or lists
249:17 - as they are known in python
249:19 - we take a peek under the hood at how
249:21 - arrays are created and stored and
249:23 - examine some of the common operations
249:25 - carried out on arrays
249:27 - these are operations that we write and
249:29 - execute all the time but here we took a
249:31 - step back and evaluated the run times of
249:33 - these operations and how they affect the
249:35 - performance of our code
249:37 - after that we jumped into an entirely
249:39 - new world where we wrote our own data
249:41 - structure a singly linked list
249:44 - admittedly linked lists aren't used much
249:46 - in day-to-day problem solving but it is
249:49 - a good data structure to start off with
249:51 - because it is fairly straightforward to
249:53 - understand and not that much different
249:55 - from an array
249:56 - we carried out the same exercise as we
249:58 - did on arrays in that we looked at
250:00 - common data operations but since this
250:02 - was a type we defined on our own we
250:05 - implemented these operations ourselves
250:07 - and got to examine with a fine-tooth
250:09 - comb how our code and the structure of
250:11 - the type affected the runtime of these
250:13 - operations
250:15 - the next topic we tackled was
250:16 - essentially worlds colliding we
250:18 - implemented a sorting algorithm to sort
250:21 - two different data structures
250:23 - here we got to see how all of the
250:25 - concepts we've learned so far
250:26 - algorithmic thinking time and space
250:29 - complexity and data structures all come
250:31 - together to tackle the problem of
250:33 - sorting data
250:35 - this kind of exercise is one we're going
250:37 - to focus on moving forward as we try to
250:39 - solve more real-world programming
250:41 - problems using different data structures
250:43 - and algorithms
250:45 - if you've stuck with this content so far
250:47 - keep up the great work this can be a
250:49 - complex topic but a really interesting
250:51 - one and if you take your time with it
250:53 - you will get a deeper understanding of
250:55 - programming and problem solving as
250:57 - always check the notes for more
250:59 - resources and happy coding
251:03 - [Music]
251:10 - you may have heard that algorithms and
251:12 - computer science are boring or
251:14 - frustrating they certainly can be hard
251:16 - to figure out especially the way some
251:18 - textbooks explain them but once you
251:20 - understand what's going on algorithms
251:22 - can seem fascinating clever or even
251:24 - magical
251:25 - to help further your understanding of
251:27 - algorithms this course is going to look
251:29 - at two categories sorting algorithms and
251:32 - searching algorithms you could argue
251:34 - that these are the easiest kinds of
251:35 - algorithms to learn but in learning how
251:38 - these algorithms are designed we'll
251:39 - cover useful concepts like recursion and
251:42 - divide and conquer that are used in many
251:44 - other sorts of algorithms and can even
251:46 - be used to create brand new ones
251:48 - by the way all the code samples i'm
251:50 - going to show in the videos will be in
251:51 - python because it's a popular language
251:53 - that's relatively easy to read but you
251:56 - don't need to know python to benefit
251:57 - from this course you can see the
251:59 - teacher's notes for each video for info
252:01 - on implementing these algorithms in your
252:03 - own favorite language
252:05 - our goal with this course is to give you
252:07 - an overview of how sorting and searching
252:08 - algorithms work but many algorithms have
252:11 - details that can be handled in different
252:13 - ways some of these details may distract
252:15 - from the big picture so we've put them
252:17 - in the teachers notes instead you don't
252:19 - need to worry about these when
252:20 - completing the course for the first time
252:22 - but if you're going back and referring
252:23 - to it later be sure to check the
252:25 - teacher's notes for additional info
252:27 - suppose we have a list of names it's a
252:29 - pretty big list a hundred thousand names
252:31 - long this list could be part of an
252:33 - address book or social media app
252:36 - and we need to find the locations of
252:37 - individual names within the list
252:39 - possibly to look up additional data
252:41 - that's connected to the name
252:43 - let's assume there's no existing
252:44 - function in our programming language to
252:46 - do this or that the existing function
252:48 - doesn't suit our purpose in some way
252:50 - for an unsorted list our only option may
252:53 - be to use linear search also known as
252:55 - sequential search
252:57 - linear search is covered in more detail
252:58 - elsewhere on our site check the
253:00 - teacher's notes for a link if you want
253:01 - more details
253:03 - you start at the first element you
253:05 - compare it to the value you're searching
253:06 - for if it's a match you return it if not
253:09 - you go to the next element
253:11 - you compare that to your target if it's
253:13 - a match you return it if not you go to
253:15 - the next element and so on through the
253:17 - whole list
253:19 - the problem with this is that you have
253:20 - to search the entire list every single
253:23 - time we're not doing anything to narrow
253:25 - down the search each time we have to
253:27 - search all of it
253:29 - if you're searching a big list or
253:30 - searching it repeatedly this amount of
253:32 - time can slow your whole lap down to the
253:34 - point that people may not want to use it
253:36 - anymore that's why it's much more common
253:39 - to use a different algorithm for
253:40 - searching lists binary search
253:43 - binary search is also covered in more
253:45 - detail elsewhere on our site check the
253:47 - teacher's notes for a link
253:49 - binary search does narrow the search
253:51 - down for us specifically it lets us get
253:53 - rid of half the remaining items we need
253:55 - to search through each time
253:57 - it does this by requiring that the list
253:59 - of values be sorted
254:02 - it looks at the value in the middle of
254:04 - the list
254:05 - if the value it finds is greater than
254:07 - the target value it ignores all values
254:09 - after the value it's looking at
254:11 - if the value it finds is less than the
254:13 - target value it ignores all values
254:14 - before the value it's looking at
254:17 - then it takes the set of values that
254:18 - remain and looks at the value in the
254:20 - middle of that list again if the value
254:23 - it finds is greater than the target
254:24 - value it ignores all values after the
254:26 - value it's looking at if the value it
254:28 - finds is less than the target value it
254:30 - ignores all values before the value it's
254:32 - looking at
254:33 - but as we mentioned binary search
254:35 - requires the list of values you're
254:36 - searching through to be sorted
254:39 - if the lists weren't sorted you would
254:40 - have no idea which half of the values to
254:42 - ignore because either half could contain
254:44 - the value you're looking for you'd have
254:46 - no choice but to use linear search
254:49 - so before we can use binary search on a
254:51 - list we need to be able to sort that
254:53 - list we'll look at how to do that next
254:56 - our end goal is to sort a list of names
254:58 - but comparing numbers is a little easier
255:01 - than comparing strings so we're going to
255:02 - start by sorting a list of numbers
255:05 - i'll show you how to modify our examples
255:07 - to sort strings at the end of the course
255:10 - to help make clear the importance of
255:12 - choosing a good sorting algorithm we're
255:14 - going to start with a bad one it's
255:16 - called bogosort basically bogosort just
255:19 - randomizes the order of the list
255:21 - repeatedly until it's sorted
255:24 - here's a python code file where we're
255:26 - going to implement bogosort
255:28 - it's not important to understand this
255:29 - code here at the top although we'll have
255:31 - info on it in the teachers notes if you
255:33 - really want it all you need to know is
255:35 - that it takes the name of a file that we
255:37 - pass on the command line loads it and
255:39 - returns a python list which is just like
255:42 - an array in other languages containing
255:44 - all the numbers that it read from the
255:45 - file
255:47 - let me have the program print out the
255:48 - list of numbers it loads so you can see
255:50 - it we'll call the print method and we'll
255:53 - pass it the list of numbers
255:55 - save that let's run it real quick
255:58 - with python bogosort.pi
256:03 - oh whoops and we need to
256:05 - provide it
256:06 - the name of the file here on the command
256:08 - line that we're going to load so it's in
256:10 - the numbers directory a slash separates
256:13 - the directory name from the file name
256:16 - five dot text
256:18 - and there's our list of numbers that was
256:20 - loaded from the file
256:21 - okay let me delete that print statement
256:23 - and then we'll move on
256:25 - bogo sort just randomly rearranges the
256:28 - list of values over and over so the
256:30 - first thing we're going to need is a
256:32 - function to detect when the list is
256:34 - sorted
256:35 - we'll write an is sorted function that
256:37 - takes a list of values as a parameter
256:42 - it'll return true if the list passed in
256:44 - is sorted or false if it isn't
256:47 - we'll loop through the numeric index of
256:49 - each value in the list from 0 to 1 less
256:52 - than the length of the list like many
256:54 - languages python list indexes begin at 0
256:57 - so a list with a length of 5 has indexes
257:00 - going from 0 through 4.
257:02 - if the list is sorted then every value
257:04 - in it will be less than the one that
257:06 - comes after it so we test to see whether
257:09 - the current item is greater than the one
257:11 - that follows it
257:12 - if it is it means the whole list is not
257:15 - sorted so we can return false
257:17 - if we get down here it means the loop
257:19 - completed without finding any unsorted
257:21 - values python uses white space to mark
257:24 - code blocks so unindenting the code like
257:26 - this marks the end of the loop
257:28 - since all the values are sorted we can
257:30 - return true
257:32 - now we need to write the function that
257:34 - will actually do the so-called sorting
257:36 - the bogosort function will also take the
257:38 - list of values it's working with as a
257:40 - parameter
257:41 - we'll call our is sorted function to
257:44 - test whether the list is sorted we'll
257:46 - keep looping until is sorted returns
257:48 - true
257:49 - python has a ready-made function that
257:51 - randomizes the order of elements in the
257:53 - list
257:54 - since the list isn't sorted we'll call
257:56 - that function here
257:57 - and since this is inside the loop it'll
257:59 - be randomized over and over until our is
258:02 - sorted function returns true
258:04 - if the loop exits it means is sorted
258:07 - returned true and the list is sorted so
258:09 - we can now return the sorted list
258:12 - finally we need to call our bogosort
258:14 - function pass it the list we loaded from
258:16 - the file and print the sorted list it
258:18 - returns
258:20 - okay let's save this and try running it
258:22 - we do so with python the name of the
258:25 - script bogosort.pi
258:27 - and the name of the file we're going to
258:29 - run it on
258:30 - numbers directory5.txt
258:35 - it looks like it's sorting our list
258:37 - successfully
258:38 - but how efficient is this let's add some
258:41 - code to track the number of times it
258:42 - attempts to sort the list
258:44 - up here at the top of the bogus sort
258:46 - function we'll add a variable to track
258:48 - the number of attempts it's made we'll
258:50 - name it attempts and we'll set its
258:51 - initial value to zero since we haven't
258:53 - made any attempts yet
258:56 - with each pass through the loop we'll
258:58 - print the current number of attempts
259:01 - and then here at the end of the loop
259:02 - after attempting to shuffle the values
259:04 - we'll add one to the count of attempts
259:10 - let's save this and let's try running it
259:12 - again a couple times
259:15 - in the console i can just press the up
259:17 - arrow to bring up the previous command
259:18 - and re-run it
259:20 - so it looks like this first run to sort
259:22 - this five element list took 363 attempts
259:25 - let's try it again
259:27 - this time it only took 91 attempts
259:30 - we're simply randomizing the list with
259:32 - each attempt so each
259:34 - run of the program takes a random number
259:36 - of attempts
259:38 - now let's try this same program with a
259:40 - larger number of items
259:42 - python bogo sort
259:45 - numbers
259:47 - i have a list of eight items set up here
259:50 - in this other file
259:53 - this time it takes 11 000 attempts
259:58 - only 487 this time
260:02 - and this time thirteen thousand you can
260:04 - see that the trend is increasing
260:06 - steadily
260:07 - the problem with bogosort is that it
260:09 - doesn't make any progress toward a
260:11 - solution with each pass
260:13 - it could generate a list where just one
260:15 - value is out of order but then on the
260:16 - next attempt it could generate a list
260:18 - where all the elements are out of order
260:20 - again
260:21 - stumbling on a solution is literally a
260:23 - matter of luck and for lists with more
260:25 - than a few items it might never happen
260:28 - up next we'll look at selection sort
260:31 - it's a sorting algorithm that's still
260:33 - slow but it's better than bogo's sort
260:36 - previously we showed you bogo sort a
260:38 - terrible sorting algorithm that
260:40 - basically randomizes the order of a list
260:42 - and then checks to see if it happens to
260:44 - be sorted
260:46 - the problem with bogo's sort is that it
260:48 - doesn't get any closer to a solution
260:50 - with each operation and so with lists
260:52 - that have more than a few items it'll
260:53 - probably never finish sorting them
260:56 - now we're going to look at an algorithm
260:58 - named selection sort it's still slow but
261:01 - at least each pass through the list
261:02 - brings it a little closer to completion
261:06 - our implementation of selection sort is
261:08 - going to use two arrays an unsorted
261:10 - array and a sorted one some versions
261:13 - move values around within just one array
261:15 - but we're using two arrays to keep the
261:17 - code simpler the sorted list starts out
261:19 - empty but we'll be moving values from
261:22 - the unsorted list to the sorted list one
261:24 - at a time
261:25 - with each pass we'll look through each
261:27 - of the values in the unsorted array find
261:29 - the smallest one and move that to the
261:31 - end of the sorted array
261:33 - we'll start with the first value in the
261:35 - unsorted array and say that's the
261:36 - minimum or smallest value we've seen so
261:39 - far
261:40 - then we'll look at the next value and
261:41 - see if that's smaller than the current
261:43 - minimum if it is we'll mark that as the
261:45 - new minimum
261:46 - then we'll move to the next value and
261:48 - compare that to the minimum again if
261:50 - it's smaller that becomes the new
261:52 - minimum we continue that way until we
261:54 - reach the end of the list
261:56 - at that point we know whatever value we
261:58 - have marked as the minimum is the
262:00 - smallest value in the whole list
262:02 - now here's the part that makes selection
262:04 - sort better than bogo sort we then move
262:07 - that minimum value from the unsorted
262:09 - list to the end of the sorted list
262:11 - the minimum value isn't part of the
262:13 - unsorted list anymore so we don't have
262:15 - to waste time looking at it anymore all
262:18 - our remaining comparisons will be on the
262:20 - remaining values in the unsorted list
262:23 - then we start the process over at this
262:25 - point our list consists of the numbers 8
262:27 - 5 4 and 7. our first minimum is 8.
262:31 - we start by comparing the minimum to
262:33 - five five is smaller than eight so five
262:36 - becomes the new minimum then we compare
262:38 - five to four and four becomes the new
262:40 - minimum four is not smaller than seven
262:42 - though so four remains the minimum four
262:45 - gets moved to the end of the sorted
262:46 - array becoming its second element
262:49 - the process repeats again eight is the
262:51 - first minimum but five is smaller so
262:53 - that becomes the minimum seven is larger
262:55 - so five stays is the minimum and five is
262:57 - what gets moved over to the sort of
262:59 - array and so on until there are no more
263:01 - items left in the unsorted array and all
263:03 - we have left is the sorted array
263:06 - so that's how selection sort works in
263:09 - general now let's do an actual
263:10 - implementation of it
263:12 - this code here at the top is the same as
263:14 - we saw in the bogo sword example it just
263:17 - loads a python list of numbers from a
263:19 - file
263:20 - let's implement the function that will
263:22 - do our selection sort we're going to
263:24 - pass in our python list containing all
263:26 - the unsorted numbers we'll create an
263:29 - empty list that will hold all our sorted
263:31 - values
263:33 - we'll loop once for each value in the
263:35 - list
263:37 - we call a function named index submin
263:39 - which we're going to write in just a
263:41 - minute that finds the minimum value in
263:43 - the unsorted list and returns its index
263:46 - then we call the pop method on the list
263:48 - and pass it the index of the minimum
263:50 - value pop will remove that item from the
263:52 - list and return it we then add that
263:55 - value to the end of the sorted list
263:57 - going up a level of indentation signals
264:00 - to python that we're ending the loop
264:02 - after the loop finishes we return the
264:04 - sorted list
264:06 - now we need to write the function that
264:08 - picks out the minimum value we pass in
264:10 - the list we're going to search
264:13 - we mark the first value in the list as
264:15 - the minimum it may or may not be the
264:17 - actual minimum but it's the smallest
264:19 - we've seen on this pass through the list
264:21 - it's also the only value we've seen on
264:23 - this pass through the list so far
264:25 - now we loop through the remaining values
264:27 - in the list after the first
264:30 - we test whether the value we're
264:32 - currently looking at is less than the
264:33 - previously recorded
264:36 - minimum if it is then we set the current
264:39 - index as the new index of the minimum
264:41 - value
264:42 - after we've looped through all the
264:44 - values we return the index of the
264:45 - smallest value we found
264:47 - lastly we need to actually run our
264:49 - selection sort method and print the
264:51 - sorted list it returns
264:53 - let's save this and now let's try
264:55 - running it we run the python command and
264:58 - pass it the name of our script
265:00 - selectionsort.pi
265:03 - in the numbers directory i've saved
265:04 - several data files filled with random
265:06 - numbers one on each line five dot text
265:08 - has five lines eight dot text has eight
265:11 - lines and to help us really measure the
265:12 - speed of our algorithms ten thousand dot
265:15 - text has ten thousand lines i've even
265:17 - created a file with a million numbers
265:20 - our script takes the path of a file to
265:22 - load as an argument so i'll give it the
265:23 - path of our file with five numbers
265:25 - numbers slash five dot text
265:29 - the script runs reads the numbers in the
265:31 - file into a list calls our selection
265:33 - sort method with that list and then
265:35 - prints the sorted list
265:37 - let me add a couple print statements
265:39 - within the selection sort function so
265:41 - you can watch the sort happening
265:43 - don't worry about figuring out the
265:44 - python formatting string that i use it's
265:46 - just there to keep the two lists neatly
265:48 - aligned
265:49 - i'll add the first print statement
265:51 - before the loop runs at all
265:57 - i'll have it print out the unsorted list
265:59 - and the sorted list
266:01 - i'll add an identical print statement
266:03 - within the loop so we can watch values
266:05 - moving from the unsorted list to the
266:06 - sorted list
266:09 - let's save this
266:13 - and we'll try running the same command
266:15 - again the output looks like this you can
266:18 - see the unsorted list on the left and
266:20 - the sorted list on the right
266:22 - initially the sorted list is empty on
266:24 - the first pass it selects the lowest
266:26 - number 1 and moves it to the sorted list
266:29 - then it moves the next lowest number
266:31 - over four
266:33 - this repeats until all the numbers have
266:34 - been moved to the sorted list
266:37 - i have another file with eight different
266:38 - numbers in it let's try our program with
266:40 - that
266:41 - python selection sort dot pi numbers
266:45 - 8.text
266:49 - you can see the same process at work
266:51 - here notice that this file had some
266:53 - duplicate values too that's okay though
266:56 - because the index of min function only
266:58 - updates the minimum index if the current
267:00 - value is less than the previous minimum
267:02 - if they're equal it just keeps the first
267:04 - minimum value it found and waits to move
267:06 - the duplicate value over until the next
267:09 - pass through the list
267:11 - so now we know that the selection sort
267:13 - algorithm works but the data sets we've
267:16 - been giving it sort are tiny in the real
267:18 - world algorithms need to work with data
267:21 - sets of tens of thousands or even
267:23 - millions of items and do it fast i have
267:26 - another file with ten thousand random
267:28 - numbers in it
267:33 - let's see if selection sort can handle
267:35 - that
267:36 - if i run this as it is now though it'll
267:38 - print out a lot of debug info as it
267:40 - sorts the list so first i'm going to go
267:42 - into the program and remove the two
267:44 - print statements in the selection sort
267:46 - function
267:50 - now let's run the program again on the
267:53 - dot text file and see how long it takes
267:56 - python selection sort dot pi
267:59 - numbers
268:01 - ten thousand dot text
268:04 - one one thousand two one thousand three
268:06 - one four one thousand five one thousand
268:09 - six one thousand seven one thousand
268:10 - eight one thousand nine one thousand ten
268:12 - one thousand eleven thousand twelve
268:15 - thousand thirteen thousand and it prints
268:17 - out all ten thousand of those numbers
268:19 - neatly sorted it took a little bit
268:21 - though how long well counting the time
268:24 - off vocally isn't very precise and other
268:26 - programs running on the system can skew
268:28 - the amount of time your program takes to
268:30 - complete
268:31 - let me show you a unix command that's
268:33 - available here in workspaces which can
268:35 - help you type time followed by a space
268:38 - and then the command you want to run
268:41 - so this command by itself will print the
268:43 - contents of our 5.txt file cat as in
268:46 - concatenate numbers 5.text
268:50 - and this command will do the same thing
268:51 - but it'll also keep track of how long it
268:54 - takes the cat program to complete and
268:56 - report the result time
268:58 - cat
268:59 - numbers five dot text
269:03 - the real row in the results is the
269:05 - actual amount of time for when the
269:07 - program started running to when it
269:09 - completed we can see it finished in a
269:11 - fraction of a second but as we said
269:13 - other programs running on the system can
269:15 - take cpu resources in which case your
269:18 - program will seem slower than it is so
269:20 - we generally want to ignore the real
269:22 - result
269:23 - the user result is the amount of time
269:25 - the cpu actually spent running the
269:27 - program code so this is the total amount
269:30 - of time the code inside the cat program
269:32 - took to run
269:33 - the sys result is the amount of time the
269:36 - cpu spent running linux kernel calls
269:38 - that your code made the linux kernel is
269:41 - responsible for things like network
269:42 - communications and reading files so
269:45 - loading the 5.txt file is probably
269:47 - included in this result
269:49 - in evaluating code's performance we're
269:51 - generally going to want to add together
269:53 - the user and sys results but cad is a
269:56 - very simple program let's try running
269:58 - the time command on our code and see if
270:01 - we get a more interesting result
270:03 - time python
270:05 - selection sort dot pi
270:07 - numbers
270:08 - ten thousand dot text
270:13 - this takes much longer to complete
270:15 - nearly 12 seconds according to the real
270:17 - time measurement but as we said the real
270:20 - result is often skewed so let's
270:21 - disregard that
270:23 - if we add the user and cis runtimes
270:26 - together we get about 6 seconds
270:30 - the time for the program to complete
270:32 - will vary a little bit each time you run
270:33 - it but if it's doing the same operations
270:36 - it usually won't change more than a
270:37 - fraction of a second if i run our
270:39 - selection sort script on the same file
270:41 - you can see it completes in roughly the
270:43 - same time
270:44 - now let's try it on another file with 1
270:47 - million numbers time python selection
270:50 - sort dot pi numbers
270:52 - 1 million dot text
270:56 - how long does this one take i don't even
270:58 - know while designing this course i tried
271:00 - running this command and my workspace
271:02 - connection timed out before it completed
271:04 - so we'll just say that selection sort
271:06 - takes a very very long time to sort a
271:09 - million numbers
271:10 - if we're going to sort a list that big
271:12 - we're going to need a faster algorithm
271:14 - we'll look into alternative sorting
271:16 - algorithms shortly
271:18 - the next two sorting algorithms we look
271:20 - at will rely on recursion which is the
271:22 - ability of a function to call itself so
271:25 - before we move on we need to show you
271:27 - how recursion works
271:29 - recursive functions can be very tricky
271:31 - to understand imagine a row of dominoes
271:34 - stood on end where one domino falling
271:36 - over causes the next domino to fall over
271:38 - which causes the next domino to fall
271:40 - over causing a chain reaction it's kind
271:43 - of like that
271:44 - let's suppose we need to write a
271:46 - function that adds together all the
271:47 - numbers in an array or in the case of
271:49 - python a list
271:51 - normally we'd probably use a loop for
271:53 - this sort of operation
271:55 - the function takes a list of the numbers
271:57 - we want to add
271:58 - the total starts at zero
272:01 - we loop over every number contained in
272:03 - the list and we add the current number
272:05 - to the total
272:07 - once we're done looping we return the
272:09 - accumulated total
272:11 - if we call this sum function with a list
272:13 - of numbers it'll return the total when
272:16 - we run this program it'll print out that
272:18 - return value 19. let's try it real quick
272:21 - python
272:23 - recursion.pi oh whoops
272:25 - mustn't forget to save my work here
272:28 - and run it and we see the result 19.
272:32 - to demonstrate how recursion works let's
272:35 - revise the sum function to use recursion
272:37 - instead note that recursion is not the
272:39 - most efficient way to add a list of
272:41 - numbers together but this is a good
272:43 - problem to use to demonstrate recursion
272:45 - because it's so simple one thing before
272:48 - i show you the recursive version though
272:50 - this example is going to use the python
272:52 - slice syntax so i need to take a moment
272:54 - to explain that for those not familiar
272:56 - with it
272:57 - a slice is a way to get a series of
272:59 - values from a list
273:01 - let's load up the python repel or read
273:03 - evaluate print loop so i can demonstrate
273:07 - we'll start by creating a list of
273:08 - numbers to work with numbers equals
273:11 - a list
273:13 - with 0 1 2 3 and 4
273:16 - containing those numbers
273:18 - like arrays in most other languages
273:20 - python list indexes start at 0 so
273:23 - numbers
273:24 - 1
273:25 - will actually get the second item from
273:27 - the list
273:28 - with slice notation i can actually get
273:30 - several items back
273:32 - it looks just like accessing an
273:34 - individual index of a list
273:37 - but then i type a colon
273:39 - followed by the list index that i want
273:41 - up to but not including
273:43 - so numbers 1 colon 4 would get us the
273:46 - second up to but not including the fifth
273:49 - items from the list that is it'll get us
273:52 - the second through the fourth items
273:54 - now i know what you're thinking and
273:56 - you're right that up to but not
273:57 - including rule is a little
273:58 - counterintuitive but you can just forget
274:01 - all about it for now because we won't be
274:03 - using a second index with any of our
274:04 - python slice operations in this course
274:08 - here's what we will be using when you
274:10 - leave the second index off of a python
274:12 - slice it gives you the items from the
274:14 - first index up through the end of the
274:16 - list wherever that is so numbers 1 colon
274:19 - with no index following it will give us
274:21 - items from the second index up through
274:23 - the end of the list
274:25 - numbers 2 colon will give us items from
274:27 - the third index up to the end of the
274:29 - list
274:30 - you can also leave the first index off
274:32 - to get everything from the beginning of
274:34 - the list numbers
274:36 - colon 3 will get us everything from the
274:39 - beginning of the list up to but not
274:41 - including the third index
274:43 - it's also worth noting that if you take
274:45 - a list with only one item and you try to
274:47 - get everything from the non-existent
274:49 - second item onwards the result will be
274:51 - an empty list
274:54 - so if i create a list with just one item
274:57 - in it
274:58 - and i try
275:00 - to access from the second element
275:03 - onwards the second element doesn't exist
275:06 - so the result will be an empty list
275:09 - don't worry too much about remembering
275:11 - python slice syntax it's not an
275:13 - essential part of sorting algorithms or
275:15 - recursion i'm only explaining it to help
275:17 - you read the code you're about to see
275:20 - so i'm going to exit the python rebel
275:22 - now that we've covered recursion we can
275:24 - convert our sum function to a recursive
275:27 - function
275:28 - it'll take the list of numbers to add
275:30 - just like before
275:32 - now here's the recursive part we'll have
275:34 - the sum function call itself we use
275:37 - slice notation to pass the entire list
275:39 - of numbers except the first one
275:42 - then we add the first number in the list
275:43 - to the result of the recursive function
275:45 - call and return the result
275:48 - so if we call sum with four numbers
275:50 - first it'll call itself with the
275:52 - remaining three numbers that call to sum
275:54 - will then call itself with the remaining
275:56 - two numbers and so on
275:58 - but if we save this and try to run it
276:01 - pythonrecursion.pi
276:07 - well first we get a syntax error it
276:08 - looks like i accidentally indented
276:10 - something i shouldn't have so let me go
276:12 - fix that real quick
276:16 - there we go that's suggested to python
276:18 - that there was a loop or something there
276:19 - when there wasn't
276:21 - so let's go back to the terminal and try
276:22 - running this again
276:24 - there we go now we're getting the error
276:26 - i was expecting recursion error maximum
276:29 - recursion depth exceeded
276:31 - this happens because some gets into an
276:33 - infinite loop it keeps calling itself
276:35 - over and over the reason is that when we
276:38 - get down to a list of just one element
276:40 - and we take a slice from the
276:41 - non-existent second element to the end
276:43 - the result is an empty list that empty
276:46 - list gets passed to the recursive call
276:48 - to sum which passes an empty list in its
276:50 - recursive call to sum and so on until
276:53 - the python interpreter detects too many
276:55 - recursive calls and shuts the program
276:57 - down what we need is to add a base case
276:59 - to this recursive function a condition
277:02 - where the recursion stops this will keep
277:04 - it from getting into an infinite loop
277:06 - with the sum function the base case is
277:08 - when there are no elements left in the
277:10 - list in that case there is nothing left
277:12 - to add and the recursion can stop
277:15 - a base case is the alternative to a
277:17 - recursive case a condition where
277:19 - recursion should occur for the sum
277:21 - function the recursive case is when
277:23 - there are still elements in the list to
277:25 - add together
277:26 - let's add a base case at the top of the
277:28 - function
277:31 - python treats a list that contains one
277:33 - or more values as a true value and it
277:35 - treats a list containing no values as a
277:38 - false value
277:39 - so we'll add an if statement that says
277:41 - if there are no numbers in the list we
277:43 - should return a sum of zero that way the
277:46 - function will exit immediately without
277:48 - making any further recursive calls to
277:50 - itself
277:51 - we'll leave the code for the recursive
277:52 - case unchanged
277:54 - if there are still numbers in the list
277:56 - the function will call itself with any
277:57 - numbers after the first then add the
277:59 - return value to the first number in the
278:01 - list
278:02 - let's save this and try running it again
278:05 - python recursion dot pi
278:10 - output the sum of the numbers in the
278:11 - list 19 but it's still not really clear
278:14 - how this worked let's add a couple print
278:16 - statements that will show us what it's
278:18 - doing
278:19 - we'll show the recursive call to sum and
278:21 - what it's being called with
278:26 - we'll also add a call to print right
278:28 - before we return showing which of the
278:29 - calls the sum is returning and what it's
278:32 - returning
278:38 - let me save this and resize the console
278:41 - a bit
278:43 - and let's try running it again
278:46 - python recursion.pi
278:49 - since the print calls are inside the sum
278:51 - function the first call to sum 1279
278:54 - isn't shown only the recursive calls are
278:57 - this first call to sum ignores the first
278:59 - item in the list 1 and calls itself
279:02 - recursively it passes the remaining
279:04 - items from the list 2 7 and 9.
279:07 - that call to sum again ignores the first
279:09 - item in the list it receives 2 and again
279:11 - calls itself recursively it passes the
279:13 - remaining items in the list 7 and 9.
279:16 - that call ignores the 7 and calls itself
279:19 - with a 9
279:20 - and the last call shown here ignores the
279:22 - 9
279:23 - and calls itself with an empty list
279:26 - at this point none of our recursive
279:27 - calls to sum have returned yet each of
279:30 - them is waiting on the recursive call it
279:32 - made to sum to complete
279:34 - python and other programming languages
279:36 - use something called a call stack to
279:38 - keep track of this series of function
279:40 - calls each function call is added to the
279:42 - stack along with the place in the code
279:44 - that it needs to return when it
279:46 - completes
279:47 - but now the empty list triggers the base
279:49 - case causing the recursion to end and
279:51 - the sum function to return zero
279:54 - that zero value is returned to its
279:56 - caller the caller adds the zero to the
279:58 - first and only value in its list nine
280:01 - the result is nine
280:03 - that nine value gets returned to the
280:04 - caller which adds it to the first value
280:06 - in the list it received seven
280:09 - the result is sixteen
280:12 - that sixteen value is returned to the
280:14 - caller which adds it to the first value
280:15 - in the list it received two the result
280:18 - is 18.
280:21 - that 18 value is returned to the caller
280:23 - which adds it to the first value in the
280:25 - list it received one the result is 19.
280:30 - that 19 value is returned to the caller
280:32 - which is not the sum function
280:33 - recursively calling itself but our main
280:35 - program this is our final result which
280:38 - gets printed
280:40 - it's the same result we got from the
280:41 - loop-based version of our program the
280:44 - end
280:45 - we don't want the print statements in
280:46 - our final version of the program so let
280:48 - me just delete those real quick
280:51 - and there you have it a very simple
280:52 - recursive function well the function is
280:55 - simple but as you can see the flow of
280:57 - control is very complex don't worry if
280:59 - you didn't understand every detail here
281:01 - because we won't be using this
281:03 - particular example again
281:05 - there are two fundamental mechanisms you
281:07 - need to remember a recursive function
281:09 - needs a recursive case that causes it to
281:11 - call itself
281:12 - and it also needs to eventually reach a
281:15 - base case that causes the recursion to
281:17 - stop
281:18 - you've seen bogo sort which doesn't make
281:20 - any progress towards sorting a list with
281:22 - each pass either it's entirely sorted or
281:25 - it isn't
281:26 - you've seen selection sort which moves
281:29 - one value over to a sorted list with
281:31 - each pass so that it has fewer items to
281:33 - compare each time
281:35 - now let's look at an algorithm that
281:36 - speeds up the process further by further
281:38 - reducing the number of comparisons it
281:40 - makes it's called quick sort
281:43 - here's some python code where we'll
281:45 - implement quick sort again you can
281:47 - ignore these lines at the top we're just
281:49 - using them to load a file full of
281:51 - numbers into a list
281:52 - the quick sort algorithm relies on
281:54 - recursion to implement it we'll write a
281:57 - recursive function we'll accept the list
281:59 - of numbers to sort as a parameter
282:02 - quicksort is recursive because it keeps
282:04 - calling itself with smaller and smaller
282:06 - subsets of the list you're trying to
282:07 - sort we're going to need a base case
282:10 - where the recursion stops so it doesn't
282:12 - enter an infinite loop
282:14 - lists that are empty don't need to be
282:16 - sorted and lists with just one element
282:18 - don't need to be sorted either in both
282:20 - cases there's nothing to flip around so
282:23 - we'll make that our base case if there
282:25 - are zero or one elements in the list
282:27 - passed to the quick sort function we'll
282:29 - return the unaltered list to the caller
282:32 - lastly we need to call our quick sort
282:34 - function with our list of numbers and
282:36 - print the list it returns
282:42 - that takes care of our base case now we
282:44 - need a recursive case
282:46 - we're going to rely on a technique
282:48 - that's common in algorithm design called
282:50 - divide and conquer basically we're going
282:52 - to take our problem and split it into
282:54 - smaller and smaller problems until
282:56 - they're easy to solve
282:58 - in this case that means taking our list
283:00 - and splitting it into smaller lists
283:02 - viewers a suggestion the process i'm
283:05 - about to describe is complex there's
283:07 - just no way around it if you're having
283:09 - trouble following along remember the
283:11 - video playback controls feel free to
283:13 - slow the play back down rewind or pause
283:15 - the video as needed after you watch this
283:18 - the first time you may also find it
283:20 - helpful to rewind and make your own
283:21 - diagram of the process as we go
283:24 - okay ready here goes
283:27 - suppose we load the numbers from our
283:29 - 8.txt file into a list how do we divide
283:32 - it
283:33 - it would probably be smart to have our
283:35 - quicksort function divide the list in a
283:37 - way that brings it closer to being
283:39 - sorted let's pick an item from the list
283:42 - we'll just pick the first item for now
283:44 - four
283:45 - we'll call this value we've picked the
283:46 - pivot like the center of a seesaw on a
283:48 - playground
283:50 - we'll break the list into two sublists
283:52 - the first sub-list will contain all the
283:54 - items in the original list that are
283:55 - smaller than the pivot the second
283:57 - sub-list will contain all the items in
283:59 - the original list that are greater than
284:00 - the pivot
284:02 - the sub list of values less than and
284:04 - greater than the pivot aren't sorted
284:06 - but what if they were you could just
284:09 - join the sub lists and the pivot all
284:10 - together into one list and the whole
284:12 - thing would be sorted
284:14 - so how do we sort the sublist we call
284:16 - our quick sort function recursively on
284:18 - them this may seem like magic but it's
284:21 - not it's the divide and conquer
284:23 - algorithm design technique at work
284:25 - if our quick sort function works on the
284:27 - big list then it will work on the
284:29 - smaller list too
284:31 - for our first sub list we take the first
284:33 - item it's the pivot again
284:35 - that's three
284:37 - we break the sub list into two sub lists
284:39 - one with everything less than the pivot
284:41 - and one with everything greater than the
284:42 - pivot
284:43 - notice that there's a value equal to the
284:45 - pivot that gets put into the less than
284:47 - sub-list our finished quicksort function
284:50 - is actually going to put everything
284:51 - that's less than or equal to the pivot
284:53 - in the first sub-list
284:55 - but i don't want to say less than or
284:57 - equal to over and over so i'm just
284:58 - referring to it as the less than pivot
285:01 - sub-list
285:02 - also notice that there are no values
285:04 - greater than the pivot that's okay when
285:06 - we join the sub-lists back together that
285:08 - just means nothing will be in the return
285:10 - list after the pivot
285:12 - we still have one sub list that's more
285:14 - than one element long so we call our
285:16 - quick sort function on that too you and
285:18 - i can see that it's already sorted but
285:20 - the computer doesn't know that so it'll
285:22 - call it anyway just in case
285:24 - it picks the first element 2 as a pivot
285:27 - there are no elements less than the
285:28 - pivot and only one element greater than
285:30 - the pivot
285:32 - that's it for the recursive case we've
285:34 - finally hit the base case for our quick
285:35 - sort function it'll be called on both
285:38 - the empty list of elements less than the
285:39 - pivot and the one item list of elements
285:41 - greater than the pivot but both of these
285:44 - lists will be returned as they are
285:46 - because there's nothing to sort
285:48 - so now at the level of the call stack
285:50 - above this the return sorted lists are
285:52 - used in place of the unsorted sub-list
285:54 - that's less than the pivot and the
285:55 - unsorted sub-list that's greater than
285:57 - the pivot
285:58 - these are joined together into one
286:00 - sorted list remember that any empty
286:02 - lists get discarded
286:04 - then at the level of the call stack
286:06 - above that the return sorted lists are
286:08 - used in place of the unsorted sub-lists
286:10 - there again they were already sorted but
286:12 - the quick sort method was called on them
286:14 - anyway just in case
286:16 - the sub-lists are joined together into
286:18 - one sorted list at the level of the call
286:21 - stack above that the return sorted list
286:23 - is used in place of the unsorted
286:25 - sub-list that's less than the pivot so
286:27 - now everything that's less than or equal
286:28 - to the pivot is sorted
286:30 - now we call quick sort on the unsorted
286:32 - sub-list that's greater than the pivot
286:34 - and the process repeats for that
286:36 - sub-list
286:37 - we pick the first element six is the
286:39 - pivot we split the sub-list into
286:41 - sub-lists of elements that are less than
286:43 - and greater than this pivot and we
286:45 - recursively call the quicksort function
286:47 - until those sub-lists are sorted
286:49 - eventually a sorted sub-list is returned
286:52 - to our first quick sort function call
286:54 - we combine the sub-list that's less than
286:56 - or equal to the pivot the pivot itself
286:59 - and the sub-list that's greater than the
287:00 - pivot into a single list and because we
287:03 - recursively sorted the sub lists the
287:05 - whole list is sorted
287:07 - so that's how the quick sort function is
287:09 - going to work in the next video we'll
287:11 - show you the actual code
287:13 - quicksort works by picking a pivot value
287:15 - then splitting the full list into two
287:17 - sub-lists the first sub-list has all the
287:20 - values less than or equal to the pivot
287:22 - and the second sub-list has all the
287:23 - values greater than the pivot the quick
287:25 - sort function recursively calls itself
287:28 - to sort these sub-lists and then to sort
287:30 - the sub-lists of those sub-lists until
287:32 - the full list is sorted
287:34 - now it's time to actually implement this
287:36 - in code
287:38 - we already have the base case written
287:40 - any list passed in that consists of 0 or
287:42 - 1 values will be returned as is because
287:45 - there's nothing to sort
287:46 - now we need to create a list that will
287:48 - hold all the values less than the pivot
287:50 - that list will be empty at first we do
287:53 - the same for values greater than the
287:55 - pivot
287:56 - next we need to choose the pivot value
287:58 - for now we just grab the first item from
288:00 - the list
288:01 - then we loop through all the items in
288:03 - the list following the pivot
288:06 - we check to see whether the current
288:07 - value is less than or equal to the pivot
288:10 - if it is we copy it to the sub-list of
288:13 - values less than the pivot
288:16 - otherwise the current value must be
288:18 - greater than the pivot
288:20 - so we copy it to the other list
288:26 - this last line is where the recursive
288:28 - magic happens we call quick sort
288:31 - recursively on the sub-list that's less
288:33 - than the pivot we do the same for the
288:35 - sub-list that's greater than the pivot
288:37 - those two calls will return sorted lists
288:40 - so we combine the sort of values less
288:42 - than the pivot the pivot itself and the
288:44 - sort of values greater than the pivot
288:46 - that gives us a complete sorted list
288:48 - which we return
288:50 - this took a lot of prep work are you
288:52 - ready let's try running it python
288:57 - quick sort
288:58 - dot pi
288:59 - numbers 8.text
289:02 - it outputs our sorted list
289:04 - i don't know about you but this whole
289:06 - thing still seems a little too magical
289:08 - to me let's add a couple print
289:09 - statements to the program so we can see
289:11 - what it's doing
289:13 - first we'll add a print statement right
289:14 - before the first call to the quick sort
289:16 - function so we can see the unsorted list
289:19 - we'll also add a print right within the
289:21 - quick sort function right before the
289:23 - recursive calls again this string
289:25 - formatting code is just to keep the info
289:27 - aligned in columns
289:38 - let's try running this again
289:40 - and now you can see our new debug output
289:43 - each time quicksort goes to call itself
289:45 - recursively it prints out the pivot as
289:47 - well as the sub list of items less than
289:49 - or equal to the pivot if any and the sub
289:52 - list of items greater than the pivot if
289:54 - any you can see that first it sorts the
289:56 - sub list of items less than the pivot at
289:58 - the top level
290:00 - it goes through a couple levels of
290:02 - recursion to do that
290:04 - there are actually additional levels of
290:06 - recursion but they're from calls to
290:08 - quick sort with a list of 0 or 1
290:10 - elements and those calls return before
290:12 - the print statement is reached
290:14 - then it starts sorting the second sub
290:16 - list from the top level with items
290:18 - greater than the original pivot
290:20 - you can see a couple levels of recursion
290:22 - for that sort as well
290:24 - finally when both sublists are
290:26 - recursively sorted the original call to
290:28 - the quicksort function returns and we
290:30 - get the sorted list back
290:32 - so we know that it works the next
290:34 - question is how well does it work let's
290:36 - go back to our file of ten thousand
290:38 - numbers and see if it can sort those
290:41 - first though i'm going to remove our two
290:42 - debug calls to print so it doesn't
290:44 - produce unreadable output
290:47 - a quick note if you try running this on
290:49 - a file with a lot of repeated values
290:51 - it's possible you'll get a runtime error
290:53 - maximum recursion depth exceeded
290:56 - if you do see the teacher's notes for a
290:58 - possible solution
291:01 - now let's try running our quick sort
291:02 - program against the ten thousand dot
291:04 - text file python
291:06 - quick sort dot pi
291:08 - numbers 10 000 dot text
291:12 - there we go and it seems pretty fast but
291:15 - how fast exactly let's run it with the
291:17 - time command to see how long it takes
291:19 - time python
291:22 - quick sort dot pi
291:24 - numbers 10 000.text
291:28 - remember we need to ignore the real
291:30 - result and add the user and sys results
291:33 - it took less than a second of cpu time
291:36 - to sort 10 000 numbers with quicksort
291:39 - remember that selection sort took about
291:41 - 13 seconds
291:42 - that's a pretty substantial improvement
291:45 - and with a million numbers selection
291:47 - sort took so long that it never even
291:49 - finished successfully let's see if
291:51 - quicksort performs any better
291:54 - time python quick sort dot pi
291:58 - numbers
292:00 - 1 million dot text
292:08 - not only did quicksort sort a million
292:10 - numbers successfully it only took about
292:12 - 11 seconds of cpu time
292:15 - quicksort is clearly much much faster
292:17 - than selection sort how much faster
292:20 - that's something we'll discuss in a
292:21 - later video
292:23 - what we've shown you here is just one
292:24 - way to implement quicksort
292:26 - although the basic algorithm is always
292:28 - the same the details can vary like how
292:31 - you pick the pivot see the teacher's
292:33 - notes for more details
292:35 - let's review another sorting algorithm
292:38 - merge sort so that we can compare it
292:39 - with quick sort merge sort is already
292:42 - covered elsewhere on the site so we
292:44 - won't go into as much detail about it
292:46 - but we'll have more info in the
292:47 - teacher's notes if you want it
292:49 - both quicksort and merge sword are
292:51 - recursive the difference between them is
292:54 - in the sorting mechanism itself whereas
292:56 - quicksort sorts a list into two
292:58 - sub-lists that are less than or greater
293:00 - than a pivot value
293:02 - merge sort simply splits the list in
293:04 - half recursively and then sorts the
293:06 - halves as it merges them back together
293:08 - that's why it's called merge sort
293:11 - you may recognize this code at the top
293:13 - by now it just loads a file full of
293:15 - numbers into a list
293:17 - let's define a recursive merge sort
293:19 - function as usual it'll take the list or
293:22 - sub-list that we want it to sort
293:24 - our base case is the same as with
293:26 - quicksort if the list has zero or one
293:28 - values there's nothing to sort so we
293:30 - return it as is
293:32 - if we didn't return it means we're in
293:34 - the recursive case so first we need to
293:36 - split the list in half we need to know
293:39 - the index we should split on so we get
293:41 - the length of the list and divide it by
293:43 - two so for example if there are eight
293:45 - items in the list we'll want an index of
293:47 - four
293:48 - but what if there were an odd number of
293:50 - items in the list like seven we can't
293:52 - have an index of 3.5 so we'll need to
293:54 - round down in that case since we're
293:56 - working in python currently we can take
293:59 - advantage of a special python operator
294:01 - that divides and rounds the result down
294:03 - the floor division operator it consists
294:06 - of a double slash
294:09 - now we'll use the python slice syntax to
294:11 - get the left half of the list
294:13 - we'll pass that list to a recursive call
294:15 - to the merge sort function
294:18 - we'll also use slice syntax to get the
294:20 - right half of the list and pass that to
294:22 - merge sort as well
294:26 - now we need to merge the two halves
294:27 - together and sort them as we do it we'll
294:30 - create a list to hold the sorted values
294:33 - and now we get to the complicated part
294:35 - merging the two halves together and
294:37 - sorted them as we do it
294:39 - we'll be moving from left to right
294:40 - through the left half of the list
294:42 - copying values over to the sorted values
294:44 - list as we go this left index variable
294:47 - will help us keep track of our position
294:50 - at the same time we'll also be moving
294:52 - from left to right through the right
294:53 - half of the list and copying values over
294:56 - so we need a separate write index
294:57 - variable to track that position as well
295:00 - we'll keep looping until we've processed
295:02 - all of the values in both halves of the
295:04 - list
295:13 - we're looking to copy over the lowest
295:15 - values first so first we test whether
295:17 - the current value on the left side is
295:20 - less than the value on the right side
295:23 - if the left side value is less that's
295:26 - what we'll copy over to the sorted list
295:32 - and then we'll move to the next value in
295:34 - the left half of the list
295:36 - otherwise the current value from the
295:38 - right half must have been lower
295:40 - so we'll copy that value to the sorted
295:42 - list instead
295:49 - and then we'll move to the next value in
295:50 - the right half of the list
295:53 - that ends the loop at this point one of
295:55 - the two unsorted halves still has a
295:57 - value remaining and the other is empty
295:59 - we won't waste time checking which is
296:01 - which we'll just copy the remainder of
296:03 - both lists over to the sorted list the
296:05 - one with the value left will add that
296:07 - value and the empty one will add nothing
296:10 - all the numbers from both halves should
296:11 - now be copied to the sorted list so we
296:13 - can return it
296:15 - finally we need to kick the whole
296:16 - process off we'll call the merge sort
296:18 - function with the list of numbers we
296:20 - loaded and print the result
296:30 - let's save this
296:36 - and we'll try it out on our file with
296:38 - eight numbers
296:39 - python merge sort dot pi
296:42 - numbers
296:43 - eight dot text
296:45 - and it prints out the sorted list
296:47 - but again this seems pretty magical
296:50 - let's add some print statements to get
296:51 - some insight into what it's doing
296:55 - first we'll print the unsorted list so
296:57 - we can refer to it we'll add a print
296:59 - statement right before we call the merge
297:01 - sort function for the first time
297:06 - then we'll add another print statement
297:07 - within the merge sort function right
297:09 - after the recursive calls this will show
297:12 - us the sorted left half and right half
297:13 - that it's returning again don't worry
297:15 - about the fancy python formatting string
297:18 - it just keeps the values neatly aligned
297:20 - let me resize my console
297:23 - clear the screen
297:24 - and then we'll try running this again
297:28 - what we're seeing are the values being
297:30 - returned from the recursive merge sort
297:32 - function calls not the original calls to
297:34 - merge sort so what you see here is after
297:37 - we reach the base case with a list
297:38 - that's only one item in length and the
297:40 - recursive calls start returning
297:43 - the original list gets split into two
297:45 - unsorted halves four six three and two
297:48 - and nine seven three and five
297:51 - the first half gets split in half again
297:53 - four and six and three and two
297:57 - and each of those halves is halved again
297:59 - into single element lists
298:01 - there's nothing to sort in the single
298:03 - element list so they're returned from
298:05 - the merge sort function as is
298:07 - those single element lists get merged
298:09 - into two sub lists and sorted as they do
298:11 - so the four and six sub-list looks the
298:14 - same after sorting as it did before
298:16 - sorting but the three and the two get
298:18 - sorted as they're combined into a
298:19 - sub-list the new order is two three
298:23 - the order is shifted again when those
298:24 - two sub-lists get combined back into a
298:27 - single list two three four six
298:30 - then we recursively sort the right half
298:32 - of the original list
298:34 - nine seven three five
298:36 - it gets split in half again nine seven
298:39 - and three five
298:41 - and each of those halves get broken into
298:43 - single element lists
298:45 - there's nothing to sort there so the
298:46 - single element lists are returned as is
298:50 - the first two are sorted as they're
298:51 - merged seven nine and so are the second
298:54 - three five
298:56 - and then those two sub lists get sorted
298:58 - as they're combined into another sub
299:00 - list three five seven nine
299:04 - and finally everything is sorted as it's
299:06 - merged back into the full sorted list
299:09 - two three three four five six seven nine
299:13 - that's how merge sort works on a list of
299:15 - eight numbers let's see if it works on a
299:16 - bigger list
299:19 - first i'll remove the two print
299:21 - statements so we don't get an
299:22 - overwhelming amount of debug output
299:28 - then i'll run it on a list of ten
299:30 - thousand items python merge sort dot pi
299:33 - numbers ten thousand dot
299:36 - text
299:38 - not only did it work it was pretty fast
299:41 - but which is faster merge sort or quick
299:43 - sort we'll look at that next
299:46 - i've removed the call to print that
299:47 - displays the sorted list at the end of
299:49 - our selection sort quick sort and merge
299:51 - sort scripts
299:53 - that way it'll still run the sort but
299:55 - the output won't get in the way of our
299:56 - comparing runtimes
299:59 - let's try running each of these scripts
300:01 - and see how long it takes
300:04 - time python
300:07 - selection sort we'll do that one first
300:09 - numbers
300:10 - 10 000 dot text
300:16 - we combine the user and sys results and
300:19 - that gives us about six seconds
300:21 - now let's try quick sort time python
300:24 - quick sort
300:26 - dot pi numbers
300:28 - ten thousand dot text
300:32 - much faster less than a second and
300:34 - finally time python
300:37 - merge sort dot pi numbers ten thousand
300:41 - dot text
300:44 - a little longer but far less than a
300:46 - second so even on a list with just 10
300:49 - 000 numbers selection sort takes many
300:52 - times as long as quicksort and merge
300:54 - sort
300:55 - and remember i ran the selection sort
300:57 - script on a file with a million numbers
300:59 - and it took so long that my workspace
301:01 - timed out before it completed
301:04 - it looks like selection sort is out of
301:06 - the running as a viable sorting
301:07 - algorithm it may be easy to understand
301:10 - and implement but it's just too slow to
301:12 - handle the huge data sets that are out
301:14 - in the real world
301:17 - now let's try quicksort and merge sort
301:18 - on our file with a million numbers and
301:20 - see how they compare there time python
301:25 - quicksort dot pi
301:27 - numbers
301:29 - million
301:30 - dot text
301:35 - looks like it took about 11 seconds of
301:37 - cpu time
301:38 - now let's try merge sort time python
301:42 - merge sort dot pi
301:44 - numbers
301:45 - 1 million
301:47 - dot text
301:51 - that took about 15 seconds of cpu time
301:54 - it looks like quicksort is marginally
301:56 - faster than merge sort on this sample
301:58 - data
301:59 - we had to learn a lot of details for
302:01 - each algorithm we've covered in this
302:03 - course developers who need to implement
302:05 - their own algorithms often need to
302:07 - choose an algorithm for each and every
302:08 - problem they need to solve and they
302:11 - often need to discuss their decisions
302:12 - with other developers can you imagine
302:15 - needing to describe all the algorithms
302:17 - in this same level of detail all the
302:19 - time you'd spend all your time in
302:21 - meetings rather than programming
302:23 - that's why big o notation was created as
302:26 - a way of quickly describing how an
302:28 - algorithm performs as the data set it's
302:30 - working on increases in size
302:32 - big o notation lets you quickly compare
302:34 - several algorithms to choose the best
302:36 - one for your problem
302:38 - the algorithms we've discussed in this
302:40 - course are very well known some job
302:42 - interviewers are going to expect you to
302:44 - know their big o run times so let's look
302:46 - at them
302:47 - remember that the n in big o notation
302:50 - refers to the number of elements you're
302:51 - operating on with selection sort you
302:54 - need to check each item in the list to
302:56 - see if it's the lowest so you can move
302:57 - it over to the sorted list so that's in
303:00 - operations
303:02 - suppose you're doing selection sort on a
303:04 - list of five items and in this case
303:06 - would be five so that's five operations
303:09 - before you can move an item to the
303:10 - sorted list
303:11 - but with selection sort you have to loop
303:14 - over the entire list for each item you
303:15 - want to move there are five items in the
303:18 - list and you have to do five comparisons
303:20 - to move each one so it's more like 5
303:22 - times 5 operations or if we replace 5
303:25 - with n it's n times n or n squared
303:29 - but wait you might say half of that 5 by
303:31 - 5 grid of operations is missing because
303:34 - we're testing one fewer item in the
303:35 - unsorted list with each pass so isn't it
303:38 - more like one half times n times n
303:41 - and this is true we're not doing a full
303:43 - n squared operations
303:46 - but remember in big o notation as the
303:48 - value of n gets really big constants
303:50 - like one half become insignificant and
303:53 - so we discard them
303:55 - the big o runtime of selection sword is
303:57 - widely recognized as being o n squared
304:01 - quicksort requires one operation for
304:03 - each element of the list it's sorting
304:06 - it needs to select a pivot first and
304:07 - then it needs to sort elements into
304:09 - lists that are less than or greater than
304:11 - the pivot
304:12 - so that's n operations to put that
304:14 - another way if you have a list of eight
304:16 - items then n is eight so it will take
304:18 - eight operations to split the list
304:20 - around the pivot
304:22 - but of course the list isn't sorted
304:24 - after splitting it around the pivot just
304:25 - once you have to repeat those eight
304:27 - operations several times in the best
304:30 - case you'll pick a pivot that's right in
304:31 - the middle of the list so that you're
304:33 - dividing the list exactly in half
304:35 - then you keep dividing the list in half
304:37 - until you have a list with a length of
304:39 - one
304:40 - the number of times you need to divide n
304:42 - in half until you reach one is expressed
304:44 - as log n
304:47 - so you need to repeat n sorting
304:48 - operations log n times that leaves us
304:51 - with the best case run time for quick
304:53 - sort of o n log n
304:56 - but that's the best case what about the
304:59 - worst case well if you pick the wrong
305:01 - pivot you won't be dividing the list
305:03 - exactly in half if you pick a really bad
305:05 - pivot the next recursive call to
305:07 - quicksort will only reduce the list
305:08 - length by one
305:10 - since our quicksort function simply
305:12 - picks the first item to use as a pivot
305:14 - we can make it pick the worst possible
305:16 - pivot repeatedly simply by giving it a
305:18 - list that's sorted in reverse order
305:20 - if we pick the worst possible pivot
305:22 - every time we'll have to split the list
305:24 - once for every item it contains and then
305:27 - do end sorting operations on it
305:30 - you already know another sorting
305:31 - algorithm that only manages to reduce
305:33 - the list by one element with each pass
305:35 - selection sort
305:37 - selection sort has a runtime of o n
305:39 - squared and in the worst case that's the
305:41 - run time for quicksort as well
305:43 - so which do we consider when trying to
305:45 - decide whether to use quicksort the best
305:47 - case or the worst case
305:49 - well as long as your implementation
305:51 - doesn't just pick the first item as a
305:53 - pivot which we did so we could
305:55 - demonstrate this issue
305:56 - it turns out that on average quicksort
305:58 - performs closer to the best case
306:01 - many quicksort implementations
306:03 - accomplish this simply by picking a
306:04 - pivot at random on each recursive loop
306:07 - here we are sorting our reverse sorted
306:09 - data again but this time we pick pivots
306:12 - at random which reduces the number of
306:13 - recursive operations needed
306:16 - sure random pivots sometimes give you
306:18 - the best case and sometimes you'll
306:19 - randomly get the worst case but it all
306:21 - averages out over multiple calls to the
306:23 - quick sort function
306:25 - now with merge sort there's no pivot to
306:27 - pick your list of n items always gets
306:30 - divided in half log n times
306:32 - that means merged sort always has a big
306:35 - o runtime of o and log in
306:38 - contrast that with quicksort which only
306:40 - has a runtime of o and log n in the best
306:42 - case in the worst case quick sorts
306:44 - runtime is o n squared
306:46 - and yet out in the real world quicksort
306:49 - is more commonly used than merge sort
306:51 - now why is that if quicksort's big o
306:53 - runtime can sometimes be worse than
306:55 - merge sorts
306:56 - this is one of those situations where
306:58 - big o notation doesn't tell you the
307:00 - whole story all big o can tell you is
307:02 - the number of times an operation is
307:04 - performed it doesn't describe how long
307:06 - that operation takes
307:08 - and the operation mergesor performs
307:10 - repeatedly takes longer than the
307:12 - operation quicksort performs repeatedly
307:15 - big-o is a useful tool for quickly
307:17 - describing how the runtime of an
307:18 - algorithm increases is the data set it's
307:20 - operating on gets really really big
307:23 - but you can't always choose between two
307:24 - algorithms based just on their big o
307:26 - runtimes sometimes there's additional
307:29 - info you need to know about an algorithm
307:31 - to make a good decision
307:33 - now that we can sort a list of items
307:34 - we're well on our way to being able to
307:36 - search a list efficiently as well we'll
307:38 - look at how to do that in the next stage
307:42 - [Music]
307:46 - now that we've covered sorting
307:48 - algorithms the groundwork has been laid
307:50 - to talk about searching algorithms
307:52 - if you need to search through an
307:53 - unsorted list of items binary search
307:56 - isn't an option because you have no idea
307:58 - which half of the list contains the item
308:00 - you're looking for your only real option
308:03 - is to start at the beginning and compare
308:05 - each item in the list to your target
308:06 - value one at a time until you find the
308:09 - value you're looking for
308:10 - this algorithm is called linear search
308:13 - or sequential search because the search
308:15 - proceeds in a straight line or sequence
308:18 - even though linear search is inefficient
308:20 - searching for just one name will happen
308:22 - so fast that we won't be able to tell
308:24 - anything useful about the algorithm's
308:26 - runtime so let's suppose we had a
308:28 - hundred different names and that we
308:29 - needed to know where they appear in a
308:30 - list of unsorted names
308:32 - here's some code that demonstrates
308:35 - as usual this code at the top isn't
308:37 - relevant to the search algorithm it's
308:39 - just like the code that loaded a list of
308:41 - numbers from a file in the previous
308:42 - stage but this code calls a different
308:44 - function load strings that loads a list
308:47 - of strings in
308:48 - if you want the load strings python code
308:50 - we'll have it for you in the teacher's
308:52 - notes
308:53 - here's a separate hard-coded list
308:55 - containing the 100 names we're going to
308:57 - search for we'll loop through each name
308:59 - in this list and pass it to our search
309:01 - function to get the index within the
309:03 - full list where it appears
309:05 - now let's implement the search function
309:07 - compared to the sorting algorithms this
309:09 - is going to be short the index of item
309:12 - function takes the python list you want
309:14 - to search through and a single target
309:15 - value you want to search for
309:18 - now we need to loop over each item in
309:20 - the list the range function gives us a
309:22 - range of numbers from its first argument
309:24 - up to but not including its second
309:27 - argument so if our list had a length of
309:29 - 5 this would loop over the indexes 0
309:31 - through 4.
309:33 - we test whether the list item at the
309:34 - current index matches our target
309:37 - if it does then we return the index of
309:40 - the current item this will exit the
309:42 - index of item function without looping
309:44 - over the remaining items in the list
309:46 - if we reach the end of the loop without
309:48 - finding the target value that means it
309:50 - wasn't in the list so instead of
309:52 - returning an index we return the special
309:54 - python value none which indicates the
309:56 - absence of a value
309:58 - other languages have similar values like
310:01 - nil or null but if yours doesn't you
310:03 - might have to return a value that would
310:04 - otherwise be impossible like an index of
310:06 - negative 1.
310:08 - now let's call our new search function
310:10 - we start by looping over the list of 100
310:12 - values we're looking for we're using the
310:14 - values themselves this time not their
310:16 - indexes within the list so there's no
310:18 - need to mess with python's range
310:20 - function here's the actual call to the
310:23 - index of item function we pass it the
310:25 - full list of names that we loaded from
310:27 - the file plus the name we want to search
310:29 - for within that list then we store the
310:31 - index it returns in a variable
310:33 - and lastly we print the index we get
310:35 - back from the index of item function
310:38 - let's save this and go to our console
310:40 - and see if it works
310:42 - python linear search dot pi
310:45 - names
310:48 - unsorted dot text
310:51 - and it'll print out the list of indexes
310:53 - for each name
310:55 - i actually set it up so that the last
310:56 - two items in the list of names we're
310:58 - going to search for corresponded to the
311:00 - first and last name within the file
311:03 - so if we open up our unsorted.txt file
311:06 - we'll see mary rosenberger is the first
311:08 - name and alonso viviano is the last name
311:13 - and those are the last two values in our
311:15 - list of names we're searching for
311:17 - so it returned an index of zero for that
311:19 - second to last name and you can see that
311:21 - name here on line one of the file
311:24 - the line numbering starts at one and the
311:26 - python list indexes start at zero so
311:28 - that makes sense
311:30 - and for the last name it returned an
311:31 - index of 109873
311:36 - and you can see that name here on line
311:38 - 109 874 so we can see that it's
311:41 - returning the correct indexes
311:43 - but right now we're just searching for a
311:44 - hundred different names in a list of one
311:46 - hundred thousand names in the real world
311:49 - we're going to be looking for many more
311:50 - names than that within much bigger lists
311:52 - than that can we do this any faster yes
311:56 - but we'll need to use the binary search
311:58 - algorithm and for that to work we need
312:00 - to sort our list of strings we'll do
312:02 - that in the next video
312:04 - before we can use the binary search
312:06 - algorithm on our list of names we need
312:08 - to sort it let's do that now we need to
312:10 - load our unsorted list of names from a
312:12 - file sorted and write the sorted names
312:14 - back out to a new file
312:16 - again this code at the top just loads a
312:18 - file full of strings into a list
312:21 - we'll use our quick sort method to sort
312:23 - the list of names its code is completely
312:25 - unchanged from when you saw it in the
312:27 - previous stage
312:28 - we just call our quick sort function on
312:30 - the list of names loaded from the file
312:32 - and save the list to a variable
312:36 - then we loop through each name in the
312:37 - sorted list
312:40 - and we print that name
312:43 - that's all there is to it let's save
312:45 - this script and try running it
312:47 - python
312:49 - quicksort strings stop pi
312:52 - and we'll pass it the
312:54 - names unsorted.text file
312:56 - let me resize the console window here a
312:59 - little bit
313:03 - that prints the sorted list of names out
313:05 - to the terminal but we need it in a file
313:08 - so we'll do what's called a redirect of
313:10 - the program's output we'll run the same
313:12 - command as before but at the end we'll
313:14 - put a greater than sign followed by the
313:16 - path to a file that we want the program
313:18 - output written to names
313:21 - sorted dot text
313:26 - redirecting works not only on linux
313:28 - based systems like workspaces but also
313:30 - on macs and even on windows machines you
313:33 - just need to be careful because if you
313:34 - redirect to an existing file its
313:36 - contents will be overwritten without
313:38 - asking you
313:40 - let me refresh the list of files in the
313:42 - sidebar
313:44 - and you'll see that we now have a new
313:46 - sorted dot text file in the names
313:47 - directory
313:49 - it's the same number of lines as the
313:51 - unsorted dot text file but all the names
313:53 - are sorted now
313:55 - now we can load this file of sorted
313:56 - names into a list and we'll be able to
313:58 - use that list with the binary search
314:00 - algorithm we'll see how to do that next
314:03 - now that we have our list of names
314:04 - sorted we can use the binary search
314:07 - algorithm on it let's see if we can use
314:09 - it to speed up our search for the
314:10 - indexes of 100 names
314:13 - binary search keeps narrowing down the
314:14 - list until it has the value it's looking
314:16 - for it's faster than linear search
314:18 - because it discards half the potential
314:20 - matches each time
314:22 - our code here at the top of our binary
314:24 - search script is unchanged from the
314:26 - previous scripts we just call the load
314:27 - strings function to load our 100 000
314:30 - sorted names from a file
314:32 - here we've hard coded the list of 100
314:34 - names we're going to search for again
314:36 - it's identical to the list from the
314:37 - linear search script except that i've
314:39 - again changed the last two names to
314:41 - correspond to the names on the first and
314:42 - last lines of the file we'll be loading
314:45 - now let's write the function that will
314:46 - implement our binary search algorithm
314:49 - like the linear search function before
314:51 - it'll take two arguments the first is
314:53 - the list we're going to search through
314:55 - and the second is the target value we'll
314:57 - be searching for again the binary search
315:00 - function will return the index it found
315:01 - the value at or the special value none
315:04 - if it wasn't found
315:05 - binary search is faster than a linear
315:07 - search because it discards half the
315:09 - values it has to search through each
315:11 - time to do this it needs to keep track
315:13 - of a range that it still needs to search
315:15 - through
315:16 - to start that range is going to include
315:18 - the full list
315:19 - the first variable will track the lowest
315:22 - index in the range we're searching to
315:24 - start it's going to be 0 the first index
315:26 - in the full list
315:28 - likewise the last variable will track
315:30 - the highest index in the range we're
315:32 - searching to start we'll set it to the
315:34 - highest index in the full list
315:36 - if the first and last variables are
315:38 - equal then it means the size of the
315:40 - search range has shrunk to zero and
315:42 - there is no match until that happens
315:44 - though we'll keep looping to continue
315:46 - the search we want to divide the list of
315:48 - potential matches in half each time to
315:51 - do that we need to check the value
315:52 - that's in the middle of the range we're
315:54 - searching in
315:55 - we add the indexes in the first and last
315:57 - variables then divide by two to get
316:00 - their average we might get a fractional
316:02 - number which can't be used as a list
316:04 - index so we also round down using
316:06 - python's double slash floor division
316:09 - operator
316:10 - all this will give us the index of the
316:12 - list element that's the midpoint of the
316:14 - range we're searching we store that in
316:16 - the midpoint variable
316:18 - whoops looks like my indentation got
316:20 - mixed up there let me fix that real
316:22 - quick there we go now we test whether
316:24 - the list element at the midpoint matches
316:27 - the target value
316:32 - if it does we return the midpoint index
316:34 - without looping any further our search
316:36 - is complete
316:37 - otherwise if the midpoint element's
316:39 - value is less than the target value
316:45 - then we know that our target value can't
316:46 - be at the midpoint or any index prior to
316:49 - that so we move the new start of our
316:51 - search range to just after the old
316:53 - midpoint
316:54 - otherwise the midpoint element's value
316:56 - must have been greater than the target
316:58 - value
316:59 - we know that our target value can't be
317:00 - at the midpoint or any index after that
317:03 - so we move the new end of our search
317:05 - range to just before the old midpoint
317:08 - by unindenting here we mark the end of
317:10 - the loop if the loop completes it means
317:12 - the search range shrank to nothing
317:14 - without our finding a match and that
317:16 - means there's no matching value in the
317:17 - list so we return the special python
317:20 - value none to indicate this
317:23 - lastly just as we did in our linear
317:25 - search script we need to search for each
317:27 - of the 100 names we loop over each name
317:30 - in our hard-coded list
317:32 - and we call the binary search function
317:34 - with the sorted list of names we're
317:35 - going to load from the file and the
317:37 - current name we're searching for
317:39 - we store the returned list index in the
317:41 - index variable
317:43 - and finally we print that variable
317:47 - let's save this and go to our console
317:49 - and try running it
317:51 - python
317:52 - binarysearch.pi
317:54 - and it's important to give it the name
317:55 - of the sorted file if it loads the
317:57 - unsorted file the binary search won't
317:59 - work so names
318:01 - sorted dot text
318:05 - again it prints out the list of indexes
318:07 - for each name
318:08 - i once again set it up so the last two
318:10 - items in the list of names we're going
318:12 - to search for corresponded to the first
318:14 - and last name in the file
318:16 - so it returned an index of zero for the
318:18 - second to last name
318:21 - and you can see that name
318:27 - here's the second to last name aaron
318:29 - augustine
318:32 - you can see that name here on line one
318:34 - of the file
318:35 - and for the last name it returned an
318:37 - index of one zero nine eight seven three
318:40 - and you can see that name here on line
318:42 - one zero nine eight seven four
318:49 - let's check the third to last name for
318:50 - good measure it looks like an index of
318:54 - 97022 was printed for that name stephen
318:57 - daras
319:00 - let's search for steve and daras within
319:03 - the file
319:05 - and here it is on line 97023
319:09 - remember that line numbers start on one
319:11 - instead of zero so this actually matches
319:13 - up with the printed list index of 97022
319:17 - it looks like our binary search script
319:19 - is working correctly
319:21 - let's try our linear search and binary
319:23 - search scripts out with the time command
319:25 - and see how they compare i've commented
319:27 - out the lines that print the indexes of
319:29 - matches in the two scripts
319:32 - that way they'll still call their
319:33 - respective search functions what the 100
319:36 - names we're searching for but they won't
319:37 - actually print the indexes out so we
319:39 - won't have a bunch of output obscuring
319:41 - the results of the time command
319:44 - first let's try the linear search script
319:47 - time python
319:49 - linear search dot pi
319:51 - names
319:53 - and we can just use the unsorted list of
319:54 - names for linear search
319:59 - remember we want to ignore the real
320:01 - result and add the user and sys results
320:03 - together
320:04 - it looks like it took about .9 seconds
320:07 - for linear search to find the 100 names
320:09 - in the list of one hundred thousand
320:11 - now let's try timing the binary search
320:13 - script time
320:15 - python
320:16 - binarysearch.pi
320:18 - names and for this one we need to use
320:21 - the sorted list of names
320:25 - looks like that took around a quarter
320:27 - second so less than half as long
320:29 - bear in mind that part of this time is
320:31 - spent loading the file of names into a
320:32 - list the difference between linear
320:34 - search and binary search will be even
320:36 - more pronounced as you search through
320:38 - bigger lists or search for more items
320:41 - let's wrap up the course by looking at
320:43 - the big o runtimes for linear search and
320:45 - binary search these are going to be much
320:47 - simpler to calculate than the sorting
320:49 - algorithms were
320:50 - for linear search you need to do one
320:52 - comparison to the target value for each
320:55 - item in the list again theoretically we
320:57 - could find the target value before
320:59 - searching the whole list but big o
321:01 - notation is only concerned with the
321:02 - worst case where we have to search the
321:04 - entire list so for a list of eight items
321:07 - that means eight operations
321:10 - the big o runtime for linear search is o
321:13 - n where n is the number of items we're
321:15 - searching through
321:16 - this is also known as linear time
321:18 - because when the number of items and
321:20 - number of operations are compared on a
321:22 - graph the result is a straight line
321:25 - linear search looks pretty good until
321:27 - you compare it to binary search for
321:29 - binary search the number of items you
321:31 - have to search through and therefore the
321:32 - number of operations is cut in half with
321:35 - each comparison
321:36 - remember the number of times you can
321:38 - divide n by two until you reach one is
321:40 - expressed as log n so the run time of
321:43 - binary search in big o notation is o log
321:46 - n
321:47 - even for very large values of n that is
321:50 - very large lists you have to search
321:51 - through the number of operations needed
321:53 - to search is very small binary search is
321:56 - a very fast efficient algorithm
321:59 - that's our tour of sorting and searching
322:01 - algorithms be sure to check the
322:03 - teacher's notes for opportunities to
322:05 - learn more thanks for watching

Cleaned transcript:

this is a fulllength course from treehouse we at free code camp are longtime fans of their learning platform they were kind enough to let our nonprofit make this course freely available on our youtube channel if you like this course treehouse has a lot more courses like this one the link is in the description along with time codes to the different sections in this course hi my name is passan i'm an instructor here at treehouse and welcome to introduction to algorithms whether you are a high school or college student a developer in the industry or someone who is learning to code you have undoubtedly run into the term algorithm for many people this word is kind of scary it represents this body of knowledge that seems just out of reach only people with computer science degrees know about algorithms now to others this brings up feelings of imposter syndrome you might already know how to code but you're not a real developer because you don't know anything about algorithms personally it made me frame certain jobs as above my skill level because the interview contained algorithm questions well whatever your reasons are in this course our goal is to dispel all those feelings and get you comfortable with the basics of algorithms like any other subject i like to start my courses with what the course is and is not in this course we're going to cover the very basic set of knowledge that you need as a foundation for learning about algorithms this course is less about specific algorithms and more about the tools you will need to evaluate algorithms understand how they perform compare them to each other and make a statement about the utility of an algorithm in a given context now don't worry none of this will be theoretical and we will learn these concepts by using wellknown algorithms in this course we will also be writing code so i do expect you to have some programming experience if you intend to continue with this topic you can definitely stick around even if you don't know how to code but you might want to learn the basics of programming in the meantime in this course we will be using the python programming language python reads a lot like regular english and is the language you will most likely encounter when learning about algorithms these days if you don't know how to code or if you know how to code in a different language check out the notes section of this video for links to other content that might be useful to you as long as you understand the fundamentals of programming you should be able to follow along pretty well if you're a javascript developer or a student who's learning javascript for example chances are good that you'll still be able to understand the code we write later i'll be sure to provide links along the way if you need anything to follow up on let's start with something simple what is an algorithm an algorithm is a set of steps or instructions for completing a task this might sound like an over simplification but really that's precisely what an algorithm is a recipe is an algorithm your morning routine when you wake up is an algorithm and the driving directions you follow to get to a destination is also an algorithm in computer science the term algorithm more specifically means the set of steps a program takes to finish a task if you've written code before any code really generally speaking you have written an algorithm given that much of the code we write can be considered an algorithm what do people mean when they say you should know about algorithms now consider this let's say i'm a teacher in a classroom and i tell everyone i have an assignment for them on their desks they have a picture of a maze and their task is to come up with a way to find the quickest way out of the maze everyone does their thing and comes up with a solution every single one of these solutions is a viable solution and is a valid example of an algorithm the steps one needs to take to get out of the maze but from being in classrooms or any group of any sort you know that some people will have better ideas than others we all have a diverse array of skill sets over time our class picks the best of these solutions and any time we want to solve a maze we go with one of these solutions this is what the field of algorithms is about there are many problems in computer science but some of them are pretty common regardless of what project you're working on different people have come up with different solutions to these common problems and over time the field of computer science has identified several that do the job well for a given task when we talk of algorithms we're referring to two points we're primarily saying there's an established body of knowledge on how to solve particular problems well and it's important to know what the solutions are now why is it important if you're unaware that a solution exists you might try to come up with one yourself and there's a likelihood that your solution won't be as good or efficient whatever that means compared to those that have been thoroughly reviewed but there's a second component to it as well part of understanding algorithms is not just knowing that an algorithm exists but understanding when to apply it understanding when to apply an algorithm requires properly understanding the problem at hand and this arguably is the most important part of learning about algorithms and data structures as you progress through this content you should be able to look at a problem and break it down into distinct steps when you have a set of steps you should then be able to identify which algorithm or data structure is best for the task at hand this concept is called algorithmic thinking and it's something we're going to try and cultivate together as we work through our content lastly learning about algorithms gives you a deeper understanding about complexity and efficiency in programming having a better sense of how your code will perform in different situations is something that you'll always want to develop in hone algorithmic thinking is why algorithms also come up in big tech interviews interviewers don't care as much that you are able to write a specific algorithm in code but more about the fact that you can break a seemingly insurmountable problem into distinct components and identify the right tools to solve each distinct component and that is what we plan on doing in this course though we're going to focus on some of the tools and concepts you'll need to be aware of before we can dive into the topic of algorithms if you're ready let's get started hey again in this video we're going to do something unusual we're going to play a game and by we i mean me and my two friends here brittany and john this game is really simple and you may have played it before it goes something like this i'm going to think of a number between 1 and 10 and they have to guess what the number is easy right when they guess a number i'll tell them if their guess is too high or too low the winner is the one with the fewest tries all right john let's start with you i'm thinking of a number between one and ten what is it between you and me the answer is three uh quick question does the range include one and ten that is a really good question so what john did right there was to establish the bounds of our problem no solution works on every problem and an important part of algorithmic thinking is to clearly define what the problem set is and clarify what values count as inputs yeah 1 and ten are both included is it one too low is it two too low is it three correct okay so that was an easy one it took john three tries to get the answer let's switch over to brittany and play another round using the same number as the answer okay brittany i'm thinking of a number between 1 and 10 inclusive so both 1 and 10 are in the range what number am i thinking of is it 5 too high 2 too low is it 3 correct all right so what we had there was two very different ways of playing the same game somehow with even such a simple game we saw different approaches to figuring out a solution to go back to algorithmic thinking for a second this means that with any given problem there's no one best solution instead what we should try and figure out is what solution works better for the current problem in this first pass at the game they both took the same amount of turns to find the answer so it's not obvious who has the better approach and that's mostly because the game was easy let's try this one more time now this time the answer is 10. all right john you first is it one too low is it two still too low is it three too low is it four too low is it five still too low is it six too low is it seven too low is it eight low is it nine do low is it ten correct you got it okay so now same thing but with britney this time is it five too low eight too low is it nine still too low it's ten all right so here we start to see a difference between their strategies when the answer was three they both took the same number of turns this is important when the number was larger but not that much larger 10 in this case we start to see that britney strategy did better she took four tries while john took 10. we've played two rounds so far and we've seen a different set of results based on the number they were looking for if you look at john's way of doing things then the answer being 10 the round we just played is his worst case scenario he will take the maximum number of turns 10 to guess it when we picked a random number like three it was hard to differentiate which strategy was better because they both performed exactly the same but in john's worst case scenario a clear winner in terms of strategy emerges in terms of algorithmic thinking we're starting to get a sense that the specific value they're searching for may not matter as much as where that value lies in the range that they've been given identifying this helps us understand our problem better let's do this again for a range of numbers from one to one hundred we'll start by picking five as an answer to trick them okay so this time we're going to run through the exercise again this time from one to one hundred and both one and one hundred are included is it one at this point without even having to run through it we can guess how many tries john is going to take since he starts at one and keeps going he's going to take five tries as we're about to see is it five cool correct okay now for brittany's turn is it 50 too high is it 25 still too high is it 13 too high is it seven too high is it four too low is it six too high is it five correct let's evaluate john took five tries brittany on the other hand takes seven tries so john wins this round but again in determining whose strategy is preferred there's no clear winner right now what this tells us is that it's not particularly useful to look at the easy answers where we arrive at the number fairly quickly because it's at the start of the range instead let's try one where we know john is going to do poorly let's look at his worst case scenario where the answer is 100 and see how britney performs in such a scenario okay john let's do this one more time one through 100 again is it one we can fast forward this scene because well we know what happens john takes 100 tries hi brittany you're up is it 50 too low is it 75 too low 88 too low 94 too low is it 97 too low 99 too low 100. okay so that took brittney seven turns again and this time she is the clear winner if you compare their individual performances for the same number set you'll see that britney's approach leaves john's in the dust when the answer was five so right around the start of the range john took five turns but when the answer was 100 right at the end of the range he took 100 tries it took him 20 times the amount of tries to get that answer compared to britney on the other hand if you compare britney's efforts when the number was 5 she took seven tries but when the number was 100 she took the same amount of tries this is pretty impressive if we pretend that the number of tries is the number of seconds it takes britney and john to run through their attempts this is a good estimate for how fast their solutions are ok we've done this a couple times and brittany and john are getting tired let's take a break in the next video we'll talk about the point of this exercise in the last video we ran through an exercise where i had some of my coworkers guess what number i was thinking so was the point of that exercise you might be thinking hey i thought i was here to learn about algorithms the exercise we just did was an example of a real life situation you will run into when building websites apps and writing code both approaches taken by john and brittany to find the number i was thinking of are examples of searching for a value it might be weird to think that there's more than one way to search but as you saw in the game the speed at which the result was obtained differed between john and brittany think about this problem from the perspective of a company like facebook at the time of this recording facebook has 2.19 billion active users let's say you're traveling in a different country and meet someone you want to add on facebook you go into the search bar and type out this person's name if we simplify how the facebook app works it has to search across these 2.19 billion records and find the person you are looking for the speed at which you find this person really matters imagine what kind of experience it would be if when you search for a friend facebook put up a spinning activity indicator and said come back in a couple hours i don't think we'd use facebook as much if that was the case from the company's perspective working on making search as fast as possible using different strategies really matters now i said that the two strategies britney and john used were examples of search more specifically these are search algorithms the strategy john took where he started at the beginning of the range and just counted one number after the other is a type of search called linear search it is also called sequential search which is a better description of how it works or even simple search since it really is quite simple but what makes his approach an algorithm as opposed to just looking for something remember we said that an algorithm is a set of steps or instructions to complete a task linear search is a search algorithm and we can define it like this we start at the beginning of the list or the range of values then we compare the current value to the target if the current value is the target value that we're looking for we're done if it's not we'll move on sequentially to the next value in the list and then repeat step 2. if we reach the end of the list then the target value is not in the list this definition has nothing to do with programming and in fact you can use it in the real world for example i could tell you to walk into a bookstore and find me a particular book and one of the ways you could do it is using the linear search algorithm you could start at the front of the bookstore and read the cover or the spine of every book to check that it matches the book that you're looking for if it doesn't you go to the next book and repeat until you find it or run out of books what makes this an algorithm is the specificity of how it is defined in contrast to just jumping into a problem and solving it as we go along an algorithm follows a certain set of guidelines and we use the same steps to solve the problem each time we face it an important first step to defining the algorithm isn't the algorithm itself but the problem we're trying to solve our first guideline is that an algorithm must have a clear problem statement it's pretty hard to define an instruction set when you don't have a clear idea of what problem you're trying to solve in defining the problem we need to specify how the input is defined and what the output looks like when the algorithm has done its job for linear search the input can be generally described as a series of values and the output is a value matching the one we're looking for right now we're trying to stay away from anything code related so this problem statement definition is pretty generic but once we get to code we can actually tighten this up once we have a problem an algorithm is a set of steps that solves this problem given that the next guideline is that an algorithm definition must contain a specific set of instructions in a particular order we really need to be clear about the order in which these instructions are executed taking our simple definition of linear search if i switched up the order and said move sequentially to the next value before specifying that first comparison step if the first value were the target one our algorithm wouldn't find it because we moved to the second value before comparing now you might think okay that's just an avoidable mistake and kind of common sense the thing is computers don't know any of that and just do exactly as we tell them so specific order is really important the third guideline is that each step in our algorithm definition must not be a complex one and needs to be explicitly clear what i mean by that is that you shouldn't be able to break down any of the steps into further into additional subtasks each step needs to be a distinct one we can't define linear search as search until you find this value because that can be interpreted in many ways and further broken down into many more steps it's not clear next and this one might seem obvious but algorithms should produce a result if it didn't how would we know whether the algorithm works or not to be able to verify that our algorithm works correctly we need a result now when using a search algorithm the end result can actually be nothing which indicates that the value wasn't found but that's perfectly fine there are several ways to represent nothing in code and as long as the algorithm can produce some results we can understand its behavior the last guideline is that the algorithm should actually complete and cannot take an infinite amount of time if we let john loose in the world's largest library and asked him to find a novel we have no way of knowing whether he succeeded or not unless he came back to us with a result okay so quick recap what makes an algorithm an algorithm and not just something you do one it needs to have a clearly defined problem statement input and output when using linear search the input needs to be just a series of values but to actually use brittany's strategy there's one additional precondition so to speak if you think about her strategy it required that the numbers be sorted in ascending order this means that where the input for john is just a series of values to solve the problem the input to brittany's algorithm needs to be a sorted series of values so clearly defined problem statement clearly defined input and clearly defined output second the steps in the algorithm need to be in a very specific order the steps also need to be distinct you should not be able to break it down into further subtasks next the algorithm should produce a result and finally the algorithm should complete in a finite amount of time these guidelines not only help us define what an algorithm is but also helps us verify that the algorithm is correct executing the steps in an algorithm for a given input must result in the same output every time if in the game i played the answer was 50 every time then every single time john must take 50 turns to find out that the answer is 50. if somehow he takes 50 turns in one round then 30 the next and we technically don't have a correct algorithm consistent results for the same set of values is how we know that the algorithm is correct i should stress that we're not going to be designing any algorithms on our own and we'll start off and spend most of our time learning the tried and true algorithms that are known to efficiently solve problems the reason for talking about what makes for a good algorithm though is that the same set of guidelines makes for good algorithmic thinking which is one of the most important skills we want to cultivate when we encounter a problem before rushing in and thinking about solutions what we want to do is work through the guidelines first we break down the problem into any possible number of smaller problems where each problem can be clearly defined in terms of an input and an output now that we know how to generally define an algorithm let's talk about what it means to have a good algorithm an important thing to keep in mind is that there's no one single way to measure whether an algorithm is the right solution because it is all about context earlier we touched on two concepts correctness and efficiency let's define correctness more clearly because before we can evaluate an algorithm on efficiency we need to ensure its correctness before we define our algorithms we start by defining our problem in the definition of that problem we have a clearly defined input satisfying any preconditions and a clearly defined output an algorithm is deemed correct if on every run of the algorithm against all possible values in the input data we always get the output we expect part of correctness means that for any possible input the algorithm should always terminate or end if these two are not true then our algorithm isn't correct if you were to pick up an algorithm's textbook and look up correctness you will run into a bunch of mathematical theory this is because traditionally algorithm correctness is proved by mathematical induction which is a form of reasoning used in mathematics to verify that a statement is correct this approach involves writing what is called a specification and a correctness proof we won't be going into that in this course proof through induction is an important part of designing algorithms but we're confident that you can understand algorithms both in terms of how and when to use them without getting into the math so if you pick up a textbook and feel daunted don't worry i do too but we can still figure things out without it all right so once we have a correct algorithm we can start to talk about how efficient an algorithm is remember that this efficiency ultimately matters because they help us solve problems faster and deliver a better end user experience in a variety of fields for example algorithms are used in the sequencing of dna and more efficient sequencing algorithms allow us to research and understand diseases better and faster but let's not get ahead of ourselves we'll start simple by evaluating john's linear search algorithm in terms of its efficiency first what do we mean by efficiency there are two measures of efficiency when it comes to algorithms time and space sounds really cool and very scifi huh efficiency measured by time something you'll hear called time complexity is a measure of how long it takes the algorithm to run time complexity can be understood generally outside the context of code and computers because how long it takes to complete a job is a universal measure of efficiency the less time you take the more efficient you are the second measure of efficiency is called space complexity and this is pretty computer specific it deals with the amount of memory taken up on the computer good algorithms need to balance between these two measures to be useful for example you can have a blazingly fast algorithm but it might not matter if the algorithm consumes more memory than you have available both of these concepts time and space complexity are measured using the same metric but it is a very technical sounding metric so let's build up to it slowly and start simple a few videos ago i played a game with brittany and john where they tried to guess the number i was thinking of effectively they were searching for a value so how do we figure out how efficient each algorithm is and which algorithm was more suited to our purposes if we consider the number of tries they took to guess or search for the value as an indicator of the time they take to run through the exercise this is a good indicator of how long the algorithm runs for a given set of values this measurement is called the running time of an algorithm and we'll use it to define time complexity in the game we play it four rounds let's recap those here focusing on john's performance in round one we had 10 values the target was 3 and john took 3 turns in round 2 we had 10 values the target was 10 and john took 10 turns in round 3 we had 100 values the target was john took five tries and finally in round four when the target was 100 given 100 values john took 100 tries on paper it's hard to gauge anything about this performance when it comes to anything with numbers though i like to put it up on a graph and compare visually on the vertical or yaxis let's measure the number of tries it took john to guess the answer or the running time of the algorithm on the horizontal or xaxis what do we put for each turn we have a number of values as well as a target value we could plot the target value on the horizontal axis but that leaves some context and meaning behind it's far more impressive that john took five tries when the range went up to 100 then when he took three tries for a maximum of 10 values we could plot the maximum range of values but then we're leaving out the other half of the picture there are data points however that satisfy both requirements if we only plot the values where the target the number john was looking for was the same as the maximum range of values we have a data point that includes both the size of the data set as well as his effort there's an additional benefit to this approach as well there are three ways we can measure how well john does or in general how well any algorithm does first we can check how well john does in the best case or good scenarios from the perspective of his strategy in the range of 100 values the answer being a low number like three at the start of the range is a good scenario he can guess it fairly quickly one is his best case scenario or we could check how well he does on average we could run this game a bunch of times and average out the running time this would give us a much better picture of john's performance over time but our estimates would be too high if the value he was searching for was at the start of the range or far too low if it was at the end of the range let's imagine a scenario where facebook naively implements linear search when finding friends they looked at the latest u.s census saw that 50 of names start with the letters a through j which is the first 40 of the alphabet and thought okay on average linear search serves us well but what about the rest of those whose names start with the letter after j in the alphabet searching for my name would take longer than the average and much longer for someone whose name starts with the letter z so while measuring the run time of an algorithm on average might seem like a good strategy it won't necessarily provide an accurate picture by picking the maximum in the range we're measuring how our algorithm does in the worst case scenario analyzing the worst case scenario is quite useful because it indicates that the algorithm will never perform worse than we expect there's no room for surprises back to our graph we're going to plot the number of tries a proxy for running time of the algorithm against the number of values in the range which will shorten to n n here also represents john's worst case scenario when n is 10 he takes 10 turns when n is 100 he takes 100 turns but these two values alone are insufficient to really get any sort of visual understanding moreover it's not realistic john may take a long time to work through 100 numbers but a computer can do that in no time to evaluate the performance of linear search in the context of a computer we should probably throw some harder and larger ranges of values at it the nice thing is by evaluating a worst case scenario we don't actually have to do that work we know what the result will be for a given value of n using linear search it will take n tries to find the value in the worst case scenario so let's add a few values in here to build out this graph okay so we have a good picture of what this is starting to look like as the values get really large the running time of the algorithm gets large as well we sort of already knew that before we dig into this runtime any deeper let's switch tracks and evaluate brittany's work by having something to compare against it should become easier to build a mental model around time complexity the algorithm john used linear search seemed familiar to us and you could understand it because it's how most of us search for things in real life anyway brittany's approach on the other hand got results quickly but it was a bit harder to understand so let's break it down just like john's approach britney started with a series of values or a list of numbers as her input where john just started at the beginning of the list and searched sequentially brittany's strategy is to always start in the middle of the range from there she asks a comparison question is the number in the middle of the range equal to the answer she's looking for and if it's not is it greater than or less than the answer if it's greater than she can eliminate all the values less than the one she's currently evaluating if it's lesser than the answer she can eliminate all the values greater than the one she's currently evaluating with the range of values that she's left over with she repeats this process until she arrives at the answer let's visualize how she did this by looking at round three in round three the number of values in the range was 100 the answer was 5. the bar here represents the range of values one of the left 100 at the right and this pointer represents the value britney chooses to evaluate so she starts in the middle at 50. she asks is it equal to the answer i say it's too high so this tells her that the value she is evaluating is greater than our target value which means there's no point in searching any of the values to the right of 50 that is values greater than 50 in this range so she can discard those values altogether she only has to consider values from 1 to 50 now the beauty of this strategy and the reason why britney was able to find the answer in such few turns is that with every value she evaluates she can discard half of the current range on her second turn she picks the value in the middle of the current range which is 25. she asks the same question i say that the value is too high again and this tells her that she can discard everything greater than 25 and the range of values drops from 1 to 25. again she evaluates the number in the middle roughly so that'd be 13 here i tell her this is still too high she discards the values greater moves to value at 7 which is still too high then she moves to 4 which is now too low she can discard everything less than 4 which leaves the numbers 4 through 7. here she picked 6 which was too high which only leaves one value 5. this seems like a lot of work but being able to get rid of half the values with each turn is what makes this algorithm much more efficient now there's one subtlety to using binary search and you might have caught on to this for this search method to work as we've mentioned the values need to be sorted with linear search it doesn't matter if the values are sorted since a linear search algorithm just progresses sequentially checking every element in the list if the target value exists in the list it will be fouled but let's say this range of values 100 was unsorted britney would start at the middle with something like 14 and ask if this value was too low or too high i say it's too high so she discards everything less than 14. now this example starts to fall apart here because well britney knows what numbers are less than 14 and greater than one she doesn't need an actual range of values to solve this a computer however does need that remember search algorithms are run against lists containing all sorts of data it's not always just a range of values containing numbers in a real use case of binary search which we're going to implement in a bit the algorithm wouldn't return the target value because we already know that it's a search algorithm so we're providing something to search for instead what it returns is the position in the list that the target occupies without the list being sorted a binary search algorithm would discard all the values to the left of 14 which over here could include the position where our target value is eventually we'd get a result back saying the target value doesn't exist in the list which is inaccurate earlier when defining linear simple search i said that the input was a list of values and the output was the target value or more specifically the position of the target value in the list so with binary search there's also that precondition the input list must be sorted so let's formally define binary search first the input a sorted list of values the output the position in the list of the target value we're searching for or some sort of values indicate that the target does not exist in the list remember our guidelines for defining an algorithm let me put those up again really quick the steps in the algorithm need to be in a specific order the steps also need to be very distinct the algorithms should produce a result and finally the algorithm should complete in a finite amount of time let's use those to define this algorithm step one we determine the middle position of the sorted list step two we compare the element in the middle position to the target element step three if the elements match we return the middle position and end if they don't match in step 4 we check whether the element in the middle position is smaller than the target element if it is then we go back to step 2 with a new list that goes from the middle position of the current list to the end of the current list in step five if the element in the middle position is greater than the target element then again we go back to step two with a new list that goes from the start of the current list to the middle position of the current list we repeat this process until the target element is found or until a sub list contains only one element if that single element sublist does not match the target element then we end the algorithm indicating that the element does not exist in the list okay so that is the magic behind how britney managed to solve the round much faster in the next video let's talk about the efficiency of binary search we have a vague understanding that britney's approach is better in most cases but just like with linear search it helps to visualize this much like we did with linear search when determining the efficiency of an algorithm and remember we're still only looking at efficiency in terms of time time complexity as it's called we always want to evaluate how the algorithm performs in the worst case scenario now you might be thinking well that doesn't seem fair because given a series of data if the target value we're searching for is somewhere near the front of the list then linear search may perform just as well if not slightly better than binary search and that is totally true remember a crucial part of learning algorithms is understanding what works better in a given context when measuring efficiency though we always use the worst case scenarios as a benchmark because remember it can never perform worse than the worst case let's plot these values on the graph we started earlier with the number of tris or the runtime of the algorithm on the y axis and the maximum number of values in the series or n on the horizontal axis to represent the worst case scenario we have two data points when n equals 10 britney took four tries using binary search and when n equals 100 it took seven tries but even side by side these data points are sort of meaningless remember that while there is quite a difference between the run time of linear search and binary search at an n value of 100 for a computer that shouldn't matter what we should check out is how the algorithm performs at levels of n that might actually slow a computer down as n grows larger and larger how do these algorithms compare to one another let's add that to the graph okay now a picture starts to emerge as n gets really large the performance of these two algorithms differs significantly the difference is kind of staggering actually even with the simple game we saw that binary search was better but now we have a much more complete idea of how much better for example when n is 1000 the runtime of linear search measured by the number of operations or turns is also 1000. for binary search it takes just 10 operations now let's look at what happens when we increase n by factor of 10 at 10 000 linear search takes 10 000 operations while binary search takes 14 operations and increased by a factor of 10 in binary search only needs four more operations to find a value if we increase it again by a factor of 10 once more to an n value of 100 000 binary search takes only 17 operations it is blazing fast what we've done here is plotted on a graph how the algorithm performs as the input set it is working on increases in other words we've plotted the growth rate of the algorithm also known as the order of growth different algorithms grow at different rates and by evaluating their growth rates we get a much better picture of their performance because we know how the algorithm will hold up as n grows larger this is so important in fact it is the standard way of evaluating an algorithm and brings us to a concept called big o you might have heard this word thrown about and if you found it confusing don't worry we've already built up a definition in the past few videos we just need to bring it all together let's start with a common statement you'll see in studies on algorithms big o is a theoretical definition of the complexity of an algorithm as a function of the size wow what a mouthful this sounds really intimidating but it's really not let's break it down big o is a notation used to describe complexity and what i mean by notation is that it simplifies everything we've talked about down into a single variable an example of complexity written in terms of big o looks like this as you can see it starts with an uppercase letter o that's why we call it big o it's literally a big o the o comes from order of magnitude of complexity so that's where we get the big o from now complexity here refers to the exercise we've been carrying out in measuring efficiency if it takes brittany 4 tries when n is 10 how long does the algorithm take when n is 10 million when we use big o for this the variable used which we'll get to distills that information down so that by reading the variable you get a big picture view without having to run through data points and graphs just like we did it's important to remember that complexity is relative when we evaluate the complexity of the binary search algorithm we're doing it relative to other search algorithms not all algorithms bigo is a useful notation for understanding both time and space complexity but only when comparing amongst algorithms that solve the same problem the last bit in that definition of big o is a function of the size and all this means is that big o measures complexity as the input size grows because it's not important to understand how an algorithm performs in a single data set but in all possible data sets you will also see big o referred to as the upper bound of the algorithm and what that means is that big o measures how the algorithm performs in the worst case scenario so that's all big o is nothing special it's just a notation that condenses the data points and graphs that we've built up down to one variable okay so what do these variables look like for john's strategy linear search we say that it has a time complexity of big o and then n so that's again big o with an n inside parentheses for britney strategy binary search we say that it has a time complexity of big o of log n that's big o with something called a log and an n inside parentheses now don't worry if you don't understand that we'll go into that in more detail later on in the course each of these has a special meaning but it helps to work through all of them to get a big picture view so over the next few videos let's examine what are called common complexities or common values of big o that you will run into and should internalize in our discussions of complexity we made one assumption that the algorithm as a whole had a single measure of complexity that isn't true and we'll get at how we arrive at these measures for the entire algorithm at the end of this exercise but each step in the algorithm has its own space and time complexity in linear search for example there are multiple steps and the algorithm goes like this start at the beginning of the list or range of values compare the current value to the target if the current value is the target value that we're looking for we're done if it's not we'll move on sequentially to the next value in the list and repeat step two if we reach the end of the list then the target value is not in the list let's go back to step two for a second comparing the current value to the target does the size of the data set matter for this step when we're at step two we're already at that position in the list and all we're doing is reading the value to make a comparison reading the value is a single operation and if we were to plot it on a graph of runtime per operations against n it looks like this a straight line that takes constant time regardless of the size of n since this takes the same amount of time in any given case we say that the run time is constant time it doesn't change in big o notation we represent this as big o with a 1 inside parentheses now when i first started learning all this i was really confused as to how to read this even if it was in my own head should i say big o of one when you see this written you're going to read this as constant time so reading a value in a list is a constant time operation this is the most ideal case when it comes to run times because input size does not matter and we know that regardless of the size of n the algorithm runtime will remain the same the next step up in complexity so to speak is the situation we encountered with the binary search algorithm traditionally explaining the time complexity of binary search involves math i'm going to try to do it both with and without when we played the game using binary search we notice that with every turn we were able to discard half of the data but there's another pattern that emerges that we didn't explore let's say n equals 10. how long does it take to find an item at the 10th position of the list we can write this out so we go from 10 to 5 to 8 to 9 and then down to 10. here it takes us four tries to cut down the list to just one element and find the value we're looking for let's double the value of n to 20 and see how long it takes for us to find an item at the 20th position so we start at 20 and then we pick 10 from there we go to 15 17 19 and finally 20. so here it takes us five tries okay let's double it again so that n is 40 and we try to find the item in the 40th position so when we start at 40 the first midpoint we're going to pick is 20 from there we go to 30 then 35 37 39 and then 40. notice that every time we double the value of n the number of operations it takes to reduce the list down to a single element only increases by 1. there's a mathematical relationship to this pattern and it's called a logarithm of n you don't really have to know what logarithms truly are but i know that some of you like underlying explainers so i'll give you a quick one if you've taken algebra classes you may have learned about exponents here's a quick refresher 2 times 1 equals 2. now this can be written as 2 raised to the first power because it is our base case two times one is two now two times two is four this can be written as two raised to the second power because we're multiplying two twice first we multiply two times one then the result of that times 2. 2 times 2 times 2 is 8 and we can write this as 2 raised to the 3rd power because we're multiplying 2 3 times in 2 raised to 2 and 2 raised to 3 the 2 and 3 there are called exponents and they define how the number grows with 2 raised to 3 we start with the base value and multiply itself 3 times the inverse of an exponent is called a logarithm so if i say log to the base 2 of 8 equals 3 i'm basically saying the opposite of an exponent instead of saying how many times do i have to multiply this value i'm asking how many times do i have to divide 8 by two to get the value one this takes three operations what about the result of log to the base two of sixteen that evaluates to four so why does any of this matter notice that this is sort of how binary search works log to the base 2 of 16 is 4. if n was 16 how many triads does it take to get to that last element well we start in the middle at 8 that's too low so we move to 12 then we move to 14 then to 15 and then to 16 which is 5 tries or log to the base 2 of 16 plus 1. in general for a given value of n the number of tries it takes to find the worst case scenario is log of n plus one and because this pattern is overall a logarithmic pattern we say that the runtime of such algorithms is logarithmic if we plot these data points on our graph a logarithmic runtime looks like this in big o notation we represent a logarithmic runtime as big o of log n which is written as big o with log n inside parentheses or even sometimes as l n n inside parentheses when you see this read it as logarithmic time as you can see on the graph as n grows really large the number of operations grows very slowly and eventually flattens out since this line is below the line for a linear runtime which we'll look at in a second you might often hear algorithms with logarithmic runtimes being called sublinear logarithmic or sublinear runtimes are preferred to linear because they're more efficient but in practice linear search has its own set of advantages which we'll take a look at in the next video next up let's look at the situation we encountered with the linear search algorithm we saw that in the worst case scenario whatever the value of n was john took exactly that many tries to find the answer as in linear search when the number of operations to determine the result in the worst case scenario is at most the same as n we say that the algorithm runs in linear time we represent this as big o of n now you can read that as big o of n like i just said or you can say linear time which is more common when we put that up on a graph against constant time and logarithmic time we get a line that looks like this any algorithm that sequentially reads the input will have linear time so remember anytime you know a problem involves reading every item in a list that means a linear run time as you saw from the game we played brittany's strategy using binary search was clearly better and we can see that on the graph so if we had the option why would we use linear search which runs in linear time remember that binary search had a precondition the input set had to be sorted while we won't be looking at sorting algorithms in this course as you learn more about algorithms you'll find that sorting algorithms have varying complexities themselves just like search does so we have to do additional work prior to using binary search for this reason in practice linear search ends up being more performant up to a certain value of n because the combination of sorting first and then searching using binary search adds up the next common complexity you will hear about is when an algorithm runs in quadratic time if the word quadratic sounds familiar to you it's because you might have heard about it in math class quadratic is a word that means an operation raised to the second power or when something is squared let's say you and your friends are playing a tower defense game and to start it off you're going to draw a map of the terrain this map is going to be a grid and you pick a random number to determine how large this grid is let's set n the size of the grid to four next you need to come up with a list of coordinates so you can place towers and enemies and stuff on this map so how would we do this if we start out horizontally we'd have coordinate points that go 1 1 1 2 1 3 and 1 4. then you go up one level vertically and we have points 2 1 2 2 2 3 and 2 4. go up one more and you have the points 3 1 3 2 3 3 and 3 4 and on that last row you have the points 4 1 4 2 4 3 and 4 4. notice that we have a pattern here for each row we take the value and then create a point by adding to that every column value the range of values go from 1 to the value of n so we can generally think of it this way for the range of values from 1 to n for each value in that range we create a point by combining that value with the range of values from 1 to n again doing it this way for each value in the range of 1 to n we create an n number of values and we end up with 16 points which is also n times n or n squared this is an algorithm with a quadratic runtime because for any given value of n we carry out n squared number of operations now i picked a relatively easy so to speak example here because in english at least we often denote map sizes by height times width so we would call this a 4 by 4 grid which is just another way of saying 4 squared or n squared in big o notation we would write this as big o of n squared or say that this is an algorithm with a quadratic runtime many search algorithms have a worst case quadratic runtime which you'll learn about soon now in addition to quadratic runtimes you may also run into cubic runtimes as you encounter different algorithms in such an algorithm for a given value of n the algorithm executes n raised to the third power number of operations these aren't as common as quadratic algorithms though so we won't look at any examples but i think it's worth mentioning thrown up on our graph quadratic and cubic runtimes look like this so this is starting to look pretty expensive computationally as they say we can see here that for small changes in n there's a pretty significant change in the number of operations that we need to carry out the next worst case runtime we're going to look at is one that's called quasilinear and a sort of easier to understand for lack of better word by starting with the big o notation quasilinear runtimes are written out as big o of n times log n we learned what log n was right a logarithmic runtime whereas n grew the number of operations only increased by a small factor with a quasilinear runtime what we're saying is that for every value of n we're going to execute a log n number of operations hence the run time of n times log n so you saw earlier with the quadratic runtime that for each value of n we conducted n operations it's sort of the same in that as we go through the range of values in n we're executing login operations in comparison to other runtimes a quasilinear algorithm has a runtime that lies somewhere between a linear runtime and a quadratic runtime so where would we expect to see this kind of runtime in practical use well sorting algorithms is one place you will definitely see it merge sort for example is a sorting algorithm that has a worst case runtime of big o of n log n let's take a look at a quick example let's say we start off with a list of numbers that looks like this and we need to sort it merge sort starts by splitting this list into two lists down the middle it then takes each sub list and splits that in half down the middle again it keeps doing this until we end up with a list of just a single number when we're down to single numbers we can do one sort operation and merge these sublists back in the opposite direction the first part of merge sort cuts those lists into sublists with half the numbers this is similar to binary search where each comparison operation cuts down the range to half the values you know the worst case runtime in binary search is log n so these splitting operations have the same runtime big o of log n or logarithmic but splitting into half isn't the only thing we need to do with merge sort we also need to carry out comparison operations so we can sort those values and if you look at each step of this algorithm we carry out an n number of comparison operations and that brings the worst case runtime of this algorithm to n times log n also known as quasi linear don't worry if you didn't understand how merge sort works that wasn't the point of this demonstration we will be covering merge sorts soon in a future course the run times we've looked at so far are all called polynomial runtimes an algorithm is considered to have a polynomial runtime if for a given value of n its worst case runtime is in the form of n raised to the k power where k just means some value so it could be n squared where k equals 2 for a quadratic runtime n cubed for a cubic runtime and so on all of those are in the form of n raised to some power anything that is bounded by this and what i mean by that is if we had a hypothetical line on our graph of n raised to the k power anything that falls under this graph is considered to have a polynomial runtime algorithms with an upper bound or a runtime with a big o value that is polynomial are considered efficient algorithms and are likely to be used in practice now the next class of runtimes that we're going to look at are a runtimes that we don't consider efficient and these are called exponential runtimes with these runtimes as n increases slightly the number of operations increases exponentially and as we'll see in a second these algorithms are far too expensive to be used an exponential runtime is an algorithm with a big o value of some number raised to the nth power imagine that you wanted to break into a locker that had a padlock on it let's assume you forgot your code this lock takes a two digit code and the digit for the code ranges from zero to nine you start by setting the dials to zero and then with the first dial remaining on zero you change the second dial to one and try and open it if it doesn't work you set it to two then try again you would keep doing this and if you still haven't succeeded with the second dial set to 9 then you go back to that first dial set it to 1 and start the second dial over the range of values you'd have to go through is 0 0 to 9 9 which is 100 values this can be generalized as 10 to the second power since there are 10 values on each dial raised to two dials searching through each individual value until you stumble on the right one is a strategy called brute force and brute force algorithms have exponential run times here there are two dials so n is 2 and each dial has 10 values so again we can generalize this algorithm as 10 raised to n where n represents the number of dials the reason that this algorithm is so inefficient is because with just one more dial on the lock the number of operations increases significantly with three dials the number of combinations in the worst case scenario where the correct code is the last digit in the range is 10 raised to 3 or 1 000 values with an additional wheel it becomes 10 raised to 4 or 10 000 values as n increases the number of operations increases exponentially to a point where it's unsolvable in a realistic amount of time now you might think well any computer can crack a four digit numerical lock and that's true because n here is sufficiently small but this is the same principle that we use for passwords in a typical password field implemented well users are allowed to use letters of the english alphabet so up to 26 characters numbers from 0 to 9 and a set of special characters of which there can be around 33 so typically that means each character in a password can be one out of 69 values this means that for a one character password it takes 69 to the nth power so 1 which equals 69 operations in the worst case scenario to figure out the password just increasing n to 2 increases the number of operations needed to guess the password to 69 squared or 4761 operations now usually on a secure website there isn't really a limit but in general passwords are limited to around 20 characters in length with each character being a possible 69 values and there being 20 characters the number of operations needed to guess the password in the worst case scenario is 69 raised to the 20th power or approximately 6 followed by 36 zeros number of operations an intel cpu with five cores can carry out roughly about 65 000 million instructions per second that's a funny number i know to crack our 20digit passcode in this very simplistic model it would take this intel cpu to race to 20th power years to brute force the password so while this algorithm would eventually produce a result it is so inefficient that it's pointless this is one of the reasons why people recommend you have longer passwords since brute forcing is exponential in the worst case each character you add increases the number of combinations by an exponent the next class of exponential algorithms is best highlighted by a popular problem known as the traveling salesman the problem statement goes like this given a list of cities and the distance between each pair of cities what is the shortest possible route that visits each city and then returns to the origin city this seems like a simple question but let's start with a simple case three cities a b and c to figure out what the shortest route is we need to come up with all the possible routes with three cities we have six routes in theory at least some of these routes can be discarded because abc is the same as c b a but in the opposite direction but as we do know sometimes going from a to c through b may go through a different route than c to a through b so we'll stick to the six routes and from there we could determine the shortest no big deal now if we increase this to four cities we jump to 24 combinations the mathematical relationship that defines this is called a factorial and is written out as n followed by an exclamation point factorials are basically n times n minus one repeated until you reach the number one so for example the factorial of three is three times two times one which is six which is the number of combinations we came up with for three cities the factorial of four is four times three times two times one or 24 which is the number of combinations we arrived at with four cities in solving the traveling salesman problem the most efficient algorithm will have a factorial runtime or a combinatorial runtime as it's also called at low values of n algorithms with a factorial runtime may be used but with an n value of say 200 it would take longer than humans have been alive to solve the problem for sake of completeness let's plot a combinatorial runtime on our graph so that we can compare an algorithm such as one that solves the traveling salesman problem as a worst case run time of big o of n factorial studying exponential runtimes like this are useful for two reasons first in studying how to make such algorithms efficient we develop strategies that are useful across the board and can potentially be used to make existing algorithms even more efficient second it's important to be aware of problems that take a long time to solve knowing right off the bat that a problem is somewhat unsolvable in a realistic time means you can focus your efforts on other aspects of the problem as beginners though we're going to steer clear of all this and focus our efforts on algorithms with polynomial runtimes since we're much more likely to work with and learn about such algorithms now that we know some of the common complexities in the next video let's talk about how we determine the complexity of an algorithm because there are some nuances over the last few videos we took a look at common complexities that we would encounter in studying algorithms but the question remains how do we determine what the worst case complexity of an algorithm is earlier i mentioned that even though we say that an algorithm has a particular upper bound or worst case runtime each step in a given algorithm can have different run times let's bring up the steps for binary search again assuming the list is sorted the first step is to determine the middle position of the list in general this is going to be a constant time operation many programming languages hold on to information about the size of the list so we don't actually need to walk through the list to determine the size now if we didn't have information about the size of the list we would need to walk through counting each item one by one until we reached the end of the list and this is a linear time operation but realistically this is a big o of 1 or constant time step 2 is to compare the element in the middle position to the target element we can assume that in most modern programming languages this is also a constant time operation because the documentation for the language tells us it is step 3 is our success case and the algorithm ends this is our best case and so far we have only incurred two constant time operations so we would say that the best case run time of binary search is constant time which is actually true but remember that best case is not a useful metric step 4 if we don't match is splitting the list into sublists assuming the worst case scenario the algorithm would keep splitting into sublists until a single element list is reached with the value that we're searching for the run time for this step is logarithmic since we discard half the values each time so in our algorithm we have a couple steps that are constant time and one step that is logarithmic overall when evaluating the run time for an algorithm we say that the algorithm has as its upper bound the same runtime as the least efficient step in the algorithm think of it this way let's say you're participating in a triathlon which is a race that has a swimming running and a cycling component you could be a phenomenal swimmer and a really good cyclist but you're a pretty terrible runner no matter how fast you are at swimming or cycling your overall race time is going to be impacted the most by your running race time because that's the part that takes you the longest if you take an hour 30 to finish the running component 55 minutes to swim and 38 minutes to bike it won't matter if you can fine tune your swimming technique down to finish in 48 minutes and your cycle time to 35 because you're still bounded at the top by your running time which is close to almost double your bike time similarly with the binary search algorithm it doesn't matter how fast we make the other steps they're already as fast as they can be in the worst case scenario the splitting of the list down to a single element list is what will impact the overall running time of your algorithm this is why we say that the time complexity or run time of the algorithm in the worst case is big o of log n or logarithmic as i alluded to though your algorithm may hit a best case runtime and in between the two best and worst case have an average run time as well this is important to understand because algorithms don't always hit their worst case but this is getting a bit too complex for us for now we can safely ignore average case performances and focus only on the worst case in the future if you decide to stick around we'll circle back and talk about this more now that you know about algorithms complexities and big o let's take a break from all of that and write code in the next video so far we've spent a lot of time in theory and while these things are all important things to know you get a much better understanding of how algorithms work when you start writing some code as i mentioned earlier we're going to be writing python code in this and all subsequent algorithm courses if you do have programming experience but in another language check the notes section of this video for an implementation in your language if you don't have any experience i'll try my best explain as we go along on the video you're watching right now you should see a launch workspaces button we're going to use a treehouse coding environment call workspaces to write all of our code if you're familiar with using python in a local environment then feel free to keep doing so workspaces is an inbrowser coding environment and will take care of all the setup and installation so you can focus on just writing and evaluating code workspaces is quite straightforward to use on the left here we have a file navigator pane which is currently empty since we haven't created a new file on the top we have an editor where we write all our code and then below that we have a terminal or a command line prompt where we can execute the scripts that we write let's add a new file here so at the top in the editor area we're going to go to file new file and we'll name this linear underscore search dot pi in here we're going to define our linear search algorithm as a standalone function we start with the keyword def which defines a function or a block of code and then we give it the name linear underscore search this function will accept two arguments first the list we're searching through and then the target value we're looking for both of these arguments are enclosed in a set of parentheses and there's no space between the name of the function and the arguments after that we have a colon now there might be a bit of confusion here since we already have this target value what are we searching for unlike the game we played at the beginning where john's job was to find the value in a true implementation of linear search we're looking for the position in the list where the value exists if the target is in the list then we return its position and since this is a list that position is going to be denoted by an index value now if the target is not found we're going to return none the choice of what to return in the failure case may be different in other implementations of linear search you can return 1 since that isn't typically an index value you can also raise an exception which is python speak for indicating an error occurred now i think for us the most straightforward value we can return here is none now let's add a comment to clarify this so hit enter to go to the next line and then we're going to add three single quotes and then below that on the next line we'll say returns the position or the index position of the target if found else returns none and then on the next line we'll close off those three quotes this is called a doc string and is a python convention for documenting your code the linear search algorithm is a sequential algorithm that compares each item in the list until the target is found to iterate or loop or walk through our list sequentially we're going to use a for loop now typically when iterating over a list in python we would use a loop like this we'd say for item in list this assigns the value at each index position to that local variable item we don't want this though since we primarily care about the index position instead we're going to use the range function in python to create a range of values that start at 0 and end at the number of items in the list so we'll say 4 i i stands for index here in range starting at 0 and going all the way up to the length of the list we can get the number of items in the list using the len function now going back to our talk on complexity and how individual steps in an algorithm can have its own run times this is a line of code that we would have to be careful about python keeps track of the length of a list so this function call here len list is a constant time operation now if this were a naive implementation let's say we wrote the implementation of the list and we iterate over the list every time we call this length function then we've already incurred a linear cost okay so once we have a range of values that represent index positions in this list we're going to iterate over that using the for loop and assign each index value to this local variable i using this index value we can obtain the item at that position using subscript notation on the list now this is also a constant time operation because the language says so so we'll do if list so once we have this value which we'll get by using subscript notation so we'll say list i once we have this value we'll check if it matches the target so if the value at i equals target well if it does then we'll return that index value because we want the position and once we hit this return statement we're going to terminate our function if the entire for loop is executed and we don't hit this return statement then the target does not exist in the list so at the bottom here we'll say return none even though all the individual operations in our algorithm run in constant time in the worst case scenario this for loop here will have to go through the entire range of values and read every single element in the list therefore giving the algorithm a big o value of n or running in linear time now if you've written code before you've definitely written code like this a number of times and i bet you didn't know but all along you are implementing what is essentially a wellknown algorithm so i hope this goes to show you that algorithms are pretty approachable topic like everything else this does get advanced but as long as you take things slow there's no reason for it to be impossible remember that not any block of code counts as an algorithm to be a proper implementation of linear search this block of code must return a value must complete execution in a finite amount of time and must output the same result every time for a given input set so let's verify this with a small test let's write a function called verify that accepts an index value if the value is not none it prints the index position if it is none it informs us that the target was not found in the list so def verify and this is going to take an index value and we'll say if index is not none then we'll print target found at index oops that's a colon here index else that needs to go back there we go else we'll say target not found in list okay using this function let's define a range of numbers now so this will be a list numbers and we'll just go from 1 to let's say 10. now if you've written python code before you know that i can use a list comprehension to make this easier but we'll keep things simple we can now use our linear search function to search for the position of a target value in this list so we can say result equal linear underscore search and we're going to pass in the numbers list that's the one we're searching through and we want to look for the position where the value 12 exists and then we'll verify this result if our algorithm works correctly the verify function should inform us that the target did not exist so make sure you save the file which you can do by going up to file and save or hitting command s and then below in the terminal you're going to type out python linear search or you can hit tab and it should auto complete linear search dot pi as you can see correct the target was not found in the list so the output of our script is what we expect for our second test let's search for the value 6 in the list so you can copy this command c to copy and then paste it again and we'll just change 12 here to 6 and then come back down to the terminal hit the up arrow to execute the same command again and hit enter you'll notice that i forgot to hit save so it did not account for that new change we'll try that again and there you'll see that if it works correctly which it did the index should be number five run the program on your end and make sure everything works as expected our algorithm returned a result in each case it executed in a finite time and the results were the ones we expect in the next video let's tackle binary search in the last video we left off with an implementation of linear search let's do the same for binary search so that we get an understanding of how this is represented in code so we'll do this in a new file back to file new file and we'll name this one binary search dot py like before we're going to start with a function named binary search so we'll say def binary underscore search that takes a list and a target if you remember binary search works by breaking the array or list down into smaller sets until we find the value we're looking for we need a way to keep track of the position of the list that we're working with so let's create two variables first and last to point to the beginning and end of the array so first equal zero now if you're new to programming list positions are represented by index values that start at zero instead of one so here we're setting first to zero to point to the first element in the list last is going to point to the last element in the list so we'll say last equal len list minus one now this may be confusing to you so a quick sidebar to explain what's going on let's say we have a list containing 5 elements if we called len on that list we should get 5 back because there are 5 elements but remember that because the position numbers start at 0 the last value is not at position 5 but at 4. in nearly all programming languages getting the position of the last element in the list is obtained by determining the length of the list and deducting 1 which is what we're doing okay so we know what the first and last positions are when we start the algorithm for our next line of code we're going to create a while loop a while loop takes a condition and keeps executing the code inside the loop until the condition evaluates to false for our condition we're going to say to keep executing this loop until the value of first is less than or equal to the value of last so while first less than or equal to last well why you ask why is this our condition well let's work through this implementation and then a visualization should help inside the while loop we're going to calculate the midpoint of our list since that's the first step of binary search midpoint equal so we'll say first plus last and then we'll use the floor division double slash here divided by two now the two forward slashes here are what python calls a floor division operator what it does is it rounds down to the nearest whole number so if we have an eight element array first is zero last is 7 if we divided 0 plus 7 which is 7 by 2 we would get 3.5 now 3.5 is not a valid index position so we round that down to 3 using the floor division operator okay so now we have a midpoint the next step of binary search is to evaluate whether the value at this midpoint is the same as the target we're looking for so say if list value at midpoint equals the target well if it is then we'll go ahead and return the midpoint so we'll say return midpoint the return statement terminates our algorithm and over here we're done this is our best case scenario next we'll say else if list at midpoint or value at midpoint is less than the target now here if the value is less the value at midpoint is less than the target then we don't care about any of the values lower than the midpoint so we redefine first to point to the value after the midpoint so we'll say midpoint plus 1. now if the value at the midpoint is greater than the target then we can discard the values after the midpoint and redefine last to point to the value prior to the midpoint so we'll say else last equal midpoint minus 1. let's visualize this we're going to start with a list of nine integers to make this easier to understand let's specify these integers to be of the same value as its index position so we have a range of values from 0 to 8. our target is the worst case scenario we're looking for the position of the value 8. at the start our algorithm sets first to point to the index 0 and last to point to the length of the list minus 1 which is 8. next we hit our while loop the logic of this loop is going to be executed as long as the value of first is not greater than the value of last or as we've defined it we're going to keep executing the contents of the loop as long as first is less than or equal to last on the first pass this is true so we enter the body of the loop the midpoint is first plus last divided by two and rounded down so we get a nice even four the value at this position is four now this is not equal to the target so we move to the first else if four is less than eight so now we redefine first to point to midpoint plus one which is five first is still less than last so we run through the body of the loop again the midpoint is now six six is less than eight so we move first to point to seven seven is still less than or equal to eight so we go for another iteration of the loop the midpoint is seven oddly enough and seven is still less than the target so we move first to point to eight first is equal to last now but our condition says keep the loop going as long as first is less than or equal to last so this is our final time through the loop the midpoint is now 8 which makes the value at the midpoint equal to the target and we finally exit our algorithm and return the position of the target now what if we had executed all this code and never hit a case where midpoint equal the target well that would mean the list did not contain the target value so after the while loop at the bottom will return none we have several operations that make up our binary search algorithm so let's look at the runtime of each step we start by assigning values to first and last the value assigned to last involves a call to the len function to get the size of the list but we already know this is a constant time operation in python so both of these operations run in constant time inside the loop we have another value assignment and this is a simple division operation so again the runtime is constant in the next line of code we're reading a value from the list and comparing the midpoint to the target both of these again are constant time operations the remainder of the code is just a series of comparisons and value assignments and we know that these are all constant time operations as well so if all we have are a series of constant time operations why does this algorithm have in the worst case a logarithmic runtime it's hard to evaluate by just looking at the code but the while loop is what causes the run time to grow even though all we're doing is a comparison operation by redefining first and last over here or rather in the last two steps over here we're asking the algorithm to run as many times as it needs until first is equal or greater than last now each time the loop does this the size of the data set the size of the list grows smaller by a certain factor until it approaches a single element which is what results in the logarithmic runtime okay just like with linear search let's test that our algorithm works so we'll go back to linear search.hi and we're going to copy paste so command c to copy if you're on a mac then go back to binary search and at the bottom oops we're going to paste in that verify function okay we'll also go back and grab this numbers you know what let's go ahead and copy all all of these things so numbers and the two verify cases we'll paste that in as well and the only thing we need to change here is instead of calling linear search this is going to call binary search okay we'll hit command s to save the file and then i'm going to drag up my console and we'll run python binary search dot and hit enter and you'll see like just like before we get the same results back now note that an extremely important distinction needs to be made here the numbers list that we've defined for our test cases right here has to be sorted the basic logic of binary search relies on the fact that if the target is greater than the midpoint then our potential values lie to the left or vice versa since the values are sorted in ascending order if the values are unsorted our implementation of binary search may return none even if the value exists in the list and just like that you've written code to implement two search algorithms how fun was that hopefully this course has shown you that it isn't a topic to be afraid of and that algorithms like any other topic with code can be broken down and understood piece by piece now we have a working implementation of binary search but there's actually more than one way to write it so in the next video let's write a second version i'm going to create a new file as always file new file and we'll name this recursive underscore binary underscore search dot p y okay so we're going to add our new implementation here so that we don't get rid of that first implementation we wrote let's call this new function recursive binary search unlike our previous implementation this version is going to behave slightly differently in that it won't return the index value of the target element if it exists instead it will just return a true value if it exists and a false if it doesn't so recursive underscore binary underscore search and like before this is going to take a list it accepts a list and a target to look for in that list we'll start the body of the function by considering what happens if an empty list is passed in in that case we would return false so i would say if the length of the list which is one way to figure out if it's empty if it's equal to zero then we'll return false now you might be thinking that in the previous version of binary search we didn't care if the list was empty well we actually did but in a roundabout sort of way so in the previous version of binary search our function had a loop and that loop condition was true when first was less than or equal to last so as long as it's less than or equal to last we continue the loop now if we have an empty list then first is greater than last and the loop would never execute and we return none at the bottom so this is the same logic we're implementing here we're just doing it in a slightly different way if the list is not empty we'll implement an else clause now here we'll calculate the midpoint by dividing the length of the list by 2 and rounding down again there's no use of first and last here so we'll say length of list and then using the floor division operator we'll divide that by 2. if the value at the midpoint which we'll check by saying if list using subscript notation we'll say midpoint as the index now if this value at the midpoint is the same as the target then we'll go ahead and return true so far this is more or less the same except for the value that we're returning let me actually get rid of all that okay all right so if this isn't the case let's implement an else clause now here we have two situations so first if the value at the midpoint is less than the target so if value at midpoint is less than the target then we're going to do something new we're going to call this function again this recursive binary search function that we're in the process of defining we're going to call that again and we're going to give it the portion of the list that we want to focus on in the previous version of binary search we moved the first value to point to the value after the midpoint now here we're going to create a new list using what is called a slice operation and create a sub list that starts at midpoint plus 1 and goes all the way to the end we're going to specify the same target as a search target and when this function call is done we'll return the value so we'll say return the return is important then we'll call this function again recursive binary search and this function takes a list and here we're going to use that subscript notation to perform a slice operation by using two indexes a start and an end so we'll say our new list that we're passing in needs to start at midpoint plus one and then we'll go all the way to the end and this is a python syntactic sugar so to speak if i don't specify an end index python knows to just go all the way to the end all right so this is our new list that we're working with and we need a target we'll just pass it through if you're confused bear with me just like before we'll visualize this at the end okay we have another else case here and this is a scenario where the value at the midpoint is greater than the target which means we only care about the values in the list from the start going up to the midpoint now in this case as well we're going to call the binary search function again and specify a new list to work with this time the list is going to start at the beginning and then go all the way up to the midpoint so it looks the same we'll say return recursive binary search we're going to pass in a list here so if we just put a colon here without a start index python knows to start at the beginning and we're going to go all the way up to the midpoint the target here is the same and this is our new binary search function so let's see if this works actually yes down here we'll make some space and we'll define a verify function we're not going to copy paste the previous one because we're not returning none or an integer here so we'll verify the result that we pass in and we'll say print target found and this is just going to say true or false whether we found it okay so like before we need a numbers list and we'll do something one two three four all the way up to eight okay and now let's test this out so we'll call our recursive binary search function and we'll pass in the numbers list and the target here is 12. we're going to verify this verify the result make sure it works and then we'll call it again this time making sure that we give it a target that is actually in the list so here we'll say 6 and we'll verify this again make sure you hit command s to save and then in the console below we're going to type out python recursive binarysearch.pi run it and you'll see that we've verified that search works while we can't verify the index position of the target value which is a modification to how our algorithm works we can guarantee by running across all valid inputs that search works as intended so why write a different search algorithm here a different binary search algorithm and what's the difference between these two implementations anyway the difference lies in these last four lines of code that you see here we did something unusual here now before we get into this a small word of advice this is a confusing topic and people get confused by it all the time don't worry that doesn't make you any less of a programmer in fact i have trouble with it often and always look it up including when i made this video this version of binary search is a recursive binary search a recursive function is one that calls itself this is hard for people to grasp sometimes because there's few easy analogies that make sense but you can think of it and sort this way so let's say you have this book that contains answers to multiplication problems you're working on a problem and you look up an answer in the book the answer for your problem says add 10 to the answer for problem 52 okay so you look up problem 52 and there it says add 12 to the answer for problem 85 well then you go and look up the answer to problem 85 and finally instead of redirecting you somewhere else that answer says 10. so you take that 10 and then you go back to problem 52 because remember the answer for problem 52 was to add 12 to the answer for problem 85 so you take that 10 and then you now have the answer to problem 85 so you add 10 to 12 to get 22. then you go back to your original problem where it said to add 10 to the answer for problem 52 so you add 10 to 22 and you get 32 to end up with your final answer so that's a weird way of doing it but this is an example of recursion the solution to your first lookup in the book was the value obtained by another lookup in the same book which was followed by yet another lookup in the same book the book told you to check the book until you arrived at some base value our function works in a similar manner so let's visualize this with an example of list like before we have a nine element list here with values zero through eight the target we're searching for is the value eight we'll check if the list is empty by calling len on it this list is not empty so we go to the else clause next we calculate the midpoint 9 divided by 2 is 4.5 rounded down is 4 so our first midpoint value is 4. we'll perform our first check is the value at the midpoint equal to the target not true so we go to our else clause we'll perform another check here is the value at the midpoint less than the target now in our case this is true earlier when we evaluated this condition we simply change the value of first here we're going to call the recursive binary search function again and give it a new list to work with the list starts at midpoint plus 1 so at index position 5 all the way to the end notice that this call to recursive binary search inside of recursive binary search includes a return statement this is important and we'll come back to that in a second so now we're back at the top of a new call to recursive binary search with effectively a new list although technically just a sub list of the first one the list here contains the numbers 6 7 and 8. starting with the first check the list is not empty so we move to the else the midpoint in this case length of the list 3 divided by 2 rounded down is 1. is the value of the midpoint equal to the target well the value at that position is 7 so no in the else we perform the first check is the value at the midpoint less than the target indeed it is so we call recursive binary search again and provided a new list this list starts at midpoint plus 1 and goes to the end so in this case that's a single element list since this is a new call to recursive binary search we start back up at the top is the list empty no the midpoint is zero is the value at the midpoint the same as the target it is so now we can return true remember a minute ago i pointed out that when we call recursive binary search from inside the function itself it's preceded by a return statement that plays a pretty important role here so back to our visualization we start at the top and recall binary search with a new list but because that's got a return statement before it what we're saying is hey when you run binary search on this whatever value you get back return it to the function that called you then at the second level we call binary search again along with another return statement like with the first call we're instructing the function to return a value back to the code that called it at this level we find the target so the function returns true back to the caller but since this inner function was also called by a function with instructions to return it keeps returning that true value back up until we reach the very first function that called it going back to our book of answers recursive binary search instructs itself to keep working on the problem until it has a concrete answer once it does it works its way backwards giving the answer to every function that called it until the original caller has an answer now like i said at the beginning this is pretty complicated so you should not be concerned if this doesn't click honestly this is not one thing that you're going to walk away with knowing fully how to understand recursion after your first try i'm really not lying when i say i have a pretty hard time with recursion now before we move on i do want to point out one thing even though the implementation of recursion is harder to understand it is easier in this case to understand how we arrive at the logarithmic run time since we keep calling the function with smaller lists let's take a break here in the next video let's talk a bit more about recursion and why it matters in the last video we wrote a version of binary search that uses a concept called recursion recursion might be a new concept for you so let's formalize how we use it a recursive function is one that calls itself in our example the recursive binary search function called itself inside the body of the function when writing a recursive function you always need a stopping condition and typically we start the body of the recursive function with this stopping condition it's common to call this stopping condition the base case in our recursive binary search function we had two stopping conditions the first was what the function should return if an empty list is passed in it seems weird to evaluate an empty list because you wouldn't expect to run search on an empty list but if you look at how our function works recursive binary search keeps calling itself and with each call to itself the size of the list is cut in half if we searched for a target that didn't exist in the list then the function would keep halving itself until it got to an empty list consider a three element list with numbers one two three where we're searching for a target of four on the first pass the midpoint is 2 so the function would call itself with the list 3. on the next pass the midpoint is 0 and the target is still greater so the function would call itself this time passing in an empty list because an index of 0 plus 1 in a single element list doesn't exist when we have an empty list this means that after searching through the list the value wasn't found this is why we define an empty list as a stopping condition or a base case that returns false if it's not an empty list then we have an entirely different set of instructions we want to execute first we obtain the midpoint of the list once we have the midpoint we can introduce our next base case or stopping condition if the value at the midpoint is the same as the target then we return true with these two stopping conditions we've covered all possible paths of logic through the search algorithm you can either find the value or you don't once you have the base cases the rest of the implementation of the recursive function is to call the function on smaller sublists until we hit one of these base cases going back to our visualization for a second we see that recursive binary search calls itself a first time which then calls itself again for the initial list we started with the function only calls itself a few times before a stopping condition is reached the number of times a recursive function calls itself is called recursive depth now the reason i bring all of this up is because if after you start learning about algorithms you decide you want to go off and do your own research you may start to see a lot of algorithms implemented using recursion the way we implemented binary search the first time is called an iterative solution now when you see the word iterative it generally means the solution was implemented using a loop structure of some kind a recursive solution on the other hand is one that involves a set of stopping conditions and a function that calls itself computer scientists and computer science textbooks particularly from back in the day favor and are written in what are called functional languages in functional languages we try to avoid changing data that is given to a function in our first version of binary search we created first and last variables using the list and then modified first and last as we needed to arrive at a solution functional languages don't like to do this all this modification of variables and prefer a solution using recursion a language like python which is what we're using is the opposite and doesn't like recursion in fact python has a maximum recursion depth after which our function will halt execution python prefers an iterative solution now i mentioned all of this for two reasons if you decide that you want to learn how to implement the algorithm in a language of your choice that's not python then you might see a recursive solution as the best implementation in that particular language i'm an ios developer for example and i work with a language called swift swift is different from python in that it doesn't care about recursion depth and does some neat tricks where it doesn't even matter how many times your function calls itself so if you want to see this in swift code then you need to know how recursion works well and now you have some idea now the second reason i bring it up is actually way more important and to find out on to the next video at the beginning of this series i mentioned that there were two ways of measuring the efficiency of an algorithm the first was time complexity or how the run time of an algorithm grows as n grows larger the second is space complexity we took a pretty long route to build up this example but now we're in a good place to discuss space complexity space complexity is a measure of how much working storage or extra storage is needed as a particular algorithm grows we don't think about it much these days but every single thing we do on a computer takes up space in memory in the early days of computing considering memory usage was of paramount importance because memory was limited and really expensive these days were spoiled our devices are rich with memory this is okay when we write everyday code because most of us aren't dealing with enormously large data sets when we write algorithms however we need to think about this because we want to design our algorithms to perform as efficiently as it can as the size of the data set n grows really large like time complexity space complexity is measured in the worst case scenario using bigo notation since you are familiar with the different kinds of complexities let's dive right into an example in our iterative implementation of binary search the first one we wrote that uses a while loop let's look at what happens to our memory usage as n gets large let's bring up that function let's say we start off with a list of 10 elements now inspecting the code we see that our solution relies heavily on these two variables first and last first points to the start of the list and last to the end when we eliminate a set of values we don't actually create a sub list instead we just redefine first and last as you see here to point to a different section of the list since the algorithm only considers the values between first and last when determining the midpoint by redefining first and last as the algorithm proceeds we can find a solution using just the original list this means that for any value of n the space complexity of the iterative version of binary search is constant or that the iterative version of binary search takes constant space remember that we would write this as big o of one this might seem confusing because as n grows we need more storage to account for that larger list size now this is true but that storage is not what space complexity cares about measuring we care about what additional storage is needed as the algorithm runs and tries to find a solution if we assume something simple say that for a given size of a list represented by a value n it takes n amount of space to store it whatever that means then for the iterative version of binary search regardless of how large the list is at the start middle and end of the algorithm process the amount of storage required does not get larger than n and this is why we consider it to run in constant space now this is an entirely different story with the recursive version however in the recursive version of binary search we don't make use of variables to keep track of which portion of the list we're working with instead we create new lists every time with a subset of values or sublists with every recursive function call let's assume we have a list of size n and in the worst case scenario the target element is the last in the list calling the recursive implementation of binary search on this list and target would lead to a scenario like this the function would call itself and create a new list that goes from the midpoint to the end of the list since we're discarding half the values the size of the sub list is n by 2. this function will keep calling itself creating a new sub list that's half the size of the current one until it arrives at a single element list and a stopping condition this pattern that you see here where the size of the sublist is reduced by a factor on each execution of the algorithmic logic well we've seen that pattern before do you remember where this is exactly how binary search works it discards half the values every time until it finds a solution now we know that because of this pattern the running time of binary search is logarithmic in fact the space complexity of the recursive version of binary search is the same if we start out with a memory allocation of size n that matches the list on each function call of recursive binary search we need to allocate additional memory of size n by 2 n by 4 and so on until we have a sub list that is either empty or contains a single value because of this we say that the recursive version of the binary search algorithm runs in logarithmic time with a big o of log n now there's an important caveat here this totally depends on the language remember how i said that a programming language like swift can do some tricks to where recursion depth doesn't matter the same concept applies here if you care to read more about this concept it's called tail optimization it's called tail optimization because if you think of a function as having a head and a tail if the recursive function call is the last line of code in the function as it is in our case we call this tail recursion since it's the last part of the function that calls itself now the trick that swift does to reduce the amount of space and therefore computing overhead to keep track of this recursive calls is called tail call optimization or tail call elimination it's one of those things that you'll see thrown around a loss in algorithm discussions but may not always be relevant to you now what if any of this is relevant to us well python does not implement tail call optimization so the recursive version of binary search takes logarithmic space if we had to choose between the two implementations given that time complexity or run time of both versions the iterative and the recursive version are the same we should definitely go with the iterative implementation in python since it runs in constant space okay that was a lot but all of this with all of this we've now established two important ways to distinguish between algorithms that handle the same task and determine which one we should use we've arrived at what i think is a good spot to take a long break and let all of these new concepts sink in but before you go off to the next course let's take a few minutes to recap everything we've learned so far while we did implement two algorithms in this course in actual code much of what we learned here was conceptual and will serve as building blocks for everything we're going to learn in the future so let's list all of it out the first thing we learned about and arguably the most important was algorithmic thinking algorithmic thinking is an approach to problem solving that involves breaking a problem down into a clearly defined input and output along with a distinct set of steps that solves the problem by going from input to output algorithmic thinking is not something you develop overnight by taking one course so don't worry if you're thinking i still don't truly know how to apply what i learned here algorithmic thinking sinks in after you go through several examples in a similar fashion to what we did today it also helps to apply these concepts in the context of a real example which is another thing we will strive to do moving forward regardless it is important to keep in mind that the main goal here is not to learn how to implement a specific data structure or algorithm off the top of your head i'll be honest i had to look up a couple code snippets for a few of the algorithms myself in writing this course but in going through this you now know that binary search exists and can apply to a problem where you need a faster search algorithm unlike most courses where you can immediately apply what you have learned to build something cool learning about algorithms and data structures will pay off more in the long run the second thing we learned about is how to define and implement algorithms we've gone over these guidelines several times i won't bore you here again at the end but i will remind you that if you're often confused about how to effectively break down a problem in code to something more manageable following those algorithm guidelines is a good place to start next we learned about big o and measuring the time complexity of algorithms this is a mildly complicated topic but once you've abstracted the math away it isn't as hazy a topic as it seems now don't get me wrong the math is pretty important but only for those designing and analyzing algorithms our goal is more about how to understand and evaluate algorithms we learned about common run times like constant linear logarithmic and quadratic runtimes these are all fairly new concepts but in time you will immediately be able to distinguish the runtime of an algorithm based on the code you write and have an understanding of where it sits on an efficiency scale you will also in due time internalize runtimes of popular algorithms like the fact that binary search runs in logarithmic time and constant space and be able to recommend alternative algorithms for a given problem all in all over time the number of tools in your tool belt will increase next we learned about two important search algorithms and the situations in which we select one over the other we also implemented these algorithms in code so that you got a chance to see them work we did this in python but if you are more familiar with a different language and haven't gotten the chance to check out the code snippets we've provided you should try your hand at implementing it yourself it's a really good exercise to go through finally we learned about an important concept and a way of writing algorithmic code through recursion recursion is a tricky thing and depending on the language you write code with you may run into it more than others it is also good to be aware of because as we saw in our implementation of binary search whether recursion was used or not affected the amount of space we used don't worry if you don't fully understand how to write recursive functions i don't truly know either the good part is you can always look these things up and understand how other people do it anytime you encounter recursion in our courses moving forward you'll get a full explanation of how and why the function is doing what it's doing and that brings us to the end of this course i'll stress again that the goal of this course was to get you prepared for learning about more specific algorithms by introducing you to some of the tools and concepts you will need moving forward so if you're sitting there thinking i still don't know how to write many algorithms or how to use algorithmic thinking that's okay we'll get there just stick with it as always have fun and happy coding hi my name is passant i'm an instructor at treehouse and welcome to the introduction to data structures course in this course we're going to answer one fundamental question why do we need more data structures than a programming language provides before we answer that question some housekeeping if you will in this course we're going to rely on concepts we learned in the introduction to algorithms course namely bigo notation space and time complexity and recursion if you're unfamiliar with those concepts or just need a refresher check out the prerequisites courses listed in addition this course does assume that you have some programming experience we're going to use data structures that come built into nearly all programming languages as our point of reference while we will explain the basics of how these structures work we won't be going over how to use them in practice if you're looking to learn how to program before digging into this content check the notes section of this video for helpful links if you're good to go then awesome let's start with an overview of this course the first thing we're going to do is to explore a data structure we are somewhat already familiar with arrays if you've written code before there's a high chance you have used an array in this course we're going to spend some time understanding how arrays work what are the common operations on an array and what are the run times associated with those operations once we've done that we're going to build a data type of our own called a linked list in doing so we're going to learn that there's more than one way to store data in fact there's way more than just one way we're also going to explore what motivates us to build specific kinds of structures and look at the pros and cons of these structures we'll do that by exploring four common operations accessing a value searching for a value inserting a value and deleting a value after that we're actually going to circle back to algorithms and implement a new one a sorting algorithm in the introductions to algorithms course we implemented a binary search algorithm a precondition to binary search was that the list needed to be sorted we're going to try our hand at sorting a list and open the door to an entirely new category of algorithms we're going to implement our sorting algorithm on two different data structures and explore how the implementation of one algorithm can differ based on the data structure being used we'll also look at how the choice of data structure potentially influences the run time of the algorithm in learning about sorting we're also going to encounter another general concept of algorithmic thinking called divide and conquer along with recursion dividing conquer will be a fundamental tool that we will use to solve complex problems all in due time in the next video let's talk about arrays a common data structure built into nearly every programming language is the array arrays are a fundamental data structure and can be used to represent a collection of values but it is much more than that arrays are also used as building blocks to create even more custom data types and structures in fact in most programming languages text is represented using the string type and under the hood strings are just a bunch of characters stored in a particular order in an array before we go further and dig into arrays what exactly is a data structure a data structure is a way of storing data when programming it's not just a collection of values and the format they're stored in but the relationship between the values in the collection as well as the operations applied on the data stored in the structure an array is one of very many data structures in general an array is a data structure that stores a collection of values where each value is referenced using an index or a key a common analogy for thinking about arrays is as a set of train cars each car has a number and these cars are ordered sequentially inside each car the array or the train in this analogy stores some data while this is the general representation of an array it can differ slightly from one language to another but for the most part all these fundamentals remain the same in a language like swift or java arrays are homogeneous containers which means they can only contain values of the same type if you use an array to store integers in java it can only store integers in other languages arrays are heterogeneous structures that can store any kind of value in python for example you can mix numbers and text with no issues now regardless of this nuance the fundamental concept of an array is the index this index value is used for every operation on the array from accessing values to inserting updating and deleting in python the language we're going to be using for this course it's a tiny bit confusing the type that we generally refer to as an array in most languages is best represented by the list type in python python does have a type called array as well but it's something different so we're not going to use it while python calls it a list when we use a list in this course we'll be talking about concepts that apply to arrays as well in other languages so definitely don't skip any of this there's one more thing in computer science a list is actually a different data structure than an array and in fact we're going to build a list later on in this course generally though this structure is called a linked list as opposed to just list so hopefully the terminology isn't too confusing to properly understand how arrays work let's take a peek at how arrays are stored under the hood an array is a contiguous data structure this means that the array is stored in blocks of memory that are right beside each other with no gaps the advantage of doing this is that retrieving values is very easy in a noncontiguous data structure we're going to build one soon the structure stores a value as well as a reference to where the next value is to retrieve that next value the language has to follow that reference also called a pointer to the next block of memory this adds some overhead which as you will see increases the runtime of common operations a second ago i mentioned that depending on the language arrays can either be homogeneous containing the same type of value or heterogeneous where any kind of value can be mixed this choice also affects the memory layout of the array for example in a language like c swift or java where arrays are homogeneous when an array is created since the kind of value is known to the language compiler and you can think of the compiler as the brains behind the language it can choose a contiguous block of memory that fits the array size and values created if the values were integers assuming an integer took up space represented by one of these blocks then for a five item array the compiler can allocate five blocks of equally sized memory in python however this is not the case we can put any value in a python list there's no restriction the way this works is a combination of contiguous memory and the pointers or references i mentioned earlier when we create a list in python there is no information about what will go into that array which makes it hard to allocate contiguous memory of the same size there are several advantages to having contiguous memory since the values are stored beside each other accessing the values happens in almost constant time so this is a characteristic we want to preserve the way python gets around this is by allocating contiguous memory and storing init not the value we want to store but a reference or a pointer to the value that's stored somewhere else in memory by doing this it can allocate equally sized contiguous memory since regardless of the value size the size of the pointer to that value is always going to be equal this incurs an additional cost in that when a value is accessed we need to follow the pointer to the block of memory where the value is actually stored but python has ways of dealing with these costs that are outside the scope of this course now that we know how an array stores its values let's look at common operations that we execute on an array regardless of the kind of data structure you work with all data structures are expected to carry out four kinds of operations at minimum we need to be able to access and read values stored in the structure we need to be able to search for an arbitrary value we also need to be able to insert a value at any point into the structure and finally we need to be able to delete structures let's look at how these operations are implemented on the array structure in some detail starting with access elements in an array are identified using a value known as an index and we use this index to access and read the value most programming languages follow a zerobased numbering system when it comes to arrays and all this means is that the first index value is equal to zero not one generally speaking when an array is declared a base amount of contiguous memory is allocated as the array storage computers refer to memory through the use of an address but instead of keeping a reference to all the memory allocated for an array the array only has to store the address of the first location because the memory is contiguous using the base address the array can calculate the address of any value by using the index position of that value as an offset if you want to be more specific think of it this way let's say we want to create an array of integers and then each integer takes up a certain amount of space in memory that we'll call m let's also assume that we know how many elements we're going to create so the size of the array is some number of elements we'll call n the total amount of space that we need to allocate is n times the space per item m if the array keeps track of the location in memory where the first value is held so let's label that m0 then it has all the information it needs to find any other element in the list when accessing a value in an array we use the index to get the first element in the list we use the zeroth index to get the second we use the index value 1 and so on given that the array knows how much storage is needed for each element it can get the address of any element by starting off with the address for the first element and adding to that the index value times the amount of storage per element for example to access the second value we can start with m0 and to that add m times the index value 1 giving us m1 as the location in memory for the second address this is a very simplified model but that's more or less how it works this is only possible because we know that array memory is contiguous with no gaps let's switch over to some code as i mentioned earlier we're going to be using python in this course if you don't know how to code or you're interested in this content but know a language other than python check the notes section of this video for more information while the code will be in python the concepts are universal and more importantly simple enough that you should have no issue following along in your favorite programming language and to get started click on the launch workspaces button on the video page that you're watching right now this should spin up an instance of a treehouse workspace an inbrowser coding environment right now your workspace should be empty and that's expected so let's add a new file in here i'm going to go to file new file and we'll call this arrays dot py pi creating a list in python is quite simple so we'll call this new underscore list we use a set of square brackets around a set of values to create a list so one and we comma separate them so space two and space three this allocates a base amount of memory for the array to use or when i say array know that in python i mean a list since this is python the values aren't stored in memory instead the values 1 2 and 3 are stored elsewhere in memory and the array stores references to each of those objects to access a value we use a subscript along with an index value so to get the first value we use the index 0 and if we were to assign this to another variable we would say result equal new list we write out new lists since this is the array that we're accessing the value from and then a subscript notation which is a square bracket and then the index value as we saw since the array has a reference to the base location in memory the position of any element can be determined pretty easily we don't have to iterate over the entire list all we need to do is a simple calculation of an offset from the base memory since we're guaranteed that the memory is contiguous for this reason access is a constant time operation on an array or a python list this is also why an array crashes if you try to access a value using an index that is out of bounds of what the array stores if you've used an array before you've undoubtedly run into an error or a crash where you try to access a value using an index that was larger than the number of elements in the array since the array calculates the memory address on the fly when you access a value with an out of bounds index as it's called the memory address returned is not one that's part of the array structure and therefore cannot be read by the array now in python this is represented by an index error and we can make this happen by using an index we know our array won't contain now i'm writing out my code here inside of a text editor which obviously doesn't run the code so let's drag up this console area here and i'm going to write python to bring up the python interpreter and in here we can do the same thing so i can say new list equal one comma two comma three and now this is an interpreter so it's actually going to evaluate our code all right so now we have a new list if i type out new list it gets printed out into the console okay i can also do new list square bracket 0 and you'll see that i get the value 1 which is the value stored at the zeroth index now to highlight that index error we can do new list and inside the square brackets we can provide an index that we know our array doesn't contain so here i'll say index 10 and if i hit enter you'll see it say index error list index out of range and those are the basics of how we create and read values from an array in the next video let's take a look at searching in the last video we learned what happens under the hood when we create an array and read a value using an index in this video we're going to look at how the remaining data structure operations work on arrays if you took the introduction to algorithms course we spent time learning about two search algorithms linear search and binary search while arrays are really fast at accessing values they're pretty bad at searching taking an array as is the best we can do is use linear search for a worst case linear runtime linear search works by accessing and reading each value in the list until the element in concern is found if the element we're looking for is at the end of the list then every single element in the list will have been accessed and compared even though accessing and comparing our constant time operations having to do this for every element results in an overall linear time let's look at how search works in code in python we can search for an item in an array in one of two ways we can use the in operator to check whether a list contains an item so i can say if one in new underscore list then print true the in operator actually calls a contains method that is defined on the list type which runs a linear search operation in addition to this we can also use a for loop to iterate over the list manually and perform a comparison operation so i can say for n in new list if n equals one then print true and then after that break out of the loop this is more or less the implementation of linear search if the array were sorted however we could use binary search but because sort operations incur a cost of their own languages usually stay away from sorting the list and running binary search since for smaller arrays linear search on its own may be faster now again remember that since this is an editor this is just a text file none of these lines of code are evaluated so you can try that out in here so we'll copy that we can come down here and say python and hit enter and then when it starts up we can paste in our list and now we can try what we just did so if one in new list print true and there you go it prints true now because we've already learned about linear and binary search in a previous course there's nothing new going on here what's more interesting to look at in my opinion is inserting and deleting values in an array let's start with inserting in general most array implementations support three types of insert operations the first is a true insert using an index value where we can insert an element anywhere in the list this operation has a linear runtime imagine you wanted to insert an item at the start of the list when we insert into the first position what happens to the item that is currently in that spot well it has to move to the next spot at index value one what happens to the second item at index position one that one moves to the next spot at index position two this keeps happening until all elements have been shifted forward one index position so in the worst case scenario inserting at the zeroth position of an array every single item in the array has to be shifted forward and we know that any operation that involves iterating through every single value means a linear runtime now the second way we can insert an item into an array is by appending appending although technically an insert operation in that it inserts an item into an existing array doesn't incur the same runtime cost because appends simply add the item to the end of the list we can simplify and say that this is constant time this is a constant time operation but it depends on the language implementation of array to highlight why that matters let's consider how lists in python work in python when we create a list the list doesn't know anything about the size of the list and how many elements we're going to store creating a new empty list like so so numbers equal and two empty brackets so this creates a list and allocates a space of size n plus one since n here is zero there are no elements in this array in this list space is allocated for a one element list to start off because the space allocated for the list and the space used by the list are not the same what do you think happens when we ask python for the length of this list so i can say len numbers we correctly get 0 back this means that the list doesn't use the memory allocation as an indicator of its size because as i mentioned it has allocated space for a one element list but it returns zero so it determines it in other ways okay so numbers this list currently has space for one element let's use the append method defined on the type to insert a number at the end of the list so you can say numbers dot append and i'll pass in 2. now the memory allocation and the size of the list are the same since the list contains one element now what if i were to do something like this numbers.append there needs to be a dot and i'll add another value 200. now since the list only has an allocation for one item at this point before it can add the new element to the list it needs to increase the memory allocation and thereby the size of the list it does this by calling a list resize operation list resizing is quite interesting because it shows the ingenuity in solving problems like this python doesn't resize the list to accommodate just the element we want to add instead in this case it would allocate four blocks of memory to increase the size to a total of four contiguous blocks of memory it does this so that it doesn't have to resize the list every single time we add an element but at very specific points the growth pattern of the list type in python is 0 4 8 16 25 35 46 and so on this means that as the list size approaches these specific values resize is called again if you look at when the size of the list is four this means that when appending four more values until the size of eight each of those append operations do not increase the amount of space taken at specific points however when resizing is triggered space required increases as memory allocation increases this might signify that the append method has a nonconstant space complexity but it turns out that because some operations don't increase space and others do when you average all of them out append operations take constant space we say that it has an amortized constant space complexity this also happens with insert operations if we had a four element array we would have four elements and a memory allocation of four an insert operation at that point doesn't matter where it happens on the list but at that point it would trigger a resize inserting is still more expensive though because after the resize every element needs to be shifted over one the last insert operation that is supported in most languages is the ability to add one list to another in python this is called an extend and looks like this so i'll say numbers now if you let me actually clear out the console oh actually you will let's exit python we'll clear this out so we're back at the top and we'll start again so i'll say numbers and we'll set it to an empty list and now we can say numbers dot extend and as an argument we're going to pass in a new list entirely so here we'll say 4 comma 5 comma 6 and then once i hit enter if i were to print out numbers you'll see that it now contains the values 4 5 and 6. so extend takes another list to add extend effectively makes a series of append calls on each of the elements in the new list until all of them have been appended to the original list this operation has a run time of big o of k where k represents the number of elements in the list that we're adding to our existing list the last type of operation we need to consider are delete operations deletes are similar to inserts in that when a delete operation occurs the list needs to maintain correct index values so where an insert shifts every element to the right a delete operation shifts every element to the left just like an insert as well if we delete the first element in the list every single element in the list needs to be shifted to the left delete operations have an upper bound of big o of n also known as a linear runtime now that we've seen how common operations work on a data structure that we're quite familiar with let's switch tracks and build our own data structure over the next few videos we're going to build a data structure that you may have worked with before a linked list before we get into what a linked list is let's talk about why we build data structures instead of just using the ones that come built into our languages each data structure solves a particular problem we just went over the basics of the array data structure and looked at the cost of common operations that we carry out on arrays we found that arrays were particularly good at accessing reading values happens in constant time but arrays are pretty bad at inserting and deleting both of which run in linear time linked lists on the other hand are somewhat better at this although there are some caveats and if we're trying to solve a problem that involves far more inserts and deletes than accessing a linked list can be a better tool than an array so what is a linked list a linked list is a linear data structure where each element in the list is contained in a separate object called a node a node models two pieces of information an individual item of the data we want to store and a reference to the next node in the list the first node in the linked list is called the head of the list while the last node is called the tail the head and the tail nodes are special the list only maintains a reference to the head although in some implementations it keeps a reference to the tail as well this aspect of linked lists is very important and as you'll see most of the operations on the list need to be implemented quite differently compared to an array the opposite of the head the tail denotes the end of the list every node other than the tail points to the next node in the list but tail doesn't point to anything this is basically how we know it's the end of the list nodes are what are called selfreferential objects the definition of a node includes a link to another node and selfreferential here means the definition of node includes the node itself linked lists often come in two forms a singly linked list where each node stores a reference to the next node in the list or a doubly linked list where each node stores a reference to both the node before and after if an array is a train with a bunch of cars in order then a linked list is like a treasure hunt when you start the hunt you have a piece of paper with the location of the first treasure you go to that location and you find an item along with a location to the next item of treasure when you finally find an item that doesn't also include a location you know that the hunt has ended now that we have a high level view of what a linked list is let's jump into code and build one together we'll focus on building a singly linked list for this course there are advantages to having a doubly linked list but we don't want to get ahead of ourselves let's start here by creating a new file we're going to put all our code for our linked list so we'll call this linked underscore list dot pi and first we're going to create a class to represent a node say class node now node is a simple object in that it won't model much so first we'll add a data variable it's an instance variable here called data and we'll assign the value none initially and then we'll add one more we'll call this next node and to this we'll assign none as well so we've created two instance variables data to hold on to the data that we're storing and next node to point to the next node in the list now we need to add a constructor to make this class easy to create so we'll add an init method here that takes self and some data to start off and all we're going to do is assign data to that instance variable we created so that's all we need to model node before we do anything else though let's document this so right after the class definition let's create a docs string so three quotes next line and we'll say an object for storing a single node of a linked list and then on the next line we'll say models two attributes data and the link to the next node in the list and then we'll close this doc string off with three more quotation marks okay using the node class is fairly straightforward so we can create a new instance of node with some data to store now the way we're going to do this is we're going to bring up the console and we're going to type out like we've been typing out before python followed by the name of the script that we wrote which is linked list linked underscore list.pi but before we do that we're going to pass an argument to the python command we're going to say dash or python i and then the name of the script linked underscore list dot pi so what this does is this is going to run the python repl the read evaluate print loop in the console but it's going to load the contents of our file into that so that we can use it so i'll hit enter and we have a new instance going and now we can use the node in here so we can say n1 equal node and since we defined that constructor we can pass it some data so we'll say 10 here now if we try to inspect this object the representation returned isn't very useful which will make things really hard to debug as our code grows so for example if i type out n1 you'll see that we have a valid instance here but it's not very helpful the way it's printed out so we can customize this by adding a representation of the object using the wrapper function now in the terminal still we'll type out exit like that hit enter to exit the console and then down here let's add in some room okay and here we'll say def double underscore wrapper another set of double underscores and then this function takes the argument self and in here we can provide a string representation of what we want printed to the console when we inspect that object inside of it inside of a console so here we'll say return again this is a string representation so inside quotes we'll say node so this represents a node instance and the data it contains here we'll say percent s which is a python way of substituting something into a string string interpolation and outside of the string we can say percent again and here we're saying we want to replace this percent s with self.data okay let's hit save and before we move on let's verify that this works so i'm going to come in here type clear to get rid of everything and then we'll do what we did again and you can just hit the up arrow a couple times to get that command all right so hit enter and now just so you know every time you run this you start off you know from scratch so n1 that we created earlier not there anymore so let's go ahead and create it n1 equal node 10 and we can type n1 again and hit enter and you have a much better representation now so we can see that we have a node and it contains the data 10. we can also create another one n2 equal node that contains the data 20 and now we can say n1.next n1.nextnode equal n2 so n1 now points to n2 and if we say n1.nextnode you'll see that it points to that node the node containing 20. nodes are the building blocks for a list and now that we have a node object we can use it to create a singly linked list so again i'm going to exit out of this and then go back to the text editor and here we'll create a new class so class linked list the linked list class is going to define a head and this attribute models the only node that the list is going to have a reference to so here we'll say head and we'll assign none initially and then like we did earlier let's create a constructor so double underscore init double underscore this takes self and then inside like before we'll say self dot head equal none this is the same as doing this so we can actually get rid of that and just use the constructor okay so again this head attribute models the only node that the list will have a reference to since every node points to the next node to find a particular node we can go from one node to the next in a process called list traversal so in the class constructor here we've set the default value of head to none so that new lists created are always empty again you'll notice here that i didn't explicitly declare the head attribute at the top of the class definition and don't worry that's not an oversight the self.head in the initializer means that it's still created okay so that's all there is to modeling a linked list now we can add methods that make it easier to use this data structure first a really simple docstring to provide some information so here we'll to create a docstring three quotation marks and then we'll say singly linked list and then close it off a common operation carried out on data structures is checking whether it contains any data or whether it's empty at the moment to check if a list is empty we would need to query these instance variables head and so on every time ideally we would like to not expose the inner workings of our data structure to code that uses it instead let's make this operation more explicit by defining a method so we'll say def is empty and this method takes self as an argument and here we'll say return self.head double equal none all we're doing here is checking to see if head is none if it is this condition evaluates to true which indicates the list is empty now before we end this video let's add one more convenience method to calculate the size of our list the name convenience method indicates that what this method is doing is not providing any additional functionality that our data structure can't handle right now but instead making existing functionality easier to use we could calculate the size of our linked list by traversing it every time using a loop until we hit a tail node but doing that every time is a hassle okay so we'll call this method size and as always it takes self unlike calling len on a python list not to be confused with a linked list which is a constant time operation our size operation is going to run in linear time the only way we can count how many items we have is to visit each node and call next until we hit the tail node so we'll start by getting a reference to the head we'll say current equal self.head let's also define a local variable named count with an initial value of 0 that will increment every time we visit a node once we hit the tail count will reflect the size of that list next we'll define a while loop that will keep going until there are no more nodes so say while current while current is the same as writing out while current does not equal none but it's more succinct so we'll go with this former if the ladder is more precise for you you can go with that now inside this loop we'll increment the count value so count plus equal one plus equal if you haven't encountered it before is the same as writing count equal count plus one so if count is zero initially so it's zero plus one is one and then we'll assign that back to count okay so count plus equal one next we're going to assign the next node in the list to current so current equal current dot next node this way once we get to the tail and call next node current will equal none and the while loop terminates so the end we can return count as you can see we need to visit every node to determine the size meaning our algorithm runs in linear time so let's document this up in our docs string which we'll add now to size we'll say returns the number of nodes in the list takes linear time let's take a break here we can now create lists check if they're empty and check the size in the next video let's start implementing some common operations at the moment we can create an empty list but nothing else let's define a method to add data to our list technically speaking there are three ways we can add data to a list we can add nodes at the head of the list which means that the most recent node we created will be the head and the first node we created will be the tail or we could flip that around most recent nodes are the tail of the list and the first node to be added is the head i mentioned that one of the advantages of linked lists over arrays is that inserting data into the list is much more efficient than to the array this is only true if we're inserting at the head or the tail technically speaking this isn't an insert and you'll often see this method called add prepend if the data is added to the head or append if it's added to the tail a true insert is where you can insert the data at any point in the list which is our third way of adding data we're going to circle back on that if we wanted to insert at the tail then the list needs a reference to the tail node otherwise we would have to start at the head and walk down the length of the list or traverse it to find the tail since our list only keeps a reference to the head we're going to add new items at the head of the list now before we add our new method i forgot that i didn't show you in the last video how to actually use the code we just added and how to check every time you know when we add new code that it works correctly so like before we're gonna bring up the console and here we're gonna say python dash i linked underscore list dot pi which should load it load the contents of our file and now we'll start here by creating a linked list so l equal linked list and then we'll use a node so n1 equal node with the value 10 and now we can assign n1 to the nodes or to the linked lists head attribute so l1 dot head equal n1 and then we can see if size works correctly so if we call l1 dot size and since this is a method we need a set of parentheses at the end and enter you'll see that we get back one correctly okay so it works now let's add our new method which we're going to call add add is going to accept some data to add to the list inside of a node so we'll say def add and every python method takes self as an argument and then we want to add some data to this node so we're going to say data for the second argument inside the method first we'll create a new node to hold on to the data so new underscore node equal node with the data before we set the new node as the head of the list we need to point the new node's next property at whatever node is currently at head this way when we set the new node as the head of the list we don't lose a reference to the old head so new underscore node dot next node equal self.head now if there was no node at head this correctly sets next node to none now we can set the new node as the head of the node so say self.head equal new underscore node because the insert operation is simply a reassignment of the head and next node properties this is a constant time operation so let's add that in as a docs string first what the method does so it adds a new node containing data at the head of the list this operation takes constant time which is our best case scenario okay let's test this out so i'm going to bring the console back up we'll exit out of our current reply and we'll load the contents of the file again and now we don't need to create a node like we did earlier so we can say l equal linked list l.add one okay let's see if this works we'll call size and if it worked the linked list should now have a size of one there we go you can also do l.add2 l.add three and l dot size should now be three there we go now if we i were to type l and just hit print again what we get in the repel is nothing useful so like before we'll implement the wrapper function for our linked list now i'm just going to copy paste this in and we'll walk through it okay so this is what our implementation of wrapper looks like for the linked list object you can grab this code from the notes section of this video okay so at the top you'll see a docs string where it says it returns a string representation of the list and like everything we need to do with a linked list we need to traverse it so this is going to take linear time we start by creating an empty list now i need to distinguish this is a python list not a linked list so we create an empty list called nodes and two nodes we're going to add strings that have a description that provide a description of each node but we're not going to use the description that we implemented in the node class because we're going to customize it a bit here next we start by assigning self.head to current so we sort of have a pointer to the head node as long as current does not equal none which means we're not at the tail we're going to implement some logic so in the first scenario if the node assigned to current is the same as the head then we're going to append this string to our nodes list and the string is simply going to say that hey this is a head node and it contains some data which will extract using current.data next scenario is if the node assigned to current's next node is none meaning we're at the tail node then we'll assign a different kind of string so it's the same as earlier except we're saying tail here and then finally in any other scenario which means we're not at the head or not of the tail we'll simply print the node's value inside and again we'll extract it using current.data with every iteration of the loop we'll move current forward by calling current.nextnode and reassigning it and then at the very end when we're done we'll join all the strings that are inside the nodes list together using the python join method and we'll say that with every join so when you join these two strings together to make one string you need to put this set of characters in between all right so let's see what this looks like so i'm going to come down here exit out of the console again clear it out load the contents of the file again and let's try that so we'll say l equal linked list all right so l dot add one l dot add two l dot add three that seems enough and then now if i type out l and hit enter we get a nice string representation of the list so you can see that we add every new node to the head so we added one first one ends up being the tail because it keeps getting pushed out then two and then finally three so three is at the head so far we've only implemented a single method which functions much like the append method on a python list or an array except it adds it to the start of the linked list it prepens it like append this happens in constant time in the next video let's add the ability to search through our list for the search operation we're going to define a method that takes a value to search for and returns either the node containing the value if the value is found or none if it isn't so right after actually you know what we'll make sure wrapper is the last function our last method in our class so we'll add it above it so here we'll say def search self and then key in the last video we implemented the wrapper method to provide a string representation of the list so we're going to use similar logic here to implement the search function we'll start by setting a local variable current to point to the head of the list while the value assigned to current is a valid node that is it isn't none we'll check if the data on that node matches the key that we're searching for so while current we'll say if current.data is the key then we'll return current if it does match we'll go ahead and return it like we've done here but if it doesn't we'll assign the next node in the list to current and check again so say else current equal current dot next node once we hit the tail node and haven't found the key current gets set to none and the while loop exits at this point we know the list doesn't contain the key so we can return none okay that completes the body of our method let's add a docs string to document this so up at the top we'll say search for the first node containing data that matches the key now this is important because if our linked list contains more than one node with the same value it doesn't matter we're going to return the first one with this implementation we'll also say here that it returns the node or none if not found in the worst case scenario we'll need to check every single node in the list before we find the key or fail and as a result this operation runs in linear time so i'll say takes o of n or linear time so far we haven't seen anything that indicates this data structure has any advantage over an array or a python list but we knew that i mentioned the strength of linked lists comes in inserts and deletes at specific positions we'll check that out in the next video but as always before we end this one let's make sure everything works so we'll load the contents of the file again l equal linked list and then we'll say l.add 10 l dot add 20 2 doesn't matter l dot add 45 and one more metal dot add 15. now we can say l.search and we need to give it a value so we'll say 45 and this returns a node or none so we'll say n equal and then we'll hit enter if this works n should be a node okay weirdly n does not work here at least it says it's not a node which means i made a mistake in typing out our code and looking at it immediately it's fairly obvious so this return none needs to be outside of the while loop okay so i'm going to hit save now so make sure it's on the same indentation here which means it's outside the while loop and then we'll run through this again okay so l is linked list l.add 10 l dot add 2 l.add 45 and what was the last one we did i believe it was 15 and now we should be able to say l.search remember we're assigning this to a node to a variable so l.search 45 and there you go we get that node back and we can hit l and we'll see a representation of our list okay so again in the next video inserts and deletes at specific positions insert operations on linked lists are quite interesting unlike arrays where when you insert an element into the array all elements after the particular index need to be shifted with a linked list we just need to change the references to next on a few nodes and we're good to go since each node points to the next one by swapping out these references we can insert a node at any point in the list in constant time much like binary search though there's a catch to find the node at that position we want to insert we need to traverse the list and get to that point we just implemented our search algorithm for the linked list type and we know that this runs in linear time so while actually inserting is fast finding the position in the list you want to insert it is not this is why i mentioned that there were some caveats to inserting anyway let's see what this looks like in code we'll define a method named insert that takes data to insert along with an index position so we'll do this after search right here say def insert and this takes some data to insert and a position to insert it at you may be thinking wait a minute linked lists don't have index positions right and you're correct but we can mimic that behavior by just counting the number of times we access next node if the index value passed into this argument is 0 that means we want to insert the new node at the head of the list this is effectively the same behavior as calling add which means the logic is the same so we don't need to repeat it we can call the add method we wrote earlier so we'll say if index if index equals 0 or if index is 0 then self dot add data if the index is greater than 0 then we need to traverse the list to find the current node at that index so if index is greater than zero now before we do that we need to create a new node containing the data we want to insert so we'll say new equal node with some data i'm going to assign index the argument passed to our function to a local variable named position and the head of the list to a variable named current position equal index current equal self.head every time we call current.nextnode meaning we're moving to the next node in the list we'll decrease the value of position by 1. when position is zero we'll have arrived at the node that's currently at the position we want to insert in in reality though we don't want to decrease it all the way to zero imagine we have a list with five nodes and we want to insert a node at position 3. to insert a node at position 3 we need to modify the nodes at positions 2 and 3. node 2's next node attribute is going to point to the new node and the new node's next node attribute will point to node 3. in this way an insert is a constant time operation we don't need to shift every single element we just modify a few next node references in a doubly linked list we can use node 3 to carry out both of these operations node 3 in a doubly linked list would have a reference to node 2 and we can use this reference to modify all the unnecessary links and a singly linked list though which is what we have if we kept decreasing position until we're at 0 we arrive at node 3. we can then set the new node's next node property to point to node 3 but we have no way of getting a reference to node 2 which we also need for this reason it's easier to decrease position to just 1 when it equals 1 and stop at node 2. so in here we'll say while position is greater than one now while the position is greater than one we'll keep calling next node and reassigning the current node so current equal node.next node and at the same time we'll decrement position so position equal to position minus one which you can also succinctly write as minus equal one this way when the position equals one the loop exits and current will refer to the node at the position before the insert point so outside the while loop we'll say previous equal current and next equal current dot next node to make things more clear what i've done here is name the node before the new one previous and the node after the new one next all that's left to do now is to insert the new node between previous and next so we'll say previous dot next node equal new and then new dot next node equal next now it seems like there's an issue with variable naming here and i'm most probably conflicting with some globally named next variable so actually go ahead and call this next node and previous node so that we don't mess things up here previous node so the dot next node is obviously the attribute on a node but this is just a local variable let's document this method so up at the top we'll add a docs string and it will say inserts a new node containing data at index position insertion takes constant time but finding the node at the insertion point takes linear time let's add this to the next line there we go and then we'll say therefore it takes an overall linear time this is why even though we can easily insert a new node without having to shift the rest ultimately adding to either the head or the tail if you have a reference is much more efficient we have one more operation to add to our linked list that will make it a robust data structure much like inserts removing a node is actually quite fast and occurs in constant time but to actually get to the node that we want to remove and modify the next connections we need to traverse the entire list in our worst case so in the worst case this takes linear time let's add this operation to our data structure there are two ways we can define the remove method one where we provide a key to remove as an argument and one where we provide an index now in the former the key refers to the data the node stores so in order to remove that node we would first need to search for data that matches the key i'm going to implement that first method which we'll call remove and i'll leave it up to you to get some practice in and implement a remove at index method to complete our data structure so we'll add this after the insert method right here remove is going to accept a key which we'll need to search for before we can remove a node earlier we defined a search method that found a node containing data that matches a key but we can't use that method as is for the implementation of remove when we remove a node much like the insert operation we need to modify the next node references the node before the match needs to point to the node after the match if we use the search method we defined earlier we get the node we want to remove as a return value but because this is a singly linked list we can't obtain a reference to the previous node like i said earlier if this was a doubly linked list we could use the search method since we would have a reference to that previous node we'll start here by setting a local variable named current to point to the head let's also define a variable named previous that will set to none to keep track of the previous node as we traverse the list finally let's declare a variable named found that we'll set to false found is going to serve as a stopping condition for the loop that we'll define we'll use the loop to keep traversing the linked list as long as found is false meaning we haven't found the key that we're looking for once we've found it we'll set found to true and the loop terminates so let's set up our loop so we'll say while current and not found here we're defining a while loop that contains two conditions first we tell the loop to keep iterating as long as current does not equal none when current equals none this means we've gone past the tail node and the key doesn't exist the second condition asks the loop to keep evaluating as long as not found equals true now this might be tricky because it involves a negation here right now found is set to false so not found not false equals true this not operator flips the value when we find the key and we set found to true not true not found we'll equal false then and the loop will stop the end in the while loop means that both conditions current being a valid node and not found equalling true both have to be true if either one of them evaluates to false then the loop will terminate now inside the loop there are three situations that we can run into first the key matches the current node's data and current is still at the head of the list this is a special case because the head doesn't have a previous node and it's the only node being referenced by the list let's handle this case so we'll say if current.data double equals the key and current is self.head which you can write out as current equal self.head or current is self.head now if we hit this case we'll indicate that we found the key by setting found to true and then this means that on the next pass this is going to evaluate to false because not true will be false and then the loop terminates once we do that we want to remove the current node and since it's the head node all we need to do is point head to the second node in the list which we can get by referencing the next node attribute on current self.head equal current.nextnode so when we do this there's nothing pointing to that first node so it's automatically removed the next scenario is when the key matches data in the node and it's a node that's not the head so here we'll say else if current dot data equal key if the current node contains the key we're looking for we need to remove it to remove the current node we need to go to the previous node and modify its next node reference to point to the node after current but first we'll set found to true and then we'll switch out the references so previous.nextnode equal current.nextnode so far we haven't written any code to keep track of the previous node we'll do that in our else case here so if we hit the else case it means that the current node we're evaluating doesn't contain the data that matches the key so in this case we'll make previous point to the current node and then set current to the next node so previous equal current and current equal current.nextnode and that's it for the implementation of remove now we're not doing anything at the moment with the node we're removing but it's common for remove operations to return the value being removed so at the bottom outside the while loop let's return current and with that we have a minimal implementation of a linked list and your first custom data structure how cool is that there's quite a bit we can do here to improve our data structure particularly in making it easy to use but this is a good place to stop before we move on to the next topic let's document our method so the top another docs string and here we'll say removes node containing data that matches the key also it returns the node or none if the key doesn't exist and finally this takes linear time because in the worst case scenario we need to search the entire list if you'd like to get in some additional practice implementing functionality for linked lists two methods you can work on are remove it index and node at index to allow you to easily delete or read values in a list at a given index now that we have a linked list let's talk about where you can use them the honest answer is not a lot of places linked lists are really useful structures to build for learning purposes because they're relatively simple and are a good place to start to introduce the kinds of operations we need to implement for various data structures it is quite rare however that you will need to implement a linked list on your own there are typically much better and by that i mean much more efficient data structures that you can use in addition many languages like java for example provide an implementation of a linked list already now that we have a custom data structure let's do something with it let's combine the knowledge we have and look at how a sorting algorithm can be implemented across two different data structures now that we've seen two different data structures let's circle back and apply what we know about algorithms to these new concepts one of the first algorithms you learned about was binary search and we learned that with binary search there was one precondition the data collection needs to be sorted over the next few videos let's implement the merge sort algorithm which is one of many sorting algorithms on both arrays or python lists and the singly linked list we just created this way we can learn a new sorting algorithm that has real world use cases and see how a single algorithm can be implemented on different data structures before we get into code let's take a look at how merge sort works conceptually and we'll use an array to work through this we start with an unsorted array of integers and our goal is to end up with an array sorted in ascending order merge sort works like binary sort by splitting up the problem into sub problems but it takes the process one step further on the first pass we're going to split the array into two smaller arrays now in binary search one of these subarrays would be discarded but that's not what happens here on the second pass we're going to split each of those subarrays into further smaller evenly sized arrays and we're going to keep doing this until we're down to single element arrays after that the merge sort algorithm works backwards repeatedly merging the single element arrays and sorting them at the same time since we start at the bottom by merging to single element arrays we only need to make a single comparison to sort the resulting merge array by starting with smaller arrays that are sorted as they grow merge sort has to execute fewer sort operations than if it sorted the entire array at once solving a problem like this by recursively breaking down the problem into subparts until it is easily solved is an algorithmic strategy known as divide and conquer but instead of talking about all of this in the abstract let's dive into the code this way we can analyze the runtime as we implement it for our first implementation of merge sort we're going to use an array or a python list while the implementation won't be different conceptually for a linked list we will have to write more code because of list traversal and how nodes are arranged so once we have these concepts squared away we'll come back to that let's add a new file here we'll call this merge underscore sort dot pi in our file let's create a new function named merge sort that takes a list and remember when i say list unless i specify linked list i mean a python list which is the equivalent of an array so we'll say def merge underscore sort and takes a list in the introduction to algorithms course we started our study of each algorithm by defining the specific steps that comprise the algorithm let's write that out as a docstring in here the steps of the algorithm so that we can refer to it right in our code this algorithm is going to sort the given list in an ascending order so we'll start by putting that in here as a simple definition sorts a list in ascending order there are many variations of merge sort and in the one we're going to implement we'll create and return a new sorted list other implementations will sort the list we pass in and this is less typical in an operation known as sort in place but i think that returning a new list makes it easier to understand the code now these choices do have implications though and we'll talk about them as we write this code for our next bit of the docs string let's write down the output of this algorithm so returns a new sorted list merge sort has three main steps the first is the divide step where we find the midpoint of the list so i'll say divide find the mid point of the list and divide into sublists the second step is the conquer step where we sort the sublist that we created in the divide step so we'll say recursively sort the sublists created in previous step and finally the combine the combined step where we merge these recursively sorted sublists back into a single list so merge the sorted sublists created in previous step when we learned about algorithms we learned that a recursive function has a basic pattern first we start with a base case that includes a stopping condition after that we have some logic that breaks down the problem and recursively calls itself our stopping condition is our end goal a sorted array now to come up with a stopping condition or a base case we need to come up with the simplest condition that satisfies this end result so there are two possible values that fit a single element list or an empty list now in both of these situations we don't have any work to do if we give the merge sort function an empty list or a list with one element it's technically already sorted we call this naively sorting so let's add that as our stopping condition we'll say if len list if the length of the list is less than or equal to one then we can return the list okay so this is a stopping condition and now that we have a stopping condition we can proceed with the list of steps first we need to divide the list into sub lists to make our functions easier to understand we're going to put our logic in a couple different functions instead of one large one so i'll say it left half comma right half equal split list so here we're calling a split function that splits the list we pass in and returns two lists split at the midpoint because we're returning two lists we can capture them in two variables now you should know that this split function is not something that comes built into python this is a global function that we're about to write next is the conquer step where we sort each sublist and return a new sorted sublist so we'll say left equal merge sort left half and right equal merge sort right half this is the recursive portion of our function so here we're calling merge sort on this divided sub list so we divide the list into two here and then we call merge sort on it again this further splits that sublist into two in the next pass through of merge sort this is going to be called again and again and again until we reach our stopping condition where we have single element lists or empty lists when we've subdivided until we cannot divide any more then we'll end up with a left and a right half and we can start merging backwards so we'll say return merge left and right that brings us to the combined step once two sublists are sorted and combined we can return it now obviously none of these functions merge merge sort well merge sort is written but merge and split haven't been written so all we're going to do here if we run it is raise an error so in the next video let's implement the split operation the first bit of logic we're going to write is the divide step of the algorithm this step is fairly straightforward and only requires a few lines of code but is essential to get the sorting process going all right so as we saw earlier we're going to call the function for the divide step split so we'll say def split and split is going to take as an argument a list to split up let's document how this function works so we'll say divide the unsorted list at midpoint into sub lists and it's always good to say what we're returning as well so we'll say returns to sublists left and right all right so the first step is to determine the midpoint of this list of this array we're going to use the floor division operator for this floor division carries out a division operation and if we get a noninteger value like 2.5 back it just gets rounded down to two we'll define the midpoint to be the length of the list divided by two and then rounded down so lan list and using the two forward slashes for the floor division operator we'll put number two after it okay once we have the midpoint we can use the slicing notation in python to extract portions of the list we want to return for instance we can define left as the left sublist that goes all the way from the start of the list all the way up to the midpoint without including the midpoint now over here we're using the slicing syntax where it's like using the you know subscript notation to access a value from a list but instead we give two index values as a start and stop if we don't include a start value as i've done here python interprets that as starting from the zeroth index or the start of the list now similarly we can define right to be values on the right of the midpoint so starting at the midpoint and going all the way up to the end of the list so a couple things to note as i said earlier when you don't include the starting index it interprets it as to start at the very beginning of the list the index you give as the stopping condition that value is not included in the slice so over here we're starting at the very beginning of list and we go all the way up to midpoint but not including midpoint and then right starts at midpoint so it includes that value and then goes all the way to the end of the list now once we have these two sublists we can return them so we'll return left and right notice that we're returning two values here and then in the merge sort function when we call that split function we're declaring two variables left half and right half to assign so that we can assign these two sub lists to them okay and that's all there is to the split function in the next video let's implement the crucial portion of the merge sort logic once we run the split function recursively over the array we should end up with several single member or empty arrays at this point we need to merge them all back and sort them in the process which is what our merge function is for the merge function is going to take two arrays or lists as arguments and to match the naming conventions we used in the split function we'll call this left and right as well so we'll say def merge takes a left and a right list now like before let's add some documentation to our function so this function merges to lists or arrays sorting them in the process and then it returns a new merged list since our function is going to return a new list let's start by creating one now in the process of merging we need to sort the values in both lists to sort we need to compare values from each array or each list so next let's create two local variables to keep track of index values that we're using for each list so the convention here is i and j so we'll stick to it so i equals 0 j equals 0. as we inspect each value in either list we'll use the variables to keep track of the indexes of those values so we'll use i to keep track of indexes in the left list and j for indexes in the right list when merging we want to keep sorting the values until we've worked through both lists so for our loop let's set up two conditions with an and operator so we'll say while let's just stay up here while i is less than while i is less than the length of the left list and j is less than the length of the right list then we'll keep executing our loop so here we're ensuring that as long as i is less than the length of the left list and the and is important and j is less than the length of the right list we're going to keep executing the code now i and j are both set to zero initially which means that our first comparison operation will be on the first element of each list respectively so we'll say if left i so i zero so this is going to get the first value out of the left list is less than right j and again here j is zero so we're going to get the first value out of the right list now if the value at index i in the left list is less than the value at index j in the right list what do we do well that means the value being compared in left is less than the value in the right and can be placed at position 0 in the new array l that we created earlier so here we'll say l dot append left i since we've read and done something with the value at position i let's increment that value so we move forward to evaluate the next item in the left list i plus one or we can say i plus equal one okay next is an else statement and here we'll say if the value at index i so i don't have to write out the actual logic because it's implied so here we're saying that left the value at left is less than the value at right now in the else clause if the value at so i equal is greater and i haven't written out that condition because it's implied so here we're saying if the value in the left is less than the value in the right so in the else clause it's going to mean that the value in the left is either greater than or equal to the value in the right but when we hit the else clause if the value at index i in the left list is greater then we place the value at index j from the right list at the start of the new one list l and similarly increment j so here we'll say l dot append right j and then j equal j plus one doing this doesn't necessarily mean that in one step we'll have a completely sorted array but remember that because we start with single element arrays and combine with each merge step we will eventually sort all the values more than one time and by the time the entire process is done all the values are correctly sorted now this isn't all we need to do in the merge step however there are two situations we can run into one where the left array is larger than the right and vice versa so this can occur when an array containing an odd number of elements needs to be split so how do you split a three element array or list well the left can have two elements and the right can have one or the other way around in either case our while loop uses an and condition where the variables used to store the indexes need to be less than the length of the lists if the left list is shorter than the right then the first condition returns false and the entire loop returns false because it's an and condition this means that in such an event when the while loop terminates not all the values in the right list will have been moved over to the new combined list so to account for this let's add two more while loops the first while loop is going to account for a situation where the right list is shorter than the left and the previous loop terminated because we reached the end of the right list first so in this case what we're going to do is simply add the remaining elements in the left to the new list we're not going to compare elements because we're going to assume that within a list the elements are already sorted so while i is less than length of left then it's the same logic l dot append left i and i plus equal one so the while loop is going to have the similar condition keep the loop going until it's at the last index inside the body we're incrementing the index with every iteration of the loop our final loop accounts for the opposite scenario where the left was shorter than the right the only difference here is that we're going to use the variable j along with the right list so we'll say while j is less than length of right l dot append right j and j plus equal one okay let's stop here in the next video let's test out merge sort make sure our code is running correctly and everything is written well and then we'll wrap up this stage by documenting our code and evaluating the run time of our algorithm in the last video we completed our implementation for the merge sort algorithm but we didn't test it in any way let's define a new list at the bottom that contains several numbers you can put whatever you want in there but make sure that the numbers are not in order i'll call mine a list and in here we'll say 54 26 or 62 doesn't matter 93 17 77 31 just add enough so that you can make out that it's sorted okay next we're going to call the merge sort algorithm and pass in our list let's assign this to some variables so we'll say l equal merge underscore sort a list and then if it works correctly we should be able to print this list and see what it looks like so i'm going to hit save down here in the console we'll tap out python merge sort dot pi and before i hit enter i actually noticed i made an error in the last video but i'll hit enter anyway and you should see the error pop up okay so what i forgot to do which is a pretty crucial part of our algorithm is in the merge function i forgot to return the list containing the sorted numbers after carrying out all this logic so here at the bottom we'll say return l all right we'll save again and now we'll clear this out and try that one more time and there we go you should see a sorted list printed out we can write out a more robust function to test this because with bigger arrays visually evaluating that printed list won't always be feasible so bring this back down let's get rid of this and we'll call our function verify sorted and this will take a list first we're going to check inside the body of the function we'll check the length of the list if the list is a single element list or an empty list we don't need to do any unnecessary work because remember it is naively sorted so we'll say if n equals 0 or if n equals 1 then we'll return true we've verified that it's sorted now to conclude our function we're going to write out one line of code that will actually do quite a bit of work so first we'll say return list zero so we'll take the first element out of the list and we'll compare and see if that's less than the second element in the list okay so first we'll check that the first element in the list is less than the second element in the list this returns either true or false so we can return that directly but this isn't sufficient if it were we could trick the verify function by only sorting the first two elements in the list so to this return statement we're going to use an and operator to add on one more condition for this condition we're going to make a recursive function call back to verify sorted and for the argument we're going to pass in the list going from the second element all the way to the end let's visualize how this would work we'll use a five element list as an example so we'll call verify sorted and pass in the entire list this list is not one or zero elements long so we skip that first if statement there's only one line of code left in the function and first we check that the element at index 0 is less than the element at index 1. if this is false the function returns immediately with a false value an and operator requires both conditions to be true for the entire line of code to return true since the first condition evaluates to false we don't need to bother evaluating the second the second condition is a recursive call with a sublist containing elements from the original list starting at position 1 and going to the end so on the second call again we can skip that first if statement and proceed to check whether the value at element 0 is less than the value at element 1. remember that because this list is a sublist of the original starting at the element that was the second element in the original list by comparing the elements at position 0 and 1 in the sub list we're effectively comparing the elements at position 1 and 2 in the original list with each recursive call as we create new sub lists that start at index position 1 we're able to check the entire list without having to specify any checks other than the first two elements since this is a recursive function it means we need a stopping condition and we have it already it's that first if condition as we keep making sub lists once we reach a single element list that element is already sorted by definition so we can return true since this recursive function call is part of an and condition it means that every single recursive call has to return true all the way back to the beginning for our top level function to return true and for the function to say yes this is sorted now we could have easily done this using an iterative solution and a for loop but this way you get another example of recursion to work through and understand so let's use this function at the bottom we'll say print verify sorted and first we'll pass in a list oops we got rid of that didn't we okay let me write it out again so a list equal and i think i have those original numbers here somewhere so we'll say 54 26 93 okay and then we assigned to l the result of calling merge sort on a list okay so now here we're going to use the verify sorted function and we'll check first that a list is sorted that should return false and then we'll check the same call on we'll pass an l and this should return true okay so now at the bottom here in the console we'll call python merge sort dot pi and there we go it returned false for a list meaning it's not sorted but l is sorted cool so our merge sort function works in the next video let's talk about the cost of this algorithm if we go back to the top level the merge sort function what is the run time of this function look like and what about space complexity how does memory usage grow as the algorithm runs to answer those questions let's look at the individual steps starting with the split function in the split function all we're doing is finding the midpoint of the list and splitting the list at the midpoint this seems like a constant time operation but remember that the split function isn't called once it's called as many times as we need it to to go from the initial list down to a single element list now this is a pattern we've seen a couple times now and we know that overall this runs in logarithmic time so let's add that as a comment so here i'll say takes overall big o of log n time now there's a caveat here but we'll come back to that so next up is the merge step in the merge step we've broken the original list down into single element lists and now we need to make comparison operations and merge them back in the reverse order for a list of size n we will always need to make an n number of merge operations to get back from single element lists to a merge list this makes our overall runtime big o of n times log n because that's an n number of merge steps multiplied by log n number of splits of the original list so to our merge step here let's add a comment we'll say it runs in overall oops there we go runs an overall linear time right it takes an n number of steps number of merge steps but now that we have these two so linear here and logarithmic here we can multiply these and say that the merge sort function the top level function we can conclude that the runtime of the overall sorting process is big o of n times log n now what about that caveat i mentioned earlier so if we go back to our split function here right here there we go let's take a look at the way we're actually splitting the list so we're using python's list slicing operation here and passing in two indexes where the split occurs now if you go and poke around the python documentation which i've done it says that a slicing operation is not a constant time operation and in fact has a runtime of big o of k where k represents the slice size this means that in reality our implementation of split this implementation of split does not run in logarithmic time but k times logarithmic time because there is a slice operation for each split this means that our implementation is much more expensive so overall that makes our overall top level merge sort function not n times log n but k n times log n which is much more expensive now let's get rid of all that to fix this we would need to remove this slicing operation now we can do that by using a technique we learned in a previous course in the introduction to algorithms course we looked at two versions of binary search in python a recursive and an iterative version in the recursive one we use list slicing with every recursion call but we achieve the same end result using an iterative approach without using list slicing over there we declared two variables to keep track of the starting and ending positions in the list we could rewrite merge sort to do the same but i'll leave that as an exercise for you if you want some hints if you want any direction i've included a link in the notes with an implementation so that is time complexity now just so we know before moving on for python here our overall run time is not what i've listed here but this is what the actual run time of the merge sort algorithm looks like so the merge step runs in linear time and the split step takes logarithmic time for an overall n times log n and that is how merge sort actually works okay so what about space complexity the merge sort algorithm takes linear space and this is weird to think about it first but as always a visualization helps so if we start at the top again with our full list and carry out the split method until we have single element lists each of these new lists take up a certain amount of space so the second level here we have two lists where each take up an n by two amount of space now this makes it seem that the sum of all this space is the additional space needed for merge sort but that's not actually the case in reality there are two factors that make a difference first not every single one of these sub lists are created simultaneously at step two we create two n by two size sub lists when we move to the next step however we don't hold on to the n by two sub lists and then create four n by four size sub lists for the next split instead after the four n by four size sub lists are created the n by two ones are deleted from memory there's no reason to hold on to them any longer now the second point is that our code doesn't execute every path simultaneously think of it this way when we pass our list to the top level merge sort function our implementation calls split which returns a left half and a right half the next line of code then calls merge sort on the left half again this runs the function the merge sort function again with a new list in that second run of the function split is called again we get a second left and right half and then again like before we call merge sort on this left half as well what this means is that the code walks down the left path all the way down until that initial left half is sorted and merged back into one array then it's going to walk all the way down the right path and sort that until we're back to that first split with two n by two sized sublists essentially we don't run all these paths of code at once so the algorithm doesn't need additional space for every sublist in fact it is the very last step that matters in the last step the two sublists are merged back into the new sorted list and returned that sorted list has an equal number of items as the original unsorted list and since this is a new list it means that at most the additional space the algorithm will require at a given time is n yes at different points in the algorithm we require log n amount of space but log n is smaller than n and so we consider the space complexity of merge sort to be linear because that is the overall factor okay that was a lot so let's stop here don't worry if you've got questions about merge sort because we're not done yet over the next few videos let's wrap up this course by implementing merge sort on a linked list over the last few videos we implemented the merge sort algorithm on the array or list type in python merge sort is probably the most complicated algorithm we've written so far but in doing so we learned about an important concept divide and conquer we also concluded the last video by figuring out the run time of merge sort based on our implementation over the next few videos we're going to implement merge sort again this time on the linked list type in doing so we're going to get a chance to see how the implementation differs based on the data structure while still keeping the fundamentals of the algorithm the same and we'll also see how the run time may be affected by the kinds of operations we need to implement let's create a new file to put our second implementation of merge sort in so file over here new file and it's going to have a rather long name we'll call this linked list merge sort with underscores everywhere dot pi we're going to need the linked list class that we created earlier so we'll start at the top by importing the linked list class from the linkedlist.pi file the way we do that is we'll say from linked list import linked list right so that imports the class uh let's test if this works really quick we'll just do something like l equal linked list l.add ten or one doesn't matter print l okay and if i hit save and then down here we'll say python linked list merge sword dot pi okay it works so this is how we get some of the code how we reuse the code that we've written in other files into this current file and get rid of this now okay like we did with the first implementation of merge sort we're going to split the code up across three functions the main function merge sort a split function and a merge function now if you were to look up a merge sort implementation in python both for a regular list an array or a linked list you would find much more concise versions out there but they're kind of hard to explain so splitting it up into three will sort of help it you know be easier to understand so we'll call this merge sort at the top level and this time it's going to take a linked list let's add a dog string to document the function so say that this function sorts a linked list in ascending order and like before we'll add the steps in here so we'll say you first recursively divide the linked list into sub lists containing a single node then we repeatedly merge these sublists to produce sorted sublists until one remains and then finally this function returns a sorted linked list the implementation of this top level merge function is nearly identical to the array or list version we wrote earlier so first we'll provide a stopping condition or two if the size of the list is one or it's an empty list we'll return the linked list since it's naively sorted so if linked list dot size remember that function we run equal one then we'll return linked list else if linked list dot head is none meaning it's an empty list then we'll return linked list as well okay next let's split the linked list into a left and right half conceptually this is no different but in practice we need to actually traverse the list we'll implement a helper method to make this easier but we'll say left half comma right half equal split linked list now once we have two sub lists like before we can call merge sort the top level function on each so left equal merge sort left half and right equal merge sort on the right half finally we'll merge these two toplevel sublists and return it so merge left and right okay nothing new here but in the next video let's implement the split logic the next step in the merge sort algorithm is the divide step or rather an implementation of the split function so down here we'll call this split like before and this is going to take a linked list documenting things is good and we've been doing it so far so let's add a docstring divide the unsorted list at midpoint into sublists now of course when i say sublists here i mean sublinked lists but that's a long word to say now here's where things start to deviate from the previous version with the list type we could rely on the fact that finding the midpoint using an index and list slicing to split into two lists would work even if an empty list was passed in since we have no automatic behavior like that we need to account for this when using a linked list so our first condition is if the linked list is none or if it's empty that is if head is equal to none so we'll say if linked list equal none or you can write is there it doesn't matter or linked list dot head is none well linked list can be none for example if we call split on a linked list containing a single node a split on such a list would mean left would contain the single node while right would be none now in either case we're going to assign the entire list to the left half and assign none to the right so we'll say left half equal linked list and then right half equal none you could also assign the single element list or none to left and then create a new empty linked list assigned to the right half but that's unnecessary work so now that we've done this we can return left half and right half so that's our first condition let's add an else clause to account for nonempty linked lists first we'll calculate the size of the list now this is easy because we've done the work already and we can just call the size method that we've defined we'll say size equal linked underscore list dot size using this size we can determine the midpoint so mid equal size and here we'll use that floor division operator to divide it by two once we have the midpoint we need to get the node at that midpoint now make sure you hit command s to save here and we're going to navigate back to linkedlist.hi in here we're going to add a convenience method at the very bottom right before the wrapper function right here and this convenience method is going to return a node at a given index so i'll call this node at index and it's going to take an index value this way instead of having to traverse the list inside of our split function we can simply call node at index and pass it the midpoint index we calculated to give us the node right there so we can perform the split okay so this method accepts as an argument the index we want to get the node for if this index is zero then we'll return the head of the list so if index double equals zero return self.head the rest of the implementation involves traversing the linked list and counting up to the index as we visit each node the rest of the implementation involves traversing the linked list and counting up to the index as we visit each node so i'll add an else clause here and we'll start at the head so we'll say current equal self.head let's also declare a variable called position to indicate where we are in the list we can use a while loop to walk down the list our condition here is as long as the position is less than the index value so i'll say while position is less than index inside the loop we'll assign the next node to current and increment the value of position by one so current equal current dot next node position plus equal one once the position value equals the index value current refers to the node we're looking for and we can return it we'll say return current let's get rid of all this empty space there we go now back in linked list merge sort dot pi we can use this method to get at the node after we've calculated the midpoint to get the node at the midpoint of the list so we'll say mid node equal linked list dot node at index and here i'm going to do something slightly confusing i'm going to do mid minus 1. remember we're subtracting 1 here because we used size to calculate the midpoint and like the len function size will always return a value greater than the maximum index value so think of a linked list with two nodes size would return two the midpoint though and the way we're calculating the index we always start at zero which means size is going to be one greater than that so we're going to deduct one from it to get the value we want but we're using the floor division operator so it's going to round that down even more no big deal with the node at the midpoint now that we have this midnote we can actually split the list so first we're going to assign the entire linked list to a variable named left half so left half equal linked list this seems counterintuitive but make sense in a second for the right half we're going to assign a new instance of linked list so right half equal linked list this newly created list is empty but we can fix that by assigning the node that comes after the midpoint so after the midpoint of the original linked list we can assign the node that comes after that midpoint node as the head of this newly created right linked list so here we'll say right half dot head equal mid node dot node once we do that we can assign none to the next node property on mid node to effectively sever that connection and make what was the mid node now the tail node of the left linked list so i'll say mid node dot next node equal none if that's confusing here's a quick visualization of what just happened we start off with a single linked list and find the midpoint the node that comes after the node at midpoint is assigned to the head of a newly created linked list and the connection between the midpoint node and the one after is removed we now have two distinct linked lists split at the midpoint and with that we can return the two sub lists so we'll return left half and right half in the next video let's tackle our merge function in the last video we defined an implementation for the version of the split function that works on linked lists it contained a tiny bit more code than the array or list version that was expected the merge function is no different because like with the split function after we carry out a comparison operation we also need to swap references to corresponding nodes all right let's add our merge function over here at the bottom below the split functions we'll call this merge and it's going to take a left and right now because this can get complicated we're going to document this function extensively and as always we're going to start with a doc string so we'll say that this function merges two linked lists sorting by data in the nodes and it returns a new merged list remember that in the merge step we're going to compare values across two linked lists and then return a new linked list with nodes where the data is sorted so first we need to create that new linked list let's add a comment in here we'll say create a new linked list that contains nodes from let's add a new line merging left and right okay and then create the list so merged equal new linked list to this list we're going to do something unusual we're going to add a fake head this is so that when adding sorted nodes we can reduce the amount of code we have to write by not worrying about whether we're at the head of the list once we're done we can assign the first sorted node as the head and discard the fake head now this might not make sense at first but not having to worry about whether the new linked list already contains a head or not makes the code simpler we'll add another comment and a fake hand that is discarded later we'll say merged dot add zero like we've been doing so far we'll declare a variable named current to point to the head of the list set current to the head of the linked list and then current equal merged dot head next we'll get a reference to the head on each of the linked lists left and right so we'll say obtain head nodes for left and right linked lists and here's call this left head equal left dot head and right hand equal right dot head okay so with that setup out of the way let's start iterating over both lists so another comment iterate over left and right as long or we'll say until the until we reach the tail node of either and we'll do that by saying while left head or right head so this is a pattern that we've been following all along we're going to iterate until we hit the tail nodes of both lists and we'll move this pointer forward every time so that we traverse the list with every iteration if you remember the logic behind this from the earlier version once we hit the tail note of one list if there are nodes left over in the other linked list we don't need to carry out a comparison operation anymore and we can simply add those nodes to the merged list the first scenario we'll consider is if the head of the left linked list is none this means we're already past the tail of left and we can add all the nodes from the right linked list to the final merge list so here i'll say if the head node of left is none we're past the tail add the node from the right from right to merged linked list so here we'll say if left head is none current dot next node remember current points to the head of the merge list that we're going to return so here we're setting its next node reference to the head node on the right link list so we'll say right head then when we do that we'll move the right head forward to the next node so let's say right head equal right hand dot next node this terminates the loop on the next iteration let's look at a visualization to understand why let's say we start off with a linked list containing four nodes so we keep calling split on it until we have lists with just a single head single node linked lists essentially so let's focus on these two down here that we'll call left and right we haven't implemented the logic for this part yet but here we would compare the data values and see which one is less than the other so we'll assume that left's head is lesser than right's head so we'll set this as the next node in the final merge list left is now an empty length list so left dot head equals none on the next pass through the loop left head is none which is the situation we just implemented here we can go ahead and now assign right head as the next note in the merge link list we know that right is also a singly linked list here's the crucial bit when we move the pointer forward by calling next node on the right node there is no node and the right link the right linked list is also empty now which means that both left head and right head are none and either one of these would cause our loop condition to terminate so what we've done here is encoded a stopping condition for the loop so we need to document this because it can get fuzzy so right above that line of code i'll say call next on right to set loop condition to false okay there's another way we can arrive at this stopping condition and that's in the opposite direction if we start with the right head being none so here we'll say i'm going to add another comment if oops not there there if the head node of right is none we're past the tail then we'll say add the tail node from left to merged linked list and then we'll add that condition we'll say else if right head is none now remember we can enter these even if left head is none we can still go into this condition we can still enter this if statement and execute this logic because the while loop the loop condition here is an or statement so even if left head is false if this returns true because there's a value there there's a node there the loop will keep going okay now in this case we want to set the head of the left linked list as the next node on the merge list so this is simply the opposite of what we did over here we'll set current dot next node equal to left head and then we'll move so after doing that we can move the variable pointing to left head forwards which as we saw earlier is past the tail node and then results in the loop terminating so we'll say left hand equal left head dot next node and we'll add that comment here as well so we'll say call next on left to set loop condition to false because here right head is none and now we make left head none these two conditions we looked at where either the left head or right head were at the tail nodes of our respective lists those are conditions that we run into when we've reached the bottom of our split where we have single element linked lists or empty linked lists let's account for our final condition where we're evaluating a node that is neither the head nor the tail of the list and this condition we need to reach into the nodes and actually compare the data values to one another before we can decide which node to add first to the merged list so here this is an else because we've arrived at our third condition third and final and here we'll say not at either tail node obtain no data to perform comparison operations so let's get each of those data values out of the respective nodes so that we can compare it so we'll say left data equal left head dot data and write data equal right head righthead.data okay what do we do next well we compare but first let's add a comment so we'll say if data on left is less than right set current to left node and then move actually we'll add this in a second so here we'll say if left data is less than write data then current dot next node equal left head and then we'll add a comment and we'll say move left head to next node on that list so we'll say left head equal left head dot next node just as our comment says we'll check if the left data is less than the right data if it is since we want a list in ascending order we'll assign the left node to be the next node in the merged list we'll also move the left head forward to traverse down to the next node in that particular list now if left is larger than right then we want to do the opposite so we'll go back to spaces another comment if data on left is greater than right set current to right node okay so else here we assign the right head to be the next node in the merge list so current.nextnode equal right head and then comment move right head to next node so right head equal right head dot next node okay after doing that we move the right head pointer to reference the next node in the right list and finally at the end of each iteration of the while loop so not here but two spaces back right make sure we're indented at the same level as the while so we got to go yep or not the same level as the wild but the same outer scope and then there we're going to say move current to next node so current equal current dot next node okay don't worry if this is confusing as always we'll look at a visualization in just a bit so we'll wrap up this function by discarding that fake head we set earlier setting the correct node as head and returning the linked list so we'll add a comment discard fake head and set first merged node as head so here we'll say head equal merged dot head dot next node and then merged dot head equal head and finally return merged okay we wrote a lot of code here a lot of it was comments but still it's a bunch let's take a quick break in the next video we'll test this out evaluate our results and determine the runtime of our algorithm okay first things first let's test out our code now we'll keep it simple because writing a robust verify function would actually take up this entire video instead i'll leave that up to you to try as homework okay so at the very end let's create a new linked list let's add a few notes to this so l add i'm going to copy paste this so it makes it easier for me not to have to retype a bunch so i'll add 10 uh then set 2 44 15 and something like 200. okay then we'll go ahead and print l so that we can inspect this list next let's create a declare variable here so we'll call this sorted linked list and to this we're going to assign the result of calling merge sort on l and then we'll print this so sorted linked list okay since we've taken care of all the logic we know that this gets added in as nodes and then let's see what this looks like all right so hit save and then bring up the console we're going to type out python linked list underscore merge sort dot pi and then enter okay so we see that linked list we first created remember that what we add first right that eventually becomes a tail or right yeah so 10 is the tail 200 is the last one so 200 is the head because i'm calling add it simply adds each one to the head of the list so here we have 10 to 44 15 and 200 in the order we added and then the sorted linked list sorts it out so it's 2 10 15 44 and 200. look at that a sorted linked list okay so let's visualize this from the top we have a linked list containing five nodes with integers 10 2 4 15 and 200 as data respectively our merge sort function calls split on this list the split function calls size on the list and gets back 5 which makes our midpoint 2. using this midpoint we can split the list using the node at index method remember that when doing this we deduct 1 from the value of mid so we're going to split here using an index value of 1. effectively this is the same since we're starting with an index value of 0 1 means we split after node 2. we assign the entire list to left half then create a new list and assign that to right half we can assign node 3 at index value 2 as the head of the right list and remove the references between node two and node three so far so good right okay so now we're back in the merge sort function after having called split and we have two linked lists let's focus on just the left half because if you go back and look at our code we're going to call merge sort on the left linked list again this means the next thing we'll do is run through that split process since this is a linked list containing two nodes this means that split is going to return a new left and right list each with one node again we're back in the merge sort function which means that we call merge sort on this left list again since this is a single node linked list on calling merge sort on it we immediately return before we split since we hit that stopping condition so we go to the next line of code which is calling merge sort on the right list as well but again we'll get back immediately because we hit that stopping condition now that we have a left and right that we get back from calling merge sort we can call merge on them inside the merge function we start by creating a new linked list and attaching a fake head then we evaluate whether either the left or the right head is none since neither condition is true we go to the final step where we evaluate the data in each node in this case the data in the right node is less than the left node so we assign the right node as the next node in the merge link list and move the right head pointer forward in the merge link list we move our current pointer forward to this new node we've added and that completes one iteration of the loop on the next iteration righthead now points to none since that was a single node list and we can assign the rest of the left linked list which is effectively the single node over to the merge link list here we discard the fake head move the next node up to be the correct head and return the newly merged sorted linked list remember that at this point because right head and left head pointed to none are while loop terminated so in this way we recursively split and repeatedly merge sublists until we're back with one sorted linked list the merge sort algorithm is a powerful sorting algorithm but ultimately it doesn't really do anything complicated it just breaks the problem down until it's really simple to solve remember the technique here which we've talked about before is called divide and conquer so i like to think of merge sort in this way there's a teacher at the front of the room and she has a bunch of books that she needs to sort into alphabetical order instead of doing all that work herself she splits that pile into two and hands it to two students at the front each of those students split it into two more and hand it to the four students seated behind them as each student does this eventually a bunch of single students has two books to compare and they can sort it very easily and hand it back to the student who gave it to them in front of them who repeats the process backwards so ultimately it's really simple work is just efficiently delegated now back to our implementation here let's talk about runtime so far other than the node swapping we had to do it seems like most of our implementation is the same right in fact it is including the problems that we ran into in the list version as well so in the first implementation of merge sort we thought we had an algorithm that ran in big o of n log n but turns out we didn't why well the python list slicing operation if you remember actually takes up some amount of time amounting to big o of k a true implementation of merge sort runs in quasilinear or log linear time that is n times log n so we almost got there but we didn't now in our implementation of merge sort on a linked list we introduce the same problem so if we go back up to the merge or rather the split function this is where it happens now swapping node references that's a constant time operation no big deal comparing values also constant time the bottleneck here like list slicing is in splitting a late list at the midpoint if we go back to our implementation you can see here that we use the node at index method which finds the node we want by traversing the list this means that every split operation incurs a big o of k cost where k here is the midpoint of the list effectively n by 2 because we have to walk down the list counting up the index until we get to that node given that overall splits take logarithmic time our split function just like the one we wrote earlier incurs a cost of big o of k log n so here we'll say it takes big o of k times log n now the merge function also like the one we wrote earlier takes linear time so that one is good that one runs in the expected amount of time so here we'll say runs in linear time and that would bring our overall run time so up at the merge sort function we can say this runs in big o of k n times log n it's okay though this is a good start and one day when we talk about constant factors and look at ways we can reduce the cost of these operations using different strategies we can come back and reevaluate our code to improve our implementation for now as long as you understand how merge sort works conceptually what the run time and space complexities look like and where the bottlenecks are in your code that's plenty of stuff if you're interested in learning more about how we would solve this problem check out the notes in the teachers video in the next video let's wrap this course up and with that let's wrap up this course in the prerequisite to this course introduction to algorithms we learned about basic algorithms along with some concepts like recursion and big o that set the foundation for learning about implementing and evaluating algorithms in this course we learned what a data structure is and how data structures go hand in hand with algorithms we started off by exploring a data structure that many of us use in our daytoday programming arrays or lists as they are known in python we take a peek under the hood at how arrays are created and stored and examine some of the common operations carried out on arrays these are operations that we write and execute all the time but here we took a step back and evaluated the run times of these operations and how they affect the performance of our code after that we jumped into an entirely new world where we wrote our own data structure a singly linked list admittedly linked lists aren't used much in daytoday problem solving but it is a good data structure to start off with because it is fairly straightforward to understand and not that much different from an array we carried out the same exercise as we did on arrays in that we looked at common data operations but since this was a type we defined on our own we implemented these operations ourselves and got to examine with a finetooth comb how our code and the structure of the type affected the runtime of these operations the next topic we tackled was essentially worlds colliding we implemented a sorting algorithm to sort two different data structures here we got to see how all of the concepts we've learned so far algorithmic thinking time and space complexity and data structures all come together to tackle the problem of sorting data this kind of exercise is one we're going to focus on moving forward as we try to solve more realworld programming problems using different data structures and algorithms if you've stuck with this content so far keep up the great work this can be a complex topic but a really interesting one and if you take your time with it you will get a deeper understanding of programming and problem solving as always check the notes for more resources and happy coding you may have heard that algorithms and computer science are boring or frustrating they certainly can be hard to figure out especially the way some textbooks explain them but once you understand what's going on algorithms can seem fascinating clever or even magical to help further your understanding of algorithms this course is going to look at two categories sorting algorithms and searching algorithms you could argue that these are the easiest kinds of algorithms to learn but in learning how these algorithms are designed we'll cover useful concepts like recursion and divide and conquer that are used in many other sorts of algorithms and can even be used to create brand new ones by the way all the code samples i'm going to show in the videos will be in python because it's a popular language that's relatively easy to read but you don't need to know python to benefit from this course you can see the teacher's notes for each video for info on implementing these algorithms in your own favorite language our goal with this course is to give you an overview of how sorting and searching algorithms work but many algorithms have details that can be handled in different ways some of these details may distract from the big picture so we've put them in the teachers notes instead you don't need to worry about these when completing the course for the first time but if you're going back and referring to it later be sure to check the teacher's notes for additional info suppose we have a list of names it's a pretty big list a hundred thousand names long this list could be part of an address book or social media app and we need to find the locations of individual names within the list possibly to look up additional data that's connected to the name let's assume there's no existing function in our programming language to do this or that the existing function doesn't suit our purpose in some way for an unsorted list our only option may be to use linear search also known as sequential search linear search is covered in more detail elsewhere on our site check the teacher's notes for a link if you want more details you start at the first element you compare it to the value you're searching for if it's a match you return it if not you go to the next element you compare that to your target if it's a match you return it if not you go to the next element and so on through the whole list the problem with this is that you have to search the entire list every single time we're not doing anything to narrow down the search each time we have to search all of it if you're searching a big list or searching it repeatedly this amount of time can slow your whole lap down to the point that people may not want to use it anymore that's why it's much more common to use a different algorithm for searching lists binary search binary search is also covered in more detail elsewhere on our site check the teacher's notes for a link binary search does narrow the search down for us specifically it lets us get rid of half the remaining items we need to search through each time it does this by requiring that the list of values be sorted it looks at the value in the middle of the list if the value it finds is greater than the target value it ignores all values after the value it's looking at if the value it finds is less than the target value it ignores all values before the value it's looking at then it takes the set of values that remain and looks at the value in the middle of that list again if the value it finds is greater than the target value it ignores all values after the value it's looking at if the value it finds is less than the target value it ignores all values before the value it's looking at but as we mentioned binary search requires the list of values you're searching through to be sorted if the lists weren't sorted you would have no idea which half of the values to ignore because either half could contain the value you're looking for you'd have no choice but to use linear search so before we can use binary search on a list we need to be able to sort that list we'll look at how to do that next our end goal is to sort a list of names but comparing numbers is a little easier than comparing strings so we're going to start by sorting a list of numbers i'll show you how to modify our examples to sort strings at the end of the course to help make clear the importance of choosing a good sorting algorithm we're going to start with a bad one it's called bogosort basically bogosort just randomizes the order of the list repeatedly until it's sorted here's a python code file where we're going to implement bogosort it's not important to understand this code here at the top although we'll have info on it in the teachers notes if you really want it all you need to know is that it takes the name of a file that we pass on the command line loads it and returns a python list which is just like an array in other languages containing all the numbers that it read from the file let me have the program print out the list of numbers it loads so you can see it we'll call the print method and we'll pass it the list of numbers save that let's run it real quick with python bogosort.pi oh whoops and we need to provide it the name of the file here on the command line that we're going to load so it's in the numbers directory a slash separates the directory name from the file name five dot text and there's our list of numbers that was loaded from the file okay let me delete that print statement and then we'll move on bogo sort just randomly rearranges the list of values over and over so the first thing we're going to need is a function to detect when the list is sorted we'll write an is sorted function that takes a list of values as a parameter it'll return true if the list passed in is sorted or false if it isn't we'll loop through the numeric index of each value in the list from 0 to 1 less than the length of the list like many languages python list indexes begin at 0 so a list with a length of 5 has indexes going from 0 through 4. if the list is sorted then every value in it will be less than the one that comes after it so we test to see whether the current item is greater than the one that follows it if it is it means the whole list is not sorted so we can return false if we get down here it means the loop completed without finding any unsorted values python uses white space to mark code blocks so unindenting the code like this marks the end of the loop since all the values are sorted we can return true now we need to write the function that will actually do the socalled sorting the bogosort function will also take the list of values it's working with as a parameter we'll call our is sorted function to test whether the list is sorted we'll keep looping until is sorted returns true python has a readymade function that randomizes the order of elements in the list since the list isn't sorted we'll call that function here and since this is inside the loop it'll be randomized over and over until our is sorted function returns true if the loop exits it means is sorted returned true and the list is sorted so we can now return the sorted list finally we need to call our bogosort function pass it the list we loaded from the file and print the sorted list it returns okay let's save this and try running it we do so with python the name of the script bogosort.pi and the name of the file we're going to run it on numbers directory5.txt it looks like it's sorting our list successfully but how efficient is this let's add some code to track the number of times it attempts to sort the list up here at the top of the bogus sort function we'll add a variable to track the number of attempts it's made we'll name it attempts and we'll set its initial value to zero since we haven't made any attempts yet with each pass through the loop we'll print the current number of attempts and then here at the end of the loop after attempting to shuffle the values we'll add one to the count of attempts let's save this and let's try running it again a couple times in the console i can just press the up arrow to bring up the previous command and rerun it so it looks like this first run to sort this five element list took 363 attempts let's try it again this time it only took 91 attempts we're simply randomizing the list with each attempt so each run of the program takes a random number of attempts now let's try this same program with a larger number of items python bogo sort numbers i have a list of eight items set up here in this other file this time it takes 11 000 attempts only 487 this time and this time thirteen thousand you can see that the trend is increasing steadily the problem with bogosort is that it doesn't make any progress toward a solution with each pass it could generate a list where just one value is out of order but then on the next attempt it could generate a list where all the elements are out of order again stumbling on a solution is literally a matter of luck and for lists with more than a few items it might never happen up next we'll look at selection sort it's a sorting algorithm that's still slow but it's better than bogo's sort previously we showed you bogo sort a terrible sorting algorithm that basically randomizes the order of a list and then checks to see if it happens to be sorted the problem with bogo's sort is that it doesn't get any closer to a solution with each operation and so with lists that have more than a few items it'll probably never finish sorting them now we're going to look at an algorithm named selection sort it's still slow but at least each pass through the list brings it a little closer to completion our implementation of selection sort is going to use two arrays an unsorted array and a sorted one some versions move values around within just one array but we're using two arrays to keep the code simpler the sorted list starts out empty but we'll be moving values from the unsorted list to the sorted list one at a time with each pass we'll look through each of the values in the unsorted array find the smallest one and move that to the end of the sorted array we'll start with the first value in the unsorted array and say that's the minimum or smallest value we've seen so far then we'll look at the next value and see if that's smaller than the current minimum if it is we'll mark that as the new minimum then we'll move to the next value and compare that to the minimum again if it's smaller that becomes the new minimum we continue that way until we reach the end of the list at that point we know whatever value we have marked as the minimum is the smallest value in the whole list now here's the part that makes selection sort better than bogo sort we then move that minimum value from the unsorted list to the end of the sorted list the minimum value isn't part of the unsorted list anymore so we don't have to waste time looking at it anymore all our remaining comparisons will be on the remaining values in the unsorted list then we start the process over at this point our list consists of the numbers 8 5 4 and 7. our first minimum is 8. we start by comparing the minimum to five five is smaller than eight so five becomes the new minimum then we compare five to four and four becomes the new minimum four is not smaller than seven though so four remains the minimum four gets moved to the end of the sorted array becoming its second element the process repeats again eight is the first minimum but five is smaller so that becomes the minimum seven is larger so five stays is the minimum and five is what gets moved over to the sort of array and so on until there are no more items left in the unsorted array and all we have left is the sorted array so that's how selection sort works in general now let's do an actual implementation of it this code here at the top is the same as we saw in the bogo sword example it just loads a python list of numbers from a file let's implement the function that will do our selection sort we're going to pass in our python list containing all the unsorted numbers we'll create an empty list that will hold all our sorted values we'll loop once for each value in the list we call a function named index submin which we're going to write in just a minute that finds the minimum value in the unsorted list and returns its index then we call the pop method on the list and pass it the index of the minimum value pop will remove that item from the list and return it we then add that value to the end of the sorted list going up a level of indentation signals to python that we're ending the loop after the loop finishes we return the sorted list now we need to write the function that picks out the minimum value we pass in the list we're going to search we mark the first value in the list as the minimum it may or may not be the actual minimum but it's the smallest we've seen on this pass through the list it's also the only value we've seen on this pass through the list so far now we loop through the remaining values in the list after the first we test whether the value we're currently looking at is less than the previously recorded minimum if it is then we set the current index as the new index of the minimum value after we've looped through all the values we return the index of the smallest value we found lastly we need to actually run our selection sort method and print the sorted list it returns let's save this and now let's try running it we run the python command and pass it the name of our script selectionsort.pi in the numbers directory i've saved several data files filled with random numbers one on each line five dot text has five lines eight dot text has eight lines and to help us really measure the speed of our algorithms ten thousand dot text has ten thousand lines i've even created a file with a million numbers our script takes the path of a file to load as an argument so i'll give it the path of our file with five numbers numbers slash five dot text the script runs reads the numbers in the file into a list calls our selection sort method with that list and then prints the sorted list let me add a couple print statements within the selection sort function so you can watch the sort happening don't worry about figuring out the python formatting string that i use it's just there to keep the two lists neatly aligned i'll add the first print statement before the loop runs at all i'll have it print out the unsorted list and the sorted list i'll add an identical print statement within the loop so we can watch values moving from the unsorted list to the sorted list let's save this and we'll try running the same command again the output looks like this you can see the unsorted list on the left and the sorted list on the right initially the sorted list is empty on the first pass it selects the lowest number 1 and moves it to the sorted list then it moves the next lowest number over four this repeats until all the numbers have been moved to the sorted list i have another file with eight different numbers in it let's try our program with that python selection sort dot pi numbers 8.text you can see the same process at work here notice that this file had some duplicate values too that's okay though because the index of min function only updates the minimum index if the current value is less than the previous minimum if they're equal it just keeps the first minimum value it found and waits to move the duplicate value over until the next pass through the list so now we know that the selection sort algorithm works but the data sets we've been giving it sort are tiny in the real world algorithms need to work with data sets of tens of thousands or even millions of items and do it fast i have another file with ten thousand random numbers in it let's see if selection sort can handle that if i run this as it is now though it'll print out a lot of debug info as it sorts the list so first i'm going to go into the program and remove the two print statements in the selection sort function now let's run the program again on the dot text file and see how long it takes python selection sort dot pi numbers ten thousand dot text one one thousand two one thousand three one four one thousand five one thousand six one thousand seven one thousand eight one thousand nine one thousand ten one thousand eleven thousand twelve thousand thirteen thousand and it prints out all ten thousand of those numbers neatly sorted it took a little bit though how long well counting the time off vocally isn't very precise and other programs running on the system can skew the amount of time your program takes to complete let me show you a unix command that's available here in workspaces which can help you type time followed by a space and then the command you want to run so this command by itself will print the contents of our 5.txt file cat as in concatenate numbers 5.text and this command will do the same thing but it'll also keep track of how long it takes the cat program to complete and report the result time cat numbers five dot text the real row in the results is the actual amount of time for when the program started running to when it completed we can see it finished in a fraction of a second but as we said other programs running on the system can take cpu resources in which case your program will seem slower than it is so we generally want to ignore the real result the user result is the amount of time the cpu actually spent running the program code so this is the total amount of time the code inside the cat program took to run the sys result is the amount of time the cpu spent running linux kernel calls that your code made the linux kernel is responsible for things like network communications and reading files so loading the 5.txt file is probably included in this result in evaluating code's performance we're generally going to want to add together the user and sys results but cad is a very simple program let's try running the time command on our code and see if we get a more interesting result time python selection sort dot pi numbers ten thousand dot text this takes much longer to complete nearly 12 seconds according to the real time measurement but as we said the real result is often skewed so let's disregard that if we add the user and cis runtimes together we get about 6 seconds the time for the program to complete will vary a little bit each time you run it but if it's doing the same operations it usually won't change more than a fraction of a second if i run our selection sort script on the same file you can see it completes in roughly the same time now let's try it on another file with 1 million numbers time python selection sort dot pi numbers 1 million dot text how long does this one take i don't even know while designing this course i tried running this command and my workspace connection timed out before it completed so we'll just say that selection sort takes a very very long time to sort a million numbers if we're going to sort a list that big we're going to need a faster algorithm we'll look into alternative sorting algorithms shortly the next two sorting algorithms we look at will rely on recursion which is the ability of a function to call itself so before we move on we need to show you how recursion works recursive functions can be very tricky to understand imagine a row of dominoes stood on end where one domino falling over causes the next domino to fall over which causes the next domino to fall over causing a chain reaction it's kind of like that let's suppose we need to write a function that adds together all the numbers in an array or in the case of python a list normally we'd probably use a loop for this sort of operation the function takes a list of the numbers we want to add the total starts at zero we loop over every number contained in the list and we add the current number to the total once we're done looping we return the accumulated total if we call this sum function with a list of numbers it'll return the total when we run this program it'll print out that return value 19. let's try it real quick python recursion.pi oh whoops mustn't forget to save my work here and run it and we see the result 19. to demonstrate how recursion works let's revise the sum function to use recursion instead note that recursion is not the most efficient way to add a list of numbers together but this is a good problem to use to demonstrate recursion because it's so simple one thing before i show you the recursive version though this example is going to use the python slice syntax so i need to take a moment to explain that for those not familiar with it a slice is a way to get a series of values from a list let's load up the python repel or read evaluate print loop so i can demonstrate we'll start by creating a list of numbers to work with numbers equals a list with 0 1 2 3 and 4 containing those numbers like arrays in most other languages python list indexes start at 0 so numbers 1 will actually get the second item from the list with slice notation i can actually get several items back it looks just like accessing an individual index of a list but then i type a colon followed by the list index that i want up to but not including so numbers 1 colon 4 would get us the second up to but not including the fifth items from the list that is it'll get us the second through the fourth items now i know what you're thinking and you're right that up to but not including rule is a little counterintuitive but you can just forget all about it for now because we won't be using a second index with any of our python slice operations in this course here's what we will be using when you leave the second index off of a python slice it gives you the items from the first index up through the end of the list wherever that is so numbers 1 colon with no index following it will give us items from the second index up through the end of the list numbers 2 colon will give us items from the third index up to the end of the list you can also leave the first index off to get everything from the beginning of the list numbers colon 3 will get us everything from the beginning of the list up to but not including the third index it's also worth noting that if you take a list with only one item and you try to get everything from the nonexistent second item onwards the result will be an empty list so if i create a list with just one item in it and i try to access from the second element onwards the second element doesn't exist so the result will be an empty list don't worry too much about remembering python slice syntax it's not an essential part of sorting algorithms or recursion i'm only explaining it to help you read the code you're about to see so i'm going to exit the python rebel now that we've covered recursion we can convert our sum function to a recursive function it'll take the list of numbers to add just like before now here's the recursive part we'll have the sum function call itself we use slice notation to pass the entire list of numbers except the first one then we add the first number in the list to the result of the recursive function call and return the result so if we call sum with four numbers first it'll call itself with the remaining three numbers that call to sum will then call itself with the remaining two numbers and so on but if we save this and try to run it pythonrecursion.pi well first we get a syntax error it looks like i accidentally indented something i shouldn't have so let me go fix that real quick there we go that's suggested to python that there was a loop or something there when there wasn't so let's go back to the terminal and try running this again there we go now we're getting the error i was expecting recursion error maximum recursion depth exceeded this happens because some gets into an infinite loop it keeps calling itself over and over the reason is that when we get down to a list of just one element and we take a slice from the nonexistent second element to the end the result is an empty list that empty list gets passed to the recursive call to sum which passes an empty list in its recursive call to sum and so on until the python interpreter detects too many recursive calls and shuts the program down what we need is to add a base case to this recursive function a condition where the recursion stops this will keep it from getting into an infinite loop with the sum function the base case is when there are no elements left in the list in that case there is nothing left to add and the recursion can stop a base case is the alternative to a recursive case a condition where recursion should occur for the sum function the recursive case is when there are still elements in the list to add together let's add a base case at the top of the function python treats a list that contains one or more values as a true value and it treats a list containing no values as a false value so we'll add an if statement that says if there are no numbers in the list we should return a sum of zero that way the function will exit immediately without making any further recursive calls to itself we'll leave the code for the recursive case unchanged if there are still numbers in the list the function will call itself with any numbers after the first then add the return value to the first number in the list let's save this and try running it again python recursion dot pi output the sum of the numbers in the list 19 but it's still not really clear how this worked let's add a couple print statements that will show us what it's doing we'll show the recursive call to sum and what it's being called with we'll also add a call to print right before we return showing which of the calls the sum is returning and what it's returning let me save this and resize the console a bit and let's try running it again python recursion.pi since the print calls are inside the sum function the first call to sum 1279 isn't shown only the recursive calls are this first call to sum ignores the first item in the list 1 and calls itself recursively it passes the remaining items from the list 2 7 and 9. that call to sum again ignores the first item in the list it receives 2 and again calls itself recursively it passes the remaining items in the list 7 and 9. that call ignores the 7 and calls itself with a 9 and the last call shown here ignores the 9 and calls itself with an empty list at this point none of our recursive calls to sum have returned yet each of them is waiting on the recursive call it made to sum to complete python and other programming languages use something called a call stack to keep track of this series of function calls each function call is added to the stack along with the place in the code that it needs to return when it completes but now the empty list triggers the base case causing the recursion to end and the sum function to return zero that zero value is returned to its caller the caller adds the zero to the first and only value in its list nine the result is nine that nine value gets returned to the caller which adds it to the first value in the list it received seven the result is sixteen that sixteen value is returned to the caller which adds it to the first value in the list it received two the result is 18. that 18 value is returned to the caller which adds it to the first value in the list it received one the result is 19. that 19 value is returned to the caller which is not the sum function recursively calling itself but our main program this is our final result which gets printed it's the same result we got from the loopbased version of our program the end we don't want the print statements in our final version of the program so let me just delete those real quick and there you have it a very simple recursive function well the function is simple but as you can see the flow of control is very complex don't worry if you didn't understand every detail here because we won't be using this particular example again there are two fundamental mechanisms you need to remember a recursive function needs a recursive case that causes it to call itself and it also needs to eventually reach a base case that causes the recursion to stop you've seen bogo sort which doesn't make any progress towards sorting a list with each pass either it's entirely sorted or it isn't you've seen selection sort which moves one value over to a sorted list with each pass so that it has fewer items to compare each time now let's look at an algorithm that speeds up the process further by further reducing the number of comparisons it makes it's called quick sort here's some python code where we'll implement quick sort again you can ignore these lines at the top we're just using them to load a file full of numbers into a list the quick sort algorithm relies on recursion to implement it we'll write a recursive function we'll accept the list of numbers to sort as a parameter quicksort is recursive because it keeps calling itself with smaller and smaller subsets of the list you're trying to sort we're going to need a base case where the recursion stops so it doesn't enter an infinite loop lists that are empty don't need to be sorted and lists with just one element don't need to be sorted either in both cases there's nothing to flip around so we'll make that our base case if there are zero or one elements in the list passed to the quick sort function we'll return the unaltered list to the caller lastly we need to call our quick sort function with our list of numbers and print the list it returns that takes care of our base case now we need a recursive case we're going to rely on a technique that's common in algorithm design called divide and conquer basically we're going to take our problem and split it into smaller and smaller problems until they're easy to solve in this case that means taking our list and splitting it into smaller lists viewers a suggestion the process i'm about to describe is complex there's just no way around it if you're having trouble following along remember the video playback controls feel free to slow the play back down rewind or pause the video as needed after you watch this the first time you may also find it helpful to rewind and make your own diagram of the process as we go okay ready here goes suppose we load the numbers from our 8.txt file into a list how do we divide it it would probably be smart to have our quicksort function divide the list in a way that brings it closer to being sorted let's pick an item from the list we'll just pick the first item for now four we'll call this value we've picked the pivot like the center of a seesaw on a playground we'll break the list into two sublists the first sublist will contain all the items in the original list that are smaller than the pivot the second sublist will contain all the items in the original list that are greater than the pivot the sub list of values less than and greater than the pivot aren't sorted but what if they were you could just join the sub lists and the pivot all together into one list and the whole thing would be sorted so how do we sort the sublist we call our quick sort function recursively on them this may seem like magic but it's not it's the divide and conquer algorithm design technique at work if our quick sort function works on the big list then it will work on the smaller list too for our first sub list we take the first item it's the pivot again that's three we break the sub list into two sub lists one with everything less than the pivot and one with everything greater than the pivot notice that there's a value equal to the pivot that gets put into the less than sublist our finished quicksort function is actually going to put everything that's less than or equal to the pivot in the first sublist but i don't want to say less than or equal to over and over so i'm just referring to it as the less than pivot sublist also notice that there are no values greater than the pivot that's okay when we join the sublists back together that just means nothing will be in the return list after the pivot we still have one sub list that's more than one element long so we call our quick sort function on that too you and i can see that it's already sorted but the computer doesn't know that so it'll call it anyway just in case it picks the first element 2 as a pivot there are no elements less than the pivot and only one element greater than the pivot that's it for the recursive case we've finally hit the base case for our quick sort function it'll be called on both the empty list of elements less than the pivot and the one item list of elements greater than the pivot but both of these lists will be returned as they are because there's nothing to sort so now at the level of the call stack above this the return sorted lists are used in place of the unsorted sublist that's less than the pivot and the unsorted sublist that's greater than the pivot these are joined together into one sorted list remember that any empty lists get discarded then at the level of the call stack above that the return sorted lists are used in place of the unsorted sublists there again they were already sorted but the quick sort method was called on them anyway just in case the sublists are joined together into one sorted list at the level of the call stack above that the return sorted list is used in place of the unsorted sublist that's less than the pivot so now everything that's less than or equal to the pivot is sorted now we call quick sort on the unsorted sublist that's greater than the pivot and the process repeats for that sublist we pick the first element six is the pivot we split the sublist into sublists of elements that are less than and greater than this pivot and we recursively call the quicksort function until those sublists are sorted eventually a sorted sublist is returned to our first quick sort function call we combine the sublist that's less than or equal to the pivot the pivot itself and the sublist that's greater than the pivot into a single list and because we recursively sorted the sub lists the whole list is sorted so that's how the quick sort function is going to work in the next video we'll show you the actual code quicksort works by picking a pivot value then splitting the full list into two sublists the first sublist has all the values less than or equal to the pivot and the second sublist has all the values greater than the pivot the quick sort function recursively calls itself to sort these sublists and then to sort the sublists of those sublists until the full list is sorted now it's time to actually implement this in code we already have the base case written any list passed in that consists of 0 or 1 values will be returned as is because there's nothing to sort now we need to create a list that will hold all the values less than the pivot that list will be empty at first we do the same for values greater than the pivot next we need to choose the pivot value for now we just grab the first item from the list then we loop through all the items in the list following the pivot we check to see whether the current value is less than or equal to the pivot if it is we copy it to the sublist of values less than the pivot otherwise the current value must be greater than the pivot so we copy it to the other list this last line is where the recursive magic happens we call quick sort recursively on the sublist that's less than the pivot we do the same for the sublist that's greater than the pivot those two calls will return sorted lists so we combine the sort of values less than the pivot the pivot itself and the sort of values greater than the pivot that gives us a complete sorted list which we return this took a lot of prep work are you ready let's try running it python quick sort dot pi numbers 8.text it outputs our sorted list i don't know about you but this whole thing still seems a little too magical to me let's add a couple print statements to the program so we can see what it's doing first we'll add a print statement right before the first call to the quick sort function so we can see the unsorted list we'll also add a print right within the quick sort function right before the recursive calls again this string formatting code is just to keep the info aligned in columns let's try running this again and now you can see our new debug output each time quicksort goes to call itself recursively it prints out the pivot as well as the sub list of items less than or equal to the pivot if any and the sub list of items greater than the pivot if any you can see that first it sorts the sub list of items less than the pivot at the top level it goes through a couple levels of recursion to do that there are actually additional levels of recursion but they're from calls to quick sort with a list of 0 or 1 elements and those calls return before the print statement is reached then it starts sorting the second sub list from the top level with items greater than the original pivot you can see a couple levels of recursion for that sort as well finally when both sublists are recursively sorted the original call to the quicksort function returns and we get the sorted list back so we know that it works the next question is how well does it work let's go back to our file of ten thousand numbers and see if it can sort those first though i'm going to remove our two debug calls to print so it doesn't produce unreadable output a quick note if you try running this on a file with a lot of repeated values it's possible you'll get a runtime error maximum recursion depth exceeded if you do see the teacher's notes for a possible solution now let's try running our quick sort program against the ten thousand dot text file python quick sort dot pi numbers 10 000 dot text there we go and it seems pretty fast but how fast exactly let's run it with the time command to see how long it takes time python quick sort dot pi numbers 10 000.text remember we need to ignore the real result and add the user and sys results it took less than a second of cpu time to sort 10 000 numbers with quicksort remember that selection sort took about 13 seconds that's a pretty substantial improvement and with a million numbers selection sort took so long that it never even finished successfully let's see if quicksort performs any better time python quick sort dot pi numbers 1 million dot text not only did quicksort sort a million numbers successfully it only took about 11 seconds of cpu time quicksort is clearly much much faster than selection sort how much faster that's something we'll discuss in a later video what we've shown you here is just one way to implement quicksort although the basic algorithm is always the same the details can vary like how you pick the pivot see the teacher's notes for more details let's review another sorting algorithm merge sort so that we can compare it with quick sort merge sort is already covered elsewhere on the site so we won't go into as much detail about it but we'll have more info in the teacher's notes if you want it both quicksort and merge sword are recursive the difference between them is in the sorting mechanism itself whereas quicksort sorts a list into two sublists that are less than or greater than a pivot value merge sort simply splits the list in half recursively and then sorts the halves as it merges them back together that's why it's called merge sort you may recognize this code at the top by now it just loads a file full of numbers into a list let's define a recursive merge sort function as usual it'll take the list or sublist that we want it to sort our base case is the same as with quicksort if the list has zero or one values there's nothing to sort so we return it as is if we didn't return it means we're in the recursive case so first we need to split the list in half we need to know the index we should split on so we get the length of the list and divide it by two so for example if there are eight items in the list we'll want an index of four but what if there were an odd number of items in the list like seven we can't have an index of 3.5 so we'll need to round down in that case since we're working in python currently we can take advantage of a special python operator that divides and rounds the result down the floor division operator it consists of a double slash now we'll use the python slice syntax to get the left half of the list we'll pass that list to a recursive call to the merge sort function we'll also use slice syntax to get the right half of the list and pass that to merge sort as well now we need to merge the two halves together and sort them as we do it we'll create a list to hold the sorted values and now we get to the complicated part merging the two halves together and sorted them as we do it we'll be moving from left to right through the left half of the list copying values over to the sorted values list as we go this left index variable will help us keep track of our position at the same time we'll also be moving from left to right through the right half of the list and copying values over so we need a separate write index variable to track that position as well we'll keep looping until we've processed all of the values in both halves of the list we're looking to copy over the lowest values first so first we test whether the current value on the left side is less than the value on the right side if the left side value is less that's what we'll copy over to the sorted list and then we'll move to the next value in the left half of the list otherwise the current value from the right half must have been lower so we'll copy that value to the sorted list instead and then we'll move to the next value in the right half of the list that ends the loop at this point one of the two unsorted halves still has a value remaining and the other is empty we won't waste time checking which is which we'll just copy the remainder of both lists over to the sorted list the one with the value left will add that value and the empty one will add nothing all the numbers from both halves should now be copied to the sorted list so we can return it finally we need to kick the whole process off we'll call the merge sort function with the list of numbers we loaded and print the result let's save this and we'll try it out on our file with eight numbers python merge sort dot pi numbers eight dot text and it prints out the sorted list but again this seems pretty magical let's add some print statements to get some insight into what it's doing first we'll print the unsorted list so we can refer to it we'll add a print statement right before we call the merge sort function for the first time then we'll add another print statement within the merge sort function right after the recursive calls this will show us the sorted left half and right half that it's returning again don't worry about the fancy python formatting string it just keeps the values neatly aligned let me resize my console clear the screen and then we'll try running this again what we're seeing are the values being returned from the recursive merge sort function calls not the original calls to merge sort so what you see here is after we reach the base case with a list that's only one item in length and the recursive calls start returning the original list gets split into two unsorted halves four six three and two and nine seven three and five the first half gets split in half again four and six and three and two and each of those halves is halved again into single element lists there's nothing to sort in the single element list so they're returned from the merge sort function as is those single element lists get merged into two sub lists and sorted as they do so the four and six sublist looks the same after sorting as it did before sorting but the three and the two get sorted as they're combined into a sublist the new order is two three the order is shifted again when those two sublists get combined back into a single list two three four six then we recursively sort the right half of the original list nine seven three five it gets split in half again nine seven and three five and each of those halves get broken into single element lists there's nothing to sort there so the single element lists are returned as is the first two are sorted as they're merged seven nine and so are the second three five and then those two sub lists get sorted as they're combined into another sub list three five seven nine and finally everything is sorted as it's merged back into the full sorted list two three three four five six seven nine that's how merge sort works on a list of eight numbers let's see if it works on a bigger list first i'll remove the two print statements so we don't get an overwhelming amount of debug output then i'll run it on a list of ten thousand items python merge sort dot pi numbers ten thousand dot text not only did it work it was pretty fast but which is faster merge sort or quick sort we'll look at that next i've removed the call to print that displays the sorted list at the end of our selection sort quick sort and merge sort scripts that way it'll still run the sort but the output won't get in the way of our comparing runtimes let's try running each of these scripts and see how long it takes time python selection sort we'll do that one first numbers 10 000 dot text we combine the user and sys results and that gives us about six seconds now let's try quick sort time python quick sort dot pi numbers ten thousand dot text much faster less than a second and finally time python merge sort dot pi numbers ten thousand dot text a little longer but far less than a second so even on a list with just 10 000 numbers selection sort takes many times as long as quicksort and merge sort and remember i ran the selection sort script on a file with a million numbers and it took so long that my workspace timed out before it completed it looks like selection sort is out of the running as a viable sorting algorithm it may be easy to understand and implement but it's just too slow to handle the huge data sets that are out in the real world now let's try quicksort and merge sort on our file with a million numbers and see how they compare there time python quicksort dot pi numbers million dot text looks like it took about 11 seconds of cpu time now let's try merge sort time python merge sort dot pi numbers 1 million dot text that took about 15 seconds of cpu time it looks like quicksort is marginally faster than merge sort on this sample data we had to learn a lot of details for each algorithm we've covered in this course developers who need to implement their own algorithms often need to choose an algorithm for each and every problem they need to solve and they often need to discuss their decisions with other developers can you imagine needing to describe all the algorithms in this same level of detail all the time you'd spend all your time in meetings rather than programming that's why big o notation was created as a way of quickly describing how an algorithm performs as the data set it's working on increases in size big o notation lets you quickly compare several algorithms to choose the best one for your problem the algorithms we've discussed in this course are very well known some job interviewers are going to expect you to know their big o run times so let's look at them remember that the n in big o notation refers to the number of elements you're operating on with selection sort you need to check each item in the list to see if it's the lowest so you can move it over to the sorted list so that's in operations suppose you're doing selection sort on a list of five items and in this case would be five so that's five operations before you can move an item to the sorted list but with selection sort you have to loop over the entire list for each item you want to move there are five items in the list and you have to do five comparisons to move each one so it's more like 5 times 5 operations or if we replace 5 with n it's n times n or n squared but wait you might say half of that 5 by 5 grid of operations is missing because we're testing one fewer item in the unsorted list with each pass so isn't it more like one half times n times n and this is true we're not doing a full n squared operations but remember in big o notation as the value of n gets really big constants like one half become insignificant and so we discard them the big o runtime of selection sword is widely recognized as being o n squared quicksort requires one operation for each element of the list it's sorting it needs to select a pivot first and then it needs to sort elements into lists that are less than or greater than the pivot so that's n operations to put that another way if you have a list of eight items then n is eight so it will take eight operations to split the list around the pivot but of course the list isn't sorted after splitting it around the pivot just once you have to repeat those eight operations several times in the best case you'll pick a pivot that's right in the middle of the list so that you're dividing the list exactly in half then you keep dividing the list in half until you have a list with a length of one the number of times you need to divide n in half until you reach one is expressed as log n so you need to repeat n sorting operations log n times that leaves us with the best case run time for quick sort of o n log n but that's the best case what about the worst case well if you pick the wrong pivot you won't be dividing the list exactly in half if you pick a really bad pivot the next recursive call to quicksort will only reduce the list length by one since our quicksort function simply picks the first item to use as a pivot we can make it pick the worst possible pivot repeatedly simply by giving it a list that's sorted in reverse order if we pick the worst possible pivot every time we'll have to split the list once for every item it contains and then do end sorting operations on it you already know another sorting algorithm that only manages to reduce the list by one element with each pass selection sort selection sort has a runtime of o n squared and in the worst case that's the run time for quicksort as well so which do we consider when trying to decide whether to use quicksort the best case or the worst case well as long as your implementation doesn't just pick the first item as a pivot which we did so we could demonstrate this issue it turns out that on average quicksort performs closer to the best case many quicksort implementations accomplish this simply by picking a pivot at random on each recursive loop here we are sorting our reverse sorted data again but this time we pick pivots at random which reduces the number of recursive operations needed sure random pivots sometimes give you the best case and sometimes you'll randomly get the worst case but it all averages out over multiple calls to the quick sort function now with merge sort there's no pivot to pick your list of n items always gets divided in half log n times that means merged sort always has a big o runtime of o and log in contrast that with quicksort which only has a runtime of o and log n in the best case in the worst case quick sorts runtime is o n squared and yet out in the real world quicksort is more commonly used than merge sort now why is that if quicksort's big o runtime can sometimes be worse than merge sorts this is one of those situations where big o notation doesn't tell you the whole story all big o can tell you is the number of times an operation is performed it doesn't describe how long that operation takes and the operation mergesor performs repeatedly takes longer than the operation quicksort performs repeatedly bigo is a useful tool for quickly describing how the runtime of an algorithm increases is the data set it's operating on gets really really big but you can't always choose between two algorithms based just on their big o runtimes sometimes there's additional info you need to know about an algorithm to make a good decision now that we can sort a list of items we're well on our way to being able to search a list efficiently as well we'll look at how to do that in the next stage now that we've covered sorting algorithms the groundwork has been laid to talk about searching algorithms if you need to search through an unsorted list of items binary search isn't an option because you have no idea which half of the list contains the item you're looking for your only real option is to start at the beginning and compare each item in the list to your target value one at a time until you find the value you're looking for this algorithm is called linear search or sequential search because the search proceeds in a straight line or sequence even though linear search is inefficient searching for just one name will happen so fast that we won't be able to tell anything useful about the algorithm's runtime so let's suppose we had a hundred different names and that we needed to know where they appear in a list of unsorted names here's some code that demonstrates as usual this code at the top isn't relevant to the search algorithm it's just like the code that loaded a list of numbers from a file in the previous stage but this code calls a different function load strings that loads a list of strings in if you want the load strings python code we'll have it for you in the teacher's notes here's a separate hardcoded list containing the 100 names we're going to search for we'll loop through each name in this list and pass it to our search function to get the index within the full list where it appears now let's implement the search function compared to the sorting algorithms this is going to be short the index of item function takes the python list you want to search through and a single target value you want to search for now we need to loop over each item in the list the range function gives us a range of numbers from its first argument up to but not including its second argument so if our list had a length of 5 this would loop over the indexes 0 through 4. we test whether the list item at the current index matches our target if it does then we return the index of the current item this will exit the index of item function without looping over the remaining items in the list if we reach the end of the loop without finding the target value that means it wasn't in the list so instead of returning an index we return the special python value none which indicates the absence of a value other languages have similar values like nil or null but if yours doesn't you might have to return a value that would otherwise be impossible like an index of negative 1. now let's call our new search function we start by looping over the list of 100 values we're looking for we're using the values themselves this time not their indexes within the list so there's no need to mess with python's range function here's the actual call to the index of item function we pass it the full list of names that we loaded from the file plus the name we want to search for within that list then we store the index it returns in a variable and lastly we print the index we get back from the index of item function let's save this and go to our console and see if it works python linear search dot pi names unsorted dot text and it'll print out the list of indexes for each name i actually set it up so that the last two items in the list of names we're going to search for corresponded to the first and last name within the file so if we open up our unsorted.txt file we'll see mary rosenberger is the first name and alonso viviano is the last name and those are the last two values in our list of names we're searching for so it returned an index of zero for that second to last name and you can see that name here on line one of the file the line numbering starts at one and the python list indexes start at zero so that makes sense and for the last name it returned an index of 109873 and you can see that name here on line 109 874 so we can see that it's returning the correct indexes but right now we're just searching for a hundred different names in a list of one hundred thousand names in the real world we're going to be looking for many more names than that within much bigger lists than that can we do this any faster yes but we'll need to use the binary search algorithm and for that to work we need to sort our list of strings we'll do that in the next video before we can use the binary search algorithm on our list of names we need to sort it let's do that now we need to load our unsorted list of names from a file sorted and write the sorted names back out to a new file again this code at the top just loads a file full of strings into a list we'll use our quick sort method to sort the list of names its code is completely unchanged from when you saw it in the previous stage we just call our quick sort function on the list of names loaded from the file and save the list to a variable then we loop through each name in the sorted list and we print that name that's all there is to it let's save this script and try running it python quicksort strings stop pi and we'll pass it the names unsorted.text file let me resize the console window here a little bit that prints the sorted list of names out to the terminal but we need it in a file so we'll do what's called a redirect of the program's output we'll run the same command as before but at the end we'll put a greater than sign followed by the path to a file that we want the program output written to names sorted dot text redirecting works not only on linux based systems like workspaces but also on macs and even on windows machines you just need to be careful because if you redirect to an existing file its contents will be overwritten without asking you let me refresh the list of files in the sidebar and you'll see that we now have a new sorted dot text file in the names directory it's the same number of lines as the unsorted dot text file but all the names are sorted now now we can load this file of sorted names into a list and we'll be able to use that list with the binary search algorithm we'll see how to do that next now that we have our list of names sorted we can use the binary search algorithm on it let's see if we can use it to speed up our search for the indexes of 100 names binary search keeps narrowing down the list until it has the value it's looking for it's faster than linear search because it discards half the potential matches each time our code here at the top of our binary search script is unchanged from the previous scripts we just call the load strings function to load our 100 000 sorted names from a file here we've hard coded the list of 100 names we're going to search for again it's identical to the list from the linear search script except that i've again changed the last two names to correspond to the names on the first and last lines of the file we'll be loading now let's write the function that will implement our binary search algorithm like the linear search function before it'll take two arguments the first is the list we're going to search through and the second is the target value we'll be searching for again the binary search function will return the index it found the value at or the special value none if it wasn't found binary search is faster than a linear search because it discards half the values it has to search through each time to do this it needs to keep track of a range that it still needs to search through to start that range is going to include the full list the first variable will track the lowest index in the range we're searching to start it's going to be 0 the first index in the full list likewise the last variable will track the highest index in the range we're searching to start we'll set it to the highest index in the full list if the first and last variables are equal then it means the size of the search range has shrunk to zero and there is no match until that happens though we'll keep looping to continue the search we want to divide the list of potential matches in half each time to do that we need to check the value that's in the middle of the range we're searching in we add the indexes in the first and last variables then divide by two to get their average we might get a fractional number which can't be used as a list index so we also round down using python's double slash floor division operator all this will give us the index of the list element that's the midpoint of the range we're searching we store that in the midpoint variable whoops looks like my indentation got mixed up there let me fix that real quick there we go now we test whether the list element at the midpoint matches the target value if it does we return the midpoint index without looping any further our search is complete otherwise if the midpoint element's value is less than the target value then we know that our target value can't be at the midpoint or any index prior to that so we move the new start of our search range to just after the old midpoint otherwise the midpoint element's value must have been greater than the target value we know that our target value can't be at the midpoint or any index after that so we move the new end of our search range to just before the old midpoint by unindenting here we mark the end of the loop if the loop completes it means the search range shrank to nothing without our finding a match and that means there's no matching value in the list so we return the special python value none to indicate this lastly just as we did in our linear search script we need to search for each of the 100 names we loop over each name in our hardcoded list and we call the binary search function with the sorted list of names we're going to load from the file and the current name we're searching for we store the returned list index in the index variable and finally we print that variable let's save this and go to our console and try running it python binarysearch.pi and it's important to give it the name of the sorted file if it loads the unsorted file the binary search won't work so names sorted dot text again it prints out the list of indexes for each name i once again set it up so the last two items in the list of names we're going to search for corresponded to the first and last name in the file so it returned an index of zero for the second to last name and you can see that name here's the second to last name aaron augustine you can see that name here on line one of the file and for the last name it returned an index of one zero nine eight seven three and you can see that name here on line one zero nine eight seven four let's check the third to last name for good measure it looks like an index of 97022 was printed for that name stephen daras let's search for steve and daras within the file and here it is on line 97023 remember that line numbers start on one instead of zero so this actually matches up with the printed list index of 97022 it looks like our binary search script is working correctly let's try our linear search and binary search scripts out with the time command and see how they compare i've commented out the lines that print the indexes of matches in the two scripts that way they'll still call their respective search functions what the 100 names we're searching for but they won't actually print the indexes out so we won't have a bunch of output obscuring the results of the time command first let's try the linear search script time python linear search dot pi names and we can just use the unsorted list of names for linear search remember we want to ignore the real result and add the user and sys results together it looks like it took about .9 seconds for linear search to find the 100 names in the list of one hundred thousand now let's try timing the binary search script time python binarysearch.pi names and for this one we need to use the sorted list of names looks like that took around a quarter second so less than half as long bear in mind that part of this time is spent loading the file of names into a list the difference between linear search and binary search will be even more pronounced as you search through bigger lists or search for more items let's wrap up the course by looking at the big o runtimes for linear search and binary search these are going to be much simpler to calculate than the sorting algorithms were for linear search you need to do one comparison to the target value for each item in the list again theoretically we could find the target value before searching the whole list but big o notation is only concerned with the worst case where we have to search the entire list so for a list of eight items that means eight operations the big o runtime for linear search is o n where n is the number of items we're searching through this is also known as linear time because when the number of items and number of operations are compared on a graph the result is a straight line linear search looks pretty good until you compare it to binary search for binary search the number of items you have to search through and therefore the number of operations is cut in half with each comparison remember the number of times you can divide n by two until you reach one is expressed as log n so the run time of binary search in big o notation is o log n even for very large values of n that is very large lists you have to search through the number of operations needed to search is very small binary search is a very fast efficient algorithm that's our tour of sorting and searching algorithms be sure to check the teacher's notes for opportunities to learn more thanks for watching
