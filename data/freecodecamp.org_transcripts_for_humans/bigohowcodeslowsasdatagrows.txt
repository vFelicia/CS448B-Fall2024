With timestamps:

00:00 - so as ned said my name is ned batchelder
00:02 - i'm ned batt on most social media and
00:04 - there's a bitly short link there at the
00:06 - bottom of the slide that links to this
00:08 - talk online if you want to follow along
00:10 - with the slides
00:11 - and those two
00:12 - short things will be at the bottom of
00:13 - most of the slides
00:14 - too so i've been writing software for a
00:17 - long time and one of the things that
00:18 - interests me about writing software is
00:20 - that there are two mindsets that inform
00:23 - the process of writing software the
00:25 - first is computer science which is
00:27 - really a branch of mathematics and so
00:29 - it's very theoretical
00:30 - you do proofs you think about very
00:32 - abstract concepts
00:34 - the other mindset that informs writing
00:35 - software is software engineering which
00:37 - is very pragmatic and is basically
00:39 - concerned only with whether you are
00:41 - writing software that works how can we
00:42 - write software that works
00:44 - and there is some crossover lots of
00:46 - computer science underpins software
00:48 - engineering but we don't think about it
00:49 - every day but there's a few topics that
00:52 - do cross over into the everyday of
00:54 - software engineering and in particular
00:56 - i'm interested in people who are working
00:57 - in software engineering who don't have
00:59 - formal computer science backgrounds and
01:01 - maybe feel a little bit insecure about
01:03 - that and one of the things that they
01:05 - seem to feel keeps them from sitting at
01:07 - the grownups table is this thing called
01:09 - big o
01:12 - a big o
01:13 - is really a simple thing and i've made a
01:15 - rhyme here to help you remember what it
01:17 - is it's about how your code slows as
01:20 - your data grows
01:22 - and an english major friend of mine
01:23 - pointed out that the rhyme is on the o
01:25 - sound like big o and i didn't intend
01:27 - that at all but that's cool
01:31 - so the question is how does your code
01:33 - slow down as the data gets larger and
01:35 - larger and this is not the same as the
01:37 - running time of any particular run of
01:39 - your code we're not trying to measure
01:41 - the time in seconds we're not trying to
01:43 - figure out exactly how many of them you
01:45 - can do in one transaction
01:47 - we're talking about the trend over time
01:49 - over many runs of your code
01:52 - how does it slow down as the data gets
01:54 - larger and larger and larger and one
01:56 - very pragmatic way to think about this
01:58 - is let's say you have a chunk of code
02:00 - you give it a certain amount of data it
02:02 - takes a certain amount of time
02:03 - how much longer will it take to work on
02:05 - 10 times as much data if i give it 10
02:07 - times as much data how much longer does
02:09 - it take and you might think intuitively
02:11 - well it'll take 10 times as long
02:13 - obviously but that turns out not to be
02:15 - true some code will take 10 times as
02:18 - long some code will take twice as long
02:20 - some code will take a hundred times as
02:21 - long and some code won't take any longer
02:23 - at all
02:24 - and big o is all about characterizing
02:26 - that growth
02:28 - of the time of the code as the code as
02:30 - the data grows how the code slows as the
02:32 - data grows
02:35 - and computer science people approach
02:37 - this topic in a very very mathematical
02:39 - way
02:40 - but software engineers approach it in a
02:42 - very very pragmatic way and i'm trying
02:43 - to i'm going to explain the pragmatic
02:45 - approach it doesn't have to be done in a
02:47 - mathy way it can be done in a pragmatic
02:49 - way
02:51 - so let's get some terminology out of the
02:53 - way
02:54 - this is called the big o notation and
02:56 - the way it's written is a capital o and
02:57 - a parenthesis and then a bunch of stuff
02:59 - with an n in it and a closed parenthesis
03:02 - okay i told you it was simple
03:07 - the n
03:08 - is meant to stand in for how much data
03:10 - you have
03:12 - and the o stands for order of and the
03:14 - idea is that we're talking about the
03:16 - running time of your code grows on the
03:17 - same order of some mathematical
03:20 - expression of n
03:23 - the key thing here is although it looks
03:24 - like a function call there's a name and
03:26 - then parentheses with stuff inside it's
03:28 - not a function call it's just a notation
03:30 - and there's probably a mathy reason why
03:32 - it looks like a function call but it
03:33 - doesn't matter just know that it's not a
03:35 - function call it's just a way of
03:37 - labeling a piece of code as having a
03:38 - certain growth pattern
03:48 - so let's take a real world example let's
03:49 - say we have to count the number of beans
03:51 - and jars right we've got this guy on the
03:53 - left he opens up the jars he starts
03:56 - pulling out beans one by one
03:58 - right we can see here n is the number of
04:00 - beans right if we give this guy a jar
04:03 - with 10 times as many beans it's going
04:04 - to take him 10 times as long obviously
04:07 - and he's sweating
04:09 - this is what's known as o of an meaning
04:11 - that the time it takes to complete the
04:13 - task grows in the same way that n grows
04:16 - it's on the order of n if n doubles the
04:18 - time doubles if n is 10 times more the
04:21 - time is 10 times more
04:23 - and you might think well there's no
04:24 - other way to approach this task
04:26 - but there is of course you get beans
04:28 - jars that have labels on them that tell
04:30 - you how many beans are in them right
04:31 - this guy on the right has a much easier
04:33 - job you can see how much happier he is
04:35 - about it because it doesn't matter how
04:37 - large a jar you give this guy on the
04:39 - right it's going to take the same amount
04:40 - of time for him to tell you how many
04:42 - beans are in the jar this is what's
04:43 - known as o of one which is kind of a
04:45 - weird mathematician's way of saying that
04:47 - n isn't involved at all no matter what
04:49 - happens to n
04:51 - the running time remains the same
04:54 - o of n is is
04:56 - slower in the long run than o of one
04:59 - and this is a silly silly real world
05:01 - example
05:02 - um but for instance when you do for x in
05:05 - my list you you have an o of n operation
05:08 - because you have to look at every
05:09 - element in the list every being in the
05:11 - jar
05:12 - when you do len of my list you don't
05:13 - have to look at the elements of the list
05:15 - at all turns out python lists are kind
05:16 - of like those charts on the right the
05:18 - length of the list is written on a label
05:20 - on the outside of the list and so no
05:22 - matter how long the list is getting the
05:23 - length of the list is a constant time
05:25 - operation
05:28 - by the way these drawings were drawn by
05:29 - my son
05:30 - who is
05:32 - in art school one thing you might not
05:34 - have noticed if you look at their
05:35 - eyebrows they're shaped like beans
05:39 - that's that's art school for you
05:44 - [Applause]
05:48 - all right another real world example
05:50 - let's say
05:51 - i tell you i'm going to give you a book
05:52 - and i want you to find a certain word in
05:53 - the book like horse right if i hand you
05:55 - a novel you're going to start reading
05:58 - until maybe you find the word horse
06:00 - right this is sounds like an o of n
06:02 - operation again right because if i give
06:04 - you a novel that's twice as long it
06:05 - might take you twice as long until you
06:07 - encounter the word horse
06:09 - but let's say i give you a different
06:10 - book i give you an encyclopedia now you
06:12 - open the encyclopedia to the middle if
06:14 - the word you're looking for is earlier
06:15 - than that then you do another divide and
06:17 - conquer step until you find the word
06:19 - horse if i now if i give you an
06:21 - encyclopedia that's twice as large it's
06:23 - not going to take you twice as long
06:24 - there's just one more divide and conquer
06:26 - step to find it that's what's called o
06:28 - log of n which is a fancy
06:30 - mathematician's way of saying that
06:32 - so these are both real world examples of
06:35 - the kinds of tasks that sort of sound
06:36 - similar when you first hear them but how
06:38 - you organize the data and therefore what
06:40 - algorithm you can use on the data really
06:42 - affects how the length of time it takes
06:44 - you to do the task changes as the size
06:47 - of the data changes right that's what
06:49 - we're talking about is how your code
06:50 - slows as your data grows
06:54 - let's get some other terms out of the
06:56 - way because i'm going to be speaking
06:57 - here and i might throw out some words
06:58 - that are a little bit different than
06:59 - earlier when we say o of one we might
07:02 - call it constant time i might say that
07:04 - the labeled bean jars are a constant
07:06 - time algorithm because the time remains
07:09 - the same no matter what
07:12 - o of n is often called a linear
07:14 - operation because if you look at it
07:16 - mathematically there's a
07:18 - linear relationship between the size the
07:20 - data and the running time
07:22 - of n squared is a thing we haven't seen
07:24 - yet but we will
07:25 - that's the case where when you give it
07:27 - 10 times more data it takes a hundred
07:28 - times as long to run and that's called
07:30 - quadratic because now you've got a
07:31 - quadratic equation involved if you don't
07:33 - know what a quadratic equation is it
07:35 - doesn't matter it's just a word that
07:37 - means n squared
07:39 - and some other words for big o it's
07:41 - sometimes called complexity or time
07:43 - complexity or algorithmic complexity or
07:45 - if you want to sound really fancy you
07:46 - can call it asymptotic complexity it's
07:48 - all the same thing
07:50 - right one of the underlying themes of
07:51 - this talk is that this this this topic
07:54 - of big o notation is
07:57 - littered with mathematical
07:59 - detritus
08:01 - that doesn't really matter to the key
08:03 - concept and don't let that stuff throw
08:05 - you that's just chaff being thrown at
08:07 - you by mathematicians you don't have to
08:09 - let it throw you off the path
08:14 - so how do you actually determine the big
08:16 - o of a piece of code
08:18 - the first step is you figure out what
08:19 - code you're talking about and that
08:21 - sounds kind of silly but in a large
08:23 - system
08:24 - there's you might be looking at one
08:25 - function
08:26 - and that might be the important thing
08:27 - but it really might actually be
08:28 - important to consider all the callers of
08:30 - the function or maybe you're looking at
08:31 - too large a chunk of code you need to
08:32 - think about a small piece if you're
08:34 - going to describe a huge piece of code
08:36 - be very clear about what piece of code
08:37 - you're talking about
08:40 - and then when you look at that code you
08:41 - should figure out what n is and i don't
08:42 - mean like whether it's a hundred or a
08:44 - thousand i mean what is it measuring so
08:46 - if you're
08:47 - you have some code that's iterating over
08:49 - all the records in a database then n is
08:51 - how many records in the database and our
08:53 - bean example it was how many beans in
08:54 - the jar if you're doing a string search
08:56 - it might be the length of the string
09:00 - and then here's where the real work
09:02 - comes in
09:03 - you're going to think about that code
09:05 - running
09:06 - and you're going to figure out how many
09:07 - steps there are in the code in a typical
09:10 - run
09:11 - let me tell you what i mean by typical
09:13 - first
09:14 - there's two meanings of typical one is
09:16 - what kind of data is it going to get in
09:18 - the real world there's sort of real
09:20 - world data that kind of is what you kind
09:21 - of can expect and then there's
09:23 - worst-case data right a string of 40 000
09:26 - spaces is not typical data typical data
09:29 - is you know last names it's mostly ascii
09:32 - it's about most to 15 characters along
09:34 - that kind of thing so you can think
09:36 - about what's your typical data
09:37 - and then another meaning of a typical
09:39 - run is that over many runs of your
09:42 - algorithm
09:43 - there's a certain number of times a loop
09:45 - might run or a certain length of string
09:47 - it might get so you kind of think about
09:49 - the design center of your code
09:51 - and you imagine running that code
09:52 - through that design center and you count
09:54 - the steps and what i mean by steps is
09:56 - very vague and in a way a lot of this
09:58 - topic is very vague there are no units
10:00 - in anything we're talking about and the
10:02 - number of steps it kind of doesn't
10:04 - matter what you count as a step and it
10:07 - kind of doesn't matter that some steps
10:08 - might actually take longer than others
10:10 - because really what you're thinking
10:11 - about is if i'm doing n equals 10 i'll
10:14 - have this many steps now how many for n
10:16 - equals a hundred and so the exactly what
10:19 - steps there are doesn't matter as much
10:20 - as how does that count grow
10:23 - and i'll show you some examples so
10:24 - you'll get a sense of it
10:26 - so you count the steps in a typical run
10:28 - and since we put in n at the top and not
10:30 - 10 exactly the number of steps is going
10:32 - to be an expression in n you might end
10:34 - up with well it's 3n plus 47 steps
10:37 - something like that
10:39 - and then what you do is you keep only
10:40 - the most significant part of that
10:42 - expression so you keep only the highest
10:44 - coefficient piece and then you throw
10:46 - away the coefficients so if you had 47 n
10:48 - squared plus 53n plus 101 that's n
10:51 - squared you throw away all the lower
10:53 - order components and the exp and the
10:55 - coefficients and the reason is that as n
10:58 - gets larger and larger and larger the
11:00 - lower order components matter less and
11:02 - less right three n plus one the one is
11:05 - really important when n is one but when
11:06 - n is a billion who cares about the one
11:09 - right we're trying to get to that long
11:10 - term trend as the data gets very very
11:12 - large
11:13 - and if it's 3n that doubles when n
11:16 - doubles just the way n doubles when n
11:18 - doubles so the 3 is irrelevant too
11:21 - so you get rid of the lower order
11:22 - components and get rid of the
11:23 - coefficients and what's left is your big
11:25 - o notation
11:27 - now let's look at some examples true
11:28 - fact i wrote this code in november and
11:30 - it didn't occur to me until i was lying
11:31 - in bed this morning that this code is
11:34 - about moms and it's mother's day
11:40 - so here's an example of some code what
11:42 - we're going to do is we're going to have
11:43 - a data structure called moms which is a
11:46 - list of tuples and the tuples are people
11:48 - and their mothers
11:49 - right and then we're going to write a
11:51 - function called find mom and find mom is
11:53 - going to take that list of moms in the
11:54 - name of a child and it's going to find
11:56 - the child's mom
11:57 - okay
11:58 - now if you think about searching through
12:00 - this list in a typical run in some runs
12:02 - you'll find it in the first entry and
12:04 - some runs will find it in the last entry
12:06 - so on average in a typical case we'll
12:09 - find it about
12:10 - n over 2 times we're going to look
12:12 - through half the list
12:15 - so if we come down to this line this
12:17 - loop is going to run n over two times
12:20 - and i'm going to say that there are
12:21 - three steps in this loop we have to get
12:23 - the tuple out of the list and then we
12:24 - have to assign the child to child name
12:26 - and we have to assign the mom to mom
12:27 - name so there's three steps which means
12:29 - that this line is going to contribute
12:31 - three times n over two steps to our
12:33 - count
12:36 - this comparison we're going to do n over
12:38 - 2 times
12:39 - and there's only one step so that gives
12:41 - us another n over 2.
12:43 - and then this line is only going to
12:44 - happen once because it's the end of the
12:46 - function so that's going to give us one
12:48 - more step
12:49 - and so what our total is going to be 3n
12:51 - over 2 plus n over 2 plus 1 which
12:53 - simplifies down to 2n plus 1. i said
12:55 - that we get rid of the lower order
12:57 - components which is the 1 we get rid of
12:58 - the coefficient which is 2. this is an o
13:00 - of n
13:01 - function
13:03 - right so we've just determined that the
13:06 - algorithmic the sorry the asymptotic
13:08 - complexity of find mom is o of n and the
13:12 - way people say that in the real world in
13:14 - a cubicle is find mom is o of n or find
13:17 - mom is linear
13:21 - bind mom is of n
13:23 - okay and so you saw when we were going
13:25 - through the steps we didn't really care
13:27 - which steps were expensive which weren't
13:29 - we all we wanted to know is the
13:30 - relationship between the n and the
13:31 - number of steps and when n changes the
13:33 - number of steps is going to change and
13:35 - that's what we're looking for and notice
13:36 - we have no idea whether this is fast or
13:38 - slow right we don't know whether this
13:40 - function is going to take a minute or a
13:41 - millisecond all we know is that if we
13:43 - give it 10 times more data it's probably
13:44 - going to take 10 times as long
13:48 - let's look at another example
13:50 - also about moms the same same data
13:52 - structure the same moms data structure
13:54 - but now what we're going to do is we're
13:55 - going to
13:56 - write a function which tells us in that
13:58 - data structure how many grandmothers are
13:59 - there that is how many
14:01 - people how many people are mentioned
14:02 - both as a mom and as a child in our list
14:04 - right and so now we're going to go all
14:05 - the way through to the end of the list
14:08 - and i mentioned that n over 2 before but
14:10 - remember we're throwing away
14:11 - coefficients so in a way the half never
14:13 - mattered and as you work through this
14:16 - more you'll sort of get a sense of what
14:17 - you can not collect in the first place
14:20 - because you're going to throw it away
14:21 - anyway
14:23 - so for instance this line is going to
14:24 - run n times
14:26 - right and notice i didn't write 3 times
14:28 - n here because like i said we're
14:29 - throwing away coefficients this is an o
14:31 - of n line we're going to run this line n
14:33 - times
14:36 - now this line is going to run n times
14:38 - also
14:39 - but it's calling a function find mom
14:41 - which we just determined was an o of n
14:44 - operation in and of itself when you call
14:46 - it once at so of n and we're going to
14:47 - call it n times that's going to give us
14:49 - n squared
14:52 - and we can continue on and say this is n
14:55 - but remember we're going to throw away
14:56 - the lower order components we already
14:57 - found an n squared it's kind of
14:58 - uninteresting to keep finding the ends
15:02 - right but we're finding a bunch of n's
15:05 - we're going to end up with n squared
15:06 - plus some number of n plus 1 which is o
15:08 - of n squared
15:10 - right so find how many grandmothers is a
15:12 - quadratic function it's o of n squared
15:21 - now the ideal of course is o of one
15:23 - right constant time you can do the same
15:25 - amount you can work on any amount of
15:27 - data and not take it have it take any
15:29 - longer and it seems kind of impossible
15:31 - like how could that be but remember we
15:32 - saw len of my list as of one because no
15:35 - matter how long the list is the length
15:37 - is written on the outside we can just
15:38 - pick it up that's kind of boring
15:41 - really interesting is the looking up a
15:43 - key in a dictionary is of one no matter
15:44 - how many keys are in a dictionary it
15:47 - takes about the same amount of time to
15:48 - look up a key as in a one element
15:50 - dictionary and in the million element
15:51 - dictionary which is why dictionaries are
15:54 - heavily optimized and engineered and
15:57 - underpin every name lookup in python
15:59 - because they're fast
16:01 - and we'll get back to why it is but very
16:02 - quickly
16:03 - it's because there's a thing called a
16:05 - hash function which turns a key into a
16:06 - number and in typical data the numbers
16:09 - are all different and so you can very
16:11 - quickly use that number to find the
16:13 - place in the dictionary where the value
16:14 - is
16:18 - now no discussion of big-o notation be
16:21 - complete without showing you the graph
16:23 - along the bottom we have that flat green
16:24 - line labeled one of the the x-axis is
16:27 - data so data grows to the right and then
16:30 - the time grows going up um so the big
16:33 - flat line at the bottom is o of one that
16:35 - looks great log n was looking through
16:37 - the encyclopedia the linear line going
16:40 - diagonally is o of n
16:42 - and the big red one n squared just zooms
16:45 - literally off the chart right so the n
16:48 - squared is one of those bad things you
16:49 - try to avoid because it really grows
16:51 - really fast when things get big
16:57 - now when we looked at
16:58 - our code we have to understand what
17:01 - functions we're calling and how what
17:03 - kind of complexities they're adding into
17:05 - our total function this is a chart of
17:07 - the typical operational complexities of
17:09 - lists dicts and sets
17:12 - in python by the way when people say
17:14 - dictionaries are o of one that sentence
17:16 - doesn't make any sense and nouns like
17:18 - dictionary can't have an algorithmic
17:20 - complexity operations have an
17:21 - algorithmic complexity so what you're
17:23 - supposed to say is that looking up a key
17:25 - in a dictionary is of one
17:28 - now you'll notice that a lot of these
17:29 - are kind of the same appending to a list
17:31 - is o of one adding a key in the
17:32 - dictionary is of one adding a value and
17:35 - a set is o of one a big difference is
17:37 - looking up a value in a list is o of n
17:40 - so if you're going to search for a value
17:41 - in a list it's gonna have to look at
17:43 - every element in the list it's gonna be
17:44 - that left bean counting guy with the
17:46 - sweat coming off of his forehead
17:48 - but looking up a key in a dictionary or
17:50 - a value in a set is o of one which is
17:52 - why they're really really
17:54 - valued
17:56 - so pro tip right off the bat if you've
17:57 - got a program that's going too slow look
18:00 - to see if you're looking up a value in a
18:01 - list and replace it with lookup in a set
18:08 - but there are trade-offs so when i say
18:10 - replace the list lookup with a set
18:12 - lookup you've got to keep your eye on
18:13 - the big picture and this is where
18:14 - understanding what piece of code you
18:16 - care about matters
18:18 - so i said replace a list lookup with a
18:19 - set lookup let's say we have some code
18:21 - like this where we're going to make a
18:22 - list we make a list and then we try to
18:24 - find the thing in the list
18:26 - and that line is of n right we just saw
18:28 - that on the table of python complexities
18:31 - so you might think well i know what i'll
18:32 - do
18:33 - i'll make a set instead and then i can
18:35 - look it up in a set that's good if you
18:36 - can do that that's good so you don't
18:38 - make the list in the first place you
18:39 - make a set
18:42 - bad is you go ahead and you make a list
18:44 - anyway
18:44 - and then you convert it to a set and
18:46 - then you do the lookup in a set right so
18:48 - now that last line is great it's so of
18:50 - one but you've added a line before it
18:52 - turning the list into a set which itself
18:54 - is o of n
18:55 - like literally you've actually slowed
18:57 - down your program by a tiny amount you
18:59 - still have the o of n operation of
19:01 - converting the list into a set
19:04 - so it's very easy once you get into this
19:06 - algorithmic complexity stuff to get sort
19:08 - of focused on the little things and lose
19:10 - sight of the big picture right this
19:12 - would be a bad trade-off
19:14 - a good trade-off would be even if you're
19:16 - making a list if you can convert into a
19:17 - set once and then do many lookups so if
19:19 - you're going to do many lookups in your
19:21 - list then it makes sense to turn it into
19:22 - a set once and then you have o you have
19:25 - one o of n and then many o of one
19:28 - and your program will go faster
19:31 - so you always have to keep in mind what
19:32 - the real usage is of your code and where
19:35 - the time is being spent about where
19:37 - you're going to sort of work on reducing
19:39 - the algorithmic complexity and whether
19:40 - it's worth it
19:43 - now this is don't read this code the
19:45 - code doesn't matter this is a real
19:46 - example of code that got me started down
19:48 - this path this was from a project last
19:50 - summer the code on the left is shorter
19:52 - and has fewer data structures and fewer
19:54 - functions and in fact fewer loops but
19:56 - it's slower than the code on the right
19:58 - and the reason is that if we label
19:59 - things as of n and o of one the code on
20:02 - the left has an o of n operation there
20:04 - because it's looking up a value in a
20:05 - list
20:07 - the code on the right only has of one
20:09 - operations so the code on the right even
20:11 - though it's longer and has more
20:12 - functions and more data structures and
20:14 - more loops is o of one where the one on
20:17 - the right is o of n and in fact i was
20:18 - using it to draw drawings like this
20:21 - these functions work over an entire list
20:23 - of points and if you go up a level in
20:25 - the code you'll see actually that that
20:26 - function is called once for each point
20:28 - in the list of points
20:30 - so the o of n on our slow side was
20:32 - turning into o of n squared
20:34 - and so the slow code was taking 20
20:36 - seconds the fast code was taking a half
20:38 - a second on only 2 000 points right i
20:40 - said i've been giving examples like oh
20:42 - when n gets to a billion n got to 2 000
20:45 - here and made a huge difference in my
20:47 - running time because n squared is really
20:49 - worse than n with an n of 2000 n squared
20:53 - is 4 million operations i know of n is 2
20:55 - 000 operations and that's a big
20:57 - difference
20:58 - so it really does pay off sometimes to
21:00 - reduce the algorithmic complexity of
21:02 - your code to reduce the running time
21:08 - now we've been talking about o of one
21:09 - and o of n and over n squared there's
21:11 - more possibilities so there's more
21:13 - possibilities of kinds of complexities
21:14 - you might encounter in the real world
21:17 - of course there's o of n cubed and then
21:19 - fourth right if we called how many
21:20 - grandmothers once for every
21:22 - child for some reason we'd have an n
21:24 - cubed operation if i used that point
21:26 - algorithm once again for every point i'd
21:29 - have gone back up on an n level i'd add
21:31 - another coefficient to my n
21:34 - you can also have worse things like o of
21:36 - 2 to the n
21:38 - if you have n boolean choices and you
21:40 - try all the combinations of them you've
21:42 - got 2 to the n if you have n things and
21:44 - you try to
21:45 - try all permutations of those n things
21:47 - you'll have n factorial so
21:50 - as bad as n squared is there's sort of
21:52 - no upper limit to how horrible your code
21:54 - can get
21:57 - so think about how your loops are
21:59 - working where how much data you're
22:00 - working on and keep an eye on where
22:03 - those complexities are getting really
22:04 - really big
22:07 - the other kinds of possibilities is
22:08 - there could be more dimensions so we've
22:10 - been talking about doing algorithmic
22:11 - analysis where we have one variable n
22:14 - but you could have others right if i'm
22:16 - telling you that i've got a string
22:18 - search algorithm
22:20 - over some number of strings i should
22:22 - also have to consider the length of
22:23 - those strings typically they're fairly
22:25 - short but if you're doing you know bio
22:27 - python or something you have dna samples
22:29 - that are millions of characters long and
22:31 - then suddenly the lengths of the strings
22:32 - matter too
22:34 - for example when i was doing that
22:36 - point
22:37 - drawing
22:39 - code there was a line intersection
22:40 - algorithm that i found whose stated
22:43 - complexity was n plus k times log of n
22:46 - where n is the number of lines and k is
22:47 - the number of intersections among those
22:49 - lines and i don't know how to figure
22:50 - that out that's like a mathy thing that
22:52 - you can just read about i didn't have to
22:54 - figure out what that complexity was
23:00 - now we saw this graph before
23:04 - algorithm the complexity really matters
23:05 - as numbers get large but another place
23:08 - where you have to be careful not to over
23:10 - apply the idea is when numbers are small
23:12 - so let's zoom into that lower left-hand
23:14 - corner of this of the graph
23:17 - if we zoom in there suddenly the lines
23:18 - don't look so clear-cut the green line
23:21 - is actually above the other lines for
23:23 - most of them and the n squared line that
23:25 - red line is actually below everything
23:27 - for a lot of the time and the reason is
23:29 - that when numbers are small all those
23:31 - coefficients and lower order components
23:33 - that we threw away those mattered right
23:36 - three n plus one when n is one that one
23:38 - at the end really matters and also we
23:41 - haven't taken into account what the
23:43 - actual time of the steps is you might
23:44 - have an n squared operation where you
23:48 - are doing n squared times a millisecond
23:50 - and you might be comparing that with a
23:51 - constant time algorithm that always
23:53 - takes a minute
23:55 - well n has to get pretty large before
23:56 - that constant time algorithm is worth it
23:58 - when n is a billion it's worth it but
24:00 - when n is 10 you should stick with the n
24:02 - squared algorithm so as rob pike once
24:04 - said fancy algorithms are small when n
24:06 - is small and n is usually small so don't
24:10 - go overboard with
24:13 - trying to fancy up your algorithmic
24:15 - complexity
24:16 - it doesn't matter when n is small and
24:18 - usually your n is small
24:21 - all right some advanced topics there's a
24:23 - thing called amortization
24:24 - which is really a long term averaging
24:26 - over operations so when i say that
24:28 - appending to a list is o of 1 that
24:30 - doesn't mean that every single time you
24:32 - append to the list it takes a small
24:33 - amount of time in fact it usually takes
24:35 - a small amount of time but every once in
24:36 - a while the whole list has to be copied
24:38 - and moved someplace else which is which
24:40 - gets longer and longer as the list gets
24:42 - longer but it also gets less and less
24:44 - frequent as the list gets longer so that
24:46 - over the long run the average is still
24:48 - of one so amortization is a fancy word
24:51 - meaning averaging and it means that
24:53 - individual operations can take different
24:55 - amount of times
24:56 - algorithmic analysis is really about the
24:58 - long-term trends over many many runs
25:02 - and we haven't talked about the worst
25:04 - case so earlier we talked about the
25:06 - typical case and some people think big o
25:08 - implies typical case or big implies
25:10 - worst case no you have to say whether
25:12 - you're talking about the complexity of
25:13 - the typical case or the worst case
25:15 - here's an example where i make a set of
25:17 - 50 000 numbers which differ by 47 i'm
25:20 - kind of walking up the numbers by 47
25:23 - adding that number into the set is an of
25:25 - one operation so the whole building the
25:26 - whole set is o of n and it took about 10
25:28 - milliseconds
25:30 - here i'm building another set of
25:31 - integers exactly the same size 50 000
25:33 - numbers but i happened to choose
25:36 - a step that i happen to know was going
25:38 - to make all the hashes exactly the same
25:40 - so all the numbers got exactly the same
25:42 - hash which turns it into an of n
25:43 - operation which means making this set
25:45 - took 34 seconds 3 300 times longer
25:50 - dix also have this problem and people
25:52 - were using it to ddos web servers which
25:55 - is why python added hash randomization
25:57 - and it's a fascinating topic but it's an
25:59 - example where although dicks are of one
26:01 - in the typical case occasionally you
26:03 - have to worry about the worst case
26:07 - and there's more math so if you dig into
26:08 - the math basically mathematicians have
26:10 - taken every letter that either looks
26:12 - like an o or sounds like an o and given
26:13 - it a meaning
26:15 - and you don't need it you don't need it
26:16 - at all and there might be mathematicians
26:18 - in the audience right now who are going
26:19 - to say you know you're not really even
26:21 - talking about big o yeah shut up i don't
26:22 - care
26:25 - we all we all this is what we which is
26:27 - what we mean by big o
26:30 - and those experts by the way so i wrote
26:33 - a blog post when i was first starting to
26:34 - think about this with the same title big
26:36 - o how code slows as data grows and a lot
26:38 - of people liked it but one guy
26:41 - wasn't so pleased
26:43 - um
26:44 - he thought that not only had i gotten
26:45 - something wrong but the thing i'd gotten
26:47 - wrong was so important that the entire
26:48 - blog post was something that i should be
26:50 - ashamed of and i actually looked into it
26:52 - i gave him the benefit of the doubt i
26:54 - learned a little bit more about
26:54 - algorithmic analysis i concluded he was
26:56 - actually wrong he remains convinced he
26:59 - is right the good news is i got another
27:01 - blog post out of it
27:10 - so if on your journey to explore these
27:12 - things you find people like this just
27:14 - walk around them and keep going it
27:16 - doesn't matter
27:18 - and i mean if you're into the math go
27:19 - and do the math but if you're just
27:20 - trying to do software engineering it
27:22 - doesn't matter
27:24 - all right so in conclusion big o is
27:25 - useful it can help you understand how
27:27 - your code might perform when the data
27:29 - gets very large
27:30 - it doesn't have to be complicated it
27:32 - doesn't have to be mathy and you can do
27:34 - the thing
27:36 - thanks
27:37 - [Applause]

Cleaned transcript:

so as ned said my name is ned batchelder i'm ned batt on most social media and there's a bitly short link there at the bottom of the slide that links to this talk online if you want to follow along with the slides and those two short things will be at the bottom of most of the slides too so i've been writing software for a long time and one of the things that interests me about writing software is that there are two mindsets that inform the process of writing software the first is computer science which is really a branch of mathematics and so it's very theoretical you do proofs you think about very abstract concepts the other mindset that informs writing software is software engineering which is very pragmatic and is basically concerned only with whether you are writing software that works how can we write software that works and there is some crossover lots of computer science underpins software engineering but we don't think about it every day but there's a few topics that do cross over into the everyday of software engineering and in particular i'm interested in people who are working in software engineering who don't have formal computer science backgrounds and maybe feel a little bit insecure about that and one of the things that they seem to feel keeps them from sitting at the grownups table is this thing called big o a big o is really a simple thing and i've made a rhyme here to help you remember what it is it's about how your code slows as your data grows and an english major friend of mine pointed out that the rhyme is on the o sound like big o and i didn't intend that at all but that's cool so the question is how does your code slow down as the data gets larger and larger and this is not the same as the running time of any particular run of your code we're not trying to measure the time in seconds we're not trying to figure out exactly how many of them you can do in one transaction we're talking about the trend over time over many runs of your code how does it slow down as the data gets larger and larger and larger and one very pragmatic way to think about this is let's say you have a chunk of code you give it a certain amount of data it takes a certain amount of time how much longer will it take to work on 10 times as much data if i give it 10 times as much data how much longer does it take and you might think intuitively well it'll take 10 times as long obviously but that turns out not to be true some code will take 10 times as long some code will take twice as long some code will take a hundred times as long and some code won't take any longer at all and big o is all about characterizing that growth of the time of the code as the code as the data grows how the code slows as the data grows and computer science people approach this topic in a very very mathematical way but software engineers approach it in a very very pragmatic way and i'm trying to i'm going to explain the pragmatic approach it doesn't have to be done in a mathy way it can be done in a pragmatic way so let's get some terminology out of the way this is called the big o notation and the way it's written is a capital o and a parenthesis and then a bunch of stuff with an n in it and a closed parenthesis okay i told you it was simple the n is meant to stand in for how much data you have and the o stands for order of and the idea is that we're talking about the running time of your code grows on the same order of some mathematical expression of n the key thing here is although it looks like a function call there's a name and then parentheses with stuff inside it's not a function call it's just a notation and there's probably a mathy reason why it looks like a function call but it doesn't matter just know that it's not a function call it's just a way of labeling a piece of code as having a certain growth pattern so let's take a real world example let's say we have to count the number of beans and jars right we've got this guy on the left he opens up the jars he starts pulling out beans one by one right we can see here n is the number of beans right if we give this guy a jar with 10 times as many beans it's going to take him 10 times as long obviously and he's sweating this is what's known as o of an meaning that the time it takes to complete the task grows in the same way that n grows it's on the order of n if n doubles the time doubles if n is 10 times more the time is 10 times more and you might think well there's no other way to approach this task but there is of course you get beans jars that have labels on them that tell you how many beans are in them right this guy on the right has a much easier job you can see how much happier he is about it because it doesn't matter how large a jar you give this guy on the right it's going to take the same amount of time for him to tell you how many beans are in the jar this is what's known as o of one which is kind of a weird mathematician's way of saying that n isn't involved at all no matter what happens to n the running time remains the same o of n is is slower in the long run than o of one and this is a silly silly real world example um but for instance when you do for x in my list you you have an o of n operation because you have to look at every element in the list every being in the jar when you do len of my list you don't have to look at the elements of the list at all turns out python lists are kind of like those charts on the right the length of the list is written on a label on the outside of the list and so no matter how long the list is getting the length of the list is a constant time operation by the way these drawings were drawn by my son who is in art school one thing you might not have noticed if you look at their eyebrows they're shaped like beans that's that's art school for you all right another real world example let's say i tell you i'm going to give you a book and i want you to find a certain word in the book like horse right if i hand you a novel you're going to start reading until maybe you find the word horse right this is sounds like an o of n operation again right because if i give you a novel that's twice as long it might take you twice as long until you encounter the word horse but let's say i give you a different book i give you an encyclopedia now you open the encyclopedia to the middle if the word you're looking for is earlier than that then you do another divide and conquer step until you find the word horse if i now if i give you an encyclopedia that's twice as large it's not going to take you twice as long there's just one more divide and conquer step to find it that's what's called o log of n which is a fancy mathematician's way of saying that so these are both real world examples of the kinds of tasks that sort of sound similar when you first hear them but how you organize the data and therefore what algorithm you can use on the data really affects how the length of time it takes you to do the task changes as the size of the data changes right that's what we're talking about is how your code slows as your data grows let's get some other terms out of the way because i'm going to be speaking here and i might throw out some words that are a little bit different than earlier when we say o of one we might call it constant time i might say that the labeled bean jars are a constant time algorithm because the time remains the same no matter what o of n is often called a linear operation because if you look at it mathematically there's a linear relationship between the size the data and the running time of n squared is a thing we haven't seen yet but we will that's the case where when you give it 10 times more data it takes a hundred times as long to run and that's called quadratic because now you've got a quadratic equation involved if you don't know what a quadratic equation is it doesn't matter it's just a word that means n squared and some other words for big o it's sometimes called complexity or time complexity or algorithmic complexity or if you want to sound really fancy you can call it asymptotic complexity it's all the same thing right one of the underlying themes of this talk is that this this this topic of big o notation is littered with mathematical detritus that doesn't really matter to the key concept and don't let that stuff throw you that's just chaff being thrown at you by mathematicians you don't have to let it throw you off the path so how do you actually determine the big o of a piece of code the first step is you figure out what code you're talking about and that sounds kind of silly but in a large system there's you might be looking at one function and that might be the important thing but it really might actually be important to consider all the callers of the function or maybe you're looking at too large a chunk of code you need to think about a small piece if you're going to describe a huge piece of code be very clear about what piece of code you're talking about and then when you look at that code you should figure out what n is and i don't mean like whether it's a hundred or a thousand i mean what is it measuring so if you're you have some code that's iterating over all the records in a database then n is how many records in the database and our bean example it was how many beans in the jar if you're doing a string search it might be the length of the string and then here's where the real work comes in you're going to think about that code running and you're going to figure out how many steps there are in the code in a typical run let me tell you what i mean by typical first there's two meanings of typical one is what kind of data is it going to get in the real world there's sort of real world data that kind of is what you kind of can expect and then there's worstcase data right a string of 40 000 spaces is not typical data typical data is you know last names it's mostly ascii it's about most to 15 characters along that kind of thing so you can think about what's your typical data and then another meaning of a typical run is that over many runs of your algorithm there's a certain number of times a loop might run or a certain length of string it might get so you kind of think about the design center of your code and you imagine running that code through that design center and you count the steps and what i mean by steps is very vague and in a way a lot of this topic is very vague there are no units in anything we're talking about and the number of steps it kind of doesn't matter what you count as a step and it kind of doesn't matter that some steps might actually take longer than others because really what you're thinking about is if i'm doing n equals 10 i'll have this many steps now how many for n equals a hundred and so the exactly what steps there are doesn't matter as much as how does that count grow and i'll show you some examples so you'll get a sense of it so you count the steps in a typical run and since we put in n at the top and not 10 exactly the number of steps is going to be an expression in n you might end up with well it's 3n plus 47 steps something like that and then what you do is you keep only the most significant part of that expression so you keep only the highest coefficient piece and then you throw away the coefficients so if you had 47 n squared plus 53n plus 101 that's n squared you throw away all the lower order components and the exp and the coefficients and the reason is that as n gets larger and larger and larger the lower order components matter less and less right three n plus one the one is really important when n is one but when n is a billion who cares about the one right we're trying to get to that long term trend as the data gets very very large and if it's 3n that doubles when n doubles just the way n doubles when n doubles so the 3 is irrelevant too so you get rid of the lower order components and get rid of the coefficients and what's left is your big o notation now let's look at some examples true fact i wrote this code in november and it didn't occur to me until i was lying in bed this morning that this code is about moms and it's mother's day so here's an example of some code what we're going to do is we're going to have a data structure called moms which is a list of tuples and the tuples are people and their mothers right and then we're going to write a function called find mom and find mom is going to take that list of moms in the name of a child and it's going to find the child's mom okay now if you think about searching through this list in a typical run in some runs you'll find it in the first entry and some runs will find it in the last entry so on average in a typical case we'll find it about n over 2 times we're going to look through half the list so if we come down to this line this loop is going to run n over two times and i'm going to say that there are three steps in this loop we have to get the tuple out of the list and then we have to assign the child to child name and we have to assign the mom to mom name so there's three steps which means that this line is going to contribute three times n over two steps to our count this comparison we're going to do n over 2 times and there's only one step so that gives us another n over 2. and then this line is only going to happen once because it's the end of the function so that's going to give us one more step and so what our total is going to be 3n over 2 plus n over 2 plus 1 which simplifies down to 2n plus 1. i said that we get rid of the lower order components which is the 1 we get rid of the coefficient which is 2. this is an o of n function right so we've just determined that the algorithmic the sorry the asymptotic complexity of find mom is o of n and the way people say that in the real world in a cubicle is find mom is o of n or find mom is linear bind mom is of n okay and so you saw when we were going through the steps we didn't really care which steps were expensive which weren't we all we wanted to know is the relationship between the n and the number of steps and when n changes the number of steps is going to change and that's what we're looking for and notice we have no idea whether this is fast or slow right we don't know whether this function is going to take a minute or a millisecond all we know is that if we give it 10 times more data it's probably going to take 10 times as long let's look at another example also about moms the same same data structure the same moms data structure but now what we're going to do is we're going to write a function which tells us in that data structure how many grandmothers are there that is how many people how many people are mentioned both as a mom and as a child in our list right and so now we're going to go all the way through to the end of the list and i mentioned that n over 2 before but remember we're throwing away coefficients so in a way the half never mattered and as you work through this more you'll sort of get a sense of what you can not collect in the first place because you're going to throw it away anyway so for instance this line is going to run n times right and notice i didn't write 3 times n here because like i said we're throwing away coefficients this is an o of n line we're going to run this line n times now this line is going to run n times also but it's calling a function find mom which we just determined was an o of n operation in and of itself when you call it once at so of n and we're going to call it n times that's going to give us n squared and we can continue on and say this is n but remember we're going to throw away the lower order components we already found an n squared it's kind of uninteresting to keep finding the ends right but we're finding a bunch of n's we're going to end up with n squared plus some number of n plus 1 which is o of n squared right so find how many grandmothers is a quadratic function it's o of n squared now the ideal of course is o of one right constant time you can do the same amount you can work on any amount of data and not take it have it take any longer and it seems kind of impossible like how could that be but remember we saw len of my list as of one because no matter how long the list is the length is written on the outside we can just pick it up that's kind of boring really interesting is the looking up a key in a dictionary is of one no matter how many keys are in a dictionary it takes about the same amount of time to look up a key as in a one element dictionary and in the million element dictionary which is why dictionaries are heavily optimized and engineered and underpin every name lookup in python because they're fast and we'll get back to why it is but very quickly it's because there's a thing called a hash function which turns a key into a number and in typical data the numbers are all different and so you can very quickly use that number to find the place in the dictionary where the value is now no discussion of bigo notation be complete without showing you the graph along the bottom we have that flat green line labeled one of the the xaxis is data so data grows to the right and then the time grows going up um so the big flat line at the bottom is o of one that looks great log n was looking through the encyclopedia the linear line going diagonally is o of n and the big red one n squared just zooms literally off the chart right so the n squared is one of those bad things you try to avoid because it really grows really fast when things get big now when we looked at our code we have to understand what functions we're calling and how what kind of complexities they're adding into our total function this is a chart of the typical operational complexities of lists dicts and sets in python by the way when people say dictionaries are o of one that sentence doesn't make any sense and nouns like dictionary can't have an algorithmic complexity operations have an algorithmic complexity so what you're supposed to say is that looking up a key in a dictionary is of one now you'll notice that a lot of these are kind of the same appending to a list is o of one adding a key in the dictionary is of one adding a value and a set is o of one a big difference is looking up a value in a list is o of n so if you're going to search for a value in a list it's gonna have to look at every element in the list it's gonna be that left bean counting guy with the sweat coming off of his forehead but looking up a key in a dictionary or a value in a set is o of one which is why they're really really valued so pro tip right off the bat if you've got a program that's going too slow look to see if you're looking up a value in a list and replace it with lookup in a set but there are tradeoffs so when i say replace the list lookup with a set lookup you've got to keep your eye on the big picture and this is where understanding what piece of code you care about matters so i said replace a list lookup with a set lookup let's say we have some code like this where we're going to make a list we make a list and then we try to find the thing in the list and that line is of n right we just saw that on the table of python complexities so you might think well i know what i'll do i'll make a set instead and then i can look it up in a set that's good if you can do that that's good so you don't make the list in the first place you make a set bad is you go ahead and you make a list anyway and then you convert it to a set and then you do the lookup in a set right so now that last line is great it's so of one but you've added a line before it turning the list into a set which itself is o of n like literally you've actually slowed down your program by a tiny amount you still have the o of n operation of converting the list into a set so it's very easy once you get into this algorithmic complexity stuff to get sort of focused on the little things and lose sight of the big picture right this would be a bad tradeoff a good tradeoff would be even if you're making a list if you can convert into a set once and then do many lookups so if you're going to do many lookups in your list then it makes sense to turn it into a set once and then you have o you have one o of n and then many o of one and your program will go faster so you always have to keep in mind what the real usage is of your code and where the time is being spent about where you're going to sort of work on reducing the algorithmic complexity and whether it's worth it now this is don't read this code the code doesn't matter this is a real example of code that got me started down this path this was from a project last summer the code on the left is shorter and has fewer data structures and fewer functions and in fact fewer loops but it's slower than the code on the right and the reason is that if we label things as of n and o of one the code on the left has an o of n operation there because it's looking up a value in a list the code on the right only has of one operations so the code on the right even though it's longer and has more functions and more data structures and more loops is o of one where the one on the right is o of n and in fact i was using it to draw drawings like this these functions work over an entire list of points and if you go up a level in the code you'll see actually that that function is called once for each point in the list of points so the o of n on our slow side was turning into o of n squared and so the slow code was taking 20 seconds the fast code was taking a half a second on only 2 000 points right i said i've been giving examples like oh when n gets to a billion n got to 2 000 here and made a huge difference in my running time because n squared is really worse than n with an n of 2000 n squared is 4 million operations i know of n is 2 000 operations and that's a big difference so it really does pay off sometimes to reduce the algorithmic complexity of your code to reduce the running time now we've been talking about o of one and o of n and over n squared there's more possibilities so there's more possibilities of kinds of complexities you might encounter in the real world of course there's o of n cubed and then fourth right if we called how many grandmothers once for every child for some reason we'd have an n cubed operation if i used that point algorithm once again for every point i'd have gone back up on an n level i'd add another coefficient to my n you can also have worse things like o of 2 to the n if you have n boolean choices and you try all the combinations of them you've got 2 to the n if you have n things and you try to try all permutations of those n things you'll have n factorial so as bad as n squared is there's sort of no upper limit to how horrible your code can get so think about how your loops are working where how much data you're working on and keep an eye on where those complexities are getting really really big the other kinds of possibilities is there could be more dimensions so we've been talking about doing algorithmic analysis where we have one variable n but you could have others right if i'm telling you that i've got a string search algorithm over some number of strings i should also have to consider the length of those strings typically they're fairly short but if you're doing you know bio python or something you have dna samples that are millions of characters long and then suddenly the lengths of the strings matter too for example when i was doing that point drawing code there was a line intersection algorithm that i found whose stated complexity was n plus k times log of n where n is the number of lines and k is the number of intersections among those lines and i don't know how to figure that out that's like a mathy thing that you can just read about i didn't have to figure out what that complexity was now we saw this graph before algorithm the complexity really matters as numbers get large but another place where you have to be careful not to over apply the idea is when numbers are small so let's zoom into that lower lefthand corner of this of the graph if we zoom in there suddenly the lines don't look so clearcut the green line is actually above the other lines for most of them and the n squared line that red line is actually below everything for a lot of the time and the reason is that when numbers are small all those coefficients and lower order components that we threw away those mattered right three n plus one when n is one that one at the end really matters and also we haven't taken into account what the actual time of the steps is you might have an n squared operation where you are doing n squared times a millisecond and you might be comparing that with a constant time algorithm that always takes a minute well n has to get pretty large before that constant time algorithm is worth it when n is a billion it's worth it but when n is 10 you should stick with the n squared algorithm so as rob pike once said fancy algorithms are small when n is small and n is usually small so don't go overboard with trying to fancy up your algorithmic complexity it doesn't matter when n is small and usually your n is small all right some advanced topics there's a thing called amortization which is really a long term averaging over operations so when i say that appending to a list is o of 1 that doesn't mean that every single time you append to the list it takes a small amount of time in fact it usually takes a small amount of time but every once in a while the whole list has to be copied and moved someplace else which is which gets longer and longer as the list gets longer but it also gets less and less frequent as the list gets longer so that over the long run the average is still of one so amortization is a fancy word meaning averaging and it means that individual operations can take different amount of times algorithmic analysis is really about the longterm trends over many many runs and we haven't talked about the worst case so earlier we talked about the typical case and some people think big o implies typical case or big implies worst case no you have to say whether you're talking about the complexity of the typical case or the worst case here's an example where i make a set of 50 000 numbers which differ by 47 i'm kind of walking up the numbers by 47 adding that number into the set is an of one operation so the whole building the whole set is o of n and it took about 10 milliseconds here i'm building another set of integers exactly the same size 50 000 numbers but i happened to choose a step that i happen to know was going to make all the hashes exactly the same so all the numbers got exactly the same hash which turns it into an of n operation which means making this set took 34 seconds 3 300 times longer dix also have this problem and people were using it to ddos web servers which is why python added hash randomization and it's a fascinating topic but it's an example where although dicks are of one in the typical case occasionally you have to worry about the worst case and there's more math so if you dig into the math basically mathematicians have taken every letter that either looks like an o or sounds like an o and given it a meaning and you don't need it you don't need it at all and there might be mathematicians in the audience right now who are going to say you know you're not really even talking about big o yeah shut up i don't care we all we all this is what we which is what we mean by big o and those experts by the way so i wrote a blog post when i was first starting to think about this with the same title big o how code slows as data grows and a lot of people liked it but one guy wasn't so pleased um he thought that not only had i gotten something wrong but the thing i'd gotten wrong was so important that the entire blog post was something that i should be ashamed of and i actually looked into it i gave him the benefit of the doubt i learned a little bit more about algorithmic analysis i concluded he was actually wrong he remains convinced he is right the good news is i got another blog post out of it so if on your journey to explore these things you find people like this just walk around them and keep going it doesn't matter and i mean if you're into the math go and do the math but if you're just trying to do software engineering it doesn't matter all right so in conclusion big o is useful it can help you understand how your code might perform when the data gets very large it doesn't have to be complicated it doesn't have to be mathy and you can do the thing thanks
