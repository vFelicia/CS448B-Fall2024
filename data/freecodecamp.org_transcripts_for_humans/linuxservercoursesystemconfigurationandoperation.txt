With timestamps:

00:00 - linux is a popular operating system for
00:02 - server administration because it's
00:04 - secure stable and flexible in this
00:06 - course sean powers from cbt nuggets will
00:09 - detail every part of configuring
00:11 - monitoring and supporting a server setup
00:14 - that runs the linux operating system
00:16 - this course will teach you everything
00:18 - you need to know to configure linux
00:20 - servers including the boot process
00:22 - kernel modules network connection
00:24 - parameters localization groups and more
00:28 - let's get started
00:31 - [Music]
00:34 - bios and uefi are two tools that
00:36 - basically do the same sort of thing but
00:39 - they can be a little bit confusing when
00:42 - it comes to how do you know which one to
00:44 - use on a computer and and do i need to
00:46 - support one or the other or both or the
00:49 - nice thing is you usually either have
00:51 - bios or uefi now bios is an older
00:55 - program it just stands for basic input
00:57 - output system whereas uefi is the new
01:00 - kit on the block and this stands for
01:02 - unified extensible firmware interface
01:05 - which sounds confusing but really it's
01:08 - just the the way that we can interact
01:10 - between the hardware and the operating
01:12 - system now let's say that we have two
01:15 - different vehicles now these are two
01:17 - obviously very very different vehicles
01:19 - one of them is a sports car and one of
01:22 - them is an awesome yellow volkswagen
01:24 - beetle i actually have a volkswagen
01:27 - beetle that's yellow like this and it's
01:29 - awesome but nonetheless you could be a
01:31 - sports car person either way even though
01:33 - they're ridiculously different vehicles
01:35 - they both have some common interfaces
01:38 - right they're both going to have brake
01:40 - pedals they're both going to have
01:42 - steering wheels
01:44 - they're both going to have windshields
01:46 - and those interfaces are fairly common
01:48 - across all vehicles now the brakes in
01:51 - the underlying system are going to be
01:52 - different this sports car probably has
01:54 - really nice disc brakes whereas these
01:56 - old volkswagens have drum brakes now
01:59 - drum brakes aren't as good but the
02:00 - interface itself is very very similar
02:03 - you push the brake pedal and you stop
02:05 - and that's kind of what bios and uefi
02:09 - are they're interfaces between the
02:11 - hardware and the operating system itself
02:14 - now they do work a little bit
02:15 - differently so here we have our hard
02:17 - drive on our system now using bios like
02:20 - the old method of booting a computer you
02:23 - would have like the very first sector on
02:26 - the hard drive would be the boot sector
02:28 - and that's where the mbr the master boot
02:30 - record would live and then that would
02:33 - tell the computer where the partitions
02:35 - are and point it to where to boot now
02:37 - there's a lot of limitations with bile
02:39 - so you could only have
02:41 - four partitions using the bios and mbr
02:43 - combination there are some hacks to get
02:46 - around that you know they would take a
02:47 - partition and do extended partitions
02:49 - inside that but that's a whole nother
02:50 - nugget still there was this limitation
02:52 - also a limitation of size with the drive
02:55 - how much this boot sector or this master
02:58 - boot record can actually reference it
02:59 - can be a small amount like two terabytes
03:01 - instead of exabytes of data and so uefi
03:05 - is a replacement for the bios technology
03:08 - and rather than just have the single
03:11 - boot sector what it does is there's an
03:12 - entire partition
03:14 - on the system and that partition is
03:18 - where all of the boot code is for
03:20 - whatever operating systems might be on
03:22 - the computer so rather than just the
03:23 - boot sector pointing to
03:25 - you know the rest of the hard drive this
03:26 - is an actual
03:27 - specialized partition
03:30 - on the computer and that's where the
03:32 - uefi code is stored also it uses a
03:35 - different partition scheme so you can
03:37 - have tons and tons of partitions and it
03:39 - can address a much larger hard drive
03:41 - there's also other things like secure
03:42 - boot that uefi supports that bios
03:45 - doesn't basically just know that uefi is
03:48 - the replacement for bios it replaces the
03:51 - functionality of connecting the hardware
03:54 - to the software of the operating system
03:56 - now the good news is if you have a
03:58 - computer that's older or even some new
04:00 - ones so come with bios you can still get
04:02 - around a lot of the limitations because
04:04 - there are hacks that will let you use
04:06 - really big hard drives or hacks that
04:08 - will allow you to do some of the things
04:10 - that you can't normally do out of the
04:12 - box but most computers now are coming
04:14 - with uefi and from a from an installer
04:18 - point of view there's very little you
04:20 - have to do because the operating system
04:22 - is going to say okay i was booted using
04:24 - uefi so i'm going to create a partition
04:26 - a uefi partition and i'm going to put
04:28 - all the boot code in there so from your
04:31 - standpoint from the end user or the
04:33 - installer standpoint there's very little
04:35 - difference but under the hood a lot of
04:38 - new cool stuff is going on and that's
04:40 - why uefi is kind of the way of the
04:42 - future
04:44 - remember when you were a kid and in the
04:46 - doctor's waiting room there were these
04:48 - magazines and one of the games inside
04:50 - the magazines was that you had to find
04:53 - the difference in one of a bunch of
04:55 - similar looking
04:57 - figures like for example here are what i
04:59 - have nine different jokers and one of
05:02 - them is a little different now it's not
05:04 - terribly difficult to see which one is
05:06 - different here if you watched while they
05:07 - appeared this one has a tiny little spot
05:10 - right there on his shirt that's
05:12 - different than all the others so we've
05:14 - been able to identify it but you can see
05:15 - they're very very very very similar well
05:18 - the same thing is true about grub
05:21 - and grub 2 which of course is the next
05:24 - iteration of grub now grub stands for
05:26 - grand unified boot loader but really
05:28 - it's just the way that the computer
05:30 - transitions from bios or uefi booting up
05:33 - conditions into the actual operating
05:35 - system itself it's what tells the
05:37 - computer okay where's my partitions and
05:39 - stuff like that it's very easy to
05:41 - confuse which is which on your system
05:44 - which seems silly but it can be very
05:46 - very embarrassing when you sit down at a
05:48 - system you're like okay so is this
05:50 - running grub or grub 2 because they do
05:54 - the same thing right they both boot the
05:56 - computer but they do have some minor
05:58 - nuanced differences grub of course is
06:01 - older you probably guessed that by it
06:03 - not having a 2 at the end of its name
06:05 - but grub is often called grub legacy
06:07 - because it is older but it's still on a
06:09 - few operating systems not many but a few
06:12 - i think slackware still uses grub legacy
06:14 - now the biggest and easiest way to
06:17 - figure out if you're dealing with grub
06:19 - or grub 2 on your system is to look
06:22 - inside the boot grub folder and if you
06:26 - see
06:27 - menu.lst
06:29 - or grub.com that means you're running
06:33 - grub legacy because grub 2 does not have
06:36 - those configuration files grub 2 has
06:38 - this configuration file grub.cfg
06:41 - now it's easy to confuse
06:44 - cfg.com so i always just look for
06:46 - menu.lst if that exists you're on grub
06:50 - legacy now the thing about grub is it's
06:52 - kind of difficult to modify it was uh
06:54 - it's very easy for automated systems
06:57 - like when you install a new kernel to
06:58 - figure out how to put how to you know
07:00 - update the menu and update the boot code
07:02 - but it's really difficult for the end
07:03 - user to modify that stuff and the boot
07:06 - menu usually when it's booting up it
07:08 - says you know press something in 10
07:09 - seconds in order to change the way it
07:12 - boots so this usually just appears for
07:13 - you and you can see it now grub 2
07:16 - is a lot more customizable in fact if
07:18 - you go into the etc default grub file
07:21 - this is a configuration file that's easy
07:23 - to read and it's going to allow you to
07:25 - change the way that it looks when it
07:26 - boots up now grub 2 also has a ton of
07:28 - other cool features it can boot from an
07:31 - iso file from a usb disk it can name or
07:34 - it can identify hard drives based on
07:36 - their uuid or their device like dev sda
07:38 - so it's a lot more advanced than grub
07:41 - one but one of the you know the
07:43 - advancements that's also a frustration
07:45 - is that boot menu that i talked about
07:46 - that comes up with grub it's hidden with
07:49 - grub 2. so it just goes right to the
07:51 - login screen you never see anything from
07:53 - grub 2. and if you don't know this
07:55 - little trick it can be a real bugger to
07:57 - get into that menu if you want to change
07:59 - something during boot so be sure to just
08:01 - hold down the shift key when the
08:02 - computer is booting up and then boom all
08:04 - of a sudden you're going to get into the
08:06 - grub 2 interactive menu that you can
08:09 - change boot code things on the fly it's
08:11 - really really cool i'll show you what i
08:13 - mean but it's not difficult once you
08:15 - know if you have grub or grub 2 grub 2
08:18 - which allows you to do a lot more
08:19 - configuration stuff now i'm here on an
08:21 - ubuntu system this has grub 2 and if we
08:24 - were to look at let's just look at it
08:27 - etc default
08:29 - grub we're going to see this is just a
08:31 - configuration file we can change these
08:33 - things once you make a change you do
08:35 - have to do sudo
08:37 - update grub you do that it's going to
08:40 - update the boot code inside the boot
08:42 - folder and if we look in that boot
08:44 - folder so go into boot
08:47 - grub type ls i look at that grub dot cfg
08:50 - there's no menu.lst so we know that this
08:53 - is grub 2. so that's how you manage
08:55 - things in grub 2. i'll show you really
08:57 - quickly before we end this is a computer
08:58 - that's turned off if i start this
09:00 - computer and hold down the shift key as
09:03 - long as i hold down the shift key boom
09:05 - we get this menu but if i don't hold
09:06 - that we don't see any menu
09:08 - for us to interact with grub at all now
09:11 - figuring out if you have grub or grub 2
09:13 - can be a little bit challenging if you
09:15 - don't know that simple trick to look for
09:16 - menu.lst but they both do the same thing
09:20 - they tell the computer how to boot up
09:22 - and how to mount its different
09:23 - partitions it's just that grub 2 is
09:25 - definitely an advancement it's more
09:27 - configurable it does more things and
09:29 - it's easier for the end user to manage i
09:31 - hope this has been informative for you
09:33 - and i'd like to thank you for viewing
09:35 - linux is so flexible it can boot from an
09:38 - incredible number of
09:40 - methods or sources now there are several
09:43 - different things we need to understand
09:44 - about the boot processes like is it
09:46 - hardware based or is it software based
09:48 - and i'll tell you what i mean about that
09:49 - in a minute but there are just this
09:51 - multitude of ways that a linux system
09:54 - can boot it can boot over pxe or pxe
09:56 - boot if you haven't seen that it's just
09:57 - the coolest thing ever it's when it
09:58 - boots up completely over the network we
10:01 - can boot from usb from cd ipxc is a more
10:04 - advanced version of pixie i'll talk
10:05 - about that too and then iso images just
10:07 - like the dot iso file you can actually
10:10 - make a grub 2 entry that will boot
10:12 - directly from the iso even though you
10:14 - don't have it burned to a disk or to a
10:16 - usb drive it can live on your system and
10:18 - you can have a grub menu item that boots
10:21 - directly to that iso image it's really
10:23 - flexible what it can do but that
10:25 - hardware software thing i wanted to talk
10:26 - about because where the boot process is
10:28 - taking place is really important some of
10:31 - it is linux specific some of it is not
10:33 - so first of all let's talk about the
10:34 - hardware things now when i say hardware
10:36 - i mean the bios or the uefi the part
10:39 - that takes place before linux is ever
10:41 - introduced and pxe is one of those
10:43 - things pxc which stands for pre-boot
10:46 - execution environment is the way that
10:49 - the hardware says okay i don't see a
10:52 - hard drive or i'm not set up to use a
10:54 - hard drive so i'm going to query the
10:56 - network and it just queries a dhcp
10:58 - server and the dhcp server responds with
11:02 - not only an ip address which is what
11:04 - dhcp servers normally do but also a boot
11:07 - file in a tftp location a tftp server is
11:11 - just a place you can store files on a
11:13 - network and then basically the computer
11:16 - then downloads that
11:18 - image that boot image from the tftp
11:21 - server and that's where linux comes into
11:22 - play that kernel or that boot file is
11:25 - the linux kernel so it downloads it off
11:27 - the internet or off the internet um off
11:30 - of your local network and then it puts
11:33 - it in memory and boots itself from there
11:35 - so pxe starts as just a hardware thing
11:38 - and then turns into a software there's
11:40 - also ipxc which is very similar but
11:43 - instead of using tftp to download that
11:46 - thing it allows you to use http which is
11:50 - faster and usually more reliable than
11:52 - the old-fashioned tftp but it's very
11:54 - similar in concept ipxc if your computer
11:57 - supports it then usb this is also right
11:59 - on the hardware now the hardware
12:01 - determines exactly how to boot from the
12:04 - usb but on the usb itself is where the
12:07 - linux code is right so the hardware
12:09 - knows that it can boot from usb same
12:11 - thing with cd and to be quite honest
12:13 - same thing with a hard drive right the
12:15 - hard drive is booted too because the
12:18 - computer itself understands how to do
12:20 - that but when linux comes into play
12:23 - we'll say the software side of things
12:25 - this is once it loads into linux linux
12:28 - does some things on its own like the iso
12:31 - booting with grub2 that's after linux
12:33 - starts right or after grub starts it
12:35 - says okay i'm in grub now what are we
12:38 - going to do we're going to either mount
12:40 - a partition or we're going to look at
12:42 - this iso file and actually use that as
12:45 - our operating system just like it was
12:47 - burned to a cd so some of this stuff is
12:50 - software this is where you like select a
12:52 - kernel this is where you can have mem
12:54 - test so a lot of the stuff is done in
12:56 - software but most things actually all
12:58 - things have to start in the hardware
13:00 - otherwise you're never going to you know
13:02 - get to the point where the software
13:03 - takes control but it's important to know
13:05 - that pxe
13:06 - ipxc these are not linux specific boot
13:10 - methods these are boot methods the
13:11 - computer supports that linux also
13:13 - supports in that they can provide the
13:15 - boot file so i don't want you to confuse
13:17 - pxe network booting with something
13:19 - specific to linux same with usb and cd
13:22 - we can boot windows from usb or cds
13:25 - and it works you know because the
13:27 - hardware supports it if i'm honest
13:29 - probably the most fun way to boot a
13:31 - computer is using pxe only because
13:34 - there's no media right it just boots
13:35 - directly off the network and for some
13:37 - reason that's just really awesome to to
13:39 - be a part of it's really awesome to see
13:42 - but there are multiple ways that you can
13:44 - boot linux and it's important to know
13:46 - that they're all there it's okay if you
13:48 - don't know exactly how to you know boot
13:50 - from an iso file using grub 2. anybody's
13:53 - going to have to google the specifics of
13:54 - that in order to make it work but
13:56 - knowing all these different processes
13:58 - exist for booting a computer that's
14:01 - vital because that will help you learn
14:03 - and help you troubleshoot when you run
14:05 - into a booting issue when it comes to
14:06 - actually booting the linux kernel there
14:09 - are a lot of problems that need to be
14:12 - solved and it's sometimes sort of like a
14:14 - chicken and an egg scenario you have to
14:16 - do one thing but you can't do that thing
14:18 - until the other thing is done and what
14:19 - do you do first and how do you do it
14:21 - it can be really confusing but basically
14:23 - we want to get the full kernel with all
14:25 - of its modules running so the boot
14:28 - process is very complicated and it might
14:31 - seem over complicated but the issue is
14:33 - to try to get the kernel running and
14:36 - then allow it to access the modules that
14:39 - are stored on the hard drive on your
14:41 - system and it ends up being
14:43 - like i said fairly complicated and if
14:45 - i'm honest i've been a
14:48 - system administrator for over 20 years
14:50 - i've passed lots of certification tests
14:53 - and i have never fully understood every
14:56 - step of the process of booting the
14:58 - kernel so you're going to be like the
15:00 - king of the next nerdy party that you go
15:03 - to because you're going to you're going
15:05 - to know all the trivial pursuit answers
15:07 - when it comes to linux kernels so let's
15:09 - look and see what the boot process
15:11 - actually is and then i'll show you on a
15:14 - system where those files live so i have
15:16 - sort of a little flow chart here now a
15:18 - lot of these steps we probably are
15:20 - already familiar with first the computer
15:21 - either has bios or uefi and that's the
15:24 - hardware on the computer which looks for
15:27 - something that grub or grub too it's one
15:29 - or the other here will provide and then
15:32 - that boot code of you know that grub
15:34 - uses points us to the kernel now this is
15:38 - the stuff where it starts to get a
15:39 - little not confusing so much but
15:41 - complicated it's almost like this fine
15:43 - dance that has to be done now norm what
15:46 - the process is normally is we have the
15:48 - actual linux kernel itself which is a
15:52 - file one file and it's called vm linux
15:56 - or you probably currently see it called
15:58 - vm linus with the z and the only
16:01 - difference here these are the same file
16:04 - the pr the only difference is that this
16:06 - is compressed right it's the z means
16:08 - that it's compressed so it's just a
16:10 - space-saving method uh so usually we use
16:13 - vm linus just to save some room on the
16:16 - system and then this is the the kernel
16:19 - itself with no modules right so there's
16:22 - no modules this is just the base kernel
16:24 - and then
16:25 - once the kernel boots up it will mount
16:28 - all the file systems and then it will
16:30 - have access to all of the modules that
16:34 - it needs to insert to make things work
16:36 - like you know your usb
16:38 - mouse and your keyboard and your monitor
16:40 - and your video card all those things are
16:42 - modules that are loaded
16:45 - into the kernel they're not part of the
16:48 - the static kernel right i mean we could
16:50 - build this huge kernel that includes
16:52 - everything but that's just a waste of
16:54 - resources and so we have this
16:57 - stripped-down kernel that has just
17:00 - enough stuff to make sure that we can
17:02 - mount the file system so that it can
17:03 - have access to all of its modules and
17:06 - then become the full running kernel on
17:08 - our system there's a few problems what
17:10 - if this full kernel
17:13 - and all the modules yeah so basically
17:15 - what if all the modules here are on a
17:17 - file system
17:18 - that this stripped down kernel doesn't
17:21 - know
17:22 - how to mount right it's like well i
17:24 - don't know how to get onto a raid device
17:27 - or i don't have any idea how to mount
17:30 - this fancy new ssd drive that you put
17:33 - into the pcie slot or something well
17:35 - that's where init rd or init ram disk
17:38 - comes into play this is just enough
17:42 - information
17:43 - like module information and driver
17:47 - information to be able to have the linux
17:50 - kernel
17:51 - access the file system so that it can
17:54 - get to its modules so rather than make
17:57 - you know a custom
17:59 - bigger kernel for every specific system
18:02 - what we've done is we have a generic
18:04 - strip down kernel and then this init rd
18:08 - has the stuff we need in order to load
18:12 - the modules
18:14 - by mounting the hard drive or you know
18:16 - whatever we need maybe this is stored in
18:18 - a network drive so we have to have the
18:20 - nfs stuff in order to mount a remote
18:23 - disk on the on the computer so it can
18:25 - access its module so this is just like
18:27 - the the temporary staging area for
18:29 - kernel stuff that we need and this is
18:31 - kind of inserted right into this
18:35 - running stripped down kernel okay now i
18:38 - want to mention this because a lot of
18:40 - people confuse the init ram disk with
18:43 - the init ram file system and they seem
18:47 - similar conceptually they are fairly
18:49 - similar conceptually except that the
18:52 - init ram fs is actually part of the
18:55 - actual kernel itself it's part of this
18:57 - vm linus or vm linux kernel and this is
19:00 - just like it says it's a file system
19:03 - that it creates and mounts in ram and i
19:06 - want to mention this tool even though
19:07 - we're not going to go into it much
19:08 - dracut is a tool that has made this
19:10 - extremely generic using udev and and
19:14 - stuff like that so that it's very
19:15 - flexible but this is basically the tiny
19:18 - little file system that the linux kernel
19:20 - uses in order to do what it needs to do
19:23 - to get to the point where it can load
19:25 - the full kernel by loading modules so
19:28 - init ram fs is a file system that it
19:30 - loads into ram but that's part of the
19:32 - kernel itself this init rd is a ram disk
19:35 - that is mounted alongside of the kernel
19:38 - that allows it to get to the point so it
19:40 - can use the full kernel and knit ram
19:43 - disk is not used after the system is
19:45 - booted up this is just like a temporary
19:47 - staging ground to get to the full kernel
19:51 - wow that was a lot of information but
19:53 - it's really
19:54 - fairly straightforward when you see why
19:56 - it's doing all of those complicated
19:58 - things now i want to show you really
19:59 - quickly some of the files that we talked
20:01 - about so here i am inside ubuntu and
20:04 - this is the boot folder and we can see
20:06 - we have here's the init ram sure enough
20:08 - and this is i have a couple different
20:10 - kernels in here there is an update but
20:12 - the kernel numbers are listed here but
20:14 - here's the init ram disk and here is the
20:17 - actual kernel file itself there's a
20:19 - couple other files the system map
20:21 - actually tells the kernel where on the
20:23 - file system all of its modules live and
20:26 - then config this is the configuration
20:28 - file when the kernel was actually
20:30 - compiled so if you want to see the
20:32 - options that were used there but those
20:34 - are the main files that are on the
20:36 - system one other thing i want to show
20:37 - you is this system.map tells the kernel
20:40 - to look in
20:41 - lib modules and then the name of the
20:44 - actual running kernel and inside there
20:47 - is where you'll see the actual different
20:49 - modules okay and this is where the
20:52 - modules that are going to be loaded in
20:53 - to get that full kernel live so that's
20:56 - the basic uh basics of starting a linux
20:58 - kernel on your system now i know we
21:01 - covered a lot of things and a lot of
21:03 - terms and a lot of concepts but they
21:06 - should make sense once you go through
21:07 - that flowchart of what's happening and
21:09 - why it's happening and in the end which
21:11 - comes first the chicken or the egg well
21:12 - it depends on what you need to do right
21:14 - if you need an egg then the egg comes
21:16 - first if the chicken can handle it on
21:18 - its own no eggs are needed
21:20 - now a kernel panic certainly seems like
21:23 - a really good time to panic but i assure
21:25 - you just because the colonel panics
21:27 - doesn't mean you should soldier but
21:30 - really kernel panic just means that
21:32 - something went wrong with the kernel now
21:33 - there's a couple common causes and
21:35 - there's a couple things we can do
21:36 - depending on what the root cause is now
21:38 - if all of a sudden you're starting to
21:40 - get kernel panics and you haven't done
21:42 - any system updates recently
21:44 - and it's kind of hit or miss it'll
21:46 - happen it won't happen it happens when
21:48 - you're doing something but not another
21:50 - chances are you may have a piece of
21:51 - faulty hardware and this is one of the
21:53 - really common ways that you can
21:55 - experience a kernel panic
21:57 - if you have an overclocked cpu for
21:59 - example that can sometimes cause kernel
22:01 - panics a bad stick of ram is another one
22:03 - that'll really do it and you know some
22:05 - add-on card like video cards are
22:07 - sometimes you know guilty of doing this
22:10 - really anything on your system any
22:11 - hardware if it fails it can cause a
22:14 - kernel panic there's a couple ways to
22:16 - you know try different things especially
22:17 - if you have multiple sticks of ram
22:19 - sometimes you can pull out a stick of
22:21 - ram see if you still get a kernel panic
22:23 - if you do then put that stick back in
22:25 - and keep pulling them out one at a time
22:27 - and see if the kernel panics
22:28 - automatically go away when you remove
22:30 - one certain stick hey chances are that's
22:32 - what's wrong with you know that certain
22:34 - stick is a bad stick of ram it can be
22:37 - frustrating to track down hardware
22:39 - related kernel panics but there's also
22:41 - another very common kind of kernel panic
22:43 - that happens and that's when you upgrade
22:45 - your system so let's say that you just
22:48 - ran a system update on your system and
22:50 - it installed a new kernel you reboot the
22:53 - computer and then boom it says something
22:55 - like failure to init or kernel panic
22:57 - blah blah blah blah if that happens
23:00 - there is something that you can do which
23:02 - is fairly easy to do and you can you
23:04 - know rescue your system so to speak so
23:07 - when it boots up we're just going to go
23:08 - into grub and we're going to pick an
23:10 - older kernel because when you update a
23:11 - system
23:12 - it keeps the older kernel for this exact
23:15 - reason if something goes wrong you can
23:17 - pick a different kernel and boot the
23:18 - system and then fix it so i'll show you
23:20 - what i mean
23:21 - here we are on our ubuntu system and
23:24 - let's say i just recently updated it and
23:26 - during the update it says now i need to
23:29 - restart my system so i'm going to
23:30 - restart the system and everything should
23:32 - go well blah blah blah blah
23:35 - and then boom we get a kernel panic now
23:37 - this doesn't actually give us the words
23:38 - kernel panic because i did something
23:40 - fairly simple to simulate what would
23:42 - happen if there was some corruption in
23:44 - the kernel uh but it says here you know
23:46 - it's unable to find the init rd
23:48 - sometimes you might see kernel panic
23:49 - init
23:50 - or unable to kill init or something like
23:52 - that it says press any key to continue
23:54 - but
23:55 - there's nothing that's going to happen
23:56 - right it's kind of locked up on me
23:58 - because it aired out there's no init rd
24:00 - so what we're going to do is restart the
24:02 - system and when we do that we're going
24:03 - to initiate the grub menu by holding
24:05 - down the shift key so first of all we do
24:08 - need to restart the computer i'm going
24:10 - to hold down the shift key and we should
24:11 - get presented with a grub menu sure
24:13 - enough here's our grub menu
24:16 - advanced options and we're going to see
24:18 - here we have multiple kernels
24:20 - this is the kernel the newest one is the
24:22 - one that it tried to boot by default so
24:24 - i'm just going to go back to the last
24:26 - successful kernel that we had
24:28 - press enter and it should boot this up
24:30 - and our system should be just fine then
24:31 - all we need to do is remove that kernel
24:34 - package with the failed thing using our
24:36 - app system reinstall it again you know
24:39 - do an update so it reinstalls and then
24:41 - it should install that init rd back in
24:43 - place and we should be fine so it's nice
24:46 - that linux keeps a few of the older
24:49 - kernels kind of in line so that we can
24:51 - do exactly this if something goes wrong
24:53 - with one kernel
24:55 - or we've deleted modules or just
24:56 - something doesn't load or we get a
24:58 - kernel panic we can select a previously
25:01 - successful kernel and then here we are
25:03 - on our system working perfectly and now
25:05 - we can go through and remove that
25:07 - package and fix it etcetera etcetera
25:09 - etcetera so if you do get the kernel
25:11 - panic especially on a production system
25:14 - oh it can be really scary but know that
25:16 - there are ways
25:17 - you know to fix it if it's just a kernel
25:19 - corruption you can boot to a previous
25:21 - kernel if it's a hardware issue it's a
25:22 - little harder to troubleshoot but you
25:24 - can also do something like boot from a
25:26 - usb drive or a cd and at the very least
25:29 - you'll be able to access the hard drives
25:31 - so that you can get stuff off of your
25:33 - computer if it is a hardware failure
25:36 - that's causing the problem and then
25:38 - you'll have your data to put onto a new
25:40 - computer but kernel panics are no reason
25:42 - to panic it's just one more thing that
25:43 - we need to understand how is working so
25:45 - we can fix it or troubleshoot the
25:47 - problem and move on from there
25:50 - we know that the linux kernel is modular
25:52 - in that it doesn't load all of the
25:54 - drivers that it could possibly ever need
25:57 - automatically by default it has these
25:59 - modules that it dynamically loads when
26:01 - it discovers that it needs like a driver
26:03 - for a certain sound card on boot now we
26:05 - can configure it to automatically load
26:08 - those modules by setting it up in a
26:10 - config file which we're going to look at
26:12 - now what happens though is it'll load
26:14 - that module but then on behind the
26:16 - scenes there's some incredible
26:17 - dependency checking going on and that
26:20 - dependency checking is really really
26:23 - good at knowing what modules one you
26:26 - know a particular module will depend on
26:29 - there may not actually be a robot called
26:31 - modbot in your system or even in my
26:33 - system i just like to think of a cheesy
26:35 - robot plugging in different modules in
26:37 - my computer but nonetheless the computer
26:40 - is really good at finding
26:42 - dependencies in fact sometimes it's too
26:44 - good and we need to blacklist certain
26:47 - modules so that modbot doesn't say oh
26:49 - this one will work and put it in i'm
26:51 - going to show you why we would do that
26:53 - and also how to do that but we're
26:54 - looking at two concepts today how to
26:56 - automatically load a module on boot and
26:58 - how to make sure that the wrong module
27:00 - isn't loaded on boot automatically by
27:02 - the system detecting a dependency that
27:04 - you don't want it to use
27:06 - now here on our this is an ubuntu system
27:08 - but all linux distributions have kernels
27:11 - we're going to go into the etc folder
27:14 - and there's a file here called modules
27:16 - so we're going to look at that
27:18 - we can see right now it's empty it says
27:20 - kernel modules to load at boot time this
27:22 - contains the names of the kernel modules
27:24 - that should be loaded at boot time one
27:26 - per line lines begin with this are
27:28 - ignored so what we would do is put the
27:30 - name of the module
27:33 - and we would add something like e
27:35 - 1000 which is the name of an intel
27:37 - network card module now normally the
27:40 - system automatically detects the
27:42 - hardware and it knows what to load but
27:44 - we want to i'm going to give you an
27:45 - example of how you can manually load a
27:47 - kernel module and have it done
27:49 - automatically on boot even if the system
27:51 - doesn't you know detect it on boot like
27:53 - oh i have new hardware i need this
27:55 - particular module so this is how you
27:56 - would manually do it right now we're not
27:58 - actually going to save this because i
28:00 - don't want to load that module but let's
28:02 - say that's loaded now if there are
28:03 - dependencies it will also load those
28:06 - dependencies let's say that the intel
28:08 - e1000 network card also requires the pci
28:11 - bus to work well then it would load the
28:12 - pci bus
28:14 - module so it's really smart but let's
28:16 - say that there are two different pci
28:18 - modules there's like uh pci version one
28:21 - and pci version 37 and by default it's
28:24 - going to load pci version one and we
28:26 - don't want that so what we would do is
28:28 - blacklist it so if we go into
28:30 - etc mod probe.d first i want to show you
28:33 - there's a bunch of files in here
28:34 - anything with dot conf is read in right
28:36 - so it doesn't matter which one you put
28:38 - it in there's just some conventions here
28:40 - anything we're going to manually put in
28:41 - we're going to put into blacklist.com so
28:44 - let's actually edit that file
28:47 - and we're going to see there are a bunch
28:49 - of things that are already blacklisted
28:51 - now what this does it just makes it so
28:54 - that the kernel doesn't automatically
28:57 - load it like here's a let's look at this
28:59 - ethernet one okay so this is an e1394
29:02 - this is a firewire network device all
29:05 - right it's saying here we never want to
29:07 - use a firewire network device so if you
29:10 - detect a firewire device i do not want
29:12 - you to load the e1394
29:15 - or the ethernet module for a firewire
29:18 - port so if we put blacklist here it's
29:21 - not going to load it even if we put in
29:23 - the firewire module itself so that it
29:26 - activates that port this will make sure
29:28 - that the kernel or modbot or whatever
29:30 - however you like to visualize it doesn't
29:32 - say oh well you're putting that in why
29:34 - don't we also activate the ethernet
29:36 - ability of that port we don't want that
29:37 - so we put blacklist in here same thing
29:39 - like that i've i've done this the most
29:41 - with sound cards because there are like
29:43 - 150 different sound card drivers for the
29:46 - ac 97 model of sound cards so a lot of
29:50 - times you have to blacklist a bunch of
29:52 - different models and see there's a
29:53 - blacklisted sound card right here right
29:55 - this is saying don't load this module
29:57 - that's not what i want and this looks
29:59 - like it's because of an ubuntu bug all
30:01 - right so anyway this is where we tell it
30:03 - what not to load even if it's compiled
30:05 - in and it's part of the module we would
30:06 - put it in here or any of the files in
30:09 - this folder
30:10 - anything that says blacklist space blank
30:12 - is going to be blacklisted honestly like
30:15 - i mentioned the linux kernel is usually
30:18 - really good at automatically detecting
30:21 - hardware and loading all of the modules
30:23 - all on its own by detecting new hardware
30:26 - on the system but if there are times
30:28 - where you want to manually load
30:30 - something but you want it to be done on
30:32 - boot that's where we would put it in the
30:33 - etc modules file and then blacklisting
30:36 - is really important especially if you're
30:37 - having problems with something like uh
30:39 - it just doesn't work with this kernel
30:40 - module let me blacklist that so the
30:42 - kernel will pick another
30:44 - module and try to activate that one
30:48 - one of the things that makes the linux
30:50 - kernel very special is that it's modular
30:52 - so we don't have to install all of the
30:55 - drivers for all the potential hardware
30:57 - in a system we just installed the
30:58 - drivers and the modules for the things
31:00 - that exist it makes the linux kernel
31:02 - very efficient because we don't have all
31:04 - that bloat all those unneeded things
31:07 - sitting in memory where they're never
31:08 - going to be used so the kernel can be
31:10 - very very efficient because of its
31:12 - modular design but in order to take full
31:15 - advantage of that we have to make sure
31:16 - that we're putting the right modules in
31:19 - the right place with all the
31:20 - dependencies so we don't end up with a
31:22 - kernel looking like this right with
31:23 - pieces that don't belong thankfully
31:25 - there are a handful of tools that we can
31:27 - use to properly manipulate the kernel
31:29 - modules and the first two i want to look
31:31 - at are ins mod and mod probe because on
31:34 - the surface they appear to do the same
31:35 - thing insert modules into the running
31:38 - kernel this is after we're booted it'll
31:40 - insert them into the running kernel so
31:42 - let's compare them to scientists right
31:44 - so ins mod is a very basic program you
31:48 - have to give it the full path of the
31:50 - kernel module that you want to install
31:52 - it doesn't do any dependency checking it
31:54 - just kind of slams it into the running
31:56 - kernel and if the kernel doesn't work
31:59 - like it's the wrong kernel version or it
32:01 - doesn't have the proper dependencies
32:02 - it's just going to fail and it's not
32:04 - going to give you an explanation as to
32:06 - why it's just going to say well i
32:07 - crammed it in there and it didn't work
32:09 - boom
32:10 - but mod probe on the other hand is a
32:12 - more advanced program it's a more
32:14 - efficient model and what it does you can
32:17 - give it just the name of the kernel
32:19 - module you don't have to give it the
32:20 - full path you can give it the just the
32:22 - name of the module itself it will look
32:25 - and determine all of the dependencies
32:27 - that that module needs so if one module
32:29 - needs you know depends on another module
32:31 - it will say oh let's load that other one
32:33 - first it does a really good job now it
32:35 - does need us to have a map of all the
32:37 - all the needs and dependencies on a
32:39 - system but there's a program to do this
32:41 - we just make sure we type dep mod when
32:44 - we're done installing a new kernel
32:45 - module and it will recreate that map so
32:47 - modprobe knows where to you know find
32:50 - the dependencies now you might be
32:51 - thinking okay well obviously i'm going
32:53 - to use modprobe when i insert modules
32:55 - and exactly that's what you should do
32:57 - why does insmod even exist well here's
33:00 - the deal mod probe
33:02 - actually knows what modules to install
33:05 - and then behind the scenes it uses ins
33:08 - mod to do the actual inserting so mod
33:10 - probe is really kind of a front end
33:12 - that's very intelligent about what to do
33:14 - on the system but that's why insmod
33:16 - still exists it's still the you know the
33:18 - basic tool that is used to cram modules
33:21 - into the running kernel modprobe just
33:23 - knows which kernels to load and in what
33:25 - order to load them so modprobe is what
33:27 - the humans use and ins mod is what
33:30 - modprobe uses if that makes sense
33:32 - it's really easy to see that in action
33:34 - on our system so i'm root on ubuntu here
33:37 - and let's go into lib
33:39 - modules and we're going to see we have
33:41 - folders for each of the kernels
33:43 - installed on the system we're going to
33:44 - go into the one that we're currently
33:45 - running which is the latest one
33:47 - and inside here we'll see this is where
33:49 - we have a lot of like the map files for
33:51 - the mod probe so it knows where the
33:53 - kernel modules live let's go into the
33:55 - kernel folder where the modules live
33:58 - type ls let's go into the drivers folder
34:01 - type ls again oh there's still a lot of
34:03 - drivers let's go into the net folder all
34:05 - right there we actually see some kernel
34:07 - modules here okay so uh we're gonna play
34:09 - with thunderbolt
34:10 - net but let's say we wanted to load this
34:12 - in so we could use thunderbolt as a
34:14 - network device if we wanted to use ins
34:16 - mod we would say
34:18 - ins mod lib
34:21 - modules
34:23 - generic kernel
34:26 - drivers net
34:28 - thunderbolt net because we need the
34:29 - entire path we're going to press enter
34:31 - and it's going to say well i can't do
34:33 - that unknown symbol in module it doesn't
34:35 - work blah blah blah it has a dependency
34:37 - it doesn't tell us what the dependency
34:38 - is but if we were to use mod probe and
34:40 - we were to say
34:41 - mod probe i want you to put in
34:44 - thunderbolt
34:46 - net and that's all we don't have to say
34:48 - a path we don't have to put ko at the
34:49 - end just press enter
34:51 - boom it's automatically installed how do
34:53 - we know well one we didn't get an error
34:55 - it just installed it but if we type ls
34:56 - mod
34:57 - will show us all the installed modules
34:59 - and if we scroll a bit to the top we're
35:01 - going to see well look at that
35:03 - thunderbolt net is installed into the
35:05 - running kernel and we can see the
35:07 - dependency was thunderbolt but modprobe
35:10 - knew to do that and it installed it now
35:12 - there's another tool for removing them
35:13 - and that's rm mod now there's no super
35:15 - smart
35:16 - version and dumb version of these rm mod
35:19 - is kind of halfway between because if we
35:21 - were to say rm mod thunderbolt it's not
35:24 - going to do it it's going to say i can't
35:25 - do that it's in use but at least it
35:27 - tells us what it's in use by right
35:30 - thunderbolt underscore net which that's
35:32 - a little strange because the name of it
35:34 - doesn't have an underscore right if we
35:35 - want to get rid of this we could say
35:37 - first we have to rm mod
35:40 - thunderbolt dash net
35:43 - okay no errors and now if we do the up
35:45 - arrow twice now we can rm mod the
35:47 - thunderbolt module
35:49 - and again no problems because we we got
35:51 - rid of that dependency first so now if
35:54 - we do ls mod we're going to see if we
35:55 - scroll up that they're not in there
35:57 - anymore they're not installed into the
35:59 - running kernel and that's how you can
36:01 - manipulate kernel modules on your system
36:04 - some of the tools are smarter than
36:05 - others but it's really important to
36:07 - understand why they're smarter and like
36:09 - insmod is important still even though
36:11 - you don't want to use it on the command
36:12 - line for doing very much at all now one
36:15 - more thing i just want to tell you about
36:16 - is if you install a new kernel module
36:18 - like you download a piece of hardware
36:21 - and it has its own kernel module and you
36:23 - compile it and so then you have the
36:25 - kernel module that's put in this folder
36:26 - you know so that it's in our current
36:28 - running modules folder you have to type
36:30 - depmod in order for it to update that
36:34 - database or that system map so that
36:36 - modprobe knows where
36:39 - it lives and what dependencies it might
36:40 - have
36:41 - it takes a while and now we have it
36:43 - updated so now we can install that new
36:45 - kernel module because it knows all the
36:47 - dependencies on the system now i know in
36:49 - the picture here i have these you know
36:51 - wrong puzzle pieces put into the puzzle
36:53 - this is actually a lot harder to
36:54 - accomplish than you would think because
36:56 - the tools just fail right they say oh
36:58 - this won't fit or this is the wrong
36:59 - kernel module some of them are smarter
37:01 - about it than others but it's really
37:03 - difficult to put the wrong kernel module
37:05 - into a running kernel but i want to make
37:06 - sure that you understand all of these
37:08 - tools exist how they work what they look
37:10 - for and the difference between them
37:13 - first off we're talking about network
37:15 - connectivity today and the first thing i
37:17 - want to show you is
37:18 - always check the cable now it seems
37:21 - silly right of course the cable is fine
37:22 - this is plugged in just fine however
37:26 - ah see it wasn't plugged in tight and
37:28 - whether it's a jokester in the office or
37:31 - somebody who ran over a cable with a
37:33 - vacuum cleaner or something sometimes it
37:36 - is the cable and you can spend a ton of
37:37 - time on the command line and be very
37:39 - frustrated to discover that it was just
37:41 - a bad cable so that's just a pro tip
37:43 - check the cable make sure it's plugged
37:45 - in tightly and that the little lights on
37:47 - the back are flashing and doing their
37:48 - thing
37:49 - now we could probably do an entire
37:51 - course on troubleshooting a network
37:53 - connection that's not working or not
37:55 - working right but today i want to go
37:56 - over just a couple quick tools so you
37:58 - can determine very quickly what your
38:00 - issue might be on the network so i want
38:02 - to look at ping and then just to check
38:04 - out our address on our computer and so
38:06 - the first thing i want to do is give you
38:07 - this scenario we're on the computer and
38:09 - we're trying to go somewhere like google
38:11 - and it says unable to connect oh we
38:13 - can't get online so the first thing i
38:15 - would do is open up a command line
38:17 - window and i would say let's try to ping
38:19 - google so ping
38:20 - google.com network is unreachable okay
38:23 - well i know that the google dns server
38:26 - has an ip address of 8.8.8.8 so that's
38:29 - the next thing i would do to eliminate
38:32 - whether it's dns right maybe it's dns
38:34 - that's not working so i want to see if
38:36 - my network itself is working so i'm
38:37 - going to say ping
38:39 - 8.8.8.8
38:40 - ah network is still unreachable okay so
38:43 - there's an issue with more than dns that
38:45 - means that there's a problem with my
38:47 - network so let's look and see what our
38:49 - network address looks like now there's a
38:51 - couple tools you can try to type
38:53 - ifconfig if this doesn't work yes see
38:55 - it's not on my system it's a newer
38:57 - system so type
38:58 - ipad for address
39:01 - and it will give you the ip addresses on
39:03 - your local computer for your network
39:06 - devices so this is the localhost
39:09 - 127.0.0.1 this is just like a virtual
39:11 - host that says it's it's local so we
39:13 - don't want to use this one but this is
39:14 - our ethernet port right eth0 and it
39:17 - looks like we have an address of
39:18 - 10.10.10.10.
39:20 - so i would say let's ping that so let's
39:22 - make sure that you know our network
39:24 - stack itself is working okay so this is
39:26 - working i can ping my own ip address so
39:29 - the ip stack itself is working i'm going
39:31 - to hit control c
39:33 - but for some reason i'm unable to ping
39:35 - out of the network now let's assume that
39:37 - my
39:38 - gateway address is 10.10.10.1
39:41 - ping 10.10.10.1
39:45 - okay i can ping other computers on my
39:48 - network so it's not well first of all we
39:50 - know it's not my computer cable right
39:52 - because i can ping a different computer
39:54 - that's across the network but i'm still
39:57 - unable to ping google so for some reason
40:00 - my internet connection is down
40:03 - well let's look at our ip routing
40:05 - information i'm going to type ipspace
40:09 - route
40:10 - and press enter
40:12 - alright it looks like
40:14 - 10.10.10.0-24 so this network is
40:17 - directly accessible via eth0 okay so
40:20 - that's working that's why we could ping
40:21 - this up here but if you'll notice now
40:24 - you might not notice but there's a line
40:25 - here that's missing we don't have a
40:28 - default route set so that means that the
40:30 - computer while it can access other
40:33 - network or other computers on our local
40:34 - network it doesn't know where to send
40:36 - network or where to send packets that
40:38 - are destined for another network there's
40:40 - no
40:41 - ip routing information here now if this
40:44 - is an ip routing nugget i could say okay
40:46 - let's add a route manually but this is
40:48 - more of a troubleshooting nugget so
40:49 - here's the deal probably when we got our
40:52 - dhcp address we didn't get all the
40:54 - information or something fell apart the
40:56 - first thing you do is either turn it off
40:58 - and turn it back on right that's that's
41:00 - the standard it guy response right well
41:02 - in this case it might work or we could
41:04 - actually go and just go to our wired
41:07 - network interface here turn just the
41:10 - network interface off
41:13 - connect again turn it back on and now
41:16 - let's well let's just look and see what
41:17 - our ip route information looks like
41:19 - hahaha look when we brought our network
41:22 - interface down and back up now we have a
41:24 - default route added so now let's see if
41:26 - we can ping google.com
41:29 - boom we're able to ping it we go over
41:31 - here and we should be able to access
41:34 - google as well and sure enough there's
41:36 - google so we've successfully
41:38 - troubleshooted the problem with our
41:39 - network connection in our case it was a
41:41 - route thing and really just turning off
41:43 - the network interface and turning it
41:44 - back on was the way to solve it but the
41:47 - important part is going through the
41:48 - process of determining where the problem
41:50 - is we knew it wasn't a network line or
41:52 - that network cable because we were able
41:54 - to ping other computers on our network
41:57 - so it's just a matter of troubleshooting
41:59 - and going down the list of where can i
42:01 - connect where can i connect and what
42:03 - might cause those issues figuring out
42:05 - connectivity problems is sometimes more
42:07 - of an art form than it is a science you
42:09 - just have to kind of put yourself in the
42:11 - mindset of a packet and say okay what
42:13 - information do i need to go from point a
42:16 - to point b and what is stopping me from
42:19 - getting in between there tools like ping
42:21 - were are able to make sure that you can
42:23 - or can't connect to remote computers and
42:25 - then using the ipadd you can see what
42:28 - your address is we also used iprout
42:32 - to look at the ip route information and
42:34 - that's where we discovered we didn't
42:36 - have a default route but tools like this
42:38 - are what you can use to troubleshoot
42:39 - your connectivity to figure out what's
42:41 - going on but don't forget always check
42:43 - that cable because sometimes it's just a
42:45 - physical cable unplugged
42:48 - dns or domain name system is the way
42:52 - that your computer converts a url or a
42:54 - domain name like google.com into the
42:58 - numbers the ip address that the actual
43:01 - computer knows how to connect to using
43:03 - ip routing so google.com
43:06 - isn't really helpful for anything except
43:08 - for dns really what the computer needs
43:10 - is that ip address so that it can get
43:11 - there now there's a couple tools that we
43:13 - can use to test dns on our system one of
43:15 - the most common is ping right you just
43:17 - ping and see if it can reach it if it
43:18 - can reach it and get a response hey it's
43:20 - working great but there are three other
43:22 - common tools that i want to cover dig
43:25 - nslookup in host and let's actually look
43:27 - at those individually because they all
43:29 - do about the same thing but they do it
43:31 - in a slightly different way now dig is
43:34 - the program that i use most often only
43:36 - because it has a cool name right there's
43:38 - really no reason that i use it other
43:39 - than the word dig is just kind of cool
43:41 - so
43:42 - dig
43:43 - you type dig and then you can just type
43:45 - the host you want to look up like dig
43:47 - google.com and it will use your default
43:50 - dns server and look it up if you want to
43:52 - specify a dns server for it to use
43:55 - instead of the one that your system is
43:57 - currently using which is a way that you
43:59 - can test your particular dns server you
44:01 - you put the at symbol and then the ip
44:04 - address of the server that you want to
44:06 - query now the other two programs can do
44:08 - the same thing they just do it in a
44:09 - different order you can say nslookup and
44:11 - the host you want to look up and then if
44:14 - you want to specify a server again you
44:15 - don't have to but if you want to then
44:17 - you put the server after that same thing
44:19 - with the host command you put type host
44:22 - the host name you want to look up and
44:23 - then if you want to specify a server you
44:25 - put it there if you leave it off it'll
44:27 - use your default and they all like i
44:29 - said do kind of the same thing so let's
44:31 - look at them really quickly and then
44:33 - i'll talk about how to query different
44:35 - servers and do just a little bit of
44:36 - troubleshooting on your system because
44:38 - testing dns is kind of vital if you want
44:41 - things to work on your system and they
44:43 - don't seem to be so first of all let's
44:45 - look at them all
44:46 - how they function just by default so i'm
44:47 - going to say dig
44:49 - google.com
44:50 - all right and this is the response i get
44:52 - it's queried my default server and we
44:55 - can look my default server is listed
44:57 - right here
44:59 - 8.8.8.8 so that's pretty awesome and
45:02 - this is the response it gets there's a
45:03 - bunch of a records for google.com now
45:06 - let me clear the screen we can do the
45:08 - same thing with ns lookup
45:10 - google.com and this is going to give us
45:12 - a little bit different format right it
45:14 - says our server right here this is the
45:16 - oop i missed i highlighted the wrong
45:18 - line that's our server the same server
45:20 - of course it's our default server and it
45:22 - gives us all the addresses these are all
45:24 - the a records a little less detail but
45:26 - it gives us the same information now if
45:28 - somebody tells you that nslookup is
45:30 - deprecated and it's not used anymore and
45:32 - it's going to be abandoned if there's a
45:34 - weird case with nslookup that that's
45:36 - true
45:37 - until it wasn't they were going to get
45:39 - rid of nslookup and replace it with dig
45:42 - but then they came out with bind 9.3 and
45:45 - for some reason decided that ns lookup
45:47 - was going to stick around so we have
45:49 - nslookup and dig and lastly let's clear
45:51 - the screen again we can just say host
45:54 - google.com and this will give us
45:56 - information as well it actually doesn't
45:58 - tell us what default server it's using
46:00 - but it gives us a lot of information
46:01 - these are all of the a records for it
46:03 - these are all you know the ip addresses
46:05 - this one actually also tells us the mail
46:07 - handlers the mx records for google.com's
46:10 - domain which is kind of interesting as
46:11 - well so just pick the one that you like
46:13 - the best and that's what you can use and
46:16 - the reason that it's important to
46:18 - pick one that you like and use it often
46:20 - is so that you're comfortable with it
46:21 - right like i said i usually use dig and
46:23 - that's weird i know because it's the
46:25 - least like common way to
46:27 - handle specifying a server right the
46:29 - other two you do you know like nslookup
46:31 - what you're looking up and then a server
46:33 - with dig you say dig
46:35 - at and let's say
46:38 - 127.0.0.1 that's localhost right so dig
46:41 - at localhost
46:43 - for google.com
46:45 - and we got a response okay google.com
46:49 - and it gave us this address which is
46:51 - different but it came from our local
46:54 - server now it did not get all the same
46:56 - responses that we got from the
46:58 - 8.8.8.8 domain server you know our
47:00 - default domain server if we were to say
47:03 - dig
47:03 - at 8.8.8.8
47:07 - google.com we would get a totally
47:09 - different
47:10 - set of ip addresses so that's why it's
47:12 - really important to be able to query
47:14 - different servers so that you can get
47:16 - different responses because on this
47:17 - computer if we try to ping
47:19 - google.com
47:21 - it's going to try to ping that other
47:22 - address and time to live exceeded it's
47:24 - not ever going to work so there's
47:27 - something wrong with the dns server
47:28 - running on localhost now i'll show you
47:31 - what the problem is
47:34 - the hosts file i've actually sabotaged
47:36 - here this is the etc hosts file this is
47:38 - where your computer will look first for
47:41 - any dns lookups and sure enough i've
47:43 - kind of bamboozled it here i put the the
47:46 - wrong ip address for google.com so i put
47:48 - that in there
47:52 - now if i restart our local server
47:55 - now if i try to ping google.com
47:58 - it's going to be just fine because it's
48:00 - going to actually use the real address
48:01 - that it looked up on the internet but
48:03 - that's how you can use the different
48:05 - tools to specify not only what host you
48:07 - want to look up but what server you want
48:09 - to query when you look it up so
48:12 - whichever one of these makes the most
48:13 - sense to you or whichever formatted
48:16 - results you like best just pick one and
48:18 - use that because you can specify a
48:19 - server for each of the different
48:21 - commands but knowing how to use them is
48:23 - important because troubleshooting
48:25 - usually means querying more than one dns
48:27 - server so you can figure out what the
48:29 - heck is going on
48:31 - one of my favorite things about linux is
48:33 - that everything is configured with text
48:35 - files right it's just plain text files
48:38 - it's awesome and the network is no
48:40 - different it's configured with text
48:42 - files now there are a few differences
48:44 - when it comes to distributions if you're
48:45 - on debian or if you're on centos there
48:48 - are going to be some configuration files
48:49 - that are different because they
48:51 - configure their networks different now
48:53 - there's nothing wrong with being unique
48:55 - i really like the differences in the
48:57 - distributions but there are some files
49:00 - that are consistent regardless of what
49:02 - distribution you're using we're going to
49:04 - call those the common files and that's
49:06 - what we're going to look like look at in
49:07 - this nugget
49:09 - these files here are consistent across
49:11 - the board they're just kind of standard
49:13 - linux files so i want to show you what
49:15 - they are and and where they live and how
49:17 - they are configured and what they do and
49:19 - that's again just the common files
49:21 - there's going to be specifics in the
49:22 - different distros but we're just looking
49:24 - at the files that are common to linux in
49:26 - general in this case we're going to look
49:28 - at the files in ubuntu and the first one
49:30 - i want to look at is the etc hosts file
49:33 - so let's actually become root if we want
49:35 - to edit them we have to be root to do so
49:37 - so i'm going to look at etc hosts
49:39 - there's no extension at the end it's
49:41 - just etc hosts
49:43 - and this is a file that acts kind of
49:45 - like the first resort dns lookup so
49:49 - before a system even looks things up via
49:52 - dns it looks in here and there's a
49:54 - couple things here like localhost and
49:56 - ubuntu this is our hostname are set up
49:58 - in here but we can add something we can
50:00 - say what if we wanted to make sure users
50:02 - were never able to go to google we could
50:04 - say okay
50:06 - 127.0.0.1 which is our localhost is now
50:09 - going to be
50:10 - google.com so if somebody tries to go to
50:12 - google.com they're going to try to hit
50:14 - our local machine and we don't have a
50:16 - web server at all so it's just going to
50:17 - error out so if we save that
50:20 - now on our computer if somebody tries to
50:22 - go to google it's going to fail
50:24 - so google.com
50:26 - unable to connect even though our
50:28 - network is working just fine we could go
50:30 - to yahoo if we wanted
50:33 - it's just a matter of that dns entry in
50:35 - the et cetera hosts file that we kind of
50:37 - broke all right so another place we can
50:39 - look is in the etc ns
50:42 - switch.conf
50:44 - file now this configures a bunch of
50:45 - things on our system like group and
50:48 - password files but what i want to show
50:49 - you specifically here
50:51 - is the hosts line so this is saying
50:53 - where does it look for dns lookups to
50:56 - you know find what the host's ip
50:58 - addresses are the first one is called
51:00 - files this points to etc hosts if we
51:03 - have this listed first that means before
51:06 - it queries a dns server it's going to
51:09 - query that file it's going to look in
51:10 - that file first and then in order these
51:13 - are what it's going to do from there
51:14 - it's going to do you know the mdns for
51:17 - local lookups and then it's going to use
51:19 - a dns server so this is the order that
51:21 - it's going in but the first one it looks
51:23 - in is the files specifically that etc
51:26 - hosts file
51:27 - so you can change the order of that i
51:28 - don't recommend changing the order
51:30 - because that's kind of what we wanted to
51:32 - do but one more thing i want to show you
51:34 - is etc
51:37 - resolve.conf and this is a little bit of
51:39 - a confusing file because while this
51:42 - tells the computer what name server to
51:45 - use you'll notice up here it says do not
51:48 - edit this file now back in the day you
51:50 - would just put name server and then the
51:52 - name server you wanted to use on your
51:54 - system but now this is all handled with
51:56 - the
51:57 - systemd resolve d service and and it
52:00 - creates this file on the fly so really
52:03 - we do not edit this at all network
52:05 - manager handles all of the name server
52:08 - entries in here um or whatever your
52:10 - distribution uses for configuring the
52:12 - network so we don't actually edit this
52:14 - but if you want to see what server your
52:18 - your particular computer is using you
52:20 - can look in here and it looks like it's
52:22 - it's querying
52:24 - 127.0.0.53 which is another ip address
52:26 - to our local computer so that means that
52:29 - it has some sort of local caching dns
52:31 - server and it's not querying directly to
52:34 - the internet
52:35 - now you probably noticed that all of
52:37 - these common files have to do with dns
52:40 - and that's fine but it's it's
52:42 - interesting that dns is kind of commonly
52:45 - configured
52:46 - across the distributions when it comes
52:48 - to actually configuring the individual
52:50 - network
52:51 - devices that's drastically different
52:54 - from one distribution to the next but
52:56 - the common files are generally dns files
52:58 - and that's what we looked at in this
53:00 - nugget
53:01 - ubuntu is one of the most common
53:03 - distributions out there but it's based
53:05 - on and built upon debian so when we look
53:08 - at configuring one it's very similar to
53:10 - configuring the other and that's the
53:12 - case with network files now i want to
53:14 - show you where to find the different
53:15 - network files on an ubuntu or debian
53:18 - system but it's important to note that
53:20 - in the last
53:22 - iteration of their long-term support for
53:24 - ubuntu that's when a change has been
53:26 - made to how the network is configured
53:29 - and what files are used and how those
53:31 - files are configured now there is one
53:33 - commonality between them and that's if
53:35 - you use network manager to manage your
53:37 - network instead of editing the
53:38 - configuration files that hasn't changed
53:40 - you can still use that across all the
53:42 - versions that are current with ubuntu
53:44 - and debian but these two ways have
53:47 - changed so first i want to look at the
53:48 - older version of ubuntu and when i say
53:51 - older i mean one that's still valid to
53:53 - use as of the date today you can still
53:55 - use version 1604 let's see what version
53:58 - this is etc os release so this is ubuntu
54:02 - 1604.1
54:04 - lts long term support and this is still
54:07 - as of today a valid version of ubuntu to
54:10 - use in production but this is configured
54:12 - by going into the etc
54:15 - network directory and there's a single
54:17 - file in here called interfaces so if we
54:20 - look at
54:21 - interfaces we're going to see this is
54:23 - how you configure a network interface
54:25 - with an older version of ubuntu or
54:27 - debian now this is pretty much the
54:30 - stanza that you configure there's
54:31 - usually something in here you can look
54:32 - at and base it off of but this is the
54:34 - format for it we're not going to go
54:36 - through the you know configuring
54:37 - different things it's pretty
54:38 - straightforward you don't have to indent
54:40 - this is just for
54:41 - ease of readability but this is how you
54:44 - configure a network interface via the
54:47 - configuration files now we could still
54:49 - use the um network manager if we were on
54:52 - a gui system like this is a gui system
54:54 - we could use that but i'm actually ssh
54:56 - into a server that doesn't have a gui
54:58 - interface and that's where we use these
55:01 - network files to configure it so let's
55:03 - quit this one this is how you configure
55:05 - an older version but a modern version
55:06 - like
55:07 - 1804 or the most modern version of
55:10 - debian uses a completely different
55:12 - network configuration system now here
55:15 - we're on a newer version of ubuntu this
55:17 - is actually 1804. let's check that out
55:20 - so this is
55:21 - 18.04.1 another long-term support
55:24 - release of ubuntu and this one uses the
55:26 - net plan system for configuring network
55:29 - interface so if you go into etc net plan
55:33 - and you look in here depending on what
55:34 - kind of system you have you're going to
55:36 - have some sort of file in here it's
55:37 - either going to be like zero one
55:38 - networkmanager.yaml
55:40 - or 50 cloudanit.yaml if it's a server
55:43 - the thing about yaml files though let's
55:45 - look at this
55:47 - yaml files do depend on indentation in
55:50 - order to be properly configured so while
55:53 - with the etc network interfaces of the
55:55 - old ubuntu system it didn't matter how
55:57 - things were indented here it really
55:59 - really does and so here this is where we
56:01 - would configure like a static ip address
56:03 - and we you know here's our ip addresses
56:05 - gateway name servers the format's a
56:07 - little different but it's fairly similar
56:09 - conceptually to the old system like i
56:11 - said except for that white space that's
56:13 - really important that it's indented
56:16 - properly so if you make a change to this
56:18 - then you just would do sudo netplan
56:22 - apply and then it will activate the new
56:25 - changes that you made now i do want to
56:26 - point out again this is a gui system so
56:28 - it has network manager but there's also
56:31 - nmtui
56:33 - which is network manager text user
56:35 - interface and then you can edit
56:36 - interfaces using just the text boxes on
56:40 - your screen if you don't have a gui
56:41 - system installed so that's just one more
56:43 - way that you can edit your file in
56:45 - ubuntu or debian if you don't have a gui
56:47 - but you want to use network manager as
56:49 - opposed to just configuring those files
56:51 - on your own whether you have an older
56:53 - system using etc network interfaces or a
56:56 - newer system using configuration files
56:58 - and etc netplan configuring the network
57:00 - on debian and ubuntu is not difficult
57:02 - and you know it's usually just a matter
57:04 - of changing what's existing but if you
57:06 - want to use the network manager to
57:08 - manage all those interfaces either using
57:10 - gui or text tools you can do that as
57:12 - well it's very flexible and even with
57:14 - the change in versions it's not a whole
57:16 - lot more difficult to do one over the
57:19 - other if we've been learning together
57:20 - for a while you know that i prefer
57:22 - debian based distributions and ubuntu is
57:25 - my jam
57:26 - but i have to admit when it comes to
57:28 - network configuration files centos or
57:31 - red hat they really have an elegant
57:33 - solution for how to configure the
57:35 - network and there's no confusion between
57:37 - if you're using the gui network manager
57:40 - or if you're just using the
57:41 - configuration files in the etc config
57:44 - folder let me show you what i mean
57:45 - because centos is just so awesome when
57:48 - it comes to network configuration now
57:50 - here we are on a centos system and it
57:52 - doesn't matter which way you configure
57:54 - the network interface you can go up here
57:56 - and go into the network manager to
57:59 - configure it or we can go directly into
58:01 - etc sysconfig and i just wanted to show
58:04 - you quickly inside the sysconfig folder
58:06 - and sent to us is where all of these
58:08 - configuration things are this is kind of
58:10 - unique to centos and red hat they put
58:12 - most of their configuration inside this
58:14 - etc sysconfig folder but inside here
58:17 - there's also another folder called
58:18 - network scripts and that's where our
58:20 - networking is configured if we look in
58:22 - here we're going to see there is this
58:23 - configuration file for our interface
58:26 - ifconfig dash eth0 so let's look at that
58:30 - if we edit this it's going to change the
58:33 - way our network interface comes up like
58:35 - if we were to change boot proto from
58:37 - dhcp to
58:39 - static and then we were to add a line
58:42 - ipaddr equals let's say 10.10.10.111
58:47 - which is not currently the ip address if
58:49 - we were to do that and save it
58:52 - and then just do a quick sudo service
58:55 - network restart
58:57 - we would be able to restart our service
58:59 - and if we were to do ifconfig
59:02 - we're going to see scroll up here sure
59:04 - enough that is our
59:06 - ip address now now there are a couple
59:07 - problems we didn't add like netmask or
59:09 - anything so let's go up into
59:11 - network manager
59:13 - wired settings
59:15 - click on the configuration thing so
59:16 - we're to edit this in network manager if
59:18 - we go over to ipv4 we can see it went
59:21 - from it used to be dhcp now it's manual
59:24 - this is the ip address i put in and it
59:26 - just guessed this netmask because the 10
59:29 - range is a class a ip address so it
59:31 - guessed that i wanted to use a class a
59:33 - netmask but i don't i want to use
59:35 - 255.255.255.0
59:38 - and i didn't supply a gateway so let's
59:40 - do that now 10.10.10.1
59:43 - and i didn't supply any dns server so we
59:45 - can do that here
59:47 - 8.8.8.8
59:48 - and now we'll save this we'll click
59:50 - apply
59:51 - if we go back and edit that script sudo
59:53 - vi
59:55 - we're going to see that some changes
59:56 - have been made now this stuff is still
59:58 - the same boot proto equals static ip
60:00 - address but look down here it added the
60:02 - gateway address that we added it added
60:03 - the dns server that we added and it
60:06 - added the prefix 255.255.255.0
60:09 - translates to
60:11 - 24-bit netmask so put that in there just
60:13 - use a different form and we can see that
60:15 - it made the changes to the actual
60:17 - configuration files rather than having
60:19 - like two different systems and you have
60:21 - to decide which one you want to use
60:23 - centos allows us to make changes in one
60:25 - place and whether we're using the gui
60:27 - interface or just this text-based
60:29 - interface it's going to allow us to make
60:31 - those changes i love that it has all the
60:34 - change places in a single file as
60:36 - opposed to
60:37 - conflicting and trying to decide which
60:39 - method you're going to use if you use
60:41 - network manager you you're using this
60:43 - configuration file if you don't use
60:44 - network manager you're using this
60:46 - configuration file so it's a really
60:48 - elegant way to handle network interface
60:51 - configuration like i said at the
60:52 - beginning i'm a debian ubuntu man that's
60:54 - what i usually use in my server
60:56 - situations but when it comes to
60:58 - configuring network interfaces man
61:00 - centos has really stolen the show they
61:02 - have an elegant way of doing it either
61:04 - using network manager gui tools or using
61:07 - the scripts inside the network scripts
61:08 - folder of etc sysconfig
61:12 - network bonding in linux is really just
61:15 - a way to utilize computers that have
61:17 - more than one network port and most
61:18 - servers nowadays come with that let's
61:20 - say you have a server and it has
61:23 - three network ports built into it well
61:26 - if you have to connect it to a switch
61:30 - all each of these connected to a switch
61:33 - you would ideally like to use all of
61:35 - that bandwidth and you don't want to
61:36 - have to supply three different ip
61:38 - addresses to the computer you just want
61:40 - to use all of the available bandwidth on
61:43 - all three of those wires and that's
61:45 - where network bonding comes into place
61:47 - now there's basically two different
61:49 - kinds of network bonds that we're going
61:51 - to look at those that require special
61:53 - switch support and those that are
61:55 - generic and don't require the switch to
61:57 - know at all what's going on now linux
62:00 - does provide some pretty cool options
62:02 - when it comes to there but the options
62:03 - can be a little overwhelming i mean this
62:05 - screen is like oh my goodness there's so
62:06 - many choices well that's okay because
62:08 - your choices are going to be fairly
62:10 - limited once you learn what all of these
62:12 - are now the first thing i want you to
62:13 - look at when we're looking at this list
62:15 - is does it require switch support and
62:17 - what i mean by that is a lot of switches
62:20 - especially layer 3 switches or smart
62:22 - switches they're sometimes called will
62:24 - support link aggregation and different
62:26 - vendors call it different things but
62:28 - it's going to be either link aggregation
62:30 - or lacp or ether channel the idea is a
62:33 - smart switch will have built-in code
62:35 - that will allow them to work together so
62:38 - if you have a smart switch chances are
62:40 - that you have the ability to use link
62:42 - aggregation now some of these require a
62:45 - switch that supports that i'm going to
62:47 - start with the confusing one here so
62:49 - balance round robin basically how
62:51 - balanced round robin works is you have
62:53 - multiple ports and it says okay when i
62:56 - transmit packet 1 i'm going to use this
62:58 - port packet 2 is this port packet 3 is
63:00 - this port packet four is this port
63:02 - packet five is this port packet six is
63:04 - this and it just keeps going through
63:07 - packet by packet and transmitting across
63:10 - all of your interfaces now the reason i
63:12 - said it sorta requires switch support is
63:15 - if you're plugging these into a switch
63:18 - it does require a switch that supports
63:20 - link aggregation but what a lot of
63:22 - people do is they'll use this balanced
63:24 - round robin and they will connect two
63:27 - servers together with crossover cables
63:30 - and actually with gigabit we don't need
63:31 - crossover cables anymore but they'll
63:32 - connect two servers together and they'll
63:35 - use balanced route robin and in that
63:37 - case you don't need a switch support or
63:39 - a switch that supports it because there
63:40 - is no switch involved right they're just
63:42 - directly connecting the two computers
63:44 - together and so like if you have a file
63:46 - server that you want to connect to your
63:47 - other server a lot of times this is a
63:49 - way that you can increase throughput
63:51 - without requiring any special switch
63:53 - support so if you're connecting directly
63:55 - computers you don't need to have switch
63:57 - support if you're connecting to a switch
63:59 - you do all right that's mode 0. mode 1
64:02 - is an active backup and this one's
64:03 - pretty easy to understand basically
64:05 - however many ports you have on your
64:06 - system only one is going to be active
64:09 - and if that one fails then another one
64:11 - is going to turn active and that's just
64:13 - how it works they always have one this
64:15 - is fault tolerance but this doesn't
64:16 - speed anything up it just means if one
64:18 - fails another one's going to come online
64:20 - all right this doesn't require switch
64:22 - support because
64:23 - you know it basically just only uses one
64:26 - port unless the next one fails and then
64:27 - you switch to that port and the switch
64:29 - is like okay so now we're going to use
64:30 - this port great uh balance xor
64:33 - does require special switch support and
64:36 - this is kind of cool how balance xor
64:38 - works basically you have your computer
64:41 - here your linux computer and it does a
64:43 - hash based on your mac address and the
64:47 - client's mac address and so here's
64:49 - another client over here and basically
64:51 - it says okay based on this hash i'm
64:53 - always going to use port 1 to connect to
64:56 - this client and then over here the hash
64:58 - is different so i'm always going to
64:59 - connect to this computer so it's
65:01 - basically a way to just spread out
65:04 - which computers use which port but this
65:06 - computer is always going to use this
65:08 - port and this computer is always going
65:10 - to use this port now this isn't used
65:13 - very often because if you have switch
65:15 - support which is required chances are
65:17 - you're going to use
65:18 - 802.3ad which is the industry standard
65:21 - link aggregation protocol right this
65:24 - means that your switch knows what to do
65:25 - and your client knows what to do and
65:27 - it's just a really smart way of
65:29 - increasing through throughput and
65:31 - availability fault tolerance so if your
65:33 - switch supports link aggregation you
65:36 - should use 802.3ad or mode4 which is the
65:39 - industry standard okay so even though
65:42 - balance xor is cool it's not very often
65:44 - used because if you can use it you might
65:45 - as well be using the one that's even
65:47 - better broadcast i kind of skipped over
65:49 - that broadcast is only in very specific
65:51 - cases that you would use it it just
65:53 - takes all of your ports on the server
65:55 - and spews all of the data out all of the
65:57 - ports at once it's not used very often
66:00 - and it definitely requires your switch
66:01 - to know what on earth is happening now
66:03 - the two i do want to focus on a little
66:05 - bit are these bottom two let's say you
66:07 - have a dumb switch it's not a switch
66:09 - that supports link aggregation it's just
66:11 - one you bought from amazon for 40 bucks
66:13 - it's gigabit but it doesn't have any
66:15 - smarts built in
66:16 - linux is smart enough to be able to
66:18 - utilize the dumb switch and increase
66:21 - bandwidth and throughput and reliability
66:23 - and there's two different ways it does
66:24 - it balance tlb which is balance transmit
66:27 - load balance and the balance of all load
66:31 - balancing and what this does is let's
66:32 - say we have four ports on our switch
66:35 - okay with tlb it's going to transmit
66:39 - from whichever port is currently the
66:41 - least busy so all the transmit is going
66:44 - to be balanced out now incoming is still
66:46 - going to always go to one active portent
66:48 - so that's not as good as alb which is
66:51 - the same concept except it does it for
66:54 - incoming and outgoing so basically the
66:56 - least busy port gets the traffic and
66:58 - this is really brilliant and how it does
67:00 - is it constantly changes the mac address
67:04 - on these ports or on these ethernet
67:06 - cards so the switch is like oh you moved
67:08 - again oh you moved again oh you moved
67:10 - again and from the switches standpoint
67:12 - it doesn't care how many times you
67:13 - switch the mac address so it generally
67:15 - works just fine now some people have
67:17 - issued with this i've used this in
67:19 - production for years and never had a
67:21 - problem so balance alb if you have a
67:24 - dumb switch i highly recommend you use
67:26 - mode six okay if your switch supports
67:29 - link aggregation i highly recommend mode
67:31 - four because that's the industry
67:33 - standard and if you're just connecting
67:34 - two servers together with multiple
67:36 - cables my shirt mode zero works really
67:38 - great to be honest the most difficult
67:40 - part of linux network bonding is
67:42 - figuring out which mode to work but
67:45 - really it's not that tough of a decision
67:46 - if your switch supports it use 802.3ad
67:49 - there's really no reason not to if your
67:51 - switch is a dumb switch and doesn't
67:53 - support it i highly recommend mode 6 or
67:55 - balance alb if you want to take
67:58 - advantage of bonded network interfaces
68:00 - but you don't have the expensive
68:02 - hardware that will support it
68:04 - once you know the type of bond that you
68:06 - want to set up for your two ethernet or
68:08 - two or more ethernet connections on the
68:10 - computer configuring them is pretty
68:12 - straightforward although it's
68:13 - drastically different depending on
68:15 - whether you're on ubuntu or centos so
68:17 - i'm gonna show you how to do it on both
68:19 - and then i'll show you also how to test
68:21 - it to make sure that the bonding modules
68:23 - are working so here i am on ubuntu i've
68:25 - actually already configured it so i've
68:27 - gone into etc
68:29 - netplan and i have my file in here so
68:32 - let's look at this file and what i've
68:34 - done is i've set up the proper yaml
68:37 - format with indentation and everything
68:38 - to set up bonding so i'll go over it
68:40 - really quickly first of all we need to
68:42 - use network d as our renderer we can't
68:45 - use network manager in order to
68:47 - configure bonds because it just doesn't
68:48 - support bonding
68:50 - we do have to define the ethernet cards
68:53 - themselves so ethernet eth0 i set dhcp4
68:56 - to false because i don't want it to
68:58 - actually assign an address to eth0 after
69:01 - all that is done we don't need to set up
69:03 - anything more for the ethernet ports
69:04 - themselves then we need to set up a
69:06 - bonded interface so a new section here
69:09 - bonds the name of it is bond 0 dhcp 4 is
69:13 - false because i'm going to assign a
69:14 - static ip address and the interfaces is
69:18 - a section that we tell it what
69:19 - interfaces are going to be a part of
69:21 - this bond in our case we only have one
69:23 - which is silly i mean we're only bonding
69:25 - one interface but you can do that one or
69:28 - more it's just silly that we only have
69:30 - one to bond that's just all this
69:31 - computer has i've set up the addresses
69:34 - the gateway
69:35 - the name server is set up just like it
69:37 - would be if we were setting it up for
69:38 - eth0 really the only new difference down
69:40 - here is the parameters and the
69:42 - parameters what i've done is mode active
69:45 - backup that's just one of our modes it
69:47 - happens to be
69:48 - mode one but we actually say active
69:50 - backup we specify by name the mode here
69:52 - and then we save this file and do
69:55 - netplan
69:56 - apply now how you can tell if it's
69:58 - working we can do ipadd
70:00 - to see that you know sure enough it's up
70:02 - here's our bond zero interface here's
70:03 - our ethernet zero interface which is up
70:05 - but doesn't have an ip address and
70:07 - another way you can test is to look at
70:09 - so we're just going to cap the file it's
70:11 - in the virtual file system proc under
70:14 - net
70:15 - bonding
70:16 - bond zero so if we look at this file
70:19 - it's going to tell us the information
70:20 - about it and say bonding mode is fault
70:22 - tolerance active backup
70:24 - primary slave none but the current
70:26 - active slave is eat zero it's our only
70:27 - one that we have it's up and it looks
70:30 - like everything is working good here's
70:31 - our slave interface eth0 down here and
70:33 - that's all working well as well so we
70:36 - have it all set up and it's working now
70:38 - with centos the setup is a little bit
70:40 - different because centos is set up
70:43 - differently in centos in order to
70:45 - configure our ethernet ports we go into
70:47 - etc sysconfig
70:50 - network scripts and in here we have our
70:53 - ifconfig files so i'm going to first
70:55 - look at the changes we have to make to
70:56 - ifconfig eat zero so
70:58 - here we basically we make it pretty
71:00 - simple ethernet type product boot
71:02 - protocol is none uh the name the device
71:04 - is zero on boot we want it to come up
71:06 - but notice i haven't given any ip
71:08 - addressing information and here are the
71:10 - two things the master
71:12 - is bond zero and yes this is going to be
71:14 - a slave to this bond all right so those
71:16 - are the changes we make here if we look
71:19 - at if config bonds zero this is where we
71:22 - set up bond zero we name it bond zero
71:24 - bonding master is yes i did give this
71:27 - one an ip address and a prefix which
71:29 - means the subnet mask
71:31 - on boot we want it to come up a boot
71:32 - protocol is none and then here the
71:34 - bonding options we tell it what we want
71:36 - it to do i actually set this one up to
71:38 - mode 6 mode 6 is balance all load
71:42 - balancing so it does its own version of
71:44 - load balancing for incoming and outgoing
71:46 - traffic again i only have the one
71:48 - interface on this computer but that's
71:50 - okay
71:51 - let's do the same thing now checking to
71:53 - make sure it's working is similar we can
71:55 - do ipadd we can see sure enough here's
71:58 - the bond set up right here and our
72:00 - ethernet is right here
72:02 - and really for configuring the bonds
72:04 - themselves that's all there is to it
72:05 - ubuntu and centos are configured
72:07 - differently but underneath they're both
72:09 - doing the same thing they're using the
72:11 - bonding kernel module and they're
72:12 - allowing you to do some awesome things
72:14 - with multiple ethernet ports on your
72:16 - computer
72:18 - gpt and mbr are two different ways of
72:22 - taking a hard drive and chopping it up
72:24 - into pieces so that those pieces can be
72:27 - recognized and mounted as different
72:28 - drives and stuff on your on your system
72:30 - so they're they do the same thing but
72:32 - gpt is much newer and much more
72:36 - feature-rich than the old-school mbr now
72:39 - there's also a really cool thing called
72:40 - protective mbr i want to talk about but
72:42 - basically we're going to talk about the
72:43 - differences between gpt and mbr here i
72:46 - have two big squares and i'm going to
72:48 - say that these actually they're
72:49 - rectangles
72:50 - i'm going to say that they represent the
72:52 - drives themselves and they're going to
72:53 - be partitioned with two different
72:56 - systems so we'll start over here with
72:57 - the old one mbr which stands for master
73:01 - boot record okay now how this would work
73:04 - is let's say this drive is device
73:08 - sda okay so this is the device on the
73:11 - linux system it recognizes this drive
73:14 - now the mbr is a little tiny bit of
73:17 - reserved space
73:19 - at the very beginning of the disc and
73:21 - this is basically like a table of
73:22 - contents and it describes
73:25 - how
73:26 - this is chopped up okay so this is
73:29 - chopped up into let's say four pieces
73:31 - there's four primary partitions you can
73:33 - also cut each one of the primary
73:35 - partitions into four so we could have
73:37 - you know mult we could have this one be
73:38 - like
73:41 - have four partitions there if we wanted
73:43 - but basically what this means is once
73:45 - the mbr defines in this little
73:48 - first part of the drive right here what
73:50 - is what we come up with other devices on
73:53 - the system we have
73:55 - dev sda1
73:58 - devsda2 and it goes on down the line and
74:01 - so each of these chunks is referenced on
74:04 - the system as their partition name now
74:07 - this is the raw device dev sda doesn't
74:10 - go anywhere it's still there but the
74:12 - individual partitions get their own
74:14 - device and that's how we reference them
74:16 - on the system so mbr has this you know
74:19 - this little tiny table of contents at
74:21 - the beginning and it says where things
74:23 - are chopped up now gpt or
74:26 - gui guid partition table which i think
74:28 - is interesting because there's an
74:30 - acronym wrapped inside of an acronym
74:32 - this stands for globally unique
74:34 - identifier partition table and basically
74:36 - it does conceptually the same thing
74:39 - right it will chop up this drive and
74:41 - you'll end up with dev sda for the raw
74:44 - device and so on down the line the
74:46 - differences are though gpt does have a
74:50 - spot at the beginning with the table of
74:51 - contents but then it also scatters them
74:54 - copies of them
74:56 - around the drive so that if something
74:58 - were to happen with one of the copies of
75:01 - the gpt it can still figure out what's
75:05 - on the disk it also does some crc
75:07 - correction on the files itself or on the
75:10 - the file system itself so that it knows
75:12 - if there's some corruption and it can
75:13 - fix it it's just very very robust along
75:16 - with the ability to use much bigger
75:18 - drives mbr is limited to two terabytes
75:21 - gpt is limited to i think petabytes i
75:23 - mean there's no practical limit at this
75:25 - point of how much drive space it can
75:27 - talk about rather than being
75:29 - limited by four partitions you can have
75:31 - tons of partitions on here so you know
75:33 - we could chop it up into as many as we
75:35 - wanted and it just goes down the line
75:37 - sda1 sda2 sda3 and one other really
75:41 - interesting thing and i wanted to talk
75:43 - about is the protective mbr so gpt is
75:47 - used on newer systems it's specifically
75:50 - part of uefi that replaces bios but
75:52 - biosystems can still see gpt drives and
75:55 - part of the reason they can do that is
75:56 - at the very beginning
75:58 - there's this section that looks exactly
76:01 - like an mbr basically it's an mbr record
76:04 - here that says okay there is one big
76:06 - partition on this drive the entire drive
76:08 - is a partition
76:10 - there's no room available on this
76:12 - partition and it takes up the entire
76:14 - drive now why would that exist it's
76:16 - basically so systems don't say oh well
76:18 - there's no mbr in this drive so this
76:21 - must be a blank drive with nothing on it
76:24 - well we don't want that to happen so
76:25 - basically what it means is it allows the
76:28 - mbr system or a bio system that is used
76:31 - to seeing mbr it will allow it to
76:33 - realize that this is not an empty drive
76:35 - it's just something it can't read and
76:37 - then there's some really cool stuff
76:39 - where part of this initial mbr can be
76:42 - hints to the operating system itself as
76:45 - to what's happening underneath and so
76:47 - some older bio systems are able to boot
76:50 - from a gpt drive
76:52 - even though it shouldn't be able to
76:54 - because the linux system or you know the
76:56 - operating system itself says okay i see
76:59 - mbr is saying that um you know it's
77:01 - really a gpt drive but i know enough
77:03 - about gpt myself that i'm going to be
77:05 - able to find the software and you know
77:08 - the the drive files and partitions on
77:10 - here myself and so you can actually use
77:13 - it on there but it's because of that
77:14 - protective mbr that all that is possible
77:17 - so when it comes down to it mbr is the
77:19 - old way of doing things gpt is the new
77:22 - kid on the block honestly there's no
77:24 - reason not to use gpt it does everything
77:27 - mbr does it does it more efficiently
77:29 - more reliably
77:30 - it's just the better way to go so i
77:32 - recommend that you use gpt and while
77:35 - there are significant differences a lot
77:37 - of those backwards compatibility issues
77:39 - make it so that you don't even have to
77:40 - worry about them
77:42 - the linux file system is actually really
77:45 - cool because all of the network mounts
77:48 - and different hard drives and usb drives
77:49 - are all on one
77:51 - giant file system it's not there's no a
77:54 - drive b drive c drive d drive there's
77:56 - nothing like that it's just all one big
77:58 - file system hierarchy and we're gonna
78:00 - look at the different things because
78:02 - whether it's a real or virtual file
78:03 - system it's on the same file system
78:05 - whether it's relative or absolute this
78:07 - is just how you traverse
78:09 - the actual file system we're gonna look
78:10 - on the command line about that network
78:12 - mounts they're just you know on a remote
78:14 - system but they just show up as a folder
78:17 - on your local computer it's just it's
78:20 - really cool so first of all let's let's
78:22 - start at the very beginning it's a very
78:24 - good place to start so we have a hard
78:27 - drive right so we're going to say it's
78:29 - dev sda1 and that has the root partition
78:33 - on it you know just forward slash so
78:34 - this is like where everything starts the
78:36 - root partition is the basis of our linux
78:39 - file system it's it's on a hard drive of
78:42 - some sort in our case it's this one now
78:44 - inside there there's tons of files and
78:47 - folders some of the files are directly
78:49 - on that dev sda drive right on the root
78:52 - file system or folders within that root
78:54 - file system they're actually stored on
78:56 - that hard drive
78:57 - some of the folders and the files in
78:59 - there are a virtual file system if you
79:02 - look inside the
79:03 - root directory there's a proc folder an
79:06 - assist folder these are dynamically
79:08 - created file systems that are just a way
79:11 - to interact with the kernel itself so if
79:14 - you want to make a change to the running
79:15 - kernel you can make a change to one of
79:17 - the files in these folders and it's
79:19 - going to affect the running kernel but
79:20 - it's not actually files it's a virtual
79:24 - file system it's just a way that they
79:25 - represent that interface with the kernel
79:27 - is this virtual file system then we also
79:30 - have like a remote nfs server it could
79:32 - be nfs or samba or you know whatever
79:35 - your network protocol of choice is but
79:37 - it's mounted on the folder
79:40 - inside of your root system so this could
79:42 - be like the home folder on your system
79:44 - might not be on the actual hard drive it
79:47 - might actually be on a remote system but
79:49 - it's mounted inside your file system and
79:52 - from you know from a layman's position
79:54 - just looking at the system you don't
79:56 - know if it's a remote system or a local
79:58 - system or a virtual system because they
80:00 - all look the same same thing when you
80:02 - insert a usb drive it doesn't come
80:03 - through as like you know like the e
80:05 - drive or the f drive like on a windows
80:07 - system it's just mounted somewhere on a
80:10 - folder inside that same
80:13 - one solid
80:14 - monolithic file system even if you put
80:17 - in another hard drive so this is we're
80:19 - saying this is dev sda1 like the
80:20 - partition one on the sda drive even if
80:23 - you were to put in a second hard drive
80:25 - in the system it's still going to mount
80:28 - inside a folder or a subfolder of this
80:31 - root
80:32 - drive right it's gonna be like in mount
80:34 - data or wherever you happen to mount it
80:36 - it's just going to appear as a folder
80:38 - inside this root file system so
80:41 - everything is inside that root file
80:43 - system it's really a neat way to handle
80:46 - all of the various things that can be
80:48 - stored in linux and the virtual file
80:50 - system is very unique in that it's not
80:52 - really a file system it's more like an
80:54 - interface designed as a file system so
80:56 - you can interact with the kernel itself
80:58 - alright so let's go to
81:00 - our file system here and i just want to
81:02 - show you kind of how it looks and i want
81:04 - to talk about this whole absolute versus
81:05 - relative thing all right so i'm just
81:07 - going to
81:08 - show you the file system here we have ls
81:10 - and we have a folder called pictures we
81:11 - can do ls picture gotta spell it right
81:15 - ls
81:16 - pictures and i have a folder cbt gold
81:19 - which is this background and then a
81:20 - trips folder uh what if we were to do ls
81:24 - trips
81:25 - okay there's a folder called grocery
81:27 - store mexico orlando now there's a cool
81:29 - program called tree i'm going to use so
81:31 - let's say tree
81:33 - pictures it's going to show us a tree
81:35 - representation of all the files and
81:37 - folders inside here so we can look we
81:39 - have the pictures folder there's a one
81:41 - file in there called cbt gold then
81:42 - there's a folder called trips inside
81:44 - that folder are three different folders
81:46 - grocery store mexico orlando inside each
81:48 - of those there are files that are stored
81:51 - in there this is the hierarchy of that
81:53 - now honestly
81:54 - this mexico folder this could be like a
81:57 - remote nfs share we don't know because
81:59 - it's all on the local file system that
82:02 - is you know mounted everywhere on the
82:03 - system it's just one file system so we
82:05 - don't know where these are we just
82:07 - interact with them as if they're all one
82:09 - local file system so that's just a
82:10 - really cool thing about it but this
82:12 - whole absolute versus relative let's go
82:14 - over to
82:15 - the pictures folder
82:17 - and trips
82:19 - i'm going to clear the screen and if we
82:20 - were to type ls minus a we're going to
82:22 - see all of the things in here now we
82:24 - already saw some of these orlando mexico
82:26 - grocery store but these dot and dot dot
82:30 - now these are special folder entries
82:32 - that mean the dot means the current
82:34 - folder so if i were to say cd to dot
82:37 - i'm in the same folder dot dot means the
82:40 - directory above me so if we are in trips
82:44 - dot dot is going to be pictures so if we
82:47 - were to say cd dot dot
82:50 - all of a sudden you'll see now we're in
82:51 - the pictures folder that's pretty cool
82:54 - right now we can use that as a folder
82:56 - name anywhere so let's go into
82:58 - trips orlando okay and inside here let's
83:01 - do an ls minus la we can see here are
83:04 - all the the pictures that we have and
83:06 - then these two folders dot dot and dot
83:08 - in every single folder you're gonna find
83:10 - a dot and a dot dot because it's just a
83:12 - pointer that means
83:13 - this directory
83:15 - the directory above me okay so we can
83:17 - put that in a string too let's say i
83:19 - want to cd to dot dot
83:22 - dot dot
83:24 - and see where this takes us
83:25 - two pictures now why did it do that well
83:27 - because dot dot if we're in orlando dot
83:30 - dot means trips and then we're in trips
83:33 - the second dot dot means pictures so
83:35 - that's interesting we could do something
83:37 - really complicated and i'll end with
83:38 - this we could say cd
83:41 - trips
83:43 - orlando
83:44 - dot dot
83:47 - grocery store
83:49 - oh okay what happened there and we'll
83:51 - end with this one like i said so we
83:52 - started in pictures and we went to the
83:54 - trips folder
83:56 - orlando
83:57 - back to the trips folder and then into
83:59 - the grocery store folder and sure enough
84:01 - that's where we ended up pictures trips
84:03 - grocery stores so you can use dot dot
84:05 - anywhere in your thing to talk about the
84:08 - relative path of where you're where
84:11 - you're going okay now the absolute path
84:13 - of this is going to
84:14 - be cd home
84:18 - bob
84:19 - pictures
84:20 - trips
84:21 - grocery store
84:23 - and then sure enough we're in the same
84:24 - folder this is the absolute path
84:25 - starting at the root level but we can
84:28 - use relatives paths using things like
84:30 - dot dots one other thing i said i was
84:32 - going to end there and i guess i lied to
84:33 - you this tilde image or this tilde thing
84:36 - is a shortcut for your home directory so
84:38 - if we were to say cd to the tilde which
84:41 - is usually to the left of your one key
84:43 - boom now we are in the tilde which if we
84:46 - type pwd we'll see is home
84:49 - bob all right so those are the relative
84:51 - path tools that you can use to construct
84:53 - where you want to go but that's really
84:54 - how it works and the tree program is
84:56 - just kind of a cool way to look at
84:57 - things you can see everything else
84:59 - spelled out there the big takeaway is
85:01 - that the linux file system is just one
85:03 - big monolithic file system and whether
85:06 - you're mounting network shares or you're
85:07 - mounting usb drives or second hard
85:09 - drives or even a virtual file system to
85:12 - interact with the kernel it's all under
85:14 - the same bunch of file system in this
85:17 - big hierarchy
85:19 - partitions are really just
85:20 - organizational units that are on a hard
85:22 - drive you kind of chop a hard drive up
85:24 - into
85:25 - different partitions and those
85:26 - partitions are used for different things
85:28 - like swap space or you know a particular
85:31 - folder on your system
85:33 - you can use partitions for a lot of
85:34 - things and depending on the system that
85:36 - you're using whether it's going to be an
85:37 - mbr master boot record or gpt gui id
85:41 - partition table you can have a multitude
85:44 - of partitions or just a few and
85:47 - depending on which scheme you use it
85:48 - depends how big your hard drive can be
85:50 - now we're going to look at a handful of
85:51 - tools like parted and g parted and this
85:54 - stands for partition editor and
85:56 - graphical partition editor
85:59 - and the old school f disk which is just
86:01 - a command line tool that didn't used to
86:03 - handle the gpt partition table but now
86:06 - it does so there's this one size fits
86:08 - all tool called fdisk and then i do want
86:11 - to show you a couple tools that will let
86:12 - you see what block devices are available
86:15 - to partition on your system so let's
86:16 - actually go right to the command line so
86:19 - that we can check out what's on our
86:20 - system and i chose to use centos today
86:22 - for no other reason than i thought let's
86:24 - use centos so i want to show you first
86:27 - of all how we can identify the block
86:28 - devices on our system we can type lsblk
86:32 - and it's going to show us this really
86:33 - nice little cool tree thing right so we
86:35 - have fd0 floppy disk which actually is a
86:37 - lie there's not a floppy disk on this
86:39 - system it's a virtual machine but then
86:41 - sda is the main hard drive on the system
86:44 - and it's partitioned into two partitions
86:47 - the sda1 which is the boot partition and
86:50 - then sda2 which is used in lvm which
86:53 - we'll talk about in a different nugget
86:55 - then there are a bunch of other hard
86:57 - drives installed on the system that
86:58 - don't have anything on them at all there
87:00 - are four 10 gigabyte drives sdb sdcsd
87:04 - and sde and we can also see these things
87:07 - if we look at proc partitions this is
87:10 - again the virtual file system that shows
87:12 - us an interaction with the kernel and we
87:13 - can see sure enough here they are sdb c
87:16 - d and e and this is how big they are the
87:18 - number of blocks so they're 10 gigabytes
87:20 - and of course if we just looked in the
87:22 - dev folder and gripped for sd we would
87:25 - see all these devices too right sd b c d
87:28 - and e so what i want to do is just
87:29 - partition one of them we're going to
87:30 - pick sdb and there's a couple ways we
87:33 - can do this we could use g parted now
87:35 - notice i'm root we have to be rude if
87:37 - we're going to do the system level stuff
87:38 - like partition a hard drive but we could
87:40 - say g parted and it'll start up and scan
87:42 - for all the drives available and there's
87:44 - a little drop down here sdb very cool we
87:47 - would have to start by creating a
87:49 - partition table and then we could create
87:51 - a partition with gui tools it's really
87:53 - really easy to use g parted but a lot of
87:55 - times the system doesn't have a gui so i
87:57 - don't want to do it that way now we
87:58 - could also use parted or partition
88:01 - editor this is the cli version of the
88:03 - program that we just used we can type
88:05 - help and it'll show us it does the exact
88:08 - same things it's just used you know use
88:10 - text to tell it what to do all right so
88:12 - i'm not going to use this one either but
88:13 - i want you to know that they exist and
88:15 - if you're more comfortable with them
88:16 - they're a little bit newer and they have
88:18 - a few more features and that sort of a
88:20 - thing but what i want to show you is the
88:22 - old school f disk and i want to show you
88:23 - this because if you're on a server it
88:26 - may not have partition editor you know
88:28 - part ed or especially g parted the
88:30 - graphical one so i want to show you what
88:32 - almost every linux system is going to
88:34 - have and we're going to use that one to
88:36 - actually do our work so we're going to
88:37 - say fdisk
88:39 - dev sd b we have to tell it what device
88:42 - we want to use so it's a good thing we
88:43 - know how to find the available devices
88:45 - on our system
88:46 - type this and type m for help now we
88:50 - just have these single letter commands
88:52 - but that's okay the first thing we're
88:53 - going to need to do is to create a
88:55 - partition table and it can be one of two
88:57 - types we can create a gpt partition
89:00 - table by pressing g
89:01 - or we can create the old
89:03 - mbr or ms-dos partition table using o oh
89:07 - i'm just going to pick since it's not
89:09 - over two terabytes in size either one is
89:11 - going to work so let's just use gpt for
89:14 - the heck of it so i'm going to say g
89:15 - press enter it says okay it built a new
89:18 - gpt disk label this is its id and now we
89:21 - have a partition table but no partitions
89:23 - to create a partition we can actually
89:25 - type p to see what's there there's gonna
89:27 - be nothing there so to create one let's
89:29 - press m again so we get the whole help
89:31 - screen we can say n for add a new
89:33 - partition so new and now it says
89:36 - partition number one through 128
89:38 - right this is not mbr mbr would only
89:41 - support one through four this is gpt so
89:44 - one through 128 i'm just going to pick
89:46 - the default
89:47 - first sector the default is going to be
89:49 - the first one available last sector the
89:51 - default is going to be the last one
89:52 - available right this is our range and it
89:54 - chose the last one so i want to fill up
89:55 - the entire partition or the entire drive
89:58 - with this one partition so if we type p
90:00 - now we're going to see sure enough we
90:01 - have a partition there it's 10 gigabytes
90:03 - linux file system type that's okay
90:05 - and now to quit let's press m again for
90:07 - help to quit we could just press q and
90:10 - it would quit without saving changes but
90:11 - we actually want to make these changes
90:13 - so we're going to make the change to the
90:14 - disk so we're going to use w to exit
90:16 - which means right so w
90:19 - enter and now our partition table has
90:21 - been done we can say lsblk and look at
90:24 - that now it shows up that we have a
90:25 - partition created on our system it
90:27 - happens to be a gpt partition table and
90:29 - it's a single partition that takes up
90:32 - the entire drive i didn't actually go
90:34 - through the process of using parted or
90:36 - g-parted they're very straightforward
90:38 - and they walk you through it and the
90:39 - help screen is right there and easy to
90:40 - use so if you want to use those by all
90:42 - means go ahead but i wanted to show you
90:44 - the more complicated f-disk because it's
90:46 - going to be on every single system that
90:48 - you run across
90:49 - there's a wide variety of file systems
90:52 - that you can put on an empty partition
90:53 - in linux but the idea of all of them is
90:55 - the same you take a big empty partition
90:58 - like a big field and you divide it up so
91:02 - that you can store data efficiently like
91:04 - if this parking lot were
91:06 - an example of a file system each car
91:08 - would be a piece of data that is stored
91:10 - logically in its place and and some file
91:13 - systems have you know bigger spaces for
91:15 - parking buses some have littler spaces
91:18 - for parking compact cars but
91:19 - conceptually they all do the same thing
91:22 - now like i said there's a bunch of
91:23 - different options available in linux ext
91:26 - is the most common family of file
91:30 - systems that you can use on linux it's
91:31 - very mature it's been around forever the
91:33 - later versions support journals so that
91:35 - if a computer gets powered down before
91:37 - the reads and writes are synced up
91:39 - usually you can salvage the data on
91:41 - there so it's fairly robust as well xfs
91:45 - is a file system that's been around for
91:47 - a long time this used to be what you had
91:48 - to use for really really big drives but
91:51 - now everything supports big drive so
91:53 - that's not really the issue anymore it
91:55 - is still used by centos and red hat
91:57 - though so xfs is still widely used
92:00 - interestingly it has its own set of
92:02 - tools like for
92:04 - file checking fixing things manipulating
92:07 - the the xfs file system itself rather
92:09 - than using standard linux tools it has
92:11 - its own set of xfs tools there's a
92:14 - couple others
92:15 - btrfs it's often called butterfs
92:18 - this was the new kit on the block right
92:20 - this has awesome features like
92:22 - snapshotting and it's just awesome
92:24 - unfortunately it's kind of been
92:25 - abandoned which is weird but nonetheless
92:28 - it's kind of what happened so btrfs is
92:30 - still functional but it's not widely
92:33 - used anymore and then of course i'm just
92:34 - going to mention them dos or the windows
92:37 - world has their own file systems like
92:38 - ntfs vfat fat32 things that you're
92:41 - familiar with if you're in the windows
92:43 - world and linux can usually read and
92:45 - even write to most of these file systems
92:48 - but when we're talking about linux file
92:49 - systems we're generally talking about
92:51 - linux specific ones and ext
92:54 - is awesome i'll be honest i like ext
92:56 - there's generally three there's e xt 2
92:59 - ext3 and ext4 now this isn't necessarily
93:03 - like one is better than the next they
93:05 - each have their own features but ext4 is
93:07 - the newest and has the most features
93:09 - i'll be honest i almost always pick ext4
93:12 - as my default file system not because
93:15 - it's necessarily better than any other
93:17 - option but because it's so widely used
93:19 - that means there are a ton of tools and
93:21 - utilities and tutorials online to get
93:24 - data back if you have some corruption so
93:27 - ext4 is my file system of choice mainly
93:30 - because it's used in so many places when
93:32 - it comes to creating a file system
93:34 - basically you need a partitioned hard
93:36 - drive so that you can you know have a
93:38 - partition or that empty field in order
93:40 - to draw the lines for your parking lot
93:41 - or put that file system on now i'm going
93:43 - to say lsblk
93:45 - and we're going to see these are the
93:46 - different block devices on our system i
93:49 - have two partitions created i have sdb1
93:52 - and sdc1 these are just 10 gigabyte
93:54 - partitions on 10 gigabyte drives so it
93:56 - takes up the whole drive and we're going
93:58 - to format each of them all right so i'm
94:00 - going to say oh here's another really
94:01 - cool thing about linux the file
94:03 - formatting or the hard drive formatting
94:05 - programs all start with mk for make fs
94:09 - file system and then just hit tab a
94:11 - couple times and you're gonna see all
94:12 - the various tools for creating the
94:14 - different types of file systems so let's
94:17 - say we're going to make a butterfs file
94:19 - system so we would say
94:22 - mkfs.btrfs and then what partition we
94:25 - want to create that file system on in
94:27 - our case dev
94:29 - sdb1 all right so it's created it it
94:31 - says we have 10 gigabytes idu's one
94:35 - that's the path okay so it looks like it
94:36 - did that without any problems let's
94:38 - create another uh file system so uh mkfs
94:42 - hit tab a couple times so i can see our
94:44 - options now like i said i usually use
94:46 - ext4 so i'm going to say ext4
94:49 - dev sdc one which was the other
94:52 - partition that i had created on this
94:53 - system and it went through and it
94:55 - created that file system and now a
94:56 - really cool thing if we use the lsblk
95:00 - but we add the dash f it's going to show
95:02 - us the file systems that are on the
95:04 - particular block devices as well so if
95:06 - we do that we're going to see up here we
95:08 - have sda1 which remember i said red hat
95:11 - and sent to us use xfs well sure enough
95:13 - there it is xfs is used sdb1 is that
95:16 - butterfs file system that we created and
95:18 - then down here is an ext4 file system
95:21 - that we created on our system so that's
95:24 - all there is to creating the various
95:26 - file systems and really you can pick
95:28 - whichever one you want but you do have
95:29 - to have a partition in existence before
95:31 - you can create a file system on it and
95:33 - while yes there are a whole bunch of
95:34 - file systems that you can use on linux
95:36 - and it supports a bunch i really
95:38 - recommend that ext family of file
95:41 - systems if you have a choice and you're
95:43 - just trying to decide what one to use
95:45 - largely because it's used so often
95:47 - there's so much support if something
95:49 - goes wrong you can find a lot of help
95:50 - online
95:51 - if we want to be able to access the data
95:53 - that's on the drives or the partitions
95:55 - of the drives that we put into our
95:57 - system we have to mount them into our
96:00 - local file system now we can do that
96:02 - manually using tools like mount or
96:03 - u-mount or we can use the et cetera fs
96:06 - tab file to do it automatically on boot
96:09 - but conceptually what's going on is that
96:11 - we have a new hard drive or a
96:13 - partitioned hard drive and we want to
96:15 - incorporate that into our file system
96:18 - now it's really important to note that
96:19 - when you mount a partition or a hard
96:21 - drive it goes into a folder and that
96:24 - folder becomes what is in that drive now
96:27 - we couldn't mount it on this folder
96:29 - because this folder already has
96:32 - things inside of it it has to be mounted
96:34 - on an empty folder because it doesn't
96:35 - make sense to have all the things in
96:38 - this hard drive in this folder and then
96:40 - have these other folders alongside of it
96:42 - so it has to be in an empty folder that
96:44 - we mount a partition or a drive so we
96:47 - could actually take this hard drive and
96:49 - mount it into this folder if it's an
96:51 - empty folder and that's what we're going
96:52 - to do on our system is find an empty
96:55 - folder and then this all the files
96:57 - inside this drive will become part of
97:00 - our bigger linux file system starting at
97:02 - this mount point
97:04 - now i'm here on an ubuntu system and i
97:06 - have the regular route mounted hard
97:09 - drive and then i also have an additional
97:11 - 10 gig drive and if we do lsblk we can
97:14 - see that we have well a bunch of things
97:16 - here loop loopback devices but we have
97:18 - down here sdb1 okay this is a 10
97:20 - gigabyte partition and and it's all set
97:23 - up and ready to mount but it's not
97:24 - currently mounted on our system so in
97:27 - order to mount that i actually want to
97:29 - mount it in a folder if we look inside
97:31 - mnt there's a folder called 10 gig and
97:34 - inside that folder
97:36 - is nothing it's an empty folder so i
97:38 - want to mount it on there now if we do
97:41 - blk id we can see a little bit more
97:44 - information about the device itself so
97:47 - here is dev sdb1 now here is the
97:50 - universally unique identifier for this
97:52 - partition uh keep note that this is
97:55 - something that's specified about the
97:56 - partition we'll look at that in a second
97:58 - but we know that it's type
97:59 - ext4 okay that's good to know and we
98:02 - know that this is where the actual
98:04 - partition lives so we can say mount now
98:07 - i'm going to say dash t for type ext4
98:10 - usually mount can automatically figure
98:13 - out what kind of file system it is but
98:15 - if we know there's no reason not to do
98:17 - that so i'm going to say mount dev
98:20 - sdb1
98:22 - and where do i want it to mount on mnt
98:25 - 10 gig
98:27 - press enter
98:28 - and now if we go into
98:30 - mnt
98:32 - 10 gig
98:33 - we're gonna see sure enough now there's
98:34 - a lost and found thing this is the root
98:36 - level of that hard drive but now it
98:38 - actually lives in our file system right
98:40 - here we can type mount alone on the on
98:43 - the line it will show us where it's
98:44 - mounted right so right here it's mounted
98:46 - on mnt 10 gig so let me type cd and then
98:50 - we can unmount it by typing umount
98:53 - mnt
98:54 - 10 gig it's not unmount it's umount all
98:57 - right so there now that that's how we
98:59 - manually mounted and unmounted if we
99:00 - wanted to have it mount automatically on
99:03 - boot we would edit the file etc fs
99:06 - tab all right and i'm actually going to
99:08 - stretch this out so we can see
99:10 - everything on here we can see there's
99:12 - already an entry in here for the root
99:14 - file system sda or s yeah
99:18 - sda1 but notice it doesn't specify it by
99:22 - its device it specifies it by its
99:24 - universally unique identifier which we
99:26 - could get by using that blk id command
99:29 - it'll get us that uuid for the
99:31 - particular partition now we could use
99:33 - the uuid for a dev sdb1
99:37 - and we could put that in here to mount
99:39 - it but i'm actually going to use let me
99:41 - make another entry here i'm actually
99:43 - going to
99:44 - say that the file system is on
99:47 - dev sd b1 we can specify it by device or
99:51 - by uuid or we could even use the drive
99:53 - label if we want but in this case i'm
99:55 - just going to use the the device itself
99:58 - and then tab over the next field here is
100:01 - the mount point so i want this mounted
100:03 - on mnt
100:04 - 10 gig
100:06 - tab over the next thing is type it's an
100:09 - ext4
100:11 - options i'm just going to say defaults
100:14 - and then the last two fields are dump
100:16 - and pass now dump is an old school
100:19 - backup program it used to dump the files
100:21 - to a backup this is really deprecated
100:23 - it's not really used anymore at all so
100:25 - dump you're going to want to put zero
100:27 - for dump the last line though pass this
100:30 - means do you want it to run a file
100:33 - system check zero means never run a file
100:35 - system check
100:37 - one means run the file system check
100:39 - first so you put a one on the root
100:42 - partition
100:43 - any other partition that you wanna have
100:45 - checked when the system boots up you're
100:47 - gonna put a two
100:48 - so i'm going to put dump of 0 you always
100:50 - put 0 for dump and then pass is 2. it's
100:53 - the second most important because the
100:55 - root is the first one you want to have
100:56 - scanned and then everything else is
100:57 - going to be 2. so you can have like five
100:59 - different partitions mounted they could
101:00 - all have pass of 2. now there's more to
101:03 - it to get it to scan automatically on
101:05 - boot but when you're setting up the
101:06 - fstab file this is where you do it so
101:08 - save this and now we could just type
101:11 - mount minus a and it's going to mount
101:13 - everything that is specified in fstab
101:17 - and we can look by saying
101:19 - mount and it'll show us that sure enough
101:20 - it remounted that because we defined it
101:22 - in fstab and if we reboot the computer
101:25 - it's going to automatically boot it or
101:27 - it's going to automatically mount it as
101:28 - well it's really easy to mount
101:30 - partitions using the manual tools it's
101:32 - also pretty easy to use the fstab file
101:35 - to specify it and you can either specify
101:37 - it by device name or the uuid that we
101:40 - can find out using the blk id program
101:45 - to scan a linux file system generally
101:47 - you use the tool fsck or just fisk as
101:51 - it's often referred to now the real key
101:53 - though is to have it scan automatically
101:56 - periodically on boots so you don't have
101:58 - to manually do it because here's the
102:00 - deal in order to run fisk the file
102:03 - system itself has to be unmounted that's
102:05 - not really a problem for secondary or
102:07 - tertiary drive mounts like the home
102:09 - directory or something like that but the
102:11 - root directory it's pretty difficult to
102:13 - unmount the root directory and scan it
102:15 - unless you're in the boot up process or
102:17 - you've booted from a cd or something so
102:19 - i want to talk specifically about how to
102:21 - set up the system to scan the the file
102:24 - system on boot including the root file
102:27 - system so that you can you know have it
102:29 - automatically maintain itself now when
102:31 - it comes to scanning automatically
102:33 - there's a few different flowchart things
102:36 - that go on the very first thing the
102:37 - kernel looks for is inside your fs tab
102:40 - file if the pass setting is set up if
102:44 - it's a zero you know if you have your
102:46 - your dump and pass and if the pass is
102:48 - set to zero then it won't scan it just
102:50 - absolutely will refuse to scan it
102:52 - doesn't even look any further it just
102:53 - stops right there and continues booting
102:55 - up the system if however you have that
102:57 - partition set up with either a one or a
103:00 - two a one for the root partition a two
103:02 - for any other kind of partition if you
103:04 - have it set up then it looks at the
103:07 - drive itself and it says okay has the
103:10 - maximum number of allowed mounts been
103:13 - reached and if that threshold has been
103:16 - reached it will scan the drive before it
103:18 - boots up if not if it hasn't met this
103:22 - maximum yet then it's not going to scan
103:24 - it's going to increment it's going to
103:25 - say okay i'm going to mount one more
103:27 - time and add it to the number of times
103:28 - i've been mounted but i'm not going to
103:30 - scan it even if the pass is set up to
103:33 - scan it's not going to scan it if it
103:35 - hasn't met the max now the max by
103:38 - default is negative one which means it's
103:40 - never going to scan because that's just
103:43 - the way of saying like don't ever
103:45 - automatically mount so by default you're
103:47 - never going to get an automatic scanning
103:48 - which is a little bit frustrating
103:50 - because you do want to have your system
103:52 - automatically scanned so on ubuntu here
103:55 - i have
103:56 - dev
103:57 - sdb1 mounted on mnt 10 gig okay so this
104:01 - is another partition this is not my root
104:03 - partition but it's mounted on mnt 10 gig
104:06 - all right if i look inside of the fs tab
104:10 - file
104:11 - we're going to see that on this
104:13 - partition that i have automatically
104:14 - mount on boot it's set up with a pass of
104:17 - 2 which means that it is going to check
104:20 - to see if it should scan automatically
104:22 - okay it's not the root partition so i
104:24 - don't have it set up with a pass of one
104:26 - but since it's set up with two it's
104:28 - going to check and say okay if it's time
104:30 - i'm gonna scan this
104:32 - so let's get out of here and how we can
104:34 - see what the maximum number of allowable
104:36 - mounts before it will scan is is to use
104:39 - the tune 2fs
104:42 - we're going to do dash l for a listing
104:44 - of
104:45 - dev sdb1
104:47 - and it's going to show us this now what
104:49 - you want to look for in here is this
104:51 - maximum amount count remember i said by
104:53 - default it doesn't ever scan that's
104:55 - because this is set to negative one and
104:57 - you're never going to reach that because
104:59 - that's just the way of saying disable it
105:01 - if we want to have it automatically scan
105:03 - every so often we need to change this so
105:05 - we would say
105:06 - tune 2fs
105:09 - c for count and i'm going to say
105:12 - every 10 months i want it to scan in dev
105:16 - sd b1 okay so now the maximum amount
105:19 - count is 10. we can look at that by
105:22 - doing the same command over and we can
105:23 - see now the maximum amount count is 10.
105:26 - okay so what does that mean well every
105:28 - time the system boots it mounts the
105:30 - partition well we could speed that up we
105:33 - could say u-mount dev sd b1
105:36 - mount dev sdb one we do that and now if
105:40 - we look it's going to increment it by
105:42 - one because we unmounted it and
105:44 - remounted it so if we do that a bunch of
105:46 - times
105:49 - and now we look back and see okay our
105:51 - mount count is now 16 and our maximum
105:54 - count that is allowed is 10. well why
105:57 - didn't it automatically scan well it
105:59 - only does that on boot so if we were to
106:01 - reboot the system it would go through
106:03 - the flowchart and it would say okay you
106:06 - have your pass set to two so that means
106:08 - i need to check the drive and say okay
106:10 - drive is your mount count higher than
106:13 - the maximum amount count allowed before
106:16 - a scan and it will be so if we do a
106:18 - reboot
106:21 - and once it's booted back up we look and
106:24 - run that tune 2fs again so sudo
106:27 - tune2fs-l
106:30 - dev sdb1
106:32 - now we're going to see that the mount
106:33 - count is down to 1 which means that it
106:36 - scanned it it ran fisk on boot before it
106:38 - was mounted and it reset the mount count
106:41 - to zero and then of course it mounted it
106:42 - so now it's one so now it's not going to
106:45 - rescan that on boot until the mount
106:48 - count gets above this maximum mount
106:51 - count and then it will do it
106:52 - automatically and figuring out what
106:54 - number to set here can be kind of tricky
106:56 - because here's the deal if you're on a
106:58 - laptop you might reboot fairly often so
107:01 - you're going to want to have that number
107:02 - kind of high so it doesn't scan every
107:04 - single time but if it's a big file
107:06 - server maybe you rarely ever reboot it
107:09 - maybe once a year well in that case you
107:12 - might want the mount count to be very
107:13 - low so that every time it gets rebooted
107:16 - it does scan for consistency's sake and
107:19 - then one more thing if it's a huge
107:22 - partition with just millions and
107:23 - millions of files it's going to take
107:25 - hours or days to scan so maybe you don't
107:28 - want to scan it every time it mounts it
107:30 - really depends on your situation so plan
107:32 - out how often you want it to scan and
107:34 - the type of situation that you're in to
107:36 - make the best choice for your particular
107:38 - partition and honestly the best choice
107:41 - for you might actually be just to run
107:43 - fisk manually just unmount the partition
107:46 - or boot from a cd or usb drive if you
107:48 - need to scan the root partition maybe
107:50 - you never want to automate the process
107:52 - you only want to do it manually for some
107:54 - cases that's fine too regardless it's
107:56 - important to know how the system works
107:59 - so that if you think you're
108:00 - automatically scanning you really are
108:03 - lvm or logical volume manager is
108:06 - basically like a software version of a
108:10 - storage area network it allows you to
108:13 - take a whole bunch of physical devices
108:16 - and lump them into one big group that
108:19 - allows you to kind of carve out slices
108:21 - of storage for the use in your local
108:23 - system it consists of a bunch of parts
108:25 - physical volumes volume groups and
108:27 - logical volumes they use the word
108:29 - volumes a lot in there
108:31 - but nonetheless it's a way of taking raw
108:33 - storage and combining it into a thing
108:36 - that you can just slice up and expand
108:38 - and contract and add more things to it
108:41 - without disrupting the existing services
108:43 - here's how it actually works or looks in
108:46 - practice so we have we start with
108:47 - physical volumes i'm just calling them
108:49 - pv and we're going to say that we have
108:52 - four 10 gigabyte drives now in the
108:54 - standard practice these could be actual
108:57 - hard drives they could be raid devices
108:59 - or they could be partitions on a hard
109:02 - drive it doesn't really matter what they
109:03 - are they're just chunks of storage and
109:06 - you create physical volumes out of them
109:08 - and then you combine those physical
109:10 - volumes into volume groups and so then
109:15 - all of these are combined into a volume
109:17 - group and if you add 10 times 4 you get
109:20 - 40 gigs so then you have this volume
109:22 - group which is like just a big bucket of
109:24 - storage and that bucket of storage has
109:27 - no protection now i know that's not like
109:29 - a feature right but i really want you to
109:31 - know that if you just have 10 gigabyte
109:33 - drives all in a bucket so you have 40
109:36 - gigabytes of storage and you're just
109:38 - gonna take and carve out chunks of that
109:40 - that doesn't offer you any protection so
109:42 - if one of these drives fails
109:45 - all of a sudden you could have your end
109:47 - result be completely corrupt and useless
109:49 - so physical volumes being raw devices
109:52 - and not having any redundancy is a
109:54 - little bit scary so anyway just wanted
109:56 - to throw that in there once you have
109:57 - this volume group you carve out a slice
110:01 - of it and it can be a small slice like
110:04 - here i just said this is about seven
110:05 - gigs i just kind of like spatially
110:07 - guessed how much of 40 gigs that would
110:08 - be um but you can carve out a big chunk
110:11 - so you could have like a 30 gig slice or
110:14 - you could have like a 32 gig slice that
110:16 - would use like two plus a little of
110:18 - another drive basically you don't know
110:20 - underneath what's going on the volume
110:22 - group is just a big chunk of storage
110:25 - that you carve out a slice of and then
110:27 - this slice is called a logical volume
110:30 - that logical volume is what you format
110:33 - with a file system and mount on your
110:36 - local hard drive or on your local file
110:37 - system and what you install linux on so
110:40 - it's a long step process but it allows
110:42 - for a lot of flexibility and so i want
110:44 - to show you what it looks like in
110:46 - practice even though it's not this
110:48 - robust system with multiple physical
110:50 - volumes if you have centos installed
110:53 - they actually use lvm even if you only
110:55 - have one drive so i'm going to show you
110:58 - here if we look at the etc fs tab
111:02 - file what we have here is our device is
111:05 - called dev mapper centos dash root okay
111:09 - now this is like devmapper what is that
111:11 - well this is where logical volume
111:13 - manager creates those logical volumes
111:16 - for us to use and you'll see here this
111:18 - devmapper sent to us root is mounted on
111:21 - the root of the hard drive or root of
111:23 - the file system it has xfs as a file
111:26 - system and it's installed same thing
111:27 - down here they have another carved out
111:29 - slice called centos swap and that's
111:32 - mounted as swap space on the system so
111:35 - if we look inside dev mapper
111:38 - we're going to see we have sent to us
111:40 - root and sent to us swap now i want to
111:42 - show you really quickly a handful of
111:44 - commands so you can see what's going on
111:46 - if we look at pv
111:48 - display this is physical volume display
111:51 - it's going to show us what's going on
111:53 - behind the scenes right like what makes
111:55 - up the bucket of data that we're going
111:56 - to use we only have one physical volume
111:59 - i know it seems weird right why am i
112:01 - making a bucket out of one device well
112:03 - you can expand it if you want so they're
112:05 - giving you the room to expand later if
112:08 - you want to do it after it's already
112:09 - installed so we have one physical volume
112:12 - and it's a partition it's dev sda2
112:15 - all right it's in a volume group named
112:19 - centos
112:20 - and that gives us it looks like about 19
112:24 - gigabytes of storage okay so this
112:27 - physical volume is inside a volume group
112:29 - called centos so our our bucket with all
112:32 - the data is called sent to us and it
112:33 - looks like we carved out root
112:35 - and swap
112:37 - from that bucket so let's look at that
112:38 - really quick we're just going to look at
112:40 - lv
112:41 - display logical volume display and we
112:44 - should see two and sure enough we have
112:45 - two logical volumes i'm going to scroll
112:47 - up a little bit here our first one
112:50 - is named swap it's in the centos volume
112:53 - group because that's the only volume
112:55 - group we have it happens to be two
112:57 - gigabytes in size and it lives in dev
112:59 - centos
113:00 - swap this is an interesting thing you
113:02 - can you can use dev mapper and the name
113:04 - of it or you could use
113:06 - dev
113:07 - the name of the volume group
113:10 - the name of the logical volume as like
113:13 - in folders here so that's another way
113:15 - that you can reference it and find the
113:16 - actual logical volumes and then the same
113:18 - thing down here dev sent to us root
113:21 - the logical volume name is root it's
113:23 - carved out of the centos volume group
113:25 - and this one looks like it's about 17
113:27 - gigabytes so if we look inside
113:30 - dev sent to us we're going to see sure
113:32 - enough there we have root and swap as
113:35 - our two different logical volumes that
113:37 - we could one is formatted right this is
113:39 - formatted with xfs and this is just swap
113:41 - space i know we didn't go through the
113:43 - process of actually creating all of the
113:45 - different parts but hopefully you
113:46 - understand exactly what lvm is doing
113:49 - taking physical volumes whether
113:51 - partitions or hard drives or raid
113:52 - devices combining them into a volume
113:55 - group and then carving out logical
113:57 - volumes that you can use as regular
114:00 - devices on your system and it just
114:02 - allows for flexibility kind of like a
114:04 - software-based san in your own computer
114:08 - building an lvm is actually one of the
114:10 - most straightforward things that you can
114:12 - do when it comes to block storage
114:14 - devices on linux it's surprisingly
114:16 - consistent all the way through the
114:18 - process for you know the names of the
114:20 - tools and it really is kind of fun and
114:22 - once it's built you can expand it by you
114:24 - know adding more drives to the system or
114:27 - you know stretching your existing
114:28 - volumes but let's go ahead and actually
114:30 - create from start to finish an lvm
114:32 - system on our linux device now here i am
114:35 - on ubuntu and if we do lsblk we're gonna
114:38 - see that well we have a bunch of stuff
114:39 - but down here this is the drive that our
114:41 - system is installed on so we're not
114:43 - gonna touch this one we're going to use
114:44 - these four
114:46 - devices so sdbcd and e which are 10
114:49 - gigabyte devices notice i don't have
114:51 - partitions created on these now you can
114:53 - create partitions some people prefer to
114:55 - use partitions for their physical
114:58 - volumes in an lvm some people prefer to
115:01 - use the raw devices either one works
115:03 - fine they work the same
115:04 - the advantage of setting up a partition
115:06 - is that if somebody else comes to your
115:08 - system they're gonna see that there's
115:10 - partitions on the system and they're
115:12 - gonna know that something is already
115:14 - done there whereas if you leave them raw
115:16 - devices they might think oh look empty
115:18 - drives now it's kind of far-fetched and
115:20 - you're not just going to like start
115:21 - formatting drives in somebody's system
115:23 - but that's the reason some people like
115:24 - to use partitions i'm just going to use
115:26 - the raw devices and to turn these raw
115:28 - devices into physical volumes we use pv
115:32 - create and we just make a list of the
115:35 - devices we want to use so dev sdb devs
115:38 - dc dev sdd and dev sde so it created
115:41 - them now we can do pv display if we want
115:44 - and it's going to show all the devices
115:46 - that we have now each of them is 10
115:47 - gigabytes and the name of it is just
115:50 - the device itself dev sdc so the next
115:52 - step is to create a volume group now to
115:56 - do that we just do vg create which is
115:59 - very very nice
116:00 - the first flag or the first command
116:02 - argument here is the name of the volume
116:05 - group so i'm going to call this
116:07 - bucket because it's just a big bucket of
116:09 - our hard drives right of our data that's
116:11 - available so i'm going to call the
116:12 - volume group bucket and then we just
116:14 - make a list of the physical volumes that
116:17 - we want to add to it so those same
116:19 - volumes we just did before
116:21 - and then press enter our volume group
116:22 - bucket was created we can look at that
116:24 - by doing vg display there's four
116:27 - metadata areas meaning we have four
116:29 - devices our current active pv are four
116:32 - we have 40 gigabytes of storage just
116:34 - about all together because each one was
116:36 - 10
116:37 - and now we have this thing called bucket
116:39 - that we can carve a slice out of if we
116:42 - want and use that slice and put a file
116:45 - system on it so to do that we're going
116:46 - to use i'm sure you guessed it lv create
116:49 - i love the consistency of these tools
116:51 - it's really nice here we do dash capital
116:53 - l and how big of a slice we want in this
116:56 - case let's do something that's going to
116:58 - span all four disks so i'm going to say
117:00 - 32 capital g for 32 gigabytes and then
117:04 - dash n the name that i want to call it
117:07 - i'm going to call this big underscore
117:10 - slice and then we have to tell it where
117:13 - we want to get the data from well in our
117:15 - case it's in bucket the name of our
117:17 - volume group right there is bucket so
117:19 - what we're doing is creating a 32
117:22 - gigabyte slice called big slice out of
117:25 - the volume group bucket
117:27 - press enter logical volume big slice is
117:29 - created so we can do lv display
117:33 - and sure enough lv size is 32 gigabytes
117:35 - so we know if that worked the name is
117:37 - big slice it's in bucket the lv path is
117:40 - dev
117:41 - bucket big slice which is exactly what
117:43 - we would expect because we've created
117:45 - this volume group called bucket and now
117:47 - this logical volume lives inside there
117:49 - so the last step is that we would
117:51 - actually use this as an actual block
117:54 - device or as a hard drive in and of
117:56 - itself and then we could just do
117:57 - something like mkfs
117:59 - ext4 dev bucket big slice
118:04 - and there we go so now we've done that
118:05 - we could mount it somewhere and every
118:07 - time the system starts it's going to be
118:09 - available in dev bucket big slice and
118:11 - then if we put it in fstab it's going to
118:13 - be mounted on boot and then there are
118:15 - other tools that we can go and like lv
118:17 - extend if we wanted to make it bigger so
118:19 - we could do something like lv extend
118:22 - dash l i'm going to say plus 5 gigs dev
118:26 - bucket big slice and now it says the
118:29 - size has gone from 32 gigabytes to 37
118:31 - gigabytes it was resized and we did that
118:34 - without adding anything to the system we
118:36 - just used the tools to change the size
118:39 - of our logical volume the thing to
118:42 - remember again about lvm is that it
118:44 - provides flexibility in your system it
118:46 - doesn't provide any redundancy so if you
118:48 - have one physical volume fail it's going
118:51 - to crash the entire volume group and
118:53 - logical volumes are going to get messed
118:55 - up so you want those pvs to be like a
118:57 - raid device if you're worried about
118:58 - something going wrong underneath and
119:00 - losing data but setting up lvm as you
119:02 - can tell is very simple and honestly
119:05 - even kind of fun raid is a redundant
119:08 - array of independent disks or drives and
119:11 - basically what it means is that it you
119:13 - take a bunch of drives and put them
119:15 - together and you end up with a larger
119:18 - pool of storage now the cool thing about
119:19 - raid is that it doesn't just pull things
119:22 - together like lvm and give you a bunch
119:24 - of data it allows you to do some pretty
119:26 - neat things with performance or
119:28 - redundancy so i want to talk about the
119:29 - different raid levels that we can offer
119:32 - using raid specifically linux raid linux
119:35 - has a software version of raid which is
119:37 - very powerful very robust and
119:39 - surprisingly efficient so basically i
119:42 - want to make sure we cover all the
119:43 - different raid types and to do that i
119:45 - want to show you this big scary dragon
119:47 - now here's the idea i'm going to i'm
119:49 - going to talk about
119:50 - raid but instead of a redundant array of
119:53 - independent drives we're going to say
119:54 - they're a redundant array of independent
119:57 - knights of the round table okay and in
119:59 - true 80s fashion our knights are going
120:01 - to be just squares like from atari's
120:03 - adventure anyway here's the deal with
120:05 - raid 0 our drives are set up in a stripe
120:09 - which means that they work together
120:11 - reads and writes happen across two
120:13 - drives very very quickly and so with
120:16 - raid 0 or a striped array it's very fast
120:20 - right you can get dual writes and dual
120:22 - reads at the same time the problem is
120:25 - let's say that the knights are attacking
120:27 - the dragon and the dragon is attacking
120:29 - the knights if we lose one of the
120:31 - knights well then the dragon can get
120:33 - right through to the king because if you
120:36 - lose a single drive in a striped array
120:40 - or in a raid 0 all of a sudden your data
120:42 - is gone because half of your data is
120:44 - written on that other drive so while
120:47 - it's very fast they better kill the
120:49 - dragon quick because if one of them
120:50 - fails all of their date is gone and the
120:53 - king gets destroyed now raid 1 is kind
120:56 - of the opposite it still has multiple
120:58 - drives but they're set up in a mirror
121:00 - which means every time you write
121:02 - something to the first drive you write
121:04 - it to the second drive so you have a
121:06 - complete copy of both now because
121:08 - they're in a line like this they can
121:10 - only attack the dragon one at a time
121:12 - right they can't both attack the dragon
121:14 - like over here in the striped setup but
121:16 - the advantage here is if one of them
121:18 - dies there's still another full night
121:21 - there protecting the king same thing
121:23 - with your data right if one of these
121:24 - dies you still have a full set of your
121:27 - data because you've been writing it to
121:29 - two drives the entire time so a mirror
121:32 - doesn't give you any speed increase
121:34 - because you're not spreading the writes
121:36 - across here you're actually writing all
121:38 - of your data two times once to each
121:40 - drive so you don't get any advantage
121:42 - over a single drive when it comes to
121:43 - speed but you do get that advantage of
121:46 - either drive can die and you still have
121:49 - full protection for your data now raid 5
121:52 - this is a little bit different raid 5
121:54 - uses a parity disk and how that actually
121:57 - works is like some digital magic that is
122:00 - beyond the scope of this nugget and to
122:02 - be quite honest it still stumps me a
122:04 - little bit but conceptually how it works
122:06 - is you have multiple drives three
122:08 - minimum and any of these drives can die
122:12 - any one of them it doesn't matter which
122:14 - one and all of your data is still in
122:16 - place so you get the advantage of being
122:19 - able to lose any drive in your array the
122:22 - disadvantage is you lose one drive's
122:25 - worth of storage now what do i mean by
122:28 - that let's say these are all five
122:29 - gigabyte drives okay if these are all
122:32 - five gigabyte drives together that would
122:34 - be 15 gigabytes but you lose one drive's
122:38 - worth of storage for that parody magic
122:41 - and so if you have three five gigabyte
122:43 - drives you're only going to have 10
122:45 - gigabytes of usable space but the
122:48 - advantage is it's writing to multiple
122:50 - drives as it's going along and if one of
122:53 - the drives dies you still have all of
122:55 - your data represented in the remaining
122:58 - drives now if you lose two of course
123:00 - then your date is corrupt and and you're
123:01 - done but you can lose any drive and it's
123:03 - not just like this is the drive you can
123:05 - lose no you can lose any one of these
123:07 - and all of your data is still there it
123:10 - really is magical and so that means that
123:12 - you can lose a knight and the dragon is
123:14 - still going to be stopped by whatever
123:16 - two of the knights are still protecting
123:18 - the king hopefully that makes sense with
123:20 - the different raid levels and just
123:21 - briefly i want to talk about there are
123:23 - some hybrid levels as well so raid 0
123:26 - could be raid 0 1
123:28 - where you have like four drives and what
123:31 - happens here is you have a stripe of
123:33 - mirrors your drives are mirrored and
123:35 - then striped across the mirror or raid
123:37 - 1-0 which is a mirror of stripes or vice
123:40 - versa but basically four drives and
123:42 - you're mirroring two two of them and
123:44 - then striping those two mirrors and vice
123:47 - versa you're gonna stripe them and then
123:49 - mirror those two stripes with raid five
123:51 - there's actually a raid six which is
123:54 - cool but it requires another drive and
123:56 - then you can lose up to two drives and
123:59 - still have your data there's two of
124:01 - those parity drives in place now the
124:03 - downside is you lose two drives worth of
124:06 - storage on your full array but it's
124:08 - awesome because you can lose more than
124:09 - one drive now even if you don't follow
124:11 - along with my awesome dragons and
124:13 - knights kind of scenario hopefully the
124:15 - raid levels make sense now my trick my
124:18 - little mental trick to remember what it
124:20 - is i look at the number after raid and i
124:23 - say how many disks can i lose because
124:25 - for years i would confuse raid 0 and
124:27 - raid 1. but here's the deal with raid 0
124:29 - you can lose zero drives right because
124:32 - if you lose one you lose your data with
124:34 - raid 1 you can lose one drive and you
124:36 - still have another drive raid five i
124:38 - guess you could lose one of five i don't
124:41 - know it kind of falls apart but rate
124:42 - zero and raid one are the ones that i
124:43 - would always struggle with so i think of
124:45 - that number as how many drives i could
124:47 - lose and still have my data
124:50 - you can go to the store and buy a raid
124:52 - card like a hardware raid device and
124:54 - then you can use that on your system and
124:56 - you'll be able to have hardware raid but
124:58 - linux has a really awesome and powerful
125:01 - software raid
125:02 - program that will use kernel level tools
125:04 - to allow you to create your own raid
125:06 - devices without needing any specialty
125:09 - hardware at all now there's a couple
125:10 - things we need to discuss like
125:12 - partitions versus using raw devices i
125:14 - want to make sure we cover all the
125:15 - configuration stuff but conceptually
125:18 - it's really easy rather than having a
125:20 - hardware based card we just use our
125:22 - regular sata controllers and then our
125:24 - hard drives can all work together in a
125:27 - raid array that we choose now when i
125:29 - talk about partitions versus raw devices
125:31 - let's say we have 200 gigabyte hard
125:33 - drives but they're from different
125:34 - manufacturers now they both say that
125:37 - they're 100 gigabytes however if you
125:39 - look close they might have slightly
125:41 - different number of sectors and slightly
125:43 - different size so this one says it's 100
125:45 - gigabytes but it might be actually 1028
125:48 - megabytes and this one says 100
125:49 - gigabytes but it might actually be 1022
125:51 - megabytes now they round for marketing
125:54 - purposes and that's perfectly fine and
125:56 - usually on a system it doesn't matter
125:58 - but if you have like this drive fail in
126:01 - a raid array and you need to replace it
126:03 - with another drive and you try to use
126:05 - this drive and all of a sudden oh you
126:07 - created a raw disk device raid array
126:10 - unit and it has
126:11 - 1028 megabytes of space and you try to
126:14 - replace it with another hundred gigabit
126:15 - drive but this one only has a thousand
126:17 - and twenty two megabytes you're not
126:19 - going to be able to work it because this
126:22 - isn't big enough so generally what
126:24 - people do is you take and make a
126:26 - partition
126:27 - inside of your drive that is slightly
126:30 - smaller than the hard drive itself so it
126:33 - might be
126:34 - 99.9 gigabytes and then the same thing
126:37 - when you have a new drive you're going
126:39 - to have enough room to create a 99.9
126:42 - gigabyte partition and so even though
126:45 - this drive itself is slightly different
126:47 - the underlying partition is going to be
126:49 - the exact same size so you're going to
126:51 - be able to use it to replace a failed
126:53 - drive in an array so that's why we
126:55 - generally use partitions even though
126:57 - using a raw device would work until you
127:00 - need to replace it with a smaller drive
127:02 - that is your replacement now here on our
127:04 - ubuntu system if we do lsb lk we're
127:06 - going to see we have four 10 gigabyte
127:09 - drives that are installed in here we're
127:10 - going to make a right array with those
127:11 - this sda1
127:13 - this is our root partition this is where
127:14 - our system is installed but these drives
127:17 - down here are the ones that we're going
127:18 - to use to create our raid array now i've
127:20 - already partitioned the first three
127:22 - you'll notice it's a 10 gigabyte drive
127:25 - and the partitions are 9.9 gigabytes
127:27 - awesome we're going to do the sde the
127:29 - last one together and so fdisk dev sd e
127:34 - and the first thing well we can press m
127:36 - to see all of our different options but
127:37 - i'm going to go kind of quickly i'm
127:38 - going to say o to create a new partition
127:40 - type and it's going to be a dos one it
127:42 - doesn't have to be dos but i just
127:43 - decided dos and then i'm going to say n
127:45 - for a new partition and i want the
127:47 - partition to be primary so default
127:50 - number one default the start point on
127:52 - the drive default and here's where
127:54 - rather if i choose default it's going to
127:55 - be 10 gigabytes in size and that would
127:57 - be fine as long as our replacement was
127:59 - the exact same kind but i want to do it
128:01 - slightly smaller than the drive itself
128:03 - so here i'm going to say plus
128:05 - 9.9
128:07 - g and press enter and now we have it 9.9
128:10 - gigabytes in size so any 10 gigabyte
128:12 - drive we'll be able to replace it with
128:14 - because we'll just create a 9.9 gigabyte
128:15 - partition inside now the one last thing
128:18 - if we do t for type and press enter it's
128:21 - going to say okay what partition type do
128:24 - you want here it says it created a new
128:25 - partition with type of linux but if we
128:27 - type capital l we're going to see all of
128:29 - the available codes here now this is not
128:32 - a format this is just like a flag to
128:34 - give the kernel a hint as to what sort
128:37 - of partition this is supposed to be so
128:39 - the one that we want to put on here is
128:41 - actually f d
128:43 - for linux auto raid so i'm going to type
128:45 - f d
128:47 - and now it says change type of partition
128:49 - to linux auto raid or linux right auto
128:51 - detect i'm going to press w to write
128:53 - this change to the disk and now if we do
128:55 - lsblk we're going to see we have all of
128:58 - them here now that partition type is
129:00 - just to give the kernel a hint if you
129:01 - put these drives in a new system it's
129:03 - going to say oh look at those partitions
129:05 - those are part of a linux array or of a
129:08 - raid array so we're going to treat it as
129:10 - such so it's just a hint but it works
129:12 - even if you don't change that partition
129:13 - type all right so now it's pretty easy
129:15 - to create the actual raid device we're
129:18 - going to create a raid 5 device with
129:20 - four 10 gigabyte devices well about 10
129:22 - gigabytes and so we should end up with
129:24 - about a 30 gigabyte usable space with
129:27 - our raid 5 array now the tool we use is
129:30 - mdadm and we're going to say dash dash
129:33 - create because we want to create a brand
129:35 - new one i'm going to say verbose just so
129:37 - we can see it do things as we type it in
129:40 - and now what device do i want to create
129:42 - well the devices are dev
129:44 - md and then the number of the raid
129:46 - device you're creating so we're going to
129:48 - start with md0 because that's our first
129:50 - device and we don't have any raid
129:52 - devices on here yet i'm going to do dash
129:54 - dash
129:55 - level equals
129:57 - 5 i want it to be a raid 5 device and
130:00 - then dash dash
130:01 - raid
130:02 - devices
130:04 - equals 4 because we have 4 devices and
130:07 - now we need to list those devices out
130:09 - and we're going to list the actual
130:10 - partition so
130:11 - dev
130:12 - sdb1
130:14 - dev sdc1
130:16 - dev sdd1 and dev sd e1 we'll press enter
130:22 - boom it created it that quickly now we
130:23 - can see the details of it if we were to
130:26 - look at
130:27 - md stat this is the virtual file system
130:29 - proc and this is going to show us the md
130:32 - stat which is the current rate arrays in
130:34 - our system so here we have
130:36 - it's currently a raid 5 array it shows
130:38 - our devices here lots of information it
130:40 - says recovery that's because it's
130:42 - building the array but we can use it
130:43 - while it's currently using the array
130:45 - which is really awesome okay so we look
130:48 - into
130:49 - dev
130:50 - grip for md we're going to see there we
130:52 - have md0 so we have a device all created
130:55 - and we can now use this as a hard drive
130:57 - in our system before we do that though i
130:59 - want to save this configuration of this
131:01 - raid5 array into our system so that on
131:04 - boot it knows exactly what sort of array
131:07 - to build to do that we just do md adm
131:10 - detail
131:11 - scan and if we do that it's going to
131:13 - show us the configuration so that's the
131:15 - configuration for our current array what
131:17 - we want to do is save that so i'm going
131:18 - to redirect it into etc
131:21 - mdadm
131:23 - md
131:23 - mdadm.conf so we're going to save that
131:26 - result into this file
131:28 - and now every time we boot the system
131:29 - md0 is going to be created and then we
131:31 - just treat it like any other hard drive
131:34 - on our system so
131:35 - nkfs
131:39 - dev md0 and boom it created it and now
131:42 - it's part of our system and it's going
131:44 - to be about 30 gigs in size let's see
131:46 - lsblk and we look down here md0 shows up
131:50 - as 29.6 gigabytes about 30 gigabytes of
131:54 - raid 5 storage on our system if you're
131:57 - thinking that was a little bit too easy
131:58 - well you're right linux raid is awesome
132:01 - it's super simple to set up it allows
132:03 - you to use the regular drives in your
132:05 - system and set them up as a raid device
132:08 - so as long as you can save that file
132:10 - into mdadm.com
132:12 - that detail scan that we did and you can
132:14 - check for the progress or the status of
132:17 - your current rate array in mdstat you
132:19 - are really set that's all it takes to
132:21 - use raid on a system using nothing but
132:23 - software provided with the linux kernel
132:25 - installing tarballs sounds like some
132:27 - sort of prank you might pull on somebody
132:28 - in high school but really this is the
132:30 - way that we would install software on
132:32 - linux for years before package
132:34 - management systems came out now you can
132:36 - still download tarball files of source
132:38 - code for programs and install them
132:40 - although it's not terribly common
132:42 - anymore now we used to refer to this as
132:44 - the three step we would extract compile
132:46 - and install and this is the process to
132:48 - convert source code into an executable
132:50 - program now i'll show you how to do that
132:52 - really quick i've downloaded already a
132:54 - very simple program as a tarball file
132:57 - it's called sun weight it's just a
132:58 - simple program that waits until the sun
133:00 - goes down and then executes so if you
133:02 - have a script that you want to run at
133:03 - sunset that's kind of a cool tool to use
133:05 - to do it but the first step is to
133:07 - extract it so we're going to say tar
133:08 - minus the xvf
133:10 - sun wait we'll go into the folder that's
133:13 - created if we type ls we'll see there's
133:15 - a couple files in here this is the
133:16 - source code and also a make file now
133:20 - sometimes there's more complicated
133:21 - things like dependencies sometimes
133:23 - there's going to be a config file so
133:24 - we'll run
133:25 - config and go through that process and
133:27 - it'll tell you if you need other
133:28 - dependencies since we don't have a
133:30 - config file i'm just going to type make
133:32 - because make will compile it into an
133:35 - executable program now you'll notice we
133:36 - got some warnings but we didn't get any
133:39 - errors a lot of times if it's a big
133:40 - program it'll say oh i need this
133:42 - dependency or oh you forgot this library
133:45 - and you'll have to download and install
133:46 - those dependencies before you can
133:48 - compile it but this one is very very
133:50 - simple if we type ls again we're going
133:51 - to see now we have a result here we have
133:54 - the sun weight program now we can
133:56 - execute it right here by saying dot
133:58 - forward slash sun weight and we'll see
134:00 - sure enough there it runs and we could
134:01 - use this to wait until sunset to execute
134:03 - a program if we want to install it
134:05 - though we have to either copy it to our
134:08 - user local bin folder or sometimes
134:10 - they'll include in the make file an
134:12 - installer so we could say something like
134:15 - make install this one doesn't actually
134:17 - have that ability to install it it's a
134:19 - very simple program so if we wanted to
134:21 - install it if you get an error like this
134:23 - you just simply say okay i'm gonna move
134:26 - sun weight to user local bin and now if
134:29 - we type sunway it's going to execute
134:31 - because it's in our path now i told you
134:33 - that three-step process is really really
134:35 - simple and it is you just basically type
134:37 - make and it compiles and then you have a
134:39 - binary that you can install either using
134:40 - a script or just putting it in your user
134:42 - local bin file there is a big
134:44 - disadvantage though if you compile
134:45 - things from source and that is there are
134:48 - no update mechanisms for getting a newer
134:50 - version if you use a package manager
134:52 - it'll update old software but if you
134:55 - just compile it yourself and install it
134:56 - manually there's no way to update it and
134:59 - that can be a real problem especially
135:00 - when security concerns come up so while
135:02 - it's important to understand how to use
135:04 - tarballs to compile and install programs
135:07 - it's not really the best way to go about
135:09 - it if you have any other options
135:12 - deb files are the way that programs are
135:14 - packaged up in the debian and ubuntu
135:16 - world now there's a couple different
135:18 - ways that we can manage the subsystem
135:20 - and there's a little bit of confusion as
135:22 - far as what tool to use now behind the
135:23 - scenes they all use d package which is
135:26 - like the the lowest level of interaction
135:28 - with dev files and i'll show you why
135:30 - this is not what you use on a regular
135:31 - basis in order to actually install
135:34 - packages on debian or ubuntu there are
135:36 - three different options though for
135:38 - installing packages using the proper
135:40 - system apt apt-get and aptitude now i
135:43 - want to talk about them because they do
135:45 - the same thing but it's just a matter of
135:48 - being replaced by something better so
135:50 - apt just apt all on its own is the
135:53 - newest program to interact with the app
135:56 - system it's new it's simple this is the
135:59 - one to use so just use apps i'm going to
136:01 - try to use apt if i can
136:03 - aptitude you may find online
136:05 - instructions people telling you how to
136:06 - install packages with aptitude this is
136:09 - older but it still works i don't
136:10 - recommend you use it though it's a
136:12 - little bit strange when it comes to
136:13 - dependencies but it's been outmoded and
136:15 - now apt is the way to go now here's the
136:18 - other thing apt get has been around for
136:20 - a very long time it's the oldest of the
136:22 - three it still works but i don't
136:24 - recommend you use it because again apt
136:27 - is by far the best way that you can go
136:29 - about installing packages here's the
136:31 - real problem though i've been installing
136:33 - packages on debian and ubuntu for so
136:35 - long that sometimes if you're watching
136:37 - me in a nugget i might use apt-get by
136:40 - mistake just because it's a habit it
136:42 - still works there's nothing wrong with
136:44 - doing it but the proper way to go about
136:46 - installing packages is to use apt that's
136:48 - what i'm going to try to do and that's
136:49 - what i'm going to show you now now on
136:51 - our ubuntu system if we type ls we're
136:53 - going to see i have a deb file so it's
136:55 - kate kate is a text editor that works in
136:58 - the kde environment and this is the
137:00 - installer the dev file now remember i
137:02 - said that d package is the program that
137:04 - is used behind the scenes that's how you
137:06 - interact with dev files here's the
137:08 - problem so i'm going to say sudo d
137:10 - package minus i for install kate.deb
137:14 - it's going to try to do it but here's
137:16 - the problem it doesn't resolve any
137:18 - dependencies so if i want to use d
137:20 - package to install it i'm going to have
137:23 - to find every one of these deb files on
137:25 - the internet download them install them
137:27 - one by one find out if they have
137:29 - dependencies and it can be a real mess
137:32 - so thankfully the apt system takes care
137:34 - of all the dependencies for us so i have
137:36 - to erase this so i'm going to say pseudo
137:39 - d package minus r kate and it's going to
137:42 - undo the mess that i made and now we're
137:44 - back to square one rather than
137:46 - downloading the deb file we can use the
137:48 - app package management system and just
137:50 - say sudo
137:51 - apt
137:52 - install
137:53 - kate and it's going to look for the
137:55 - latest version and get all of the
137:57 - dependencies and you can see there are a
138:00 - ton it would have taken me a week to
138:01 - come up with all these dependencies so
138:03 - if i say yes it's going to install them
138:05 - all
138:08 - and it's finally finished that took like
138:10 - over three minutes i sped it up so i
138:12 - didn't have to sit here with me the
138:13 - whole time but now all we have to do is
138:15 - run kate because it's installed with all
138:16 - of its dependencies on the system and
138:18 - here it is our little text editor kate
138:20 - now another nice thing about using a
138:22 - package management system is we can keep
138:24 - things updated so we could say sudo apt
138:27 - update which is going to download the
138:29 - latest repository information to find
138:31 - out all the updates that are out there
138:32 - for us to install and then once we have
138:34 - the freshest versions of what's out
138:36 - there we can use that same program apt
138:39 - and say sudo apt upgrade and looks like
138:42 - we have a couple things we could upgrade
138:43 - hit enter and it's going to keep our
138:45 - system up to date that easy not worrying
138:47 - about dependencies it does all that on
138:49 - its own so while all three of these will
138:51 - technically work for installing and
138:53 - updating packages you really want to use
138:55 - just the simplest one which is apt it's
138:57 - the newest and it's the easiest to
138:59 - remember
139:01 - rpm is the red hat package manager and
139:04 - it does just that it manages packages on
139:07 - a red hat based meaning red hat or sent
139:09 - to us or anything else that uses the rpm
139:11 - system that's how it manages their
139:13 - dependencies and their installation and
139:14 - their programs and updates etc now
139:16 - there's a few tools we need to know how
139:18 - to use in order to really utilize rpm
139:20 - yum is kind of the de facto standard but
139:22 - there's a new kit on the block called
139:24 - dnf i want to talk about that and then
139:26 - of course rpm itself is not only a
139:28 - package management system but it's also
139:30 - the tool the low-level tool that we use
139:32 - to actually handle packages now the cool
139:35 - thing about rpm is there's no two steps
139:37 - required when it comes to installing now
139:40 - what do i mean by the two-step process
139:42 - well here we have our multiple programs
139:44 - i just mentioned now yum is yellow dog
139:47 - updater modified which may seem silly
139:50 - but just for a brief second yellowdog
139:52 - was a linux distribution that ran on
139:55 - powerpc or old apple hardware and its
139:58 - claim to fame was actually that it
140:00 - started this yum program for managing
140:02 - packages now the operating system itself
140:05 - didn't do well after the power pc
140:07 - platform kind of faded out but yum is
140:09 - still around today and in fact that's
140:11 - the program that we use on almost every
140:13 - rpm distribution it handles dependencies
140:15 - um it is it updates the repo information
140:18 - as it's installing and upgrading so
140:20 - unlike app you don't have to say like
140:21 - yum update and then yum upgrade when you
140:24 - upgrade yum updates before it does
140:26 - anything else now dnf which and i'm not
140:29 - kidding here it stands for dandified yum
140:32 - but dnf is the new program it's in
140:35 - fedora right now it's not in centos but
140:37 - it's going to replace yum it's just a
140:39 - rewrite it has some features that are
140:41 - that it worked better but similarly
140:44 - structured and how it works and then of
140:46 - course rpm is what happens behind the
140:48 - scene there's a program called rpm and
140:50 - this is the low level tool but it
140:52 - doesn't handle dependencies so we
140:54 - generally don't use the rpm tool on its
140:57 - own let me show you what i mean i'm on
140:59 - centos here and we're going to use yum
141:01 - to install packages but first i want to
141:03 - show you rpm now if we look i have
141:05 - downloaded this program called kate this
141:06 - is just a text editor that works in the
141:09 - kde environment but if we were to
141:10 - install it with rpm we would have a
141:12 - problem if we say rpm dash i for install
141:15 - the name of the package it's going to
141:17 - say i can't do that because you have 110
141:19 - billion different dependencies that
141:21 - aren't installed so what you'd have to
141:23 - do is find every dependency every rpm
141:26 - install them one by one
141:28 - these probably have dependencies of
141:30 - their own that we'd have to track down
141:31 - but thankfully that's where the package
141:33 - management system yum comes into play so
141:35 - we could simply just say
141:37 - yum
141:38 - install kate and it's going to go to our
141:40 - repositories it's going to update the
141:43 - cached information like show us the
141:45 - latest information from those
141:46 - repositories and then it's going to
141:49 - search for kate it's going to find all
141:50 - of the dependencies right here say it's
141:53 - looking for all the dependencies and the
141:54 - dependencies of the dependencies and now
141:57 - it says would you like to install it
141:58 - along with the 77 dependent packages
142:01 - okay i'm going to say yes actually i'm
142:02 - going to say no if we install it then it
142:05 - would just download install all of those
142:06 - packages and life would be good and we
142:08 - would have kate installed but what i'm
142:11 - going to do is now show you that's how
142:12 - we would install a package but if we
142:14 - want to update the system all we need to
142:15 - do is type yum
142:17 - upgrade and it's going to query all of
142:19 - our repositories it's going to download
142:22 - all the package information dependency
142:24 - information that we need and now it's
142:26 - just telling us if we want to get our
142:27 - system updated we're going to have to
142:29 - download 116 packages install one new
142:32 - package and it's going to take 308
142:34 - megabytes of space do we want to do that
142:36 - i'm just going to say yes and it's going
142:38 - to go through the whole process of
142:39 - downloading all those rpms it's going to
142:42 - use the rpm tool in the background
142:44 - install each one of them one by one
142:46 - until they're all updated and all
142:48 - installed to the latest version so yum
142:51 - is very simple to use and knowing that
142:53 - it comes from you know a distribution
142:56 - that's no longer even a valid
142:57 - distribution i don't know that's just
142:59 - cool for me that you know yum is still
143:01 - around even though yellow dog hasn't
143:03 - been a distro for many years now i don't
143:05 - have a fedora system to show you dnf
143:07 - first hand but it's a very similar
143:09 - program to yum it's going to work very
143:11 - very similarly so if you find yourself
143:13 - on fedora just use it very similarly to
143:15 - how you'd use yum and you'd be fine one
143:17 - of the big things i want to point out
143:18 - though is that there's no two-step
143:19 - process like there is with the apt
143:22 - environment meaning you don't have to
143:24 - update your repositories before you
143:26 - install it you just run yum and it's
143:28 - going to update and upgrade all the
143:30 - information before it does any
143:32 - installing or upgrading
143:35 - one of the nicest things about the app
143:37 - package management system is that you
143:38 - can add repositories which are just
143:41 - different groups of software packages so
143:43 - if something's not in the standard
143:45 - ubuntu or debian system and somebody
143:47 - else has written something you can add a
143:49 - whole repository of new software and
143:52 - then the app system can use it just like
143:54 - any other package on your system it's
143:56 - pretty cool now we're going to look how
143:57 - to add something and there are a few
143:59 - gotchas that we need to look at but
144:01 - thankfully it's a fairly simple system
144:03 - when it comes to configuring new
144:05 - repositories so i'm here in the etc apt
144:08 - folder on an ubuntu system if we do an
144:10 - ls we're going to see there are two
144:11 - things i want to show you there's
144:12 - sources.list this is a folder and
144:15 - there's actually nothing inside there
144:17 - right now but we could create a new
144:19 - file.list in here and it would be read
144:22 - just like this
144:24 - systemdefaultsources.list so it doesn't
144:25 - matter where things go i'll just add
144:27 - something to the existing sources.list
144:29 - so we're going to look and you'll see
144:30 - there's already a bunch of repositories
144:32 - added each one of these lines is a
144:34 - repository containing software that the
144:36 - apt system can install now some of them
144:39 - are commented so we would just uncomment
144:41 - the existing ones like here this is a
144:42 - partner archive we could uncomment this
144:44 - and then this would be an active
144:45 - repository but if we want to add a third
144:48 - party repository we can like i said
144:50 - either add it at the end of this file or
144:52 - create a new file in that sources.list.d
144:54 - folder and call it like new program dot
144:57 - list but i'm just gonna add it in here
144:58 - and i'm going to add the opera browser
145:01 - repository in here let's say we wanted
145:03 - to add the opera browser it looks like
145:05 - this
145:06 - it's okay if every field here doesn't
145:08 - make perfect sense to you this is just
145:10 - the format it says what kind of a
145:11 - package it is where it's stored what
145:14 - folder in there and then what version of
145:16 - the software we want to actually add the
145:18 - stable and then it's non-free as far as
145:20 - like what what type of repository it is
145:22 - so this is the line right this is where
145:24 - the opera browser is stored on the opera
145:26 - website so if we save this we would be
145:28 - able to do apt update no dash
145:33 - apt space update and if you look it is
145:36 - updating them all but i do want to show
145:37 - you back at the top here it gave us a
145:40 - little warning slash error oh and it
145:42 - repeated the error down here so i'm
145:43 - going to show you here it says this
145:44 - repository is not signed so what that
145:47 - means is we don't have their key because
145:49 - we don't want there to be a man
145:50 - man-in-the-middle attack where somebody
145:52 - you know takes over opera.com and then
145:55 - starts sending us bogus packages so they
145:57 - do signing key signing in order to make
145:58 - sure that we get the right software so
146:00 - if we want to use the repository we have
146:02 - to add their gpg key to our system now
146:06 - it's not difficult to do we just do kind
146:08 - of a two step here we have to download
146:09 - the key itself so wget
146:12 - so we're going to download the file and
146:13 - then pipe that into
146:15 - apt key add all right so we'll do that
146:18 - it's going to download the key and it's
146:20 - going to add it to our system and it
146:21 - just says ok we can see if it's there by
146:24 - typing apt key list and it's going to
146:26 - list all of the keys in our system and
146:29 - if we look we should be able to find
146:31 - sure enough here's the opera one so
146:33 - there's the upper key that we installed
146:34 - and now if we do apt update
146:36 - notice there's no errors at all it
146:38 - updated and now we could just install it
146:39 - using our app system app install
146:42 - opera stable and it would install our
146:46 - needed packages you know it would
146:47 - resolve dependencies even if they're in
146:49 - other distributions and it would install
146:51 - it for us just fine i'm going to say no
146:53 - because i want to show you another way
146:54 - you can add a repository and this is
146:55 - kind of a cool thing that ubuntu added
146:57 - it's called a ppa or a personal package
147:00 - archive there's a particular text editor
147:03 - that i really like it's called adam atom
147:05 - and they have a ppa which is a
147:07 - repository and to add it you just say
147:10 - add apt repository ppa colon and then
147:14 - where it lives or what user it is web
147:19 - team and the name of the repository is
147:22 - atom
147:23 - press enter it's going to do a couple
147:25 - things it's going to add it to our
147:27 - sources.list but it's also going to
147:29 - download that key for us so all we have
147:31 - to do is say press enter and it's going
147:33 - to install the key it even does an apt
147:36 - update for us and then we would just do
147:38 - apt install adam and sure enough it
147:40 - would be able to do it from that package
147:42 - or from that repository that we just
147:44 - added it can be a bit overwhelming once
147:46 - you have to start adding gpg keys for
147:48 - the repositories that you put into your
147:49 - sources.list but ppas make it really
147:52 - really really simple they do everything
147:54 - for you i really like ppas i think it's
147:55 - been a wonderful addition to the way we
147:58 - handle apt packages
148:01 - repositories in an rpm system using yum
148:03 - are very similar to that of an app
148:05 - system adding them is maybe even easier
148:08 - and editing the config is about the same
148:10 - just with different files let me show
148:12 - you what i mean now the main
148:13 - configuration file in centos is going to
148:15 - be the yum.com file so let's look at
148:17 - that it's in etc and it's
148:20 - yum.com here's where we can do a couple
148:22 - things one we set the main configuration
148:25 - things like where the cache is stored
148:27 - and things like this one thing i want to
148:28 - point out in here is this gpg check now
148:31 - you can actually turn off yum's ability
148:34 - to verify using signed key pairs and
148:38 - this will just allow it to install any
148:40 - repo or install from any repo that you
148:42 - install in your system it's not a good
148:44 - idea to turn this off though because
148:45 - again this is a safety check to make
148:47 - sure you're not getting a man in the
148:48 - middle attack but the main section is at
148:50 - top you can put new repos in the bottom
148:53 - or like it says right here you can put
148:55 - them in etc
148:56 - yum.repose.d as individual files with
148:59 - the format file dot repo that's usually
149:02 - what's done but we could add them here
149:03 - let's look inside the
149:05 - yum.repos.d folder so let's get out of
149:07 - here go into etc
149:09 - yum.repost.d
149:12 - and here's where all of the repos that
149:14 - are currently installed on our system
149:16 - live so let's look at one really quick
149:19 - look at the base file here and we're
149:20 - gonna see how it is set up so there's
149:23 - the base configuration up here the name
149:25 - of it the release where the actual files
149:28 - are stored gpg check you can turn this
149:30 - on and off for individual repos as well
149:32 - so if you set up a repo and you don't
149:34 - have signed key pair you could turn it
149:36 - off for just one repo and then this
149:37 - tells us where the actual key file if
149:40 - it's turned on lives so we can put our
149:43 - put it in there manually so that yum
149:44 - knows where to look to find that key now
149:47 - normally these aren't added manually but
149:49 - you could type all of these in these are
149:51 - all individual repos that are defined in
149:53 - here in this bracketed section usually
149:55 - what you do though it's really elegant
149:57 - is let's get out of here you just
149:59 - install the package so one of the really
150:00 - popular packages is called apple which
150:02 - stands for extra packages for enterprise
150:05 - linux and to do that we just do yum
150:07 - install apple release
150:10 - i'm gonna say yes to install it and now
150:12 - if we look in here we're gonna see here
150:14 - we go the apple.repo has been added so
150:17 - now if we were to install packages apple
150:19 - would also be one of the places that we
150:21 - could pull packages from we can just cap
150:23 - this to look at it
150:25 - and we can see it's enabled the gpg
150:27 - check is on it tells us to use this key
150:30 - file for checking the signatures and
150:32 - that's all there is to installing a
150:34 - repository in yum and since yum
150:36 - automatically updates we don't have to
150:38 - like update the cache when we install a
150:40 - package it's going to automatically fill
150:41 - in the blanks for us yum is an
150:43 - incredibly awesome and powerful package
150:45 - manager and we can tell just by how easy
150:47 - it is to add a repo and edit that config
150:49 - either using the yum.com file or the
150:52 - individual files inside etc yum.repo.d
150:57 - apt and yum are certainly the most
150:59 - common package managers out there but
151:01 - there are a few less common package
151:03 - managers that you should still be aware
151:05 - of now arch linux uses a program called
151:08 - pacman for managing their packages and
151:11 - open souza uses zipper now i'm not
151:13 - saying that the mascot for zipper is a
151:15 - purple horse but i'm just saying maybe
151:17 - it should be nevertheless it's not too
151:20 - difficult to use them even if you're not
151:21 - familiar here i have two terminal
151:23 - windows to two different linux
151:25 - distributions the first one is arch
151:27 - linux which uses the pacman package
151:29 - manager so if we just do pacman minus h
151:32 - we're gonna see here are a list of the
151:34 - commands now it's not immediately clear
151:36 - how you go about installing a package
151:38 - unfortunately it's not just install it's
151:40 - actually capital s for sync we kind of
151:43 - want to sync the system into a state
151:46 - that we wanted so to install a package
151:47 - it isn't difficult once you know what to
151:49 - do just pac-man minus capital s and
151:52 - let's install vim my favorite text
151:53 - editor you have to actually spell pacman
151:56 - correctly pac-man
151:58 - minus capital s vim it'll say do you
152:00 - want to install it and say yes it's
152:02 - installed it and now if we type vim you
152:04 - can see sure enough there's vim my
152:06 - favorite text editor now over here in
152:08 - open souza it's a little bit different
152:10 - here we use a program called zipper
152:12 - zypper we're gonna do zipper minus h and
152:16 - it'll show us all of the help commands
152:18 - that are available this one it is pretty
152:20 - easy you just do install or you can
152:22 - shorten it to just in so we could say
152:24 - zipper
152:26 - in
152:27 - vim we'll install vim again press enter
152:29 - it's going to retrieve the repositories
152:31 - online just like yum or apt would do and
152:33 - then it's going to install the vim
152:36 - package for us and we'll be able to use
152:37 - it on open souza
152:41 - it'll ask us so we want to continue i'm
152:42 - going to say yes
152:44 - and it installs all of the packages and
152:47 - the dependencies so now same thing here
152:48 - we get startup vim and sure enough
152:50 - there's vim this time on open souza
152:53 - pac-man and zipper are really two of the
152:55 - more popular alternative package
152:57 - managers but there are some others if
152:59 - you're on slack where you're gonna have
153:00 - to install things by hand using tar.gz
153:03 - files but these two along with apt and
153:05 - yum will get you through most systems
153:09 - managing local users on your server is
153:11 - really easy and there's a bunch of tools
153:13 - that make it even easier now there's the
153:15 - standard command line tools that allow
153:17 - you to add modify and delete accounts
153:19 - and there's this super cool script that
153:21 - i really like which makes adding a user
153:24 - very very easy there are a whole bunch
153:27 - of different facts about a user that is
153:29 - stored in the system though full name
153:31 - username password all of these things
153:33 - plus some others that aren't even listed
153:35 - are on there and it's important to know
153:37 - that the tools will manipulate all of
153:39 - these but you don't have to specify
153:41 - every single thing every time especially
153:43 - things like office number those aren't
153:45 - even really used anymore but there are
153:48 - possibilities that you can add specific
153:50 - information in the local group now i
153:52 - just want to go right to the command
153:53 - line so we can actually start adding
153:55 - users and at first we're going to use
153:57 - the tools that you know kind of come
153:59 - with it like the low level tools user
154:02 - add user dell and user mod so first of
154:04 - all i want to say
154:05 - user add because we don't have any extra
154:07 - users on our thing i'm going to do dash
154:09 - h and that'll give us the help screen
154:11 - now you can see there are tons of
154:13 - options for adding users but the format
154:16 - is pretty much the same user add
154:18 - whatever options you want to add and
154:20 - then the login or the username for the
154:22 - new user now we're going to use just a
154:24 - couple and i want to show you the
154:25 - problem with using user ad as opposed to
154:28 - that fancy script add user
154:30 - we have to determine like a home
154:32 - directory and a user shell and all sorts
154:35 - of things like that so let's say we
154:36 - wanted to make a user we're just going
154:38 - to say now notice i'm root we have to be
154:39 - root to make a system user but we would
154:41 - say user add and i want to do minus d
154:46 - home susie minus s for shell i'm going
154:49 - to use the bin
154:50 - bash shell and her username is going to
154:53 - be susie
154:55 - press enter it's all done now there is
154:56 - no password for susie we'd have to
154:58 - actually do that with the password
154:59 - command uh we'll actually say
155:01 - p-a-s-s-w-d
155:03 - suzy all right so now she has a new
155:05 - password and we can here let's start a
155:07 - new window
155:08 - ssh suzy at
155:10 - localhost susie
155:12 - and we're logged in as susie oh but see
155:16 - it says unable to change to directory
155:17 - home susie there's no such file or
155:19 - directory well dog on it we said that
155:21 - that's our home directory but by default
155:23 - it doesn't make that directory we
155:25 - actually have to make that directory or
155:27 - there's another command dash m which
155:29 - will create it as we're adding the user
155:31 - so it's possible again to use this tool
155:34 - and set all of those different flags up
155:36 - as we create the user if we go through
155:38 - all of these things but it's much easier
155:40 - to use the add user script and i'll show
155:42 - you what i mean
155:43 - add user frank and now it says okay i'm
155:46 - creating the frank user added a new
155:48 - group for frank created frank's home
155:50 - directory copying all the files from
155:52 - etsy scale and now it's asking me put to
155:54 - give frank a password so i'm going to do
155:56 - that now the full name of frank his room
155:59 - number which again we don't use room
156:01 - number work phone all those things
156:03 - anymore really but full name is nice to
156:05 - have in there and is this information
156:06 - correct i'm going to say yes
156:08 - and so now we have a really nicely set
156:11 - up user so if we were to open a new tab
156:13 - again and do ssh frank at localhost log
156:17 - in with frank i have to type frank's
156:19 - password correctly and now you'll see
156:21 - sure enough if we do pwd we're gonna see
156:23 - we're in frank's home directory he has a
156:25 - perfectly usable account because that
156:26 - add user script goes through and
156:28 - remembers all the various things that we
156:30 - need to do so let's get out of here now
156:33 - there is another tool we can use user
156:36 - dell so we can say user dell let's do
156:39 - minus h so we can see the options
156:40 - there's really only one option that's
156:42 - ever really important that is minus r
156:44 - which removes the user's files so we
156:46 - could say user dell
156:48 - [Music]
156:49 - r for remove frank and it removed all
156:52 - those files that said there wasn't any
156:54 - mail form so i couldn't remove that but
156:55 - now frank's home directory is gone now
156:58 - we could modify suzy's account if we use
157:01 - user mod so let's see user mod minus h
157:05 - and we're gonna see same thing we have a
157:06 - whole bunch of tools and this is just to
157:08 - modify an existing user so we could do
157:11 - something like change her shell like
157:13 - right now it's been bashed but we could
157:15 - change her shell so we could say user
157:17 - mod
157:18 - minus s let's say bin
157:21 - false susie
157:22 - what this is going to do it's going to
157:24 - change her shell so if she tries to log
157:26 - in it's going to fail so let's open up a
157:28 - new tab and try to log in as her ssh
157:32 - susie at localhost it logged in and then
157:35 - immediately logged out see it says it
157:37 - still doesn't have a user account we are
157:39 - a user directory we didn't make her a
157:40 - home directory but then it says
157:42 - connection to localhost closed that's
157:44 - because as soon as it logged in her
157:46 - shell is been false which immediately
157:48 - exits and then we're back to being
157:50 - logged out we're logged in as bob here
157:52 - so the ability to modify a user account
157:55 - is really important but if you're going
157:57 - to add a user i highly recommend you use
157:59 - that script you can certainly use the
158:02 - manual tools with all the flags to add
158:05 - that account but add user just makes it
158:06 - so easy by remembering all the steps and
158:09 - i'll be honest i often have a difficult
158:10 - time remembering is it add user or user
158:13 - ad add user user ad for my own sake i
158:16 - like to think okay alphabetically add
158:18 - user is first and that's the first tool
158:20 - i want to use for adding a user so
158:22 - that's just the trick i use regardless
158:24 - of what tools you use to add users it's
158:26 - important to understand that modifying
158:28 - them on the command line is fairly
158:30 - simple and not that difficult to learn
158:33 - local groups on a linux system are
158:35 - fairly straightforward to handle but
158:37 - it's important to understand the
158:38 - difference between primary groups and
158:39 - secondary or supplementary groups on
158:42 - each individual user now every user on
158:45 - the system is going to have their own
158:47 - personal primary group now usually
158:50 - that's going to be the same as their
158:52 - username so bob is going to have a
158:53 - username of bob and he's also going to
158:55 - have a primary group of bob and that's
158:57 - usually how it goes it could be a it
158:59 - could be a different group that says
159:01 - primary group but almost always that's
159:03 - how the primary group is going to be on
159:05 - a linux system and then there are all of
159:07 - the supplementary groups which happen to
159:09 - be like things that he belongs to like
159:11 - maybe he's in the admin group maybe he's
159:13 - in the sales group marketing third floor
159:16 - public all of these groups are in
159:18 - addition to his primary group and it
159:20 - will give him access to certain folders
159:22 - on the system that he might not have
159:24 - access to if he didn't have these
159:26 - different group memberships now in order
159:28 - to actually create groups the tools are
159:30 - very straightforward just like adding
159:32 - users with much fewer options so let's
159:35 - actually look at that and i want to show
159:36 - you how to manipulate users in groups on
159:39 - a system now here we are on an ubuntu
159:41 - system and we can simply say
159:43 - group add
159:45 - public it's going to add the group
159:46 - public to our system we could say
159:49 - group ad
159:50 - sales and it's going to do that now we
159:52 - could look at the different options
159:53 - group add minus h you see there aren't
159:56 - too many options we could specify a
159:58 - group id number if we wanted and we
160:00 - could do some things with group mod to
160:03 - change some of those features like the
160:05 - group id and then of course we could
160:06 - delete them with group dell and the same
160:09 - sort of thing you know we just delete
160:11 - them as we would use user dell on a
160:13 - system to get rid of a user so this is
160:16 - pretty straightforward but the part
160:17 - about primary and secondary can be
160:19 - confusing so right now if we type groups
160:22 - bob we're going to see that bob is in
160:24 - all of these groups the first one listed
160:26 - is his primary group so bob's primary
160:29 - group is bob and then these are all of
160:31 - his supplementary or secondary groups
160:34 - now i want to show you user mod minus h
160:37 - because it shows us how we can
160:39 - manipulate groups right here so
160:42 - lowercase g is how we force a user to a
160:45 - new primary group so we could change
160:47 - bob's primary group using the lowercase
160:50 - g flag on user mod in order to change
160:53 - his secondary or supplementary groups we
160:56 - use a capital g but here's where the
160:58 - gotcha comes into play if we do like
161:01 - minus capital g public it's going to
161:04 - delete all of his other supplementary
161:06 - groups and only make him part of the
161:07 - public group and that's not what you
161:09 - almost ever want so there's this nice
161:12 - dash a which means append the user to
161:16 - another supplementary group without
161:18 - getting rid of the additional groups
161:20 - that he already belongs to so let's
161:21 - actually do that in practice so we're
161:22 - going to say user mod minus a for append
161:26 - minus capital g
161:27 - public bob okay and now if we do groups
161:31 - bob we're going to say that bob still
161:32 - belongs to all those secondary groups
161:35 - and also to the public group which is
161:37 - exactly what we wanted to have happen
161:39 - because if we wouldn't have used that a
161:41 - this is what would have happened if we
161:42 - would have done user mod minus g let's
161:46 - do sales for bob this seems like it's
161:49 - going to do the right thing no errors
161:51 - but then if we do
161:52 - groups bob we're going to see uh he
161:54 - still has his primary group of bob and
161:56 - then just sales and so we'd have to
161:58 - manually go back through and add him to
162:01 - each individual group which is really a
162:03 - pain in the butt so you don't want to do
162:05 - that you want to always remember to use
162:06 - the dash a now in function the
162:08 - difference between a primary group and a
162:10 - secondary group the primary group is
162:12 - what is used if bob were to create a
162:15 - file so let's open up a new tab here we
162:18 - are bob
162:19 - say this is in our home folder if bob
162:21 - were to
162:22 - touch a file and we do ls minus l we're
162:24 - going to see the file that we just
162:26 - created is owned by bob and the group
162:29 - membership is bob's primary group of bob
162:31 - so that's really the difference between
162:33 - primary and secondary is when you have a
162:35 - primary group that's what the group
162:37 - membership of a new file you create is
162:39 - going to belong to now creating groups
162:41 - is really easy with the group ad group
162:43 - mod group dell tools so i didn't even go
162:45 - into that very much in depth the real
162:47 - important takeaway is the idea of
162:49 - primary versus secondary or
162:51 - supplementary and how to take individual
162:53 - users and put them into groups without
162:56 - deleting all of their other
162:57 - supplementary groups that they already
162:59 - belong to i hope this has been
163:01 - informative for you and i'd like to
163:02 - thank you for viewing figuring out what
163:04 - users are on your system and what
163:06 - accounts they're using and what they're
163:07 - doing is really an important part of
163:09 - forensics but also it's an important
163:11 - part of a sanity check like why is my
163:13 - computer running slow or what user am i
163:16 - logged in as and there is a handful of
163:18 - simple command line tools that we're
163:19 - going to go over that will just help you
163:21 - figure out the users on your system now
163:24 - the first scenario this happens a lot if
163:25 - you log into embedded systems like
163:28 - routers and stuff where you don't get a
163:29 - prompt that tells you who you are you
163:31 - just get like a hashtag here well what
163:33 - you can do is say who am i which seems a
163:35 - little bit silly but it will just give
163:37 - you the user that you're currently
163:38 - logged in as if you're starting to use
163:40 - sudo and su and you're sshing from one
163:43 - computer to another sometimes just
163:44 - figuring out what user you're logged in
163:46 - as is really important so that's it
163:49 - seems silly but it's something that i
163:50 - use in nuggets even you'll see me use
163:52 - that quite a bit another one if we just
163:54 - do who it'll show us who is logged into
163:57 - our current system it actually gives us
163:59 - quite a bit of information we have bob
164:01 - frank it looks like susie is logged in
164:03 - twice and it tells us when that person
164:05 - logged in and where they're logged in
164:07 - from so bob is our current user and i'm
164:10 - logged in on the display zero meaning
164:13 - i'm using an x windows session here and
164:15 - then frank and susie are both logged
164:17 - into localhost probably ssh and then
164:19 - suzy is logged in from a remote computer
164:21 - probably over ssh as well so who tells
164:24 - you that w which is like what although
164:27 - you don't have to type the rest of it
164:28 - it's just a w gives you the same
164:30 - information with a little bit more so
164:32 - these are the users where they're coming
164:33 - from when they logged in how long
164:36 - they've been idle and then what again
164:38 - that's where that what came from what
164:40 - they're actually doing so here i'm using
164:42 - a gdmx session uh looks like frank is
164:44 - sleeping on the job frank judging you
164:47 - buddy
164:48 - and then susie's just logged into a bash
164:51 - terminal so these are tools that give
164:53 - you information another one is pinky
164:56 - which seems like a silly tool but it
164:57 - replaces the older tool finger that has
165:00 - kind of been abandoned about 10 years
165:02 - ago but this gives you even more
165:04 - information but not the what's going on
165:06 - this gives you the login name full name
165:09 - when they logged in how long they've
165:10 - been idle where they're coming from and
165:12 - a combination of these really usually
165:14 - any one of them will give you the
165:15 - information that you want but these are
165:17 - all here to give you information on
165:19 - who's logged into the system now if you
165:21 - want further information about it you
165:22 - could also do id susie and it will give
165:25 - you information about susie's id
165:27 - including her user id her primary group
165:30 - id what groups she belongs to looks like
165:32 - she only belongs to the susie group so
165:34 - let's do an id on bob and yeah bob
165:37 - belongs to a whole bunch of other groups
165:38 - so it'll give you all of that and then
165:40 - lastly lastly there's the command last
165:44 - hahaha if you type last it gives you a
165:46 - history of the people that have been
165:48 - logged into the system recently so it
165:50 - looks like we go back all the way to
165:52 - march 7th here today's march 13th and i
165:55 - logged in a bunch of times just so we
165:56 - would get some results here and we can
165:58 - see who logged in where they logged in
166:00 - from and when they logged out or if
166:03 - they're still logged in so you can see i
166:05 - logged in at 1001 logged out at 1001 up
166:07 - here logged in at 1003 still logged in
166:10 - from the local host computer and that is
166:14 - susie so there's a lot of tools that you
166:16 - can use to figure out who's logged into
166:17 - your system how long they've been there
166:19 - where they're coming from what they're
166:20 - doing and these really might seem like
166:22 - throwaway commands like why would i need
166:24 - to figure out who's logged into the
166:25 - system but knowing the simple little
166:27 - tools like who and w and last can really
166:30 - be convenient when you're tracking down
166:32 - what's going on on your server who's
166:34 - logged in who was logged in did somebody
166:36 - log in bob said he logged in did he
166:38 - really log in well check out last and
166:40 - it'll tell you the point is there's a
166:42 - bunch of tools that are available
166:43 - that'll give you information on users on
166:45 - your system whether they're logged in
166:47 - not logged in or have been logged in in
166:49 - the past
166:51 - passwords and group memberships are
166:53 - things in linux that are stored in text
166:54 - files just like everything else but the
166:56 - problem is we don't want people to have
166:59 - access to our passwords and so there's
167:01 - kind of this elegant system of shadow
167:04 - files that has been invented this is
167:05 - fairly recent in the world of technology
167:08 - it used to be everything was stored in a
167:10 - single file called etc password but
167:12 - things have been changed now so that
167:14 - everybody doesn't have access to seeing
167:16 - the encrypted passwords so how it works
167:19 - now is this let's say this is our user
167:21 - bob now bob when he logs into a system
167:24 - has to be able to see what his home
167:26 - directory is so he needs to have access
167:28 - to a lot of information about his
167:31 - account however we don't want bob to
167:33 - have access to everybody's encrypted
167:35 - passwords so that's kept over in another
167:37 - file called a shadow file so what we
167:40 - have in the password file is we have
167:42 - bob's username and then literally just
167:44 - an x that takes the place of where the
167:46 - password used to be stored and if this x
167:48 - is here then the system knows okay i
167:50 - need to go over and look in the shadow
167:52 - file for the actual encrypted password
167:55 - of bob and then it does system
167:57 - authentication with root access as
167:59 - opposed to
168:01 - bob's access which is to read the
168:02 - password file but he doesn't have access
168:05 - to everybody's encrypted password the
168:07 - reason we don't want that is if he were
168:09 - to do like a brute force attack if he
168:12 - had access to everybody's encrypted
168:14 - password he could just keep hammering
168:16 - away at it until he finally figured out
168:18 - what the password was we don't want that
168:20 - so we don't want every user to have
168:22 - access we just want root or the system
168:25 - to be able to authenticate and check out
168:27 - you know the encrypted password file so
168:29 - anyway there's a neat and elegant system
168:31 - of how this works now notice obviously
168:33 - the etc password file is readable by all
168:36 - the etc shadow file is only accessible
168:39 - by the root user but when it comes to
168:41 - editing those there are special tools to
168:43 - make sure that we do it properly now
168:45 - first let's actually look at the files
168:47 - so if we do an ls minus all of etc pass
168:50 - sswd we're going to see that sure enough
168:52 - it's owned by root and everybody on the
168:55 - system can read it right only root can
168:57 - write to it but everybody can read it
168:59 - but if we look at the etc shadow file
169:02 - we're gonna see that only root can write
169:04 - to it and only people in the shadow
169:07 - group can read to it everybody else in
169:08 - the system can't even read it so our
169:10 - encrypted passwords are protected now
169:12 - there's also if we do ls minus l etc
169:15 - group we'll see the same scenario where
169:17 - this is the group definitions for users
169:20 - on the system and the same settings
169:22 - permission wise as etc password has and
169:25 - if we do ls minus l
169:27 - etc
169:28 - g shadow we're gonna see it's the exact
169:30 - same thing as the shadow file now groups
169:32 - don't normally have passwords associated
169:35 - with them but they can so the shadow
169:37 - system does the same with the group file
169:39 - as it does with the password file now if
169:41 - we wanted to edit one of these files we
169:43 - could do something like sudo vi et
169:46 - cetera pass
169:48 - wd and it's going to let us edit this
169:50 - file using our text editor but this is
169:52 - not the ideal way to go about it because
169:55 - we want to do it the shadowy way right
169:57 - we want to be able to edit it and then
169:59 - be sure that the shadow file matches so
170:02 - let's get out of here let's not make any
170:03 - changes to properly edit these files
170:05 - what you do is say sudo
170:08 - vipw this is part of the shadow package
170:11 - and it's going to ask us what editor we
170:13 - want to use if you want to use nano this
170:15 - is the easiest one like it says right
170:16 - here easiest you can use this i prefer
170:18 - to use vim so i'm just going to choose
170:20 - selection number two but number one is
170:22 - perfectly fine and it opens up and looks
170:24 - exactly the same and we make changes
170:26 - here and we could like go down here
170:29 - you'll notice all the things about bob
170:30 - are listed in this line right here's his
170:32 - username his password is just a
170:34 - placeholder here as an x because the
170:36 - actual encrypted password is in the
170:38 - shadow file but his user id is group id
170:40 - his full name his home directory his
170:43 - shell we could make any changes we
170:44 - wanted here and then we would just go
170:47 - and save the changes
170:49 - and this is what it would tell us it
170:50 - would say okay you've modified the etc
170:52 - password file you may want to modify the
170:54 - shadow file too and to do that you do
170:56 - vipw dash s for shadow so the same thing
170:59 - we would say
171:00 - sudo vi pw
171:02 - dash s
171:04 - and now this is the actual shadow file
171:06 - that we're editing and we can go through
171:08 - and if we wanted to make changes here
171:10 - this is bob's encrypted password now
171:12 - this is obviously not bob's plain text
171:15 - password his plain text password is just
171:16 - the word bob we know that but this is
171:19 - what it's like encrypted but since this
171:21 - is only accessible to root there's
171:23 - nobody who's going to be able to do a
171:24 - brute force attack to try to decrypt
171:26 - this
171:27 - because they don't have access to the
171:28 - passwords encrypted themselves so we'll
171:31 - get out of here and now the exact same
171:32 - thing with the group password or the
171:35 - group file in the group shadow file is
171:37 - done too we can say sudo vigr
171:40 - and this is going to edit the group file
171:42 - and sudo vi gr
171:45 - s and that's going to edit the g shadow
171:47 - file all right so that's the proper way
171:49 - to go about editing those files manually
171:51 - if you man if you edit these files it's
171:53 - going to do the same thing as if we did
171:55 - like user mod and changed somebody's
171:57 - home directory it just does it by
171:59 - editing the underlying configuration
172:01 - files so yes group and password files
172:03 - are still just text files but there is
172:05 - this elegant shadow system that allows
172:08 - us to make sure that the right people
172:10 - have access to the encrypted passwords
172:11 - and not everyone on the system can see
172:13 - everyone else's encrypted passwords
172:16 - quotas are the way that we make sure
172:18 - users or groups don't overuse the hard
172:22 - drive we don't want a particular user a
172:23 - particular group to use up too much of a
172:25 - hard drive and stop other people from
172:27 - saving files now there's soft quota
172:29 - limits and hard quota limits the
172:31 - difference is with a soft limit you're
172:33 - warned every day hey you've gone over
172:35 - your limit hey you've gone over your
172:36 - limit whereas if you reach the hard
172:38 - limit you're no longer able to save
172:41 - files at all now there's a couple things
172:42 - that we have to do to get our system
172:44 - ready for using quotas and keeping track
172:47 - of things but the first thing is we have
172:48 - to make sure that our partition is
172:50 - mounted correctly so on our system here
172:53 - i have a disk mounted or a 10 gigabyte
172:56 - drive mounted on
172:58 - mnt
172:59 - disk okay now if we look into our etc fs
173:02 - tab file i'm going to show you how you
173:05 - mount it so that quotas are enabled so
173:07 - here's our drive it's dev sdb1 and it's
173:09 - mounted on mnt disk it's ext4 i've used
173:13 - default mounting options and then i've
173:15 - added a comma and usr quota now we could
173:19 - also put grp quota we could put another
173:21 - comma and grp quota i'm just going to do
173:23 - user quotas group quotas work the same
173:25 - way so we'll learn one and we'll know
173:27 - how to use both but we have to make sure
173:29 - that it's mounted this way so if you're
173:31 - making this change you want going to
173:32 - want to reboot your system to make sure
173:33 - that it actually takes effect
173:36 - and then if we type mount we're going to
173:38 - see that sure enough mount disk or mnt
173:40 - disk is mounted with quotas enabled
173:43 - specifically user quota management okay
173:47 - so we know that the drive is able to
173:49 - support it but out of the box quotas are
173:52 - not turned on so what we need to do is
173:55 - first of all scan the existing drive for
173:58 - files owned by a particular user so we
174:00 - need to actually do sudo
174:02 - quota check
174:04 - dash a for all partitions that support
174:06 - quotas dash u for user owned files press
174:10 - enter it's going to go through it's
174:12 - going to check our drive and now if we
174:13 - look in mnt
174:15 - disk
174:16 - we're going to see sure enough now
174:18 - there's a quota file that has been
174:20 - created and it shows just you know all
174:22 - the files on the drive who owns them
174:23 - which is none right now but we're going
174:25 - to change that in a minute now the other
174:26 - thing is so that prepares the drive but
174:28 - we actually have to turn quotas on so
174:30 - we're going to say sudo quota on
174:33 - a for all supported partitions
174:35 - so now quota quota ing
174:38 - is actually turned on but if we want to
174:40 - set a particular quota for a user we're
174:42 - going to have to use ed quota so sudo
174:46 - ed quota and i want to do this for the
174:48 - user bob on our system so we're going to
174:50 - do it for
174:51 - bob and then we get this list now it's
174:53 - going to show us all the file systems
174:54 - that support quotas in our case that's
174:56 - just this one dev sdb1 now i have to
174:59 - explain really quickly there's two kinds
175:01 - of quotas we can do we can set up quotas
175:03 - for inodes or we can set up quotas for
175:06 - block usage now an inode means a file so
175:10 - we could say how many files a person can
175:13 - store on a particular partition but this
175:15 - isn't all that useful right i mean what
175:17 - if they have two files but those two
175:19 - files are like 27 gigabytes each so
175:22 - rather than set limits on inodes i tend
175:24 - to like to set them based on blocks now
175:27 - by default these are one kilobyte blocks
175:30 - so this zero means how many are
175:32 - currently in use and there's nothing on
175:33 - the drive owned by bob so that's set to
175:35 - zero right now
175:37 - but i'm going to make some changes here
175:38 - i'm going to say i want the soft limit
175:42 - to be 500 kilobytes and i want the hard
175:45 - limit to be 1000 kilobytes okay or like
175:48 - one megabyte this is not practical
175:50 - number you'd probably do something
175:51 - bigger in real life but we're going to
175:53 - save this say yes and now as bob if we
175:56 - go over to mount disk ls we're going to
175:59 - see that's in there now there's no usage
176:01 - currently but let's say i were to create
176:04 - a file now to do that i'm going to use
176:05 - dd it's okay if you don't use the dd
176:07 - command basically we're going to say an
176:09 - input file of dev zero output file of
176:14 - file one block size equals one kilobyte
176:18 - so that we can know exactly how many
176:19 - kilobytes we're using up and count
176:22 - equals let's say 400 so this should make
176:24 - a 400 kilobyte file if i press enter and
176:27 - do ls minus l we're going to see sure
176:29 - enough we have a 400 kilobyte file on
176:31 - here and this is fine this isn't meeting
176:33 - our quota at all we haven't done
176:35 - anything bad we're not even up to our
176:36 - soft quota but if we do this again i'm
176:38 - just going to push the up arrow and
176:40 - change this to file 2 and press enter
176:43 - okay it's done the same thing we do ls
176:45 - minus l now there's 800 kilobytes stored
176:48 - on this particular disk what this means
176:50 - is it hasn't stopped us from creating it
176:52 - but every day we're going to get an
176:54 - email from the system that says
176:55 - hey you've gone over your soft limit you
176:58 - really need to delete some files and
177:00 - we'll maybe do that maybe we won't do
177:01 - that but here's what happens if we try
177:03 - to create another 400 kilobyte file
177:06 - which will take us over the limit right
177:08 - because 400 plus 400 plus 400 would be
177:11 - 1200 but we only have a thousand
177:13 - kilobyte limit so what's gonna happen if
177:15 - we press enter it says that there's an
177:18 - error writing file three the disk quota
177:20 - is exceeded so let's do ls minus l and
177:22 - see what happened so it looks like it
177:23 - went along and it was creating fine
177:25 - creating fine it got to 200 kilobytes
177:27 - and all of a sudden it couldn't write
177:28 - anymore and that makes sense because 800
177:31 - or 400 plus 400 is 800 plus 200 it is a
177:34 - thousand so we hit our hard limit and
177:36 - that's exactly how quotas work on the
177:38 - system once quotas are turned on it's
177:40 - really a hands-off kind of thing they
177:42 - take care of themselves the emails go
177:44 - out automatically every day if the user
177:46 - goes over their soft quota and at the
177:48 - hard quota it stops them no matter what
177:50 - so quotas are easy to set up once you
177:53 - remember to use quota on to turn it on
177:55 - make sure that it's mounted with the
177:56 - proper options and run that initial
177:59 - quota check so that it knows what files
178:01 - are on there so it knows when it is or
178:04 - isn't getting close to the actual quota
178:06 - that you set
178:07 - user profiles are where initial settings
178:10 - are set for a particular user like if if
178:12 - they're going to set up like aliases or
178:14 - they need their path variables those are
178:16 - the sort of things that profiles will do
178:18 - and there are system-wide profiles and
178:20 - also individual profiles and it can be a
178:23 - little bit overwhelming because not
178:25 - every linux system is the same now there
178:27 - are some commonalities usually there's
178:29 - an etc environment file and that file
178:31 - will
178:32 - often but not always set up the path
178:35 - variables so that the users who log in
178:37 - get a particular
178:38 - path set up now there's almost always an
178:41 - etc profile in the system-wide file in a
178:44 - profile is something that is run on a
178:48 - login shell so like the very first time
178:50 - you log into a system like if you're
178:51 - logging into a gui
178:53 - that first time you log in you will
178:54 - execute the profile these are settings
178:56 - that only need to be executed one time
178:58 - like it doesn't matter if you're going
179:00 - to open a new terminal you only need to
179:02 - set these settings one time and then
179:05 - there's also going to be one of these
179:08 - not both of these it's usually either
179:09 - one or the other either the etc bash rc
179:12 - or etc bash dot bash rc uh this one is
179:16 - pretty common in ubuntu this one is
179:17 - pretty common in centos
179:19 - but nonetheless these are they serve the
179:21 - same function so you're going to have
179:22 - one or the other and these are profile
179:25 - settings that need to be set every time
179:27 - you open a shell so let's say you're
179:28 - already logged in to
179:30 - x windows like in a gui session you're
179:32 - in there and you click on an icon to
179:34 - open up a new terminal window well
179:36 - that's not considered a login shell this
179:38 - is just considered a sub shell of your
179:41 - main system login so you will not
179:43 - execute a profile you will only execute
179:46 - your bashrc now this is again systemwide
179:50 - so that happens to everybody when you
179:51 - log in you get the systemid profiles
179:53 - applied and then every individual has
179:56 - the possibility to have these individual
179:59 - files in their home folders now they all
180:00 - start with a dot so they're all hidden
180:03 - you have to do ls minus a if you want to
180:04 - see them
180:05 - but these are the same things up here
180:08 - it's just if you have any changes or
180:09 - additions you want to make to the
180:10 - system-wide settings you put them in
180:12 - your own personal folder and you put it
180:15 - in like bash rc and that'll run every
180:17 - time you open a sub shell everything in
180:19 - here will be set up and then the dot
180:21 - profile or bash underscore profile
180:23 - depending on which system you have
180:25 - you're going to have one or the other
180:26 - one of these but this is executed
180:28 - the very first time you log in just like
180:30 - the system-wide profile your personal
180:32 - profile only gets executed that initial
180:35 - time when you log into the system any
180:37 - sub shells will only apply the bashrc
180:40 - files but that's the way it works i'll
180:41 - show you really quick how it's set up on
180:44 - a system
180:45 - but the hierarchy is really the
180:46 - important thing to understand now this
180:49 - is bob's home folder i did an ls minus
180:51 - la so you can see all of the things in
180:53 - here he has a dot bash rc file and he
180:56 - also has a dot profile that means that
180:59 - these are going to be applied after the
181:01 - system-wide settings because the
181:03 - system-wide settings are given to
181:04 - everybody and then any personalizations
181:06 - like if you have an alias that you want
181:07 - to set on your own you would put it in
181:09 - your own personal bash rc file now
181:11 - inside etc there are those common files
181:15 - like ls minus l grep4
181:18 - profile we're going to see we have etc
181:21 - profile and also etc profile.d this is a
181:25 - folder if we go in there we're going to
181:27 - see there's a bunch of sh scripts all of
181:29 - these are included in the dot profile in
181:32 - fact i'll show you what i mean vi dot
181:34 - dot
181:35 - let's look at that system-wide profile
181:37 - file
181:38 - and if we look all the way down in the
181:40 - bottom here it's going to call in all of
181:43 - those files inside profile.d so when i
181:46 - say that the profile is executed by
181:48 - everybody not only etc profile but also
181:51 - etc profile.d everything in here is
181:55 - going to be executed as well to every
181:57 - user on the system now there's a couple
181:59 - important things to remember don't worry
182:01 - so much about what exactly is the name
182:04 - of the file when you look in the etc
182:05 - folder you're either going to find an
182:07 - etc bash rc file or an etc dot bash rc
182:11 - file don't worry too much about which
182:13 - system has which whichever one is there
182:15 - is the one that you need to use now as
182:17 - far as the hierarchy goes it's important
182:20 - to remember that the system-wide stuff
182:22 - is executed for everybody so everything
182:24 - in etc is executed for all users and
182:27 - then if you have changes or additions
182:29 - you put them in your personal dot
182:31 - profile or dot bash rc user profiles are
182:34 - pretty easy to track down and once you
182:36 - understand how the system-wide and
182:37 - individual settings work it's a snap to
182:40 - figure out which comes first
182:42 - if you're working on the command line a
182:44 - text editor is going to be an invaluable
182:46 - tool because pretty much everything in
182:47 - linux is text based now you should use
182:50 - nano nano is the editor you should use
182:53 - it's a wonderfully simplistic
182:55 - straightforward intuitive text editor
182:58 - that works just fine and then there's vi
183:01 - vi is clunky it's hard to use it doesn't
183:04 - make a whole lot of sense and it's the
183:07 - editor that i use almost exclusively now
183:10 - i know that doesn't make any sense
183:12 - but here's the deal vi has been around
183:14 - for a very very long time like since the
183:17 - beginning so even though it's difficult
183:19 - to use i've managed to learn to use it
183:21 - and it's just what my fingers do with
183:23 - muscle memory so i encourage you use
183:26 - nano unless you've been using vi long
183:29 - enough that it's the only option that
183:31 - seems to make sense now there is one
183:34 - scenario that you may want to learn at
183:36 - least the basics of vi
183:38 - sometimes you're going to come across a
183:39 - system that doesn't have nano installed
183:42 - most systems do but if you end up on a
183:44 - system that only has vi
183:47 - these couple commands are going to kind
183:49 - of save your bacon so here's the deal
183:52 - this is what makes vi so confusing there
183:54 - are two modes there's command mode which
183:57 - is what it starts in an insert mode
183:59 - which is what you use when you actually
184:01 - type text now the way that i can kind of
184:04 - describe this is if you're sitting down
184:07 - and typing you're going to be in insert
184:09 - mode because insert mode is where you
184:11 - insert text and delete text and use your
184:13 - arrow keys to you know go around and
184:15 - change text but then if you need to do
184:18 - some command
184:20 - like save or quit or anything like that
184:23 - you're going to go into command mode and
184:25 - the way i think about it is let's say
184:27 - you are on a standard word processor if
184:29 - you're typing
184:31 - you're in insert mode if all of a sudden
184:33 - you need to reach for the mouse and
184:35 - click on something
184:37 - you're going to be in command mode so
184:38 - while it's not a perfect analogy if you
184:40 - want to do something like save you're
184:42 - going to want to go into command mode so
184:44 - that you can save and quit now since
184:46 - there's no mouse it's all still text
184:49 - things that you're doing but think
184:51 - mentally okay i'm in the mode where i'd
184:53 - be using my mouse to save things instead
184:56 - of just typing out text now to go back
184:58 - and forth that can be confusing too so
185:01 - you start out when you open vi you're in
185:03 - command mode if you want to start typing
185:05 - something you press either i for insert
185:08 - or a for append meaning like do you want
185:10 - to insert right where the cursor is or
185:13 - to the right of where the cursor is but
185:14 - either one is going to work fine so
185:16 - either i or a and then if you want to
185:18 - get back
185:20 - into command mode you press escape so
185:23 - those are your magic keys to go back and
185:25 - forth right i or a i usually use i to go
185:28 - into your typing text mode or insert
185:30 - mode escape to go back so that back and
185:33 - forth that's how it works
185:35 - now the actual commands to save or quit
185:39 - or save and quit are right here and they
185:42 - may not make sense but if you're in
185:43 - command mode you're going to press colon
185:47 - and then type w
185:49 - and then q and press enter that's going
185:52 - to save your document and quit a lot of
185:54 - times people get stuck in vi and have no
185:57 - idea how to get out it can be so
185:59 - frustrating
186:00 - so this will get you out also if you
186:02 - want to quit without saving like you've
186:04 - accidentally made changes and you didn't
186:05 - mean to you press escape again to get
186:08 - into command mode and then you press
186:10 - colon q
186:12 - exclamation point and press enter and
186:14 - that will that will exit without saving
186:17 - and then if you just want to save
186:18 - halfway through a document you can just
186:20 - do colon w enter and it's going to save
186:22 - but you'll stay in then you'll press i
186:24 - to go back into insert mode and continue
186:26 - making edits i'll show you really
186:28 - quickly well i'll show you what nano is
186:30 - and then i'll show you vi just so you
186:32 - can see it in practice like i said
186:34 - almost every distribution is going to
186:36 - have both nano and vi so i'll show you
186:38 - nano first if we do an ls we'll see i
186:39 - have this text file dot
186:41 - so if we just type
186:43 - nanotextfile.txt it's going to open the
186:45 - editor with this and you can just use
186:47 - arrow keys and you can start editing
186:49 - right away this is like you would expect
186:51 - any text editor to work okay so press
186:53 - enter it's going to insert blank lines
186:55 - and then if you want to save you can
186:57 - look right down here we have control x
187:00 - to exit and you can do other things too
187:02 - there's all sorts of commands but i'm
187:03 - going to show you the basics here
187:05 - control x to exit and then it says would
187:08 - you like to save your changes and you
187:10 - can say see the options here y for yes
187:13 - and for no control c for cancel i'm
187:15 - going to say yes and then it says what
187:18 - file name would you like to write well
187:19 - it'll default to the current text file
187:21 - but if you wanted to save it as like
187:23 - copy 2 you could i'm just going to hit
187:24 - enter
187:25 - and boom we're done the text file has
187:27 - been edited it's very simple very easy
187:29 - to use and again i recommend you use it
187:31 - now vi i'm going to look at the same
187:33 - text file with vi so vi
187:35 - text file
187:37 - and here we have we're in what mode are
187:39 - we in we're in command mode now we can
187:41 - still use the arrow keys to get around
187:43 - but we can't edit any text or insert any
187:46 - text if we want to add text we press i
187:49 - and then look down here it says insert
187:51 - so this is a little cheat it tells you
187:52 - that you're in insert mode if we're
187:54 - going to go back into command mode press
187:56 - escape i'm going to go into insert mode
187:57 - press i escape to get out insert mode
188:00 - and once you're in insert mode you can
188:03 - type text and then if you want to save
188:05 - it again you have to press escape
188:07 - and then
188:08 - colon
188:10 - w
188:11 - q
188:12 - enter and then boom we've saved the file
188:14 - we can look and see the text file has
188:17 - been changed all this changes were saved
188:19 - and that's how you use vi it's confusing
188:22 - but that's how it works so again use
188:24 - nano it just makes sense it's easy
188:26 - that's what i recommend you use but if
188:28 - you have to use vi at least now you know
188:30 - the couple shortcuts that are going to
188:31 - get you through so that you can actually
188:33 - use it to edit text now remember i said
188:36 - i use vi all the time and it's true but
188:39 - the funny thing is that it's followed me
188:41 - into things like word processors so
188:42 - sometimes in my microsoft word documents
188:45 - even on the very bottom you'll see colon
188:47 - wq because my head i just automatically
188:50 - do that when i'm done editing text
188:52 - anyway use nano but vi is fun and it's a
188:55 - good skill to have i hope this has been
188:57 - informative for you and i'd like to
188:58 - thank you for viewing viewing text files
189:00 - is an extremely common thing for a
189:03 - system administrator to do on a linux
189:04 - system so we're going to look at a bunch
189:06 - of tools that allow us to examine text
189:09 - files in a way that allows us to view
189:11 - but also to search and i'm just going to
189:13 - go right to the command line so that we
189:14 - can see these things work in real time
189:17 - now i've created in my folder here a
189:19 - file called two cities now this is just
189:22 - the public domain tale of two cities
189:24 - this is the first chapter i'll just type
189:26 - cat so we can look at it
189:28 - two cities see it's just a tale of two
189:29 - cities the first chapter all right so
189:32 - let's clear the screen now the first
189:33 - thing i'm going to show you is the head
189:35 - command and what it does it'll show you
189:37 - the first lines of a text file so the
189:40 - head or the beginning of it so we can
189:42 - just say head two cities and it's going
189:45 - to show us the first 10 lines of the
189:47 - story so the best of times the worst
189:49 - times that's part that we're familiar
189:51 - with now we can change that how many
189:52 - lines it shows us if we were to do
189:54 - head
189:55 - dash n 20 it's going to show us 20 lines
189:59 - the first 20 lines of the file so see
190:02 - it's a little bit longer now and it's
190:04 - shown us all 20 lines now head isn't
190:06 - usually as commonly used as its
190:09 - companion which is tail so let me clear
190:12 - the screen oop and it spell clear right
190:15 - clear the screen now if we were to do
190:17 - tail
190:18 - two cities this shows us you've probably
190:20 - already guessed it the last ten lines
190:23 - and we could do the same thing with the
190:24 - dash n and a number we could decide how
190:26 - many lines of the file we want to see
190:28 - now this is really useful if you're
190:30 - looking at log files and that's almost
190:32 - exclusively where i use the tail command
190:34 - if i'm looking at a log file i just want
190:36 - to see the last things that were written
190:38 - to a log file so i'll do a tail of the
190:41 - log file in question and i'll see what
190:43 - was added to the very end so i don't
190:45 - have to look like 27 megabytes of text
190:47 - for all of the logs just the last little
190:50 - bit of it and so that's a very useful
190:52 - command and again you can use n20 if you
190:54 - want to see 20 or whatever number you
190:56 - want to see 10 is the default now the
190:58 - other ones i want to show you are less
191:00 - and more we'll start with more this is
191:01 - the older command so let me clear the
191:03 - screen if we were to type more
191:05 - two cities this is going to show us the
191:07 - entire file and if we want to scroll
191:10 - through it we press the space bar and
191:12 - it'll go to the next page space bar go
191:14 - to the next page the enter key will go
191:16 - line by line
191:17 - but this is that's it so we're all the
191:19 - way to the end of the into the first
191:20 - chapter but that's how more works you
191:23 - just kind of go through it like that you
191:24 - can also search but it only searches
191:27 - down and it's not one that i use very
191:29 - often anymore because it's been outmoded
191:32 - by the much more powerful
191:34 - although it has a more diminutive name
191:37 - less so actually let me clear the screen
191:39 - so if we were to say less
191:41 - two cities
191:43 - now we have what looks like a similar
191:45 - kind of interface but we can use our
191:47 - arrow keys to scroll up and down
191:50 - page up and page down work so we don't
191:52 - have to worry about like hitting the
191:54 - spacebar to go down a page we can
191:56 - spacebar will go down a page but then we
191:57 - can scroll back up with the up key now
192:00 - the other really nice thing about less
192:02 - and more does this to an extent but less
192:04 - is even more powerful if you type
192:06 - forward slash in a term so let's search
192:09 - for france press enter it's going to
192:11 - take us to the first entry of the word
192:14 - that we searched for it's going to
192:15 - highlight it and it's highlighted all of
192:18 - them so if we were to press forward
192:19 - slash and enter again it's going to
192:20 - repeat the same search and here we are
192:23 - francis found again france is down here
192:25 - again forward slash it'll take us to
192:27 - that one put it right to the top of the
192:28 - screen so we can search through an
192:30 - entire text file as well so it's very
192:33 - very powerful to use the command that
192:35 - seems like it would be less powerful
192:36 - because it's named less but really it's
192:39 - a lot better okay so to get out of here
192:41 - and this actually confused me for a long
192:43 - time if you just press q just the letter
192:45 - q it'll exit the less command and get
192:48 - you out of it so less more head tail
192:50 - they're very commonly used i usually use
192:53 - less and tail more commonly than the
192:55 - other two but that's just because i want
192:57 - to see the end of a log file and i want
192:59 - to be able to go up and down when i'm
193:01 - scrolling through a text file and search
193:03 - really powerfully it's not a really
193:05 - tough nugget because these are pretty
193:07 - straightforward tools they're all useful
193:09 - and you'll probably find yourself using
193:11 - them fairly frequently on the command
193:13 - line
193:14 - sometimes when i'm at the grocery store
193:16 - i wish i could search for where things
193:18 - are like i think pizza sauce should be
193:19 - right next to spaghetti sauce but it
193:21 - almost never is well thankfully when it
193:23 - comes to searching for text in a linux
193:26 - system there is an awesome tool that
193:27 - allows you to do just that narrow down
193:30 - what you're looking for with tool called
193:32 - grep now greb does use regular
193:35 - expressions or regex and if you're
193:37 - interested in the you know very
193:38 - fine-tuned filters you can get with
193:40 - regex i cover that really great in the
193:42 - linux foundations course but today i
193:44 - want to talk about searching for strings
193:46 - of text using grep because it can be a
193:49 - real powerful way to get the information
193:51 - you want really quickly now i said that
193:53 - grep uses regular expressions so if you
193:56 - want to make sure that it's absolutely
193:58 - searching just for strings of text you
194:00 - can use the dash capital f flag and that
194:02 - means just fixed strings
194:05 - usually you don't have to do that
194:06 - because if you just search for a string
194:09 - it's going to generally find it in the
194:11 - file but occasionally your string might
194:14 - be regular expression characters and you
194:16 - can cause yourself some headaches so if
194:18 - you want to be safe use dash capital f i
194:21 - usually don't because it's usually not
194:23 - an issue but i just want you to be aware
194:25 - that if you do grep minus capital f it's
194:27 - going to just search for strings now
194:30 - there's two different ways that we can
194:31 - use grep we can say graph
194:34 - the string
194:36 - from a file and it's going to search the
194:38 - file for the string that we specified
194:41 - and that works really well but there's
194:43 - also another way you can do it you can
194:45 - do this cat file or anything that has a
194:48 - text output like ls or anything that's
194:50 - going to output text and then you can
194:53 - use the pipe symbol and kind of push it
194:56 - through grep and search for a string
194:58 - i'll show you why this is a really
195:00 - powerful way to use grep because it
195:02 - seems a little backwards like why
195:04 - wouldn't we just say you know grep this
195:06 - string from this file i'm on an ubuntu
195:08 - system here and i'm just going to search
195:10 - a log file okay so i'm going to say grep
195:13 - now we can say dash capital f or we can
195:16 - leave that off i'm just searching for
195:17 - strings i just again want you to know
195:19 - that capital f is going to force it to
195:21 - just use strings but i want to grep for
195:24 - dhcp from the var log syslog file press
195:30 - enter and it's going to find all of the
195:32 - lines in that text file that have dhcp
195:34 - in it actually even highlights the dhcp
195:37 - which is really convenient now there's
195:38 - another place we could get some system
195:40 - log information and that is using the
195:42 - dmesg command
195:44 - but here's the problem that's a command
195:47 - that has output but it's not a file we
195:49 - can't grep the d message command so
195:52 - that's where the pipe symbol comes into
195:54 - play and it works really really well we
195:56 - could just say d message and instead of
195:58 - just having it print to the screen we
196:00 - can use the pipe symbol which is usually
196:02 - above the enter key in a us keyboard
196:04 - grep i'm going to use minus f this time
196:07 - we don't have to necessarily
196:09 - dhcp
196:10 - and then it's going to take all of that
196:12 - output from d message and grep for dhcp
196:15 - and sure enough there's two lines that
196:17 - have dhcp now another really useful way
196:20 - that we can use the pipe symbol let's
196:22 - clear the screen
196:23 - because i want to show you that first
196:24 - one that we did right we we actually
196:26 - grew up for dhcp from that file and we
196:29 - got these results let's say we had just
196:31 - pages and pages of results and we wanted
196:33 - just to look for things that mentioned
196:36 - init init what we could do is kind of
196:39 - like chain grips along we could say
196:42 - grep dhcp from var syslog and then pipe
196:46 - those results into
196:48 - grep init and press enter and now we're
196:51 - just going to get the lines that were in
196:54 - this result that also contain the word
196:57 - init and so here now we've filtered all
196:59 - the way down to these two lines of text
197:02 - from the log files so even if you're not
197:04 - getting super fancy with regular
197:06 - expressions you can do some really
197:08 - powerful searching of strings using the
197:10 - grep tool for things like log files or
197:13 - any kind of text that you want to search
197:14 - for and remember you can chain those
197:16 - grep commands together so that you get a
197:19 - really fine filter looking for exactly
197:21 - what you're looking for
197:23 - every application in the linux system
197:26 - has three sort of like pipes it has
197:29 - standard input standard output and
197:31 - standard error and basically it's just a
197:33 - way to get information in and out it's
197:35 - an io type situation for every
197:38 - individual app now i have just an
197:40 - application here
197:41 - drawn out and i want to show you the
197:43 - difference between the three so standard
197:45 - input is pretty easy to understand right
197:47 - this is like if you're putting something
197:49 - into a program we use the pipe symbol if
197:51 - we're going to pipe something into it or
197:53 - we can use less than if we want to just
197:54 - assign a file to the standard input now
197:57 - a lot of programs don't accept things on
198:00 - standard input but some of them do so if
198:02 - you've ever seen me pipe a command into
198:04 - another command what i'm doing is piping
198:06 - the results of one command into another
198:09 - command so it can work on it i'll show
198:11 - you how that works on the command line
198:12 - but then there's two other pipes and one
198:15 - of them is the standard output this is
198:16 - what happens if you type ls and it shows
198:18 - you know the contents on the screen
198:20 - that's the output that it shows you is
198:22 - the standard output now there's also
198:24 - standard error if an error occurs it
198:27 - also prints the things out on the screen
198:29 - but they're different pipes now we don't
198:31 - realize the difference because they both
198:33 - end up on the command line that's like
198:35 - the default place for standard output
198:37 - and standard error to go but you can
198:40 - treat them differently so if you want to
198:42 - redirect the output of a file of an
198:44 - application into a file use the greater
198:46 - than symbol if you want to redirect the
198:49 - standard error or like you know an error
198:51 - message you have to use two greater than
198:53 - because it's a different pipe and you
198:55 - have to redirect it separately so let me
198:57 - show you what i mean first of all let's
198:58 - talk about standard input now i have in
199:00 - here a file called file.txt i'll show
199:03 - you what's inside of it okay so this is
199:05 - what we have okay just a text file with
199:07 - some text in it now if we wanted to use
199:10 - grep to search for text we could just
199:12 - say
199:13 - grep text from
199:15 - file.text and it would show us the text
199:17 - that's in there but we could also
199:19 - redirect to the standard input rather
199:22 - than telling grep what to use so we
199:23 - could say
199:25 - cat
199:26 - file.txt
199:27 - and then pipe the results into standard
199:31 - input of grep and then have grep look
199:34 - for text we should get the exact same
199:36 - results now what we've done though
199:37 - rather than telling grep you know what
199:39 - file to choose from we just piped the
199:42 - results of cat into standard input and
199:44 - then grep use that as its input for
199:47 - grepping for the word text now that is
199:49 - using the pipe symbol we can also say
199:51 - grep for
199:53 - text and i want you to use file.txt as
199:57 - your standard input
199:59 - now this looks very similar to this up
200:01 - here but it's drastically different
200:03 - because what we've done is we've used
200:05 - redirection so this is redirecting
200:08 - standard input this actually functions
200:10 - exactly the same as this one because
200:13 - here we're using the pipe symbol to
200:15 - redirect standard input here we're using
200:17 - the less than symbol to redirect
200:18 - standard input so that's how you can do
200:20 - standard input it's not something you do
200:22 - as often apart from with this scenario i
200:25 - do this a lot you know piping one thing
200:27 - into another so that you can get the
200:29 - results from there now the other thing
200:31 - is standard output and standard error so
200:33 - i have a really quick way to show you so
200:35 - we say ls and we get these are the
200:37 - contents of ls we could redirect that by
200:40 - using greater than into a file called
200:43 - results.txt and we should get no output
200:46 - because rather than redirecting the
200:48 - output to our terminal window here it's
200:50 - actually redirected the standard output
200:52 - into results.txt so if we look there's a
200:54 - file now called results.txt and if we
200:57 - look at results.txt it has the contents
201:00 - of that ls command right it just dumped
201:02 - the contents into there here's a problem
201:04 - though what if we did this let me clear
201:06 - this screen what if we did ls lsf
201:10 - and we tried to redirect the standard
201:12 - output into
201:14 - results.txt
201:16 - why did we get the error message here
201:18 - and let's look at results well there's
201:20 - nothing in results now because there was
201:22 - no standard output this is an error
201:23 - there is no file called ff for us to use
201:26 - ls on if we wanted to redirect an error
201:29 - we would have to do
201:30 - lsf
201:32 - to greater than
201:34 - error.txt if we do that ah nothing
201:36 - appeared however if we look at in the
201:38 - file here now we have a file called
201:40 - error.txt
201:41 - and if we look in that sure enough that
201:43 - was the error message we redirected it
201:45 - using a standard error redirector using
201:48 - standard input standard output and
201:50 - standard error redirection is something
201:52 - you're going to find yourself doing a
201:53 - lot because you want to see the results
201:56 - of things when you're not there to see
201:57 - it happen on the command line that's
201:59 - basically what log files are right
202:01 - they've taken errors and redirected them
202:03 - into a log file for you
202:06 - once you understand how input and output
202:08 - works with an application on the command
202:10 - line there are some really cool tricks
202:12 - and tips that we can learn to make life
202:14 - a little bit easier now there's a
202:15 - handful of things we want to look at but
202:17 - i've drawn a diagram so we can actually
202:19 - get a real good taste for what each
202:21 - thing is now devnl you may have heard of
202:24 - people call it the black hole or the bit
202:26 - bucket and basically devnull is a
202:29 - location on your file system that you
202:31 - can copy anything to and it will
202:34 - disappear forever now that seems like a
202:36 - weird concept i know but if you have
202:39 - like
202:40 - extraneous logs that you don't really
202:42 - want to ever see you just want to like
202:44 - throw them immediately in the trash dev
202:46 - null is where you want it to go for
202:48 - example you would not want to copy
202:50 - important information and redirect it to
202:52 - dev null because it will just disappear
202:54 - forever so devnet is just a place that
202:57 - everything disappears when you copy it
202:59 - there now t is an interesting command it
203:01 - doesn't seem like it does very much but
203:03 - it accomplishes a task that's remarkably
203:06 - difficult to do without using the tool
203:08 - itself so here's how it works we take
203:10 - some sort of text like output from a
203:13 - file or something like that and we put
203:15 - it into teas standard input and then all
203:18 - it does is dump that same information
203:20 - out of its standard output but it also
203:24 - writes it to a file so it does like a t
203:27 - in the road or a fork in the road it
203:29 - lets you see what it is right on the
203:30 - standard output right in your command
203:32 - window but then it also copies it to a
203:34 - file so that's what t does and it's
203:37 - really nice if you want to see what's
203:38 - happening but you also want to have a
203:39 - record of it and keep it into a file
203:41 - and then arguably the most complicated
203:44 - one but also the niftiest one maybe is
203:47 - called x-args now how x-args work is
203:50 - let's say you have a different program
203:51 - like ls or something and you do the
203:54 - command and so you see stuff come out in
203:56 - standard output and then we're going to
203:58 - pipe it in or redirect it into x args
204:01 - standard input then what x args does is
204:04 - it takes it and says okay what program
204:07 - do you want me to use this information
204:09 - that you just piped into me on and you
204:11 - tell it like application number two and
204:13 - then it executes application two and it
204:16 - uses that information as the arguments
204:18 - for the second command now the reason
204:20 - this is really powerful is not all
204:22 - applications can accept things from
204:25 - another program on their standard input
204:27 - so x args basically takes and forces a
204:30 - program to accept something from
204:32 - standard input by accepting that
204:34 - standard input itself and then putting
204:36 - it as an argument onto the application
204:38 - now let's look at all of these really
204:40 - quick because the cool part is you know
204:42 - when you actually do it now first of all
204:43 - devnl so we're going to say
204:46 - echo
204:47 - hello and it'll put it to the screen if
204:49 - we do echo
204:50 - hello and then we redirect it to
204:53 - dev no
204:54 - boom it's completely gone right that's
204:56 - how devnet works it's just a place that
204:58 - never fills up it's kind of like a
205:00 - teenage girl at a pizza party sleepover
205:02 - right it just never gets full you can
205:03 - put as much as you want in there it's
205:05 - just gonna disappear now we've
205:06 - redirected standard output we could also
205:09 - redirect standard error
205:11 - and then we should see hello because
205:13 - there was no error there right if we did
205:15 - something like this let's say
205:18 - ls
205:19 - documents
205:20 - and ff we should get both standard
205:23 - output and standard error sure enough
205:25 - here's our standard error there is no ff
205:27 - but here is the contents of documents so
205:29 - we got both now if you wanted to do
205:31 - something cool and redirect both
205:33 - standard output and standard error into
205:36 - one place you can do this you can say ls
205:39 - documents ff just like we did and i'm
205:41 - going to redirect standard output into
205:44 - dev null and here's the magic part and
205:47 - then i'm going to redirect standard
205:49 - error into
205:51 - and
205:52 - one oh my goodness what is this well the
205:54 - ampersand one is a way that we can tell
205:56 - it that what we want is standard error
205:59 - to get redirected into standard output
206:03 - so the one is standard output so what
206:04 - this does is all of our standard output
206:06 - is getting redirected to dev null so all
206:09 - of our standard error is going to go
206:11 - into standard output which is of course
206:13 - going to dev null so this should give us
206:15 - absolutely no results and sure enough
206:18 - both standard output and standard error
206:19 - have gone into devnet all right so
206:21 - that's just a really cool thing that you
206:23 - can do with redirection and then devnet
206:25 - is just a place that never fills up all
206:27 - right let's clear the screen i want to
206:28 - show you so here i have a couple files
206:31 - or one file let's look and see what's in
206:33 - there
206:34 - all right just a bunch of different
206:36 - words in the text file so what we can do
206:38 - is say
206:39 - cat file.txt i'm going to pipe my
206:42 - standard output into t's standard input
206:46 - and then i'm gonna call it copy.txt and
206:49 - now we should see the contents of
206:50 - file.txt sure enough it printed to the
206:52 - screen but then the t command also
206:55 - created that copy.txt that also contains
206:58 - this so if we look sure enough there's a
207:00 - copy of our stuff and then last i'm
207:02 - going to show you x args and it's hard
207:05 - to come up with a real good example but
207:06 - i think i have one all right what we
207:08 - want to do is i want to create a folder
207:11 - named everything in this file so i want
207:13 - to have a folder named red named yellow
207:15 - named blue and name tuna fish well turns
207:16 - out to be kind of difficult to do that
207:18 - but we can do it really simply if we say
207:21 - catfile.txt
207:23 - pipe that into x-args
207:26 - and then x args is going to mkdir and
207:30 - then it will put the standard output of
207:32 - this command and kind of like plunk it
207:34 - right there so if we press enter
207:37 - and we do ls look at that we have a
207:39 - folder with each one of those names
207:41 - which is really convenient right it was
207:43 - able to do like what would have taken us
207:45 - quite a bit of typing to do it just did
207:47 - it by piping it and then it kind of
207:48 - pastes the results right at the end of
207:50 - whatever command you tell it now we
207:52 - could also do something cool if i want
207:53 - to clean up my mess instead of mkdir i'm
207:56 - going to say rm dir
207:58 - and then they're gone
207:59 - standard input standard output standard
208:01 - error they're really cool things and
208:03 - there are some additional tips and
208:04 - tricks that make using them even more
208:06 - usable and more beneficial on the
208:09 - command line
208:10 - dealing with text on the command line is
208:12 - something that's kind of fun to do to be
208:14 - quite honest and there are some tools
208:16 - that are pretty nifty to play with so
208:18 - let's look at a couple of them right now
208:20 - we'll just go right to the command line
208:21 - now i've already set us up with a few
208:23 - files i have file one and file two so
208:25 - let's look at them just so we know
208:26 - what's in them that's what's in file one
208:29 - and that's what's in file two just a
208:31 - list of words so i wanna show you a few
208:33 - things now you'll notice that these are
208:35 - not in alphabetical order okay so we
208:38 - could use the sort command so i could
208:40 - say sort file1 dot txt and it's going to
208:43 - return the contents but notice now
208:45 - they're in alphabetical order chicken
208:47 - fish monkey turtle so what we could do
208:49 - is if we wanted to save a file call it
208:51 - sorted then we could redirect the output
208:53 - to
208:54 - sorted.txt
208:56 - and then if we look at sorted.txt now
208:58 - it's going to be a file with them in
209:00 - alphabetical order so sort does just
209:02 - that and if you look at the commands for
209:04 - sort like to the man page for sort you
209:06 - can see that it does some other things
209:07 - you know you can actually sort with
209:09 - options like you know do i ignore case
209:11 - what about numbers what about you know
209:13 - what if it's a bunch of dates so sort's
209:15 - very powerful but that's basically what
209:16 - it does it takes a text file and then it
209:18 - outputs that text file sorted however
209:20 - you tell it to do so let's clear this
209:22 - screen all right the next one i want to
209:23 - show you is word count it's pretty
209:25 - simple it's just wc and it stands for
209:26 - word count so we could do word count of
209:29 - file1.txt and it'll give us three fields
209:32 - it says 4 4 and 27 in our case what this
209:35 - means is it means there are four words
209:37 - there are four lines and there are 27
209:40 - characters now if we just want to know
209:42 - one of those things we could just say wc
209:44 - minus m for character count
209:47 - and it's just going to show us that
209:49 - there are 27 characters in file1.txt now
209:52 - the last two i want to show you are
209:53 - really the most interesting so there's
209:55 - cut and paste which i know sounds like a
209:57 - gui thing but let's actually look at our
209:59 - file again just so we know exactly what
210:00 - we're dealing with so file1.txt
210:03 - this is what we have these lines if we
210:05 - were to use the cut command we could say
210:06 - cut i'm going to do cut by characters
210:09 - and i want it to cut out character oh
210:11 - let's just say 1 from
210:14 - dot file1.txt
210:16 - if we do that we should see just the
210:18 - first oh i did file two my goodness i'm
210:21 - like those aren't the first letters at
210:22 - all
210:23 - so let's look at file one that dxd that
210:26 - makes more sense so the c in chicken the
210:28 - effing fish teen turtle m in monkey okay
210:30 - so we have those uh we could do more
210:33 - than just one character and we could do
210:34 - more than just the first character let's
210:36 - say we wanted to do cut minus c
210:39 - the third fourth and fifth character in
210:42 - the file file one dot txt so now we
210:45 - should see
210:47 - [Music]
210:48 - and what it did is it took the third
210:50 - fourth and fifth character
210:52 - but you see fish only has four
210:54 - characters so there is no fifth
210:55 - character so it just did sh for the fish
210:57 - line all right but see that's that's
210:59 - what cut does it'll actually take it
211:00 - right out of the middle of the file
211:02 - which is surprisingly difficult to do if
211:04 - you don't actually use the cut command
211:06 - and paste does kind of the exact
211:08 - opposite so let me clear the screen
211:10 - because it's kind of full and i want to
211:11 - show again file one and file two now
211:15 - let's say we wanted to put this file
211:18 - file two
211:19 - after file one right we wanted to say
211:22 - chicken lips fish whiskers turtle
211:24 - feathers and monkey flippers well that's
211:27 - kind of difficult to do if we cat them
211:29 - together it's just going to put one at
211:30 - the end of the other so that's where
211:32 - paste comes into play we can say paste
211:34 - file1.txt
211:36 - file2.txt and it's going to output
211:38 - sure enough i put a tab between them and
211:40 - then we have chicken lips fish whiskers
211:42 - if we wanted to do this and redirect it
211:44 - into a file
211:46 - and then we look at join.txt look at
211:48 - that our file now has those two pasted
211:50 - together now i know that most of what we
211:52 - did using these commands was really just
211:54 - playing around but really playing around
211:56 - is one of the best ways to learn to use
211:58 - a tool and you're going to find that
212:00 - every once in a while one of these tools
212:02 - like cut or paste especially are going
212:04 - to be extremely useful because it's kind
212:06 - of hard to put things next to each other
212:08 - in a text file or cut out the middle
212:10 - bits of a text file without using simple
212:13 - tools like this
212:15 - awk and said are text manipulation tools
212:18 - that for some reason most people are
212:21 - afraid of and i honestly don't know why
212:24 - yes they can be very complicated you can
212:25 - do a lot of powerful things but you
212:27 - don't have to you can do some very
212:29 - simple yet still powerful things with
212:31 - awk and said now what do they stand for
212:34 - said just stands for stream editor and
212:37 - awk i actually had to google this
212:39 - because i had no idea i mean i've been
212:40 - using awk for decades but i didn't
212:42 - really know what it stood for it turns
212:44 - out it's the initials of the people who
212:46 - first wrote it and i'm not going to try
212:48 - to pronounce them all but that's what
212:50 - the a the w and the k mean basically awk
212:54 - is a data extraction tool it allows you
212:57 - to pull out certain bits of data from
212:59 - text and set of course is just a stream
213:01 - editor which allows you to edit things
213:03 - without interacting with it directly so
213:05 - let's actually go to the command line so
213:06 - we can see it work now i've already
213:08 - prepared a couple files so if we do ls
213:10 - we're gonna see we have file one file
213:11 - two and joined so let's just look at
213:13 - them this is file one this is file two
213:17 - and this is basically the two of them
213:18 - joined together i actually use the paste
213:20 - command to do that so i have these three
213:22 - files and i'm going to use said and awk
213:25 - to do things with these files okay so
213:27 - first of all said is a stream editor
213:30 - which means you can put files in and
213:32 - you'll it'll output the edited version
213:34 - and how it works i'm just going to cat
213:36 - one of these files so cat file1.txt
213:40 - and i'm going to pipe that into
213:42 - said for stream editor and here's where
213:44 - i'm going to set up the rules i'm going
213:45 - to substitute so i'm going to say s and
213:48 - then forward slash what i want to search
213:50 - for is the word monkey and a forward
213:53 - slash what i want to replace it with i'm
213:55 - going to say dolphin and then a forward
213:58 - slash ng for global that just means that
214:00 - if it occurs more than one time i want
214:02 - every occurrence of monkey to be
214:04 - substituted with dolphin and then i'm
214:06 - just going to press enter
214:08 - and what we get is chicken fish turtle
214:10 - dolphin because it took
214:12 - this initial file and it substituted
214:15 - monkey for dolphin and really that's
214:17 - what said does it's a stream editor it
214:20 - allows you to edit things as it flies
214:22 - through there so you can make changes to
214:24 - text as it's being manipulated so you
214:27 - can put this inside of a script and do
214:29 - things without interacting on a like a a
214:32 - gui like you'd want to open it up with
214:34 - vi and edit it out and change monkey to
214:36 - dolphin or anything like that so that's
214:38 - what stream editor or said does now i'm
214:40 - going to clear the screen
214:42 - and we're just going to look at joint
214:44 - i'm going to say cat join just so we can
214:46 - see it now awk takes a text file and it
214:49 - will allow you to pull out bits and do
214:52 - things with them so i'm just going to
214:53 - type this out and then we'll see what
214:54 - i'm talking about so i'm going to say
214:56 - awk
214:57 - and i'm going to put single quotes open
214:59 - curly braces print
215:01 - dollar sign 1 close curly braces close
215:04 - single things and i'm going to use
215:06 - joined.txt as the file so what this is
215:09 - saying now this is maybe why people are
215:11 - scared there's a bit of an odd syntax
215:13 - here but we say awk and then this is
215:15 - what we want awk to do with the file i
215:18 - want it to print out the first field now
215:22 - it will auto detect that these fields
215:24 - are separated by a tab they could just
215:26 - be spaces and it'll auto detect it but
215:28 - basically this is on line one this is
215:30 - field one field two line two it's field
215:32 - one field two so this should print out
215:34 - the first field of every line so let's
215:37 - press enter and sure enough chicken fish
215:39 - turtlemonkey it printed them all out now
215:42 - we can do more than just one thing at a
215:44 - time so let's go back over here what if
215:46 - we wanted to do dollar sign two
215:49 - dollar sign one now we should get a
215:51 - printout of whiskers fish and and
215:54 - flippers monkey and let's see what what
215:57 - turns out here now it is gonna be a
215:58 - little hard to read and i'll show you
215:59 - why see it did do that right here we had
216:02 - chicken lips and it gave us lips chicken
216:04 - whisker fish feathers turtle flippers
216:06 - monkey now the problem is it took those
216:08 - fields and just mushed them right
216:09 - together so we could add another thing
216:11 - in there we could kind of build this out
216:13 - longer and we could say i also want
216:16 - a space in there so hopefully that makes
216:18 - sense it's going to print field 2 and
216:20 - then it's going to print this space and
216:21 - then it's going to print field 1. so
216:23 - let's see if that's what we get
216:25 - sure enough lips chicken whisker fish
216:27 - feather turtle flippers monkey and it
216:28 - doesn't have to be used just one time
216:31 - right we could do this we could say i
216:32 - want dollar sign two dollar sign two
216:35 - which means field two field two and now
216:37 - we should get
216:38 - a duplicate of each one lip slips
216:40 - whiskers whiskers so awk just takes bits
216:43 - of data and allows you to manipulate
216:45 - them and do what you want with them
216:47 - honestly the only thing i could think is
216:49 - that the syntax for all can said
216:51 - intimidate people but once you get used
216:53 - to doing it and especially with stream
216:55 - editor the syntax is very similar to the
216:58 - vi editor when it comes to replacing and
217:00 - substituting things in a text file so
217:02 - hopefully you get used to them and
217:04 - you're not afraid of them because
217:05 - they're super powerful and awesome tools
217:07 - to use especially in scripts because
217:09 - there's no interaction required you can
217:12 - put them right inside of a script and
217:13 - they work without you entering more data
217:16 - hard links and soft links or symbolic
217:19 - links as a lot of people call them are
217:21 - very similar in what you get on the file
217:23 - system as far as usability goes but they
217:25 - work drastically differently so let me
217:28 - explain the difference between them so
217:29 - when we have a hard drive we basically
217:31 - have every file that takes up a certain
217:34 - number of sectors on the hard drive
217:36 - itself so this one let's say takes up
217:37 - three blocks this one takes up
217:39 - four blocks this one takes up seven
217:42 - blocks and these are the actual
217:45 - files on the hard drive but the file
217:47 - system actually only knows where those
217:49 - files live because of the file
217:52 - allocation table so this is kind of like
217:54 - a table of contents right it says okay
217:56 - file one is actually right here and it
218:01 - extends three blocks uh this one i'll
218:03 - say okay file two
218:05 - this actually lives right here on the
218:07 - hard drive and it extends four blocks
218:10 - and then the same thing here file three
218:12 - and you know it extends all the blocks
218:15 - here and so on and so forth for all of
218:17 - the other files on the system now there
218:19 - is a difference though let's say this is
218:21 - a symbolic link okay a symbolic link
218:24 - doesn't point to the hard drive at all
218:26 - it actually just points to a file in the
218:30 - file allocation table so this is a
218:32 - standard
218:33 - you know table of contents link right
218:35 - this is pointing to this spot and
218:38 - these blocks on the hard drive but the
218:41 - symbolic link just points to
218:43 - filename.txt
218:45 - on this system itself now there's the
218:48 - other kind of link and that's a hard
218:49 - link so you probably notice there's
218:51 - another purple file here so let's say
218:53 - this is file two and it's pointing to
218:55 - you know these four blocks
218:58 - this could be like file 12 and it points
219:02 - to the exact same
219:05 - spots on the hard drive so it points to
219:07 - the exact same file location and the
219:09 - same number of blocks it's basically the
219:12 - exact same file but it has two different
219:15 - reference points in the file allocation
219:17 - table that can be really confusing but
219:20 - what's cool about it is let's say you
219:22 - accidentally delete this file well
219:24 - that's okay it's still on the hard drive
219:26 - and you can still reference it from this
219:28 - file here so let me show you what that
219:29 - looks like in practice in our system
219:31 - here we have let's do an ls minus l we
219:34 - have my file.doc now if we wanted to do
219:37 - a symbolic link we would do
219:38 - ln minus s for soft or symbolic the
219:42 - source is my file and the destination is
219:45 - going to be my
219:46 - linked file.doc
219:49 - do ls minus l and we're gonna see it
219:51 - actually shows us exactly what's
219:52 - happening my linked file dot doc is just
219:55 - pointing to the name
219:57 - myfile.doc in fact it's just pointing to
219:59 - this name itself so if we were to say
220:02 - move my file dot doc to my
220:05 - new file dot doc and then we do an ls
220:08 - minus l this link is broken because it
220:10 - still points to the name my file dot doc
220:13 - and that doesn't exist anymore so this
220:15 - is now a broken link on our system so
220:18 - symbolic links are kind of dumb they
220:20 - don't take up much space but they're
220:22 - kind of dumb in that they don't follow a
220:23 - file if you move it or rename it all
220:26 - right so that is a soft link now a hard
220:28 - link works differently a hard link if we
220:30 - were just to say ln without any flags
220:33 - my new file dot dock to
220:36 - my file dot doc and then we do ls minus
220:39 - l well one we've fixed the symbolic link
220:42 - right because now this file points to a
220:44 - file that exists now so all of a sudden
220:46 - now it's pointing to this file
220:48 - myfile.doc and you'll notice it's the
220:51 - same size as the other one and if we
220:53 - were to move the original so we're going
220:56 - to say
220:57 - move my
220:59 - new file to
221:00 - my cool file dot doc
221:04 - it doesn't break the hard link that we
221:07 - made both of them are still there
221:08 - they're still fine they're their own
221:10 - independent file name in the file
221:12 - allocation table they just happen to
221:13 - point to the same spot in the hard drive
221:15 - and we can see that if we do ls minus l
221:17 - i for inodes it's going to show us the
221:20 - spot on the hard drive that it's
221:22 - actually pointing to and sure enough
221:23 - these have a matching
221:25 - inode whereas this symbolic link has a
221:27 - completely different inode because it's
221:28 - you know just a file that only points to
221:30 - a file name but these two have the exact
221:33 - same spot on the physical hard drive now
221:35 - another cool thing this number here
221:37 - which you've probably never even thought
221:38 - about before but this says how many
221:41 - linked files
221:42 - are on this inode now this says there
221:44 - are three now of course we see two right
221:46 - here but that means somewhere on my file
221:48 - system there's another hard link to this
221:52 - inode now i did actually make it before
221:54 - we started so we could use the find
221:56 - command to find that i'm going to say
221:57 - find
221:58 - i'm going to look in my home directory
222:01 - in the same file flag and i want to find
222:04 - the same file as
222:05 - my file dot doc or i could say my cool
222:07 - file dot doc because these are the same
222:10 - files so it doesn't matter which one i
222:11 - have find look for a match for and press
222:14 - enter and it's going to find all three
222:16 - of them sure enough i have a hidden file
222:18 - right here that i that i did earlier
222:20 - before the nugget started and this is
222:21 - just one more hard linked file to this
222:24 - same
222:25 - inode we could look really quick ls
222:26 - minus li
222:28 - l i a so we can see the hidden files and
222:31 - sure enough hidden file there's that
222:32 - same inode reference that the other ones
222:34 - are referencing now honestly soft links
222:36 - are generally used more because they're
222:38 - easy to see you can do an ls and see
222:40 - where they're pointing to so they're a
222:41 - lot more convenient but hard links do
222:44 - have their place because each file acts
222:47 - as an independent file you can delete
222:48 - one and it doesn't ruin the reference
222:50 - that the other one has so hard links
222:52 - point to inodes soft links just point to
222:54 - file name references of other files on
222:56 - the existing file system
222:59 - when you're trying to figure out the
223:00 - location of files on your system there's
223:02 - basically two ways you can do it there's
223:04 - the find command and the locate command
223:06 - and both do pretty much the same thing
223:09 - but find is quite a bit more powerful
223:11 - yet has some limitations over locate i'm
223:14 - going to just show you how they work
223:15 - because trying to explain the pros and
223:17 - cons just seems a little bit silly when
223:19 - we can just actually see how it works in
223:21 - action now if we look in our documents
223:23 - folder i have a few files here i have
223:25 - new paper and it looks like it has camel
223:27 - caps here capital n capital p old file
223:30 - and research paper dot doc so let's
223:32 - actually use the locate command first
223:34 - because it's simple so we say locate and
223:37 - then what we want to look for and it can
223:38 - just be a substring so if we say old
223:40 - file and press enter it's going to give
223:42 - us the full path of old file.txt notice
223:45 - i didn't have to search for the entire
223:47 - file name i just searched for old
223:48 - file.txt
223:50 - or old file and it found old file.txt
223:52 - now we can do the same for let's see
223:54 - locate research underscore and it should
223:57 - find research paper and sure enough
223:59 - research paper now there was one more
224:01 - file in there if we type locate
224:03 - new
224:04 - paper no it doesn't find it oh did i
224:07 - spell it wrong well let's look over here
224:09 - new paper no i did not spell it wrong
224:11 - and that's where the limitation of
224:13 - locate is locate uses a database that is
224:17 - cached on your system which means it's
224:19 - super duper fast for searching for the
224:22 - names of files however the cache is only
224:25 - created once a day now we can force an
224:28 - update we could say sudo
224:30 - update db press enter and it's going to
224:33 - update the database of all the file
224:35 - names on the system and now if we just
224:37 - do up arrow and locate new paper now
224:40 - it's going to find it because we updated
224:42 - the cache of all the files on the system
224:45 - so it's very very fast but it has that
224:47 - limitation that it uses cached data now
224:50 - the find command is more powerful but it
224:53 - has the limitation that because it
224:55 - searches in real time it's a lot slower
224:57 - so how does it work pretty much the same
224:59 - way we're going to say find
225:01 - and you tell it where you want it to
225:02 - search so we can say search the root
225:05 - directory
225:06 - and now rather than just a file name
225:08 - find does a lot more things so we're
225:09 - going to say i want you to search for a
225:11 - file
225:12 - named
225:13 - let's say star
225:15 - new paper star and this should find all
225:18 - of the files that have new paper in them
225:21 - okay so i'm going to press enter
225:24 - and oh my goodness what is all this
225:25 - permission denied well find actually
225:27 - goes through the entire file system
225:30 - because i said search in the root
225:31 - directory and i actually don't have
225:33 - permission to look at all of these
225:35 - things so it's going to search through
225:37 - every single folder on the whole system
225:40 - and let's see did it actually find the
225:42 - file it should have but we have to look
225:46 - through all the error messages and if we
225:48 - scroll up sure enough it did find it all
225:49 - right it did find it just like the
225:50 - locate command but there were a lot of
225:52 - permission denied errors now we could do
225:54 - something like redirect the error right
225:56 - we could say two greater than dev null
225:58 - which will just pipe the errors into our
226:00 - dev null bit bucket and sure enough
226:02 - there it found it was pretty quick but
226:04 - it wasn't as quick as using locate now
226:06 - there are some other really cool things
226:08 - that find can do we can say find in our
226:11 - current home directory and what that'll
226:13 - do is it will allow us to search in just
226:15 - this home directory so it doesn't search
226:17 - the entire file system so that can be
226:19 - pretty convenient so if we say find dot
226:21 - and we're gonna look for the name new
226:24 - p
226:24 - oh i didn't do the stars new p
226:28 - it should find it for us sure enough it
226:29 - did find documents new paper and then it
226:31 - does this other thing we can actually
226:33 - say dash delete
226:36 - and it will delete it and how can we see
226:38 - if it's deleted
226:39 - if we look in the documents folder whoa
226:41 - it deleted that file so find does more
226:43 - than just locate files however it has
226:46 - some finicky things like this it's going
226:48 - to be using regular expressions to
226:50 - search for the files and while it's more
226:52 - powerful it's slower and it can be
226:54 - annoying when we do things like get
226:56 - errors from permission denied things
226:58 - like that so while conceptually find and
227:01 - locate do the same thing they do them in
227:03 - different ways the important thing to
227:04 - remember about locate is that it's
227:06 - always going to use old data unless you
227:08 - run that update db command now find is
227:11 - much more powerful but it works in real
227:13 - time so it's slower and there that means
227:15 - that there are pros and cons to using
227:17 - both tools
227:19 - while it's certainly possible to set up
227:21 - network shares for copying files from
227:23 - one server to another generally if
227:25 - you're going to copy files over the
227:26 - network from linux server to linux
227:28 - server you're going to use either ssh or
227:31 - really scp which stands for secure copy
227:33 - but it uses the ssh protocol in order to
227:36 - do that copying or rsync which actually
227:39 - will sync a whole bunch of files across
227:41 - the network so we're going to look at
227:43 - doing both but it's important to realize
227:45 - that ssh is the same program in the same
227:48 - protocol that we use to connect from one
227:49 - computer to another to reach its
227:51 - terminal so let me show you what i mean
227:53 - now i have two computers set up in our
227:55 - lab i have this ubuntu computer
227:57 - and i have this centos computer they're
227:59 - both on the same network so i'm going to
228:02 - ssh from one to another i'm going to say
228:04 - ssh to centos it's going to ask me for
228:08 - bob's password on centos
228:10 - and then all of a sudden now i'm logged
228:11 - in to that remote computer sent to us in
228:14 - fact if we go into the desktop folder
228:17 - and we do an ls we're gonna see over on
228:19 - sent to us we have things called like
228:20 - cool picture cool pic 2 and these things
228:23 - are on the remote desktop all right i'm
228:25 - going to exit and that's going to bring
228:26 - me back to ubuntu now if we look on the
228:28 - desktop folder we're going to see
228:30 - there's nothing in there because on our
228:31 - local ubuntu computer we don't have
228:33 - those things let's say we wanted to copy
228:36 - something over well let's go into our
228:38 - desktop folder
228:39 - again there's nothing here nothing up
228:40 - our sleeve we could use scp which is
228:44 - secure copy and this uses ssh right so
228:46 - we would say scp from
228:49 - centos now we could also specify a
228:51 - different user now it's the same user
228:53 - for us but i'm still going to specify
228:54 - i'm going to say bob at
228:56 - centos
228:57 - colon then the remote path which is
229:00 - going to be home
229:01 - bob
229:02 - desktop and let's pick one of these
229:04 - files i'm going to say cool picture cool
229:08 - picture.jpg and i want it to copy it to
229:10 - dot which means our current directory
229:12 - i'm gonna press enter it's gonna say
229:14 - okay what is bob's password bob and now
229:17 - if we do an ls we're gonna see look we
229:18 - have cool picture that was copied over
229:20 - the network using scp which uses the ssh
229:23 - protocol and it was copied over and now
229:25 - we have a copy of it here locally we can
229:28 - do the same thing we could say scp a
229:30 - local thing and rather than the
229:31 - destination be our local computer we
229:33 - could do it backwards right we could say
229:35 - scp
229:36 - coolpicture.jpg to bob at centos
229:40 - home bob i'm going to copy it just to
229:42 - his home folder and so now it's sent one
229:44 - over to bob's home folder now we could
229:47 - copy everything all at once if we wanted
229:49 - by using the rsync command now rsync is
229:52 - pretty cool in that it will even recurse
229:54 - directories if we wanted to so we could
229:56 - actually do this we could say rsync i'm
229:59 - going to say minus a so it does all the
230:01 - things including recursively going into
230:04 - directories i'm going to say v so it
230:06 - does it verbosely so we can see what
230:07 - it's doing so rsync dash av and the
230:10 - remote thing is set up just like with
230:12 - scp so bob at
230:15 - centos or we don't have to say bob add
230:17 - if it's the same username we could just
230:18 - say
230:19 - centos colon and then the path so i want
230:23 - home
230:24 - bob
230:25 - desktop i want that entire folder copied
230:28 - to here so what we should end up with is
230:31 - a new folder inside our ubuntu desktop
230:34 - folder called desktop because it's going
230:36 - to copy this folder to our current thing
230:38 - and it's going to have recursively
230:40 - everything in the remote desktop folder
230:42 - let's see if that works press enter it's
230:44 - going to say what is bob's password bob
230:46 - and so now it says receiving all of
230:48 - these files and look sure enough there's
230:50 - a folder called desktop if we were to
230:52 - look inside there now i look all those
230:54 - files that were on that remote computer
230:56 - are now on our local computer as well we
230:58 - used rsync and it will transverse all of
231:01 - the directories recursively and it will
231:03 - copy it over for us now this might seem
231:05 - like a throwaway nugget something that
231:07 - is just nice to know but you're not
231:08 - going to use i'll be honest i use scp
231:10 - and rsync almost every single day
231:13 - copying files back and forth using scp
231:16 - is so easy and so fast you don't have to
231:18 - set up servers it's just a way to get
231:20 - one file to another server without
231:23 - having to worry about installing
231:24 - anything because it uses ssh which is
231:27 - already installed on all of your servers
231:29 - so whether you just want to copy a file
231:31 - or two with scp or you want to do
231:33 - recursive directories with rsync it's
231:35 - really easy to copy files over the
231:37 - network in linux
231:39 - pretty much every distribution out there
231:41 - uses system d to manage the services
231:44 - like the various programs that are
231:46 - installed like web servers and stuff on
231:48 - their system and system ctl is the
231:50 - command line tool that we use to
231:52 - manipulate and manage those services now
231:54 - there's a couple concepts we need to
231:56 - understand we need to know enable and
231:57 - disable versus start and stop enable and
232:01 - disable is basically talking about when
232:03 - the computer boots up will it
232:05 - automatically start the service and so
232:07 - you can have something that you can
232:09 - start and stop but that doesn't mean
232:11 - it's automatically going to start or
232:12 - stop when the system boots up that's
232:14 - where the enable and disable comes into
232:15 - play now it's really really easy to tell
232:17 - what a service is doing by default and
232:20 - we can change it without much more
232:21 - difficulty at all so i'm at a computer
232:24 - right now and i've installed apache 2 on
232:27 - this centos machine so httpd is the
232:29 - package name and i've installed it
232:31 - however if we go over here we can see
232:33 - localhost it's unable to connect it's
232:35 - not running so the first thing we would
232:37 - do is say systemctl
232:40 - status httpd and it's actually giving us
232:43 - more information than it first appears
232:45 - see it's telling us that it's actually
232:47 - inactive which makes sense because we
232:49 - can't get it to load but more
232:51 - importantly it's saying that the service
232:53 - itself is disabled and the vendor preset
232:56 - meaning like when you first install it
232:58 - it's set to disabled so we can change
233:01 - that because if it's disabled it means
233:03 - it's not going to start when the
233:04 - computer boots so even if we rebooted
233:05 - this computer it still wouldn't load
233:08 - because it wouldn't start by default so
233:10 - we can say system ctl
233:12 - enable httpd
233:15 - and press enter and now if we do that
233:16 - status we're going to see that it's
233:18 - changed okay it says it's enabled even
233:20 - though the vendor preset is still
233:22 - disabled this means that you know when
233:24 - we installed it it was disabled but
233:25 - we've changed it now so it's enabled but
233:27 - you'll notice
233:28 - it's still inactive or dead now if we
233:31 - did restart the computer it would
233:33 - automatically start up but we can start
233:35 - and stop it independently from whether
233:37 - it's enabled or disabled as not we can
233:39 - just say system cto start httpd and it's
233:42 - going to start our service we can look
233:44 - at status httpd and we can see sure
233:47 - enough now that it's active right it's
233:49 - running and it's still enabled so when
233:52 - we reboot it's going to start running
233:54 - automatically now even if we left this
233:56 - disabled we could still have started it
233:59 - using the start command however when the
234:01 - computer rebooted it wouldn't start
234:03 - automatically so if you want it to
234:04 - always start up you have to make sure
234:06 - that it's enabled even if it comes
234:08 - disabled by default just a quick look
234:11 - boom it's running and sure enough it's
234:12 - right there running for us and it will
234:14 - run when the computer reboots because
234:16 - we've changed it to enabled i really
234:18 - like systemd because systemctl is kind
234:20 - of the one-stop shop it's like the swiss
234:22 - army knife for managing services on a
234:24 - computer that's controlled with the
234:25 - systemd startup init service
234:29 - sys5 or sysv or system five it's called
234:32 - a lot of different things but this is an
234:34 - older way that linux systems would put
234:36 - themselves in various modes or run
234:39 - levels that determine the type of system
234:41 - whether it's a gui system whether it's
234:43 - just a standalone network system and we
234:46 - can switch those various modes we can
234:48 - set defaults to those modes but it's
234:49 - important to understand what the modes
234:51 - actually are and there's a whole list of
234:53 - them unfortunately the modes are
234:56 - different in debian
234:57 - and centos or debbie and ubuntu centos
235:00 - souza they actually use the various
235:02 - levels differently so i just want to
235:04 - briefly go over the difference so that
235:05 - if you're on one system you kind of
235:06 - understand what's going on so centos
235:08 - actually separates them the most so
235:10 - let's go here first we have run level
235:12 - zero and run level zero is basically if
235:14 - you go into this mode it halts the
235:17 - system this is like a way to power the
235:19 - system down run level one is single user
235:22 - mode there's no networking or anything
235:23 - and there's no asking for the root
235:24 - password this is the way that you would
235:26 - recover a root password
235:28 - on a sys5 computer mode 2 is multi-user
235:32 - with no network 3 is multi-user with
235:35 - network 4 is not used at all with centos
235:39 - and 5 is the multi-user gui system like
235:42 - if you have x windows installed and then
235:44 - lastly run level six is reboot if you
235:46 - switch into run level six it will then
235:49 - reboot your computer and it will go into
235:51 - whichever default is set now w ubuntu
235:54 - are similar
235:56 - halt is the same reboot is the same
235:58 - single user mode is the same the
236:00 - difference is here
236:01 - two run level two is the full multi-user
236:05 - system just like run level three is here
236:08 - and then if there's a gui installed on
236:10 - the system the gui will start up there's
236:12 - no difference in debian ubuntu between
236:15 - having a gui system and having a not gui
236:17 - system when it comes to run levels
236:19 - that's only if the gui is installed so
236:21 - run level 2 is pretty much what we use
236:23 - all the time when we're in debian and
236:25 - ubuntu run levels 3 through 5 don't do
236:27 - anything at all they're just not used so
236:30 - that's the big big difference between
236:32 - the two we still have reboot we still
236:34 - have halt we still have single user mode
236:36 - but it's how they handle the other
236:37 - things that are a little bit different
236:38 - now switching between them and setting
236:40 - the defaults are exactly the same now
236:42 - here's the gotcha it took me a long time
236:44 - to find a system that still uses sys5.
236:47 - this is outmoded and not used in any
236:49 - modern distributions but if you go if
236:52 - you find one that is still in use like
236:53 - this is centos version six it will still
236:56 - use it so what we can do we can say run
236:58 - level
236:59 - and it will show us what run level we're
237:01 - in we're in run level five
237:03 - and the previous run level we were in
237:05 - was just a new boot okay now if we want
237:07 - to switch between run levels we can say
237:10 - tell init and then the run level we want
237:12 - to switch to so i'm going to say 3. this
237:14 - should drop us out of a gui
237:16 - and into a text only environment you can
237:19 - see here there's no gui there's just
237:20 - this text box right here so i'm going to
237:23 - log in so we can go back if i want to go
237:25 - back into the gui system i can say tell
237:28 - init 5 and it'll get us right back into
237:30 - the gui system and here we are in the
237:32 - gui system if i start up
237:35 - a terminal and we say run level we're
237:37 - gonna see we're currently in run level
237:39 - five our previous run level was three
237:42 - now if we switch into run level six it's
237:44 - going to reboot if we switch into run
237:46 - level zero it's going to just halt the
237:48 - computer and power it down if you want
237:50 - to change the default and the default is
237:52 - just what run level it automatically
237:53 - boots into we need to edit a file so i'm
237:56 - going to become root and we need to edit
237:58 - etc init
238:00 - tab
238:01 - and this file has a couple things we can
238:02 - edit but really the main thing is all
238:04 - the way down at the bottom which is the
238:06 - default run level now it gives us a
238:07 - little bit of a cheat sheet here and
238:09 - this is actually really really good
238:11 - advice
238:12 - the halt mode run level 0 do not set
238:14 - your init default to this because it'll
238:16 - boot up and immediately halt and that's
238:17 - not what we want same thing with setting
238:19 - it to run level 6. it will boot up and
238:22 - switch immediately into run level 6
238:24 - which is reboot so it's going to be in a
238:25 - constant reboot loop so you never want
238:27 - to set the default to that ours is
238:29 - currently in
238:31 - run level five for the default and it's
238:34 - right here now we could change that to
238:36 - three
238:37 - save this file and now when we reboot
238:39 - the computer it's just going to
238:41 - automatically go into the text only mode
238:44 - i'll just show you really quick what
238:45 - happens if we
238:46 - tell in it into run level six
238:49 - it's going to reboot that's what it does
238:50 - so if you run across an older system
238:52 - that uses run levels specifically system
238:55 - five run levels you need to know what
238:57 - the various modes do and remember it's
238:59 - going to be different whether it's
239:00 - ubuntu ubuntu and debian or centos and
239:02 - red hat and then you need to know how to
239:04 - switch the modes using telenit and how
239:06 - to set those defaults and most
239:08 - importantly what not to set the defaults
239:10 - to namely run level 0 or run level 6.
239:15 - i often find myself on a system thinking
239:17 - what if i need to switch between modes
239:19 - like i have a gui machine that i want to
239:20 - get rid of that gui interface so it's
239:22 - just a server with the text mode or vice
239:24 - versa well switching modes and setting
239:27 - the defaults is done one way with the
239:29 - sys5 system but if you have a newer
239:31 - system d
239:33 - initialization system it can be very
239:35 - very confusing especially if you have
239:37 - that sys5 background thankfully there
239:39 - are pretty simple comparisons when it
239:42 - comes to how it used to be and how it
239:43 - currently is now if you're not familiar
239:46 - with init 5 or with sys5 in it that's
239:48 - all right we're just going to talk about
239:50 - what the various modes are basically we
239:52 - start with run level zero which has a
239:55 - correlation
239:56 - in the system d world as a boot target
239:59 - called power off now boot targets are
240:02 - basically just modes right these are
240:04 - modes that computers are are set to so
240:07 - that they can function in a specific way
240:09 - and while it doesn't seem like a mode
240:11 - it's a really easy way to shut your
240:13 - computer down by switching into the
240:15 - power off mode now there's also one this
240:18 - is single user mode in the world of
240:20 - systemd it's called rescue mode this is
240:22 - like insist five if you want to like
240:24 - reset your root password you need to
240:26 - switch into single user mode well same
240:28 - thing with a boot target it's just
240:29 - called rescue mode then there's mode
240:31 - three which in a centos system is going
240:34 - to be like a
240:35 - non-graphical user interface with
240:37 - networking support that translates to
240:39 - just multi-user target in system d uh
240:42 - the gui mode five translates to
240:45 - graphical target and of course six is
240:47 - like the the compatriot to run level
240:50 - zero and this is how you can reboot your
240:52 - system by switching into run level six
240:54 - or boot target reboot now switching
240:57 - between them is actually easier than it
240:59 - is with the old sys5 you can actually
241:01 - just use a command line tool instead of
241:04 - editing that init tab file that you have
241:06 - to do with sys5 now i'm on centos
241:08 - version 7 here because centos version 7
241:11 - uses system d whereas centos version 6
241:13 - uses sys5 now the first thing you need
241:15 - to do is be root so i'm going to quickly
241:17 - become root once we're root we can type
241:20 - system ctl which is the way we do most
241:22 - things with systemd but systemctl
241:25 - getdefault
241:26 - and this is going to tell us what the
241:28 - default mode is and that makes sense
241:29 - because our default mode here is
241:31 - graphical target and you can see we have
241:33 - a gui interface now we could change that
241:36 - we could say systemctl
241:38 - set default to multi-user.target
241:44 - and see now it's changed that so if we
241:46 - say get default it's going to tell us
241:47 - okay now it's multi-user but notice it
241:49 - didn't change we're still in the gui
241:51 - environment well that's because it just
241:53 - changed the default if we were to reboot
241:55 - this computer it would reboot into a
241:57 - text only mode now if you want to switch
241:59 - between modes or between targets with a
242:02 - computer that's already started you
242:04 - simply type system
242:06 - ctl
242:07 - isolate and then the name of the target
242:10 - in our case let's say isolate
242:12 - multi-user and it should drop us
242:15 - directly into sure enough the text only
242:17 - mode now if you're already used to the
242:20 - world of run levels you just have to
242:22 - kind of think what the different targets
242:24 - that correspond to it are but if you're
242:25 - not familiar with run levels like this
242:27 - is something that happened before your
242:29 - time in linux that's okay because
242:30 - honestly boot targets make a heck of a
242:33 - lot more sense than the run levels did
242:36 - because they actually have their
242:37 - description right in their names and
242:39 - while it's important to understand both
242:42 - sys5 and system d you should know that
242:44 - all systems going forward are going to
242:46 - be system d so you're gonna have to know
242:49 - about the various modes how to switch
242:50 - between them using isolate and then how
242:53 - to set and get the default so you know
242:55 - what happens to a system when it boots
242:57 - up so you don't have your rack servers
242:59 - booting up to a gui environment because
243:01 - that just doesn't make any sense
243:04 - services are the various programs that
243:07 - are installed on a server that are going
243:08 - to run and serve out like web pages or
243:10 - whatever you might have installed
243:12 - they're called services and if you have
243:14 - sys5 on your computer the way that you
243:16 - manage and start and set defaults for
243:18 - those individual services are by using
243:21 - specific programs in the
243:22 - etc init.d
243:24 - folder now there are tools that we can
243:26 - use to manage those specifically service
243:28 - and check config and i want to show you
243:30 - how they work because the services are
243:32 - determined
243:34 - to start and stop based on the run level
243:37 - of a particular what the computer is set
243:38 - to so if like it's run level three a
243:41 - certain system might start and if it's
243:43 - run level five
243:45 - another service might not start let me
243:47 - show you what i'm talking about here on
243:49 - our system though this is centos 6 which
243:50 - has sys5 if we look in etc.d
243:55 - these are all the various services or
243:57 - programs that are installed on the
243:59 - computer we can see things you know like
244:01 - post fixes the email server sshd is our
244:04 - ssh server and we can start and stop
244:07 - these by using the service command so i
244:10 - can say service
244:11 - sshd
244:13 - start
244:14 - and it's going to start the service i
244:16 - can say service sshd
244:19 - stop and it will stop the service i can
244:20 - actually do status to see what it's
244:22 - currently doing so right now it's
244:24 - currently stopped it says but what i
244:27 - want to do is change how it starts or
244:29 - stops on system boot and what we can do
244:32 - is say chk config
244:34 - dash dash list sshd and it's going to
244:37 - show us what sshd is going to do on
244:40 - every run level so run level zero it's
244:43 - off one it's off two it's off three it's
244:44 - off four it's off five it's off and six
244:46 - it's off so this means it is not going
244:48 - to start up on system boot regardless of
244:51 - what run level the system is starting at
244:53 - now we can set it so that it will start
244:55 - for all of the run levels one through
244:58 - five it's never going to start
244:59 - automatically for zero or for six
245:02 - because those are
245:03 - those are halt and reboot and that would
245:04 - just be silly but if we wanted to start
245:06 - on all of them we can just say chk
245:09 - config
245:11 - sshd on and it's going to set them to on
245:14 - for all of the run levels so we can do
245:16 - that list command again and we're going
245:17 - to see now it's on four two three four
245:20 - and five actually it doesn't do it for
245:21 - single user mode so if we just say on
245:24 - it's going to set it for run level two
245:26 - three four and five it's going to be
245:27 - turned on but we can do it individually
245:29 - too so first of all let's turn it back
245:31 - off so now they're all set to off if we
245:34 - look see they're all off again we can do
245:36 - a single one so it gets a chk config
245:39 - dash dash level
245:41 - 3 on
245:43 - and now if we look it's just going to be
245:44 - on for level oop
245:47 - gotta get the format right sshd on
245:51 - and now if we look there now let's turn
245:53 - it on for run level three but the other
245:55 - ones are still off so chk config is the
245:58 - way that we change how it boots up
246:00 - whereas the service command up here is
246:02 - how we change it immediately if we want
246:04 - it to start or stop we can use the
246:05 - service command but if we want it to
246:07 - start on boot we need to use the chk
246:09 - config command because that's going to
246:12 - change the behavior at the various run
246:14 - levels thankfully when you install
246:15 - packages they create their own entries
246:17 - in the etc.d folder we don't need to
246:20 - make scripts or anything in there and
246:22 - the programs install things in there so
246:24 - that the service and the chk config
246:26 - commands know exactly what to do in
246:28 - order to start and stop or configure
246:30 - what happens on boot with a given system
246:33 - that's running sys5
246:35 - modern linux systems use system d to
246:38 - manage their services things like their
246:40 - web server their ssh server and the same
246:43 - tool is used to start them stop them
246:45 - enable them on boot and that's that
246:47 - swiss army knife that catch all tool for
246:49 - systemd
246:51 - system ctl so i want to show you how to
246:53 - go about starting and stopping
246:55 - individual services but also how to
246:57 - affect what happens on boot when a
246:59 - system boots up what happens with
247:01 - particular services but first there is
247:04 - one thing that can be frustrating about
247:05 - system d and that is that the service
247:08 - files can be scattered all over the hard
247:10 - drive so for example inside etc systemd
247:14 - system we're going to find a couple
247:16 - service files like anything that ends in
247:18 - dot service is going to be a system d
247:20 - service file but you'll notice like
247:22 - there's no ssh here well that's
247:24 - frustrating well let's actually search
247:26 - for that say locate
247:28 - sshd.service you'll find that this is
247:31 - actually located in user lib systemd
247:34 - system that's where sshd.service live so
247:37 - there's several places that you can find
247:39 - the service files whereas with sys5 it
247:42 - was always in the etc.d folder here
247:44 - there are several folders that are going
247:47 - to house system files that are service
247:50 - files for your system so that can be
247:52 - frustrating but nonetheless we
247:54 - regardless of where they're stored we
247:55 - can still use systemctl to query them so
247:58 - we can say for example systemctl sshd
248:02 - let's do a status
248:05 - and this is another gotchu if you're
248:06 - going from sys5 right into system d the
248:10 - frustrating thing is normally we would
248:11 - say like service sshd status well now
248:14 - it's backwards now we have to say
248:17 - system ctl
248:19 - status sshd gur it's frustrating but you
248:22 - get used to it unless you go back and
248:24 - forth from systems then it can be a
248:26 - little bit frustrating but nonetheless
248:28 - on system d we have to say status or
248:30 - start or stop and then the service name
248:32 - whereas it's backwards with sys5 anyway
248:35 - we have a lot of information here so we
248:37 - can see that it's active which means
248:39 - it's running so it's currently started
248:41 - there's more information here though we
248:43 - look up here it says it's loaded it's
248:45 - enabled
248:46 - and enabled in systemd world means that
248:48 - it's going to start on system boot and
248:51 - there's even some more information here
248:53 - it says vendor preset is enabled now
248:56 - what that means is when we install the
248:58 - sshd daemon it's going to automatically
249:02 - be enabled now that doesn't mean it's
249:04 - going to start unless we restart the
249:06 - system but it means that it's going to
249:08 - be set to start on the system boot
249:10 - that's what the vendor preset is now we
249:12 - can change this easily we can say
249:15 - system
249:16 - ctl disable sshd and now if we go back
249:20 - and say status sshd we're going to see
249:23 - now it's disabled and the vendor preset
249:26 - is still enabled but we've changed it
249:28 - now so that it's not going to start on
249:30 - boot however this is another important
249:32 - thing to note it's still running because
249:34 - we've changed what happens on boot but
249:36 - we haven't changed what's currently
249:38 - happening on the system so if we reboot
249:41 - it's not going to be running but if we
249:43 - want it to not run we actually have to
249:45 - tell it that so we have to say systemctl
249:48 - stop sshd
249:50 - and now if we were to say status now we
249:52 - would see it's no longer running it's
249:54 - inactive and it's not going to start on
249:57 - system boot but let's change that
249:59 - because we definitely wanted to start on
250:01 - system boot so systemctl will start it
250:04 - up
250:05 - sshd and systemctl
250:09 - enable sshd and now if we do system ctl
250:13 - again this is a catch all tool
250:15 - status sshd we're going to see it's back
250:18 - to how it should be it's running and
250:20 - it's enabled which means it's going to
250:22 - start on boot
250:24 - so regardless of what service we want to
250:26 - start stop or enable or disable that
250:29 - system ctl tool is what we use for just
250:31 - about everything in the world of system
250:33 - d really the only gotcha with going from
250:36 - sysv to system d when it comes to
250:39 - services you have to remember to use the
250:41 - system ctl tool and then you have to
250:43 - remember that the actual command goes
250:46 - before the name of the service and
250:48 - that's backwards from sys5 so we have to
250:50 - say like systemctl start sshd whereas
250:54 - with sys5 it was backwards but it's easy
250:56 - to get used to and i love having a
250:58 - one-stop tool to do all of the things
251:02 - that when we're planning servers on our
251:04 - network it's important to know that
251:05 - servers serve things they serve things
251:07 - like ntp which is a network time
251:09 - protocol ssh a secure shell so you can
251:12 - get into the computer remotely dns is
251:14 - domain name service which translates
251:16 - things like cbtnuggets.com into an ip
251:18 - address dhcp hands out ip addresses on a
251:21 - local network so you don't have to
251:22 - manually assign them a docker is a
251:24 - containerization system that allows you
251:26 - to run services in siloed environments
251:29 - and then of course configuration
251:30 - management tools allow you to centralize
251:33 - the individual configuration of servers
251:35 - so there's basically several kinds of
251:37 - services that we're going to install in
251:39 - our networks we have centralized things
251:41 - which are going to run on like one
251:42 - server for your entire network and then
251:44 - individual which are going to run on all
251:47 - or most of your servers so here we have
251:50 - things like i mentioned before dhcp dns
251:53 - configuration management server these
251:55 - are going to run on one server on your
251:58 - network you don't need more than one
251:59 - dhcp server more than one dns server
252:02 - apart from redundancy or high
252:03 - availability but generally speaking you
252:05 - only need this in one place individual
252:08 - computers all have to have an ssh server
252:11 - installed because you want to get into
252:13 - every server on your network right this
252:16 - is just something that's going to be
252:17 - installed everywhere your configuration
252:19 - management client is going to be
252:21 - installed on every computer it's a
252:24 - service that needs to be there so that
252:25 - it can take advantage of the
252:27 - configuration management system like
252:29 - chef or puppet or ansible so that it can
252:32 - you know work together to keep those
252:34 - servers in line docker is a
252:35 - containerization program that runs on an
252:37 - individual server and if you're gonna
252:39 - have a lot of different servers out
252:41 - there you may have docker installed on
252:43 - multiple computers so that it can host
252:46 - services for you now ntp is kind of the
252:48 - in the junction point of my venn diagram
252:51 - here and that's because ntp or network
252:53 - time protocol is the service that keeps
252:55 - your computer in the proper time like
252:58 - you know if you have some clock skew
252:59 - where it's a little bit too fast or a
253:00 - little bit too slow ntp will keep your
253:04 - server running in the proper time now
253:06 - there is a centralized ntp server very
253:09 - often on your network and all of the
253:11 - other computers or servers on your
253:14 - network will then query your centralized
253:17 - ntp server but here's the deal this same
253:20 - ntp server software is actually the
253:22 - client software as well so ntp does two
253:25 - things one it queries an above computer
253:28 - for the time you know in the case of the
253:30 - centralized one it's in the cloud and it
253:32 - also can serve out that time information
253:35 - to its peers or to people on your local
253:37 - network so usually we have a centralized
253:40 - ntp server but we don't even have to
253:42 - right all of these ntp server machines
253:45 - could query right out to the cloud and
253:48 - bypass a centralized ntp server it's
253:51 - just nice to have a centralized place so
253:52 - you have one time frame that your entire
253:55 - network is based on but it's the same
253:57 - server program so that's why i kind of
253:59 - put it in both camps here as individual
254:01 - and centralized now when you're setting
254:03 - up your servers on your network it's
254:06 - important to think through how it should
254:08 - work and it's actually gone through this
254:11 - change as computer hardware and
254:13 - technology has progressed there used to
254:15 - be a time where you would have a bare
254:17 - metal server for every service on your
254:20 - network if you had a dns server it would
254:22 - literally be a physical server sitting
254:25 - in your server closet and it would sit
254:27 - right next to your dhcp
254:29 - server whose sole purpose was to serve
254:31 - out dhcp same with ntp or you know a web
254:34 - server
254:35 - but then we said you know what now
254:37 - computers are getting to be so fast that
254:39 - we could put multiple services on a
254:42 - single computer because the problem here
254:44 - is it was very expensive right if you
254:47 - had to buy a physical server for every
254:48 - service you wanted to do it got
254:50 - expensive fast so what if we bought a
254:52 - decent sized server and then we
254:54 - installed dns software we installed dhcp
254:57 - software ntp software web software
255:00 - whatever we wanted to do it would all
255:01 - run alongside each other and be really
255:03 - happy
255:04 - the problem comes what if you need to
255:06 - run an update on one of these like we
255:08 - need to take dns offline so we can
255:10 - update it and maybe restart the server
255:12 - when we restart the server all of our
255:15 - services go down because we've put them
255:18 - all in one basket right all of our eggs
255:20 - are in one basket it's also a single
255:22 - point of failure basically it's messy
255:25 - and then the dawn virtualization
255:27 - happened and this is where
255:29 - server closets got really awesome really
255:31 - fast that's because we had large
255:34 - computers you know basically the same
255:36 - large computer it's not that computer
255:38 - hardware got all that much faster but
255:40 - virtualization technology allowed us to
255:43 - instead of just installing a dns service
255:45 - software we could install a virtual
255:48 - server inside here by taking a slice of
255:50 - the resources from the bigger server and
255:53 - install a completely new server
255:56 - virtualized in there that would run dns
255:57 - and we could do that alongside another
255:59 - virtual server and if we had to take
256:01 - this offline or or restart it it
256:03 - wouldn't affect the others because they
256:04 - were their own standalone virtualized
256:07 - servers it was really really awesome it
256:09 - still is a very powerful way to go about
256:11 - protecting your different services from
256:14 - each other on the network the problem is
256:16 - and this is something that i've fallen
256:17 - prey to is sprawl potential it's really
256:20 - easy to spin up another server oh i want
256:22 - to do this i'll spin up another server
256:23 - oh what if we did this sir spin up
256:25 - another server that's where docker comes
256:28 - into play and i won't go too much into
256:29 - docker other than to tell you what
256:31 - docker does is it takes a server and it
256:34 - has a single operating system running
256:36 - linux and then each service has its own
256:39 - like isolated pocket it doesn't have its
256:42 - own
256:42 - operating system it's not like a virtual
256:45 - machine all it does is have its own
256:47 - little slice of the running system where
256:50 - it has its own file system and it can
256:53 - run its own little service here and it
256:55 - doesn't affect anything else because
256:57 - it's walled off so containerization is
257:00 - an even better way to take better
257:02 - advantage of server hardware even than
257:04 - virtualization when it comes to virtual
257:06 - servers so yes there's a lot to think
257:08 - about when you're installing servers on
257:10 - your network on where to put them but
257:12 - the nice thing is whether it's a local
257:13 - service that has to be installed on
257:15 - every computer or a centralized service
257:17 - that you just install like in one place
257:19 - for your whole network planning has
257:21 - gotten a lot easier because you don't
257:23 - have to worry so much about putting all
257:24 - of your eggs in one basket we've been
257:26 - able to isolate individual services
257:29 - without the need to buy brand new
257:30 - hardware so planning is kind of fun and
257:32 - more flexible than it's ever been before
257:35 - conceptually we pretty much understand
257:37 - how a web server works you send a
257:39 - request and the web server sends back
257:40 - the web page but when you add ssl or tls
257:44 - it really does add a layer of complexity
257:46 - but that complexity is for a good reason
257:48 - because it can secure the traffic so
257:50 - nobody knows what is going through your
257:53 - internet connection like bank account
257:55 - information and stuff like that so it's
257:57 - very important that we have ssl
257:59 - encrypted secured traffic now the
258:01 - process is going to be a little bit
258:03 - different than just a standard web page
258:05 - and part of the thing that you want to
258:07 - make sure you have if it's like for a
258:08 - bank or something is a certificate
258:10 - authority now let me demonstrate exactly
258:12 - what goes on
258:14 - when you try to get a web page from a
258:16 - web server let's say this is our web
258:18 - server now if you're not talking about
258:20 - ssl basically the guy in the computer
258:22 - here says hey i would like to see your
258:24 - web page and then the computer says okay
258:26 - here is my web page and that's pretty
258:28 - much the entire process there's no
258:30 - encryption at all but when you go to
258:32 - like your bank's website you're going to
258:35 - set up an ssl session so that all of
258:37 - your information that goes back and
258:39 - forth is encrypted so basically here is
258:41 - how the process works the client sends a
258:44 - message to the server
258:45 - and it says hey i would like to start an
258:48 - ssl encrypted session with you and then
258:50 - the web server responds okay
258:53 - here is my certificate this says who i
258:56 - am and that i'm valid and look here's a
258:59 - picture of my kid playing softball maybe
259:01 - not that bar but it sends a certificate
259:03 - describing who it is to prove that it's
259:05 - who the server says it is that it's not
259:07 - like some man-in-the-middle attack now
259:09 - the only way the end user knows that
259:12 - it's real is because he contacts a
259:15 - certificate authority which is a
259:17 - centralized trusted place that signs
259:20 - certificates basically it's this
259:22 - person's job to contact this web server
259:26 - and make it prove who it is and then
259:28 - once it proves who it is it gets its
259:30 - certificate signed by the certificate
259:32 - authority so then let's say this guy's
259:35 - name is bob bob says okay i see that at
259:38 - the bottom of your certificate it was
259:40 - signed by somebody who i trust so i'm
259:42 - going to trust that you're really who
259:43 - you say you are so then after that
259:46 - identification has been verified bob
259:49 - then sends his encryption key to the web
259:52 - server so the key is sent from the end
259:54 - user to the server and then the server
259:58 - uses that key that bob sent and that is
260:01 - what is used to encrypt the actual data
260:04 - that is going to go to bob's computer so
260:07 - the actual encryption
260:09 - uses bob's key that he sends to the
260:12 - server after the server proves who it is
260:15 - and then they use this tunnel back and
260:17 - forth and that's how they communicate
260:18 - using bob's key now if you set up a
260:21 - server in your own network you've
260:23 - probably heard of something called a
260:24 - self-signed certificate now that is
260:27 - exactly what it sounds like when that
260:28 - initial request comes from bob and he
260:30 - says hey server i would like to set up
260:32 - an ssl connection the server does
260:35 - respond with a copy of its certificate
260:37 - it says here i am this is all my
260:39 - information uh this is the stuff that
260:41 - you know describes who i am i'm
260:43 - promising that i am the person i say i
260:45 - am but there's no signature on the
260:47 - bottom from the certificate authority
260:49 - it's something that bo that the server
260:51 - signed himself so bob has to like just
260:54 - trust that this server is who he says it
260:56 - is now if it's on your own local network
260:58 - that's usually fine and it's okay to
261:00 - accept a self-signed certificate but if
261:02 - it's over the internet you don't want to
261:04 - accept a self-signed certificate because
261:06 - there's no way to be sure that it's
261:08 - actually the server it says it is and if
261:11 - you trust a self-signed certificate and
261:12 - it ends up being like a
261:14 - man-in-the-middle attack you could be
261:16 - sending all of your banking data to a
261:18 - server that isn't who it says it is and
261:20 - that's very dangerous so a self-signed
261:22 - certificate encrypts the exact same way
261:25 - the problem is you're not a hundred
261:27 - percent positive who it is that
261:29 - initially set up that certificate so
261:31 - there's a lot of trust involved whereas
261:33 - if you use a certificate authority trust
261:36 - is taken out of the picture because you
261:37 - trust the certificate authority it's
261:39 - built into your web browser so the
261:42 - actual ssl encryption is the same
261:44 - whether it's a certificate authority or
261:46 - a self-signed certificate but you don't
261:48 - know if it's the server it says it is
261:50 - unless you have that certificate
261:52 - authority that signs the server
261:54 - certificate that's why it's very
261:56 - important especially on the internet to
261:58 - make sure that you don't get an error
262:00 - about not having a certificate signed by
262:02 - an authority but the process is the same
262:05 - either way
262:06 - it turns out that local network server
262:08 - roles have changed fairly dramatically
262:11 - over just the past few years now don't
262:13 - get me wrong things have come a long way
262:15 - from when we used to have a sneaker net
262:17 - so if we had a file we'd have to put it
262:19 - on a floppy disk and then you know carry
262:21 - to the cubicle next to us and pass it on
262:23 - like that but we don't even use local
262:26 - servers as much as we used to now what
262:29 - am i talking about well i'm talking
262:30 - about the introduction of cloud
262:33 - computing now let's look at file
262:34 - services for example we used to have and
262:37 - then we actually we do still have local
262:39 - file services that you know if we want
262:41 - to save files in a local centralized
262:43 - place we do things whether we're on
262:44 - windows or linux or mac if we want to
262:47 - serve to windows computers we can use
262:48 - the samba program which allows us
262:50 - through file sharing that is hosted on
262:53 - linux but is accessible from a windows
262:56 - machine it's a free way very stable very
262:58 - scalable that we can actually share
263:00 - files with windows computers same thing
263:02 - with nfs which is network file storage
263:04 - and this is applicable for linux mac
263:07 - windows and then if you have old school
263:09 - apples that only use like the apple talk
263:12 - sort of networking stuff well there's
263:14 - neta talk which uses the native apple
263:16 - file sharing but this isn't even used
263:18 - anymore so much because now macintosh
263:21 - computers can very easily do windows
263:23 - shares using samba or nfs but netetalk
263:26 - is still around if you like that native
263:27 - apple file sharing stuff the point is
263:30 - linux computers can do local file
263:32 - sharing very very well the thing to
263:35 - think about if you're implementing a
263:36 - network though is should i and that's
263:38 - where cloud services come into play
263:40 - because while sure you can serve things
263:42 - locally on a file you might want to
263:43 - consider something like dropbox or
263:45 - onedrive or google drive that works on
263:48 - almost every platform and allows you to
263:50 - not only sync things between computers
263:53 - but also have an online
263:56 - backup which is really really vital and
263:58 - it saves a ton of money if you don't
264:02 - have to buy the servers to actually
264:04 - store all of your files if it's stored
264:06 - on a cloud service that you usually pay
264:09 - a service fee for you're going to save
264:11 - that money on maintenance and hardware
264:13 - purchases and etc etc so think about
264:15 - cloud services every time you're
264:18 - thinking about local services there are
264:20 - some cases you'll want local services
264:22 - but some cases it just doesn't make any
264:24 - sense and there are complementary cloud
264:26 - services to almost every one of our
264:28 - local services that we can offer i want
264:30 - to mention the ways that you can serve
264:32 - them locally because it does make sense
264:34 - sometimes for example a print server is
264:36 - going to be cups common unix printing
264:38 - system this works across the board if
264:40 - you're sharing a printer with mac or
264:42 - linux it's going to be using cups and
264:44 - even windows can print two cups servers
264:47 - it's like a centralized place but
264:48 - honestly most printers now pretty much
264:51 - have a robust ability to share and queue
264:54 - jobs on their own so we don't always
264:56 - have to use a centralized cup server uh
264:59 - we can all print to the same printer and
265:00 - it's just handled well netatalk also has
265:02 - printing if you have an old school apple
265:04 - computer you want to use that for but
265:06 - again that's not even used very much
265:07 - anymore how could your local printer be
265:10 - used with cloud services well it doesn't
265:11 - make sense until you think about
265:13 - no configuration printing right with
265:16 - google print or air print these are ways
265:18 - that you can actually send your print
265:20 - job over the internet to a printer that
265:22 - may or may not be connected directly so
265:25 - it's something that isn't going to
265:26 - replace common everyday office printing
265:28 - but it's something to think about if you
265:30 - want to be able to configure or
265:31 - especially print from mobile devices
265:33 - having cloud solutions is very powerful
265:36 - now mail is a special case because a lot
265:38 - of times we want mail to be as secure as
265:41 - possible and that means we don't
265:43 - necessarily want to give the ability to
265:45 - another company to host our mail but
265:48 - remember with great power comes great
265:50 - responsibility
265:51 - keeping your locally hosted email files
265:54 - whether it's using postfix or xm or
265:56 - sendmail can be a full-time job because
265:59 - we want those to be really secure not
266:01 - only so people can't read our emails but
266:03 - so our servers aren't compromised and
266:06 - used to send out spam to the entire
266:07 - world so even though having that fine
266:10 - control over security of your own mail
266:12 - is important think about how nice it is
266:15 - if a huge company like google or
266:17 - microsoft or yahoo would have to worry
266:20 - about the security aspect so you can
266:22 - actually just focus on communicating
266:24 - with it so there's a lot to be said
266:26 - about using a third-party company for
266:28 - email even though you do lose some of
266:29 - that local control and then lastly i
266:31 - want to talk about a proxy now when we
266:33 - talk about proxies in linux we're
266:34 - talking about squid or squid guard which
266:36 - is an add-on to squid and generally
266:38 - proxies have historically taken the load
266:41 - off of your internet connection so a
266:43 - bunch of computers can actually request
266:46 - things one time from the cloud and then
266:48 - the proxy server kind of takes that and
266:50 - distributes it internally but our
266:52 - internet connections are very powerful
266:54 - now so we don't often do that now when
266:56 - we're thinking about proxy a lot of
266:58 - people mistakenly call a web filter a
267:01 - proxy and what a web filter does is it
267:04 - stops you from going to like
267:05 - pornographic websites and that's what
267:07 - squid guard does there's a lot of
267:08 - commercial products that keep a list of
267:11 - sites that shouldn't be visited by
267:13 - people and there are other solutions too
267:15 - like open dns is a way that you can set
267:17 - your dns server your upstream dns server
267:20 - so that it doesn't resolve sites that
267:22 - you don't want to see like pornographic
267:24 - websites they won't even resolve
267:25 - properly so your users can't get there
267:27 - now when it comes to actually caching or
267:29 - proxying large things there are some big
267:32 - companies like akamai that will cache
267:35 - entire video libraries of like netflix
267:37 - and stuff because that's a way that they
267:39 - can save bandwidth between isps but in
267:42 - general we don't use proxies as much as
267:44 - we used to although there are still a
267:46 - lot of use cases for things like squid
267:48 - guard or open dns for blocking unwanted
267:51 - websites that are going to waste time or
267:53 - expose us to things that you know we may
267:55 - not want to be exposed to in the company
267:56 - or in a school so while it's really
267:58 - important to know that there are local
268:00 - services that you can provide on your
268:02 - network using linux i encourage you to
268:05 - think through before you install a
268:07 - server on your network to do a
268:09 - particular task see if it really makes
268:11 - sense to host that locally or if going
268:13 - for a third party service might make
268:15 - more sense
268:17 - authentication services and database
268:19 - services are both things that we often
268:21 - do on local computers i want to talk
268:23 - about their purpose and their importance
268:25 - but it also might seem a little weird
268:26 - that i group them together and that's
268:28 - because we usually think about
268:29 - authentication and databases as local
268:32 - services that run on robust servers and
268:34 - that's for a reason it's because they're
268:35 - very very important to what we do on a
268:38 - regular basis and i just want to talk
268:40 - about while we oftentimes still run them
268:42 - on local computers there's still an
268:44 - argument to be made for putting those in
268:46 - the cloud as well now generally if
268:48 - you're talking about linux you're
268:49 - thinking about a sql service if you're
268:51 - thinking traditionally right this could
268:53 - be my sql or
268:55 - maria database which is basically my
268:57 - sequel only newer
268:59 - postgres there's a whole bunch of other
269:01 - sql servers that would normally run on a
269:03 - really robust server on our network now
269:05 - there are tons of other
269:08 - database services some of them are you
269:10 - know are non-sql and and some of them
269:13 - are good for certain types of data and
269:14 - bad for other types of data but there
269:16 - are tons and tons of database services
269:19 - and generally when we have a database
269:21 - server it's going to be on its own
269:24 - computer and that's just because
269:26 - database servers tend to be kind of
269:27 - robust so you're going to have something
269:29 - like mysql maria postgres they're going
269:32 - to be on their own server and
269:34 - authentication services are similar as
269:36 - well because we need to have a central
269:39 - place to authenticate all of our users
269:40 - now when you log into like gmail you're
269:43 - going to put in your username and
269:44 - password and that's all stored in a
269:46 - central place on google servers
269:48 - if you're on your local network there
269:50 - are tons of ways that you can store user
269:53 - information on your own local network in
269:56 - fact for years i would use nis on a
269:58 - linux server and i would use all of our
270:00 - computers in the network would
270:02 - authenticate to this one centralized
270:04 - server and it was so important that all
270:06 - user information was stored on there
270:08 - that i had a redundancy so that
270:11 - in case one of the servers went bad i
270:13 - would still have a backup that was live
270:15 - high availability so there's lots of
270:17 - ways we can do it but the vital
270:18 - importance of having our own server or
270:22 - even multiple servers is important but
270:25 - here's the deal you'll notice i have all
270:27 - of these things around the outside you
270:28 - can use open ldap to host your user
270:31 - accounts on your network and it's going
270:33 - to work with a bunch of different
270:34 - programs but honestly i would say 95 of
270:37 - the time user data is going to be stored
270:40 - on an active directory on a windows
270:42 - computer even if you're a linux person
270:44 - in a linux shop it seems like ad has
270:48 - taken the cake as the king when it comes
270:51 - to user authentication now the one
270:54 - alternative to that is if you're going
270:55 - to use an online like saml or i don't
270:58 - want to get too much into programming
271:00 - but there are ways that you can leverage
271:02 - online authentication for your local
271:04 - stuff like single sign-on and things
271:06 - like that so that may take the place of
271:08 - active directory on your network but if
271:11 - you're if you're talking about user
271:13 - authentication active directory is
271:15 - almost certainly going to be where a
271:17 - medium to large or even small office is
271:20 - going to host all of their user accounts
271:22 - so it's important to know that you can
271:25 - host authentication services on a linux
271:27 - machine but you may
271:29 - not end up doing that because active
271:31 - directory is probably going to be
271:32 - somewhere and it's a great place to
271:34 - centralize your users and your computers
271:37 - so the whole point of this nugget is
271:39 - really twofold one authentication can be
271:42 - done on linux and i did it for over a
271:44 - decade where everything was hosted on a
271:46 - local linux machine and i used nis for
271:49 - authentication i could have used
271:50 - openldap but generally you're not going
271:52 - to do that on a big network because you
271:54 - have more than linux machines that need
271:56 - to authenticate now the other thing
271:58 - though is database servers i want you to
272:01 - know that they're almost always going to
272:02 - be on their own server because they use
272:05 - a lot of resources a lot of memory a lot
272:08 - of cpu a lot of disk io so if you're
272:11 - setting up server roles on your network
272:13 - think about a database server as having
272:15 - its own need unless you host it out on
272:18 - the cloud in which case you're just
272:20 - paying for somebody else's resources
272:22 - which can often be even more effective
272:26 - centralized logging and monitoring is
272:29 - great for a big network because it's
272:30 - easy to lose track of a big number of
272:32 - servers but honestly it's great even if
272:35 - you have a tiny little network now i'm
272:37 - going to look at
272:38 - combining syslogs but i also want to
272:40 - peek at
272:41 - snmp because both are vital for
272:45 - combining information from multiple
272:47 - sources
272:48 - centralizing them if you will so that
272:49 - it's easier to see and easier to use for
272:53 - making predictions and adjustments in
272:55 - how your infrastructure works so first
272:57 - of all i want to talk about central
272:59 - logging okay now there are a ton of
273:01 - devices out there a lot of them can be
273:03 - you know servers you know and lots of
273:05 - servers on the rack and you don't want
273:06 - each one to have its own set of logs you
273:08 - want to combine them but there's other
273:10 - things now that can create logs but may
273:13 - not have the storage area to keep those
273:16 - logs on themselves like security cameras
273:19 - routers motion detectors smart bulbs
273:22 - printers all of these things can create
273:25 - log files and if you can redirect them
273:28 - to a centralized
273:30 - server that's going to allow you to comb
273:33 - through data in a very efficient way and
273:34 - in fact sometimes this is the only way
273:37 - you can get data from certain devices
273:39 - that have absolutely no storage on them
273:42 - but can generate logs and information
273:44 - now when you combine them all together
273:46 - it is pretty neat how it works the
273:47 - centralized log server is going to just
273:50 - have one big log file however each
273:53 - individual device is going to put its
273:56 - own name in the log file so you can sort
273:59 - by whatever computer is adding it so
274:01 - even though you have one log file that
274:03 - is going to contain everything it's easy
274:05 - to separate out the individual devices
274:08 - to see what they're doing using tools
274:10 - like grep or there are some really fancy
274:12 - devops tools that will allow you to sort
274:14 - data from combined log files
274:17 - when you combine log files like that you
274:19 - end up being able to see some trends or
274:22 - see some relationships that might not
274:24 - otherwise be easily accessible for
274:26 - example if none of these devices are
274:29 - able to access a dns server well maybe
274:31 - you have some problem with the network
274:33 - in that portion of your company and so
274:36 - you can help troubleshoot based on what
274:37 - logs are being submitted now snmp
274:41 - is slightly different and it stands for
274:44 - simple network management protocol but
274:47 - it's a little bit of a misnomer because
274:49 - traditionally this was used to not only
274:52 - read data but also remotely control
274:55 - devices using this network management
274:58 - again it was a two-way street protocol
275:00 - now there are still some instances where
275:02 - you can use this to manage devices but
275:05 - mainly it's used for pulling data and
275:08 - what i mean by that is let's say you
275:10 - have some data that you'd like to
275:12 - concatenate together for an informative
275:15 - data pie that's just delicious you're
275:17 - trying to make graphs or something you
275:19 - can use snmp
275:21 - to pull data from a device one of the
275:23 - most common things that i use it for is
275:26 - i have a router that i connect to the
275:28 - internet with and i would like to see
275:29 - some interface statistics for what's
275:32 - going on how much data is going through
275:34 - it and things like that snmp can pull
275:37 - that data it's not really like
275:39 - individual servers pushing data to a
275:41 - centralized logging it's kind of going
275:42 - the other way it's kind of using snmp to
275:46 - pull data from individual devices and
275:48 - what that looks like in practice here is
275:51 - this is actually the home page that
275:52 - every time i load up a web browser this
275:54 - is what loads up and i have a couple
275:55 - convenient links for me you know to go
275:57 - to see the weather in my area that sort
275:59 - of thing but i have these graphs that
276:01 - show the bandwidth usage both in my
276:04 - house in the town and we own a farm as
276:07 - well that has fiber internet connection
276:09 - and this shows the connection between
276:12 - them now you'll notice there's a lot of
276:14 - matching between the two and that's
276:16 - because i will often back all of my
276:18 - townhouse data to our farm because it's
276:21 - an off-site storage location so that
276:23 - makes a lot of sense but this is just a
276:25 - way that i can see what's happening on
276:26 - my network and i pull this from my
276:28 - routers using snmp now it's important to
276:31 - realize that while they accomplish sort
276:33 - of the same thing centralized logging
276:35 - allows all of your servers to push data
276:38 - to a centralized server so you can comb
276:40 - through all that data in one place
276:42 - whereas snmp is generally used as a
276:45 - protocol that you can pull data out of a
276:49 - server and do something with it like
276:51 - make cool graphs or whatever it is you
276:53 - want to do
276:54 - vpns might be something that you use
276:56 - every day but don't really understand
276:57 - what's going on so vpns or virtual
277:00 - private networks are really just a way
277:02 - to connect to a private network that
277:04 - isn't accessible from the internet
277:06 - itself now talk about the concepts then
277:08 - i want to talk about what options are
277:09 - available if you're using linux because
277:12 - vpn isn't just a one-size-fits-all thing
277:14 - there are several different protocols
277:16 - and stuff that you can use to connect
277:17 - but first of all conceptually what is
277:20 - going on well we have two networks let's
277:23 - say these networks are separated by
277:26 - being in different countries okay now
277:28 - you want to have this computer be able
277:30 - to interact with this computer or this
277:32 - server or something on the remote
277:34 - network but you don't want to open those
277:36 - ports up to the internet right because
277:38 - that's unsafe so what you do is your
277:41 - router or your linux server or something
277:43 - will establish what's called a tunnel
277:46 - and this tunnel is just a layer of
277:48 - encryption
277:50 - that goes from one side to the other and
277:52 - then inside that encrypted tunnel it
277:55 - just sets up a route like it would any
277:57 - other route on your network so your
277:59 - router just sees inside the tunnel and
278:01 - it sees a route using standard network
278:04 - addressing like let's say the vpn
278:07 - internal route is 10.10.0.5
278:12 - or something and it just sees this as
278:14 - another route so that this computer can
278:16 - route information across the tunnel to
278:19 - this router and then get into here so it
278:21 - works the same as traditional routing
278:23 - the only difference is the router or
278:26 - like i said the linux server or whatever
278:28 - it is sets up this tunnel that blocks
278:32 - anybody on the internet from actually
278:34 - seeing what's going on in the route so
278:36 - that tunnel is set up and then the route
278:38 - goes inside the tunnel so nobody sees
278:41 - the traffic now this setup that i have
278:43 - here is called a site to site vpn and
278:46 - what it means is since there is a
278:48 - standard route set up here anything on
278:51 - this network is going to be go is going
278:53 - to be able to go over here to this
278:54 - network and anything on this network is
278:57 - going to route over to this network as
278:59 - if they were in the same building it's
279:01 - just a standard route inside that
279:03 - encrypted tunnel now the other type of
279:05 - vpn is going to be just an end user
279:08 - connecting to an office and if if you're
279:10 - a remote worker or you're like a road
279:12 - warrior you're going to use this a lot
279:14 - your computer is going to establish the
279:16 - same type of tunnel to block all of the
279:19 - internal
279:20 - network traffic from the internet itself
279:23 - once it sets up that tunnel then just
279:25 - your computer is going to be connected
279:28 - to this internal network okay so you're
279:32 - actually going to as if you plugged into
279:34 - an ethernet port in the wall next to all
279:37 - of your other employees or your other
279:39 - fellow workers who happen to be in the
279:41 - headquarters so what this does is it
279:43 - puts your computer inside this local
279:46 - network by establishing a tunnel and
279:49 - then setting up routing protocols that
279:51 - will put you inside there so it's a
279:53 - little bit different than a site to site
279:55 - because these computers are probably not
279:58 - going to like serve data from your
280:00 - computer it's going to work a little bit
280:02 - differently but conceptually the same
280:04 - thing is happening your remote computer
280:06 - is now able to connect to computers
280:09 - inside the remote network and it's all
280:12 - protected from the internet by using a
280:14 - vpn now i talked about different
280:16 - protocols to do that and there are a
280:18 - bunch there's openvpn which is an open
280:21 - source program that allows you to
280:22 - establish these types of connections uh
280:25 - there's ssh which can establish a tunnel
280:27 - for you to do stuff there's l2
280:30 - tp there's ipsec there's all these
280:34 - different protocols and programs that
280:36 - will allow these tunnels to be created
280:38 - and a vast majority of these will
280:40 - actually run on a linux server inside
280:44 - the network so if your router itself
280:46 - doesn't support vpn that's okay you can
280:48 - port forward into a linux server and the
280:51 - linux server will handle all of the vpn
280:54 - routing now there are lots of nuances
280:56 - when it comes to vpn like can you only
280:58 - connect through the remote business can
281:00 - you connect to the internet and also to
281:02 - those things is all of your traffic
281:04 - routed through the vpn even if it's a
281:06 - slow link so there's a lot to think
281:08 - about when setting up vpns but once you
281:10 - understand what's going on it's a lot
281:12 - easier to plan how you're going to
281:13 - implement it so that it so that it can
281:16 - best serve your users or your multiple
281:19 - branch offices so they can communicate
281:20 - to each other i hope this has been
281:22 - informative for you and i'd like to
281:24 - thank you for viewing
281:25 - containers aren't really new technology
281:27 - they've been around for quite a while
281:29 - there have been options like lxc and
281:31 - you've probably heard of docker and also
281:34 - kubernetes comes into the mix which is
281:36 - actually like an orchestration tool that
281:38 - takes care of docker but while
281:39 - containers aren't new they are kind of
281:42 - the new kid on the block when it comes
281:43 - to devops and containerizing
281:45 - applications is something that's really
281:47 - really popular now conceptually they're
281:49 - a little bit like a virtual machine but
281:51 - they're they're different enough that
281:53 - it's important to understand the
281:54 - difference so let's say we have a
281:56 - traditional virtual machine which i'm
281:58 - going to say is the left-hand side of
281:59 - this slide so how it works is you have
282:01 - the the big computer you know and then
282:03 - that computer or that host has its own
282:06 - operating system and then on top of that
282:09 - we carve out a section of the host's cpu
282:13 - and memory
282:14 - and cards and hard drive space we carve
282:17 - that out and then on top of that we
282:18 - install another operating system inside
282:21 - this virtualized environment and then we
282:23 - can put applications on that now
282:25 - containers work in a different way they
282:28 - run right inside the host operating
282:30 - system so you just have an app
282:33 - running in another app running and
282:34 - another app running and this seems like
282:36 - the traditional server model right where
282:38 - you just install linux and then you
282:40 - install applications like apache on top
282:42 - of it the difference is with a
282:44 - containerized application they are
282:46 - running directly on the host computer
282:49 - but they have
282:50 - not really their own operating system
282:53 - all they have is like their own file
282:56 - system
282:57 - and they're jailed off or they're
282:59 - completely separate from the file system
283:02 - of the host operating system itself so
283:04 - they run in their own little world
283:08 - but they're still running directly on
283:10 - the operating system it's just like
283:12 - they're sectioned off in a little
283:14 - container
283:16 - but they're still running on the host
283:17 - operating system and that's really what
283:18 - makes them efficient you'll notice over
283:20 - here um yeah it's drawing it's not like
283:22 - the actual technology but it takes up a
283:24 - lot more hardware and and storage and
283:27 - slices of the host system itself if
283:29 - you're going to install an entire
283:32 - operating system on top of virtualized
283:35 - hardware it just you're not you can't
283:37 - put as many things on one host operating
283:39 - system plus you also have then this
283:42 - operating system to maintain along with
283:45 - this operating system these don't have
283:48 - their own operating system so you don't
283:49 - need to maintain anything except the
283:51 - application
283:53 - itself and it works really cool that
283:55 - jail system is neat so i want to show
283:56 - you how that works let's go actually
283:58 - over to a virtualized environment so
284:01 - here i am in i'm running ubuntu and i
284:03 - have docker installed so we're not going
284:06 - to get into how docker works there's a
284:08 - whole course on how docker works that i
284:10 - taught which is one of my favorite
284:11 - courses but first of all we're just
284:12 - going to start a docker container and
284:14 - we're going to put ourselves inside of
284:16 - it so this is just a little bit of free
284:18 - info we're going to say
284:19 - docker run dash it for interactive tty
284:24 - let's say we're going to run the ubuntu
284:25 - image and i want to run bin
284:28 - bash which is the shell command that i
284:30 - want to run so now boom it it was that
284:32 - fast right it created the container that
284:35 - quickly and i didn't like pause the
284:36 - video and wait for it it actually went
284:38 - that quickly and now we're inside this
284:41 - container which acts like its own
284:42 - operating system because remember it's
284:44 - jailed off but it's still running on the
284:46 - system and we can demonstrate that see
284:47 - i'm not i'm no longer on cbt docker now
284:49 - i'm on this internal container and we
284:52 - have our own file system but let's run a
284:53 - command in here we're going to run
284:55 - something that'll show up with cpu usage
284:57 - so i'm just going to say
284:58 - dd input file equals dev 0 i'll put file
285:02 - equals
285:04 - dev no and this is just something that's
285:06 - going to keep running keep running keep
285:07 - running and not really do anything
285:09 - except use up resources on the computer
285:11 - and i want to do that so we can come
285:13 - over to cbt docker here run the top
285:15 - command and even though this is now in
285:18 - its own container completely separate
285:20 - from cbt docker the operating system you
285:22 - can see look dd shows up as another
285:26 - command running inside this computer
285:28 - because even though it's separated in a
285:30 - container it's still using the same
285:33 - operating system the same kernel the
285:34 - same hardware that everything else along
285:37 - this system is it's just separated so it
285:39 - doesn't interfere and dependencies won't
285:41 - interfere with other apps it won't
285:42 - interfere with dependencies on the host
285:44 - system it's just super efficient so
285:46 - whether you're looking at lxc containers
285:49 - or docker containers which is what we
285:51 - looked at it's important to understand
285:52 - that containers are a lot like a virtual
285:55 - machine except they don't use all of
285:57 - that hardware and they don't completely
285:59 - section themselves off and most
286:00 - importantly they don't have their own
286:02 - operating system that's where containers
286:04 - make things much more efficient and much
286:06 - easier to deal with with much less
286:08 - overhead meaning you don't have to
286:09 - maintain the operating systems of
286:11 - individual apps
286:13 - it's really easy to confuse the concepts
286:15 - of clustering and load balancing because
286:18 - they kind of are the same it's kind of
286:20 - like the question is a hot dog a
286:22 - sandwich i mean they're both meat
286:23 - between bread but are they the same
286:25 - thing
286:26 - not exactly not quite but they both
286:28 - function similarly and that's what
286:30 - clustering and load balancing is like
286:32 - the difference is though clustering is
286:34 - an actual computer term for computers
286:37 - working together to do one task that can
286:39 - be split up whereas load balancing is
286:41 - more of an it concept that can be
286:43 - accomplished in the multiple ways so
286:45 - when we have a cluster basically we have
286:47 - a bunch of computers like we have here
286:49 - that are working together and then
286:50 - there's like a cluster manager it can be
286:52 - a computer it can be a software on you
286:54 - know one of the cluster computers but it
286:56 - actually keeps track of which computer
287:00 - in the cluster is doing what part of the
287:02 - task and these are designed to work
287:04 - together they know about each other
287:05 - these computers are a team and they
287:08 - really do well if susie here
287:11 - sends them a task that is designed to be
287:13 - broken up into multiple pieces so people
287:14 - can work on it at the same time so some
287:17 - jobs lend themselves to clustering
287:19 - solutions whereas some of them don't but
287:22 - basically they know about each other the
287:24 - cluster works together as a team to
287:26 - break down a bigger task into a bunch of
287:28 - smaller ones they can work on at the
287:29 - same time a load balancer like i said is
287:32 - more of a concept it load balances
287:34 - meaning it it has a big job and it
287:37 - splits that job up and lets different
287:39 - computers do the job but usually these
287:43 - computers don't know or care about each
287:46 - other at all they don't know each other
287:47 - exist the load balancer itself whether
287:50 - it's software or a hardware device
287:52 - it knows the entire big load that susie
287:55 - is sending to it and it splits it up and
287:58 - says okay she has 12 jobs so you do
288:01 - three jobs and you do three jobs and you
288:03 - do three jobs they don't know each other
288:05 - or doing jobs as far as the computers
288:07 - know there's only three jobs to do it's
288:09 - the load balancers job to keep track of
288:11 - everything so it's a little bit
288:12 - different they don't really work
288:13 - together they each work separately
288:16 - and accomplish a bigger task that the
288:18 - load balancer itself knows about now i
288:20 - said it's a concept it can work multiple
288:22 - ways right a very common way it's done
288:24 - is the load balancer will be in front of
288:26 - some web servers and there's a whole
288:28 - bunch of people that want to hit that
288:30 - web server and so the load balancer says
288:33 - okay you go to this one and now you go
288:36 - to this one and the next request will go
288:38 - to this one and it splits up the load so
288:40 - that one computer isn't doing all of the
288:42 - work now we can do that very simply on a
288:45 - linux machine if the linux machine is
288:47 - set up for round robin dns now this is
288:50 - not a great way to load balance just
288:52 - conceptually this is what it's going to
288:54 - do so here is my dns configuration on my
288:57 - network i have the domain name web it's
289:01 - actually set up for three different ip
289:03 - addresses okay so you'll see it's
289:05 - actually web web web but they have three
289:07 - different ip addresses and what my dns
289:09 - server is going to do is then round
289:10 - robin and it will split up the load
289:14 - between these three let me show you how
289:16 - that works here we are on the command
289:17 - line so i'm going to say ping web
289:20 - and we'll see we get the response from
289:22 - the web server notice the ip address
289:24 - here is
289:26 - 216-58192.238. if we do the exact same
289:29 - thing now
289:30 - boom if we get responses but notice it's
289:33 - sent it to a different computer together
289:35 - so we got these from another computer
289:38 - and if we do it again we'll get still
289:40 - that third one
289:41 - if we do it a fourth time it's going to
289:42 - wrap around and give us the first one
289:44 - again so this is technically a load
289:47 - balancer we're using dns round robin
289:49 - load balancing and it's conceptually
289:52 - splitting up the load of the pings so
289:54 - that each computer that is responding
289:56 - only gets a third of the requests so i
289:59 - guess it's fair to say that all
290:01 - clustering is load balancing but not all
290:04 - load balancing is technically clustering
290:06 - because clustering is a specific way
290:07 - that computers work together to
290:10 - accomplish a task now is a hot dog a
290:12 - sandwich i gotta leave that one up to
290:13 - you i have no idea but i hope this has
290:15 - been informative for you and i'd like to
290:17 - thank you for viewing cron jobs are
290:19 - pretty much the linux equivalent to like
290:21 - the task scheduler in windows now
290:23 - there's some really cool things that we
290:25 - need to understand and that's how to set
290:27 - up the scheduling which is kind of
290:28 - complicated but also very powerful and
290:31 - i'll be honest it's kind of fun the
290:33 - other thing i want to point out though
290:34 - is that there are pre-made folders that
290:36 - you can just drop scripts in and they
290:38 - will execute at a regular interval i'll
290:40 - show you those but first let's talk
290:41 - about how we set up the schedules
290:43 - because it can be intimidating but like
290:45 - i said it's not that bad and it's
290:47 - actually kind of fun so the scheduling
290:50 - fields which we'll look at in practice
290:52 - are separated into five different fields
290:54 - so we have minute hour day of the month
290:56 - month of the year and day of the week
290:59 - and how it works is for example this
291:02 - first line that i have has all asterisks
291:04 - and this means everything so every
291:06 - minute of every hour of every day of the
291:08 - month of every month of the year of
291:10 - every day of the week it's going to
291:11 - happen so this means every minute for
291:13 - all of eternity whatever task we
291:15 - schedule with this string is going to
291:18 - execute so every minute it's going to do
291:20 - whatever you tell it to do
291:22 - now there are some shortcuts we can use
291:24 - for example down here i have asterisk
291:26 - divided by five this means every five
291:29 - minutes now we could actually spell it
291:31 - out we could say zero comma five comma
291:33 - ten comma fifteen all the way to fifty
291:36 - five but i don't really like to do that
291:38 - because it's a big mess and you know one
291:40 - field would be this entire big string of
291:43 - numbers so rather than do that we can
291:45 - just say asterisk divided by five and
291:48 - this is going to be every five minutes
291:51 - during the third hour of the day so this
291:54 - means at am 305 a.m 3 10 a.m 3 15 a.m
291:58 - but once it gets to 4 a.m it's gonna
292:00 - stop doing it okay so this is during the
292:02 - third hour every five minutes every day
292:05 - of the month every month year every day
292:06 - of the week so this means every day it's
292:08 - going to do this but only between 3 a.m
292:10 - and 3 59 a.m and every five minutes okay
292:14 - now this one is very very very specific
292:16 - this says two minutes after the fourth
292:19 - hour so two and four means at 402 am
292:23 - on the 13th
292:25 - of july but only when that 13th of july
292:29 - lands on a tuesday the day of the week
292:31 - goes from zero to six so two is a
292:34 - tuesday right sunday is zero monday is
292:36 - one tuesday is two so this means every
292:39 - july 13th at 402 am if it happens to
292:43 - also be tuesday so this is only going to
292:46 - execute every few years when july 13th
292:49 - happens to land on a tuesday and then
292:50 - down here this one zero minutes after
292:53 - the hour so this means that six a.m
292:55 - precisely on
292:57 - every day of the month every month of
292:59 - the year
293:00 - days one through five so this means
293:02 - monday through friday at six a.m it's
293:05 - going to execute whatever task okay so
293:08 - this is basically the way of saying six
293:09 - a.m every weekday because we've
293:11 - specified one through five on the days
293:14 - of the week over here pretty cool right
293:16 - let's actually see how it works in
293:17 - practice now i'm on a centaurus system
293:19 - here and i'm root because we're talking
293:21 - about the system-wide cron jobs if we go
293:24 - into etcon.d
293:28 - we're going to see we have a few files
293:29 - in here now any file in here is going to
293:31 - be read by the cron daemon so let's
293:34 - actually look at one let's look at
293:36 - systat all right because this is already
293:38 - in there so we're going to look at
293:40 - systat and here we can see a couple
293:42 - things are scheduled here are the five
293:45 - fields that we just talked about so
293:46 - here's the first field so every 10
293:48 - minutes of every hour every day every
293:52 - month year every day of the week so this
293:54 - is going to be every 10 minutes it's
293:55 - going to execute
293:57 - as root so this field talks about what
294:00 - user it's going to run as and then the
294:02 - rest of it is what it's going to
294:04 - actually do so we have the five
294:06 - scheduling fields who it runs as and
294:09 - then the last part however long it is is
294:11 - what it's going to execute so down here
294:13 - these are commented out but let's
294:14 - pretend it's not this would be every
294:16 - hour at zero past right because this is
294:18 - like at one o'clock two o'clock three
294:21 - o'clock four o'clock it's going to run
294:22 - as root this command down here at 23
294:27 - which is 11 so 11 53 p.m
294:30 - every day see all these are asterisks so
294:32 - every day as root it's going to execute
294:35 - this so you can either add to any of
294:37 - these or really the best thing to do is
294:39 - create your own right just create a file
294:41 - and then put that scheduling the
294:43 - username and what you want it to execute
294:45 - and it will do that now the one other
294:47 - thing i wanted to mention really quick
294:48 - if you go back into
294:50 - the etc folder and let's do an ls and
294:52 - just
294:53 - look for
294:54 - cron we're going to see there are a
294:56 - bunch of folders in here there are cron
294:59 - daily cron hourly cron monthly cron
295:02 - weekly and if we go in there let's go
295:03 - into cron dot
295:05 - daily
295:07 - we're going to see these are just
295:09 - executable scripts these are not timed
295:12 - things right let's look at one real
295:13 - quick so vi log rotate notice there's no
295:16 - like startup star star star star star
295:18 - there's no scheduling in here this is
295:20 - just an executable script that we want
295:22 - to have
295:23 - execute every
295:25 - day
295:26 - so this is going to do the scheduling of
295:28 - everything in here once a day same with
295:30 - monthly same with hourly the kran deny
295:33 - is a way that we can tell a specific
295:35 - user that they're not allowed to use the
295:36 - kron daemon for personal use uh cron.d
295:39 - we looked at actually cron tab is a
295:41 - single file let's look at that one
295:42 - really quick that'll be our last thing
295:44 - etc
295:45 - cron
295:46 - tab and this is the same sort of thing
295:48 - we can put things in here if we want it
295:50 - even tells us like very specifically all
295:52 - the things the five fields the username
295:54 - we want it to run as and then the
295:56 - command to be executed this is the same
295:58 - thing as creating a file in the
296:00 - eccentric cron.d folder you can just add
296:02 - things here and they'll automatically
296:03 - execute now the pre-made folders are
296:05 - very convenient for dropping scripts in
296:07 - that you want to have execute every so
296:09 - often but really the coolest part about
296:11 - cron is just how flexible that
296:13 - scheduling system is and you can figure
296:15 - out how to do that by you know
296:17 - manipulating all of those different
296:19 - fields like we looked at in this slide
296:21 - here so i encourage you to just try to
296:22 - figure out how you would specify a
296:24 - particular time and you know just play
296:26 - with it it's a lot of fun to do
296:29 - when it comes to scheduling events you
296:31 - have a couple options as a personal end
296:33 - user on a linux account you can use
296:35 - crontab personal crontab which you
296:38 - invoke by typing crontab minus e or we
296:40 - can use the at daemon which is a one
296:42 - time thing it's not like for recurring
296:45 - events it's for events that happen just
296:47 - at a specific time so let's go right to
296:49 - the command line because it's not
296:51 - difficult to use either one and
296:52 - scheduling tasks is something that's
296:55 - really really nice to be able to do so
296:57 - here we are on a centos machine i'm
297:00 - logged in just as a user notice i'm not
297:02 - root and the first thing i want to do is
297:04 - look at my personal cron tab now this is
297:07 - a little bit different than a system
297:08 - wide crontab so first we type cron tab
297:11 - minus e and it's going to bring us into
297:14 - an editor okay and now this is my
297:16 - personal crown tab again it's not system
297:18 - wide and it's slightly different because
297:20 - i still have five fields if you're not
297:23 - familiar with the five fields look at
297:24 - the system-wide crontab nugget because
297:26 - it'll explain how this works but we have
297:29 - every 10 minutes of every hour of every
297:32 - day of every month of the year every day
297:34 - of the week so here we have every 10
297:36 - minutes it's going to do something now
297:37 - notice there's no field here that
297:40 - specifies the user in the system wide we
297:43 - have to specify what user it runs as but
297:45 - since this is my personal cron tab it
297:48 - obviously runs as me so we just have the
297:51 - five fields and then we have the command
297:54 - that we want to have it execute at the
297:56 - time scheduled here so what this what
297:58 - happens here is we have echo and this
298:00 - text string and append it to a file
298:02 - called homebob timetracker.log every 10
298:05 - minutes so every 10 minutes it should
298:06 - add a line to our field so let's see if
298:09 - this is actually running because it's
298:11 - been here a while
298:13 - let's quit here
298:14 - if we do ls we can see oh there is a
298:16 - file time tracker.log and if we look at
298:19 - it well sure enough it looks like about
298:20 - a half hour has gone by since i created
298:22 - that crontab entry and it's been adding
298:25 - to this file if we do ls minus l we can
298:28 - see the last time that was touched was
298:30 - at 18 10. so if we waited around until
298:33 - 1820 it would do the same thing again it
298:35 - would add another line to it so that's
298:38 - how you do a recurring event using a
298:40 - personal crontab now if you just have
298:42 - something you want to have execute one
298:44 - time let's clear the screen we can use
298:46 - the at daemon and first i'm going to do
298:49 - something really quickly so we'll say at
298:51 - and then can specify the time and this
298:54 - is what's nice it's very flexible we
298:56 - could say tomorrow we could say next
298:59 - week and it will interpret all of those
299:02 - different commands it uses a lot of
299:04 - fuzzy logic to figure out what you want
299:06 - i'm going to say at now plus one minute
299:10 - and then we're gonna get this at prompt
299:12 - which now allows us to execute something
299:15 - so i wanted to do echo
299:17 - this was a one off and i want to append
299:21 - that to home
299:23 - bob time tracker dot log press enter and
299:27 - now we could do another thing we could
299:29 - have like a whole list of things we
299:30 - wanted to do at now plus one minute but
299:33 - i'm just going to do control d
299:35 - and that will put it in queue so it says
299:37 - job4 is in queue and it's going to
299:39 - execute at 18 13. okay now if we type
299:42 - atq
299:44 - oh it already happened dog on it
299:47 - first of all let's look cat time tracker
299:50 - look at that it did it right it put it
299:51 - to the end of the file but i need to do
299:53 - another one so at now plus one minute
299:57 - again and i'm gonna say echo hello
300:01 - into
300:02 - home bob time tracker dot log ctrl d
300:07 - atq ah there we go okay did it in time
300:10 - so what this shows us is the queue of
300:12 - things that at is going to run so job
300:14 - number five is scheduled for thursday
300:16 - june 20th at 1814 and the user bob is
300:20 - who's doing it so if we keep pressing at
300:23 - q
300:23 - once the time rolls around it's going to
300:26 - execute that and then go away because
300:27 - it's a one-off right we could do
300:29 - multiple things we could say at tomorrow
300:32 - then i want to say echo
300:34 - test tomorrow i spelled it wrong but
300:37 - that's okay on home bob time tracker.log
300:41 - control d on a blank line now if we do
300:44 - at q we're going to see well look job 5
300:46 - executed but job 6 is going to wait
300:48 - until tomorrow and it does it the same
300:51 - time tomorrow so 24 hours from now and
300:54 - tomorrow at 18 14 it's going to do that
300:56 - command test tomorrow okay now let's say
300:59 - we don't want to do that we want to
301:00 - change our mind well then we can say at
301:02 - rm job six and now at q is gonna say
301:06 - there's no jobs because we've deleted
301:08 - job number six that was going to execute
301:10 - tomorrow but if we look at time tracker
301:12 - look at that sure enough hello was put
301:14 - there along with that this is the one
301:15 - off and if we wait around until the next
301:17 - 10 minutes pass our cron job is going to
301:19 - add another 10 minutes has passed on to
301:21 - the end of this file and that's how we
301:23 - can schedule things with crontab for a
301:25 - repeating task like these or just a
301:28 - one-off task like this by using the at
301:30 - daemon it's great to be able to do
301:32 - system-wide things using cron
301:35 - but i personally like the fact that you
301:37 - can do it as a personal end user using
301:40 - cron tab minus e and it's going to do it
301:43 - just as your user so you don't have to
301:45 - become root or worry about escalating
301:47 - privileges it's going to just execute it
301:49 - as you even if you're logged out same
301:51 - with the at daemon even if we're not
301:53 - logged in it's still going to execute it
301:55 - at the given time
301:57 - working with multiple processes on linux
302:00 - is really really easy because you can
302:01 - put them in the foreground or the
302:02 - background and interact with them
302:04 - however you want now there are some
302:05 - tools that may not seem intuitive at
302:08 - first but once you get the hang of using
302:10 - them they're really really easy and
302:11 - there's also some keystrokes that we're
302:13 - going to have to learn and a couple
302:15 - tricks that'll allow us to do things
302:16 - that we normally couldn't do so let's go
302:18 - right to the command line because this
302:20 - is the kind of stuff you have to
302:21 - experience in order to really understand
302:23 - so first of all i am on the command line
302:26 - and i'm going to show you how to put a
302:27 - process in the background now i'm just
302:29 - going to use a simple process called
302:31 - sleep the sleep command if you're not
302:32 - familiar with it it just pauses right so
302:34 - if we say sleep one it's going to sleep
302:36 - for one second and then it's going to be
302:37 - done so i'm going to say sleep for a
302:40 - whole bunch of seconds which will like i
302:41 - don't know that's probably a couple
302:42 - hours or something and then i'm going to
302:44 - put the ampersand after it now what's
302:47 - going to happen is notice it's done a
302:49 - number one and then this is the actual
302:52 - process number so our job number is one
302:54 - now we can do that with another one
302:56 - sleep two two two two two
302:58 - put that in the background and now we're
303:00 - gonna see we have job number two in the
303:01 - background with this process id and
303:03 - they're just running in the background
303:04 - if we type jobs we can see sure enough
303:07 - there they are there's sleep one one one
303:08 - there's sleep two two two job one and
303:10 - job two now if we want to start going
303:13 - and using one again like let's say we
303:14 - wanna bring one in the foreground we
303:16 - just type f g and then the job number so
303:19 - fg two and it's going to bring us it
303:22 - tells us what the command is that's
303:23 - running and here we are we're just at
303:25 - the command line here if we wanted to
303:26 - stop this we'd have to do control c it
303:28 - stopped that process and now if we do
303:30 - jobs see there's only the one job
303:32 - running it's actually really cool now
303:34 - what if we did a job like we did
303:36 - sleep
303:37 - 3333 and pressed enter and i'm like oh
303:41 - man i really wish that was in the
303:42 - background i didn't mean for it to just
303:44 - like hold my command prompt here well
303:46 - what we can do is do control z and it
303:50 - stops it and puts it in the background
303:52 - so see it says stopped if we type jobs
303:54 - we can see this first one that we did is
303:56 - still running but this one is stopped so
303:58 - if we want to make it run in the
304:00 - background we have to say
304:02 - b g for run in the background job number
304:05 - two
304:06 - and now if we do jobs we're gonna see
304:08 - now they're both running in the
304:10 - background so bg and then the job number
304:12 - will start that background task running
304:14 - fg will bring it out of the background
304:16 - and bring it right to our interactive
304:18 - terminal okay so that's kind of neat
304:20 - it's a way we can create them we can put
304:22 - them in the background if we've started
304:24 - them and we don't and we want to put
304:25 - them in the background just ctrl z will
304:27 - suspend it briefly and then we can tell
304:30 - it to go in the background by doing bg
304:32 - and then the job
304:34 - number so let's do foreground one i'm
304:37 - going to do control c
304:38 - foreground two control c and now jobs we
304:41 - have no more jobs running now there is a
304:44 - problem because if we have a job running
304:46 - in the background like i have a command
304:48 - here called my hello all right and all
304:50 - it does is every two seconds it prints
304:53 - hello on the screen
304:55 - all right so i'm gonna ctrl c what if i
304:57 - wanted to put my hello in the background
305:00 - and it's going to run in the background
305:02 - i was still going to put it out on the
305:03 - screen every hello but it's running in
305:04 - the background if we do exit it's still
305:07 - running in the background but the
305:08 - problem comes where if we log out
305:12 - and then we log back in
305:14 - we open a terminal window
305:16 - and we do a ps aux grip for my hello
305:21 - we're gonna see it's no longer running
305:23 - this is just actually the grep process
305:24 - that it actually found here but notice
305:26 - it's not running in the background
305:28 - anymore and if we wanted it to stay
305:30 - running just putting it in the
305:32 - background wouldn't work we'd have to
305:33 - use a program called no hup because
305:36 - here's the problem when we log out of
305:38 - the system up here it actually sends a
305:40 - hang up interrupt to all the running
305:42 - processes and the hang up or hup
305:46 - tells it to stop running well we can run
305:48 - a program by saying no hup which means
305:51 - don't hang up when the user logs out
305:53 - what program we want to run so my hello
305:56 - and then ampersand in the background
305:59 - it's going to put it in the background
306:00 - just like before we can see it jobs
306:02 - there it is running but it says it's
306:04 - ignoring the input and appending the
306:06 - output to nohup dot out all right so i'm
306:08 - going to exit i'm going to log out and
306:11 - then i'm going to log back in
306:15 - open up a terminal window
306:17 - i want to notice two things one if we do
306:18 - a ps minus aux grip for my hello it's
306:22 - still going to be running see here it is
306:24 - it's still running because we ran it
306:26 - with no hup but here's another cool
306:28 - thing if we do an ls see this nohup dot
306:30 - out let's look at that
306:32 - all of the output was put into that file
306:36 - so all of the hellos were put into that
306:38 - file so we know what's going on plus
306:40 - when we logged out it didn't stop
306:41 - running so not only is it really easy to
306:43 - handle processes when you're on the
306:45 - linux command line it's also possible to
306:47 - do a couple cool things like no hub if
306:49 - you want to make sure it doesn't quit
306:51 - when you log out and also control z of a
306:54 - running process which will put it in the
306:55 - background suspended and then you can
306:57 - run bg to make sure that it continues
306:59 - executing in the background
307:02 - finding specific information about local
307:04 - devices on your system can be a little
307:06 - bit challenging but thankfully there's a
307:08 - bunch of tools that will help us along
307:09 - the way d message is one that we can use
307:12 - that will kind of see how things are
307:13 - going in real time if you plug something
307:15 - in d message is going to show you the
307:17 - results of plugging that in but then
307:19 - there's a whole suite of ls tools if you
307:21 - will they start with ls it's ls usb
307:25 - lspci lsdev msblk ls cpu all of these
307:30 - tools are going to be able to be used to
307:33 - find out information about hardware on
307:35 - our system and there's a nice trick to
307:37 - remember these so that you don't have to
307:38 - actually remember pci dev blk cpu all of
307:41 - these different things so let's check
307:42 - that out and see if we can find the
307:44 - various hardware that is in our system
307:47 - you have to be root to use most of these
307:49 - some of these you can use as an end user
307:50 - but since we're going to be looking at
307:52 - all of them i became root just so that
307:54 - we get a better view of what's going on
307:56 - now first of all d message you just type
307:58 - d message on the command line and it's
308:00 - going to show you things as they happen
308:02 - and this is the location that we'll see
308:05 - things happen if we make changes to the
308:07 - system like if we plug in a usb drive or
308:10 - plug in a sata drive or something like
308:11 - that a new mouse it's going to show up
308:13 - here and it's going to give us
308:14 - information about the particular device
308:16 - but if something's already plugged in or
308:18 - you just want to see like what's built
308:20 - into the system that's where the ls
308:22 - tools come into play so let's clear the
308:24 - screen
308:25 - and what i like to do is just do ls and
308:27 - then hit tab a couple times and that's
308:30 - the trick because really tab completion
308:32 - is vital here there are so many of these
308:34 - commands now we know ls just means list
308:37 - like the file directory but they've used
308:39 - this same tool or the same keystrokes to
308:42 - prefix a bunch of commands like ls blk
308:46 - for example which will show us the block
308:48 - devices it will list the block devices
308:52 - on our system if we do that it's going
308:54 - to show us all of the block devices that
308:56 - we have like the floppy drive all of
308:58 - these are virtual loopback devices that
309:00 - were created down here these are our
309:02 - actual drives on our system sda we can
309:04 - see right here it's mounted on forward
309:06 - slash sda1 is then we have a bunch of
309:09 - other drives down here that aren't being
309:11 - used right now but they're all 10
309:12 - gigabytes in size we use those and we
309:14 - set up raid before but this shows us all
309:16 - of the block devices so if we do ls and
309:19 - hit tab a couple times again let's look
309:21 - at the next one we have ls block we have
309:23 - ls cpu
309:25 - now i actually really like this one
309:26 - because it will give you all of the
309:28 - information about the cpu in the system
309:31 - so we can here we can see here that it's
309:33 - a 64-bit processor little endian it's an
309:37 - intel
309:38 - it actually shows us the actual
309:40 - processor itself the model number it
309:42 - shows us the clock speed all sorts of
309:44 - stuff that we have vtx enabled so we can
309:47 - do virtualization and it shows us all of
309:50 - the awesome things that our cpu has
309:52 - including all the flags that it supports
309:55 - when we're compiling things anyway
309:56 - that's ls cpu if we hit ls and tab a
309:59 - couple times again i'm going to do this
310:00 - every time because it's the quickest way
310:02 - to see what's there we have lsdev i want
310:04 - to do this you have to be root to do
310:06 - this and what this shows us this might
310:08 - not be as useful as it was years and
310:10 - years ago and we would have to
310:11 - troubleshoot hardware more frequently
310:13 - but what this is going to show us is the
310:16 - the device like what device it is like
310:17 - our floppy disk our keyboard a pci port
310:20 - here and it's going to show us what
310:22 - direct memory access number it uses what
310:24 - dma what irq number it's using uh the io
310:28 - ports meaning like in memory what io
310:30 - parts of memory or what parts of memory
310:32 - is it using for i o and it will show us
310:34 - all these things now if we're trying to
310:35 - find conflicts on our system this might
310:37 - be useful but i'll be honest i've never
310:39 - once had to use this in practice it's
310:41 - important to know that it's there though
310:43 - lsdev will show you all the devices on
310:45 - the system now if you do ls tab tab
310:48 - one that i actually do use fairly often
310:50 - is
310:51 - lspci where is that up here lspci so oh
310:55 - pci will show us all the pci devices so
310:57 - here we have a vga compatible controller
311:00 - this is the acpi controller the ide
311:02 - controller
311:04 - so these are the pci devices that are
311:06 - plugged into our system uh we can do ls
311:10 - usb now i don't have anything plugged
311:12 - into our usb ports but if we did lsusb
311:15 - would show us what's there same thing ls
311:18 - pcmcia if you're a laptop user you might
311:20 - have things that show up if you type
311:22 - that and one last one we'll look at is
311:24 - ls mem and this will just give us the
311:26 - range of the memory and how it's being
311:28 - used and where it is in our system but
311:30 - really the big big pull away from this
311:32 - is ls tab tab and it'll show you all of
311:35 - the different ways that you can look at
311:37 - the devices on your system so whether
311:39 - you're trying to get an inventory of the
311:40 - things that are currently on your system
311:42 - or you want to see changes as they
311:43 - happen in real time there's some really
311:45 - simple built-in tools on our local linux
311:48 - machine for detecting and looking at
311:50 - specifics on devices
311:52 - you ever try to solve a problem only to
311:54 - make it worse yeah me too for example
311:57 - we had a wall with a nail in the wall
312:00 - and that nail would get loose after
312:02 - hanging stuff on the nail for a long
312:03 - time because it was just in the thin
312:05 - layer of drywall this nail just went
312:07 - into the drywall and then the more we
312:08 - hung stuff on it the more it got loose
312:10 - so i thought i would make things better
312:12 - by getting rid of the nail drilling a
312:14 - hole in the drywall and then getting one
312:16 - of those really nice hollow wall anchors
312:19 - that you could then put a nail or a
312:20 - screw into and it would be really nice
312:22 - and sturdy the problem is i drilled the
312:24 - hole way too big the hollow wall anchor
312:26 - didn't work at all and then i just had
312:27 - this gigantic hole in the wall that my
312:29 - wife was really upset about that's kind
312:32 - of what happened in the world of linux
312:34 - virtual file systems and let me tell you
312:36 - what i'm talking about here because the
312:38 - idea of virtual file systems has been
312:39 - around since the unix days basically
312:41 - it's a file system that's created when
312:43 - the system boots up and one of the
312:45 - really popular ones is called proc and
312:47 - what this was used for is process
312:49 - information like of running apps and
312:51 - running programs on the system they
312:53 - would store all of their runtime
312:55 - information in this virtual folder in
312:56 - memory called proc and then people said
312:59 - hey that's a really neat place to store
313:00 - things like process id number 181 for
313:03 - that application and 2 5 5 6 for that
313:06 - application what if we also put things
313:08 - about like the cpu in there or the
313:10 - network card in there and then people
313:12 - started adding things to the proc folder
313:14 - it started to get a little bit confusing
313:17 - and so we thought hey wouldn't it be a
313:19 - great idea if we separated all of that
313:22 - kernel information so that process
313:24 - information was stored in the proc
313:25 - folder and kernel information was stored
313:28 - in assist folder they're both virtual
313:31 - file systems that work the same way but
313:33 - some organizations seem to make a lot of
313:35 - sense right now the same thing with dev
313:37 - dev is a folder that we thought why
313:38 - don't we start putting things like
313:40 - information on different devices like
313:42 - hard drives or
313:44 - mouses mice mouse mice mouses anyway
313:48 - when we put those things into the dev
313:50 - folder oh here's the issue the proc
313:53 - folder had been around for a long time
313:54 - and it already had a mix of things in it
313:57 - so we said okay we'll keep all of those
313:59 - things and then we'll just put new
314:01 - things in the sys folder the problem is
314:03 - there's not that many new things so
314:04 - while the sys folder is very well
314:07 - maintained and very neat and organized
314:09 - it doesn't contain a lot of the things
314:11 - that we use on a daily basis because
314:12 - they were already in the proc file
314:14 - system and now they're there for
314:15 - backwards compatibility
314:17 - now that doesn't mean proc and sys are
314:19 - any less useful if we go into the proc
314:22 - folder and do an ls we're going to see
314:24 - here are all of those numbers these are
314:25 - the process ids that i talked about
314:27 - right and if we do like an ls of all the
314:30 - process id 47 inside here it's going to
314:32 - be all of the various things that this
314:35 - particular application is doing like
314:36 - it's it's memory io what's mounted uh
314:39 - the status all of the things about this
314:41 - running thing are in here so if you want
314:43 - to find out information about whatever
314:45 - process id number 47 is you can look at
314:48 - these various things but notice over
314:50 - here on the right hand side there's also
314:52 - a bunch of things that aren't process
314:54 - ids and that's where like mdstad mem
314:56 - info
314:58 - cpu info let's look at that so if we
315:00 - look at cpu info this is going to just
315:02 - be a complete breakdown of the cpu in
315:05 - the system right now that's running this
315:07 - is our current cpu which is really
315:09 - useful information but it might make
315:11 - more sense if it was over here in the
315:13 - sys folder because the sys folder is
315:16 - very well laid out right there's not a
315:17 - whole bunch of crud here this is laid
315:19 - out in very nice sections we could go
315:21 - into oh let's say the kernel folder and
315:24 - inside the kernel folder it's separated
315:26 - into various things about the kernel
315:28 - like the current configuration
315:30 - all of these things are nice and
315:31 - organized but it's not all inclusive
315:34 - meaning it doesn't contain a lot of the
315:36 - information if we scroll up that's still
315:38 - stored in the proc folder so here's how
315:41 - it ends up working a lot of the
315:42 - interactive stuff is going to be in the
315:44 - proc folder because it's always been
315:46 - there but a lot of the information about
315:48 - the running kernel is very organized
315:50 - well in the sys folder and that's where
315:52 - a lot of times programs will look if
315:54 - they want information about the current
315:56 - running kernel like do i work on this
315:57 - program well i don't know let's see you
315:59 - know information about hypervisor and
316:01 - kernel modules and all of those things
316:04 - it'll look in the cis folder so
316:05 - generally you'll find yourself in the
316:07 - proc folder more often all right now one
316:10 - other place that it's worth looking in
316:11 - is the dev folder and this just shows
316:14 - about system devices right these are
316:16 - hardware devices that are plugged into
316:18 - the system either like soldered in like
316:20 - the real-time clock or hardware-like
316:22 - hard drives that we put in here and this
316:24 - is going to be information binary
316:25 - information generally available to the
316:27 - system talking about the various bits of
316:30 - hardware that are plugged in now this is
316:32 - fairly well organized too and a lot of
316:34 - times programs will look here to find
316:36 - out information like hey is there
316:38 - hardware that is going to be compatible
316:39 - with my software well this is where it
316:41 - would look for hardware information so
316:44 - while it sounds like it's a big mess
316:45 - honestly it's not that bad there's three
316:47 - main places to look the proc folder
316:50 - which is what we see here the sys folder
316:52 - which we see here and then the dev
316:55 - folder which we see here all of them
316:57 - contain different information sys and
316:59 - proc have a little bit of crossover
317:01 - there's going to be some kernel
317:03 - information insist that's already living
317:05 - in the proc file system but if you're
317:07 - going to be interacting it's mostly
317:09 - going to be in the proc file system and
317:11 - applications are going to be hitting the
317:12 - sys file system pretty hard now i know
317:15 - that's a lot of information about linux
317:17 - virtual file systems the key takeaways
317:20 - are that we need to know information
317:21 - about the running kernel are going to be
317:23 - in generally one of three places in the
317:25 - virtual file system that happens when
317:27 - the computer boots up the proc file
317:29 - system the sys file system and the dev
317:31 - file system if you can look in all three
317:33 - of those places you're going to find
317:35 - what you're looking for but they might
317:37 - be spread out in places that don't make
317:38 - a whole lot of sense just because we
317:40 - keep things there for backwards
317:42 - compatibility i hope this has been
317:44 - informative for you and i'd like to
317:45 - thank you for viewing on a linux system
317:47 - the printing is going to be handled by
317:49 - cups which is common unix printing
317:52 - system which interestingly is actually
317:54 - owned by apple of all people but
317:55 - nonetheless this is what every modern
317:58 - linux distribution is going to use to
317:59 - handle the printing now the nice thing
318:01 - is there are some backwards
318:02 - compatibility command line tools so we
318:05 - can print things from the command line
318:07 - and if you don't have a gui to actually
318:09 - install the printer like if you don't
318:11 - have gnome or kde or anything like that
318:13 - installed you can use a really nice web
318:15 - interface that will allow you to
318:17 - interact with the cup system and install
318:19 - modify and do all the things you need to
318:21 - do to a printer but we're going to look
318:23 - at the web interface really quickly just
318:25 - to show you where it is and i want to
318:26 - show you how to use the command line
318:27 - tools so you can print from a command
318:30 - line system even if it's a headless
318:31 - system on a rack now i'm logged in here
318:34 - on my local ubuntu computer on my local
318:36 - network and you'll see up here i went to
318:38 - localhost
318:40 - port 631
318:42 - now that's important because cups
318:44 - listens for incoming connections on port
318:46 - 631 and if you go there it'll allow you
318:48 - to log in so you can do things like
318:50 - administration for example we are here
318:53 - and i have one printer installed we can
318:54 - click on it'll give us information about
318:57 - this particular printer called office
318:59 - underscore laser we can see it's a
319:01 - socket-based connection on this ip
319:03 - address on port 9100 and basically it's
319:06 - installed on the computer i actually
319:08 - installed it using the web interface
319:10 - instead of going into the gui on this
319:12 - computer and installing it with the
319:14 - ubuntu tools i used the web-based
319:16 - interface to make sure that it would be
319:18 - able to install correctly and it did
319:20 - once we're here we can actually do
319:22 - things you know from firefox like print
319:24 - the page but if we're on the command
319:26 - line things are a little bit different
319:28 - there are a couple tools that start with
319:29 - l p and if we hit lp and then hit tab a
319:32 - couple times for tab autocomplete we're
319:34 - going to see some of those here now
319:36 - they're not all things we're going to
319:37 - look at right now but mainly lpr
319:41 - is going to send commands to the printer
319:44 - lpq is going to show us the queue of
319:46 - things waiting to be printed so if we
319:48 - type lpq we're going to see office laser
319:50 - is our default printer it says it's
319:52 - ready but there are no entries meaning
319:53 - there's nothing that's currently being
319:55 - printed now we can use lpr to print
319:58 - things in a couple different ways we
320:00 - could actually just say echo this
320:03 - is a test
320:05 - and we can pipe that into the lpr
320:08 - command and if we look over here on my
320:10 - printer it's going to print this page
320:12 - right out
320:17 - and we can see sure enough it says this
320:19 - is a test so it'll do text just like
320:22 - that now we don't have to do line by
320:24 - line we can also do something like in my
320:26 - folder here i have a full file called my
320:28 - document so if we look at my document
320:31 - you can see it's just a string of text
320:33 - things and we can say
320:35 - lpr and then the name of the text file
320:38 - my document we'll print that and as it
320:40 - prints out on the page if we look at lpq
320:43 - we're going to see we have a job right
320:45 - here job number 10 is that we could use
320:47 - lprm if we're quick enough and get rid
320:49 - of job10 oh it's already completed but
320:51 - that is how we would delete a job that
320:53 - we had already sent so if there's a
320:54 - whole bunch of things queued up and
320:56 - something's wrong we can use lprm in
320:59 - order to fix it and if we come back over
321:01 - here we're gonna see sure enough it
321:02 - printed out that entire file for us
321:04 - right there on the page so it's really
321:06 - easy to print using the command line
321:08 - once it's set up with cups and
321:11 - thankfully cups is really easy to set up
321:13 - too because it's all web-based and that
321:16 - web interface honestly is very powerful
321:18 - we can use it to queue up jobs to stop
321:20 - jobs do test pages we can use cups for
321:23 - gui environment printing but the most
321:26 - important thing to realize is that we
321:28 - have access to the cup system by using
321:30 - the command line tools that are
321:32 - installed on our system
321:35 - udev is the user space device manager
321:38 - that has replaced dev fs and older linux
321:41 - systems now what's really cool is it
321:44 - uses sysfs which is a virtual file
321:46 - system that has information about the
321:48 - hardware and then it follows rules in
321:51 - order to keep devices with consistent
321:54 - names now what i mean by that is if
321:56 - you've ever had a linux system with
321:58 - multiple anything but we'll say hard
322:00 - drives the order that you used to put
322:02 - them in the system depended on the name
322:05 - they would get so the first one the
322:06 - system recognized would be sda then sdb
322:10 - and then sdc but on the next boot if
322:13 - they came up in like a different order
322:15 - while all of a sudden this one might be
322:17 - sda and this one would be sdb and this
322:20 - one might still be sdc and those are
322:23 - real pain in the butt so what udev does
322:25 - is it creates devices based on specifics
322:29 - with the hardware so it'll take its like
322:31 - uuid and it will create a solid
322:35 - consistent device that that's always
322:36 - going to be so if you put them in a
322:38 - different order or the buses come alive
322:40 - in a different order on boot they're
322:42 - still going to be the same devices which
322:44 - makes things a lot simpler when it comes
322:46 - to mounting drives and things like that
322:48 - now it also allows us to do some other
322:50 - things like create rules so first of all
322:53 - i want to show you some of the things
322:54 - you can do if you're root you have more
322:56 - access to do this so we're going to say
322:58 - u dev adm this is the administrator
323:01 - command that allows us to do some things
323:03 - with the udev system so the first thing
323:05 - i want to do is say udev info
323:08 - let's actually look at
323:11 - dev
323:12 - sr0 which is actually our dvd drive
323:16 - and it gives us all sorts of information
323:18 - specific to this drive so we'll see a
323:20 - couple things like let's scroll up we
323:22 - see this is n means the name so this is
323:25 - the name that it has assigned it this is
323:28 - the place in the pci device where it
323:29 - exists this is all the specific
323:31 - information about it and if we look
323:35 - lsdev
323:37 - grep sr0 we're going to see sure enough
323:40 - it's right in the dev folder sr0 it's
323:42 - dynamically created when it detects it
323:45 - and puts it in now sr0 is not terribly
323:48 - useful at least for me to recognize a
323:51 - hard drive or a dvd drive when it's in
323:53 - the system now i can see it mounted
323:55 - right here but that doesn't help me sr0
323:57 - doesn't mean dvd to me so we could do
323:59 - something like make a rule that creates
324:01 - a shortcut to it every time it
324:04 - recognizes it in the system so we're
324:06 - going to go into the etc udev rules.d
324:10 - folder and in here let's see what there
324:12 - is there's one called snap core rules
324:14 - but i'm going to create a new one i'm
324:16 - going to say vi
324:17 - we'll say number 10 so it loads it first
324:20 - sean.rules
324:22 - sean rules nice anyway it has to end in
324:24 - dot rules and then we're going to create
324:26 - our own rule now the format here is
324:28 - something that you just kind of have to
324:30 - get the hang of but the kernel is equal
324:33 - to
324:33 - sr0 meaning this is what the kernel
324:36 - knows the device as
324:38 - it's in the
324:39 - sub system
324:41 - is equal to
324:43 - block which it actually showed us when
324:45 - we did that command and then what i want
324:47 - to do is create a sim link and i want to
324:50 - call that sim link my underscore dvd
324:54 - okay so what we've done is created this
324:56 - rule anytime it has a device called sr0
324:58 - in the block subsystem meaning like a
325:00 - block device like a hard drive or a
325:02 - cd-rom i wanted to create a sim link
325:04 - called my dvd in the dev folder so i can
325:06 - reference it there instead of
325:08 - remembering sr0 so let's save this
325:12 - now we could reboot the computer or we
325:14 - could just do udev adm
325:17 - trigger
325:18 - okay and now if we go into the dev
325:20 - folder
325:21 - and we do an ls
325:23 - we should see in here
325:25 - look at that my dvd it's created that
325:28 - sim link to well in fact let's do ls
325:31 - minus l grab dvd
325:33 - my dvd is a link to sr0 it looks like
325:36 - there already was a dvd shortcut to sr0
325:40 - but we created a new one by making our
325:43 - own udev rule called my dvd and that's
325:45 - pointing to sr0 and we can use this when
325:48 - we're referencing the device in anything
325:50 - that we want like mounting and that sort
325:51 - of a thing and the cool part is this
325:53 - isn't just a sim link sitting on our
325:55 - system this is something that's going to
325:56 - be created every time the dvd is
325:59 - recognized
326:00 - and set up by the system so on boot even
326:03 - though the dev file system is a virtual
326:05 - file system it's always going to have
326:07 - that sim link my dvd pointing to sr0 now
326:10 - we just graze the surface of the things
326:12 - we can do with udev rules we can
326:15 - actually make things happen when certain
326:16 - usb drives are plugged in that sort of a
326:19 - thing but one of the key things i want
326:20 - you to take away is that it's really
326:22 - smart and it uses sysfs in order to know
326:26 - information about the drivers and
326:28 - everything else how it interacts with
326:30 - the kernel it uses the information in
326:32 - sysfs to create the entries in the dev
326:36 - folder i hope this has been informative
326:37 - for you and i'd like to thank you for
326:39 - viewing
326:45 - you

Cleaned transcript:

linux is a popular operating system for server administration because it's secure stable and flexible in this course sean powers from cbt nuggets will detail every part of configuring monitoring and supporting a server setup that runs the linux operating system this course will teach you everything you need to know to configure linux servers including the boot process kernel modules network connection parameters localization groups and more let's get started bios and uefi are two tools that basically do the same sort of thing but they can be a little bit confusing when it comes to how do you know which one to use on a computer and and do i need to support one or the other or both or the nice thing is you usually either have bios or uefi now bios is an older program it just stands for basic input output system whereas uefi is the new kit on the block and this stands for unified extensible firmware interface which sounds confusing but really it's just the the way that we can interact between the hardware and the operating system now let's say that we have two different vehicles now these are two obviously very very different vehicles one of them is a sports car and one of them is an awesome yellow volkswagen beetle i actually have a volkswagen beetle that's yellow like this and it's awesome but nonetheless you could be a sports car person either way even though they're ridiculously different vehicles they both have some common interfaces right they're both going to have brake pedals they're both going to have steering wheels they're both going to have windshields and those interfaces are fairly common across all vehicles now the brakes in the underlying system are going to be different this sports car probably has really nice disc brakes whereas these old volkswagens have drum brakes now drum brakes aren't as good but the interface itself is very very similar you push the brake pedal and you stop and that's kind of what bios and uefi are they're interfaces between the hardware and the operating system itself now they do work a little bit differently so here we have our hard drive on our system now using bios like the old method of booting a computer you would have like the very first sector on the hard drive would be the boot sector and that's where the mbr the master boot record would live and then that would tell the computer where the partitions are and point it to where to boot now there's a lot of limitations with bile so you could only have four partitions using the bios and mbr combination there are some hacks to get around that you know they would take a partition and do extended partitions inside that but that's a whole nother nugget still there was this limitation also a limitation of size with the drive how much this boot sector or this master boot record can actually reference it can be a small amount like two terabytes instead of exabytes of data and so uefi is a replacement for the bios technology and rather than just have the single boot sector what it does is there's an entire partition on the system and that partition is where all of the boot code is for whatever operating systems might be on the computer so rather than just the boot sector pointing to you know the rest of the hard drive this is an actual specialized partition on the computer and that's where the uefi code is stored also it uses a different partition scheme so you can have tons and tons of partitions and it can address a much larger hard drive there's also other things like secure boot that uefi supports that bios doesn't basically just know that uefi is the replacement for bios it replaces the functionality of connecting the hardware to the software of the operating system now the good news is if you have a computer that's older or even some new ones so come with bios you can still get around a lot of the limitations because there are hacks that will let you use really big hard drives or hacks that will allow you to do some of the things that you can't normally do out of the box but most computers now are coming with uefi and from a from an installer point of view there's very little you have to do because the operating system is going to say okay i was booted using uefi so i'm going to create a partition a uefi partition and i'm going to put all the boot code in there so from your standpoint from the end user or the installer standpoint there's very little difference but under the hood a lot of new cool stuff is going on and that's why uefi is kind of the way of the future remember when you were a kid and in the doctor's waiting room there were these magazines and one of the games inside the magazines was that you had to find the difference in one of a bunch of similar looking figures like for example here are what i have nine different jokers and one of them is a little different now it's not terribly difficult to see which one is different here if you watched while they appeared this one has a tiny little spot right there on his shirt that's different than all the others so we've been able to identify it but you can see they're very very very very similar well the same thing is true about grub and grub 2 which of course is the next iteration of grub now grub stands for grand unified boot loader but really it's just the way that the computer transitions from bios or uefi booting up conditions into the actual operating system itself it's what tells the computer okay where's my partitions and stuff like that it's very easy to confuse which is which on your system which seems silly but it can be very very embarrassing when you sit down at a system you're like okay so is this running grub or grub 2 because they do the same thing right they both boot the computer but they do have some minor nuanced differences grub of course is older you probably guessed that by it not having a 2 at the end of its name but grub is often called grub legacy because it is older but it's still on a few operating systems not many but a few i think slackware still uses grub legacy now the biggest and easiest way to figure out if you're dealing with grub or grub 2 on your system is to look inside the boot grub folder and if you see menu.lst or grub.com that means you're running grub legacy because grub 2 does not have those configuration files grub 2 has this configuration file grub.cfg now it's easy to confuse cfg.com so i always just look for menu.lst if that exists you're on grub legacy now the thing about grub is it's kind of difficult to modify it was uh it's very easy for automated systems like when you install a new kernel to figure out how to put how to you know update the menu and update the boot code but it's really difficult for the end user to modify that stuff and the boot menu usually when it's booting up it says you know press something in 10 seconds in order to change the way it boots so this usually just appears for you and you can see it now grub 2 is a lot more customizable in fact if you go into the etc default grub file this is a configuration file that's easy to read and it's going to allow you to change the way that it looks when it boots up now grub 2 also has a ton of other cool features it can boot from an iso file from a usb disk it can name or it can identify hard drives based on their uuid or their device like dev sda so it's a lot more advanced than grub one but one of the you know the advancements that's also a frustration is that boot menu that i talked about that comes up with grub it's hidden with grub 2. so it just goes right to the login screen you never see anything from grub 2. and if you don't know this little trick it can be a real bugger to get into that menu if you want to change something during boot so be sure to just hold down the shift key when the computer is booting up and then boom all of a sudden you're going to get into the grub 2 interactive menu that you can change boot code things on the fly it's really really cool i'll show you what i mean but it's not difficult once you know if you have grub or grub 2 grub 2 which allows you to do a lot more configuration stuff now i'm here on an ubuntu system this has grub 2 and if we were to look at let's just look at it etc default grub we're going to see this is just a configuration file we can change these things once you make a change you do have to do sudo update grub you do that it's going to update the boot code inside the boot folder and if we look in that boot folder so go into boot grub type ls i look at that grub dot cfg there's no menu.lst so we know that this is grub 2. so that's how you manage things in grub 2. i'll show you really quickly before we end this is a computer that's turned off if i start this computer and hold down the shift key as long as i hold down the shift key boom we get this menu but if i don't hold that we don't see any menu for us to interact with grub at all now figuring out if you have grub or grub 2 can be a little bit challenging if you don't know that simple trick to look for menu.lst but they both do the same thing they tell the computer how to boot up and how to mount its different partitions it's just that grub 2 is definitely an advancement it's more configurable it does more things and it's easier for the end user to manage i hope this has been informative for you and i'd like to thank you for viewing linux is so flexible it can boot from an incredible number of methods or sources now there are several different things we need to understand about the boot processes like is it hardware based or is it software based and i'll tell you what i mean about that in a minute but there are just this multitude of ways that a linux system can boot it can boot over pxe or pxe boot if you haven't seen that it's just the coolest thing ever it's when it boots up completely over the network we can boot from usb from cd ipxc is a more advanced version of pixie i'll talk about that too and then iso images just like the dot iso file you can actually make a grub 2 entry that will boot directly from the iso even though you don't have it burned to a disk or to a usb drive it can live on your system and you can have a grub menu item that boots directly to that iso image it's really flexible what it can do but that hardware software thing i wanted to talk about because where the boot process is taking place is really important some of it is linux specific some of it is not so first of all let's talk about the hardware things now when i say hardware i mean the bios or the uefi the part that takes place before linux is ever introduced and pxe is one of those things pxc which stands for preboot execution environment is the way that the hardware says okay i don't see a hard drive or i'm not set up to use a hard drive so i'm going to query the network and it just queries a dhcp server and the dhcp server responds with not only an ip address which is what dhcp servers normally do but also a boot file in a tftp location a tftp server is just a place you can store files on a network and then basically the computer then downloads that image that boot image from the tftp server and that's where linux comes into play that kernel or that boot file is the linux kernel so it downloads it off the internet or off the internet um off of your local network and then it puts it in memory and boots itself from there so pxe starts as just a hardware thing and then turns into a software there's also ipxc which is very similar but instead of using tftp to download that thing it allows you to use http which is faster and usually more reliable than the oldfashioned tftp but it's very similar in concept ipxc if your computer supports it then usb this is also right on the hardware now the hardware determines exactly how to boot from the usb but on the usb itself is where the linux code is right so the hardware knows that it can boot from usb same thing with cd and to be quite honest same thing with a hard drive right the hard drive is booted too because the computer itself understands how to do that but when linux comes into play we'll say the software side of things this is once it loads into linux linux does some things on its own like the iso booting with grub2 that's after linux starts right or after grub starts it says okay i'm in grub now what are we going to do we're going to either mount a partition or we're going to look at this iso file and actually use that as our operating system just like it was burned to a cd so some of this stuff is software this is where you like select a kernel this is where you can have mem test so a lot of the stuff is done in software but most things actually all things have to start in the hardware otherwise you're never going to you know get to the point where the software takes control but it's important to know that pxe ipxc these are not linux specific boot methods these are boot methods the computer supports that linux also supports in that they can provide the boot file so i don't want you to confuse pxe network booting with something specific to linux same with usb and cd we can boot windows from usb or cds and it works you know because the hardware supports it if i'm honest probably the most fun way to boot a computer is using pxe only because there's no media right it just boots directly off the network and for some reason that's just really awesome to to be a part of it's really awesome to see but there are multiple ways that you can boot linux and it's important to know that they're all there it's okay if you don't know exactly how to you know boot from an iso file using grub 2. anybody's going to have to google the specifics of that in order to make it work but knowing all these different processes exist for booting a computer that's vital because that will help you learn and help you troubleshoot when you run into a booting issue when it comes to actually booting the linux kernel there are a lot of problems that need to be solved and it's sometimes sort of like a chicken and an egg scenario you have to do one thing but you can't do that thing until the other thing is done and what do you do first and how do you do it it can be really confusing but basically we want to get the full kernel with all of its modules running so the boot process is very complicated and it might seem over complicated but the issue is to try to get the kernel running and then allow it to access the modules that are stored on the hard drive on your system and it ends up being like i said fairly complicated and if i'm honest i've been a system administrator for over 20 years i've passed lots of certification tests and i have never fully understood every step of the process of booting the kernel so you're going to be like the king of the next nerdy party that you go to because you're going to you're going to know all the trivial pursuit answers when it comes to linux kernels so let's look and see what the boot process actually is and then i'll show you on a system where those files live so i have sort of a little flow chart here now a lot of these steps we probably are already familiar with first the computer either has bios or uefi and that's the hardware on the computer which looks for something that grub or grub too it's one or the other here will provide and then that boot code of you know that grub uses points us to the kernel now this is the stuff where it starts to get a little not confusing so much but complicated it's almost like this fine dance that has to be done now norm what the process is normally is we have the actual linux kernel itself which is a file one file and it's called vm linux or you probably currently see it called vm linus with the z and the only difference here these are the same file the pr the only difference is that this is compressed right it's the z means that it's compressed so it's just a spacesaving method uh so usually we use vm linus just to save some room on the system and then this is the the kernel itself with no modules right so there's no modules this is just the base kernel and then once the kernel boots up it will mount all the file systems and then it will have access to all of the modules that it needs to insert to make things work like you know your usb mouse and your keyboard and your monitor and your video card all those things are modules that are loaded into the kernel they're not part of the the static kernel right i mean we could build this huge kernel that includes everything but that's just a waste of resources and so we have this strippeddown kernel that has just enough stuff to make sure that we can mount the file system so that it can have access to all of its modules and then become the full running kernel on our system there's a few problems what if this full kernel and all the modules yeah so basically what if all the modules here are on a file system that this stripped down kernel doesn't know how to mount right it's like well i don't know how to get onto a raid device or i don't have any idea how to mount this fancy new ssd drive that you put into the pcie slot or something well that's where init rd or init ram disk comes into play this is just enough information like module information and driver information to be able to have the linux kernel access the file system so that it can get to its modules so rather than make you know a custom bigger kernel for every specific system what we've done is we have a generic strip down kernel and then this init rd has the stuff we need in order to load the modules by mounting the hard drive or you know whatever we need maybe this is stored in a network drive so we have to have the nfs stuff in order to mount a remote disk on the on the computer so it can access its module so this is just like the the temporary staging area for kernel stuff that we need and this is kind of inserted right into this running stripped down kernel okay now i want to mention this because a lot of people confuse the init ram disk with the init ram file system and they seem similar conceptually they are fairly similar conceptually except that the init ram fs is actually part of the actual kernel itself it's part of this vm linus or vm linux kernel and this is just like it says it's a file system that it creates and mounts in ram and i want to mention this tool even though we're not going to go into it much dracut is a tool that has made this extremely generic using udev and and stuff like that so that it's very flexible but this is basically the tiny little file system that the linux kernel uses in order to do what it needs to do to get to the point where it can load the full kernel by loading modules so init ram fs is a file system that it loads into ram but that's part of the kernel itself this init rd is a ram disk that is mounted alongside of the kernel that allows it to get to the point so it can use the full kernel and knit ram disk is not used after the system is booted up this is just like a temporary staging ground to get to the full kernel wow that was a lot of information but it's really fairly straightforward when you see why it's doing all of those complicated things now i want to show you really quickly some of the files that we talked about so here i am inside ubuntu and this is the boot folder and we can see we have here's the init ram sure enough and this is i have a couple different kernels in here there is an update but the kernel numbers are listed here but here's the init ram disk and here is the actual kernel file itself there's a couple other files the system map actually tells the kernel where on the file system all of its modules live and then config this is the configuration file when the kernel was actually compiled so if you want to see the options that were used there but those are the main files that are on the system one other thing i want to show you is this system.map tells the kernel to look in lib modules and then the name of the actual running kernel and inside there is where you'll see the actual different modules okay and this is where the modules that are going to be loaded in to get that full kernel live so that's the basic uh basics of starting a linux kernel on your system now i know we covered a lot of things and a lot of terms and a lot of concepts but they should make sense once you go through that flowchart of what's happening and why it's happening and in the end which comes first the chicken or the egg well it depends on what you need to do right if you need an egg then the egg comes first if the chicken can handle it on its own no eggs are needed now a kernel panic certainly seems like a really good time to panic but i assure you just because the colonel panics doesn't mean you should soldier but really kernel panic just means that something went wrong with the kernel now there's a couple common causes and there's a couple things we can do depending on what the root cause is now if all of a sudden you're starting to get kernel panics and you haven't done any system updates recently and it's kind of hit or miss it'll happen it won't happen it happens when you're doing something but not another chances are you may have a piece of faulty hardware and this is one of the really common ways that you can experience a kernel panic if you have an overclocked cpu for example that can sometimes cause kernel panics a bad stick of ram is another one that'll really do it and you know some addon card like video cards are sometimes you know guilty of doing this really anything on your system any hardware if it fails it can cause a kernel panic there's a couple ways to you know try different things especially if you have multiple sticks of ram sometimes you can pull out a stick of ram see if you still get a kernel panic if you do then put that stick back in and keep pulling them out one at a time and see if the kernel panics automatically go away when you remove one certain stick hey chances are that's what's wrong with you know that certain stick is a bad stick of ram it can be frustrating to track down hardware related kernel panics but there's also another very common kind of kernel panic that happens and that's when you upgrade your system so let's say that you just ran a system update on your system and it installed a new kernel you reboot the computer and then boom it says something like failure to init or kernel panic blah blah blah blah if that happens there is something that you can do which is fairly easy to do and you can you know rescue your system so to speak so when it boots up we're just going to go into grub and we're going to pick an older kernel because when you update a system it keeps the older kernel for this exact reason if something goes wrong you can pick a different kernel and boot the system and then fix it so i'll show you what i mean here we are on our ubuntu system and let's say i just recently updated it and during the update it says now i need to restart my system so i'm going to restart the system and everything should go well blah blah blah blah and then boom we get a kernel panic now this doesn't actually give us the words kernel panic because i did something fairly simple to simulate what would happen if there was some corruption in the kernel uh but it says here you know it's unable to find the init rd sometimes you might see kernel panic init or unable to kill init or something like that it says press any key to continue but there's nothing that's going to happen right it's kind of locked up on me because it aired out there's no init rd so what we're going to do is restart the system and when we do that we're going to initiate the grub menu by holding down the shift key so first of all we do need to restart the computer i'm going to hold down the shift key and we should get presented with a grub menu sure enough here's our grub menu advanced options and we're going to see here we have multiple kernels this is the kernel the newest one is the one that it tried to boot by default so i'm just going to go back to the last successful kernel that we had press enter and it should boot this up and our system should be just fine then all we need to do is remove that kernel package with the failed thing using our app system reinstall it again you know do an update so it reinstalls and then it should install that init rd back in place and we should be fine so it's nice that linux keeps a few of the older kernels kind of in line so that we can do exactly this if something goes wrong with one kernel or we've deleted modules or just something doesn't load or we get a kernel panic we can select a previously successful kernel and then here we are on our system working perfectly and now we can go through and remove that package and fix it etcetera etcetera etcetera so if you do get the kernel panic especially on a production system oh it can be really scary but know that there are ways you know to fix it if it's just a kernel corruption you can boot to a previous kernel if it's a hardware issue it's a little harder to troubleshoot but you can also do something like boot from a usb drive or a cd and at the very least you'll be able to access the hard drives so that you can get stuff off of your computer if it is a hardware failure that's causing the problem and then you'll have your data to put onto a new computer but kernel panics are no reason to panic it's just one more thing that we need to understand how is working so we can fix it or troubleshoot the problem and move on from there we know that the linux kernel is modular in that it doesn't load all of the drivers that it could possibly ever need automatically by default it has these modules that it dynamically loads when it discovers that it needs like a driver for a certain sound card on boot now we can configure it to automatically load those modules by setting it up in a config file which we're going to look at now what happens though is it'll load that module but then on behind the scenes there's some incredible dependency checking going on and that dependency checking is really really good at knowing what modules one you know a particular module will depend on there may not actually be a robot called modbot in your system or even in my system i just like to think of a cheesy robot plugging in different modules in my computer but nonetheless the computer is really good at finding dependencies in fact sometimes it's too good and we need to blacklist certain modules so that modbot doesn't say oh this one will work and put it in i'm going to show you why we would do that and also how to do that but we're looking at two concepts today how to automatically load a module on boot and how to make sure that the wrong module isn't loaded on boot automatically by the system detecting a dependency that you don't want it to use now here on our this is an ubuntu system but all linux distributions have kernels we're going to go into the etc folder and there's a file here called modules so we're going to look at that we can see right now it's empty it says kernel modules to load at boot time this contains the names of the kernel modules that should be loaded at boot time one per line lines begin with this are ignored so what we would do is put the name of the module and we would add something like e 1000 which is the name of an intel network card module now normally the system automatically detects the hardware and it knows what to load but we want to i'm going to give you an example of how you can manually load a kernel module and have it done automatically on boot even if the system doesn't you know detect it on boot like oh i have new hardware i need this particular module so this is how you would manually do it right now we're not actually going to save this because i don't want to load that module but let's say that's loaded now if there are dependencies it will also load those dependencies let's say that the intel e1000 network card also requires the pci bus to work well then it would load the pci bus module so it's really smart but let's say that there are two different pci modules there's like uh pci version one and pci version 37 and by default it's going to load pci version one and we don't want that so what we would do is blacklist it so if we go into etc mod probe.d first i want to show you there's a bunch of files in here anything with dot conf is read in right so it doesn't matter which one you put it in there's just some conventions here anything we're going to manually put in we're going to put into blacklist.com so let's actually edit that file and we're going to see there are a bunch of things that are already blacklisted now what this does it just makes it so that the kernel doesn't automatically load it like here's a let's look at this ethernet one okay so this is an e1394 this is a firewire network device all right it's saying here we never want to use a firewire network device so if you detect a firewire device i do not want you to load the e1394 or the ethernet module for a firewire port so if we put blacklist here it's not going to load it even if we put in the firewire module itself so that it activates that port this will make sure that the kernel or modbot or whatever however you like to visualize it doesn't say oh well you're putting that in why don't we also activate the ethernet ability of that port we don't want that so we put blacklist in here same thing like that i've i've done this the most with sound cards because there are like 150 different sound card drivers for the ac 97 model of sound cards so a lot of times you have to blacklist a bunch of different models and see there's a blacklisted sound card right here right this is saying don't load this module that's not what i want and this looks like it's because of an ubuntu bug all right so anyway this is where we tell it what not to load even if it's compiled in and it's part of the module we would put it in here or any of the files in this folder anything that says blacklist space blank is going to be blacklisted honestly like i mentioned the linux kernel is usually really good at automatically detecting hardware and loading all of the modules all on its own by detecting new hardware on the system but if there are times where you want to manually load something but you want it to be done on boot that's where we would put it in the etc modules file and then blacklisting is really important especially if you're having problems with something like uh it just doesn't work with this kernel module let me blacklist that so the kernel will pick another module and try to activate that one one of the things that makes the linux kernel very special is that it's modular so we don't have to install all of the drivers for all the potential hardware in a system we just installed the drivers and the modules for the things that exist it makes the linux kernel very efficient because we don't have all that bloat all those unneeded things sitting in memory where they're never going to be used so the kernel can be very very efficient because of its modular design but in order to take full advantage of that we have to make sure that we're putting the right modules in the right place with all the dependencies so we don't end up with a kernel looking like this right with pieces that don't belong thankfully there are a handful of tools that we can use to properly manipulate the kernel modules and the first two i want to look at are ins mod and mod probe because on the surface they appear to do the same thing insert modules into the running kernel this is after we're booted it'll insert them into the running kernel so let's compare them to scientists right so ins mod is a very basic program you have to give it the full path of the kernel module that you want to install it doesn't do any dependency checking it just kind of slams it into the running kernel and if the kernel doesn't work like it's the wrong kernel version or it doesn't have the proper dependencies it's just going to fail and it's not going to give you an explanation as to why it's just going to say well i crammed it in there and it didn't work boom but mod probe on the other hand is a more advanced program it's a more efficient model and what it does you can give it just the name of the kernel module you don't have to give it the full path you can give it the just the name of the module itself it will look and determine all of the dependencies that that module needs so if one module needs you know depends on another module it will say oh let's load that other one first it does a really good job now it does need us to have a map of all the all the needs and dependencies on a system but there's a program to do this we just make sure we type dep mod when we're done installing a new kernel module and it will recreate that map so modprobe knows where to you know find the dependencies now you might be thinking okay well obviously i'm going to use modprobe when i insert modules and exactly that's what you should do why does insmod even exist well here's the deal mod probe actually knows what modules to install and then behind the scenes it uses ins mod to do the actual inserting so mod probe is really kind of a front end that's very intelligent about what to do on the system but that's why insmod still exists it's still the you know the basic tool that is used to cram modules into the running kernel modprobe just knows which kernels to load and in what order to load them so modprobe is what the humans use and ins mod is what modprobe uses if that makes sense it's really easy to see that in action on our system so i'm root on ubuntu here and let's go into lib modules and we're going to see we have folders for each of the kernels installed on the system we're going to go into the one that we're currently running which is the latest one and inside here we'll see this is where we have a lot of like the map files for the mod probe so it knows where the kernel modules live let's go into the kernel folder where the modules live type ls let's go into the drivers folder type ls again oh there's still a lot of drivers let's go into the net folder all right there we actually see some kernel modules here okay so uh we're gonna play with thunderbolt net but let's say we wanted to load this in so we could use thunderbolt as a network device if we wanted to use ins mod we would say ins mod lib modules generic kernel drivers net thunderbolt net because we need the entire path we're going to press enter and it's going to say well i can't do that unknown symbol in module it doesn't work blah blah blah it has a dependency it doesn't tell us what the dependency is but if we were to use mod probe and we were to say mod probe i want you to put in thunderbolt net and that's all we don't have to say a path we don't have to put ko at the end just press enter boom it's automatically installed how do we know well one we didn't get an error it just installed it but if we type ls mod will show us all the installed modules and if we scroll a bit to the top we're going to see well look at that thunderbolt net is installed into the running kernel and we can see the dependency was thunderbolt but modprobe knew to do that and it installed it now there's another tool for removing them and that's rm mod now there's no super smart version and dumb version of these rm mod is kind of halfway between because if we were to say rm mod thunderbolt it's not going to do it it's going to say i can't do that it's in use but at least it tells us what it's in use by right thunderbolt underscore net which that's a little strange because the name of it doesn't have an underscore right if we want to get rid of this we could say first we have to rm mod thunderbolt dash net okay no errors and now if we do the up arrow twice now we can rm mod the thunderbolt module and again no problems because we we got rid of that dependency first so now if we do ls mod we're going to see if we scroll up that they're not in there anymore they're not installed into the running kernel and that's how you can manipulate kernel modules on your system some of the tools are smarter than others but it's really important to understand why they're smarter and like insmod is important still even though you don't want to use it on the command line for doing very much at all now one more thing i just want to tell you about is if you install a new kernel module like you download a piece of hardware and it has its own kernel module and you compile it and so then you have the kernel module that's put in this folder you know so that it's in our current running modules folder you have to type depmod in order for it to update that database or that system map so that modprobe knows where it lives and what dependencies it might have it takes a while and now we have it updated so now we can install that new kernel module because it knows all the dependencies on the system now i know in the picture here i have these you know wrong puzzle pieces put into the puzzle this is actually a lot harder to accomplish than you would think because the tools just fail right they say oh this won't fit or this is the wrong kernel module some of them are smarter about it than others but it's really difficult to put the wrong kernel module into a running kernel but i want to make sure that you understand all of these tools exist how they work what they look for and the difference between them first off we're talking about network connectivity today and the first thing i want to show you is always check the cable now it seems silly right of course the cable is fine this is plugged in just fine however ah see it wasn't plugged in tight and whether it's a jokester in the office or somebody who ran over a cable with a vacuum cleaner or something sometimes it is the cable and you can spend a ton of time on the command line and be very frustrated to discover that it was just a bad cable so that's just a pro tip check the cable make sure it's plugged in tightly and that the little lights on the back are flashing and doing their thing now we could probably do an entire course on troubleshooting a network connection that's not working or not working right but today i want to go over just a couple quick tools so you can determine very quickly what your issue might be on the network so i want to look at ping and then just to check out our address on our computer and so the first thing i want to do is give you this scenario we're on the computer and we're trying to go somewhere like google and it says unable to connect oh we can't get online so the first thing i would do is open up a command line window and i would say let's try to ping google so ping google.com network is unreachable okay well i know that the google dns server has an ip address of 8.8.8.8 so that's the next thing i would do to eliminate whether it's dns right maybe it's dns that's not working so i want to see if my network itself is working so i'm going to say ping 8.8.8.8 ah network is still unreachable okay so there's an issue with more than dns that means that there's a problem with my network so let's look and see what our network address looks like now there's a couple tools you can try to type ifconfig if this doesn't work yes see it's not on my system it's a newer system so type ipad for address and it will give you the ip addresses on your local computer for your network devices so this is the localhost 127.0.0.1 this is just like a virtual host that says it's it's local so we don't want to use this one but this is our ethernet port right eth0 and it looks like we have an address of 10.10.10.10. so i would say let's ping that so let's make sure that you know our network stack itself is working okay so this is working i can ping my own ip address so the ip stack itself is working i'm going to hit control c but for some reason i'm unable to ping out of the network now let's assume that my gateway address is 10.10.10.1 ping 10.10.10.1 okay i can ping other computers on my network so it's not well first of all we know it's not my computer cable right because i can ping a different computer that's across the network but i'm still unable to ping google so for some reason my internet connection is down well let's look at our ip routing information i'm going to type ipspace route and press enter alright it looks like 10.10.10.024 so this network is directly accessible via eth0 okay so that's working that's why we could ping this up here but if you'll notice now you might not notice but there's a line here that's missing we don't have a default route set so that means that the computer while it can access other network or other computers on our local network it doesn't know where to send network or where to send packets that are destined for another network there's no ip routing information here now if this is an ip routing nugget i could say okay let's add a route manually but this is more of a troubleshooting nugget so here's the deal probably when we got our dhcp address we didn't get all the information or something fell apart the first thing you do is either turn it off and turn it back on right that's that's the standard it guy response right well in this case it might work or we could actually go and just go to our wired network interface here turn just the network interface off connect again turn it back on and now let's well let's just look and see what our ip route information looks like hahaha look when we brought our network interface down and back up now we have a default route added so now let's see if we can ping google.com boom we're able to ping it we go over here and we should be able to access google as well and sure enough there's google so we've successfully troubleshooted the problem with our network connection in our case it was a route thing and really just turning off the network interface and turning it back on was the way to solve it but the important part is going through the process of determining where the problem is we knew it wasn't a network line or that network cable because we were able to ping other computers on our network so it's just a matter of troubleshooting and going down the list of where can i connect where can i connect and what might cause those issues figuring out connectivity problems is sometimes more of an art form than it is a science you just have to kind of put yourself in the mindset of a packet and say okay what information do i need to go from point a to point b and what is stopping me from getting in between there tools like ping were are able to make sure that you can or can't connect to remote computers and then using the ipadd you can see what your address is we also used iprout to look at the ip route information and that's where we discovered we didn't have a default route but tools like this are what you can use to troubleshoot your connectivity to figure out what's going on but don't forget always check that cable because sometimes it's just a physical cable unplugged dns or domain name system is the way that your computer converts a url or a domain name like google.com into the numbers the ip address that the actual computer knows how to connect to using ip routing so google.com isn't really helpful for anything except for dns really what the computer needs is that ip address so that it can get there now there's a couple tools that we can use to test dns on our system one of the most common is ping right you just ping and see if it can reach it if it can reach it and get a response hey it's working great but there are three other common tools that i want to cover dig nslookup in host and let's actually look at those individually because they all do about the same thing but they do it in a slightly different way now dig is the program that i use most often only because it has a cool name right there's really no reason that i use it other than the word dig is just kind of cool so dig you type dig and then you can just type the host you want to look up like dig google.com and it will use your default dns server and look it up if you want to specify a dns server for it to use instead of the one that your system is currently using which is a way that you can test your particular dns server you you put the at symbol and then the ip address of the server that you want to query now the other two programs can do the same thing they just do it in a different order you can say nslookup and the host you want to look up and then if you want to specify a server again you don't have to but if you want to then you put the server after that same thing with the host command you put type host the host name you want to look up and then if you want to specify a server you put it there if you leave it off it'll use your default and they all like i said do kind of the same thing so let's look at them really quickly and then i'll talk about how to query different servers and do just a little bit of troubleshooting on your system because testing dns is kind of vital if you want things to work on your system and they don't seem to be so first of all let's look at them all how they function just by default so i'm going to say dig google.com all right and this is the response i get it's queried my default server and we can look my default server is listed right here 8.8.8.8 so that's pretty awesome and this is the response it gets there's a bunch of a records for google.com now let me clear the screen we can do the same thing with ns lookup google.com and this is going to give us a little bit different format right it says our server right here this is the oop i missed i highlighted the wrong line that's our server the same server of course it's our default server and it gives us all the addresses these are all the a records a little less detail but it gives us the same information now if somebody tells you that nslookup is deprecated and it's not used anymore and it's going to be abandoned if there's a weird case with nslookup that that's true until it wasn't they were going to get rid of nslookup and replace it with dig but then they came out with bind 9.3 and for some reason decided that ns lookup was going to stick around so we have nslookup and dig and lastly let's clear the screen again we can just say host google.com and this will give us information as well it actually doesn't tell us what default server it's using but it gives us a lot of information these are all of the a records for it these are all you know the ip addresses this one actually also tells us the mail handlers the mx records for google.com's domain which is kind of interesting as well so just pick the one that you like the best and that's what you can use and the reason that it's important to pick one that you like and use it often is so that you're comfortable with it right like i said i usually use dig and that's weird i know because it's the least like common way to handle specifying a server right the other two you do you know like nslookup what you're looking up and then a server with dig you say dig at and let's say 127.0.0.1 that's localhost right so dig at localhost for google.com and we got a response okay google.com and it gave us this address which is different but it came from our local server now it did not get all the same responses that we got from the 8.8.8.8 domain server you know our default domain server if we were to say dig at 8.8.8.8 google.com we would get a totally different set of ip addresses so that's why it's really important to be able to query different servers so that you can get different responses because on this computer if we try to ping google.com it's going to try to ping that other address and time to live exceeded it's not ever going to work so there's something wrong with the dns server running on localhost now i'll show you what the problem is the hosts file i've actually sabotaged here this is the etc hosts file this is where your computer will look first for any dns lookups and sure enough i've kind of bamboozled it here i put the the wrong ip address for google.com so i put that in there now if i restart our local server now if i try to ping google.com it's going to be just fine because it's going to actually use the real address that it looked up on the internet but that's how you can use the different tools to specify not only what host you want to look up but what server you want to query when you look it up so whichever one of these makes the most sense to you or whichever formatted results you like best just pick one and use that because you can specify a server for each of the different commands but knowing how to use them is important because troubleshooting usually means querying more than one dns server so you can figure out what the heck is going on one of my favorite things about linux is that everything is configured with text files right it's just plain text files it's awesome and the network is no different it's configured with text files now there are a few differences when it comes to distributions if you're on debian or if you're on centos there are going to be some configuration files that are different because they configure their networks different now there's nothing wrong with being unique i really like the differences in the distributions but there are some files that are consistent regardless of what distribution you're using we're going to call those the common files and that's what we're going to look like look at in this nugget these files here are consistent across the board they're just kind of standard linux files so i want to show you what they are and and where they live and how they are configured and what they do and that's again just the common files there's going to be specifics in the different distros but we're just looking at the files that are common to linux in general in this case we're going to look at the files in ubuntu and the first one i want to look at is the etc hosts file so let's actually become root if we want to edit them we have to be root to do so so i'm going to look at etc hosts there's no extension at the end it's just etc hosts and this is a file that acts kind of like the first resort dns lookup so before a system even looks things up via dns it looks in here and there's a couple things here like localhost and ubuntu this is our hostname are set up in here but we can add something we can say what if we wanted to make sure users were never able to go to google we could say okay 127.0.0.1 which is our localhost is now going to be google.com so if somebody tries to go to google.com they're going to try to hit our local machine and we don't have a web server at all so it's just going to error out so if we save that now on our computer if somebody tries to go to google it's going to fail so google.com unable to connect even though our network is working just fine we could go to yahoo if we wanted it's just a matter of that dns entry in the et cetera hosts file that we kind of broke all right so another place we can look is in the etc ns switch.conf file now this configures a bunch of things on our system like group and password files but what i want to show you specifically here is the hosts line so this is saying where does it look for dns lookups to you know find what the host's ip addresses are the first one is called files this points to etc hosts if we have this listed first that means before it queries a dns server it's going to query that file it's going to look in that file first and then in order these are what it's going to do from there it's going to do you know the mdns for local lookups and then it's going to use a dns server so this is the order that it's going in but the first one it looks in is the files specifically that etc hosts file so you can change the order of that i don't recommend changing the order because that's kind of what we wanted to do but one more thing i want to show you is etc resolve.conf and this is a little bit of a confusing file because while this tells the computer what name server to use you'll notice up here it says do not edit this file now back in the day you would just put name server and then the name server you wanted to use on your system but now this is all handled with the systemd resolve d service and and it creates this file on the fly so really we do not edit this at all network manager handles all of the name server entries in here um or whatever your distribution uses for configuring the network so we don't actually edit this but if you want to see what server your your particular computer is using you can look in here and it looks like it's it's querying 127.0.0.53 which is another ip address to our local computer so that means that it has some sort of local caching dns server and it's not querying directly to the internet now you probably noticed that all of these common files have to do with dns and that's fine but it's it's interesting that dns is kind of commonly configured across the distributions when it comes to actually configuring the individual network devices that's drastically different from one distribution to the next but the common files are generally dns files and that's what we looked at in this nugget ubuntu is one of the most common distributions out there but it's based on and built upon debian so when we look at configuring one it's very similar to configuring the other and that's the case with network files now i want to show you where to find the different network files on an ubuntu or debian system but it's important to note that in the last iteration of their longterm support for ubuntu that's when a change has been made to how the network is configured and what files are used and how those files are configured now there is one commonality between them and that's if you use network manager to manage your network instead of editing the configuration files that hasn't changed you can still use that across all the versions that are current with ubuntu and debian but these two ways have changed so first i want to look at the older version of ubuntu and when i say older i mean one that's still valid to use as of the date today you can still use version 1604 let's see what version this is etc os release so this is ubuntu 1604.1 lts long term support and this is still as of today a valid version of ubuntu to use in production but this is configured by going into the etc network directory and there's a single file in here called interfaces so if we look at interfaces we're going to see this is how you configure a network interface with an older version of ubuntu or debian now this is pretty much the stanza that you configure there's usually something in here you can look at and base it off of but this is the format for it we're not going to go through the you know configuring different things it's pretty straightforward you don't have to indent this is just for ease of readability but this is how you configure a network interface via the configuration files now we could still use the um network manager if we were on a gui system like this is a gui system we could use that but i'm actually ssh into a server that doesn't have a gui interface and that's where we use these network files to configure it so let's quit this one this is how you configure an older version but a modern version like 1804 or the most modern version of debian uses a completely different network configuration system now here we're on a newer version of ubuntu this is actually 1804. let's check that out so this is 18.04.1 another longterm support release of ubuntu and this one uses the net plan system for configuring network interface so if you go into etc net plan and you look in here depending on what kind of system you have you're going to have some sort of file in here it's either going to be like zero one networkmanager.yaml or 50 cloudanit.yaml if it's a server the thing about yaml files though let's look at this yaml files do depend on indentation in order to be properly configured so while with the etc network interfaces of the old ubuntu system it didn't matter how things were indented here it really really does and so here this is where we would configure like a static ip address and we you know here's our ip addresses gateway name servers the format's a little different but it's fairly similar conceptually to the old system like i said except for that white space that's really important that it's indented properly so if you make a change to this then you just would do sudo netplan apply and then it will activate the new changes that you made now i do want to point out again this is a gui system so it has network manager but there's also nmtui which is network manager text user interface and then you can edit interfaces using just the text boxes on your screen if you don't have a gui system installed so that's just one more way that you can edit your file in ubuntu or debian if you don't have a gui but you want to use network manager as opposed to just configuring those files on your own whether you have an older system using etc network interfaces or a newer system using configuration files and etc netplan configuring the network on debian and ubuntu is not difficult and you know it's usually just a matter of changing what's existing but if you want to use the network manager to manage all those interfaces either using gui or text tools you can do that as well it's very flexible and even with the change in versions it's not a whole lot more difficult to do one over the other if we've been learning together for a while you know that i prefer debian based distributions and ubuntu is my jam but i have to admit when it comes to network configuration files centos or red hat they really have an elegant solution for how to configure the network and there's no confusion between if you're using the gui network manager or if you're just using the configuration files in the etc config folder let me show you what i mean because centos is just so awesome when it comes to network configuration now here we are on a centos system and it doesn't matter which way you configure the network interface you can go up here and go into the network manager to configure it or we can go directly into etc sysconfig and i just wanted to show you quickly inside the sysconfig folder and sent to us is where all of these configuration things are this is kind of unique to centos and red hat they put most of their configuration inside this etc sysconfig folder but inside here there's also another folder called network scripts and that's where our networking is configured if we look in here we're going to see there is this configuration file for our interface ifconfig dash eth0 so let's look at that if we edit this it's going to change the way our network interface comes up like if we were to change boot proto from dhcp to static and then we were to add a line ipaddr equals let's say 10.10.10.111 which is not currently the ip address if we were to do that and save it and then just do a quick sudo service network restart we would be able to restart our service and if we were to do ifconfig we're going to see scroll up here sure enough that is our ip address now now there are a couple problems we didn't add like netmask or anything so let's go up into network manager wired settings click on the configuration thing so we're to edit this in network manager if we go over to ipv4 we can see it went from it used to be dhcp now it's manual this is the ip address i put in and it just guessed this netmask because the 10 range is a class a ip address so it guessed that i wanted to use a class a netmask but i don't i want to use 255.255.255.0 and i didn't supply a gateway so let's do that now 10.10.10.1 and i didn't supply any dns server so we can do that here 8.8.8.8 and now we'll save this we'll click apply if we go back and edit that script sudo vi we're going to see that some changes have been made now this stuff is still the same boot proto equals static ip address but look down here it added the gateway address that we added it added the dns server that we added and it added the prefix 255.255.255.0 translates to 24bit netmask so put that in there just use a different form and we can see that it made the changes to the actual configuration files rather than having like two different systems and you have to decide which one you want to use centos allows us to make changes in one place and whether we're using the gui interface or just this textbased interface it's going to allow us to make those changes i love that it has all the change places in a single file as opposed to conflicting and trying to decide which method you're going to use if you use network manager you you're using this configuration file if you don't use network manager you're using this configuration file so it's a really elegant way to handle network interface configuration like i said at the beginning i'm a debian ubuntu man that's what i usually use in my server situations but when it comes to configuring network interfaces man centos has really stolen the show they have an elegant way of doing it either using network manager gui tools or using the scripts inside the network scripts folder of etc sysconfig network bonding in linux is really just a way to utilize computers that have more than one network port and most servers nowadays come with that let's say you have a server and it has three network ports built into it well if you have to connect it to a switch all each of these connected to a switch you would ideally like to use all of that bandwidth and you don't want to have to supply three different ip addresses to the computer you just want to use all of the available bandwidth on all three of those wires and that's where network bonding comes into place now there's basically two different kinds of network bonds that we're going to look at those that require special switch support and those that are generic and don't require the switch to know at all what's going on now linux does provide some pretty cool options when it comes to there but the options can be a little overwhelming i mean this screen is like oh my goodness there's so many choices well that's okay because your choices are going to be fairly limited once you learn what all of these are now the first thing i want you to look at when we're looking at this list is does it require switch support and what i mean by that is a lot of switches especially layer 3 switches or smart switches they're sometimes called will support link aggregation and different vendors call it different things but it's going to be either link aggregation or lacp or ether channel the idea is a smart switch will have builtin code that will allow them to work together so if you have a smart switch chances are that you have the ability to use link aggregation now some of these require a switch that supports that i'm going to start with the confusing one here so balance round robin basically how balanced round robin works is you have multiple ports and it says okay when i transmit packet 1 i'm going to use this port packet 2 is this port packet 3 is this port packet four is this port packet five is this port packet six is this and it just keeps going through packet by packet and transmitting across all of your interfaces now the reason i said it sorta requires switch support is if you're plugging these into a switch it does require a switch that supports link aggregation but what a lot of people do is they'll use this balanced round robin and they will connect two servers together with crossover cables and actually with gigabit we don't need crossover cables anymore but they'll connect two servers together and they'll use balanced route robin and in that case you don't need a switch support or a switch that supports it because there is no switch involved right they're just directly connecting the two computers together and so like if you have a file server that you want to connect to your other server a lot of times this is a way that you can increase throughput without requiring any special switch support so if you're connecting directly computers you don't need to have switch support if you're connecting to a switch you do all right that's mode 0. mode 1 is an active backup and this one's pretty easy to understand basically however many ports you have on your system only one is going to be active and if that one fails then another one is going to turn active and that's just how it works they always have one this is fault tolerance but this doesn't speed anything up it just means if one fails another one's going to come online all right this doesn't require switch support because you know it basically just only uses one port unless the next one fails and then you switch to that port and the switch is like okay so now we're going to use this port great uh balance xor does require special switch support and this is kind of cool how balance xor works basically you have your computer here your linux computer and it does a hash based on your mac address and the client's mac address and so here's another client over here and basically it says okay based on this hash i'm always going to use port 1 to connect to this client and then over here the hash is different so i'm always going to connect to this computer so it's basically a way to just spread out which computers use which port but this computer is always going to use this port and this computer is always going to use this port now this isn't used very often because if you have switch support which is required chances are you're going to use 802.3ad which is the industry standard link aggregation protocol right this means that your switch knows what to do and your client knows what to do and it's just a really smart way of increasing through throughput and availability fault tolerance so if your switch supports link aggregation you should use 802.3ad or mode4 which is the industry standard okay so even though balance xor is cool it's not very often used because if you can use it you might as well be using the one that's even better broadcast i kind of skipped over that broadcast is only in very specific cases that you would use it it just takes all of your ports on the server and spews all of the data out all of the ports at once it's not used very often and it definitely requires your switch to know what on earth is happening now the two i do want to focus on a little bit are these bottom two let's say you have a dumb switch it's not a switch that supports link aggregation it's just one you bought from amazon for 40 bucks it's gigabit but it doesn't have any smarts built in linux is smart enough to be able to utilize the dumb switch and increase bandwidth and throughput and reliability and there's two different ways it does it balance tlb which is balance transmit load balance and the balance of all load balancing and what this does is let's say we have four ports on our switch okay with tlb it's going to transmit from whichever port is currently the least busy so all the transmit is going to be balanced out now incoming is still going to always go to one active portent so that's not as good as alb which is the same concept except it does it for incoming and outgoing so basically the least busy port gets the traffic and this is really brilliant and how it does is it constantly changes the mac address on these ports or on these ethernet cards so the switch is like oh you moved again oh you moved again oh you moved again and from the switches standpoint it doesn't care how many times you switch the mac address so it generally works just fine now some people have issued with this i've used this in production for years and never had a problem so balance alb if you have a dumb switch i highly recommend you use mode six okay if your switch supports link aggregation i highly recommend mode four because that's the industry standard and if you're just connecting two servers together with multiple cables my shirt mode zero works really great to be honest the most difficult part of linux network bonding is figuring out which mode to work but really it's not that tough of a decision if your switch supports it use 802.3ad there's really no reason not to if your switch is a dumb switch and doesn't support it i highly recommend mode 6 or balance alb if you want to take advantage of bonded network interfaces but you don't have the expensive hardware that will support it once you know the type of bond that you want to set up for your two ethernet or two or more ethernet connections on the computer configuring them is pretty straightforward although it's drastically different depending on whether you're on ubuntu or centos so i'm gonna show you how to do it on both and then i'll show you also how to test it to make sure that the bonding modules are working so here i am on ubuntu i've actually already configured it so i've gone into etc netplan and i have my file in here so let's look at this file and what i've done is i've set up the proper yaml format with indentation and everything to set up bonding so i'll go over it really quickly first of all we need to use network d as our renderer we can't use network manager in order to configure bonds because it just doesn't support bonding we do have to define the ethernet cards themselves so ethernet eth0 i set dhcp4 to false because i don't want it to actually assign an address to eth0 after all that is done we don't need to set up anything more for the ethernet ports themselves then we need to set up a bonded interface so a new section here bonds the name of it is bond 0 dhcp 4 is false because i'm going to assign a static ip address and the interfaces is a section that we tell it what interfaces are going to be a part of this bond in our case we only have one which is silly i mean we're only bonding one interface but you can do that one or more it's just silly that we only have one to bond that's just all this computer has i've set up the addresses the gateway the name server is set up just like it would be if we were setting it up for eth0 really the only new difference down here is the parameters and the parameters what i've done is mode active backup that's just one of our modes it happens to be mode one but we actually say active backup we specify by name the mode here and then we save this file and do netplan apply now how you can tell if it's working we can do ipadd to see that you know sure enough it's up here's our bond zero interface here's our ethernet zero interface which is up but doesn't have an ip address and another way you can test is to look at so we're just going to cap the file it's in the virtual file system proc under net bonding bond zero so if we look at this file it's going to tell us the information about it and say bonding mode is fault tolerance active backup primary slave none but the current active slave is eat zero it's our only one that we have it's up and it looks like everything is working good here's our slave interface eth0 down here and that's all working well as well so we have it all set up and it's working now with centos the setup is a little bit different because centos is set up differently in centos in order to configure our ethernet ports we go into etc sysconfig network scripts and in here we have our ifconfig files so i'm going to first look at the changes we have to make to ifconfig eat zero so here we basically we make it pretty simple ethernet type product boot protocol is none uh the name the device is zero on boot we want it to come up but notice i haven't given any ip addressing information and here are the two things the master is bond zero and yes this is going to be a slave to this bond all right so those are the changes we make here if we look at if config bonds zero this is where we set up bond zero we name it bond zero bonding master is yes i did give this one an ip address and a prefix which means the subnet mask on boot we want it to come up a boot protocol is none and then here the bonding options we tell it what we want it to do i actually set this one up to mode 6 mode 6 is balance all load balancing so it does its own version of load balancing for incoming and outgoing traffic again i only have the one interface on this computer but that's okay let's do the same thing now checking to make sure it's working is similar we can do ipadd we can see sure enough here's the bond set up right here and our ethernet is right here and really for configuring the bonds themselves that's all there is to it ubuntu and centos are configured differently but underneath they're both doing the same thing they're using the bonding kernel module and they're allowing you to do some awesome things with multiple ethernet ports on your computer gpt and mbr are two different ways of taking a hard drive and chopping it up into pieces so that those pieces can be recognized and mounted as different drives and stuff on your on your system so they're they do the same thing but gpt is much newer and much more featurerich than the oldschool mbr now there's also a really cool thing called protective mbr i want to talk about but basically we're going to talk about the differences between gpt and mbr here i have two big squares and i'm going to say that these actually they're rectangles i'm going to say that they represent the drives themselves and they're going to be partitioned with two different systems so we'll start over here with the old one mbr which stands for master boot record okay now how this would work is let's say this drive is device sda okay so this is the device on the linux system it recognizes this drive now the mbr is a little tiny bit of reserved space at the very beginning of the disc and this is basically like a table of contents and it describes how this is chopped up okay so this is chopped up into let's say four pieces there's four primary partitions you can also cut each one of the primary partitions into four so we could have you know mult we could have this one be like have four partitions there if we wanted but basically what this means is once the mbr defines in this little first part of the drive right here what is what we come up with other devices on the system we have dev sda1 devsda2 and it goes on down the line and so each of these chunks is referenced on the system as their partition name now this is the raw device dev sda doesn't go anywhere it's still there but the individual partitions get their own device and that's how we reference them on the system so mbr has this you know this little tiny table of contents at the beginning and it says where things are chopped up now gpt or gui guid partition table which i think is interesting because there's an acronym wrapped inside of an acronym this stands for globally unique identifier partition table and basically it does conceptually the same thing right it will chop up this drive and you'll end up with dev sda for the raw device and so on down the line the differences are though gpt does have a spot at the beginning with the table of contents but then it also scatters them copies of them around the drive so that if something were to happen with one of the copies of the gpt it can still figure out what's on the disk it also does some crc correction on the files itself or on the the file system itself so that it knows if there's some corruption and it can fix it it's just very very robust along with the ability to use much bigger drives mbr is limited to two terabytes gpt is limited to i think petabytes i mean there's no practical limit at this point of how much drive space it can talk about rather than being limited by four partitions you can have tons of partitions on here so you know we could chop it up into as many as we wanted and it just goes down the line sda1 sda2 sda3 and one other really interesting thing and i wanted to talk about is the protective mbr so gpt is used on newer systems it's specifically part of uefi that replaces bios but biosystems can still see gpt drives and part of the reason they can do that is at the very beginning there's this section that looks exactly like an mbr basically it's an mbr record here that says okay there is one big partition on this drive the entire drive is a partition there's no room available on this partition and it takes up the entire drive now why would that exist it's basically so systems don't say oh well there's no mbr in this drive so this must be a blank drive with nothing on it well we don't want that to happen so basically what it means is it allows the mbr system or a bio system that is used to seeing mbr it will allow it to realize that this is not an empty drive it's just something it can't read and then there's some really cool stuff where part of this initial mbr can be hints to the operating system itself as to what's happening underneath and so some older bio systems are able to boot from a gpt drive even though it shouldn't be able to because the linux system or you know the operating system itself says okay i see mbr is saying that um you know it's really a gpt drive but i know enough about gpt myself that i'm going to be able to find the software and you know the the drive files and partitions on here myself and so you can actually use it on there but it's because of that protective mbr that all that is possible so when it comes down to it mbr is the old way of doing things gpt is the new kid on the block honestly there's no reason not to use gpt it does everything mbr does it does it more efficiently more reliably it's just the better way to go so i recommend that you use gpt and while there are significant differences a lot of those backwards compatibility issues make it so that you don't even have to worry about them the linux file system is actually really cool because all of the network mounts and different hard drives and usb drives are all on one giant file system it's not there's no a drive b drive c drive d drive there's nothing like that it's just all one big file system hierarchy and we're gonna look at the different things because whether it's a real or virtual file system it's on the same file system whether it's relative or absolute this is just how you traverse the actual file system we're gonna look on the command line about that network mounts they're just you know on a remote system but they just show up as a folder on your local computer it's just it's really cool so first of all let's let's start at the very beginning it's a very good place to start so we have a hard drive right so we're going to say it's dev sda1 and that has the root partition on it you know just forward slash so this is like where everything starts the root partition is the basis of our linux file system it's it's on a hard drive of some sort in our case it's this one now inside there there's tons of files and folders some of the files are directly on that dev sda drive right on the root file system or folders within that root file system they're actually stored on that hard drive some of the folders and the files in there are a virtual file system if you look inside the root directory there's a proc folder an assist folder these are dynamically created file systems that are just a way to interact with the kernel itself so if you want to make a change to the running kernel you can make a change to one of the files in these folders and it's going to affect the running kernel but it's not actually files it's a virtual file system it's just a way that they represent that interface with the kernel is this virtual file system then we also have like a remote nfs server it could be nfs or samba or you know whatever your network protocol of choice is but it's mounted on the folder inside of your root system so this could be like the home folder on your system might not be on the actual hard drive it might actually be on a remote system but it's mounted inside your file system and from you know from a layman's position just looking at the system you don't know if it's a remote system or a local system or a virtual system because they all look the same same thing when you insert a usb drive it doesn't come through as like you know like the e drive or the f drive like on a windows system it's just mounted somewhere on a folder inside that same one solid monolithic file system even if you put in another hard drive so this is we're saying this is dev sda1 like the partition one on the sda drive even if you were to put in a second hard drive in the system it's still going to mount inside a folder or a subfolder of this root drive right it's gonna be like in mount data or wherever you happen to mount it it's just going to appear as a folder inside this root file system so everything is inside that root file system it's really a neat way to handle all of the various things that can be stored in linux and the virtual file system is very unique in that it's not really a file system it's more like an interface designed as a file system so you can interact with the kernel itself alright so let's go to our file system here and i just want to show you kind of how it looks and i want to talk about this whole absolute versus relative thing all right so i'm just going to show you the file system here we have ls and we have a folder called pictures we can do ls picture gotta spell it right ls pictures and i have a folder cbt gold which is this background and then a trips folder uh what if we were to do ls trips okay there's a folder called grocery store mexico orlando now there's a cool program called tree i'm going to use so let's say tree pictures it's going to show us a tree representation of all the files and folders inside here so we can look we have the pictures folder there's a one file in there called cbt gold then there's a folder called trips inside that folder are three different folders grocery store mexico orlando inside each of those there are files that are stored in there this is the hierarchy of that now honestly this mexico folder this could be like a remote nfs share we don't know because it's all on the local file system that is you know mounted everywhere on the system it's just one file system so we don't know where these are we just interact with them as if they're all one local file system so that's just a really cool thing about it but this whole absolute versus relative let's go over to the pictures folder and trips i'm going to clear the screen and if we were to type ls minus a we're going to see all of the things in here now we already saw some of these orlando mexico grocery store but these dot and dot dot now these are special folder entries that mean the dot means the current folder so if i were to say cd to dot i'm in the same folder dot dot means the directory above me so if we are in trips dot dot is going to be pictures so if we were to say cd dot dot all of a sudden you'll see now we're in the pictures folder that's pretty cool right now we can use that as a folder name anywhere so let's go into trips orlando okay and inside here let's do an ls minus la we can see here are all the the pictures that we have and then these two folders dot dot and dot in every single folder you're gonna find a dot and a dot dot because it's just a pointer that means this directory the directory above me okay so we can put that in a string too let's say i want to cd to dot dot dot dot and see where this takes us two pictures now why did it do that well because dot dot if we're in orlando dot dot means trips and then we're in trips the second dot dot means pictures so that's interesting we could do something really complicated and i'll end with this we could say cd trips orlando dot dot grocery store oh okay what happened there and we'll end with this one like i said so we started in pictures and we went to the trips folder orlando back to the trips folder and then into the grocery store folder and sure enough that's where we ended up pictures trips grocery stores so you can use dot dot anywhere in your thing to talk about the relative path of where you're where you're going okay now the absolute path of this is going to be cd home bob pictures trips grocery store and then sure enough we're in the same folder this is the absolute path starting at the root level but we can use relatives paths using things like dot dots one other thing i said i was going to end there and i guess i lied to you this tilde image or this tilde thing is a shortcut for your home directory so if we were to say cd to the tilde which is usually to the left of your one key boom now we are in the tilde which if we type pwd we'll see is home bob all right so those are the relative path tools that you can use to construct where you want to go but that's really how it works and the tree program is just kind of a cool way to look at things you can see everything else spelled out there the big takeaway is that the linux file system is just one big monolithic file system and whether you're mounting network shares or you're mounting usb drives or second hard drives or even a virtual file system to interact with the kernel it's all under the same bunch of file system in this big hierarchy partitions are really just organizational units that are on a hard drive you kind of chop a hard drive up into different partitions and those partitions are used for different things like swap space or you know a particular folder on your system you can use partitions for a lot of things and depending on the system that you're using whether it's going to be an mbr master boot record or gpt gui id partition table you can have a multitude of partitions or just a few and depending on which scheme you use it depends how big your hard drive can be now we're going to look at a handful of tools like parted and g parted and this stands for partition editor and graphical partition editor and the old school f disk which is just a command line tool that didn't used to handle the gpt partition table but now it does so there's this one size fits all tool called fdisk and then i do want to show you a couple tools that will let you see what block devices are available to partition on your system so let's actually go right to the command line so that we can check out what's on our system and i chose to use centos today for no other reason than i thought let's use centos so i want to show you first of all how we can identify the block devices on our system we can type lsblk and it's going to show us this really nice little cool tree thing right so we have fd0 floppy disk which actually is a lie there's not a floppy disk on this system it's a virtual machine but then sda is the main hard drive on the system and it's partitioned into two partitions the sda1 which is the boot partition and then sda2 which is used in lvm which we'll talk about in a different nugget then there are a bunch of other hard drives installed on the system that don't have anything on them at all there are four 10 gigabyte drives sdb sdcsd and sde and we can also see these things if we look at proc partitions this is again the virtual file system that shows us an interaction with the kernel and we can see sure enough here they are sdb c d and e and this is how big they are the number of blocks so they're 10 gigabytes and of course if we just looked in the dev folder and gripped for sd we would see all these devices too right sd b c d and e so what i want to do is just partition one of them we're going to pick sdb and there's a couple ways we can do this we could use g parted now notice i'm root we have to be rude if we're going to do the system level stuff like partition a hard drive but we could say g parted and it'll start up and scan for all the drives available and there's a little drop down here sdb very cool we would have to start by creating a partition table and then we could create a partition with gui tools it's really really easy to use g parted but a lot of times the system doesn't have a gui so i don't want to do it that way now we could also use parted or partition editor this is the cli version of the program that we just used we can type help and it'll show us it does the exact same things it's just used you know use text to tell it what to do all right so i'm not going to use this one either but i want you to know that they exist and if you're more comfortable with them they're a little bit newer and they have a few more features and that sort of a thing but what i want to show you is the old school f disk and i want to show you this because if you're on a server it may not have partition editor you know part ed or especially g parted the graphical one so i want to show you what almost every linux system is going to have and we're going to use that one to actually do our work so we're going to say fdisk dev sd b we have to tell it what device we want to use so it's a good thing we know how to find the available devices on our system type this and type m for help now we just have these single letter commands but that's okay the first thing we're going to need to do is to create a partition table and it can be one of two types we can create a gpt partition table by pressing g or we can create the old mbr or msdos partition table using o oh i'm just going to pick since it's not over two terabytes in size either one is going to work so let's just use gpt for the heck of it so i'm going to say g press enter it says okay it built a new gpt disk label this is its id and now we have a partition table but no partitions to create a partition we can actually type p to see what's there there's gonna be nothing there so to create one let's press m again so we get the whole help screen we can say n for add a new partition so new and now it says partition number one through 128 right this is not mbr mbr would only support one through four this is gpt so one through 128 i'm just going to pick the default first sector the default is going to be the first one available last sector the default is going to be the last one available right this is our range and it chose the last one so i want to fill up the entire partition or the entire drive with this one partition so if we type p now we're going to see sure enough we have a partition there it's 10 gigabytes linux file system type that's okay and now to quit let's press m again for help to quit we could just press q and it would quit without saving changes but we actually want to make these changes so we're going to make the change to the disk so we're going to use w to exit which means right so w enter and now our partition table has been done we can say lsblk and look at that now it shows up that we have a partition created on our system it happens to be a gpt partition table and it's a single partition that takes up the entire drive i didn't actually go through the process of using parted or gparted they're very straightforward and they walk you through it and the help screen is right there and easy to use so if you want to use those by all means go ahead but i wanted to show you the more complicated fdisk because it's going to be on every single system that you run across there's a wide variety of file systems that you can put on an empty partition in linux but the idea of all of them is the same you take a big empty partition like a big field and you divide it up so that you can store data efficiently like if this parking lot were an example of a file system each car would be a piece of data that is stored logically in its place and and some file systems have you know bigger spaces for parking buses some have littler spaces for parking compact cars but conceptually they all do the same thing now like i said there's a bunch of different options available in linux ext is the most common family of file systems that you can use on linux it's very mature it's been around forever the later versions support journals so that if a computer gets powered down before the reads and writes are synced up usually you can salvage the data on there so it's fairly robust as well xfs is a file system that's been around for a long time this used to be what you had to use for really really big drives but now everything supports big drive so that's not really the issue anymore it is still used by centos and red hat though so xfs is still widely used interestingly it has its own set of tools like for file checking fixing things manipulating the the xfs file system itself rather than using standard linux tools it has its own set of xfs tools there's a couple others btrfs it's often called butterfs this was the new kit on the block right this has awesome features like snapshotting and it's just awesome unfortunately it's kind of been abandoned which is weird but nonetheless it's kind of what happened so btrfs is still functional but it's not widely used anymore and then of course i'm just going to mention them dos or the windows world has their own file systems like ntfs vfat fat32 things that you're familiar with if you're in the windows world and linux can usually read and even write to most of these file systems but when we're talking about linux file systems we're generally talking about linux specific ones and ext is awesome i'll be honest i like ext there's generally three there's e xt 2 ext3 and ext4 now this isn't necessarily like one is better than the next they each have their own features but ext4 is the newest and has the most features i'll be honest i almost always pick ext4 as my default file system not because it's necessarily better than any other option but because it's so widely used that means there are a ton of tools and utilities and tutorials online to get data back if you have some corruption so ext4 is my file system of choice mainly because it's used in so many places when it comes to creating a file system basically you need a partitioned hard drive so that you can you know have a partition or that empty field in order to draw the lines for your parking lot or put that file system on now i'm going to say lsblk and we're going to see these are the different block devices on our system i have two partitions created i have sdb1 and sdc1 these are just 10 gigabyte partitions on 10 gigabyte drives so it takes up the whole drive and we're going to format each of them all right so i'm going to say oh here's another really cool thing about linux the file formatting or the hard drive formatting programs all start with mk for make fs file system and then just hit tab a couple times and you're gonna see all the various tools for creating the different types of file systems so let's say we're going to make a butterfs file system so we would say mkfs.btrfs and then what partition we want to create that file system on in our case dev sdb1 all right so it's created it it says we have 10 gigabytes idu's one that's the path okay so it looks like it did that without any problems let's create another uh file system so uh mkfs hit tab a couple times so i can see our options now like i said i usually use ext4 so i'm going to say ext4 dev sdc one which was the other partition that i had created on this system and it went through and it created that file system and now a really cool thing if we use the lsblk but we add the dash f it's going to show us the file systems that are on the particular block devices as well so if we do that we're going to see up here we have sda1 which remember i said red hat and sent to us use xfs well sure enough there it is xfs is used sdb1 is that butterfs file system that we created and then down here is an ext4 file system that we created on our system so that's all there is to creating the various file systems and really you can pick whichever one you want but you do have to have a partition in existence before you can create a file system on it and while yes there are a whole bunch of file systems that you can use on linux and it supports a bunch i really recommend that ext family of file systems if you have a choice and you're just trying to decide what one to use largely because it's used so often there's so much support if something goes wrong you can find a lot of help online if we want to be able to access the data that's on the drives or the partitions of the drives that we put into our system we have to mount them into our local file system now we can do that manually using tools like mount or umount or we can use the et cetera fs tab file to do it automatically on boot but conceptually what's going on is that we have a new hard drive or a partitioned hard drive and we want to incorporate that into our file system now it's really important to note that when you mount a partition or a hard drive it goes into a folder and that folder becomes what is in that drive now we couldn't mount it on this folder because this folder already has things inside of it it has to be mounted on an empty folder because it doesn't make sense to have all the things in this hard drive in this folder and then have these other folders alongside of it so it has to be in an empty folder that we mount a partition or a drive so we could actually take this hard drive and mount it into this folder if it's an empty folder and that's what we're going to do on our system is find an empty folder and then this all the files inside this drive will become part of our bigger linux file system starting at this mount point now i'm here on an ubuntu system and i have the regular route mounted hard drive and then i also have an additional 10 gig drive and if we do lsblk we can see that we have well a bunch of things here loop loopback devices but we have down here sdb1 okay this is a 10 gigabyte partition and and it's all set up and ready to mount but it's not currently mounted on our system so in order to mount that i actually want to mount it in a folder if we look inside mnt there's a folder called 10 gig and inside that folder is nothing it's an empty folder so i want to mount it on there now if we do blk id we can see a little bit more information about the device itself so here is dev sdb1 now here is the universally unique identifier for this partition uh keep note that this is something that's specified about the partition we'll look at that in a second but we know that it's type ext4 okay that's good to know and we know that this is where the actual partition lives so we can say mount now i'm going to say dash t for type ext4 usually mount can automatically figure out what kind of file system it is but if we know there's no reason not to do that so i'm going to say mount dev sdb1 and where do i want it to mount on mnt 10 gig press enter and now if we go into mnt 10 gig we're gonna see sure enough now there's a lost and found thing this is the root level of that hard drive but now it actually lives in our file system right here we can type mount alone on the on the line it will show us where it's mounted right so right here it's mounted on mnt 10 gig so let me type cd and then we can unmount it by typing umount mnt 10 gig it's not unmount it's umount all right so there now that that's how we manually mounted and unmounted if we wanted to have it mount automatically on boot we would edit the file etc fs tab all right and i'm actually going to stretch this out so we can see everything on here we can see there's already an entry in here for the root file system sda or s yeah sda1 but notice it doesn't specify it by its device it specifies it by its universally unique identifier which we could get by using that blk id command it'll get us that uuid for the particular partition now we could use the uuid for a dev sdb1 and we could put that in here to mount it but i'm actually going to use let me make another entry here i'm actually going to say that the file system is on dev sd b1 we can specify it by device or by uuid or we could even use the drive label if we want but in this case i'm just going to use the the device itself and then tab over the next field here is the mount point so i want this mounted on mnt 10 gig tab over the next thing is type it's an ext4 options i'm just going to say defaults and then the last two fields are dump and pass now dump is an old school backup program it used to dump the files to a backup this is really deprecated it's not really used anymore at all so dump you're going to want to put zero for dump the last line though pass this means do you want it to run a file system check zero means never run a file system check one means run the file system check first so you put a one on the root partition any other partition that you wanna have checked when the system boots up you're gonna put a two so i'm going to put dump of 0 you always put 0 for dump and then pass is 2. it's the second most important because the root is the first one you want to have scanned and then everything else is going to be 2. so you can have like five different partitions mounted they could all have pass of 2. now there's more to it to get it to scan automatically on boot but when you're setting up the fstab file this is where you do it so save this and now we could just type mount minus a and it's going to mount everything that is specified in fstab and we can look by saying mount and it'll show us that sure enough it remounted that because we defined it in fstab and if we reboot the computer it's going to automatically boot it or it's going to automatically mount it as well it's really easy to mount partitions using the manual tools it's also pretty easy to use the fstab file to specify it and you can either specify it by device name or the uuid that we can find out using the blk id program to scan a linux file system generally you use the tool fsck or just fisk as it's often referred to now the real key though is to have it scan automatically periodically on boots so you don't have to manually do it because here's the deal in order to run fisk the file system itself has to be unmounted that's not really a problem for secondary or tertiary drive mounts like the home directory or something like that but the root directory it's pretty difficult to unmount the root directory and scan it unless you're in the boot up process or you've booted from a cd or something so i want to talk specifically about how to set up the system to scan the the file system on boot including the root file system so that you can you know have it automatically maintain itself now when it comes to scanning automatically there's a few different flowchart things that go on the very first thing the kernel looks for is inside your fs tab file if the pass setting is set up if it's a zero you know if you have your your dump and pass and if the pass is set to zero then it won't scan it just absolutely will refuse to scan it doesn't even look any further it just stops right there and continues booting up the system if however you have that partition set up with either a one or a two a one for the root partition a two for any other kind of partition if you have it set up then it looks at the drive itself and it says okay has the maximum number of allowed mounts been reached and if that threshold has been reached it will scan the drive before it boots up if not if it hasn't met this maximum yet then it's not going to scan it's going to increment it's going to say okay i'm going to mount one more time and add it to the number of times i've been mounted but i'm not going to scan it even if the pass is set up to scan it's not going to scan it if it hasn't met the max now the max by default is negative one which means it's never going to scan because that's just the way of saying like don't ever automatically mount so by default you're never going to get an automatic scanning which is a little bit frustrating because you do want to have your system automatically scanned so on ubuntu here i have dev sdb1 mounted on mnt 10 gig okay so this is another partition this is not my root partition but it's mounted on mnt 10 gig all right if i look inside of the fs tab file we're going to see that on this partition that i have automatically mount on boot it's set up with a pass of 2 which means that it is going to check to see if it should scan automatically okay it's not the root partition so i don't have it set up with a pass of one but since it's set up with two it's going to check and say okay if it's time i'm gonna scan this so let's get out of here and how we can see what the maximum number of allowable mounts before it will scan is is to use the tune 2fs we're going to do dash l for a listing of dev sdb1 and it's going to show us this now what you want to look for in here is this maximum amount count remember i said by default it doesn't ever scan that's because this is set to negative one and you're never going to reach that because that's just the way of saying disable it if we want to have it automatically scan every so often we need to change this so we would say tune 2fs c for count and i'm going to say every 10 months i want it to scan in dev sd b1 okay so now the maximum amount count is 10. we can look at that by doing the same command over and we can see now the maximum amount count is 10. okay so what does that mean well every time the system boots it mounts the partition well we could speed that up we could say umount dev sd b1 mount dev sdb one we do that and now if we look it's going to increment it by one because we unmounted it and remounted it so if we do that a bunch of times and now we look back and see okay our mount count is now 16 and our maximum count that is allowed is 10. well why didn't it automatically scan well it only does that on boot so if we were to reboot the system it would go through the flowchart and it would say okay you have your pass set to two so that means i need to check the drive and say okay drive is your mount count higher than the maximum amount count allowed before a scan and it will be so if we do a reboot and once it's booted back up we look and run that tune 2fs again so sudo tune2fsl dev sdb1 now we're going to see that the mount count is down to 1 which means that it scanned it it ran fisk on boot before it was mounted and it reset the mount count to zero and then of course it mounted it so now it's one so now it's not going to rescan that on boot until the mount count gets above this maximum mount count and then it will do it automatically and figuring out what number to set here can be kind of tricky because here's the deal if you're on a laptop you might reboot fairly often so you're going to want to have that number kind of high so it doesn't scan every single time but if it's a big file server maybe you rarely ever reboot it maybe once a year well in that case you might want the mount count to be very low so that every time it gets rebooted it does scan for consistency's sake and then one more thing if it's a huge partition with just millions and millions of files it's going to take hours or days to scan so maybe you don't want to scan it every time it mounts it really depends on your situation so plan out how often you want it to scan and the type of situation that you're in to make the best choice for your particular partition and honestly the best choice for you might actually be just to run fisk manually just unmount the partition or boot from a cd or usb drive if you need to scan the root partition maybe you never want to automate the process you only want to do it manually for some cases that's fine too regardless it's important to know how the system works so that if you think you're automatically scanning you really are lvm or logical volume manager is basically like a software version of a storage area network it allows you to take a whole bunch of physical devices and lump them into one big group that allows you to kind of carve out slices of storage for the use in your local system it consists of a bunch of parts physical volumes volume groups and logical volumes they use the word volumes a lot in there but nonetheless it's a way of taking raw storage and combining it into a thing that you can just slice up and expand and contract and add more things to it without disrupting the existing services here's how it actually works or looks in practice so we have we start with physical volumes i'm just calling them pv and we're going to say that we have four 10 gigabyte drives now in the standard practice these could be actual hard drives they could be raid devices or they could be partitions on a hard drive it doesn't really matter what they are they're just chunks of storage and you create physical volumes out of them and then you combine those physical volumes into volume groups and so then all of these are combined into a volume group and if you add 10 times 4 you get 40 gigs so then you have this volume group which is like just a big bucket of storage and that bucket of storage has no protection now i know that's not like a feature right but i really want you to know that if you just have 10 gigabyte drives all in a bucket so you have 40 gigabytes of storage and you're just gonna take and carve out chunks of that that doesn't offer you any protection so if one of these drives fails all of a sudden you could have your end result be completely corrupt and useless so physical volumes being raw devices and not having any redundancy is a little bit scary so anyway just wanted to throw that in there once you have this volume group you carve out a slice of it and it can be a small slice like here i just said this is about seven gigs i just kind of like spatially guessed how much of 40 gigs that would be um but you can carve out a big chunk so you could have like a 30 gig slice or you could have like a 32 gig slice that would use like two plus a little of another drive basically you don't know underneath what's going on the volume group is just a big chunk of storage that you carve out a slice of and then this slice is called a logical volume that logical volume is what you format with a file system and mount on your local hard drive or on your local file system and what you install linux on so it's a long step process but it allows for a lot of flexibility and so i want to show you what it looks like in practice even though it's not this robust system with multiple physical volumes if you have centos installed they actually use lvm even if you only have one drive so i'm going to show you here if we look at the etc fs tab file what we have here is our device is called dev mapper centos dash root okay now this is like devmapper what is that well this is where logical volume manager creates those logical volumes for us to use and you'll see here this devmapper sent to us root is mounted on the root of the hard drive or root of the file system it has xfs as a file system and it's installed same thing down here they have another carved out slice called centos swap and that's mounted as swap space on the system so if we look inside dev mapper we're going to see we have sent to us root and sent to us swap now i want to show you really quickly a handful of commands so you can see what's going on if we look at pv display this is physical volume display it's going to show us what's going on behind the scenes right like what makes up the bucket of data that we're going to use we only have one physical volume i know it seems weird right why am i making a bucket out of one device well you can expand it if you want so they're giving you the room to expand later if you want to do it after it's already installed so we have one physical volume and it's a partition it's dev sda2 all right it's in a volume group named centos and that gives us it looks like about 19 gigabytes of storage okay so this physical volume is inside a volume group called centos so our our bucket with all the data is called sent to us and it looks like we carved out root and swap from that bucket so let's look at that really quick we're just going to look at lv display logical volume display and we should see two and sure enough we have two logical volumes i'm going to scroll up a little bit here our first one is named swap it's in the centos volume group because that's the only volume group we have it happens to be two gigabytes in size and it lives in dev centos swap this is an interesting thing you can you can use dev mapper and the name of it or you could use dev the name of the volume group the name of the logical volume as like in folders here so that's another way that you can reference it and find the actual logical volumes and then the same thing down here dev sent to us root the logical volume name is root it's carved out of the centos volume group and this one looks like it's about 17 gigabytes so if we look inside dev sent to us we're going to see sure enough there we have root and swap as our two different logical volumes that we could one is formatted right this is formatted with xfs and this is just swap space i know we didn't go through the process of actually creating all of the different parts but hopefully you understand exactly what lvm is doing taking physical volumes whether partitions or hard drives or raid devices combining them into a volume group and then carving out logical volumes that you can use as regular devices on your system and it just allows for flexibility kind of like a softwarebased san in your own computer building an lvm is actually one of the most straightforward things that you can do when it comes to block storage devices on linux it's surprisingly consistent all the way through the process for you know the names of the tools and it really is kind of fun and once it's built you can expand it by you know adding more drives to the system or you know stretching your existing volumes but let's go ahead and actually create from start to finish an lvm system on our linux device now here i am on ubuntu and if we do lsblk we're gonna see that well we have a bunch of stuff but down here this is the drive that our system is installed on so we're not gonna touch this one we're going to use these four devices so sdbcd and e which are 10 gigabyte devices notice i don't have partitions created on these now you can create partitions some people prefer to use partitions for their physical volumes in an lvm some people prefer to use the raw devices either one works fine they work the same the advantage of setting up a partition is that if somebody else comes to your system they're gonna see that there's partitions on the system and they're gonna know that something is already done there whereas if you leave them raw devices they might think oh look empty drives now it's kind of farfetched and you're not just going to like start formatting drives in somebody's system but that's the reason some people like to use partitions i'm just going to use the raw devices and to turn these raw devices into physical volumes we use pv create and we just make a list of the devices we want to use so dev sdb devs dc dev sdd and dev sde so it created them now we can do pv display if we want and it's going to show all the devices that we have now each of them is 10 gigabytes and the name of it is just the device itself dev sdc so the next step is to create a volume group now to do that we just do vg create which is very very nice the first flag or the first command argument here is the name of the volume group so i'm going to call this bucket because it's just a big bucket of our hard drives right of our data that's available so i'm going to call the volume group bucket and then we just make a list of the physical volumes that we want to add to it so those same volumes we just did before and then press enter our volume group bucket was created we can look at that by doing vg display there's four metadata areas meaning we have four devices our current active pv are four we have 40 gigabytes of storage just about all together because each one was 10 and now we have this thing called bucket that we can carve a slice out of if we want and use that slice and put a file system on it so to do that we're going to use i'm sure you guessed it lv create i love the consistency of these tools it's really nice here we do dash capital l and how big of a slice we want in this case let's do something that's going to span all four disks so i'm going to say 32 capital g for 32 gigabytes and then dash n the name that i want to call it i'm going to call this big underscore slice and then we have to tell it where we want to get the data from well in our case it's in bucket the name of our volume group right there is bucket so what we're doing is creating a 32 gigabyte slice called big slice out of the volume group bucket press enter logical volume big slice is created so we can do lv display and sure enough lv size is 32 gigabytes so we know if that worked the name is big slice it's in bucket the lv path is dev bucket big slice which is exactly what we would expect because we've created this volume group called bucket and now this logical volume lives inside there so the last step is that we would actually use this as an actual block device or as a hard drive in and of itself and then we could just do something like mkfs ext4 dev bucket big slice and there we go so now we've done that we could mount it somewhere and every time the system starts it's going to be available in dev bucket big slice and then if we put it in fstab it's going to be mounted on boot and then there are other tools that we can go and like lv extend if we wanted to make it bigger so we could do something like lv extend dash l i'm going to say plus 5 gigs dev bucket big slice and now it says the size has gone from 32 gigabytes to 37 gigabytes it was resized and we did that without adding anything to the system we just used the tools to change the size of our logical volume the thing to remember again about lvm is that it provides flexibility in your system it doesn't provide any redundancy so if you have one physical volume fail it's going to crash the entire volume group and logical volumes are going to get messed up so you want those pvs to be like a raid device if you're worried about something going wrong underneath and losing data but setting up lvm as you can tell is very simple and honestly even kind of fun raid is a redundant array of independent disks or drives and basically what it means is that it you take a bunch of drives and put them together and you end up with a larger pool of storage now the cool thing about raid is that it doesn't just pull things together like lvm and give you a bunch of data it allows you to do some pretty neat things with performance or redundancy so i want to talk about the different raid levels that we can offer using raid specifically linux raid linux has a software version of raid which is very powerful very robust and surprisingly efficient so basically i want to make sure we cover all the different raid types and to do that i want to show you this big scary dragon now here's the idea i'm going to i'm going to talk about raid but instead of a redundant array of independent drives we're going to say they're a redundant array of independent knights of the round table okay and in true 80s fashion our knights are going to be just squares like from atari's adventure anyway here's the deal with raid 0 our drives are set up in a stripe which means that they work together reads and writes happen across two drives very very quickly and so with raid 0 or a striped array it's very fast right you can get dual writes and dual reads at the same time the problem is let's say that the knights are attacking the dragon and the dragon is attacking the knights if we lose one of the knights well then the dragon can get right through to the king because if you lose a single drive in a striped array or in a raid 0 all of a sudden your data is gone because half of your data is written on that other drive so while it's very fast they better kill the dragon quick because if one of them fails all of their date is gone and the king gets destroyed now raid 1 is kind of the opposite it still has multiple drives but they're set up in a mirror which means every time you write something to the first drive you write it to the second drive so you have a complete copy of both now because they're in a line like this they can only attack the dragon one at a time right they can't both attack the dragon like over here in the striped setup but the advantage here is if one of them dies there's still another full night there protecting the king same thing with your data right if one of these dies you still have a full set of your data because you've been writing it to two drives the entire time so a mirror doesn't give you any speed increase because you're not spreading the writes across here you're actually writing all of your data two times once to each drive so you don't get any advantage over a single drive when it comes to speed but you do get that advantage of either drive can die and you still have full protection for your data now raid 5 this is a little bit different raid 5 uses a parity disk and how that actually works is like some digital magic that is beyond the scope of this nugget and to be quite honest it still stumps me a little bit but conceptually how it works is you have multiple drives three minimum and any of these drives can die any one of them it doesn't matter which one and all of your data is still in place so you get the advantage of being able to lose any drive in your array the disadvantage is you lose one drive's worth of storage now what do i mean by that let's say these are all five gigabyte drives okay if these are all five gigabyte drives together that would be 15 gigabytes but you lose one drive's worth of storage for that parody magic and so if you have three five gigabyte drives you're only going to have 10 gigabytes of usable space but the advantage is it's writing to multiple drives as it's going along and if one of the drives dies you still have all of your data represented in the remaining drives now if you lose two of course then your date is corrupt and and you're done but you can lose any drive and it's not just like this is the drive you can lose no you can lose any one of these and all of your data is still there it really is magical and so that means that you can lose a knight and the dragon is still going to be stopped by whatever two of the knights are still protecting the king hopefully that makes sense with the different raid levels and just briefly i want to talk about there are some hybrid levels as well so raid 0 could be raid 0 1 where you have like four drives and what happens here is you have a stripe of mirrors your drives are mirrored and then striped across the mirror or raid 10 which is a mirror of stripes or vice versa but basically four drives and you're mirroring two two of them and then striping those two mirrors and vice versa you're gonna stripe them and then mirror those two stripes with raid five there's actually a raid six which is cool but it requires another drive and then you can lose up to two drives and still have your data there's two of those parity drives in place now the downside is you lose two drives worth of storage on your full array but it's awesome because you can lose more than one drive now even if you don't follow along with my awesome dragons and knights kind of scenario hopefully the raid levels make sense now my trick my little mental trick to remember what it is i look at the number after raid and i say how many disks can i lose because for years i would confuse raid 0 and raid 1. but here's the deal with raid 0 you can lose zero drives right because if you lose one you lose your data with raid 1 you can lose one drive and you still have another drive raid five i guess you could lose one of five i don't know it kind of falls apart but rate zero and raid one are the ones that i would always struggle with so i think of that number as how many drives i could lose and still have my data you can go to the store and buy a raid card like a hardware raid device and then you can use that on your system and you'll be able to have hardware raid but linux has a really awesome and powerful software raid program that will use kernel level tools to allow you to create your own raid devices without needing any specialty hardware at all now there's a couple things we need to discuss like partitions versus using raw devices i want to make sure we cover all the configuration stuff but conceptually it's really easy rather than having a hardware based card we just use our regular sata controllers and then our hard drives can all work together in a raid array that we choose now when i talk about partitions versus raw devices let's say we have 200 gigabyte hard drives but they're from different manufacturers now they both say that they're 100 gigabytes however if you look close they might have slightly different number of sectors and slightly different size so this one says it's 100 gigabytes but it might be actually 1028 megabytes and this one says 100 gigabytes but it might actually be 1022 megabytes now they round for marketing purposes and that's perfectly fine and usually on a system it doesn't matter but if you have like this drive fail in a raid array and you need to replace it with another drive and you try to use this drive and all of a sudden oh you created a raw disk device raid array unit and it has 1028 megabytes of space and you try to replace it with another hundred gigabit drive but this one only has a thousand and twenty two megabytes you're not going to be able to work it because this isn't big enough so generally what people do is you take and make a partition inside of your drive that is slightly smaller than the hard drive itself so it might be 99.9 gigabytes and then the same thing when you have a new drive you're going to have enough room to create a 99.9 gigabyte partition and so even though this drive itself is slightly different the underlying partition is going to be the exact same size so you're going to be able to use it to replace a failed drive in an array so that's why we generally use partitions even though using a raw device would work until you need to replace it with a smaller drive that is your replacement now here on our ubuntu system if we do lsb lk we're going to see we have four 10 gigabyte drives that are installed in here we're going to make a right array with those this sda1 this is our root partition this is where our system is installed but these drives down here are the ones that we're going to use to create our raid array now i've already partitioned the first three you'll notice it's a 10 gigabyte drive and the partitions are 9.9 gigabytes awesome we're going to do the sde the last one together and so fdisk dev sd e and the first thing well we can press m to see all of our different options but i'm going to go kind of quickly i'm going to say o to create a new partition type and it's going to be a dos one it doesn't have to be dos but i just decided dos and then i'm going to say n for a new partition and i want the partition to be primary so default number one default the start point on the drive default and here's where rather if i choose default it's going to be 10 gigabytes in size and that would be fine as long as our replacement was the exact same kind but i want to do it slightly smaller than the drive itself so here i'm going to say plus 9.9 g and press enter and now we have it 9.9 gigabytes in size so any 10 gigabyte drive we'll be able to replace it with because we'll just create a 9.9 gigabyte partition inside now the one last thing if we do t for type and press enter it's going to say okay what partition type do you want here it says it created a new partition with type of linux but if we type capital l we're going to see all of the available codes here now this is not a format this is just like a flag to give the kernel a hint as to what sort of partition this is supposed to be so the one that we want to put on here is actually f d for linux auto raid so i'm going to type f d and now it says change type of partition to linux auto raid or linux right auto detect i'm going to press w to write this change to the disk and now if we do lsblk we're going to see we have all of them here now that partition type is just to give the kernel a hint if you put these drives in a new system it's going to say oh look at those partitions those are part of a linux array or of a raid array so we're going to treat it as such so it's just a hint but it works even if you don't change that partition type all right so now it's pretty easy to create the actual raid device we're going to create a raid 5 device with four 10 gigabyte devices well about 10 gigabytes and so we should end up with about a 30 gigabyte usable space with our raid 5 array now the tool we use is mdadm and we're going to say dash dash create because we want to create a brand new one i'm going to say verbose just so we can see it do things as we type it in and now what device do i want to create well the devices are dev md and then the number of the raid device you're creating so we're going to start with md0 because that's our first device and we don't have any raid devices on here yet i'm going to do dash dash level equals 5 i want it to be a raid 5 device and then dash dash raid devices equals 4 because we have 4 devices and now we need to list those devices out and we're going to list the actual partition so dev sdb1 dev sdc1 dev sdd1 and dev sd e1 we'll press enter boom it created it that quickly now we can see the details of it if we were to look at md stat this is the virtual file system proc and this is going to show us the md stat which is the current rate arrays in our system so here we have it's currently a raid 5 array it shows our devices here lots of information it says recovery that's because it's building the array but we can use it while it's currently using the array which is really awesome okay so we look into dev grip for md we're going to see there we have md0 so we have a device all created and we can now use this as a hard drive in our system before we do that though i want to save this configuration of this raid5 array into our system so that on boot it knows exactly what sort of array to build to do that we just do md adm detail scan and if we do that it's going to show us the configuration so that's the configuration for our current array what we want to do is save that so i'm going to redirect it into etc mdadm md mdadm.conf so we're going to save that result into this file and now every time we boot the system md0 is going to be created and then we just treat it like any other hard drive on our system so nkfs dev md0 and boom it created it and now it's part of our system and it's going to be about 30 gigs in size let's see lsblk and we look down here md0 shows up as 29.6 gigabytes about 30 gigabytes of raid 5 storage on our system if you're thinking that was a little bit too easy well you're right linux raid is awesome it's super simple to set up it allows you to use the regular drives in your system and set them up as a raid device so as long as you can save that file into mdadm.com that detail scan that we did and you can check for the progress or the status of your current rate array in mdstat you are really set that's all it takes to use raid on a system using nothing but software provided with the linux kernel installing tarballs sounds like some sort of prank you might pull on somebody in high school but really this is the way that we would install software on linux for years before package management systems came out now you can still download tarball files of source code for programs and install them although it's not terribly common anymore now we used to refer to this as the three step we would extract compile and install and this is the process to convert source code into an executable program now i'll show you how to do that really quick i've downloaded already a very simple program as a tarball file it's called sun weight it's just a simple program that waits until the sun goes down and then executes so if you have a script that you want to run at sunset that's kind of a cool tool to use to do it but the first step is to extract it so we're going to say tar minus the xvf sun wait we'll go into the folder that's created if we type ls we'll see there's a couple files in here this is the source code and also a make file now sometimes there's more complicated things like dependencies sometimes there's going to be a config file so we'll run config and go through that process and it'll tell you if you need other dependencies since we don't have a config file i'm just going to type make because make will compile it into an executable program now you'll notice we got some warnings but we didn't get any errors a lot of times if it's a big program it'll say oh i need this dependency or oh you forgot this library and you'll have to download and install those dependencies before you can compile it but this one is very very simple if we type ls again we're going to see now we have a result here we have the sun weight program now we can execute it right here by saying dot forward slash sun weight and we'll see sure enough there it runs and we could use this to wait until sunset to execute a program if we want to install it though we have to either copy it to our user local bin folder or sometimes they'll include in the make file an installer so we could say something like make install this one doesn't actually have that ability to install it it's a very simple program so if we wanted to install it if you get an error like this you just simply say okay i'm gonna move sun weight to user local bin and now if we type sunway it's going to execute because it's in our path now i told you that threestep process is really really simple and it is you just basically type make and it compiles and then you have a binary that you can install either using a script or just putting it in your user local bin file there is a big disadvantage though if you compile things from source and that is there are no update mechanisms for getting a newer version if you use a package manager it'll update old software but if you just compile it yourself and install it manually there's no way to update it and that can be a real problem especially when security concerns come up so while it's important to understand how to use tarballs to compile and install programs it's not really the best way to go about it if you have any other options deb files are the way that programs are packaged up in the debian and ubuntu world now there's a couple different ways that we can manage the subsystem and there's a little bit of confusion as far as what tool to use now behind the scenes they all use d package which is like the the lowest level of interaction with dev files and i'll show you why this is not what you use on a regular basis in order to actually install packages on debian or ubuntu there are three different options though for installing packages using the proper system apt aptget and aptitude now i want to talk about them because they do the same thing but it's just a matter of being replaced by something better so apt just apt all on its own is the newest program to interact with the app system it's new it's simple this is the one to use so just use apps i'm going to try to use apt if i can aptitude you may find online instructions people telling you how to install packages with aptitude this is older but it still works i don't recommend you use it though it's a little bit strange when it comes to dependencies but it's been outmoded and now apt is the way to go now here's the other thing apt get has been around for a very long time it's the oldest of the three it still works but i don't recommend you use it because again apt is by far the best way that you can go about installing packages here's the real problem though i've been installing packages on debian and ubuntu for so long that sometimes if you're watching me in a nugget i might use aptget by mistake just because it's a habit it still works there's nothing wrong with doing it but the proper way to go about installing packages is to use apt that's what i'm going to try to do and that's what i'm going to show you now now on our ubuntu system if we type ls we're going to see i have a deb file so it's kate kate is a text editor that works in the kde environment and this is the installer the dev file now remember i said that d package is the program that is used behind the scenes that's how you interact with dev files here's the problem so i'm going to say sudo d package minus i for install kate.deb it's going to try to do it but here's the problem it doesn't resolve any dependencies so if i want to use d package to install it i'm going to have to find every one of these deb files on the internet download them install them one by one find out if they have dependencies and it can be a real mess so thankfully the apt system takes care of all the dependencies for us so i have to erase this so i'm going to say pseudo d package minus r kate and it's going to undo the mess that i made and now we're back to square one rather than downloading the deb file we can use the app package management system and just say sudo apt install kate and it's going to look for the latest version and get all of the dependencies and you can see there are a ton it would have taken me a week to come up with all these dependencies so if i say yes it's going to install them all and it's finally finished that took like over three minutes i sped it up so i didn't have to sit here with me the whole time but now all we have to do is run kate because it's installed with all of its dependencies on the system and here it is our little text editor kate now another nice thing about using a package management system is we can keep things updated so we could say sudo apt update which is going to download the latest repository information to find out all the updates that are out there for us to install and then once we have the freshest versions of what's out there we can use that same program apt and say sudo apt upgrade and looks like we have a couple things we could upgrade hit enter and it's going to keep our system up to date that easy not worrying about dependencies it does all that on its own so while all three of these will technically work for installing and updating packages you really want to use just the simplest one which is apt it's the newest and it's the easiest to remember rpm is the red hat package manager and it does just that it manages packages on a red hat based meaning red hat or sent to us or anything else that uses the rpm system that's how it manages their dependencies and their installation and their programs and updates etc now there's a few tools we need to know how to use in order to really utilize rpm yum is kind of the de facto standard but there's a new kit on the block called dnf i want to talk about that and then of course rpm itself is not only a package management system but it's also the tool the lowlevel tool that we use to actually handle packages now the cool thing about rpm is there's no two steps required when it comes to installing now what do i mean by the twostep process well here we have our multiple programs i just mentioned now yum is yellow dog updater modified which may seem silly but just for a brief second yellowdog was a linux distribution that ran on powerpc or old apple hardware and its claim to fame was actually that it started this yum program for managing packages now the operating system itself didn't do well after the power pc platform kind of faded out but yum is still around today and in fact that's the program that we use on almost every rpm distribution it handles dependencies um it is it updates the repo information as it's installing and upgrading so unlike app you don't have to say like yum update and then yum upgrade when you upgrade yum updates before it does anything else now dnf which and i'm not kidding here it stands for dandified yum but dnf is the new program it's in fedora right now it's not in centos but it's going to replace yum it's just a rewrite it has some features that are that it worked better but similarly structured and how it works and then of course rpm is what happens behind the scene there's a program called rpm and this is the low level tool but it doesn't handle dependencies so we generally don't use the rpm tool on its own let me show you what i mean i'm on centos here and we're going to use yum to install packages but first i want to show you rpm now if we look i have downloaded this program called kate this is just a text editor that works in the kde environment but if we were to install it with rpm we would have a problem if we say rpm dash i for install the name of the package it's going to say i can't do that because you have 110 billion different dependencies that aren't installed so what you'd have to do is find every dependency every rpm install them one by one these probably have dependencies of their own that we'd have to track down but thankfully that's where the package management system yum comes into play so we could simply just say yum install kate and it's going to go to our repositories it's going to update the cached information like show us the latest information from those repositories and then it's going to search for kate it's going to find all of the dependencies right here say it's looking for all the dependencies and the dependencies of the dependencies and now it says would you like to install it along with the 77 dependent packages okay i'm going to say yes actually i'm going to say no if we install it then it would just download install all of those packages and life would be good and we would have kate installed but what i'm going to do is now show you that's how we would install a package but if we want to update the system all we need to do is type yum upgrade and it's going to query all of our repositories it's going to download all the package information dependency information that we need and now it's just telling us if we want to get our system updated we're going to have to download 116 packages install one new package and it's going to take 308 megabytes of space do we want to do that i'm just going to say yes and it's going to go through the whole process of downloading all those rpms it's going to use the rpm tool in the background install each one of them one by one until they're all updated and all installed to the latest version so yum is very simple to use and knowing that it comes from you know a distribution that's no longer even a valid distribution i don't know that's just cool for me that you know yum is still around even though yellow dog hasn't been a distro for many years now i don't have a fedora system to show you dnf first hand but it's a very similar program to yum it's going to work very very similarly so if you find yourself on fedora just use it very similarly to how you'd use yum and you'd be fine one of the big things i want to point out though is that there's no twostep process like there is with the apt environment meaning you don't have to update your repositories before you install it you just run yum and it's going to update and upgrade all the information before it does any installing or upgrading one of the nicest things about the app package management system is that you can add repositories which are just different groups of software packages so if something's not in the standard ubuntu or debian system and somebody else has written something you can add a whole repository of new software and then the app system can use it just like any other package on your system it's pretty cool now we're going to look how to add something and there are a few gotchas that we need to look at but thankfully it's a fairly simple system when it comes to configuring new repositories so i'm here in the etc apt folder on an ubuntu system if we do an ls we're going to see there are two things i want to show you there's sources.list this is a folder and there's actually nothing inside there right now but we could create a new file.list in here and it would be read just like this systemdefaultsources.list so it doesn't matter where things go i'll just add something to the existing sources.list so we're going to look and you'll see there's already a bunch of repositories added each one of these lines is a repository containing software that the apt system can install now some of them are commented so we would just uncomment the existing ones like here this is a partner archive we could uncomment this and then this would be an active repository but if we want to add a third party repository we can like i said either add it at the end of this file or create a new file in that sources.list.d folder and call it like new program dot list but i'm just gonna add it in here and i'm going to add the opera browser repository in here let's say we wanted to add the opera browser it looks like this it's okay if every field here doesn't make perfect sense to you this is just the format it says what kind of a package it is where it's stored what folder in there and then what version of the software we want to actually add the stable and then it's nonfree as far as like what what type of repository it is so this is the line right this is where the opera browser is stored on the opera website so if we save this we would be able to do apt update no dash apt space update and if you look it is updating them all but i do want to show you back at the top here it gave us a little warning slash error oh and it repeated the error down here so i'm going to show you here it says this repository is not signed so what that means is we don't have their key because we don't want there to be a man maninthemiddle attack where somebody you know takes over opera.com and then starts sending us bogus packages so they do signing key signing in order to make sure that we get the right software so if we want to use the repository we have to add their gpg key to our system now it's not difficult to do we just do kind of a two step here we have to download the key itself so wget so we're going to download the file and then pipe that into apt key add all right so we'll do that it's going to download the key and it's going to add it to our system and it just says ok we can see if it's there by typing apt key list and it's going to list all of the keys in our system and if we look we should be able to find sure enough here's the opera one so there's the upper key that we installed and now if we do apt update notice there's no errors at all it updated and now we could just install it using our app system app install opera stable and it would install our needed packages you know it would resolve dependencies even if they're in other distributions and it would install it for us just fine i'm going to say no because i want to show you another way you can add a repository and this is kind of a cool thing that ubuntu added it's called a ppa or a personal package archive there's a particular text editor that i really like it's called adam atom and they have a ppa which is a repository and to add it you just say add apt repository ppa colon and then where it lives or what user it is web team and the name of the repository is atom press enter it's going to do a couple things it's going to add it to our sources.list but it's also going to download that key for us so all we have to do is say press enter and it's going to install the key it even does an apt update for us and then we would just do apt install adam and sure enough it would be able to do it from that package or from that repository that we just added it can be a bit overwhelming once you have to start adding gpg keys for the repositories that you put into your sources.list but ppas make it really really really simple they do everything for you i really like ppas i think it's been a wonderful addition to the way we handle apt packages repositories in an rpm system using yum are very similar to that of an app system adding them is maybe even easier and editing the config is about the same just with different files let me show you what i mean now the main configuration file in centos is going to be the yum.com file so let's look at that it's in etc and it's yum.com here's where we can do a couple things one we set the main configuration things like where the cache is stored and things like this one thing i want to point out in here is this gpg check now you can actually turn off yum's ability to verify using signed key pairs and this will just allow it to install any repo or install from any repo that you install in your system it's not a good idea to turn this off though because again this is a safety check to make sure you're not getting a man in the middle attack but the main section is at top you can put new repos in the bottom or like it says right here you can put them in etc yum.repose.d as individual files with the format file dot repo that's usually what's done but we could add them here let's look inside the yum.repos.d folder so let's get out of here go into etc yum.repost.d and here's where all of the repos that are currently installed on our system live so let's look at one really quick look at the base file here and we're gonna see how it is set up so there's the base configuration up here the name of it the release where the actual files are stored gpg check you can turn this on and off for individual repos as well so if you set up a repo and you don't have signed key pair you could turn it off for just one repo and then this tells us where the actual key file if it's turned on lives so we can put our put it in there manually so that yum knows where to look to find that key now normally these aren't added manually but you could type all of these in these are all individual repos that are defined in here in this bracketed section usually what you do though it's really elegant is let's get out of here you just install the package so one of the really popular packages is called apple which stands for extra packages for enterprise linux and to do that we just do yum install apple release i'm gonna say yes to install it and now if we look in here we're gonna see here we go the apple.repo has been added so now if we were to install packages apple would also be one of the places that we could pull packages from we can just cap this to look at it and we can see it's enabled the gpg check is on it tells us to use this key file for checking the signatures and that's all there is to installing a repository in yum and since yum automatically updates we don't have to like update the cache when we install a package it's going to automatically fill in the blanks for us yum is an incredibly awesome and powerful package manager and we can tell just by how easy it is to add a repo and edit that config either using the yum.com file or the individual files inside etc yum.repo.d apt and yum are certainly the most common package managers out there but there are a few less common package managers that you should still be aware of now arch linux uses a program called pacman for managing their packages and open souza uses zipper now i'm not saying that the mascot for zipper is a purple horse but i'm just saying maybe it should be nevertheless it's not too difficult to use them even if you're not familiar here i have two terminal windows to two different linux distributions the first one is arch linux which uses the pacman package manager so if we just do pacman minus h we're gonna see here are a list of the commands now it's not immediately clear how you go about installing a package unfortunately it's not just install it's actually capital s for sync we kind of want to sync the system into a state that we wanted so to install a package it isn't difficult once you know what to do just pacman minus capital s and let's install vim my favorite text editor you have to actually spell pacman correctly pacman minus capital s vim it'll say do you want to install it and say yes it's installed it and now if we type vim you can see sure enough there's vim my favorite text editor now over here in open souza it's a little bit different here we use a program called zipper zypper we're gonna do zipper minus h and it'll show us all of the help commands that are available this one it is pretty easy you just do install or you can shorten it to just in so we could say zipper in vim we'll install vim again press enter it's going to retrieve the repositories online just like yum or apt would do and then it's going to install the vim package for us and we'll be able to use it on open souza it'll ask us so we want to continue i'm going to say yes and it installs all of the packages and the dependencies so now same thing here we get startup vim and sure enough there's vim this time on open souza pacman and zipper are really two of the more popular alternative package managers but there are some others if you're on slack where you're gonna have to install things by hand using tar.gz files but these two along with apt and yum will get you through most systems managing local users on your server is really easy and there's a bunch of tools that make it even easier now there's the standard command line tools that allow you to add modify and delete accounts and there's this super cool script that i really like which makes adding a user very very easy there are a whole bunch of different facts about a user that is stored in the system though full name username password all of these things plus some others that aren't even listed are on there and it's important to know that the tools will manipulate all of these but you don't have to specify every single thing every time especially things like office number those aren't even really used anymore but there are possibilities that you can add specific information in the local group now i just want to go right to the command line so we can actually start adding users and at first we're going to use the tools that you know kind of come with it like the low level tools user add user dell and user mod so first of all i want to say user add because we don't have any extra users on our thing i'm going to do dash h and that'll give us the help screen now you can see there are tons of options for adding users but the format is pretty much the same user add whatever options you want to add and then the login or the username for the new user now we're going to use just a couple and i want to show you the problem with using user ad as opposed to that fancy script add user we have to determine like a home directory and a user shell and all sorts of things like that so let's say we wanted to make a user we're just going to say now notice i'm root we have to be root to make a system user but we would say user add and i want to do minus d home susie minus s for shell i'm going to use the bin bash shell and her username is going to be susie press enter it's all done now there is no password for susie we'd have to actually do that with the password command uh we'll actually say passwd suzy all right so now she has a new password and we can here let's start a new window ssh suzy at localhost susie and we're logged in as susie oh but see it says unable to change to directory home susie there's no such file or directory well dog on it we said that that's our home directory but by default it doesn't make that directory we actually have to make that directory or there's another command dash m which will create it as we're adding the user so it's possible again to use this tool and set all of those different flags up as we create the user if we go through all of these things but it's much easier to use the add user script and i'll show you what i mean add user frank and now it says okay i'm creating the frank user added a new group for frank created frank's home directory copying all the files from etsy scale and now it's asking me put to give frank a password so i'm going to do that now the full name of frank his room number which again we don't use room number work phone all those things anymore really but full name is nice to have in there and is this information correct i'm going to say yes and so now we have a really nicely set up user so if we were to open a new tab again and do ssh frank at localhost log in with frank i have to type frank's password correctly and now you'll see sure enough if we do pwd we're gonna see we're in frank's home directory he has a perfectly usable account because that add user script goes through and remembers all the various things that we need to do so let's get out of here now there is another tool we can use user dell so we can say user dell let's do minus h so we can see the options there's really only one option that's ever really important that is minus r which removes the user's files so we could say user dell r for remove frank and it removed all those files that said there wasn't any mail form so i couldn't remove that but now frank's home directory is gone now we could modify suzy's account if we use user mod so let's see user mod minus h and we're gonna see same thing we have a whole bunch of tools and this is just to modify an existing user so we could do something like change her shell like right now it's been bashed but we could change her shell so we could say user mod minus s let's say bin false susie what this is going to do it's going to change her shell so if she tries to log in it's going to fail so let's open up a new tab and try to log in as her ssh susie at localhost it logged in and then immediately logged out see it says it still doesn't have a user account we are a user directory we didn't make her a home directory but then it says connection to localhost closed that's because as soon as it logged in her shell is been false which immediately exits and then we're back to being logged out we're logged in as bob here so the ability to modify a user account is really important but if you're going to add a user i highly recommend you use that script you can certainly use the manual tools with all the flags to add that account but add user just makes it so easy by remembering all the steps and i'll be honest i often have a difficult time remembering is it add user or user ad add user user ad for my own sake i like to think okay alphabetically add user is first and that's the first tool i want to use for adding a user so that's just the trick i use regardless of what tools you use to add users it's important to understand that modifying them on the command line is fairly simple and not that difficult to learn local groups on a linux system are fairly straightforward to handle but it's important to understand the difference between primary groups and secondary or supplementary groups on each individual user now every user on the system is going to have their own personal primary group now usually that's going to be the same as their username so bob is going to have a username of bob and he's also going to have a primary group of bob and that's usually how it goes it could be a it could be a different group that says primary group but almost always that's how the primary group is going to be on a linux system and then there are all of the supplementary groups which happen to be like things that he belongs to like maybe he's in the admin group maybe he's in the sales group marketing third floor public all of these groups are in addition to his primary group and it will give him access to certain folders on the system that he might not have access to if he didn't have these different group memberships now in order to actually create groups the tools are very straightforward just like adding users with much fewer options so let's actually look at that and i want to show you how to manipulate users in groups on a system now here we are on an ubuntu system and we can simply say group add public it's going to add the group public to our system we could say group ad sales and it's going to do that now we could look at the different options group add minus h you see there aren't too many options we could specify a group id number if we wanted and we could do some things with group mod to change some of those features like the group id and then of course we could delete them with group dell and the same sort of thing you know we just delete them as we would use user dell on a system to get rid of a user so this is pretty straightforward but the part about primary and secondary can be confusing so right now if we type groups bob we're going to see that bob is in all of these groups the first one listed is his primary group so bob's primary group is bob and then these are all of his supplementary or secondary groups now i want to show you user mod minus h because it shows us how we can manipulate groups right here so lowercase g is how we force a user to a new primary group so we could change bob's primary group using the lowercase g flag on user mod in order to change his secondary or supplementary groups we use a capital g but here's where the gotcha comes into play if we do like minus capital g public it's going to delete all of his other supplementary groups and only make him part of the public group and that's not what you almost ever want so there's this nice dash a which means append the user to another supplementary group without getting rid of the additional groups that he already belongs to so let's actually do that in practice so we're going to say user mod minus a for append minus capital g public bob okay and now if we do groups bob we're going to say that bob still belongs to all those secondary groups and also to the public group which is exactly what we wanted to have happen because if we wouldn't have used that a this is what would have happened if we would have done user mod minus g let's do sales for bob this seems like it's going to do the right thing no errors but then if we do groups bob we're going to see uh he still has his primary group of bob and then just sales and so we'd have to manually go back through and add him to each individual group which is really a pain in the butt so you don't want to do that you want to always remember to use the dash a now in function the difference between a primary group and a secondary group the primary group is what is used if bob were to create a file so let's open up a new tab here we are bob say this is in our home folder if bob were to touch a file and we do ls minus l we're going to see the file that we just created is owned by bob and the group membership is bob's primary group of bob so that's really the difference between primary and secondary is when you have a primary group that's what the group membership of a new file you create is going to belong to now creating groups is really easy with the group ad group mod group dell tools so i didn't even go into that very much in depth the real important takeaway is the idea of primary versus secondary or supplementary and how to take individual users and put them into groups without deleting all of their other supplementary groups that they already belong to i hope this has been informative for you and i'd like to thank you for viewing figuring out what users are on your system and what accounts they're using and what they're doing is really an important part of forensics but also it's an important part of a sanity check like why is my computer running slow or what user am i logged in as and there is a handful of simple command line tools that we're going to go over that will just help you figure out the users on your system now the first scenario this happens a lot if you log into embedded systems like routers and stuff where you don't get a prompt that tells you who you are you just get like a hashtag here well what you can do is say who am i which seems a little bit silly but it will just give you the user that you're currently logged in as if you're starting to use sudo and su and you're sshing from one computer to another sometimes just figuring out what user you're logged in as is really important so that's it seems silly but it's something that i use in nuggets even you'll see me use that quite a bit another one if we just do who it'll show us who is logged into our current system it actually gives us quite a bit of information we have bob frank it looks like susie is logged in twice and it tells us when that person logged in and where they're logged in from so bob is our current user and i'm logged in on the display zero meaning i'm using an x windows session here and then frank and susie are both logged into localhost probably ssh and then suzy is logged in from a remote computer probably over ssh as well so who tells you that w which is like what although you don't have to type the rest of it it's just a w gives you the same information with a little bit more so these are the users where they're coming from when they logged in how long they've been idle and then what again that's where that what came from what they're actually doing so here i'm using a gdmx session uh looks like frank is sleeping on the job frank judging you buddy and then susie's just logged into a bash terminal so these are tools that give you information another one is pinky which seems like a silly tool but it replaces the older tool finger that has kind of been abandoned about 10 years ago but this gives you even more information but not the what's going on this gives you the login name full name when they logged in how long they've been idle where they're coming from and a combination of these really usually any one of them will give you the information that you want but these are all here to give you information on who's logged into the system now if you want further information about it you could also do id susie and it will give you information about susie's id including her user id her primary group id what groups she belongs to looks like she only belongs to the susie group so let's do an id on bob and yeah bob belongs to a whole bunch of other groups so it'll give you all of that and then lastly lastly there's the command last hahaha if you type last it gives you a history of the people that have been logged into the system recently so it looks like we go back all the way to march 7th here today's march 13th and i logged in a bunch of times just so we would get some results here and we can see who logged in where they logged in from and when they logged out or if they're still logged in so you can see i logged in at 1001 logged out at 1001 up here logged in at 1003 still logged in from the local host computer and that is susie so there's a lot of tools that you can use to figure out who's logged into your system how long they've been there where they're coming from what they're doing and these really might seem like throwaway commands like why would i need to figure out who's logged into the system but knowing the simple little tools like who and w and last can really be convenient when you're tracking down what's going on on your server who's logged in who was logged in did somebody log in bob said he logged in did he really log in well check out last and it'll tell you the point is there's a bunch of tools that are available that'll give you information on users on your system whether they're logged in not logged in or have been logged in in the past passwords and group memberships are things in linux that are stored in text files just like everything else but the problem is we don't want people to have access to our passwords and so there's kind of this elegant system of shadow files that has been invented this is fairly recent in the world of technology it used to be everything was stored in a single file called etc password but things have been changed now so that everybody doesn't have access to seeing the encrypted passwords so how it works now is this let's say this is our user bob now bob when he logs into a system has to be able to see what his home directory is so he needs to have access to a lot of information about his account however we don't want bob to have access to everybody's encrypted passwords so that's kept over in another file called a shadow file so what we have in the password file is we have bob's username and then literally just an x that takes the place of where the password used to be stored and if this x is here then the system knows okay i need to go over and look in the shadow file for the actual encrypted password of bob and then it does system authentication with root access as opposed to bob's access which is to read the password file but he doesn't have access to everybody's encrypted password the reason we don't want that is if he were to do like a brute force attack if he had access to everybody's encrypted password he could just keep hammering away at it until he finally figured out what the password was we don't want that so we don't want every user to have access we just want root or the system to be able to authenticate and check out you know the encrypted password file so anyway there's a neat and elegant system of how this works now notice obviously the etc password file is readable by all the etc shadow file is only accessible by the root user but when it comes to editing those there are special tools to make sure that we do it properly now first let's actually look at the files so if we do an ls minus all of etc pass sswd we're going to see that sure enough it's owned by root and everybody on the system can read it right only root can write to it but everybody can read it but if we look at the etc shadow file we're gonna see that only root can write to it and only people in the shadow group can read to it everybody else in the system can't even read it so our encrypted passwords are protected now there's also if we do ls minus l etc group we'll see the same scenario where this is the group definitions for users on the system and the same settings permission wise as etc password has and if we do ls minus l etc g shadow we're gonna see it's the exact same thing as the shadow file now groups don't normally have passwords associated with them but they can so the shadow system does the same with the group file as it does with the password file now if we wanted to edit one of these files we could do something like sudo vi et cetera pass wd and it's going to let us edit this file using our text editor but this is not the ideal way to go about it because we want to do it the shadowy way right we want to be able to edit it and then be sure that the shadow file matches so let's get out of here let's not make any changes to properly edit these files what you do is say sudo vipw this is part of the shadow package and it's going to ask us what editor we want to use if you want to use nano this is the easiest one like it says right here easiest you can use this i prefer to use vim so i'm just going to choose selection number two but number one is perfectly fine and it opens up and looks exactly the same and we make changes here and we could like go down here you'll notice all the things about bob are listed in this line right here's his username his password is just a placeholder here as an x because the actual encrypted password is in the shadow file but his user id is group id his full name his home directory his shell we could make any changes we wanted here and then we would just go and save the changes and this is what it would tell us it would say okay you've modified the etc password file you may want to modify the shadow file too and to do that you do vipw dash s for shadow so the same thing we would say sudo vi pw dash s and now this is the actual shadow file that we're editing and we can go through and if we wanted to make changes here this is bob's encrypted password now this is obviously not bob's plain text password his plain text password is just the word bob we know that but this is what it's like encrypted but since this is only accessible to root there's nobody who's going to be able to do a brute force attack to try to decrypt this because they don't have access to the passwords encrypted themselves so we'll get out of here and now the exact same thing with the group password or the group file in the group shadow file is done too we can say sudo vigr and this is going to edit the group file and sudo vi gr s and that's going to edit the g shadow file all right so that's the proper way to go about editing those files manually if you man if you edit these files it's going to do the same thing as if we did like user mod and changed somebody's home directory it just does it by editing the underlying configuration files so yes group and password files are still just text files but there is this elegant shadow system that allows us to make sure that the right people have access to the encrypted passwords and not everyone on the system can see everyone else's encrypted passwords quotas are the way that we make sure users or groups don't overuse the hard drive we don't want a particular user a particular group to use up too much of a hard drive and stop other people from saving files now there's soft quota limits and hard quota limits the difference is with a soft limit you're warned every day hey you've gone over your limit hey you've gone over your limit whereas if you reach the hard limit you're no longer able to save files at all now there's a couple things that we have to do to get our system ready for using quotas and keeping track of things but the first thing is we have to make sure that our partition is mounted correctly so on our system here i have a disk mounted or a 10 gigabyte drive mounted on mnt disk okay now if we look into our etc fs tab file i'm going to show you how you mount it so that quotas are enabled so here's our drive it's dev sdb1 and it's mounted on mnt disk it's ext4 i've used default mounting options and then i've added a comma and usr quota now we could also put grp quota we could put another comma and grp quota i'm just going to do user quotas group quotas work the same way so we'll learn one and we'll know how to use both but we have to make sure that it's mounted this way so if you're making this change you want going to want to reboot your system to make sure that it actually takes effect and then if we type mount we're going to see that sure enough mount disk or mnt disk is mounted with quotas enabled specifically user quota management okay so we know that the drive is able to support it but out of the box quotas are not turned on so what we need to do is first of all scan the existing drive for files owned by a particular user so we need to actually do sudo quota check dash a for all partitions that support quotas dash u for user owned files press enter it's going to go through it's going to check our drive and now if we look in mnt disk we're going to see sure enough now there's a quota file that has been created and it shows just you know all the files on the drive who owns them which is none right now but we're going to change that in a minute now the other thing is so that prepares the drive but we actually have to turn quotas on so we're going to say sudo quota on a for all supported partitions so now quota quota ing is actually turned on but if we want to set a particular quota for a user we're going to have to use ed quota so sudo ed quota and i want to do this for the user bob on our system so we're going to do it for bob and then we get this list now it's going to show us all the file systems that support quotas in our case that's just this one dev sdb1 now i have to explain really quickly there's two kinds of quotas we can do we can set up quotas for inodes or we can set up quotas for block usage now an inode means a file so we could say how many files a person can store on a particular partition but this isn't all that useful right i mean what if they have two files but those two files are like 27 gigabytes each so rather than set limits on inodes i tend to like to set them based on blocks now by default these are one kilobyte blocks so this zero means how many are currently in use and there's nothing on the drive owned by bob so that's set to zero right now but i'm going to make some changes here i'm going to say i want the soft limit to be 500 kilobytes and i want the hard limit to be 1000 kilobytes okay or like one megabyte this is not practical number you'd probably do something bigger in real life but we're going to save this say yes and now as bob if we go over to mount disk ls we're going to see that's in there now there's no usage currently but let's say i were to create a file now to do that i'm going to use dd it's okay if you don't use the dd command basically we're going to say an input file of dev zero output file of file one block size equals one kilobyte so that we can know exactly how many kilobytes we're using up and count equals let's say 400 so this should make a 400 kilobyte file if i press enter and do ls minus l we're going to see sure enough we have a 400 kilobyte file on here and this is fine this isn't meeting our quota at all we haven't done anything bad we're not even up to our soft quota but if we do this again i'm just going to push the up arrow and change this to file 2 and press enter okay it's done the same thing we do ls minus l now there's 800 kilobytes stored on this particular disk what this means is it hasn't stopped us from creating it but every day we're going to get an email from the system that says hey you've gone over your soft limit you really need to delete some files and we'll maybe do that maybe we won't do that but here's what happens if we try to create another 400 kilobyte file which will take us over the limit right because 400 plus 400 plus 400 would be 1200 but we only have a thousand kilobyte limit so what's gonna happen if we press enter it says that there's an error writing file three the disk quota is exceeded so let's do ls minus l and see what happened so it looks like it went along and it was creating fine creating fine it got to 200 kilobytes and all of a sudden it couldn't write anymore and that makes sense because 800 or 400 plus 400 is 800 plus 200 it is a thousand so we hit our hard limit and that's exactly how quotas work on the system once quotas are turned on it's really a handsoff kind of thing they take care of themselves the emails go out automatically every day if the user goes over their soft quota and at the hard quota it stops them no matter what so quotas are easy to set up once you remember to use quota on to turn it on make sure that it's mounted with the proper options and run that initial quota check so that it knows what files are on there so it knows when it is or isn't getting close to the actual quota that you set user profiles are where initial settings are set for a particular user like if if they're going to set up like aliases or they need their path variables those are the sort of things that profiles will do and there are systemwide profiles and also individual profiles and it can be a little bit overwhelming because not every linux system is the same now there are some commonalities usually there's an etc environment file and that file will often but not always set up the path variables so that the users who log in get a particular path set up now there's almost always an etc profile in the systemwide file in a profile is something that is run on a login shell so like the very first time you log into a system like if you're logging into a gui that first time you log in you will execute the profile these are settings that only need to be executed one time like it doesn't matter if you're going to open a new terminal you only need to set these settings one time and then there's also going to be one of these not both of these it's usually either one or the other either the etc bash rc or etc bash dot bash rc uh this one is pretty common in ubuntu this one is pretty common in centos but nonetheless these are they serve the same function so you're going to have one or the other and these are profile settings that need to be set every time you open a shell so let's say you're already logged in to x windows like in a gui session you're in there and you click on an icon to open up a new terminal window well that's not considered a login shell this is just considered a sub shell of your main system login so you will not execute a profile you will only execute your bashrc now this is again systemwide so that happens to everybody when you log in you get the systemid profiles applied and then every individual has the possibility to have these individual files in their home folders now they all start with a dot so they're all hidden you have to do ls minus a if you want to see them but these are the same things up here it's just if you have any changes or additions you want to make to the systemwide settings you put them in your own personal folder and you put it in like bash rc and that'll run every time you open a sub shell everything in here will be set up and then the dot profile or bash underscore profile depending on which system you have you're going to have one or the other one of these but this is executed the very first time you log in just like the systemwide profile your personal profile only gets executed that initial time when you log into the system any sub shells will only apply the bashrc files but that's the way it works i'll show you really quick how it's set up on a system but the hierarchy is really the important thing to understand now this is bob's home folder i did an ls minus la so you can see all of the things in here he has a dot bash rc file and he also has a dot profile that means that these are going to be applied after the systemwide settings because the systemwide settings are given to everybody and then any personalizations like if you have an alias that you want to set on your own you would put it in your own personal bash rc file now inside etc there are those common files like ls minus l grep4 profile we're going to see we have etc profile and also etc profile.d this is a folder if we go in there we're going to see there's a bunch of sh scripts all of these are included in the dot profile in fact i'll show you what i mean vi dot dot let's look at that systemwide profile file and if we look all the way down in the bottom here it's going to call in all of those files inside profile.d so when i say that the profile is executed by everybody not only etc profile but also etc profile.d everything in here is going to be executed as well to every user on the system now there's a couple important things to remember don't worry so much about what exactly is the name of the file when you look in the etc folder you're either going to find an etc bash rc file or an etc dot bash rc file don't worry too much about which system has which whichever one is there is the one that you need to use now as far as the hierarchy goes it's important to remember that the systemwide stuff is executed for everybody so everything in etc is executed for all users and then if you have changes or additions you put them in your personal dot profile or dot bash rc user profiles are pretty easy to track down and once you understand how the systemwide and individual settings work it's a snap to figure out which comes first if you're working on the command line a text editor is going to be an invaluable tool because pretty much everything in linux is text based now you should use nano nano is the editor you should use it's a wonderfully simplistic straightforward intuitive text editor that works just fine and then there's vi vi is clunky it's hard to use it doesn't make a whole lot of sense and it's the editor that i use almost exclusively now i know that doesn't make any sense but here's the deal vi has been around for a very very long time like since the beginning so even though it's difficult to use i've managed to learn to use it and it's just what my fingers do with muscle memory so i encourage you use nano unless you've been using vi long enough that it's the only option that seems to make sense now there is one scenario that you may want to learn at least the basics of vi sometimes you're going to come across a system that doesn't have nano installed most systems do but if you end up on a system that only has vi these couple commands are going to kind of save your bacon so here's the deal this is what makes vi so confusing there are two modes there's command mode which is what it starts in an insert mode which is what you use when you actually type text now the way that i can kind of describe this is if you're sitting down and typing you're going to be in insert mode because insert mode is where you insert text and delete text and use your arrow keys to you know go around and change text but then if you need to do some command like save or quit or anything like that you're going to go into command mode and the way i think about it is let's say you are on a standard word processor if you're typing you're in insert mode if all of a sudden you need to reach for the mouse and click on something you're going to be in command mode so while it's not a perfect analogy if you want to do something like save you're going to want to go into command mode so that you can save and quit now since there's no mouse it's all still text things that you're doing but think mentally okay i'm in the mode where i'd be using my mouse to save things instead of just typing out text now to go back and forth that can be confusing too so you start out when you open vi you're in command mode if you want to start typing something you press either i for insert or a for append meaning like do you want to insert right where the cursor is or to the right of where the cursor is but either one is going to work fine so either i or a and then if you want to get back into command mode you press escape so those are your magic keys to go back and forth right i or a i usually use i to go into your typing text mode or insert mode escape to go back so that back and forth that's how it works now the actual commands to save or quit or save and quit are right here and they may not make sense but if you're in command mode you're going to press colon and then type w and then q and press enter that's going to save your document and quit a lot of times people get stuck in vi and have no idea how to get out it can be so frustrating so this will get you out also if you want to quit without saving like you've accidentally made changes and you didn't mean to you press escape again to get into command mode and then you press colon q exclamation point and press enter and that will that will exit without saving and then if you just want to save halfway through a document you can just do colon w enter and it's going to save but you'll stay in then you'll press i to go back into insert mode and continue making edits i'll show you really quickly well i'll show you what nano is and then i'll show you vi just so you can see it in practice like i said almost every distribution is going to have both nano and vi so i'll show you nano first if we do an ls we'll see i have this text file dot so if we just type nanotextfile.txt it's going to open the editor with this and you can just use arrow keys and you can start editing right away this is like you would expect any text editor to work okay so press enter it's going to insert blank lines and then if you want to save you can look right down here we have control x to exit and you can do other things too there's all sorts of commands but i'm going to show you the basics here control x to exit and then it says would you like to save your changes and you can say see the options here y for yes and for no control c for cancel i'm going to say yes and then it says what file name would you like to write well it'll default to the current text file but if you wanted to save it as like copy 2 you could i'm just going to hit enter and boom we're done the text file has been edited it's very simple very easy to use and again i recommend you use it now vi i'm going to look at the same text file with vi so vi text file and here we have we're in what mode are we in we're in command mode now we can still use the arrow keys to get around but we can't edit any text or insert any text if we want to add text we press i and then look down here it says insert so this is a little cheat it tells you that you're in insert mode if we're going to go back into command mode press escape i'm going to go into insert mode press i escape to get out insert mode and once you're in insert mode you can type text and then if you want to save it again you have to press escape and then colon w q enter and then boom we've saved the file we can look and see the text file has been changed all this changes were saved and that's how you use vi it's confusing but that's how it works so again use nano it just makes sense it's easy that's what i recommend you use but if you have to use vi at least now you know the couple shortcuts that are going to get you through so that you can actually use it to edit text now remember i said i use vi all the time and it's true but the funny thing is that it's followed me into things like word processors so sometimes in my microsoft word documents even on the very bottom you'll see colon wq because my head i just automatically do that when i'm done editing text anyway use nano but vi is fun and it's a good skill to have i hope this has been informative for you and i'd like to thank you for viewing viewing text files is an extremely common thing for a system administrator to do on a linux system so we're going to look at a bunch of tools that allow us to examine text files in a way that allows us to view but also to search and i'm just going to go right to the command line so that we can see these things work in real time now i've created in my folder here a file called two cities now this is just the public domain tale of two cities this is the first chapter i'll just type cat so we can look at it two cities see it's just a tale of two cities the first chapter all right so let's clear the screen now the first thing i'm going to show you is the head command and what it does it'll show you the first lines of a text file so the head or the beginning of it so we can just say head two cities and it's going to show us the first 10 lines of the story so the best of times the worst times that's part that we're familiar with now we can change that how many lines it shows us if we were to do head dash n 20 it's going to show us 20 lines the first 20 lines of the file so see it's a little bit longer now and it's shown us all 20 lines now head isn't usually as commonly used as its companion which is tail so let me clear the screen oop and it spell clear right clear the screen now if we were to do tail two cities this shows us you've probably already guessed it the last ten lines and we could do the same thing with the dash n and a number we could decide how many lines of the file we want to see now this is really useful if you're looking at log files and that's almost exclusively where i use the tail command if i'm looking at a log file i just want to see the last things that were written to a log file so i'll do a tail of the log file in question and i'll see what was added to the very end so i don't have to look like 27 megabytes of text for all of the logs just the last little bit of it and so that's a very useful command and again you can use n20 if you want to see 20 or whatever number you want to see 10 is the default now the other ones i want to show you are less and more we'll start with more this is the older command so let me clear the screen if we were to type more two cities this is going to show us the entire file and if we want to scroll through it we press the space bar and it'll go to the next page space bar go to the next page the enter key will go line by line but this is that's it so we're all the way to the end of the into the first chapter but that's how more works you just kind of go through it like that you can also search but it only searches down and it's not one that i use very often anymore because it's been outmoded by the much more powerful although it has a more diminutive name less so actually let me clear the screen so if we were to say less two cities now we have what looks like a similar kind of interface but we can use our arrow keys to scroll up and down page up and page down work so we don't have to worry about like hitting the spacebar to go down a page we can spacebar will go down a page but then we can scroll back up with the up key now the other really nice thing about less and more does this to an extent but less is even more powerful if you type forward slash in a term so let's search for france press enter it's going to take us to the first entry of the word that we searched for it's going to highlight it and it's highlighted all of them so if we were to press forward slash and enter again it's going to repeat the same search and here we are francis found again france is down here again forward slash it'll take us to that one put it right to the top of the screen so we can search through an entire text file as well so it's very very powerful to use the command that seems like it would be less powerful because it's named less but really it's a lot better okay so to get out of here and this actually confused me for a long time if you just press q just the letter q it'll exit the less command and get you out of it so less more head tail they're very commonly used i usually use less and tail more commonly than the other two but that's just because i want to see the end of a log file and i want to be able to go up and down when i'm scrolling through a text file and search really powerfully it's not a really tough nugget because these are pretty straightforward tools they're all useful and you'll probably find yourself using them fairly frequently on the command line sometimes when i'm at the grocery store i wish i could search for where things are like i think pizza sauce should be right next to spaghetti sauce but it almost never is well thankfully when it comes to searching for text in a linux system there is an awesome tool that allows you to do just that narrow down what you're looking for with tool called grep now greb does use regular expressions or regex and if you're interested in the you know very finetuned filters you can get with regex i cover that really great in the linux foundations course but today i want to talk about searching for strings of text using grep because it can be a real powerful way to get the information you want really quickly now i said that grep uses regular expressions so if you want to make sure that it's absolutely searching just for strings of text you can use the dash capital f flag and that means just fixed strings usually you don't have to do that because if you just search for a string it's going to generally find it in the file but occasionally your string might be regular expression characters and you can cause yourself some headaches so if you want to be safe use dash capital f i usually don't because it's usually not an issue but i just want you to be aware that if you do grep minus capital f it's going to just search for strings now there's two different ways that we can use grep we can say graph the string from a file and it's going to search the file for the string that we specified and that works really well but there's also another way you can do it you can do this cat file or anything that has a text output like ls or anything that's going to output text and then you can use the pipe symbol and kind of push it through grep and search for a string i'll show you why this is a really powerful way to use grep because it seems a little backwards like why wouldn't we just say you know grep this string from this file i'm on an ubuntu system here and i'm just going to search a log file okay so i'm going to say grep now we can say dash capital f or we can leave that off i'm just searching for strings i just again want you to know that capital f is going to force it to just use strings but i want to grep for dhcp from the var log syslog file press enter and it's going to find all of the lines in that text file that have dhcp in it actually even highlights the dhcp which is really convenient now there's another place we could get some system log information and that is using the dmesg command but here's the problem that's a command that has output but it's not a file we can't grep the d message command so that's where the pipe symbol comes into play and it works really really well we could just say d message and instead of just having it print to the screen we can use the pipe symbol which is usually above the enter key in a us keyboard grep i'm going to use minus f this time we don't have to necessarily dhcp and then it's going to take all of that output from d message and grep for dhcp and sure enough there's two lines that have dhcp now another really useful way that we can use the pipe symbol let's clear the screen because i want to show you that first one that we did right we we actually grew up for dhcp from that file and we got these results let's say we had just pages and pages of results and we wanted just to look for things that mentioned init init what we could do is kind of like chain grips along we could say grep dhcp from var syslog and then pipe those results into grep init and press enter and now we're just going to get the lines that were in this result that also contain the word init and so here now we've filtered all the way down to these two lines of text from the log files so even if you're not getting super fancy with regular expressions you can do some really powerful searching of strings using the grep tool for things like log files or any kind of text that you want to search for and remember you can chain those grep commands together so that you get a really fine filter looking for exactly what you're looking for every application in the linux system has three sort of like pipes it has standard input standard output and standard error and basically it's just a way to get information in and out it's an io type situation for every individual app now i have just an application here drawn out and i want to show you the difference between the three so standard input is pretty easy to understand right this is like if you're putting something into a program we use the pipe symbol if we're going to pipe something into it or we can use less than if we want to just assign a file to the standard input now a lot of programs don't accept things on standard input but some of them do so if you've ever seen me pipe a command into another command what i'm doing is piping the results of one command into another command so it can work on it i'll show you how that works on the command line but then there's two other pipes and one of them is the standard output this is what happens if you type ls and it shows you know the contents on the screen that's the output that it shows you is the standard output now there's also standard error if an error occurs it also prints the things out on the screen but they're different pipes now we don't realize the difference because they both end up on the command line that's like the default place for standard output and standard error to go but you can treat them differently so if you want to redirect the output of a file of an application into a file use the greater than symbol if you want to redirect the standard error or like you know an error message you have to use two greater than because it's a different pipe and you have to redirect it separately so let me show you what i mean first of all let's talk about standard input now i have in here a file called file.txt i'll show you what's inside of it okay so this is what we have okay just a text file with some text in it now if we wanted to use grep to search for text we could just say grep text from file.text and it would show us the text that's in there but we could also redirect to the standard input rather than telling grep what to use so we could say cat file.txt and then pipe the results into standard input of grep and then have grep look for text we should get the exact same results now what we've done though rather than telling grep you know what file to choose from we just piped the results of cat into standard input and then grep use that as its input for grepping for the word text now that is using the pipe symbol we can also say grep for text and i want you to use file.txt as your standard input now this looks very similar to this up here but it's drastically different because what we've done is we've used redirection so this is redirecting standard input this actually functions exactly the same as this one because here we're using the pipe symbol to redirect standard input here we're using the less than symbol to redirect standard input so that's how you can do standard input it's not something you do as often apart from with this scenario i do this a lot you know piping one thing into another so that you can get the results from there now the other thing is standard output and standard error so i have a really quick way to show you so we say ls and we get these are the contents of ls we could redirect that by using greater than into a file called results.txt and we should get no output because rather than redirecting the output to our terminal window here it's actually redirected the standard output into results.txt so if we look there's a file now called results.txt and if we look at results.txt it has the contents of that ls command right it just dumped the contents into there here's a problem though what if we did this let me clear this screen what if we did ls lsf and we tried to redirect the standard output into results.txt why did we get the error message here and let's look at results well there's nothing in results now because there was no standard output this is an error there is no file called ff for us to use ls on if we wanted to redirect an error we would have to do lsf to greater than error.txt if we do that ah nothing appeared however if we look at in the file here now we have a file called error.txt and if we look in that sure enough that was the error message we redirected it using a standard error redirector using standard input standard output and standard error redirection is something you're going to find yourself doing a lot because you want to see the results of things when you're not there to see it happen on the command line that's basically what log files are right they've taken errors and redirected them into a log file for you once you understand how input and output works with an application on the command line there are some really cool tricks and tips that we can learn to make life a little bit easier now there's a handful of things we want to look at but i've drawn a diagram so we can actually get a real good taste for what each thing is now devnl you may have heard of people call it the black hole or the bit bucket and basically devnull is a location on your file system that you can copy anything to and it will disappear forever now that seems like a weird concept i know but if you have like extraneous logs that you don't really want to ever see you just want to like throw them immediately in the trash dev null is where you want it to go for example you would not want to copy important information and redirect it to dev null because it will just disappear forever so devnet is just a place that everything disappears when you copy it there now t is an interesting command it doesn't seem like it does very much but it accomplishes a task that's remarkably difficult to do without using the tool itself so here's how it works we take some sort of text like output from a file or something like that and we put it into teas standard input and then all it does is dump that same information out of its standard output but it also writes it to a file so it does like a t in the road or a fork in the road it lets you see what it is right on the standard output right in your command window but then it also copies it to a file so that's what t does and it's really nice if you want to see what's happening but you also want to have a record of it and keep it into a file and then arguably the most complicated one but also the niftiest one maybe is called xargs now how xargs work is let's say you have a different program like ls or something and you do the command and so you see stuff come out in standard output and then we're going to pipe it in or redirect it into x args standard input then what x args does is it takes it and says okay what program do you want me to use this information that you just piped into me on and you tell it like application number two and then it executes application two and it uses that information as the arguments for the second command now the reason this is really powerful is not all applications can accept things from another program on their standard input so x args basically takes and forces a program to accept something from standard input by accepting that standard input itself and then putting it as an argument onto the application now let's look at all of these really quick because the cool part is you know when you actually do it now first of all devnl so we're going to say echo hello and it'll put it to the screen if we do echo hello and then we redirect it to dev no boom it's completely gone right that's how devnet works it's just a place that never fills up it's kind of like a teenage girl at a pizza party sleepover right it just never gets full you can put as much as you want in there it's just gonna disappear now we've redirected standard output we could also redirect standard error and then we should see hello because there was no error there right if we did something like this let's say ls documents and ff we should get both standard output and standard error sure enough here's our standard error there is no ff but here is the contents of documents so we got both now if you wanted to do something cool and redirect both standard output and standard error into one place you can do this you can say ls documents ff just like we did and i'm going to redirect standard output into dev null and here's the magic part and then i'm going to redirect standard error into and one oh my goodness what is this well the ampersand one is a way that we can tell it that what we want is standard error to get redirected into standard output so the one is standard output so what this does is all of our standard output is getting redirected to dev null so all of our standard error is going to go into standard output which is of course going to dev null so this should give us absolutely no results and sure enough both standard output and standard error have gone into devnet all right so that's just a really cool thing that you can do with redirection and then devnet is just a place that never fills up all right let's clear the screen i want to show you so here i have a couple files or one file let's look and see what's in there all right just a bunch of different words in the text file so what we can do is say cat file.txt i'm going to pipe my standard output into t's standard input and then i'm gonna call it copy.txt and now we should see the contents of file.txt sure enough it printed to the screen but then the t command also created that copy.txt that also contains this so if we look sure enough there's a copy of our stuff and then last i'm going to show you x args and it's hard to come up with a real good example but i think i have one all right what we want to do is i want to create a folder named everything in this file so i want to have a folder named red named yellow named blue and name tuna fish well turns out to be kind of difficult to do that but we can do it really simply if we say catfile.txt pipe that into xargs and then x args is going to mkdir and then it will put the standard output of this command and kind of like plunk it right there so if we press enter and we do ls look at that we have a folder with each one of those names which is really convenient right it was able to do like what would have taken us quite a bit of typing to do it just did it by piping it and then it kind of pastes the results right at the end of whatever command you tell it now we could also do something cool if i want to clean up my mess instead of mkdir i'm going to say rm dir and then they're gone standard input standard output standard error they're really cool things and there are some additional tips and tricks that make using them even more usable and more beneficial on the command line dealing with text on the command line is something that's kind of fun to do to be quite honest and there are some tools that are pretty nifty to play with so let's look at a couple of them right now we'll just go right to the command line now i've already set us up with a few files i have file one and file two so let's look at them just so we know what's in them that's what's in file one and that's what's in file two just a list of words so i wanna show you a few things now you'll notice that these are not in alphabetical order okay so we could use the sort command so i could say sort file1 dot txt and it's going to return the contents but notice now they're in alphabetical order chicken fish monkey turtle so what we could do is if we wanted to save a file call it sorted then we could redirect the output to sorted.txt and then if we look at sorted.txt now it's going to be a file with them in alphabetical order so sort does just that and if you look at the commands for sort like to the man page for sort you can see that it does some other things you know you can actually sort with options like you know do i ignore case what about numbers what about you know what if it's a bunch of dates so sort's very powerful but that's basically what it does it takes a text file and then it outputs that text file sorted however you tell it to do so let's clear this screen all right the next one i want to show you is word count it's pretty simple it's just wc and it stands for word count so we could do word count of file1.txt and it'll give us three fields it says 4 4 and 27 in our case what this means is it means there are four words there are four lines and there are 27 characters now if we just want to know one of those things we could just say wc minus m for character count and it's just going to show us that there are 27 characters in file1.txt now the last two i want to show you are really the most interesting so there's cut and paste which i know sounds like a gui thing but let's actually look at our file again just so we know exactly what we're dealing with so file1.txt this is what we have these lines if we were to use the cut command we could say cut i'm going to do cut by characters and i want it to cut out character oh let's just say 1 from dot file1.txt if we do that we should see just the first oh i did file two my goodness i'm like those aren't the first letters at all so let's look at file one that dxd that makes more sense so the c in chicken the effing fish teen turtle m in monkey okay so we have those uh we could do more than just one character and we could do more than just the first character let's say we wanted to do cut minus c the third fourth and fifth character in the file file one dot txt so now we should see and what it did is it took the third fourth and fifth character but you see fish only has four characters so there is no fifth character so it just did sh for the fish line all right but see that's that's what cut does it'll actually take it right out of the middle of the file which is surprisingly difficult to do if you don't actually use the cut command and paste does kind of the exact opposite so let me clear the screen because it's kind of full and i want to show again file one and file two now let's say we wanted to put this file file two after file one right we wanted to say chicken lips fish whiskers turtle feathers and monkey flippers well that's kind of difficult to do if we cat them together it's just going to put one at the end of the other so that's where paste comes into play we can say paste file1.txt file2.txt and it's going to output sure enough i put a tab between them and then we have chicken lips fish whiskers if we wanted to do this and redirect it into a file and then we look at join.txt look at that our file now has those two pasted together now i know that most of what we did using these commands was really just playing around but really playing around is one of the best ways to learn to use a tool and you're going to find that every once in a while one of these tools like cut or paste especially are going to be extremely useful because it's kind of hard to put things next to each other in a text file or cut out the middle bits of a text file without using simple tools like this awk and said are text manipulation tools that for some reason most people are afraid of and i honestly don't know why yes they can be very complicated you can do a lot of powerful things but you don't have to you can do some very simple yet still powerful things with awk and said now what do they stand for said just stands for stream editor and awk i actually had to google this because i had no idea i mean i've been using awk for decades but i didn't really know what it stood for it turns out it's the initials of the people who first wrote it and i'm not going to try to pronounce them all but that's what the a the w and the k mean basically awk is a data extraction tool it allows you to pull out certain bits of data from text and set of course is just a stream editor which allows you to edit things without interacting with it directly so let's actually go to the command line so we can see it work now i've already prepared a couple files so if we do ls we're gonna see we have file one file two and joined so let's just look at them this is file one this is file two and this is basically the two of them joined together i actually use the paste command to do that so i have these three files and i'm going to use said and awk to do things with these files okay so first of all said is a stream editor which means you can put files in and you'll it'll output the edited version and how it works i'm just going to cat one of these files so cat file1.txt and i'm going to pipe that into said for stream editor and here's where i'm going to set up the rules i'm going to substitute so i'm going to say s and then forward slash what i want to search for is the word monkey and a forward slash what i want to replace it with i'm going to say dolphin and then a forward slash ng for global that just means that if it occurs more than one time i want every occurrence of monkey to be substituted with dolphin and then i'm just going to press enter and what we get is chicken fish turtle dolphin because it took this initial file and it substituted monkey for dolphin and really that's what said does it's a stream editor it allows you to edit things as it flies through there so you can make changes to text as it's being manipulated so you can put this inside of a script and do things without interacting on a like a a gui like you'd want to open it up with vi and edit it out and change monkey to dolphin or anything like that so that's what stream editor or said does now i'm going to clear the screen and we're just going to look at joint i'm going to say cat join just so we can see it now awk takes a text file and it will allow you to pull out bits and do things with them so i'm just going to type this out and then we'll see what i'm talking about so i'm going to say awk and i'm going to put single quotes open curly braces print dollar sign 1 close curly braces close single things and i'm going to use joined.txt as the file so what this is saying now this is maybe why people are scared there's a bit of an odd syntax here but we say awk and then this is what we want awk to do with the file i want it to print out the first field now it will auto detect that these fields are separated by a tab they could just be spaces and it'll auto detect it but basically this is on line one this is field one field two line two it's field one field two so this should print out the first field of every line so let's press enter and sure enough chicken fish turtlemonkey it printed them all out now we can do more than just one thing at a time so let's go back over here what if we wanted to do dollar sign two dollar sign one now we should get a printout of whiskers fish and and flippers monkey and let's see what what turns out here now it is gonna be a little hard to read and i'll show you why see it did do that right here we had chicken lips and it gave us lips chicken whisker fish feathers turtle flippers monkey now the problem is it took those fields and just mushed them right together so we could add another thing in there we could kind of build this out longer and we could say i also want a space in there so hopefully that makes sense it's going to print field 2 and then it's going to print this space and then it's going to print field 1. so let's see if that's what we get sure enough lips chicken whisker fish feather turtle flippers monkey and it doesn't have to be used just one time right we could do this we could say i want dollar sign two dollar sign two which means field two field two and now we should get a duplicate of each one lip slips whiskers whiskers so awk just takes bits of data and allows you to manipulate them and do what you want with them honestly the only thing i could think is that the syntax for all can said intimidate people but once you get used to doing it and especially with stream editor the syntax is very similar to the vi editor when it comes to replacing and substituting things in a text file so hopefully you get used to them and you're not afraid of them because they're super powerful and awesome tools to use especially in scripts because there's no interaction required you can put them right inside of a script and they work without you entering more data hard links and soft links or symbolic links as a lot of people call them are very similar in what you get on the file system as far as usability goes but they work drastically differently so let me explain the difference between them so when we have a hard drive we basically have every file that takes up a certain number of sectors on the hard drive itself so this one let's say takes up three blocks this one takes up four blocks this one takes up seven blocks and these are the actual files on the hard drive but the file system actually only knows where those files live because of the file allocation table so this is kind of like a table of contents right it says okay file one is actually right here and it extends three blocks uh this one i'll say okay file two this actually lives right here on the hard drive and it extends four blocks and then the same thing here file three and you know it extends all the blocks here and so on and so forth for all of the other files on the system now there is a difference though let's say this is a symbolic link okay a symbolic link doesn't point to the hard drive at all it actually just points to a file in the file allocation table so this is a standard you know table of contents link right this is pointing to this spot and these blocks on the hard drive but the symbolic link just points to filename.txt on this system itself now there's the other kind of link and that's a hard link so you probably notice there's another purple file here so let's say this is file two and it's pointing to you know these four blocks this could be like file 12 and it points to the exact same spots on the hard drive so it points to the exact same file location and the same number of blocks it's basically the exact same file but it has two different reference points in the file allocation table that can be really confusing but what's cool about it is let's say you accidentally delete this file well that's okay it's still on the hard drive and you can still reference it from this file here so let me show you what that looks like in practice in our system here we have let's do an ls minus l we have my file.doc now if we wanted to do a symbolic link we would do ln minus s for soft or symbolic the source is my file and the destination is going to be my linked file.doc do ls minus l and we're gonna see it actually shows us exactly what's happening my linked file dot doc is just pointing to the name myfile.doc in fact it's just pointing to this name itself so if we were to say move my file dot doc to my new file dot doc and then we do an ls minus l this link is broken because it still points to the name my file dot doc and that doesn't exist anymore so this is now a broken link on our system so symbolic links are kind of dumb they don't take up much space but they're kind of dumb in that they don't follow a file if you move it or rename it all right so that is a soft link now a hard link works differently a hard link if we were just to say ln without any flags my new file dot dock to my file dot doc and then we do ls minus l well one we've fixed the symbolic link right because now this file points to a file that exists now so all of a sudden now it's pointing to this file myfile.doc and you'll notice it's the same size as the other one and if we were to move the original so we're going to say move my new file to my cool file dot doc it doesn't break the hard link that we made both of them are still there they're still fine they're their own independent file name in the file allocation table they just happen to point to the same spot in the hard drive and we can see that if we do ls minus l i for inodes it's going to show us the spot on the hard drive that it's actually pointing to and sure enough these have a matching inode whereas this symbolic link has a completely different inode because it's you know just a file that only points to a file name but these two have the exact same spot on the physical hard drive now another cool thing this number here which you've probably never even thought about before but this says how many linked files are on this inode now this says there are three now of course we see two right here but that means somewhere on my file system there's another hard link to this inode now i did actually make it before we started so we could use the find command to find that i'm going to say find i'm going to look in my home directory in the same file flag and i want to find the same file as my file dot doc or i could say my cool file dot doc because these are the same files so it doesn't matter which one i have find look for a match for and press enter and it's going to find all three of them sure enough i have a hidden file right here that i that i did earlier before the nugget started and this is just one more hard linked file to this same inode we could look really quick ls minus li l i a so we can see the hidden files and sure enough hidden file there's that same inode reference that the other ones are referencing now honestly soft links are generally used more because they're easy to see you can do an ls and see where they're pointing to so they're a lot more convenient but hard links do have their place because each file acts as an independent file you can delete one and it doesn't ruin the reference that the other one has so hard links point to inodes soft links just point to file name references of other files on the existing file system when you're trying to figure out the location of files on your system there's basically two ways you can do it there's the find command and the locate command and both do pretty much the same thing but find is quite a bit more powerful yet has some limitations over locate i'm going to just show you how they work because trying to explain the pros and cons just seems a little bit silly when we can just actually see how it works in action now if we look in our documents folder i have a few files here i have new paper and it looks like it has camel caps here capital n capital p old file and research paper dot doc so let's actually use the locate command first because it's simple so we say locate and then what we want to look for and it can just be a substring so if we say old file and press enter it's going to give us the full path of old file.txt notice i didn't have to search for the entire file name i just searched for old file.txt or old file and it found old file.txt now we can do the same for let's see locate research underscore and it should find research paper and sure enough research paper now there was one more file in there if we type locate new paper no it doesn't find it oh did i spell it wrong well let's look over here new paper no i did not spell it wrong and that's where the limitation of locate is locate uses a database that is cached on your system which means it's super duper fast for searching for the names of files however the cache is only created once a day now we can force an update we could say sudo update db press enter and it's going to update the database of all the file names on the system and now if we just do up arrow and locate new paper now it's going to find it because we updated the cache of all the files on the system so it's very very fast but it has that limitation that it uses cached data now the find command is more powerful but it has the limitation that because it searches in real time it's a lot slower so how does it work pretty much the same way we're going to say find and you tell it where you want it to search so we can say search the root directory and now rather than just a file name find does a lot more things so we're going to say i want you to search for a file named let's say star new paper star and this should find all of the files that have new paper in them okay so i'm going to press enter and oh my goodness what is all this permission denied well find actually goes through the entire file system because i said search in the root directory and i actually don't have permission to look at all of these things so it's going to search through every single folder on the whole system and let's see did it actually find the file it should have but we have to look through all the error messages and if we scroll up sure enough it did find it all right it did find it just like the locate command but there were a lot of permission denied errors now we could do something like redirect the error right we could say two greater than dev null which will just pipe the errors into our dev null bit bucket and sure enough there it found it was pretty quick but it wasn't as quick as using locate now there are some other really cool things that find can do we can say find in our current home directory and what that'll do is it will allow us to search in just this home directory so it doesn't search the entire file system so that can be pretty convenient so if we say find dot and we're gonna look for the name new p oh i didn't do the stars new p it should find it for us sure enough it did find documents new paper and then it does this other thing we can actually say dash delete and it will delete it and how can we see if it's deleted if we look in the documents folder whoa it deleted that file so find does more than just locate files however it has some finicky things like this it's going to be using regular expressions to search for the files and while it's more powerful it's slower and it can be annoying when we do things like get errors from permission denied things like that so while conceptually find and locate do the same thing they do them in different ways the important thing to remember about locate is that it's always going to use old data unless you run that update db command now find is much more powerful but it works in real time so it's slower and there that means that there are pros and cons to using both tools while it's certainly possible to set up network shares for copying files from one server to another generally if you're going to copy files over the network from linux server to linux server you're going to use either ssh or really scp which stands for secure copy but it uses the ssh protocol in order to do that copying or rsync which actually will sync a whole bunch of files across the network so we're going to look at doing both but it's important to realize that ssh is the same program in the same protocol that we use to connect from one computer to another to reach its terminal so let me show you what i mean now i have two computers set up in our lab i have this ubuntu computer and i have this centos computer they're both on the same network so i'm going to ssh from one to another i'm going to say ssh to centos it's going to ask me for bob's password on centos and then all of a sudden now i'm logged in to that remote computer sent to us in fact if we go into the desktop folder and we do an ls we're gonna see over on sent to us we have things called like cool picture cool pic 2 and these things are on the remote desktop all right i'm going to exit and that's going to bring me back to ubuntu now if we look on the desktop folder we're going to see there's nothing in there because on our local ubuntu computer we don't have those things let's say we wanted to copy something over well let's go into our desktop folder again there's nothing here nothing up our sleeve we could use scp which is secure copy and this uses ssh right so we would say scp from centos now we could also specify a different user now it's the same user for us but i'm still going to specify i'm going to say bob at centos colon then the remote path which is going to be home bob desktop and let's pick one of these files i'm going to say cool picture cool picture.jpg and i want it to copy it to dot which means our current directory i'm gonna press enter it's gonna say okay what is bob's password bob and now if we do an ls we're gonna see look we have cool picture that was copied over the network using scp which uses the ssh protocol and it was copied over and now we have a copy of it here locally we can do the same thing we could say scp a local thing and rather than the destination be our local computer we could do it backwards right we could say scp coolpicture.jpg to bob at centos home bob i'm going to copy it just to his home folder and so now it's sent one over to bob's home folder now we could copy everything all at once if we wanted by using the rsync command now rsync is pretty cool in that it will even recurse directories if we wanted to so we could actually do this we could say rsync i'm going to say minus a so it does all the things including recursively going into directories i'm going to say v so it does it verbosely so we can see what it's doing so rsync dash av and the remote thing is set up just like with scp so bob at centos or we don't have to say bob add if it's the same username we could just say centos colon and then the path so i want home bob desktop i want that entire folder copied to here so what we should end up with is a new folder inside our ubuntu desktop folder called desktop because it's going to copy this folder to our current thing and it's going to have recursively everything in the remote desktop folder let's see if that works press enter it's going to say what is bob's password bob and so now it says receiving all of these files and look sure enough there's a folder called desktop if we were to look inside there now i look all those files that were on that remote computer are now on our local computer as well we used rsync and it will transverse all of the directories recursively and it will copy it over for us now this might seem like a throwaway nugget something that is just nice to know but you're not going to use i'll be honest i use scp and rsync almost every single day copying files back and forth using scp is so easy and so fast you don't have to set up servers it's just a way to get one file to another server without having to worry about installing anything because it uses ssh which is already installed on all of your servers so whether you just want to copy a file or two with scp or you want to do recursive directories with rsync it's really easy to copy files over the network in linux pretty much every distribution out there uses system d to manage the services like the various programs that are installed like web servers and stuff on their system and system ctl is the command line tool that we use to manipulate and manage those services now there's a couple concepts we need to understand we need to know enable and disable versus start and stop enable and disable is basically talking about when the computer boots up will it automatically start the service and so you can have something that you can start and stop but that doesn't mean it's automatically going to start or stop when the system boots up that's where the enable and disable comes into play now it's really really easy to tell what a service is doing by default and we can change it without much more difficulty at all so i'm at a computer right now and i've installed apache 2 on this centos machine so httpd is the package name and i've installed it however if we go over here we can see localhost it's unable to connect it's not running so the first thing we would do is say systemctl status httpd and it's actually giving us more information than it first appears see it's telling us that it's actually inactive which makes sense because we can't get it to load but more importantly it's saying that the service itself is disabled and the vendor preset meaning like when you first install it it's set to disabled so we can change that because if it's disabled it means it's not going to start when the computer boots so even if we rebooted this computer it still wouldn't load because it wouldn't start by default so we can say system ctl enable httpd and press enter and now if we do that status we're going to see that it's changed okay it says it's enabled even though the vendor preset is still disabled this means that you know when we installed it it was disabled but we've changed it now so it's enabled but you'll notice it's still inactive or dead now if we did restart the computer it would automatically start up but we can start and stop it independently from whether it's enabled or disabled as not we can just say system cto start httpd and it's going to start our service we can look at status httpd and we can see sure enough now that it's active right it's running and it's still enabled so when we reboot it's going to start running automatically now even if we left this disabled we could still have started it using the start command however when the computer rebooted it wouldn't start automatically so if you want it to always start up you have to make sure that it's enabled even if it comes disabled by default just a quick look boom it's running and sure enough it's right there running for us and it will run when the computer reboots because we've changed it to enabled i really like systemd because systemctl is kind of the onestop shop it's like the swiss army knife for managing services on a computer that's controlled with the systemd startup init service sys5 or sysv or system five it's called a lot of different things but this is an older way that linux systems would put themselves in various modes or run levels that determine the type of system whether it's a gui system whether it's just a standalone network system and we can switch those various modes we can set defaults to those modes but it's important to understand what the modes actually are and there's a whole list of them unfortunately the modes are different in debian and centos or debbie and ubuntu centos souza they actually use the various levels differently so i just want to briefly go over the difference so that if you're on one system you kind of understand what's going on so centos actually separates them the most so let's go here first we have run level zero and run level zero is basically if you go into this mode it halts the system this is like a way to power the system down run level one is single user mode there's no networking or anything and there's no asking for the root password this is the way that you would recover a root password on a sys5 computer mode 2 is multiuser with no network 3 is multiuser with network 4 is not used at all with centos and 5 is the multiuser gui system like if you have x windows installed and then lastly run level six is reboot if you switch into run level six it will then reboot your computer and it will go into whichever default is set now w ubuntu are similar halt is the same reboot is the same single user mode is the same the difference is here two run level two is the full multiuser system just like run level three is here and then if there's a gui installed on the system the gui will start up there's no difference in debian ubuntu between having a gui system and having a not gui system when it comes to run levels that's only if the gui is installed so run level 2 is pretty much what we use all the time when we're in debian and ubuntu run levels 3 through 5 don't do anything at all they're just not used so that's the big big difference between the two we still have reboot we still have halt we still have single user mode but it's how they handle the other things that are a little bit different now switching between them and setting the defaults are exactly the same now here's the gotcha it took me a long time to find a system that still uses sys5. this is outmoded and not used in any modern distributions but if you go if you find one that is still in use like this is centos version six it will still use it so what we can do we can say run level and it will show us what run level we're in we're in run level five and the previous run level we were in was just a new boot okay now if we want to switch between run levels we can say tell init and then the run level we want to switch to so i'm going to say 3. this should drop us out of a gui and into a text only environment you can see here there's no gui there's just this text box right here so i'm going to log in so we can go back if i want to go back into the gui system i can say tell init 5 and it'll get us right back into the gui system and here we are in the gui system if i start up a terminal and we say run level we're gonna see we're currently in run level five our previous run level was three now if we switch into run level six it's going to reboot if we switch into run level zero it's going to just halt the computer and power it down if you want to change the default and the default is just what run level it automatically boots into we need to edit a file so i'm going to become root and we need to edit etc init tab and this file has a couple things we can edit but really the main thing is all the way down at the bottom which is the default run level now it gives us a little bit of a cheat sheet here and this is actually really really good advice the halt mode run level 0 do not set your init default to this because it'll boot up and immediately halt and that's not what we want same thing with setting it to run level 6. it will boot up and switch immediately into run level 6 which is reboot so it's going to be in a constant reboot loop so you never want to set the default to that ours is currently in run level five for the default and it's right here now we could change that to three save this file and now when we reboot the computer it's just going to automatically go into the text only mode i'll just show you really quick what happens if we tell in it into run level six it's going to reboot that's what it does so if you run across an older system that uses run levels specifically system five run levels you need to know what the various modes do and remember it's going to be different whether it's ubuntu ubuntu and debian or centos and red hat and then you need to know how to switch the modes using telenit and how to set those defaults and most importantly what not to set the defaults to namely run level 0 or run level 6. i often find myself on a system thinking what if i need to switch between modes like i have a gui machine that i want to get rid of that gui interface so it's just a server with the text mode or vice versa well switching modes and setting the defaults is done one way with the sys5 system but if you have a newer system d initialization system it can be very very confusing especially if you have that sys5 background thankfully there are pretty simple comparisons when it comes to how it used to be and how it currently is now if you're not familiar with init 5 or with sys5 in it that's all right we're just going to talk about what the various modes are basically we start with run level zero which has a correlation in the system d world as a boot target called power off now boot targets are basically just modes right these are modes that computers are are set to so that they can function in a specific way and while it doesn't seem like a mode it's a really easy way to shut your computer down by switching into the power off mode now there's also one this is single user mode in the world of systemd it's called rescue mode this is like insist five if you want to like reset your root password you need to switch into single user mode well same thing with a boot target it's just called rescue mode then there's mode three which in a centos system is going to be like a nongraphical user interface with networking support that translates to just multiuser target in system d uh the gui mode five translates to graphical target and of course six is like the the compatriot to run level zero and this is how you can reboot your system by switching into run level six or boot target reboot now switching between them is actually easier than it is with the old sys5 you can actually just use a command line tool instead of editing that init tab file that you have to do with sys5 now i'm on centos version 7 here because centos version 7 uses system d whereas centos version 6 uses sys5 now the first thing you need to do is be root so i'm going to quickly become root once we're root we can type system ctl which is the way we do most things with systemd but systemctl getdefault and this is going to tell us what the default mode is and that makes sense because our default mode here is graphical target and you can see we have a gui interface now we could change that we could say systemctl set default to multiuser.target and see now it's changed that so if we say get default it's going to tell us okay now it's multiuser but notice it didn't change we're still in the gui environment well that's because it just changed the default if we were to reboot this computer it would reboot into a text only mode now if you want to switch between modes or between targets with a computer that's already started you simply type system ctl isolate and then the name of the target in our case let's say isolate multiuser and it should drop us directly into sure enough the text only mode now if you're already used to the world of run levels you just have to kind of think what the different targets that correspond to it are but if you're not familiar with run levels like this is something that happened before your time in linux that's okay because honestly boot targets make a heck of a lot more sense than the run levels did because they actually have their description right in their names and while it's important to understand both sys5 and system d you should know that all systems going forward are going to be system d so you're gonna have to know about the various modes how to switch between them using isolate and then how to set and get the default so you know what happens to a system when it boots up so you don't have your rack servers booting up to a gui environment because that just doesn't make any sense services are the various programs that are installed on a server that are going to run and serve out like web pages or whatever you might have installed they're called services and if you have sys5 on your computer the way that you manage and start and set defaults for those individual services are by using specific programs in the etc init.d folder now there are tools that we can use to manage those specifically service and check config and i want to show you how they work because the services are determined to start and stop based on the run level of a particular what the computer is set to so if like it's run level three a certain system might start and if it's run level five another service might not start let me show you what i'm talking about here on our system though this is centos 6 which has sys5 if we look in etc.d these are all the various services or programs that are installed on the computer we can see things you know like post fixes the email server sshd is our ssh server and we can start and stop these by using the service command so i can say service sshd start and it's going to start the service i can say service sshd stop and it will stop the service i can actually do status to see what it's currently doing so right now it's currently stopped it says but what i want to do is change how it starts or stops on system boot and what we can do is say chk config dash dash list sshd and it's going to show us what sshd is going to do on every run level so run level zero it's off one it's off two it's off three it's off four it's off five it's off and six it's off so this means it is not going to start up on system boot regardless of what run level the system is starting at now we can set it so that it will start for all of the run levels one through five it's never going to start automatically for zero or for six because those are those are halt and reboot and that would just be silly but if we wanted to start on all of them we can just say chk config sshd on and it's going to set them to on for all of the run levels so we can do that list command again and we're going to see now it's on four two three four and five actually it doesn't do it for single user mode so if we just say on it's going to set it for run level two three four and five it's going to be turned on but we can do it individually too so first of all let's turn it back off so now they're all set to off if we look see they're all off again we can do a single one so it gets a chk config dash dash level 3 on and now if we look it's just going to be on for level oop gotta get the format right sshd on and now if we look there now let's turn it on for run level three but the other ones are still off so chk config is the way that we change how it boots up whereas the service command up here is how we change it immediately if we want it to start or stop we can use the service command but if we want it to start on boot we need to use the chk config command because that's going to change the behavior at the various run levels thankfully when you install packages they create their own entries in the etc.d folder we don't need to make scripts or anything in there and the programs install things in there so that the service and the chk config commands know exactly what to do in order to start and stop or configure what happens on boot with a given system that's running sys5 modern linux systems use system d to manage their services things like their web server their ssh server and the same tool is used to start them stop them enable them on boot and that's that swiss army knife that catch all tool for systemd system ctl so i want to show you how to go about starting and stopping individual services but also how to affect what happens on boot when a system boots up what happens with particular services but first there is one thing that can be frustrating about system d and that is that the service files can be scattered all over the hard drive so for example inside etc systemd system we're going to find a couple service files like anything that ends in dot service is going to be a system d service file but you'll notice like there's no ssh here well that's frustrating well let's actually search for that say locate sshd.service you'll find that this is actually located in user lib systemd system that's where sshd.service live so there's several places that you can find the service files whereas with sys5 it was always in the etc.d folder here there are several folders that are going to house system files that are service files for your system so that can be frustrating but nonetheless we regardless of where they're stored we can still use systemctl to query them so we can say for example systemctl sshd let's do a status and this is another gotchu if you're going from sys5 right into system d the frustrating thing is normally we would say like service sshd status well now it's backwards now we have to say system ctl status sshd gur it's frustrating but you get used to it unless you go back and forth from systems then it can be a little bit frustrating but nonetheless on system d we have to say status or start or stop and then the service name whereas it's backwards with sys5 anyway we have a lot of information here so we can see that it's active which means it's running so it's currently started there's more information here though we look up here it says it's loaded it's enabled and enabled in systemd world means that it's going to start on system boot and there's even some more information here it says vendor preset is enabled now what that means is when we install the sshd daemon it's going to automatically be enabled now that doesn't mean it's going to start unless we restart the system but it means that it's going to be set to start on the system boot that's what the vendor preset is now we can change this easily we can say system ctl disable sshd and now if we go back and say status sshd we're going to see now it's disabled and the vendor preset is still enabled but we've changed it now so that it's not going to start on boot however this is another important thing to note it's still running because we've changed what happens on boot but we haven't changed what's currently happening on the system so if we reboot it's not going to be running but if we want it to not run we actually have to tell it that so we have to say systemctl stop sshd and now if we were to say status now we would see it's no longer running it's inactive and it's not going to start on system boot but let's change that because we definitely wanted to start on system boot so systemctl will start it up sshd and systemctl enable sshd and now if we do system ctl again this is a catch all tool status sshd we're going to see it's back to how it should be it's running and it's enabled which means it's going to start on boot so regardless of what service we want to start stop or enable or disable that system ctl tool is what we use for just about everything in the world of system d really the only gotcha with going from sysv to system d when it comes to services you have to remember to use the system ctl tool and then you have to remember that the actual command goes before the name of the service and that's backwards from sys5 so we have to say like systemctl start sshd whereas with sys5 it was backwards but it's easy to get used to and i love having a onestop tool to do all of the things that when we're planning servers on our network it's important to know that servers serve things they serve things like ntp which is a network time protocol ssh a secure shell so you can get into the computer remotely dns is domain name service which translates things like cbtnuggets.com into an ip address dhcp hands out ip addresses on a local network so you don't have to manually assign them a docker is a containerization system that allows you to run services in siloed environments and then of course configuration management tools allow you to centralize the individual configuration of servers so there's basically several kinds of services that we're going to install in our networks we have centralized things which are going to run on like one server for your entire network and then individual which are going to run on all or most of your servers so here we have things like i mentioned before dhcp dns configuration management server these are going to run on one server on your network you don't need more than one dhcp server more than one dns server apart from redundancy or high availability but generally speaking you only need this in one place individual computers all have to have an ssh server installed because you want to get into every server on your network right this is just something that's going to be installed everywhere your configuration management client is going to be installed on every computer it's a service that needs to be there so that it can take advantage of the configuration management system like chef or puppet or ansible so that it can you know work together to keep those servers in line docker is a containerization program that runs on an individual server and if you're gonna have a lot of different servers out there you may have docker installed on multiple computers so that it can host services for you now ntp is kind of the in the junction point of my venn diagram here and that's because ntp or network time protocol is the service that keeps your computer in the proper time like you know if you have some clock skew where it's a little bit too fast or a little bit too slow ntp will keep your server running in the proper time now there is a centralized ntp server very often on your network and all of the other computers or servers on your network will then query your centralized ntp server but here's the deal this same ntp server software is actually the client software as well so ntp does two things one it queries an above computer for the time you know in the case of the centralized one it's in the cloud and it also can serve out that time information to its peers or to people on your local network so usually we have a centralized ntp server but we don't even have to right all of these ntp server machines could query right out to the cloud and bypass a centralized ntp server it's just nice to have a centralized place so you have one time frame that your entire network is based on but it's the same server program so that's why i kind of put it in both camps here as individual and centralized now when you're setting up your servers on your network it's important to think through how it should work and it's actually gone through this change as computer hardware and technology has progressed there used to be a time where you would have a bare metal server for every service on your network if you had a dns server it would literally be a physical server sitting in your server closet and it would sit right next to your dhcp server whose sole purpose was to serve out dhcp same with ntp or you know a web server but then we said you know what now computers are getting to be so fast that we could put multiple services on a single computer because the problem here is it was very expensive right if you had to buy a physical server for every service you wanted to do it got expensive fast so what if we bought a decent sized server and then we installed dns software we installed dhcp software ntp software web software whatever we wanted to do it would all run alongside each other and be really happy the problem comes what if you need to run an update on one of these like we need to take dns offline so we can update it and maybe restart the server when we restart the server all of our services go down because we've put them all in one basket right all of our eggs are in one basket it's also a single point of failure basically it's messy and then the dawn virtualization happened and this is where server closets got really awesome really fast that's because we had large computers you know basically the same large computer it's not that computer hardware got all that much faster but virtualization technology allowed us to instead of just installing a dns service software we could install a virtual server inside here by taking a slice of the resources from the bigger server and install a completely new server virtualized in there that would run dns and we could do that alongside another virtual server and if we had to take this offline or or restart it it wouldn't affect the others because they were their own standalone virtualized servers it was really really awesome it still is a very powerful way to go about protecting your different services from each other on the network the problem is and this is something that i've fallen prey to is sprawl potential it's really easy to spin up another server oh i want to do this i'll spin up another server oh what if we did this sir spin up another server that's where docker comes into play and i won't go too much into docker other than to tell you what docker does is it takes a server and it has a single operating system running linux and then each service has its own like isolated pocket it doesn't have its own operating system it's not like a virtual machine all it does is have its own little slice of the running system where it has its own file system and it can run its own little service here and it doesn't affect anything else because it's walled off so containerization is an even better way to take better advantage of server hardware even than virtualization when it comes to virtual servers so yes there's a lot to think about when you're installing servers on your network on where to put them but the nice thing is whether it's a local service that has to be installed on every computer or a centralized service that you just install like in one place for your whole network planning has gotten a lot easier because you don't have to worry so much about putting all of your eggs in one basket we've been able to isolate individual services without the need to buy brand new hardware so planning is kind of fun and more flexible than it's ever been before conceptually we pretty much understand how a web server works you send a request and the web server sends back the web page but when you add ssl or tls it really does add a layer of complexity but that complexity is for a good reason because it can secure the traffic so nobody knows what is going through your internet connection like bank account information and stuff like that so it's very important that we have ssl encrypted secured traffic now the process is going to be a little bit different than just a standard web page and part of the thing that you want to make sure you have if it's like for a bank or something is a certificate authority now let me demonstrate exactly what goes on when you try to get a web page from a web server let's say this is our web server now if you're not talking about ssl basically the guy in the computer here says hey i would like to see your web page and then the computer says okay here is my web page and that's pretty much the entire process there's no encryption at all but when you go to like your bank's website you're going to set up an ssl session so that all of your information that goes back and forth is encrypted so basically here is how the process works the client sends a message to the server and it says hey i would like to start an ssl encrypted session with you and then the web server responds okay here is my certificate this says who i am and that i'm valid and look here's a picture of my kid playing softball maybe not that bar but it sends a certificate describing who it is to prove that it's who the server says it is that it's not like some maninthemiddle attack now the only way the end user knows that it's real is because he contacts a certificate authority which is a centralized trusted place that signs certificates basically it's this person's job to contact this web server and make it prove who it is and then once it proves who it is it gets its certificate signed by the certificate authority so then let's say this guy's name is bob bob says okay i see that at the bottom of your certificate it was signed by somebody who i trust so i'm going to trust that you're really who you say you are so then after that identification has been verified bob then sends his encryption key to the web server so the key is sent from the end user to the server and then the server uses that key that bob sent and that is what is used to encrypt the actual data that is going to go to bob's computer so the actual encryption uses bob's key that he sends to the server after the server proves who it is and then they use this tunnel back and forth and that's how they communicate using bob's key now if you set up a server in your own network you've probably heard of something called a selfsigned certificate now that is exactly what it sounds like when that initial request comes from bob and he says hey server i would like to set up an ssl connection the server does respond with a copy of its certificate it says here i am this is all my information uh this is the stuff that you know describes who i am i'm promising that i am the person i say i am but there's no signature on the bottom from the certificate authority it's something that bo that the server signed himself so bob has to like just trust that this server is who he says it is now if it's on your own local network that's usually fine and it's okay to accept a selfsigned certificate but if it's over the internet you don't want to accept a selfsigned certificate because there's no way to be sure that it's actually the server it says it is and if you trust a selfsigned certificate and it ends up being like a maninthemiddle attack you could be sending all of your banking data to a server that isn't who it says it is and that's very dangerous so a selfsigned certificate encrypts the exact same way the problem is you're not a hundred percent positive who it is that initially set up that certificate so there's a lot of trust involved whereas if you use a certificate authority trust is taken out of the picture because you trust the certificate authority it's built into your web browser so the actual ssl encryption is the same whether it's a certificate authority or a selfsigned certificate but you don't know if it's the server it says it is unless you have that certificate authority that signs the server certificate that's why it's very important especially on the internet to make sure that you don't get an error about not having a certificate signed by an authority but the process is the same either way it turns out that local network server roles have changed fairly dramatically over just the past few years now don't get me wrong things have come a long way from when we used to have a sneaker net so if we had a file we'd have to put it on a floppy disk and then you know carry to the cubicle next to us and pass it on like that but we don't even use local servers as much as we used to now what am i talking about well i'm talking about the introduction of cloud computing now let's look at file services for example we used to have and then we actually we do still have local file services that you know if we want to save files in a local centralized place we do things whether we're on windows or linux or mac if we want to serve to windows computers we can use the samba program which allows us through file sharing that is hosted on linux but is accessible from a windows machine it's a free way very stable very scalable that we can actually share files with windows computers same thing with nfs which is network file storage and this is applicable for linux mac windows and then if you have old school apples that only use like the apple talk sort of networking stuff well there's neta talk which uses the native apple file sharing but this isn't even used anymore so much because now macintosh computers can very easily do windows shares using samba or nfs but netetalk is still around if you like that native apple file sharing stuff the point is linux computers can do local file sharing very very well the thing to think about if you're implementing a network though is should i and that's where cloud services come into play because while sure you can serve things locally on a file you might want to consider something like dropbox or onedrive or google drive that works on almost every platform and allows you to not only sync things between computers but also have an online backup which is really really vital and it saves a ton of money if you don't have to buy the servers to actually store all of your files if it's stored on a cloud service that you usually pay a service fee for you're going to save that money on maintenance and hardware purchases and etc etc so think about cloud services every time you're thinking about local services there are some cases you'll want local services but some cases it just doesn't make any sense and there are complementary cloud services to almost every one of our local services that we can offer i want to mention the ways that you can serve them locally because it does make sense sometimes for example a print server is going to be cups common unix printing system this works across the board if you're sharing a printer with mac or linux it's going to be using cups and even windows can print two cups servers it's like a centralized place but honestly most printers now pretty much have a robust ability to share and queue jobs on their own so we don't always have to use a centralized cup server uh we can all print to the same printer and it's just handled well netatalk also has printing if you have an old school apple computer you want to use that for but again that's not even used very much anymore how could your local printer be used with cloud services well it doesn't make sense until you think about no configuration printing right with google print or air print these are ways that you can actually send your print job over the internet to a printer that may or may not be connected directly so it's something that isn't going to replace common everyday office printing but it's something to think about if you want to be able to configure or especially print from mobile devices having cloud solutions is very powerful now mail is a special case because a lot of times we want mail to be as secure as possible and that means we don't necessarily want to give the ability to another company to host our mail but remember with great power comes great responsibility keeping your locally hosted email files whether it's using postfix or xm or sendmail can be a fulltime job because we want those to be really secure not only so people can't read our emails but so our servers aren't compromised and used to send out spam to the entire world so even though having that fine control over security of your own mail is important think about how nice it is if a huge company like google or microsoft or yahoo would have to worry about the security aspect so you can actually just focus on communicating with it so there's a lot to be said about using a thirdparty company for email even though you do lose some of that local control and then lastly i want to talk about a proxy now when we talk about proxies in linux we're talking about squid or squid guard which is an addon to squid and generally proxies have historically taken the load off of your internet connection so a bunch of computers can actually request things one time from the cloud and then the proxy server kind of takes that and distributes it internally but our internet connections are very powerful now so we don't often do that now when we're thinking about proxy a lot of people mistakenly call a web filter a proxy and what a web filter does is it stops you from going to like pornographic websites and that's what squid guard does there's a lot of commercial products that keep a list of sites that shouldn't be visited by people and there are other solutions too like open dns is a way that you can set your dns server your upstream dns server so that it doesn't resolve sites that you don't want to see like pornographic websites they won't even resolve properly so your users can't get there now when it comes to actually caching or proxying large things there are some big companies like akamai that will cache entire video libraries of like netflix and stuff because that's a way that they can save bandwidth between isps but in general we don't use proxies as much as we used to although there are still a lot of use cases for things like squid guard or open dns for blocking unwanted websites that are going to waste time or expose us to things that you know we may not want to be exposed to in the company or in a school so while it's really important to know that there are local services that you can provide on your network using linux i encourage you to think through before you install a server on your network to do a particular task see if it really makes sense to host that locally or if going for a third party service might make more sense authentication services and database services are both things that we often do on local computers i want to talk about their purpose and their importance but it also might seem a little weird that i group them together and that's because we usually think about authentication and databases as local services that run on robust servers and that's for a reason it's because they're very very important to what we do on a regular basis and i just want to talk about while we oftentimes still run them on local computers there's still an argument to be made for putting those in the cloud as well now generally if you're talking about linux you're thinking about a sql service if you're thinking traditionally right this could be my sql or maria database which is basically my sequel only newer postgres there's a whole bunch of other sql servers that would normally run on a really robust server on our network now there are tons of other database services some of them are you know are nonsql and and some of them are good for certain types of data and bad for other types of data but there are tons and tons of database services and generally when we have a database server it's going to be on its own computer and that's just because database servers tend to be kind of robust so you're going to have something like mysql maria postgres they're going to be on their own server and authentication services are similar as well because we need to have a central place to authenticate all of our users now when you log into like gmail you're going to put in your username and password and that's all stored in a central place on google servers if you're on your local network there are tons of ways that you can store user information on your own local network in fact for years i would use nis on a linux server and i would use all of our computers in the network would authenticate to this one centralized server and it was so important that all user information was stored on there that i had a redundancy so that in case one of the servers went bad i would still have a backup that was live high availability so there's lots of ways we can do it but the vital importance of having our own server or even multiple servers is important but here's the deal you'll notice i have all of these things around the outside you can use open ldap to host your user accounts on your network and it's going to work with a bunch of different programs but honestly i would say 95 of the time user data is going to be stored on an active directory on a windows computer even if you're a linux person in a linux shop it seems like ad has taken the cake as the king when it comes to user authentication now the one alternative to that is if you're going to use an online like saml or i don't want to get too much into programming but there are ways that you can leverage online authentication for your local stuff like single signon and things like that so that may take the place of active directory on your network but if you're if you're talking about user authentication active directory is almost certainly going to be where a medium to large or even small office is going to host all of their user accounts so it's important to know that you can host authentication services on a linux machine but you may not end up doing that because active directory is probably going to be somewhere and it's a great place to centralize your users and your computers so the whole point of this nugget is really twofold one authentication can be done on linux and i did it for over a decade where everything was hosted on a local linux machine and i used nis for authentication i could have used openldap but generally you're not going to do that on a big network because you have more than linux machines that need to authenticate now the other thing though is database servers i want you to know that they're almost always going to be on their own server because they use a lot of resources a lot of memory a lot of cpu a lot of disk io so if you're setting up server roles on your network think about a database server as having its own need unless you host it out on the cloud in which case you're just paying for somebody else's resources which can often be even more effective centralized logging and monitoring is great for a big network because it's easy to lose track of a big number of servers but honestly it's great even if you have a tiny little network now i'm going to look at combining syslogs but i also want to peek at snmp because both are vital for combining information from multiple sources centralizing them if you will so that it's easier to see and easier to use for making predictions and adjustments in how your infrastructure works so first of all i want to talk about central logging okay now there are a ton of devices out there a lot of them can be you know servers you know and lots of servers on the rack and you don't want each one to have its own set of logs you want to combine them but there's other things now that can create logs but may not have the storage area to keep those logs on themselves like security cameras routers motion detectors smart bulbs printers all of these things can create log files and if you can redirect them to a centralized server that's going to allow you to comb through data in a very efficient way and in fact sometimes this is the only way you can get data from certain devices that have absolutely no storage on them but can generate logs and information now when you combine them all together it is pretty neat how it works the centralized log server is going to just have one big log file however each individual device is going to put its own name in the log file so you can sort by whatever computer is adding it so even though you have one log file that is going to contain everything it's easy to separate out the individual devices to see what they're doing using tools like grep or there are some really fancy devops tools that will allow you to sort data from combined log files when you combine log files like that you end up being able to see some trends or see some relationships that might not otherwise be easily accessible for example if none of these devices are able to access a dns server well maybe you have some problem with the network in that portion of your company and so you can help troubleshoot based on what logs are being submitted now snmp is slightly different and it stands for simple network management protocol but it's a little bit of a misnomer because traditionally this was used to not only read data but also remotely control devices using this network management again it was a twoway street protocol now there are still some instances where you can use this to manage devices but mainly it's used for pulling data and what i mean by that is let's say you have some data that you'd like to concatenate together for an informative data pie that's just delicious you're trying to make graphs or something you can use snmp to pull data from a device one of the most common things that i use it for is i have a router that i connect to the internet with and i would like to see some interface statistics for what's going on how much data is going through it and things like that snmp can pull that data it's not really like individual servers pushing data to a centralized logging it's kind of going the other way it's kind of using snmp to pull data from individual devices and what that looks like in practice here is this is actually the home page that every time i load up a web browser this is what loads up and i have a couple convenient links for me you know to go to see the weather in my area that sort of thing but i have these graphs that show the bandwidth usage both in my house in the town and we own a farm as well that has fiber internet connection and this shows the connection between them now you'll notice there's a lot of matching between the two and that's because i will often back all of my townhouse data to our farm because it's an offsite storage location so that makes a lot of sense but this is just a way that i can see what's happening on my network and i pull this from my routers using snmp now it's important to realize that while they accomplish sort of the same thing centralized logging allows all of your servers to push data to a centralized server so you can comb through all that data in one place whereas snmp is generally used as a protocol that you can pull data out of a server and do something with it like make cool graphs or whatever it is you want to do vpns might be something that you use every day but don't really understand what's going on so vpns or virtual private networks are really just a way to connect to a private network that isn't accessible from the internet itself now talk about the concepts then i want to talk about what options are available if you're using linux because vpn isn't just a onesizefitsall thing there are several different protocols and stuff that you can use to connect but first of all conceptually what is going on well we have two networks let's say these networks are separated by being in different countries okay now you want to have this computer be able to interact with this computer or this server or something on the remote network but you don't want to open those ports up to the internet right because that's unsafe so what you do is your router or your linux server or something will establish what's called a tunnel and this tunnel is just a layer of encryption that goes from one side to the other and then inside that encrypted tunnel it just sets up a route like it would any other route on your network so your router just sees inside the tunnel and it sees a route using standard network addressing like let's say the vpn internal route is 10.10.0.5 or something and it just sees this as another route so that this computer can route information across the tunnel to this router and then get into here so it works the same as traditional routing the only difference is the router or like i said the linux server or whatever it is sets up this tunnel that blocks anybody on the internet from actually seeing what's going on in the route so that tunnel is set up and then the route goes inside the tunnel so nobody sees the traffic now this setup that i have here is called a site to site vpn and what it means is since there is a standard route set up here anything on this network is going to be go is going to be able to go over here to this network and anything on this network is going to route over to this network as if they were in the same building it's just a standard route inside that encrypted tunnel now the other type of vpn is going to be just an end user connecting to an office and if if you're a remote worker or you're like a road warrior you're going to use this a lot your computer is going to establish the same type of tunnel to block all of the internal network traffic from the internet itself once it sets up that tunnel then just your computer is going to be connected to this internal network okay so you're actually going to as if you plugged into an ethernet port in the wall next to all of your other employees or your other fellow workers who happen to be in the headquarters so what this does is it puts your computer inside this local network by establishing a tunnel and then setting up routing protocols that will put you inside there so it's a little bit different than a site to site because these computers are probably not going to like serve data from your computer it's going to work a little bit differently but conceptually the same thing is happening your remote computer is now able to connect to computers inside the remote network and it's all protected from the internet by using a vpn now i talked about different protocols to do that and there are a bunch there's openvpn which is an open source program that allows you to establish these types of connections uh there's ssh which can establish a tunnel for you to do stuff there's l2 tp there's ipsec there's all these different protocols and programs that will allow these tunnels to be created and a vast majority of these will actually run on a linux server inside the network so if your router itself doesn't support vpn that's okay you can port forward into a linux server and the linux server will handle all of the vpn routing now there are lots of nuances when it comes to vpn like can you only connect through the remote business can you connect to the internet and also to those things is all of your traffic routed through the vpn even if it's a slow link so there's a lot to think about when setting up vpns but once you understand what's going on it's a lot easier to plan how you're going to implement it so that it so that it can best serve your users or your multiple branch offices so they can communicate to each other i hope this has been informative for you and i'd like to thank you for viewing containers aren't really new technology they've been around for quite a while there have been options like lxc and you've probably heard of docker and also kubernetes comes into the mix which is actually like an orchestration tool that takes care of docker but while containers aren't new they are kind of the new kid on the block when it comes to devops and containerizing applications is something that's really really popular now conceptually they're a little bit like a virtual machine but they're they're different enough that it's important to understand the difference so let's say we have a traditional virtual machine which i'm going to say is the lefthand side of this slide so how it works is you have the the big computer you know and then that computer or that host has its own operating system and then on top of that we carve out a section of the host's cpu and memory and cards and hard drive space we carve that out and then on top of that we install another operating system inside this virtualized environment and then we can put applications on that now containers work in a different way they run right inside the host operating system so you just have an app running in another app running and another app running and this seems like the traditional server model right where you just install linux and then you install applications like apache on top of it the difference is with a containerized application they are running directly on the host computer but they have not really their own operating system all they have is like their own file system and they're jailed off or they're completely separate from the file system of the host operating system itself so they run in their own little world but they're still running directly on the operating system it's just like they're sectioned off in a little container but they're still running on the host operating system and that's really what makes them efficient you'll notice over here um yeah it's drawing it's not like the actual technology but it takes up a lot more hardware and and storage and slices of the host system itself if you're going to install an entire operating system on top of virtualized hardware it just you're not you can't put as many things on one host operating system plus you also have then this operating system to maintain along with this operating system these don't have their own operating system so you don't need to maintain anything except the application itself and it works really cool that jail system is neat so i want to show you how that works let's go actually over to a virtualized environment so here i am in i'm running ubuntu and i have docker installed so we're not going to get into how docker works there's a whole course on how docker works that i taught which is one of my favorite courses but first of all we're just going to start a docker container and we're going to put ourselves inside of it so this is just a little bit of free info we're going to say docker run dash it for interactive tty let's say we're going to run the ubuntu image and i want to run bin bash which is the shell command that i want to run so now boom it it was that fast right it created the container that quickly and i didn't like pause the video and wait for it it actually went that quickly and now we're inside this container which acts like its own operating system because remember it's jailed off but it's still running on the system and we can demonstrate that see i'm not i'm no longer on cbt docker now i'm on this internal container and we have our own file system but let's run a command in here we're going to run something that'll show up with cpu usage so i'm just going to say dd input file equals dev 0 i'll put file equals dev no and this is just something that's going to keep running keep running keep running and not really do anything except use up resources on the computer and i want to do that so we can come over to cbt docker here run the top command and even though this is now in its own container completely separate from cbt docker the operating system you can see look dd shows up as another command running inside this computer because even though it's separated in a container it's still using the same operating system the same kernel the same hardware that everything else along this system is it's just separated so it doesn't interfere and dependencies won't interfere with other apps it won't interfere with dependencies on the host system it's just super efficient so whether you're looking at lxc containers or docker containers which is what we looked at it's important to understand that containers are a lot like a virtual machine except they don't use all of that hardware and they don't completely section themselves off and most importantly they don't have their own operating system that's where containers make things much more efficient and much easier to deal with with much less overhead meaning you don't have to maintain the operating systems of individual apps it's really easy to confuse the concepts of clustering and load balancing because they kind of are the same it's kind of like the question is a hot dog a sandwich i mean they're both meat between bread but are they the same thing not exactly not quite but they both function similarly and that's what clustering and load balancing is like the difference is though clustering is an actual computer term for computers working together to do one task that can be split up whereas load balancing is more of an it concept that can be accomplished in the multiple ways so when we have a cluster basically we have a bunch of computers like we have here that are working together and then there's like a cluster manager it can be a computer it can be a software on you know one of the cluster computers but it actually keeps track of which computer in the cluster is doing what part of the task and these are designed to work together they know about each other these computers are a team and they really do well if susie here sends them a task that is designed to be broken up into multiple pieces so people can work on it at the same time so some jobs lend themselves to clustering solutions whereas some of them don't but basically they know about each other the cluster works together as a team to break down a bigger task into a bunch of smaller ones they can work on at the same time a load balancer like i said is more of a concept it load balances meaning it it has a big job and it splits that job up and lets different computers do the job but usually these computers don't know or care about each other at all they don't know each other exist the load balancer itself whether it's software or a hardware device it knows the entire big load that susie is sending to it and it splits it up and says okay she has 12 jobs so you do three jobs and you do three jobs and you do three jobs they don't know each other or doing jobs as far as the computers know there's only three jobs to do it's the load balancers job to keep track of everything so it's a little bit different they don't really work together they each work separately and accomplish a bigger task that the load balancer itself knows about now i said it's a concept it can work multiple ways right a very common way it's done is the load balancer will be in front of some web servers and there's a whole bunch of people that want to hit that web server and so the load balancer says okay you go to this one and now you go to this one and the next request will go to this one and it splits up the load so that one computer isn't doing all of the work now we can do that very simply on a linux machine if the linux machine is set up for round robin dns now this is not a great way to load balance just conceptually this is what it's going to do so here is my dns configuration on my network i have the domain name web it's actually set up for three different ip addresses okay so you'll see it's actually web web web but they have three different ip addresses and what my dns server is going to do is then round robin and it will split up the load between these three let me show you how that works here we are on the command line so i'm going to say ping web and we'll see we get the response from the web server notice the ip address here is 21658192.238. if we do the exact same thing now boom if we get responses but notice it's sent it to a different computer together so we got these from another computer and if we do it again we'll get still that third one if we do it a fourth time it's going to wrap around and give us the first one again so this is technically a load balancer we're using dns round robin load balancing and it's conceptually splitting up the load of the pings so that each computer that is responding only gets a third of the requests so i guess it's fair to say that all clustering is load balancing but not all load balancing is technically clustering because clustering is a specific way that computers work together to accomplish a task now is a hot dog a sandwich i gotta leave that one up to you i have no idea but i hope this has been informative for you and i'd like to thank you for viewing cron jobs are pretty much the linux equivalent to like the task scheduler in windows now there's some really cool things that we need to understand and that's how to set up the scheduling which is kind of complicated but also very powerful and i'll be honest it's kind of fun the other thing i want to point out though is that there are premade folders that you can just drop scripts in and they will execute at a regular interval i'll show you those but first let's talk about how we set up the schedules because it can be intimidating but like i said it's not that bad and it's actually kind of fun so the scheduling fields which we'll look at in practice are separated into five different fields so we have minute hour day of the month month of the year and day of the week and how it works is for example this first line that i have has all asterisks and this means everything so every minute of every hour of every day of the month of every month of the year of every day of the week it's going to happen so this means every minute for all of eternity whatever task we schedule with this string is going to execute so every minute it's going to do whatever you tell it to do now there are some shortcuts we can use for example down here i have asterisk divided by five this means every five minutes now we could actually spell it out we could say zero comma five comma ten comma fifteen all the way to fifty five but i don't really like to do that because it's a big mess and you know one field would be this entire big string of numbers so rather than do that we can just say asterisk divided by five and this is going to be every five minutes during the third hour of the day so this means at am 305 a.m 3 10 a.m 3 15 a.m but once it gets to 4 a.m it's gonna stop doing it okay so this is during the third hour every five minutes every day of the month every month year every day of the week so this means every day it's going to do this but only between 3 a.m and 3 59 a.m and every five minutes okay now this one is very very very specific this says two minutes after the fourth hour so two and four means at 402 am on the 13th of july but only when that 13th of july lands on a tuesday the day of the week goes from zero to six so two is a tuesday right sunday is zero monday is one tuesday is two so this means every july 13th at 402 am if it happens to also be tuesday so this is only going to execute every few years when july 13th happens to land on a tuesday and then down here this one zero minutes after the hour so this means that six a.m precisely on every day of the month every month of the year days one through five so this means monday through friday at six a.m it's going to execute whatever task okay so this is basically the way of saying six a.m every weekday because we've specified one through five on the days of the week over here pretty cool right let's actually see how it works in practice now i'm on a centaurus system here and i'm root because we're talking about the systemwide cron jobs if we go into etcon.d we're going to see we have a few files in here now any file in here is going to be read by the cron daemon so let's actually look at one let's look at systat all right because this is already in there so we're going to look at systat and here we can see a couple things are scheduled here are the five fields that we just talked about so here's the first field so every 10 minutes of every hour every day every month year every day of the week so this is going to be every 10 minutes it's going to execute as root so this field talks about what user it's going to run as and then the rest of it is what it's going to actually do so we have the five scheduling fields who it runs as and then the last part however long it is is what it's going to execute so down here these are commented out but let's pretend it's not this would be every hour at zero past right because this is like at one o'clock two o'clock three o'clock four o'clock it's going to run as root this command down here at 23 which is 11 so 11 53 p.m every day see all these are asterisks so every day as root it's going to execute this so you can either add to any of these or really the best thing to do is create your own right just create a file and then put that scheduling the username and what you want it to execute and it will do that now the one other thing i wanted to mention really quick if you go back into the etc folder and let's do an ls and just look for cron we're going to see there are a bunch of folders in here there are cron daily cron hourly cron monthly cron weekly and if we go in there let's go into cron dot daily we're going to see these are just executable scripts these are not timed things right let's look at one real quick so vi log rotate notice there's no like startup star star star star star there's no scheduling in here this is just an executable script that we want to have execute every day so this is going to do the scheduling of everything in here once a day same with monthly same with hourly the kran deny is a way that we can tell a specific user that they're not allowed to use the kron daemon for personal use uh cron.d we looked at actually cron tab is a single file let's look at that one really quick that'll be our last thing etc cron tab and this is the same sort of thing we can put things in here if we want it even tells us like very specifically all the things the five fields the username we want it to run as and then the command to be executed this is the same thing as creating a file in the eccentric cron.d folder you can just add things here and they'll automatically execute now the premade folders are very convenient for dropping scripts in that you want to have execute every so often but really the coolest part about cron is just how flexible that scheduling system is and you can figure out how to do that by you know manipulating all of those different fields like we looked at in this slide here so i encourage you to just try to figure out how you would specify a particular time and you know just play with it it's a lot of fun to do when it comes to scheduling events you have a couple options as a personal end user on a linux account you can use crontab personal crontab which you invoke by typing crontab minus e or we can use the at daemon which is a one time thing it's not like for recurring events it's for events that happen just at a specific time so let's go right to the command line because it's not difficult to use either one and scheduling tasks is something that's really really nice to be able to do so here we are on a centos machine i'm logged in just as a user notice i'm not root and the first thing i want to do is look at my personal cron tab now this is a little bit different than a system wide crontab so first we type cron tab minus e and it's going to bring us into an editor okay and now this is my personal crown tab again it's not system wide and it's slightly different because i still have five fields if you're not familiar with the five fields look at the systemwide crontab nugget because it'll explain how this works but we have every 10 minutes of every hour of every day of every month of the year every day of the week so here we have every 10 minutes it's going to do something now notice there's no field here that specifies the user in the system wide we have to specify what user it runs as but since this is my personal cron tab it obviously runs as me so we just have the five fields and then we have the command that we want to have it execute at the time scheduled here so what this what happens here is we have echo and this text string and append it to a file called homebob timetracker.log every 10 minutes so every 10 minutes it should add a line to our field so let's see if this is actually running because it's been here a while let's quit here if we do ls we can see oh there is a file time tracker.log and if we look at it well sure enough it looks like about a half hour has gone by since i created that crontab entry and it's been adding to this file if we do ls minus l we can see the last time that was touched was at 18 10. so if we waited around until 1820 it would do the same thing again it would add another line to it so that's how you do a recurring event using a personal crontab now if you just have something you want to have execute one time let's clear the screen we can use the at daemon and first i'm going to do something really quickly so we'll say at and then can specify the time and this is what's nice it's very flexible we could say tomorrow we could say next week and it will interpret all of those different commands it uses a lot of fuzzy logic to figure out what you want i'm going to say at now plus one minute and then we're gonna get this at prompt which now allows us to execute something so i wanted to do echo this was a one off and i want to append that to home bob time tracker dot log press enter and now we could do another thing we could have like a whole list of things we wanted to do at now plus one minute but i'm just going to do control d and that will put it in queue so it says job4 is in queue and it's going to execute at 18 13. okay now if we type atq oh it already happened dog on it first of all let's look cat time tracker look at that it did it right it put it to the end of the file but i need to do another one so at now plus one minute again and i'm gonna say echo hello into home bob time tracker dot log ctrl d atq ah there we go okay did it in time so what this shows us is the queue of things that at is going to run so job number five is scheduled for thursday june 20th at 1814 and the user bob is who's doing it so if we keep pressing at q once the time rolls around it's going to execute that and then go away because it's a oneoff right we could do multiple things we could say at tomorrow then i want to say echo test tomorrow i spelled it wrong but that's okay on home bob time tracker.log control d on a blank line now if we do at q we're going to see well look job 5 executed but job 6 is going to wait until tomorrow and it does it the same time tomorrow so 24 hours from now and tomorrow at 18 14 it's going to do that command test tomorrow okay now let's say we don't want to do that we want to change our mind well then we can say at rm job six and now at q is gonna say there's no jobs because we've deleted job number six that was going to execute tomorrow but if we look at time tracker look at that sure enough hello was put there along with that this is the one off and if we wait around until the next 10 minutes pass our cron job is going to add another 10 minutes has passed on to the end of this file and that's how we can schedule things with crontab for a repeating task like these or just a oneoff task like this by using the at daemon it's great to be able to do systemwide things using cron but i personally like the fact that you can do it as a personal end user using cron tab minus e and it's going to do it just as your user so you don't have to become root or worry about escalating privileges it's going to just execute it as you even if you're logged out same with the at daemon even if we're not logged in it's still going to execute it at the given time working with multiple processes on linux is really really easy because you can put them in the foreground or the background and interact with them however you want now there are some tools that may not seem intuitive at first but once you get the hang of using them they're really really easy and there's also some keystrokes that we're going to have to learn and a couple tricks that'll allow us to do things that we normally couldn't do so let's go right to the command line because this is the kind of stuff you have to experience in order to really understand so first of all i am on the command line and i'm going to show you how to put a process in the background now i'm just going to use a simple process called sleep the sleep command if you're not familiar with it it just pauses right so if we say sleep one it's going to sleep for one second and then it's going to be done so i'm going to say sleep for a whole bunch of seconds which will like i don't know that's probably a couple hours or something and then i'm going to put the ampersand after it now what's going to happen is notice it's done a number one and then this is the actual process number so our job number is one now we can do that with another one sleep two two two two two put that in the background and now we're gonna see we have job number two in the background with this process id and they're just running in the background if we type jobs we can see sure enough there they are there's sleep one one one there's sleep two two two job one and job two now if we want to start going and using one again like let's say we wanna bring one in the foreground we just type f g and then the job number so fg two and it's going to bring us it tells us what the command is that's running and here we are we're just at the command line here if we wanted to stop this we'd have to do control c it stopped that process and now if we do jobs see there's only the one job running it's actually really cool now what if we did a job like we did sleep 3333 and pressed enter and i'm like oh man i really wish that was in the background i didn't mean for it to just like hold my command prompt here well what we can do is do control z and it stops it and puts it in the background so see it says stopped if we type jobs we can see this first one that we did is still running but this one is stopped so if we want to make it run in the background we have to say b g for run in the background job number two and now if we do jobs we're gonna see now they're both running in the background so bg and then the job number will start that background task running fg will bring it out of the background and bring it right to our interactive terminal okay so that's kind of neat it's a way we can create them we can put them in the background if we've started them and we don't and we want to put them in the background just ctrl z will suspend it briefly and then we can tell it to go in the background by doing bg and then the job number so let's do foreground one i'm going to do control c foreground two control c and now jobs we have no more jobs running now there is a problem because if we have a job running in the background like i have a command here called my hello all right and all it does is every two seconds it prints hello on the screen all right so i'm gonna ctrl c what if i wanted to put my hello in the background and it's going to run in the background i was still going to put it out on the screen every hello but it's running in the background if we do exit it's still running in the background but the problem comes where if we log out and then we log back in we open a terminal window and we do a ps aux grip for my hello we're gonna see it's no longer running this is just actually the grep process that it actually found here but notice it's not running in the background anymore and if we wanted it to stay running just putting it in the background wouldn't work we'd have to use a program called no hup because here's the problem when we log out of the system up here it actually sends a hang up interrupt to all the running processes and the hang up or hup tells it to stop running well we can run a program by saying no hup which means don't hang up when the user logs out what program we want to run so my hello and then ampersand in the background it's going to put it in the background just like before we can see it jobs there it is running but it says it's ignoring the input and appending the output to nohup dot out all right so i'm going to exit i'm going to log out and then i'm going to log back in open up a terminal window i want to notice two things one if we do a ps minus aux grip for my hello it's still going to be running see here it is it's still running because we ran it with no hup but here's another cool thing if we do an ls see this nohup dot out let's look at that all of the output was put into that file so all of the hellos were put into that file so we know what's going on plus when we logged out it didn't stop running so not only is it really easy to handle processes when you're on the linux command line it's also possible to do a couple cool things like no hub if you want to make sure it doesn't quit when you log out and also control z of a running process which will put it in the background suspended and then you can run bg to make sure that it continues executing in the background finding specific information about local devices on your system can be a little bit challenging but thankfully there's a bunch of tools that will help us along the way d message is one that we can use that will kind of see how things are going in real time if you plug something in d message is going to show you the results of plugging that in but then there's a whole suite of ls tools if you will they start with ls it's ls usb lspci lsdev msblk ls cpu all of these tools are going to be able to be used to find out information about hardware on our system and there's a nice trick to remember these so that you don't have to actually remember pci dev blk cpu all of these different things so let's check that out and see if we can find the various hardware that is in our system you have to be root to use most of these some of these you can use as an end user but since we're going to be looking at all of them i became root just so that we get a better view of what's going on now first of all d message you just type d message on the command line and it's going to show you things as they happen and this is the location that we'll see things happen if we make changes to the system like if we plug in a usb drive or plug in a sata drive or something like that a new mouse it's going to show up here and it's going to give us information about the particular device but if something's already plugged in or you just want to see like what's built into the system that's where the ls tools come into play so let's clear the screen and what i like to do is just do ls and then hit tab a couple times and that's the trick because really tab completion is vital here there are so many of these commands now we know ls just means list like the file directory but they've used this same tool or the same keystrokes to prefix a bunch of commands like ls blk for example which will show us the block devices it will list the block devices on our system if we do that it's going to show us all of the block devices that we have like the floppy drive all of these are virtual loopback devices that were created down here these are our actual drives on our system sda we can see right here it's mounted on forward slash sda1 is then we have a bunch of other drives down here that aren't being used right now but they're all 10 gigabytes in size we use those and we set up raid before but this shows us all of the block devices so if we do ls and hit tab a couple times again let's look at the next one we have ls block we have ls cpu now i actually really like this one because it will give you all of the information about the cpu in the system so we can here we can see here that it's a 64bit processor little endian it's an intel it actually shows us the actual processor itself the model number it shows us the clock speed all sorts of stuff that we have vtx enabled so we can do virtualization and it shows us all of the awesome things that our cpu has including all the flags that it supports when we're compiling things anyway that's ls cpu if we hit ls and tab a couple times again i'm going to do this every time because it's the quickest way to see what's there we have lsdev i want to do this you have to be root to do this and what this shows us this might not be as useful as it was years and years ago and we would have to troubleshoot hardware more frequently but what this is going to show us is the the device like what device it is like our floppy disk our keyboard a pci port here and it's going to show us what direct memory access number it uses what dma what irq number it's using uh the io ports meaning like in memory what io parts of memory or what parts of memory is it using for i o and it will show us all these things now if we're trying to find conflicts on our system this might be useful but i'll be honest i've never once had to use this in practice it's important to know that it's there though lsdev will show you all the devices on the system now if you do ls tab tab one that i actually do use fairly often is lspci where is that up here lspci so oh pci will show us all the pci devices so here we have a vga compatible controller this is the acpi controller the ide controller so these are the pci devices that are plugged into our system uh we can do ls usb now i don't have anything plugged into our usb ports but if we did lsusb would show us what's there same thing ls pcmcia if you're a laptop user you might have things that show up if you type that and one last one we'll look at is ls mem and this will just give us the range of the memory and how it's being used and where it is in our system but really the big big pull away from this is ls tab tab and it'll show you all of the different ways that you can look at the devices on your system so whether you're trying to get an inventory of the things that are currently on your system or you want to see changes as they happen in real time there's some really simple builtin tools on our local linux machine for detecting and looking at specifics on devices you ever try to solve a problem only to make it worse yeah me too for example we had a wall with a nail in the wall and that nail would get loose after hanging stuff on the nail for a long time because it was just in the thin layer of drywall this nail just went into the drywall and then the more we hung stuff on it the more it got loose so i thought i would make things better by getting rid of the nail drilling a hole in the drywall and then getting one of those really nice hollow wall anchors that you could then put a nail or a screw into and it would be really nice and sturdy the problem is i drilled the hole way too big the hollow wall anchor didn't work at all and then i just had this gigantic hole in the wall that my wife was really upset about that's kind of what happened in the world of linux virtual file systems and let me tell you what i'm talking about here because the idea of virtual file systems has been around since the unix days basically it's a file system that's created when the system boots up and one of the really popular ones is called proc and what this was used for is process information like of running apps and running programs on the system they would store all of their runtime information in this virtual folder in memory called proc and then people said hey that's a really neat place to store things like process id number 181 for that application and 2 5 5 6 for that application what if we also put things about like the cpu in there or the network card in there and then people started adding things to the proc folder it started to get a little bit confusing and so we thought hey wouldn't it be a great idea if we separated all of that kernel information so that process information was stored in the proc folder and kernel information was stored in assist folder they're both virtual file systems that work the same way but some organizations seem to make a lot of sense right now the same thing with dev dev is a folder that we thought why don't we start putting things like information on different devices like hard drives or mouses mice mouse mice mouses anyway when we put those things into the dev folder oh here's the issue the proc folder had been around for a long time and it already had a mix of things in it so we said okay we'll keep all of those things and then we'll just put new things in the sys folder the problem is there's not that many new things so while the sys folder is very well maintained and very neat and organized it doesn't contain a lot of the things that we use on a daily basis because they were already in the proc file system and now they're there for backwards compatibility now that doesn't mean proc and sys are any less useful if we go into the proc folder and do an ls we're going to see here are all of those numbers these are the process ids that i talked about right and if we do like an ls of all the process id 47 inside here it's going to be all of the various things that this particular application is doing like it's it's memory io what's mounted uh the status all of the things about this running thing are in here so if you want to find out information about whatever process id number 47 is you can look at these various things but notice over here on the right hand side there's also a bunch of things that aren't process ids and that's where like mdstad mem info cpu info let's look at that so if we look at cpu info this is going to just be a complete breakdown of the cpu in the system right now that's running this is our current cpu which is really useful information but it might make more sense if it was over here in the sys folder because the sys folder is very well laid out right there's not a whole bunch of crud here this is laid out in very nice sections we could go into oh let's say the kernel folder and inside the kernel folder it's separated into various things about the kernel like the current configuration all of these things are nice and organized but it's not all inclusive meaning it doesn't contain a lot of the information if we scroll up that's still stored in the proc folder so here's how it ends up working a lot of the interactive stuff is going to be in the proc folder because it's always been there but a lot of the information about the running kernel is very organized well in the sys folder and that's where a lot of times programs will look if they want information about the current running kernel like do i work on this program well i don't know let's see you know information about hypervisor and kernel modules and all of those things it'll look in the cis folder so generally you'll find yourself in the proc folder more often all right now one other place that it's worth looking in is the dev folder and this just shows about system devices right these are hardware devices that are plugged into the system either like soldered in like the realtime clock or hardwarelike hard drives that we put in here and this is going to be information binary information generally available to the system talking about the various bits of hardware that are plugged in now this is fairly well organized too and a lot of times programs will look here to find out information like hey is there hardware that is going to be compatible with my software well this is where it would look for hardware information so while it sounds like it's a big mess honestly it's not that bad there's three main places to look the proc folder which is what we see here the sys folder which we see here and then the dev folder which we see here all of them contain different information sys and proc have a little bit of crossover there's going to be some kernel information insist that's already living in the proc file system but if you're going to be interacting it's mostly going to be in the proc file system and applications are going to be hitting the sys file system pretty hard now i know that's a lot of information about linux virtual file systems the key takeaways are that we need to know information about the running kernel are going to be in generally one of three places in the virtual file system that happens when the computer boots up the proc file system the sys file system and the dev file system if you can look in all three of those places you're going to find what you're looking for but they might be spread out in places that don't make a whole lot of sense just because we keep things there for backwards compatibility i hope this has been informative for you and i'd like to thank you for viewing on a linux system the printing is going to be handled by cups which is common unix printing system which interestingly is actually owned by apple of all people but nonetheless this is what every modern linux distribution is going to use to handle the printing now the nice thing is there are some backwards compatibility command line tools so we can print things from the command line and if you don't have a gui to actually install the printer like if you don't have gnome or kde or anything like that installed you can use a really nice web interface that will allow you to interact with the cup system and install modify and do all the things you need to do to a printer but we're going to look at the web interface really quickly just to show you where it is and i want to show you how to use the command line tools so you can print from a command line system even if it's a headless system on a rack now i'm logged in here on my local ubuntu computer on my local network and you'll see up here i went to localhost port 631 now that's important because cups listens for incoming connections on port 631 and if you go there it'll allow you to log in so you can do things like administration for example we are here and i have one printer installed we can click on it'll give us information about this particular printer called office underscore laser we can see it's a socketbased connection on this ip address on port 9100 and basically it's installed on the computer i actually installed it using the web interface instead of going into the gui on this computer and installing it with the ubuntu tools i used the webbased interface to make sure that it would be able to install correctly and it did once we're here we can actually do things you know from firefox like print the page but if we're on the command line things are a little bit different there are a couple tools that start with l p and if we hit lp and then hit tab a couple times for tab autocomplete we're going to see some of those here now they're not all things we're going to look at right now but mainly lpr is going to send commands to the printer lpq is going to show us the queue of things waiting to be printed so if we type lpq we're going to see office laser is our default printer it says it's ready but there are no entries meaning there's nothing that's currently being printed now we can use lpr to print things in a couple different ways we could actually just say echo this is a test and we can pipe that into the lpr command and if we look over here on my printer it's going to print this page right out and we can see sure enough it says this is a test so it'll do text just like that now we don't have to do line by line we can also do something like in my folder here i have a full file called my document so if we look at my document you can see it's just a string of text things and we can say lpr and then the name of the text file my document we'll print that and as it prints out on the page if we look at lpq we're going to see we have a job right here job number 10 is that we could use lprm if we're quick enough and get rid of job10 oh it's already completed but that is how we would delete a job that we had already sent so if there's a whole bunch of things queued up and something's wrong we can use lprm in order to fix it and if we come back over here we're gonna see sure enough it printed out that entire file for us right there on the page so it's really easy to print using the command line once it's set up with cups and thankfully cups is really easy to set up too because it's all webbased and that web interface honestly is very powerful we can use it to queue up jobs to stop jobs do test pages we can use cups for gui environment printing but the most important thing to realize is that we have access to the cup system by using the command line tools that are installed on our system udev is the user space device manager that has replaced dev fs and older linux systems now what's really cool is it uses sysfs which is a virtual file system that has information about the hardware and then it follows rules in order to keep devices with consistent names now what i mean by that is if you've ever had a linux system with multiple anything but we'll say hard drives the order that you used to put them in the system depended on the name they would get so the first one the system recognized would be sda then sdb and then sdc but on the next boot if they came up in like a different order while all of a sudden this one might be sda and this one would be sdb and this one might still be sdc and those are real pain in the butt so what udev does is it creates devices based on specifics with the hardware so it'll take its like uuid and it will create a solid consistent device that that's always going to be so if you put them in a different order or the buses come alive in a different order on boot they're still going to be the same devices which makes things a lot simpler when it comes to mounting drives and things like that now it also allows us to do some other things like create rules so first of all i want to show you some of the things you can do if you're root you have more access to do this so we're going to say u dev adm this is the administrator command that allows us to do some things with the udev system so the first thing i want to do is say udev info let's actually look at dev sr0 which is actually our dvd drive and it gives us all sorts of information specific to this drive so we'll see a couple things like let's scroll up we see this is n means the name so this is the name that it has assigned it this is the place in the pci device where it exists this is all the specific information about it and if we look lsdev grep sr0 we're going to see sure enough it's right in the dev folder sr0 it's dynamically created when it detects it and puts it in now sr0 is not terribly useful at least for me to recognize a hard drive or a dvd drive when it's in the system now i can see it mounted right here but that doesn't help me sr0 doesn't mean dvd to me so we could do something like make a rule that creates a shortcut to it every time it recognizes it in the system so we're going to go into the etc udev rules.d folder and in here let's see what there is there's one called snap core rules but i'm going to create a new one i'm going to say vi we'll say number 10 so it loads it first sean.rules sean rules nice anyway it has to end in dot rules and then we're going to create our own rule now the format here is something that you just kind of have to get the hang of but the kernel is equal to sr0 meaning this is what the kernel knows the device as it's in the sub system is equal to block which it actually showed us when we did that command and then what i want to do is create a sim link and i want to call that sim link my underscore dvd okay so what we've done is created this rule anytime it has a device called sr0 in the block subsystem meaning like a block device like a hard drive or a cdrom i wanted to create a sim link called my dvd in the dev folder so i can reference it there instead of remembering sr0 so let's save this now we could reboot the computer or we could just do udev adm trigger okay and now if we go into the dev folder and we do an ls we should see in here look at that my dvd it's created that sim link to well in fact let's do ls minus l grab dvd my dvd is a link to sr0 it looks like there already was a dvd shortcut to sr0 but we created a new one by making our own udev rule called my dvd and that's pointing to sr0 and we can use this when we're referencing the device in anything that we want like mounting and that sort of a thing and the cool part is this isn't just a sim link sitting on our system this is something that's going to be created every time the dvd is recognized and set up by the system so on boot even though the dev file system is a virtual file system it's always going to have that sim link my dvd pointing to sr0 now we just graze the surface of the things we can do with udev rules we can actually make things happen when certain usb drives are plugged in that sort of a thing but one of the key things i want you to take away is that it's really smart and it uses sysfs in order to know information about the drivers and everything else how it interacts with the kernel it uses the information in sysfs to create the entries in the dev folder i hope this has been informative for you and i'd like to thank you for viewing you
