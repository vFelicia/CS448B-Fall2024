With timestamps:

00:00 - dive into the world of python web
00:02 - scraping with this scrapey course for
00:04 - beginners learn to create scrapey
00:06 - spiders crawl websites clean and save
00:09 - data avoid getting blocked and deploy
00:12 - your scraper to the cloud all while
00:14 - using python scrapey Joe Kearney
00:17 - developed this course he is an expert in
00:19 - web scraping and he's the co-founder of
00:21 - scrape Ops hey everyone welcome to the
00:24 - free code campus crepey beginners course
00:26 - my name is Joe I'm a co-founder at
00:28 - scrape UPS a platform that gives
00:30 - developers the tooling they need to
00:32 - monitor schedule and run their web
00:34 - scrapers also the co-founder of the
00:36 - Python scrapey Playbook a complete
00:38 - collection of guides and tutorials that
00:40 - teaches you everything you need to know
00:42 - to become a scrapey developer in this
00:45 - free code Camp scrapey course we're
00:47 - going to take you from complete scrapey
00:49 - beginner to being able to build deploy
00:51 - and scale your own scrapey spiders so
00:53 - you can scrape the data you need you can
00:56 - find the code and written guides for
00:58 - this course over at the python scrapey
00:59 - Playbook book free code Camp course page
01:02 - so you can easily follow along with this
01:04 - video
01:04 - the link will be in the description
01:07 - we've broken this course down into 13
01:08 - Parts which cover all the scrapey basics
01:11 - you need to go from never having used
01:13 - scraping before to being a competent
01:15 - Scrappy developer if you want to dive
01:17 - deeper into any of these topics then
01:19 - check out the python scrapey Playbook
01:20 - itself or the scrape Ops YouTube channel
01:23 - which is linked in the description
01:26 - here you will find guides that dive
01:28 - deeper into the topics we've discussed
01:30 - in this course and cover some more
01:32 - advanced grippy topics too such as
01:34 - scraping Dynamic websites and scaling
01:37 - your web scrapers using redis
01:40 - so what is scrapey so I think the best
01:43 - little summary for that is directly on
01:46 - the scrapey.org website so scrapey is an
01:50 - open source and collaborative framework
01:51 - for extracting the data you need from
01:53 - websites in a fast simple yet extensible
01:57 - way
01:58 - so it's an open source framework that
02:01 - anyone can use with python and it makes
02:04 - scraping websites much much easier
02:07 - so it helps you do things like retrieve
02:09 - a Pages HTML parse and then process the
02:12 - data and then store that data in the
02:15 - file formats you want and in the
02:18 - location that you want
02:20 - so that's what scrapey is so now the
02:23 - next question you probably have is well
02:25 - why should we choose scraping what does
02:26 - scraping have to offer over anything
02:28 - else
02:29 - so some of you might have already done a
02:32 - bit of scraping with python and you
02:33 - might have used things like just
02:35 - straight python requests to request the
02:39 - page and then get the response and then
02:41 - you might have parsed that response
02:43 - using something like beautiful soup
02:46 - which helps you parse HTML so this is
02:50 - perfect if you're doing just very simple
02:53 - scraping and you just want to scrape a
02:54 - couple of pages of a website or you just
02:57 - have something like a one-off site that
03:00 - you want to scrape
03:01 - by all means use something like python
03:03 - requests and beautiful soup if you're
03:05 - looking to do anything
03:07 - from small medium to large scale it's
03:10 - much better to use something like
03:12 - scrapey because it comes with a load of
03:15 - features built in that you don't have to
03:17 - worry about so it helps you do things
03:20 - such as data extraction from the HTML
03:23 - using CSS selectors you can do
03:26 - automatic data formatting so it'll
03:28 - format the data you scraped into things
03:30 - like CSV Json XML and many other formats
03:35 - you can save the stuff directly into
03:38 - things like S3 buckets onto your local
03:40 - machine
03:42 - and use middlewares to save the data
03:44 - into databases so so much of that is
03:46 - already taken care for you so what else
03:49 - does that have that you don't have to
03:50 - worry about automatic retrace for
03:52 - example if you look for a page and the
03:55 - page comes back with an error it'll Auto
03:57 - retry it for you and you don't even have
04:00 - to worry about all the logic that goes
04:02 - into things like Auto retrace it looks
04:05 - after
04:06 - concurrency so you can scrape from one
04:09 - page all the way up to thousands of
04:11 - pages at the exact same time using the
04:14 - same piece of code
04:15 - so with all this stuff
04:19 - so much is taken off your plate that you
04:21 - can just focus on doing what you want to
04:24 - do which is okay this is the data I want
04:26 - from the website and this is the format
04:29 - that I want to save it into
04:31 - and the other great thing the fact that
04:33 - scrapey is an open sourced framework
04:36 - means that there's many thousands of
04:38 - developers all over the world who've
04:40 - made great plugins and extensions for it
04:43 - almost every single kind of question we
04:45 - might have is probably already answered
04:47 - so it's very easy to find the questions
04:50 - and answers online and things like stack
04:52 - Overflow for the use cases you'll be
04:54 - going through and it makes it very easy
04:57 - if you don't find the question just ask
05:00 - another one
05:01 - so I would really recommend scrapey if
05:05 - you're looking to do anything that is
05:06 - more than Justice very very very simple
05:09 - use case so if you're looking to scrape
05:10 - any kind of website scrapey would be the
05:13 - place to start off your scripting
05:15 - Journey
05:17 - okay so this course will be delivered
05:20 - through video of course which you're
05:21 - watching right now but along with the
05:23 - video we have an accompanying article
05:26 - that goes with each section
05:27 - so for example part two we have a full
05:31 - article here with all the commands you
05:34 - need to run code Snippets and more
05:37 - in-depth explanations
05:39 - so this makes things much easier if
05:41 - you're not someone who likes to watch
05:43 - video and you prefer to read things and
05:45 - take things step by step that way we
05:47 - will also have all the codes that we use
05:51 - if you want to just jump into a part
05:54 - further down the line you can go and you
05:56 - can download whatever part you need we
05:58 - will have an accompanying git repo where
06:01 - you can just download the code at that
06:03 - point in time and follow on so hopefully
06:05 - that should make your learning that bit
06:06 - easier
06:08 - okay so what we're going to cover on
06:11 - this course so we're in part one now
06:13 - part two will be setting up your virtual
06:16 - environment and setting up scrapey on
06:18 - your computer
06:19 - part three we look at how to create a
06:21 - scrappy project
06:23 - part four creating your first scrapey
06:26 - spider
06:27 - and navigating through different pages
06:29 - getting the HTML from the page and then
06:33 - extracting what we need from the HTML
06:37 - we look at crawling through multiple
06:39 - Pages then how to clean the data that
06:42 - you've just scraped using item pipelines
06:44 - and Scrappy
06:45 - then in part 7 we'll be looking at
06:48 - saving the data to files and databases
06:51 - and all the different ways we can do
06:52 - that
06:53 - part 8 looking at everything to do with
06:57 - headers and user agents and how we can
06:59 - use user agents and headers to bypass
07:03 - certain restrictions that websites might
07:06 - be putting on us when we try and collect
07:08 - the data
07:09 - part nine we'll be looking at rotating
07:12 - proxies and proxy apis and how they can
07:15 - help us bypass some of those issues
07:18 - about getting blocked
07:21 - part 10 we'll be looking at deploying
07:23 - and scheduling our spiders using
07:25 - scrapeyd so when we want to run a spider
07:28 - on a regular basis we want to have
07:31 - something set up that we don't have to
07:33 - worry about and kick off manually but
07:35 - that will be programmatically run on a
07:37 - daily basis hourly basis every 10
07:40 - minutes or whatever so we look at
07:42 - everything to do with that in Parts 10
07:44 - 11 and 12 and we're looking at the
07:46 - different options that are out there in
07:48 - terms of the free options open source
07:50 - options paid options
07:52 - the pros and cons of those options that
07:55 - we have and then that brings us to our
07:59 - recap at the end which is part 13. and
08:03 - part 13 we'll just go through everything
08:05 - we've learned and recap and talk about
08:08 - what there is left to do if you want to
08:11 - take your scripting to the next level
08:14 - so I think that's everything I wanted to
08:16 - talk about and we will see you in part
08:20 - two
08:25 - so in part two of this course we're
08:26 - going to be looking at how to install
08:28 - python
08:30 - setting up your price in Virtual
08:31 - environment on Mac Windows analytics and
08:35 - then finally how to install scraping
08:37 - okay so let's get started with that
08:41 - so first things first how to install
08:42 - python so it's fairly easy the first
08:44 - thing we want to do is we want to just
08:46 - go to
08:48 - python.org
08:49 - go to the downloads section
08:52 - and then you can click the download
08:54 - python 3.11 or whatever version it will
08:58 - be
08:59 - when you're looking at it so obviously
09:01 - I'm doing it I'm on the Mac so it
09:03 - automatically detects that and it
09:05 - automatically proposes that I download
09:07 - the version for Mac OS you guys if
09:09 - you're on Windows you'd be wanting to
09:12 - download the latest version for Windows
09:14 - so go ahead do that if you don't already
09:17 - have python
09:18 - we can quickly check if you do have
09:20 - python by going to your terminal or
09:23 - Powershell
09:24 - so open that up
09:26 - and then I'm just going to open one up
09:29 - quickly here in Visual Studio code
09:32 - and what you just want to do then is
09:34 - just type in Python
09:37 - and
09:39 - dash dash version
09:42 - and as you can see here python version
09:46 - 3.9 is installed for me so I don't need
09:49 - to go under loaders because I know it's
09:51 - already installed
09:53 - so go ahead and check if you have python
09:56 - installed if you do have it installed
09:58 - you can move on to the next section and
10:00 - if you don't just go ahead download
10:02 - Python and install it
10:05 - okay so the next thing we want to do is
10:09 - install pip if it's not already
10:11 - installed so pip is just a package
10:14 - manager for python so we can download
10:16 - third-party packages for our python
10:19 - project
10:20 - so what we do again we just check is it
10:22 - installed so it's just pip
10:25 - version again
10:27 - and as I can see I have Pip version 22
10:31 - installed here
10:33 - so if you don't have it installed
10:36 - we link to it in the documentation in
10:40 - the article that we have for this and in
10:42 - the video as well
10:45 - so to install pip
10:48 - you go to
10:50 - pip.pypa.io and go to installation and
10:54 - they have supported methods for
10:56 - installing on Linux Mac OS and windows
10:59 - and to give you the commands you need to
11:02 - run
11:03 - once you have a python installed to
11:06 - install pip so it's very
11:08 - self-explanatory and very easy to do all
11:11 - you need to do is copy this line and
11:13 - paste it into your terminal and hit
11:15 - enter so if I do it here it will
11:19 - just
11:21 - tell me that I already have it installed
11:25 - so as you can see it says requirement
11:27 - already satisfied
11:29 - pip in and then it gives the path
11:32 - so we have Pip installed and the next
11:36 - part is we want to install a virtual
11:39 - environment
11:40 - so VNV which comes with Python 3 the
11:44 - latest version of python will already be
11:47 - installed if you have python 3.3 and
11:49 - above
11:51 - if you have a lower version of python
11:54 - you might need to install the VNS
11:57 - manually and if you're on Windows you
12:00 - may need to
12:02 - install it manually as well so to do
12:05 - that you just pip install virtual ends
12:09 - if you're on Windows
12:11 - and I'll do that right now
12:14 - pip install virtual ends
12:16 - and that will go ahead and install
12:19 - virtual end for you
12:21 - if you're on Mac you don't need to do
12:25 - this
12:26 - or if you're on Ubuntu you more than
12:28 - likely won't need to do this either
12:30 - so we have python installed we have Pip
12:34 - installed we have virtual lens rvm
12:37 - installed so the next thing we can go
12:39 - ahead and do is actually create our
12:41 - virtual environment
12:43 - so a virtual environment is just think
12:46 - of it as a folder that sits on top of
12:49 - python where you can add all these
12:51 - third-party libraries and modules and
12:54 - they'll only be specific to the project
12:57 - you're currently running because what
12:59 - can happen is if you've got multiple
13:01 - python projects you can also often have
13:03 - multiple of the same packages but
13:06 - different versions to run your code and
13:09 - you don't want if you for example
13:11 - upgrade some third-party package that it
13:14 - breaks one of your other projects
13:16 - because one of your other projects
13:17 - needed an older version of that
13:20 - third-party package so by using virtual
13:22 - environments it just means that each
13:24 - project you have
13:25 - the third party libraries you installed
13:27 - are specific to that project
13:30 - so let's go ahead now and we'll just do
13:33 - python
13:35 - minus n
13:39 - and we're going to call the folder that
13:43 - we want VM also
13:45 - I just want to make sure I'm in the
13:48 - correct
13:50 - um folder so I've just made a part two
13:52 - folder with nothing in it as you can see
13:55 - and I'm just going to
13:58 - to thought no minus m
14:01 - v n f the end
14:05 - so that's gone ahead and let's create
14:07 - this VN folder with
14:10 - these
14:12 - items in it here and it's installed
14:15 - correctly
14:18 - if you're on Windows you're just going
14:19 - to be using the virtual end command
14:22 - instead
14:24 - okay so now that we have our virtual
14:26 - environment installed
14:28 - and you can see it here we want to just
14:31 - activate it so that any third-party
14:33 - package we installed after this will
14:35 - also be installed into this VN folder so
14:38 - to do that we just type in source
14:42 - and then VM bin activate
14:46 - so then you can see it's activated
14:49 - because we have the folder name VNV in
14:53 - Brackets
14:54 - here
14:55 - so that means anything we installed from
14:58 - now on using the package installer pip
15:00 - will be installed into this folder and
15:03 - be specific only to this project
15:06 - so we can go ahead now and install
15:08 - scrapey
15:10 - so we just do pip
15:13 - install
15:15 - scrapey
15:16 - and you can also get this command from
15:19 - the scrapey website itself as you can
15:23 - see pip install scrapey will install the
15:25 - latest version of scrapey 2.7.1
15:30 - so I'm just going ahead and hit enter
15:34 - and as you can see it's downloading
15:37 - everything it needs for scrape B to run
15:41 - so depending on your connection and your
15:43 - computer it can take a minute or two
15:47 - okay
15:48 - so that's installed correctly as far as
15:51 - I know
15:52 - check it's installed correctly we can
15:54 - just run
15:55 - scrapey
15:57 - and
15:58 - it should give us a list of commands so
16:02 - if you see this output here where it's
16:05 - lists the available commands you know
16:07 - that scrapey is installed correctly as
16:10 - you can see from this line here scrapey
16:12 - has detected that there's no scrapey
16:14 - project created yet so it just says no
16:16 - active project
16:18 - so that's going to be the next step in
16:20 - part three is setting up our scrippy
16:22 - project
16:23 - so let's get going into part three
16:32 - so in part three we're going to be
16:33 - looking at how to create a scrappy
16:36 - project using scrapey
16:38 - then we're going to have an overview
16:40 - look at the project files that are
16:43 - generated when you create a new project
16:46 - and then after that we're going to go
16:48 - into detail on all the different parts
16:52 - of a scrapey project so that entails
16:55 - scrapey spiders items item pipelines
16:59 - it's created middlewares and settings
17:02 - so part three is really going to be a
17:04 - kind of a theory heavy part of this
17:07 - course so if you already know a bit of
17:10 - python this is probably going to be a
17:12 - lot more interesting than if you don't
17:14 - so you can feel free to dip around and
17:17 - and have a look at what parts of this
17:20 - would be most interesting to you we also
17:22 - have an article that goes along with
17:24 - this that might be a bit easier to
17:25 - digest
17:27 - so let's get going and create our
17:30 - project so to do that I've got a folder
17:34 - here part three and inside that folder
17:37 - I've got just the full project we're
17:39 - going to go through in a second and I've
17:41 - got my virtual environment that I've
17:44 - already activated
17:45 - so you guys if you've followed on from
17:48 - part two you should have already
17:50 - activated your virtual environment and
17:53 - you should already have scrapey
17:54 - installed
17:56 - so if you don't have that done just hop
17:58 - back to part two and make sure scrape is
18:01 - installed and your virtual environment
18:03 - is activated okay so now we can go ahead
18:08 - and
18:09 - use the scrapey start project command to
18:14 - create our new project
18:16 - so it's just simply scrapey space start
18:19 - project space and then the name of your
18:22 - project we're going to call this one
18:24 - book scraper because we're going to be
18:26 - scraping a site with books in it
18:29 - so if I hit enter
18:33 - it's gone ahead and created a new folder
18:35 - here book scraper you can see and if I
18:39 - do an LS you can see book scraper is
18:41 - there as well if
18:43 - and if we go into
18:46 - book scraper itself we can see we have
18:51 - book scraper and scrapey.cfg so I'm just
18:54 - going to open up
18:56 - the folder here and we can see inside of
19:01 - that we have several different files and
19:04 - folders so first off we have our spiders
19:06 - folder
19:08 - that at the moment has no spiders in it
19:11 - but we'll be doing that in part four
19:13 - we'll be generating spiders that go in
19:15 - there then we have items middlewares
19:18 - pipelines and settings
19:21 - so your basic scrapery project will
19:25 - contain these parts now you don't have
19:27 - to use items you don't have to create
19:30 - middlewares you don't have to touch
19:32 - Pipelines
19:33 - but you will always have a spider
19:36 - so you can think of items middlewares
19:38 - and pipelines are optional but we will
19:40 - be using them because
19:43 - if you're scraping anything more than
19:45 - just one page it becomes a lot easier
19:48 - just to use the pipelines items in
19:52 - middlewares and instead of trying to
19:55 - have everything custom made in a spider
19:59 - Okay so the next thing we're going to
20:02 - quickly look at is a fully
20:05 - fleshed out spider
20:08 - so just to give you an idea of what
20:10 - would go into these things like what is
20:12 - in items what does middlewares mean what
20:15 - we put in pipelines that's going to go
20:17 - through some code and give you an
20:20 - example
20:21 - so don't be too scared if you don't
20:23 - understand any of this stuff right now
20:25 - we're going to be going through all of
20:26 - it in Parts four five six seven eight
20:29 - so we start off with our spider
20:33 - so in here in our spiders folder there's
20:36 - just a simple spider called book Spider
20:39 - it's just a simple class it's got a name
20:43 - it's got some functions there
20:47 - and inside it has things like items
20:52 - which link into our items.py file here
20:56 - as we can see we're importing it
20:59 - and this is just a basic spider so I'll
21:04 - just give you a quick overview of what
21:05 - it does once you run the spider it goes
21:08 - to start requests and it puts this URL
21:11 - into this URL variable and then it
21:15 - returns scraping that request function
21:18 - with the URL and once the request comes
21:22 - back from the page with the HTML in it
21:24 - it goes to the next function which is to
21:29 - find in
21:30 - the parse function
21:32 - so this parse function then
21:36 - lets us use the response that contains
21:39 - the HTML and we can then manipulate this
21:43 - HTML and extract the things that we want
21:47 - such as the title category description
21:49 - price once we've got those those pieces
21:53 - of data are put into our book item and
21:56 - that is then returned to us in the
22:00 - console are if you've got other things
22:04 - set up such as feeds into a file
22:08 - so that might all be completely
22:11 - overwhelming for you but don't worry
22:13 - we're going to be going through all of
22:15 - this in extreme detail and showing you
22:18 - exactly how to do everything that is
22:20 - already here this is just to give you an
22:22 - idea of what's what
22:24 - Okay so
22:26 - you might have seen this book item that
22:28 - I mentioned so book items
22:31 - then links into our items.py file and in
22:35 - that file we just describe
22:37 - how we want the item to be set up so we
22:42 - want our book should contain title
22:44 - category description and price
22:47 - so then using this we can then use this
22:50 - book item both in our spiders when we
22:54 - fill the book item with the different
22:56 - pieces of data and return it and also in
22:59 - our Pipelines
23:01 - so in our Pipelines
23:05 - we have a simple
23:06 - test one set up here which goes through
23:09 - mimicking how you would
23:11 - then get the data that is returned
23:15 - um in the book Spider
23:17 - this book item with all the details and
23:20 - it goes through how it would save the
23:24 - item in a database so think of it we
23:28 - extract the data The Next Step would be
23:31 - to push the data into the item and then
23:35 - to put the item into a database
23:38 - so Pipelines
23:40 - are what happens once you've extracted
23:43 - and you're yielding returning the data
23:46 - from your spider
23:49 - so here we have for example process item
23:51 - it's fairly self-explanatory it takes
23:54 - the item
23:55 - with the title the category description
23:58 - and it inserts it into our books
24:01 - database table
24:04 - so that's what
24:07 - gets put into items and item Pipelines
24:11 - again this could be all very confusing
24:13 - for you if you know a bit of python
24:15 - hopefully it shouldn't be too confusing
24:18 - but we'll be going into it in a lot more
24:20 - detail later on okay so
24:24 - then we have our middlewares
24:27 - so
24:28 - middlewares are where you can
24:32 - get into the nitty-gritty of
24:34 - how you want the spider to operate
24:38 - so it gives you control over lots of
24:41 - different things such as
24:43 - timing out requests how long you want
24:46 - the request to go on for what headers
24:49 - you want to send when you make a request
24:51 - what user agents should be used when you
24:55 - make a request if you want to do things
24:57 - like multiple retries you can mess
24:59 - around with that in the middlewares
25:01 - section and as you can see it comes with
25:05 - several kind of defaults that are there
25:09 - that you can either update to what you
25:12 - want or you can create your own ones
25:14 - that go in here too
25:16 - so you also have managing cookies
25:19 - caches there's everything like that
25:22 - would be dealt with in your middlewares
25:25 - now there's two types of middlewares
25:28 - there is downloader middlewares and
25:30 - spider middlewares
25:31 - most of what we'd be doing would
25:34 - probably go into the downloader
25:35 - middlewares
25:37 - but spider middlewares can also do
25:39 - things such as adding or removing
25:42 - requests or items handling different
25:47 - exceptions that crop up if there's an
25:50 - error with your spider handling things
25:52 - like that
25:53 - so all these middlewares go in the
25:56 - middlewares.py file and then last of all
25:59 - we have our settings
26:01 - so settings is fairly self-explanatory
26:03 - it's where you
26:05 - put all your settings so you've got
26:07 - basic things like do we obey our
26:10 - robots.txt file when initial request is
26:14 - made to a website do we check that first
26:16 - and if it says don't scrape this site do
26:19 - we obey that yes or no it said here the
26:22 - number of concurrent requests we make so
26:25 - if we're scraping a website do we send
26:27 - one request at a time or do we send 10
26:30 - or 100 requests at a time that's also
26:32 - set here
26:34 - so everything to do with how your spider
26:37 - and crawling operates will be either
26:41 - enabled or disabled in this settings.py
26:45 - file now we also have going back to what
26:48 - we were talking about our middlewares we
26:50 - have our spider middlewares
26:53 - as you can see here and our downloader
26:55 - middlewares as you can see here so this
26:58 - is where you can if you create a new
27:00 - middleware so this one directly links to
27:03 - the book scraper spider middleware that
27:06 - is
27:07 - right here
27:08 - so you need to make sure if you create a
27:11 - new middleware that you then enable it
27:13 - in settings also
27:15 - and also Verizon pipelines that's also
27:19 - where you need to enable if you create a
27:21 - new item pipeline that it is enabled in
27:24 - here also
27:26 - okay so I think we've gone through the
27:30 - basics of a full scrapey project and
27:33 - what's contained in there we've gone
27:35 - through what's usually in a spider gone
27:39 - through items and item pipelines how
27:42 - they can process the data once we've
27:45 - scraped the data from a page and then
27:49 - we've looked at middlewares and how in
27:52 - settings we can turn everything on or
27:54 - off
27:56 - so I think that's everything we wanted
27:58 - to cover in this part now again don't be
28:01 - too overwhelmed by this
28:03 - it does get a lot easier trust me so
28:07 - stick with it and in part four we'll be
28:10 - creating our first spider and extracting
28:14 - some data from a web page
28:20 - in part four of our scrapey beginners
28:22 - course we're going to look at how to
28:25 - create a scrapey spider
28:26 - using the Scrapy shell to find the CSS
28:30 - selectors we need
28:31 - using those CSS selectors in our spider
28:33 - to extract the data we want from the
28:36 - page and then finally we're going to get
28:39 - our spider to go through multiple pages
28:42 - and extract data from multiple pages
28:47 - so let's get going so I've got my
28:50 - terminal open here I've already
28:52 - activated my virtual environment I'm
28:55 - continuing on from part three so if
28:58 - you're just joining us here make sure
28:59 - you already have everything set up as we
29:01 - have done in part three
29:03 - I want to go all the way down into my
29:06 - spider's folder so at the moment it's
29:08 - empty it's just got an init.py file in
29:11 - it so we want to go down the level and
29:16 - down into the spiders so now I'm in my
29:18 - spiders folder and I can see there's
29:21 - just a new py there so in this spider's
29:25 - folder I'm going to run
29:27 - this command scrapey gen spider
29:31 - the name of my spider which I'm going to
29:33 - call book spider and then
29:36 - the URL of the website that we're going
29:39 - to be scraping and in this case it's
29:42 - going to be
29:43 - the books.2 scrape.com site which is a
29:48 - site that is there for people to
29:50 - practice their scraping up
29:53 - so
29:54 - if you go ahead and go down to your
29:56 - spiders folder and type this command
29:59 - into your terminal
30:00 - and hit enter
30:03 - scrapey will then create this spider as
30:06 - you can see here created spider book
30:08 - Spider using template basic in module
30:10 - and then it gives this so if we check
30:14 - that out now we can see that book
30:16 - spider.py is there and if I open this up
30:19 - in my vs code we can see it created
30:24 - book Spider here so
30:27 - this is just a very very basic spider
30:30 - we'll be adding a lot more to this
30:33 - but we'll just go through a few bits of
30:36 - what were generated here so obviously
30:39 - the name of our spider is book Spider so
30:42 - when we do Scrapy crawl to actually kick
30:45 - the spider off using scrapey we'll be
30:48 - doing scrapey crawl book Spider
30:51 - the allow domains list
30:54 - is books.2script.com
30:57 - this is important because later on when
31:00 - we're going to be doing crawling our
31:02 - spider is going to be going through
31:03 - multiple different links
31:05 - and having this allowed domains here
31:08 - listing only the domain we want to
31:10 - scrape prevents our spider from going
31:13 - off and scraping hundreds of different
31:16 - websites across the internet because as
31:18 - you can imagine URLs link from one page
31:20 - to another and sometimes a website might
31:22 - link to an outside website
31:25 - and in this case you would not want your
31:27 - spider to start crawling and scraping
31:29 - the entire internet
31:31 - so that's why we have allowed domains
31:33 - here next we have start URLs so this is
31:36 - usually just the first URL that the
31:40 - spider starts scraping but you can
31:43 - actually have multiple URLs here as well
31:45 - for it to go through one after the other
31:48 - then we have our parse function our
31:50 - parse function is
31:52 - the function that gets called once the
31:55 - response comes back so we'll be filling
31:58 - this parse function with all the
32:00 - different pieces we want to extract the
32:03 - data from the page itself
32:06 - Okay so we've gone through the basics of
32:10 - this generated spider the next thing
32:12 - we're going to do is we're going to use
32:13 - the scrapey shell
32:15 - to find the CSS selectors
32:19 - we want to get the data from the page
32:23 - so what I mean by CSS selectors for
32:26 - those of you who aren't familiar so
32:29 - first of all if you just open your
32:31 - developer tools
32:33 - you can do this by right clicking in
32:36 - Chrome or Safari or Firefox and it's
32:39 - usually inspect or sometimes it's called
32:40 - developer tools
32:42 - so you do that and this comes up here if
32:45 - you go to the elements tab you'll see
32:47 - then all the makeup of the page in HTML
32:52 - and CSS
32:54 - so here for example we've got a H3 tag
32:57 - and an a tag for links and this is the
33:00 - link to the page of this book here
33:05 - so we'll be looking now at how we can
33:09 - actually pick out these tags so that
33:12 - scrapey knows which pieces of data we
33:15 - want to extract from the page itself
33:17 - okay so let's just go back to our
33:20 - terminal and just to make using the
33:23 - Scrapy shell a little bit easier we're
33:26 - going to do
33:27 - just a pip install and then I python
33:32 - which just is a different shell would
33:37 - help if I spelled it correctly this is
33:40 - just a different shell which is a bit
33:43 - easier to read so I just did pip install
33:47 - IPython and then to activate this we
33:50 - want to go to settings
33:52 - and
33:55 - we I know we want to go to sorry not
33:57 - settings this
33:59 - scrapey.cfg
34:00 - and we're going to add
34:04 - the shell as a separate line here so
34:10 - now that that's done I can close Dash
34:14 - and we can run scrapey shell
34:17 - Scrappy shell then gives us this so what
34:23 - we want to do now we have scrape your
34:24 - shell open is as you can see we've got a
34:27 - list of the commands that it gives us
34:29 - that are available so we can do
34:32 - useful shortcuts fetch which is the
34:35 - command that we're going to be using
34:37 - so this fetches a URL and updates
34:41 - the
34:42 - local objects
34:44 - so we'll just run this fetch command now
34:49 - and I'll show you exactly what I mean
34:53 - so we want to fetch this books to
34:55 - scrape.com
34:57 - so I'm just going to paste in
34:59 - this URL here and
35:03 - it's going to go off it's going to fetch
35:06 - this and it's going to put the resulting
35:09 - HTML everything in here that we see
35:14 - into a variable inside in the scrapey
35:18 - Shell so we can access it and run
35:21 - different commands on it
35:23 - so enables us to kind of practice the
35:26 - code we want to then put into our spider
35:29 - so what we want to do is it put
35:32 - everything in from the page into this
35:34 - response
35:35 - variables so now we can just do
35:37 - response.css and
35:41 - let's say we're going to look for
35:44 - something specific on the page so what
35:47 - we can do is move our Mouse over these
35:49 - different
35:51 - tags on the page we can see article
35:54 - contains
35:56 - one
35:57 - book on the page so we can just say okay
36:00 - give me article
36:05 - and we'll just have the class name in as
36:09 - well so any class name
36:12 - needs to have just dot in front of it
36:14 - when we're referring to it like this
36:19 - so that has given us
36:21 - all the different books that are on the
36:24 - page
36:25 - now let's say we want to just get the
36:27 - first book we can just do this
36:29 - with a guess and it's giving us just the
36:33 - HTML that is
36:35 - for that first book if we want to then
36:39 - put all the books into a different
36:42 - variable so we can run some other
36:44 - commands on them within the scrapey
36:46 - shell we can do something like
36:48 - books is equal to
36:51 - response.cssarticle dot product pod
36:55 - if we do that it's after putting all the
36:59 - different books into this books variable
37:02 - so then if we run Len
37:06 - on books then in Python gets us the
37:09 - length
37:11 - so it gives us that there's 20 books if
37:13 - we go back to our page
37:15 - we can indeed see there is 20 books so
37:18 - there's four in each Road there's one
37:20 - two three four five rows showing one of
37:24 - 20 so that's correct
37:27 - Okay so
37:28 - for the purposes of this part four we're
37:31 - going to extract the name of the book
37:33 - the price of the book and the URL so we
37:38 - can actually go in and get further
37:40 - details later
37:41 - so the name the price and the URL
37:45 - so now that we've got our books what we
37:48 - can do is
37:50 - we can put the first book
37:55 - of the list of books
37:58 - so we'll make a new variable called book
38:00 - and we'll say that's equal to
38:04 - equal to books
38:06 - and the first item in the list of books
38:10 - that we have
38:11 - so now if I do
38:15 - book.css
38:17 - and then
38:19 - I go back and okay we want to get the
38:23 - the title of the book here so I can see
38:27 - from this that we've got a H3 tag and
38:30 - we've got an A tag and I want the text
38:33 - that is within this a tag here so I'm
38:37 - going to do H3 and a
38:40 - and that should get me the text that I'm
38:44 - looking for the title of the book
38:47 - so go back here
38:49 - and I do H3 a and then we just need a
38:54 - little bit extra which is just this text
38:57 - and I do get
38:59 - that gets me exactly what I was looking
39:02 - for which is a light in the dot dot dot
39:05 - this corresponds exactly to this so I've
39:08 - got the title of my book
39:11 - next I want to get the price of my book
39:15 - so I'm going to just remove these two
39:18 - pieces here
39:20 - and I'm going to inspect the price
39:23 - okay so if we look at dot product price
39:27 - and then dot price underscore color
39:30 - should give us the price so let's do
39:34 - that now so
39:36 - that's
39:38 - Dash product price
39:43 - color let's try and run that
39:47 - okay it didn't
39:50 - exactly what I I know because I S I did
39:52 - an extra double dot there you go so that
39:55 - got us exactly the price we're looking
39:58 - for and finally we want to get the URL
40:02 - so the URL
40:04 - is
40:06 - interesting because it's also part of
40:08 - this h3a tag but we want instead of the
40:12 - text within it we want this href
40:14 - attribute here which contains the part
40:18 - of the link to more information on the
40:21 - actual
40:22 - book itself so if we open this up in a
40:25 - new tab we'll see the full page that
40:28 - we're looking for and here you can see
40:32 - full project description and lots more
40:34 - details there
40:36 - Okay so
40:38 - we still want to do
40:41 - H3 and a
40:44 - but instead of text we're going to say
40:49 - a
40:52 - a trib
40:54 - e hatred that gives us our href
40:58 - attributes
41:00 - that was contained in this a tag that we
41:04 - were looking at a second ago so using
41:07 - the Scrapy shell we've managed to see
41:08 - how we can use the CSS selectors to
41:12 - extract the title the price and the URL
41:16 - for one book
41:18 - so now that we know that we can add
41:21 - these into our parse function
41:23 - and we can also Loop through all that
41:26 - list of books and get all the details
41:28 - for the 20 books that are on the page
41:30 - Okay so
41:32 - let's start adding things to our parse
41:35 - function so first I'm just going to add
41:38 - in what we initially had to get all the
41:41 - books
41:43 - that were there and that is
41:46 - books equals to response dot CSS article
41:50 - and then product underscore pod so we
41:54 - had that
41:55 - up
41:58 - here
42:00 - so I'm just taking this line here that
42:02 - we used in our Scrapy shell and I'm
42:04 - putting it in to our parse
42:07 - function okay the next thing we want to
42:10 - do is we're just going to Loop through
42:12 - it so we just want
42:14 - four book in books
42:18 - and then we are going to type yield
42:23 - so yield is like return
42:27 - and then what we want is scrapey to
42:30 - return to us is going to be the name the
42:32 - price and URL
42:34 - so we'll start with the name and then
42:36 - we're going to go up to where we
42:40 - got our text and we're going to use this
42:42 - exact
42:44 - piece here
42:45 - and then we're going to get our price
42:50 - and we're going to go to where we got
42:53 - our price
42:59 - and then
43:01 - last of all our URL
43:05 - and for that we have our
43:09 - href attributes
43:12 - okay
43:13 - now that we have that
43:15 - we should be able to go ahead and run
43:18 - our spider and see what happens so first
43:21 - let's exit our scrapey Shell by typing
43:24 - exit
43:25 - and then we might need to go up a level
43:30 - to our
43:32 - book scraper folder and we should be
43:34 - able to run
43:36 - scrapey crawl
43:38 - book Spider which is the name of our
43:40 - spider
43:42 - so
43:43 - if that goes according to plan
43:47 - we should see item script count of 20
43:49 - there are the 20 books on the page and
43:53 - you can see what was returned here the
43:56 - name there is a book name a price there
43:59 - is a price and URL there's the URL and
44:04 - if we just scroll up we can see that the
44:06 - 20 books that were on the page all the
44:09 - data was script and output to our
44:12 - terminal so that worked exactly how we
44:14 - wanted it to work
44:16 - now as you've seen we have multiple
44:18 - Pages there's not just this one page of
44:22 - 20 books there is actually a lot more
44:24 - than that so we're going to look at how
44:26 - we can go to the next page if there is
44:28 - one
44:29 - and then scrape all the books on the
44:31 - next page and then keep looping through
44:34 - all the pages of books until there are
44:36 - no more pages of books to scrape
44:40 - so
44:43 - as you can see here we have a next page
44:47 - at the bottom of every
44:49 - page of books so if we click the next
44:51 - page button it goes to catalog
44:54 - page2.html and then we have a new page
44:58 - of 20 different books and as you can see
45:02 - it's going through all the different
45:04 - pages page three and there's a previous
45:06 - there as well to go back a page so
45:10 - we're going to want
45:12 - scrapey to bring us to page Dash three
45:17 - our
45:19 - page-4.html if there is another one to
45:22 - scrape
45:23 - so we're going to go back and we're
45:27 - going to do scrape your shell again
45:32 - to open our shell we're going to again
45:35 - roll in our fetch command
45:39 - to fetch
45:41 - our website URL and then we are going to
45:46 - try and get the link so to do that we're
45:51 - going to inspect
45:52 - the next button and as we can see here
45:55 - it's in An Li tag and then it's got a
46:00 - class name of next and then within that
46:03 - we want the
46:05 - link which is contained in this href
46:09 - attribute and that's contained in an a
46:12 - tag for links
46:14 - so let's see if we can get that now
46:18 - using our scripture
46:21 - so we do response.css and then
46:26 - allies dot next so Ally for the Ally tag
46:31 - dot next for the class name and then a
46:34 - and then we want the href attribute
46:39 - so
46:40 - so that c can me to Dash
46:45 - and that gives us exactly our catalog
46:48 - forward slash page 2.html which
46:52 - corresponds to well this was not the
46:56 - exact one we're looking at we're looking
46:58 - at page one
47:00 - so I can just remove that
47:05 - and go down here and this one should
47:09 - have catalog for slash page 2.html
47:13 - and that corresponds to this
47:17 - so now that we know how we can get the
47:21 - next page we can just
47:24 - put in under our Loop
47:27 - we're just going to paste what we had
47:29 - here to get our
47:31 - link
47:33 - and we're just going to do next
47:36 - page is equal to
47:38 - and this is going to contain our next
47:41 - page link so
47:44 - the next thing we need to check for is
47:46 - if we get to the last page
47:48 - there's going to be no more next page
47:49 - link so that's how we can know
47:53 - when we've reached the end
47:56 - so we can check that by
47:59 - going to page 50.
48:02 - so if I
48:04 - type in page
48:07 - 50 and
48:10 - go to the bottom I should see that there
48:13 - is no more next button there's a
48:16 - previous button but there's no next
48:17 - button so I've reached the end so that's
48:20 - what our test is going to be we're going
48:22 - to put in an if statement and we're
48:24 - going to say if the next page URL is not
48:27 - none
48:28 - then we know there's another page so we
48:31 - can continue going until there is no
48:34 - more pages left to scrape
48:36 - so let's add that in now okay we're just
48:39 - going to do if
48:40 - next page is not none
48:45 - and then
48:46 - next page
48:49 - URL is equal to
48:52 - and here we're going to create the full
48:56 - URL because next page doesn't contain
48:58 - the full URL it's only
49:00 - a relative URL
49:02 - so we need to get
49:05 - this part of the URL plus the catalog
49:09 - forward slash whatever the next part of
49:12 - the pages
49:15 - so
49:16 - let's add that in
49:19 - save that
49:21 - and then the important part is we need
49:27 - to do
49:28 - yield
49:30 - response dot follow
49:37 - next page URL
49:39 - and then call back
49:43 - is equal to self Dot
49:46 - parse
49:48 - okay
49:49 - so what this does is it obviously
49:51 - creates our next page URL
49:53 - and then we tell scrapey to
49:57 - go to this next page URL using
49:59 - response.follow and the Callback is the
50:04 - function that's going to get executed
50:06 - once the response comes back from the
50:10 - URL that we've gone to
50:12 - so once we get the response from that
50:14 - URL it's going to kick off self.parse
50:17 - and self.parse is this function again so
50:21 - it's going to keep going through and
50:23 - keep going through and keep going
50:24 - through and calling itself until there
50:26 - is no more pages and then in that case
50:30 - it's going to stop
50:32 - let's try and run that now so let's exit
50:36 - out of our scrapey shell and let's just
50:39 - do a scrapey crawl again
50:42 - scrape your crawl and book Spider
50:48 - and see what we get
50:52 - okay
50:53 - so with an item script count of 40
50:57 - so it scraped four pages but obviously
50:59 - four pages is not 50 pages
51:02 - so there's a bit of a bug here which
51:04 - we're going to have to get to the bottom
51:06 - of let's start looking at the next page
51:10 - URL because that's obviously where it's
51:12 - going wrong if it's only finding four
51:15 - pages there must be an issue with the
51:17 - URL here so if we go back and we again
51:21 - inspect the element so here we can see
51:24 - it's page-50.html
51:27 - and
51:29 - I think we had
51:32 - and here it's 49 but then if we were on
51:36 - the initial page
51:39 - and we check here it's got catalog
51:42 - forward slash page two
51:44 - so sometimes it's got just page Dash 2
51:47 - and sometimes it's got the catalog in t
51:50 - href so that's obviously why it only
51:54 - scraped
51:56 - four pages so the fifth page only has
51:58 - page five so we're gonna have to just
52:01 - modify our next page and if statement
52:05 - here
52:06 - just to check that we have catalog in
52:10 - the href if we do then we do a slightly
52:13 - different next page URL than if we don't
52:18 - so let's just add that in now so if the
52:21 - catalog
52:22 - forward slash is in next page
52:25 - then the next page URL is going to just
52:29 - be what we currently have
52:32 - and but if it's not we want an else
52:37 - and we're going to say
52:40 - add in
52:42 - the next page URL
52:45 - but we're going to add in catalog here
52:51 - so this should ensure that the next page
52:55 - URL is correct so if it contains catalog
52:57 - we don't need to have catalog
53:00 - in this part if it doesn't contain
53:02 - catalog we do have to have it and then
53:04 - that should make the correct URL
53:07 - so hopefully that's fixed that bug so
53:10 - let's try run the scripty crawl one more
53:13 - time
53:18 - okay so this seems to be going through a
53:20 - lot more pages
53:21 - which is a good sign
53:24 - and let's give it another
53:27 - minute or two to finish up
53:29 - and we can see the total item counts at
53:33 - the end and the total page of script so
53:37 - this is kind of the process that you
53:40 - have to do when you are creating a
53:41 - spider to scrape data there'll be small
53:44 - bugs like this that pop up and you need
53:46 - to do a small bit of detective work to
53:48 - find out why your spider is failing at
53:52 - certain parts or not able to
53:54 - extract certain pieces of data from the
53:57 - page
53:58 - so we can see here we have
54:02 - response received count to 51
54:05 - I sub scraped count of a thousand
54:08 - so
54:09 - if we go back
54:11 - there is a thousand results so it
54:13 - scraped all the books that we were
54:15 - looking for that's pretty much
54:18 - everything we wanted to cover in this
54:20 - part four in part five we're going to go
54:23 - through and each book we're going to
54:26 - click into the book and then we're going
54:28 - to extract more product data from the
54:34 - product page itself
54:36 - so right now we're just kind of doing
54:38 - the easy thing of just going through one
54:42 - page one page one page just the name and
54:44 - extracting just the name the URL and the
54:47 - price but in real life scenarios most of
54:52 - the time we want to get a lot more data
54:55 - and that involves doing things like
54:57 - going in clicking into the actual
54:59 - product we're looking at and actually
55:00 - getting more in-depth data
55:04 - so we'll be looking at how we can do
55:06 - that
55:06 - in part five
55:12 - so in part five we're going to look at
55:15 - how to crawl Pages using the scrapey
55:18 - spider
55:19 - using CSS selectors and expats to
55:21 - extract more complicated pieces of data
55:25 - from Pages such as from tables
55:27 - from breadcrumbs things like that and
55:30 - then we're going to move on to saving
55:32 - the data into certain file formats such
55:35 - as CSV or Json format so let's get
55:39 - started
55:40 - we're continuing on from part four so if
55:43 - you need to get the code for that we'll
55:46 - have that available for to for you to
55:48 - download and follow on at this point or
55:51 - if you've got your part 4 already
55:52 - completed you can just continue on from
55:54 - there
55:55 - so in part four we just ran our spider
55:58 - and it went through and got us the
56:00 - details of the Thousand books that are
56:03 - on the books to scrape.com site
56:07 - so the next thing we're going to be
56:08 - doing is we're going to be instead of
56:10 - just scraping the
56:12 - URL the price and the name of the book
56:15 - we're actually going to be going into
56:16 - the book page itself and we're going to
56:19 - be taking things such as the rating
56:22 - the product description the product type
56:24 - the price excluding that including that
56:29 - the category that it's in such as poetry
56:32 - in this case
56:34 - so we're going to be looping through all
56:36 - the different books looping through
56:38 - every single page and getting all the
56:41 - specific data for each book that is on
56:44 - this site
56:45 - okay
56:47 - so let's go back to our Spyder code
56:52 - and the first thing we're going to look
56:54 - at doing is we want to start going into
56:58 - each page individually and so to do that
57:02 - we're going to do something that's very
57:05 - similar to going to each next page URL
57:08 - so we'll copy this code from the bottom
57:12 - and we're going to do this for every
57:16 - book in the list of books so instead of
57:19 - just yielding the data we're going to be
57:22 - going into the URL so we're just going
57:24 - to paste that in over our yield
57:27 - right here
57:29 - we're going to remove the
57:31 - next page non-section
57:34 - we're going to move our next page URL
57:38 - Pier
57:40 - and we're going to have to just modify
57:43 - where we get the URL for the each
57:48 - individual book so if we just go back
57:53 - into inspect
57:55 - the elements again and check out okay so
57:59 - that was H3 and a and we want the href
58:03 - for that H3 a tag
58:06 - so we just want h3a attribute href that
58:12 - should get us
58:13 - the next page
58:16 - and then it should create the correct
58:19 - URL for us
58:22 - instead let's call of next page because
58:25 - it's not the next page it's the relative
58:28 - URL of the book lets us call it relative
58:30 - URL
58:34 - and let's just
58:38 - put this
58:40 - here
58:42 - and
58:44 - the next thing we want to do is
58:48 - we want the Callback function instead of
58:52 - being parse we're going to do parse book
58:56 - page
58:59 - so we're going to make a new function to
59:02 - parse the book page individually one by
59:04 - one
59:06 - so let's go down here
59:08 - let's do Def
59:11 - passbook page
59:15 - self and response
59:18 - and
59:20 - at the moment let's just put pass in
59:22 - there so fat is going to
59:27 - Loop through
59:30 - instead of next page URL I'm going to
59:34 - call this one
59:35 - book URL
59:39 - and
59:42 - Okay so
59:45 - the only other thing to change that is
59:47 - incorrect is that obviously
59:49 - we need to Loop through the list of
59:50 - books so this book needs to be used and
59:54 - we're going to get the book.css and
59:56 - that's where we're going to get our
59:58 - link from with the
60:02 - this link here
60:04 - so that goes to relative URL and
60:07 - variable
60:09 - then we make the correct book URL and
60:12 - then using this book URL we yield
60:16 - um
60:17 - so we basically go into this
60:20 - um book URL and then the response HTML
60:24 - that comes back from this URL will get
60:26 - parsed by the parse underscore book
60:29 - underscore page function which is the
60:31 - one we made down here okay so I think we
60:35 - can save that for now
60:39 - and next thing we want to do is start
60:42 - flushing out our parse book page so what
60:45 - we're going to do first is we're going
60:47 - to open up our scrapey shell again like
60:50 - we did in part four and we're going to
60:53 - look at the different CSS selectors and
60:56 - expat selectors for the different items
60:59 - that we want to scrape on the book page
61:02 - itself so let's click into one of the
61:03 - books and we're going to see what if we
61:07 - want to extract from this page
61:09 - let's go back open up
61:11 - our scrapey shell in our terminal again
61:16 - so just Scrapy shell
61:19 - and when that opens we're just going to
61:22 - use our fetch function again to fetch
61:25 - the full URL from one of the book pages
61:29 - which in this case is just the very
61:31 - first book I've picked
61:32 - in the list
61:35 - so
61:36 - I can just put in the URL in here hit
61:40 - enter that's going to go off get the
61:43 - HTML of that page and stick it in the
61:46 - response variable
61:48 - so just like we did in part 4 we can
61:52 - see what works and what doesn't work
61:54 - with our CSS selectors
61:56 - so let's just do
61:58 - response dot CSS
62:01 - and then let's inspect the page again
62:03 - and just
62:05 - okay so we have
62:08 - product description there is an ID
62:12 - and
62:14 - there is a P tag underneath that
62:17 - so product pay underscore page there so
62:20 - that gives us the whole page so let's
62:22 - just try and see what happens if we do
62:27 - product on the score page
62:29 - that seems to give us back the whole
62:32 - page
62:34 - now let's look at getting the title of
62:38 - the book
62:39 - for example so in this case
62:41 - on this page
62:44 - it's in the product underscore Main and
62:48 - it's H1
62:49 - so let's go ahead and just do
62:55 - so Dosh
62:58 - product underscore Main
63:00 - H1
63:02 - text
63:05 - and there we have a Light in the Attic
63:07 - which matches to our Title Here
63:10 - so that's very simple just as we've done
63:12 - before
63:14 - so now let's
63:15 - do something a little more complex let's
63:18 - get the category up here so we have
63:21 - poetry in this case
63:23 - so for things that are a little bit more
63:25 - complicated like this sometimes it can
63:27 - be easier just to use expats instead of
63:30 - CSS selectors so expats are very similar
63:33 - but instead of using class names
63:35 - directly the format of how we write
63:40 - the expats is just a little different to
63:42 - how we would use
63:44 - CSS selectors so I've got one
63:47 - pre-written out which I'll just paste in
63:49 - here
63:50 - so
63:52 - paste in my XPath and that gives me
63:55 - poetry so I'll just explain to you how
63:57 - this gosh poetry
64:00 - so it went to the UL HTML tag
64:04 - it's
64:05 - put in the class breadcrumb
64:08 - so if we go back to the top here
64:12 - we should see it's in a UL
64:14 - HTML tag and the class is breadcrumb and
64:18 - then we have several Li tags and then we
64:21 - have an a tag within the LI tag and we
64:25 - have the href
64:27 - so here we can see that's where tli tag
64:31 - comes into it
64:32 - and then
64:34 - the active class
64:36 - is on the grayed out section here
64:41 - so
64:42 - it's then going from it's going to here
64:45 - and then it's saying preceding sibling
64:48 - so get me the preceding Li tag before
64:52 - the one that has the active equals class
64:55 - in it so it's going to here and then
64:57 - it's going back one to the preceding
64:59 - sibling and it's getting the text within
65:03 - here
65:04 - and it's doing that with
65:07 - the text at the end here so as you can
65:10 - see proceeding sibling Li
65:13 - one
65:14 - and then a and text at the end
65:18 - so expats are quite similar
65:21 - to CSS selectors in not every case will
65:23 - you have
65:24 - a class name or an ID tag on a HTML tag
65:29 - so in the case of the product
65:31 - description which I showed you a second
65:33 - ago we'll just look at it again there is
65:35 - no class name or no CSS ID on on this P
65:40 - for paragraph tag here so in that case
65:43 - as well we can say right go to the
65:46 - product description using the expats and
65:50 - then get me the following sibling that's
65:52 - a P tag
65:54 - so I can just show you that one so it's
65:58 - go to the product description ID
66:01 - get me the following sibling with the P
66:05 - tag and then within that get me the text
66:08 - so that's how expats work for getting
66:12 - some of these Corner cases where you
66:14 - might not have a simple class name or a
66:18 - simple ID on the HTML tag
66:22 - okay so we know how we can get the
66:25 - product description and the category tag
66:29 - up here
66:30 - we know how we can get the price and the
66:32 - title
66:33 - next let's look at extracting data from
66:37 - tables
66:38 - so
66:40 - if we inspect element again we can see
66:43 - that this is all contained within a
66:46 - table and this table has several rows
66:49 - which have TR as the HTML tag and then
66:54 - within the table row we have th and TD
66:59 - and that goes the whole way down so each
67:02 - row has
67:04 - one more th and TD so what we can do is
67:07 - we can specify okay get me all the rows
67:12 - in this table and then we can say okay
67:14 - we know that the product type is always
67:17 - going to be the second row
67:19 - so let's always look for the text that
67:24 - is within the TD of the second row if we
67:28 - wanted for example to see the product
67:29 - type
67:30 - so first we want to get all the table
67:32 - rows so we're going to look at table and
67:35 - then all the TRS that are in that table
67:38 - if so then let's assign that to
67:43 - a table underscore rows in our Scrapy
67:46 - shell so if you just use table
67:47 - underscore rows equals to response dot
67:51 - CSS and then have table space TR within
67:56 - the brackets
67:58 - that will make sure that all those table
68:00 - rows are within table rows we can
68:03 - quickly check the length of that
68:06 - that gives us seven rows and there's one
68:09 - two three four five six seven rows so
68:13 - now we can do something as simple as
68:15 - table underscore rows
68:19 - let's look at what we said the second
68:20 - ago the second one the dot CSS again and
68:24 - we want the TD
68:27 - and we wanted the text within that
68:30 - and I guess again we use get and we get
68:33 - books
68:34 - so again
68:35 - the numbering starts at zero and then
68:40 - the second one is
68:42 - second line is number one and then we
68:45 - look at the T D here
68:47 - the TD and that gets us books
68:51 - so that's how this line corresponds to
68:55 - here in this table
68:58 - so knowing that we can then go ahead and
69:01 - get things like
69:04 - the price excluding tax we can just put
69:08 - in something very similar the next row
69:11 - down
69:12 - and that should give us the price
69:14 - excluding tax
69:17 - so we now can get all the data we need
69:20 - from this table
69:22 - and the only last thing to look at is
69:25 - looking at how we can guess
69:29 - the Stars
69:32 - so if we just look at
69:34 - inspect element we can see
69:38 - we have several Stars here it has icon
69:42 - star icon star
69:44 - and star rating of three
69:47 - so that's where we can see the number of
69:51 - stars it's within this class
69:53 - that they've written three so we need to
69:56 - do something slightly different for this
69:59 - so first off we're going to get
70:01 - the star region
70:03 - so we're going to do
70:05 - response.css then p
70:09 - star rating for the class and then we're
70:12 - going to ask for the attribute of class
70:15 - so this is the attribute the attribute
70:17 - name is class and then it should give us
70:19 - our three that we're looking for
70:23 - so let's just do that now
70:27 - so
70:28 - response
70:31 - dot CSS P star rating attribute class
70:35 - and that gives us star rating of three
70:38 - so using our scrippy shell
70:42 - we've looked at how we can get all the
70:44 - different pieces of data from the page
70:47 - so let's start filling that into our
70:50 - parse book page function here
70:55 - so we can actually get all the book data
70:58 - to be scraped correctly okay so let's
71:01 - just exit
71:03 - out of our Scrappy show
71:06 - and let's first just get the table rows
71:14 - so
71:17 - table rolls are going to be equals to
71:19 - response
71:22 - dot CSS
71:24 - and then we're gonna have
71:27 - what we had up here
71:30 - where you can see table rows is table TR
71:34 - so table TR is a table rows and we can
71:38 - work with that now to fill in the rest
71:41 - of the details so we want to remove pass
71:43 - and then we're just going to do yield
71:47 - and we're going to have our details
71:50 - inside here so let's start off with the
71:55 - URL
71:57 - that's easy because we can just use
72:01 - the
72:03 - response
72:06 - sponsor URL
72:08 - so the URL of the page is contained
72:10 - within this response
72:12 - object then let's get the title
72:17 - so we had Dash up here
72:22 - here
72:24 - so
72:25 - can just copy this directly
72:28 - don't forget to add
72:31 - commas at the end
72:33 - and let's get the product type the price
72:38 - excluding tax including tax the tax
72:41 - so these are all to do with the table
72:45 - so
72:47 - we're going to be doing table row
72:50 - one for the second row and then TD text
72:53 - get for the product type and so on for
72:57 - the price excluding tax and including
72:59 - tax as you can see
73:02 - price excluding tax price including tax
73:04 - they're all one after the other here so
73:06 - all we're doing is incrementing the
73:08 - number here
73:10 - and the tax itself
73:13 - and we might as well add in the
73:16 - availability and the number of reviews
73:19 - as well
73:21 - since that's just a continuation of the
73:25 - same thing
73:28 - so
73:30 - add those two in
73:32 - we'll add in the Stars by
73:35 - doing this so let's copy
73:39 - we have them here
73:41 - and we'll just call that
73:43 - Stars
73:46 - and we can just paste that directly
73:50 - and let's also get the category and the
73:53 - description like we had a second ago
73:57 - so category
74:00 - or with that using the expats so that
74:03 - was this guy here
74:05 - let's just
74:07 - paste Dash let me have to just get it
74:10 - all in the same line
74:12 - perfect and
74:14 - let's get the description
74:19 - okay and we just want
74:23 - this XPath that we were using earlier in
74:26 - our Scrapy shell as well
74:28 - again just making sure that we
74:32 - have it all in the same line and
74:35 - that we add in our commas finally the
74:40 - only thing missing is the price
74:44 - and we can do that by getting the
74:48 - response
74:50 - and then
74:52 - dot CSS P dot price color and then the
74:56 - text from that
74:58 - so the price is up here
75:02 - and that's the class of price color
75:05 - within the P tag
75:08 - so if we save all that that should be
75:11 - everything we need to parse the
75:14 - individual book pages and the thing
75:18 - that's just missing here
75:19 - is the next page which we just need to
75:22 - add back in we deleted that earlier by
75:24 - mistake
75:25 - so that's how we get the next page URL
75:30 - okay so everything else looks correct so
75:34 - just a quick recap
75:36 - the spider is going to kick off go to
75:39 - this start URL
75:41 - the response that comes back the first
75:43 - time around
75:45 - we'll go into this parse function
75:48 - this parse function then we get all the
75:51 - books on the main page
75:52 - so that is starting with all these
75:55 - different books here
75:57 - then the next thing that happens is we
76:00 - get the relative URL and we turn that
76:04 - into the book URL we do that by getting
76:06 - each of these URLs here
76:09 - once we get the first books URL we then
76:13 - go into that book page
76:14 - so what happens is the code basically
76:16 - goes in here it's then
76:20 - goes to the Callback function which is
76:22 - parse book page down here it goes gets
76:27 - all these details here that we specified
76:31 - and then is
76:33 - Loops to the next book on the page
76:35 - because this so it comes back out
76:39 - here and then Loops back up to the start
76:41 - and goes to the second book on the page
76:45 - goes Clicks in gets the data
76:48 - comes back does the third book in the
76:51 - page so Loops through all the books on
76:52 - the pages keeps getting all the data for
76:54 - each book and then it goes to the next
76:57 - page
76:58 - and then once all the pages are done it
77:01 - finishes so if we've done everything
77:04 - correctly we should be able to
77:06 - now run our spider and see does that
77:09 - work
77:10 - so let's try and do a scrapey crawl
77:15 - the thing we're going to do that's
77:16 - slightly different this time is we're
77:19 - going to have the output go to a file so
77:22 - instead of having the output come into
77:24 - just our terminal we're going to also
77:27 - get it to save to a file so we do this
77:30 - by doing minus all uh our Dash o and
77:34 - then we're just going to call this book
77:35 - data and we'll do two
77:39 - um CSV
77:41 - CSV so CSV file formats can be opened in
77:44 - Excel or can be put into you know Google
77:48 - Sheets and different applications like
77:50 - that so it's CS face just stands for
77:52 - comma separated values
77:54 - so if we run that hopefully there's no
77:57 - issues
77:58 - and as you can see there's book data.csv
78:03 - here
78:04 - you can open that and we can see we have
78:08 - nodes of data
78:09 - so all the stuff that we were looking
78:11 - for
78:12 - price description
78:15 - everything else seems to be there
78:18 - so I'm just going to stop that now
78:20 - before it gets the end because that
78:22 - seemed like it's working correctly it
78:24 - was already on page 15 here let's see
78:27 - okay and I'll run that one more time
78:29 - except this time
78:31 - and do it in instead of book data.csv
78:36 - we're going to get her to Output to Json
78:40 - so I'm just going to
78:42 - delete that one and get it to a push to
78:46 - Json format instead
78:49 - Json format can just be a bit easier to
78:51 - read and if you're doing further coding
78:54 - it can be easier to parse as well
78:58 - so if we opening up as you can see it
79:00 - has all the data nicely formatted the
79:05 - title price including tax availability
79:10 - the number of reviews all the data is
79:13 - all there
79:14 - so that's working nicely obviously it's
79:17 - going to take a minute or two to scrape
79:19 - all a thousand books
79:21 - but I think that's everything we wanted
79:23 - to go through in part five
79:25 - in part six we're just going to be
79:27 - looking at how we can use items and item
79:32 - Pipelines
79:33 - to better structure and clean our data
79:37 - before we start saving it into things
79:39 - like a database
79:41 - so it'll just put a bit more structure
79:42 - on our code and it'll enable us through
79:45 - things such as for example we could
79:47 - change the prices from pounds to dollars
79:51 - four gets saved we could you know remove
79:56 - any trailing white space lots of
80:01 - different examples we'll go through in
80:03 - part six of how to clean up the data
80:07 - okay see in part six guys
80:14 - so in part six of the scrappy beginners
80:16 - course we're going to be looking at
80:18 - scrapey items and Scrappy Pipelines
80:21 - so first off we're going to go through
80:24 - what scrapey items are then we're going
80:26 - to use scrapey items to structure our
80:29 - existing spider a bit better then we're
80:33 - going to go into what scrapey pipelines
80:36 - are and what they do and then we're
80:38 - going to use the scrapey pipelines to
80:40 - clean our data
80:42 - so let's get started
80:45 - if you're continuing on from part five
80:47 - you should have everything already set
80:49 - up if not you can download the code from
80:53 - our repo and continue on from where we
80:56 - are now
80:57 - so I'm presuming you already have your
81:01 - book scraper project set up with your
81:04 - spider set up and you've got your
81:07 - environment activated
81:09 - and you have screen be installed and
81:13 - python installed and everything else is
81:15 - running
81:16 - okay so items so when you generate a
81:20 - Scrapy project it generates this items
81:24 - dot py file
81:26 - and this is where you put your items so
81:29 - items just help us Define what we want
81:33 - in a block of data that we're scraping
81:36 - and that were returning
81:39 - so for example here you can see in our
81:41 - book Spider we have no specific item
81:45 - declared we're not using an item that's
81:47 - been created in relationship is that py
81:50 - but instead we just have a yield with
81:54 - all the different pieces of data we're
81:56 - extracting from the page
81:58 - so that works fine but just to clean
82:00 - things up and to make things a bit less
82:03 - ambiguous the best thing to do is to use
82:06 - the items.py and declare a specific item
82:11 - in there
82:12 - so let's go ahead and do that now
82:17 - so I'm just going to copy and paste the
82:19 - one I've already got in called book item
82:22 - the book scraper item is just the
82:25 - default one you can leave that there for
82:27 - the moment
82:28 - so book item just has everything that we
82:31 - already have used in
82:33 - our book Spider so URL title product
82:36 - type all these different things
82:39 - but instead were
82:41 - declaring them specifically here so you
82:44 - might say well what's the point of that
82:46 - well one example is that if I
82:50 - do a Miss type and reviews goes in like
82:54 - this
82:55 - this might then not go into my database
82:58 - or might not go and be processed further
83:00 - down the line
83:02 - and I might not even notice it but
83:05 - if I'm using an item
83:08 - scraper will throw an error and say
83:10 - this Norm underscore reviews with two
83:13 - r's does not exist and it alert you to
83:16 - the fact that there is a typo here so
83:19 - that's one very good reason as to why we
83:21 - will use our items
83:23 - and actually Define the item first so
83:27 - now that we've got the item to find
83:29 - we've got our item class created let's
83:31 - actually start using that
83:34 - so first off we want to import that into
83:37 - our spider
83:39 - so we go up to the top
83:40 - and we're importing book item as you can
83:43 - see it brings us directly to the book
83:45 - item now the next thing we want to do is
83:48 - we just want to
83:51 - specify
83:52 - a book underscore item is equal to book
83:54 - item and then we're just going to yield
83:59 - book item at the bottom so instead of
84:02 - yielding just start texturing there
84:04 - we're going to yield book item
84:07 - and then we're going to
84:10 - remove
84:11 - those two brackets and
84:14 - we're going to say book item URL
84:19 - is equal to response to the URL and so
84:22 - on all the way down
84:24 - so change all these into using our item
84:31 - and then once that's done
84:34 - we'll start looking at
84:38 - item Pipelines
84:41 - so let's look at the data that have been
84:44 - saved into our file so this book data
84:48 - dot Json was what we did in part five
84:50 - that was the output from our spider that
84:54 - ran
84:55 - so as you can see we had things like the
84:57 - URL the title so on so forth
85:00 - but if you noticed
85:02 - we have the price excluding tax for
85:05 - example has this encoded value here so
85:09 - it looks like the pound sign did not go
85:12 - in correctly
85:14 - so you can specify
85:17 - a specific
85:20 - serializer that you want to use on a
85:22 - specific field
85:23 - so for example if it was like the price
85:26 - I've a serialized price function I can
85:29 - write
85:30 - and I can then use that serialize price
85:34 - to stick a dollar or a parent sign in
85:39 - front of the value
85:41 - so for example I can stick serialized
85:46 - price and I'll put it in front of the
85:50 - price excluding tax
85:52 - so I could just do something like
85:55 - serializer
86:00 - serializer is equal to
86:02 - and then serialized price so that would
86:05 - make
86:07 - the value go in here and then have the
86:11 - pen size applied to it before it gets
86:15 - put into price excluding tax so that's
86:19 - also a cool way that you can use items
86:22 - with serializers
86:24 - so I'm just going to remove that one for
86:26 - now because we actually end up
86:27 - processing the data from this in item
86:30 - pipelines in a second anyway I just
86:33 - wanted to show you how you could use
86:35 - this if you didn't want to do pipelines
86:37 - and you're you're only going to scrape a
86:40 - small bit of data and you didn't want to
86:41 - do a lot of post-processing there's no
86:44 - point using pipelines and you could just
86:46 - stick to using just items and have a
86:48 - serializer if you needed to but if
86:51 - you're going to do anything more complex
86:52 - and you want to do a lot more processing
86:55 - of your data you're better off using
86:56 - pipelines instead of just using serial
87:00 - lasers
87:01 - the next thing we want to do is look at
87:03 - our Pipelines
87:05 - so in our pipelines again scrapey
87:09 - defines a book scraper pipeline when you
87:12 - create the project this is just here to
87:15 - give you an idea of what you can get
87:18 - started with so using pipelines you can
87:21 - clean your data for example you can
87:23 - remove the currency signs if you want
87:25 - you could convert the price from pounds
87:29 - to dollars you can format strings to
87:33 - integers if you're going to save it into
87:34 - a database that becomes very important
87:36 - and you can do things like converting
87:39 - your relative URLs to full URLs you can
87:43 - validate your data check if the price is
87:46 - actually a price or is it sold out and
87:49 - then in that case you can you know put
87:51 - in a price of zero and you can also use
87:55 - the pipelines to store the data so
87:57 - instead of having all the data going
87:59 - into a file like we've done in part five
88:01 - we could have it we could use a pipeline
88:03 - to get the data to go directly into a
88:06 - database
88:07 - which we will be doing in future parts
88:11 - of this series
88:13 - so let's clean up our data a bit now
88:16 - what do we need to clean well straight
88:18 - away this is not good for our data this
88:21 - encoded value here so we need to sort
88:24 - that is another thing we need to sort
88:27 - out could be the availability of the
88:30 - stock so you might say okay in stock 19
88:33 - available is fine but if I needed to run
88:36 - a piece of code later
88:37 - on this data that's not very useful
88:41 - because I just want to know that there's
88:43 - 19 books I don't want to have this extra
88:46 - text here and here and brackets so if I
88:50 - just wanted availability to be 19 I
88:53 - could use the pipeline to remove the in
88:56 - stock and the available parts of the
89:00 - string and just convert that 19 into an
89:03 - integer
89:04 - okay so we'll do that also and I think I
89:08 - saw in some places that things like the
89:11 - title had a trailing white space
89:15 - are the descriptions had trailing white
89:19 - space
89:20 - so that's also something that we could
89:22 - remove
89:24 - and another thing would be changing the
89:27 - category we could change the category
89:29 - instead of it being Thriller with a
89:31 - capital we could change that to Thriller
89:33 - with lowercase
89:35 - so this kind of standardization of data
89:38 - before it gets saved into a file or into
89:40 - a database is important especially when
89:42 - you start scripting at scale and doing
89:43 - larger projects
89:45 - so we're just going to go through a
89:47 - bunch of different processing in our
89:49 - process item in our pipeline
89:52 - so we will just add everything in here
89:56 - and then the item will be returned
90:00 - so let's start with just removing the
90:04 - white space
90:05 - so I'll just paste in the code I've
90:08 - already got
90:11 - and talk you through it okay we straight
90:14 - away get our
90:16 - item which gets passed in to our process
90:19 - underscore item so we've got the item
90:21 - available we pass it into the item
90:23 - adapter
90:24 - so as you can see up here useful for
90:27 - handling different item types with a
90:29 - single interface
90:31 - with this adapter we can get all the
90:34 - field names and then we can Loop through
90:36 - using our for loop loop through all the
90:38 - field names
90:39 - and if it's
90:43 - not the description we want to use the
90:47 - strip function to strip the white space
90:50 - from the strings
90:53 - so we're just getting the field name and
90:55 - then stripping the value and putting
90:57 - that back into what was initially there
91:00 - for that value
91:03 - okay now let's quickly look at
91:06 - converting the product types uppercase
91:11 - to lowercase if there is an uppercase
91:13 - value for the for example thriller or
91:16 - poetry values
91:19 - we can specify specific keys that we're
91:21 - looking for in this as I mentioned we'd
91:24 - look at category you can also do things
91:26 - like product type
91:27 - and
91:29 - we're going to
91:31 - just
91:33 - do the same thing except we're doing the
91:36 - lower function on the value
91:41 - now let's look at cleaning the price
91:44 - data as I mentioned earlier and as part
91:48 - of that make sure that the price data is
91:51 - saved as a float
91:53 - which can be important all the prices
91:55 - aren't always going to be rounded up to
91:58 - the nearest
91:59 - dollar or pound or Euro for that kind of
92:02 - data
92:04 - so here
92:05 - we Loop through the different price Keys
92:08 - which because we're saving several
92:10 - different pieces of data we've got price
92:13 - price excluding tax price including tax
92:16 - and the tax
92:18 - and for each one of these we're
92:20 - replacing the parent sign with nothing
92:23 - and we can also
92:25 - do something like replacing the for
92:28 - example Unicode with a specific value
92:34 - the other one I wanted to do was to
92:36 - change the availability to remove the
92:40 - extra text that was in there
92:43 - so let me quickly add that in
92:46 - to do that we're just doing the split
92:49 - function on the bracket if it sees that
92:53 - there's break there's no bracket there
92:55 - then we'll just set the availability to
92:57 - zero if there is a bracket there then we
93:00 - will split the
93:03 - second piece of the array that is
93:06 - returned from this function
93:08 - and we will say okay the second piece of
93:10 - this
93:11 - we'll split that again using the split
93:14 - function and we then know that the
93:18 - first item in this availability array is
93:23 - going to be the availability number that
93:27 - we had here
93:29 - so this is going to be the first ISO in
93:32 - that availability array and this is
93:33 - going to be the second
93:37 - so that should save just the number for
93:40 - US of the availability and we'll save
93:43 - that back into our item
93:45 - let's just look at two other ones
93:47 - quickly
93:49 - so just converting the review to a
93:53 - integer
93:56 - so
93:58 - we'll just convert Dash
94:00 - so the number of reviews make sure that
94:03 - it's an INT
94:05 - so we're just going to adapter.get and
94:07 - then we're using
94:09 - our int and putting the string in inside
94:13 - the brackets and saving that back into
94:16 - the number of views variable and last of
94:21 - all we mentioned the star rating and we
94:25 - want to turn the star rating into an
94:29 - integer also
94:31 - so
94:32 - to do that we can just get the Stars
94:37 - split the string using the split
94:39 - function again we've got the array we
94:42 - take the second value in the array
94:45 - converted to lowercase and then
94:47 - depending on what the value is in that
94:51 - variable is it zero one two three four
94:54 - five then we save
94:57 - the Stars value as 0 1 2 3 4 5. so
95:02 - pretty easy
95:03 - nothing too complicated there
95:07 - so that's everything I wanted to cover
95:10 - for for pipelines so as you can see
95:13 - there's a huge amount of data processing
95:14 - that you can do on pipelines and it's a
95:18 - good idea to have a look at your data do
95:21 - one run of it like we did in part five
95:23 - and then have a look at your data and
95:26 - actually see what you can fix what needs
95:29 - to be fixed up what looks okay what
95:31 - doesn't look okay
95:33 - sometimes you'll get a missing piece of
95:35 - data there'll be blanks but this is a
95:37 - process of refinement so the first time
95:40 - around you might only you know add in
95:42 - two things to your item pipeline you run
95:45 - it again and you notice something else
95:46 - is wrong and you add in another piece
95:49 - into this pipeline
95:52 - so the next thing you want to do we talk
95:55 - about this in part three is if you've
95:57 - got a pipeline
95:59 - you want to go into your settings and
96:01 - you want to make sure that the pipeline
96:03 - is enabled
96:04 - so we've got our spider middlewares our
96:06 - downloader middlewares extensions and as
96:10 - you can see here
96:11 - we've got our item Pipelines
96:14 - so this book scraper pipeline
96:18 - should correspond to the name of our
96:22 - class here
96:24 - and if I put that in you can see they're
96:26 - the same so that should work because
96:29 - this is also generated by scrape when
96:32 - you generate the project
96:34 - so it generally works as long as you
96:36 - uncomment this section here
96:40 - so
96:41 - if everything was done correctly we
96:43 - should be able to now run our spider and
96:46 - see the results with all
96:49 - the data processed just as we want it to
96:53 - be processed here
96:55 - if there's any errors they'll pop up and
96:57 - we can fix them and run it again
97:01 - so I'm just going to
97:04 - make sure I'm in my project
97:06 - and then
97:08 - just you want scrape your list to make
97:11 - sure everything's working and then
97:14 - one Scrapy crawl book Spider
97:17 - [Music]
97:19 - hopefully there is no issues
97:22 - okay straight away I can see there's an
97:25 - error being returned so I'm just going
97:28 - to
97:29 - stop my spider
97:32 - so none time none type object has no
97:36 - attribute next call
97:39 - and we can just
97:42 - scroll up and double check this
97:46 - so spider must return request item are
97:49 - none because ice and meta in guess
97:54 - okay so let's sort out this error
97:57 - so if we just go back to our book
97:59 - spider.py file
98:02 - you can see the error is because I'm
98:06 - returning
98:07 - book item and yielding book item and
98:11 - instead it should be book underscore
98:13 - item
98:15 - so that should fix the issue
98:18 - and if I do a Scrapy crawl again
98:23 - this time I'll actually get it to go
98:25 - into another file we loot call clean
98:30 - data dot Json
98:33 - so it's the hyphen capital O clean
98:38 - data.json and hopefully there's no other
98:41 - errors
98:43 - there it does look like there's another
98:45 - error
98:46 - because if I check clean data.json
98:49 - there's nothing there okay so I'll just
98:52 - close it again and you can see okay
98:57 - error processing
98:59 - availability
99:01 - to give us anything else
99:04 - so it says
99:05 - pipelines.pyline 21. topple object has
99:09 - no attribute strip
99:12 - so we can go to our pipelines line 21
99:16 - okay so we have our
99:19 - value.strip
99:21 - and it's saying Tuple object has no
99:23 - attribute strip
99:24 - so let's just
99:27 - print out
99:29 - the value
99:31 - of value
99:34 - let's just add in something above it so
99:37 - we can just
99:40 - see where it is in the output and try
99:43 - runish one more time
99:45 - and if we stop it again and scroll up we
99:49 - should be able to see
99:51 - that we've got our stars and we've got
99:54 - in stock available at 19.
99:57 - and it is indeed being returned in a
100:00 - topple uh with the second
100:03 - value
100:05 - there's nothing there so obviously we
100:08 - need to reference the first value in the
100:11 - toggle so we need to just
100:15 - do that
100:17 - so
100:18 - adding the square brackets since zero
100:21 - should return
100:22 - just the string that we're looking for
100:25 - and then dot strip can act on this
100:28 - string
100:30 - so if we remove our print statements
100:33 - save that and build Traverse again
100:43 - and
100:45 - it looks like there's some errors coming
100:47 - in there we can just check our file
100:50 - there's nothing in here yet
100:52 - so I'll go ahead and stop the spider
100:56 - from running
100:57 - and
101:01 - we can see an error here
101:04 - pipelines.py line 21 in process item
101:09 - so that's still giving it a bit this
101:11 - line
101:13 - but this time it's saying
101:15 - type error none type object is not
101:19 - subscriptable so
101:22 - I know what this error is I've had it
101:24 - before so this is coming up because
101:26 - we're getting all the field names which
101:29 - are a from our items.py so it's getting
101:34 - all these different field names here
101:35 - it's looping through them and one of
101:38 - these field names is not
101:41 - being found so if we look at our spider
101:46 - and compare
101:48 - all these guys here
101:51 - versus
101:52 - what we have here I think I've spotted
101:55 - the one already so I think it's this
101:58 - UPC field unique product code I think it
102:02 - stands for
102:03 - and if you look here we don't have book
102:07 - item UPC
102:10 - so I can just add that in now so I'll
102:14 - add that in
102:16 - here
102:18 - and save that
102:21 - so now we should have this
102:24 - which should correspond to this and we
102:25 - should have no more errors
102:28 - so
102:29 - let's run that again
102:34 - and this time we should see our
102:37 - clean data.json file filling up
102:42 - so everything looks good there
102:45 - open up the clean data chart Json we've
102:48 - got
102:49 - what looks like
102:51 - all the data we wanted
102:55 - so
102:56 - we can go ahead and just
102:58 - stop the spider don't need it to collect
103:01 - all 1000 records you can just double
103:05 - check that everything did go in
103:07 - correctly so you can see just by either
103:10 - checking the file or scrolling up
103:13 - did everything get processed the way you
103:16 - wanted to get processed so
103:18 - did the price get get processed
103:20 - correctly is it now double the product
103:23 - type is the
103:25 - first part lower case yes it is the
103:27 - number of stars is now an integer
103:30 - so it looks like everything that went
103:33 - through our
103:34 - pipelines.py got processed correctly
103:38 - we can scroll up and check the category
103:41 - as well and the availability so
103:43 - everything worked out
103:45 - so that's just how we go through using
103:48 - pipelines and items
103:50 - I hope that's given you a good idea of
103:52 - how you can use items and pipelines
103:54 - yourselves to clean the data that you're
103:57 - scraping
103:59 - and in part seven
104:02 - which we'll be looking at next we'll be
104:03 - looking at how we can use pipelines to
104:07 - save our data into databases and also
104:11 - how to use
104:12 - feed exporters in a bit more detail so
104:15 - see you in part seven guys
104:22 - so for part 7 of our scrapey beginners
104:24 - course we're going to look at all the
104:26 - different ways we can save data so all
104:29 - the data we've scraped in the last few
104:31 - parts we're just going to see how can we
104:34 - save it to different file formats and
104:37 - then eventually look at databases
104:40 - so first off we're going to look at Via
104:43 - the command line how what commands we
104:46 - need to run to save it to different file
104:48 - formats then we're going to look at how
104:50 - we can do that instead via the feed
104:52 - settings which we can set in our
104:54 - settings are in our main spider file and
104:58 - then once we've done that we're going to
105:00 - go on and look at saving data directly
105:03 - into a database using the Pipelines
105:07 - so if you've done part six with us you
105:09 - know all about pipelines by now and
105:11 - we'll be using those pipelines in part 7
105:14 - to save the item and data into the
105:19 - database directly
105:21 - if you're just joining us now you can
105:23 - download the code from our GitHub repo
105:27 - we'll have links for that and you can
105:31 - follow on from just this part 7. to get
105:35 - going I'm just going to go into my
105:38 - book scraper folder make sure I'm in the
105:41 - right place
105:42 - and then run Scrappy crawl and the name
105:45 - of our spider
105:46 - crawl book Spider Dash capital O
105:52 - and then
105:54 - book
105:56 - data.cs V
106:00 - so this is going to Output the data into
106:04 - a CSV format which is comma separated
106:08 - values so that can be opened in Excel
106:10 - and as you can see the data is all there
106:13 - correctly okay so we can stop that now
106:18 - and if we scroll to the bottom we can
106:21 - see we have
106:23 - 321 rows
106:27 - so
106:28 - if you want to append data onto a file
106:31 - instead of The Hyphen or Dash capital O
106:35 - you can do a lowercase o
106:37 - and then if we do the same name again
106:39 - book data.csv
106:42 - and enter
106:44 - it should start pending on the data here
106:47 - so instead of overwriting the file every
106:49 - time
106:50 - if we close the file
106:53 - open it back up you can see we're
106:55 - already up to over 500 records so the
106:58 - file doesn't update automatically
107:00 - sometimes it can take a couple of
107:02 - seconds or you have to close it and
107:05 - reopen it
107:07 - so as you can see we're up to 700
107:10 - records there and if I relish once more
107:14 - and do a capital O
107:17 - it will overwrite that
107:20 - there you go so it's after wiping the
107:22 - file and now filling it again
107:24 - okay
107:25 - so that's the difference with the
107:30 - overwriting are appending and you've
107:33 - seen just by changing the file format at
107:37 - the end of your file name
107:39 - is how you can specify types of
107:43 - files that you want to write into so
107:45 - here we're going to do it again but
107:47 - we're going to do it in Json file format
107:50 - so we have a new file is created book
107:54 - data.json
107:55 - as you can see it's in Json format
107:58 - and the other one
108:00 - is comma separated values format
108:05 - now let's move on and look at how we can
108:07 - specify where the data will be saved in
108:10 - our settings file
108:12 - so if we open up our settings file
108:16 - what we can do is use the feeds so what
108:21 - we do is
108:22 - add in a feed section
108:25 - and here we're saying save the data into
108:29 - a file called data.json
108:32 - so that's the scholars
108:35 - books data
108:37 - dot Json and the format is going to be
108:40 - Json so if I delete the two files we've
108:45 - just been using there
108:47 - and save what I have in my settings
108:51 - and rerun it except this time
108:54 - remove the
108:56 - Dash o and the name of the file and if I
109:01 - just go ahead and run my spider
109:05 - we've seen it's just created at books
109:08 - data.json because I've specified it here
109:12 - and it's all in the correct format
109:15 - so I'm just going to go ahead and stop
109:17 - my spider and the next thing you want to
109:21 - do is just show you guys how you can
109:23 - specify the feed
109:27 - data in your spider so to do that we can
109:31 - use the custom settings
109:36 - so this just enables you to overwrite
109:40 - anything you have in your
109:43 - settings file
109:44 - and you can just specify it in your
109:47 - spider so just need to specify what we
109:50 - want to overwrite
109:52 - and we're going to overwrite our feed
109:56 - and we would then push
110:00 - our feed
110:01 - settings in here
110:04 - so if it sees that the feeds are set
110:08 - here it will overwrite what we have in
110:11 - our settings.py file
110:14 - so this is just an easy way that if you
110:16 - guys want to specify certain settings
110:19 - you can do them here they don't all have
110:22 - to be in your settings.py file
110:24 - one important thing to note as well with
110:27 - our feeds when we either set it in
110:29 - settings or in custom settings is we
110:33 - need to set the overwriting so like we
110:37 - did earlier
110:38 - we just have overrice true our overwrite
110:41 - false because it depends on where you're
110:46 - storing the data what the default is for
110:49 - that setting so it's better just to
110:51 - specify that we want to or write the
110:53 - file or not we can just have it there
110:56 - and run it and it'll overwrite our
110:59 - current file
111:02 - so now that we have that the next thing
111:05 - we want to look at is how to save our
111:08 - data into databases using our Pipelines
111:14 - so I've gone ahead and I've already
111:16 - installed MySQL so MySQL is a very
111:20 - popular
111:21 - database which you can get yourselves
111:24 - just by going to
111:27 - mysql.com site and their download
111:30 - section and you can just choose your
111:33 - operating system so if you've got
111:34 - windows
111:35 - obviously have it for Windows click
111:38 - download and install it then they have
111:41 - for many other operating systems the
111:43 - available downloads there too
111:45 - so once you've downloaded and installed
111:47 - that you should be able to then make
111:50 - sure it's installed correctly just by
111:52 - running MySQL and then dash dash version
111:56 - so as you can see here I have version
112:00 - 8.0.32 for Mac OS 11. and that's the
112:04 - latest 8.0.32
112:07 - so that's installed for me so the next
112:10 - thing I'm going to do is just Connect
112:12 - into my MySQL
112:15 - so I can just type MySQL and then if
112:18 - it's a simple install and you've just
112:20 - installed this you should be able to
112:21 - just hit enter and it brings you
112:23 - straight in
112:25 - and you know you're connected in because
112:26 - you've got MySQL here and then you can
112:30 - just say
112:31 - show databases
112:34 - and it shows the databases so I've
112:36 - already gone ahead and created one
112:37 - called books
112:39 - that's there but you obviously won't
112:42 - have that if you just installed it so
112:44 - you want to create a database so you
112:46 - just do create
112:48 - database
112:49 - books and then it'll say
112:53 - created I've already got the database
112:56 - already there so it says database exists
112:58 - for me
113:00 - so we need a database to actually Savor
113:02 - things into and once it's set up it'll
113:05 - be there in your list and you just do
113:07 - show
113:08 - databases
113:10 - to get the list of databases that are
113:12 - available
113:14 - so we can exit out of that once we have
113:16 - our database created
113:19 - you might have to connect in to your
113:21 - MySQL if you've set up a username and
113:24 - password are with a different host if
113:25 - you've so you could do host
113:29 - localhost
113:30 - minus U for user root and minus P if
113:35 - you've got a password and then it'll
113:37 - prompt you for a password
113:38 - so depending on what you have
113:41 - so that also works to connect in
113:44 - if I'd set up a password it would have
113:46 - asked me for a password beforehand so if
113:49 - you just type MySQL you can usually get
113:51 - in if you ever already haven't got a
113:54 - password set up or if you're using a
113:55 - different host like digitalocean or some
113:58 - other third-party provider you can stick
114:01 - the URL to where your database is hosted
114:04 - there
114:06 - so we've got our database set up and the
114:09 - next thing we want to do is we want to
114:13 - install
114:15 - our MySQL connector so just to make sure
114:19 - that python is able to connect to our
114:22 - database
114:23 - so I'm just going to paste in the
114:24 - command for that and you guys can
114:27 - have a look at my screen there and type
114:29 - it in so it's going to install the MySQL
114:31 - and MySQL connector python packages
114:35 - with using pip again so
114:38 - go ahead and run that
114:42 - and now that I have that I should be
114:44 - able to start working on the pipeline
114:49 - so we can just go directly under our
114:52 - existing book scraper pipeline
114:55 - and we're going to create
114:58 - a new class
115:00 - and we're just going to call that save
115:02 - to mySQL pipeline
115:05 - and then we're going to import our MySQL
115:09 - connector to help us connect in
115:12 - and we're going to initially just
115:16 - when this pipeline is initialized we're
115:20 - going to
115:22 - set up our connection
115:24 - and then set up our cursor
115:27 - so I'll show you guys now what that
115:30 - entails
115:31 - so we have this init function here so
115:34 - this is going to start up when
115:37 - our spider
115:38 - initializes this class
115:41 - we have we're using the MySQL
115:43 - connector.connect here to set up the
115:46 - connection we've got our host our
115:48 - username password if you have a password
115:51 - you have to add it in here and then the
115:53 - database that we just created books
115:56 - so we can save all that and then we have
115:59 - the cursor which is used to execute the
116:02 - commands
116:03 - so I have that set up here and that's
116:06 - saved into self dot Cur so we can use it
116:10 - further in other functions
116:14 - so the next thing we want to do is we're
116:16 - going to add in
116:17 - that we want a new table to be created
116:20 - if there is no table to store the data
116:24 - so this can just be handy in case you
116:27 - are running this over and over again
116:31 - or you're testing you might want to go
116:33 - in drop the table and if you don't want
116:35 - to remember did I just create that is a
116:37 - table there or not you would have this
116:39 - there so that will just make sure that
116:41 - there's a table there so it creates a
116:44 - table if it doesn't exist called books
116:47 - and that table will have the following
116:49 - columns ID URL title UPC product type
116:54 - everything that we've already been
116:56 - scraping from the page all the different
116:58 - data points
116:59 - so it will set up all these different
117:01 - columns including a primary key called
117:04 - ID and then all the data will be able to
117:07 - saved into the columns that we want it
117:10 - to be saved into
117:13 - so we'll have the table set up if it's
117:16 - not already set up so that's we don't
117:19 - have to go in and manually set it up in
117:21 - MySQL ourselves
117:24 - and the next thing we'd want to look at
117:26 - is
117:27 - we're going to again use the process
117:30 - item function
117:32 - so we've already had that in our other
117:34 - pipeline but we're going to add it in
117:37 - here
117:39 - and this is where we're going to have
117:41 - our insert statement so it's going to
117:43 - insert our data that we have in our item
117:49 - so here it is
117:51 - pretty simple so using the cursor that
117:53 - we've already defined above I'm going to
117:55 - say please execute this command insert
117:57 - into books URL title UPC product type
118:01 - all the pieces of data that we've
118:04 - already scraped
118:05 - so
118:07 - once that's insert statement just there
118:09 - we have to use commit to make sure that
118:12 - the insert is actually executed
118:14 - correctly and then we just return the
118:17 - item
118:18 - so that if we add one more layer to our
118:21 - pipeline that the item is returned and
118:23 - the next piece of our pipeline can also
118:26 - continue the only other thing we need to
118:28 - add in now is
118:31 - we want the connection to our database
118:33 - to be closed once the spider is finished
118:37 - so to do that we just add in that
118:40 - enclosed spider so this is just
118:43 - a function that's scrapey looks for if
118:45 - close spider is there it executes closed
118:48 - spider once the spider is ready to close
118:50 - at the end
118:52 - so inside enclosed spider we just add in
118:55 - cursor close and connection close so
118:59 - we're just closing the cursor and the
119:01 - connection
119:02 - just so that this stuff isn't kept open
119:05 - and using memory if we're executing this
119:09 - lots and lots of times we don't want all
119:12 - this memory to be taken up with cursors
119:14 - and connections that are not being used
119:19 - so now that we have that we need to go
119:22 - to our settings and we need to enable
119:25 - our new pipeline
119:28 - so we're just going to copy the existing
119:31 - line and we're going to say
119:35 - execute our pipeline after this
119:40 - and existing one so we want the data to
119:42 - be clean first and the second step is
119:45 - save the data into our mySQL database so
119:51 - we're going to just copy the class name
119:54 - go to settings
119:57 - paste that in here
119:59 - and then the only thing we need to do is
120:01 - we need to change this number here
120:04 - so I don't think I've talked about this
120:05 - number yet
120:07 - so what this number is is that it's just
120:09 - the order in which the items in the item
120:13 - pipeline have precedence so the lower
120:16 - the number the higher
120:18 - the order of importance the first thing
120:20 - that's going to be executed so in this
120:22 - case number 300 is going to be executed
120:25 - first and then number 400 is going to be
120:27 - executed after that so this is an easy
120:29 - way for us to say please execute this
120:32 - pipeline first and this pipeline second
120:35 - and if you had multiple pipelines you
120:37 - can just use these numbers it doesn't
120:39 - have to be three or four hundred it can
120:41 - be any number you want I've just picked
120:44 - three and four hundred for now
120:47 - okay so now that we have that we should
120:51 - be able to go ahead and check our
120:55 - database to see did the
120:57 - items get saved into the database
121:00 - correctly
121:01 - so let's do that now
121:03 - so as before we're just going to do
121:05 - Scrappy crawl book Spider
121:11 - and
121:13 - should kick off
121:16 - so we have several books after being
121:19 - scraped so that's let's stop our spider
121:22 - the next thing we want to do is we want
121:24 - to
121:26 - log back into our
121:28 - MySQL console
121:31 - and then we want to
121:35 - show database
121:37 - this is
121:39 - and then we want to use
121:43 - books
121:44 - this just enables us to select from that
121:47 - database
121:48 - so once we're using the books database
121:50 - we want to just
121:52 - show tables
121:54 - and we can see that the table was
121:56 - created that we asked to create books
122:00 - and then we can do
122:02 - select all
122:05 - from books
122:07 - and we can see that there's 138 rows
122:12 - there and Dash D data looks like it's
122:16 - safe correctly we've got the name of the
122:18 - books
122:19 - all the other pieces of data that we had
122:21 - the description the price
122:23 - the tax
122:25 - the availability the category it all
122:29 - looks like it's saved there correctly
122:33 - so we can
122:35 - drop the table
122:37 - so dropping the table just basically
122:40 - removes the table so that it won't exist
122:43 - drop table books
122:45 - if we want to start again because
122:48 - otherwise what it's going to do is it's
122:49 - going to keep appending on to the
122:51 - database
122:52 - so we can see if we show tables that
122:56 - there's no more tables in the database
122:57 - now
122:58 - but our pipeline creates a new table
123:02 - anyway so that's fine
123:05 - so that's how we create a simple
123:08 - database
123:10 - get it set up and have a simple pipeline
123:12 - script that
123:13 - once the data is cleaned up with our
123:17 - first pipeline that we did in part six
123:19 - it then inserts it into a mySQL database
123:22 - in this tutorial that we've just done
123:25 - again using our Pipelines so obviously
123:28 - if you're
123:29 - more familiar with using postgres
123:32 - databases or other types of databases
123:34 - you can just modify the pipeline
123:37 - slightly we will have available articles
123:40 - where it'll show you the exact code you
123:43 - need to use a pipeline to insert the
123:47 - database into a postgres database also
123:50 - so you guys can have a look at the
123:52 - articles that we'll attach and we'll
123:55 - also have the code repos there for you
123:57 - guys to
123:59 - just download and play around with two
124:03 - so I think that's it for part seven in
124:06 - part eight we're going to be looking at
124:08 - how we can use
124:11 - the user agents and headers to get
124:16 - around issues with being blocked when
124:20 - we're trying to scrape different
124:21 - websites
124:23 - so we're going to be looking at user
124:25 - agents and headers in detail what they
124:28 - are how to use them
124:30 - so see you in partake guys thanks for
124:32 - watching
124:36 - welcome to part 8 of the scrapey
124:39 - beginners course
124:40 - for a free code camp
124:42 - so in part 8 we're going to be looking
124:44 - at why we get blocked when we're
124:47 - scraping the web
124:48 - what types of websites might block us
124:51 - and then how to use user agents and
124:55 - headers to bypass instances where we're
124:59 - getting blocked while scraping
125:02 - so we'll start off by going straight
125:05 - into what headers are
125:09 - so if you go to the site we've been
125:12 - scraping in the last two parts
125:14 - books.2scrape.com
125:17 - you open inspect
125:20 - an element on the page go to the
125:22 - networking Tab and then
125:25 - simply refresh the page if you've got
125:28 - doc selected or all selected you will
125:31 - see what we want to see
125:34 - so because this is a just a simple
125:36 - website the HTML is sent to us and we
125:41 - can see it returned in the preview
125:45 - so you can see all the HTML is there
125:49 - and that's what we end up scraping when
125:51 - we are using scrapey
125:53 - but if we look at the headers tab
125:57 - we can see everything that is sent when
126:00 - we request this page and you've got the
126:04 - request URL which is just the URL of the
126:06 - site we're trying to scrape you've got
126:08 - the method are we trying to get the page
126:10 - are we posting data to the page
126:13 - and then you've got things like the
126:15 - status code
126:16 - and so on so on now the important stuff
126:19 - for us are the request headers
126:23 - so this is everything that we send when
126:28 - we make a request to books.2script.com
126:33 - and as part of this
126:35 - the most important part for us in this
126:39 - part eight
126:40 - of the tutorial is the user agent
126:44 - so the user agent
126:46 - gives you all the important information
126:48 - about who you are to the server that
126:53 - you're requesting the web page from so
126:56 - here we can see if I copy this string
127:00 - and I've got this site user agent
127:03 - string.com and it lets you paste in
127:07 - and analyze user agents
127:10 - so this is all the stuff that
127:12 - is is contained when we make a request
127:15 - to a website
127:17 - straight away it knows that we're using
127:18 - Chrome what version of Chrome we're
127:20 - using
127:21 - the render engine
127:24 - that we're using
127:25 - your operating system so I'm using OS X
127:28 - I'm using an Intel CPU so all this kind
127:32 - of data is sent with every request you
127:35 - make automatically
127:37 - now this is fine when you're browsing
127:40 - the web or you're building your spiders
127:43 - and you're doing a bit of testing but if
127:45 - you're doing any sort of large-scale
127:47 - scraping on any kind of commercial sites
127:50 - you're more than likely going to start
127:52 - to get blocked
127:54 - so a lot of sites think I don't know
127:56 - Amazon Walmart
127:59 - any kind of big eCommerce sites most of
128:02 - them will have some form of antibots
128:06 - that stop you from scraping now you
128:09 - might say why do they want to stop me
128:11 - I'm not doing anything bad I just want
128:13 - to collect their data well they'll say
128:15 - well this is our data we own the website
128:19 - this is only for our customers
128:22 - so on so forth now obviously
128:25 - you need to look at the terms conditions
128:27 - of the site you're scraping and judge
128:30 - for yourself if it's legal or illegal
128:34 - the rule of thumb to go by is that if
128:37 - it's publicly available and you don't
128:39 - have to log in and give your details
128:42 - then it's more than likely okay to
128:46 - scrape the data of the website if you
128:49 - have to log in first and by logging in
128:52 - you might be agreeing to certain terms
128:56 - and conditions then more than likely the
128:58 - website will have that in their terms
129:00 - and conditions that you are not allowed
129:02 - to scrape their data so that's up to you
129:05 - guys to decide in a case-by-case basis
129:09 - but for a lot of simpler sites like the
129:11 - one where
129:12 - we're scraping in our tutorial series
129:15 - here
129:16 - this site has no antibots on it so there
129:19 - is nothing which will block us even if
129:22 - we have the same user agent
129:25 - so therefore it knows if if it gets a
129:28 - thousand requests
129:30 - from this chrome and my Mac that it
129:33 - knows that it's me then it's it's not
129:36 - going to do anything about it it's not
129:37 - going to block me when I'm trying to
129:38 - scrape all these books off this website
129:41 - but that's obviously because this
129:43 - website is there for people to learn on
129:46 - so
129:48 - that's kind of why we get blocked so the
129:51 - other things that they look for is they
129:53 - look at the IP address of the machine
129:57 - that you're using so that's also a a
130:00 - very simple way for websites to block
130:03 - who the requests because they can see
130:06 - your IP address every time you make a
130:08 - request so they normally look for the IP
130:11 - address
130:12 - and they might set something in your
130:14 - cookies in your session so they might
130:17 - set some kind of flag or counter there
130:21 - so they know that it's you coming back
130:24 - every time
130:25 - so it's mainly IP address the cookies or
130:28 - the sessions and then the headers in
130:31 - general and as part of that the user
130:34 - agents so the difference between headers
130:38 - and user agents is that headers
130:41 - is everything that we have here
130:44 - so it it encompasses things like the
130:48 - accepted things that are returned does
130:51 - it take HTML or does it take images you
130:54 - know what do we accept back what does
130:56 - the browser accept back as a as a
130:59 - response to things like the language
131:02 - encoding and then the user agent is just
131:05 - one
131:07 - subset of the overall request headers
131:11 - so
131:12 - for some sites that are not too complex
131:15 - if we change the user agent each time we
131:20 - make a request
131:21 - the website will think that it is a
131:24 - different browser and a different
131:25 - computer looking for the data on the
131:28 - site every time so it'll let the request
131:31 - go through however for more complicated
131:33 - sites they'll look at everything in the
131:37 - request headers and they'll want
131:39 - everything to be different or at least
131:41 - slightly different so
131:44 - for example I have Mac OS here so if Mac
131:48 - OS is coming every single time and they
131:51 - match that plus they can see my Google
131:54 - Chrome version here as well and the
131:56 - version of chromium then they might say
131:58 - okay this looks too similar even though
132:01 - their user agent is changing every time
132:03 - this looks suspicious and they might
132:05 - flag my requests and block my requests
132:09 - or at least they might not even block
132:10 - them but they might throw up a capture
132:13 - page so that if you're not actively
132:16 - solving the capture
132:18 - the requests are being blocked
132:20 - so for the most comp for the more
132:23 - complicated sites we need to also be
132:26 - changing the entirety of the request
132:29 - headers not just the user agents
132:32 - so they're the kind of main things we
132:35 - need to look at is how can we change
132:37 - your IP address to stop getting blocked
132:39 - how can we change our user agents and
132:43 - also the entirety of the request headers
132:46 - for the more complicated sites so that's
132:48 - what we're going to be doing in part 8
132:49 - and part nine is looking how to bypass
132:52 - being blocked by changing these
132:55 - different parts that are sent when we
132:58 - make a request for a web page
133:01 - okay
133:03 - so now that we've gone through the
133:05 - theory of it of what is in a header and
133:09 - user agent and what details the websites
133:13 - are looking at when we make a request
133:15 - let's go to our spider and Implement
133:18 - different user agents every time
133:21 - how we can get multiple different user
133:25 - agents insert them into our spider use
133:27 - middlewares to do that and then also do
133:30 - it for request headers
133:33 - okay so if we go back to our spider
133:36 - we're continuing on from part seven if
133:39 - you're just joining us for this part
133:40 - eight we have a link to download the
133:43 - code so you can hop in and start right
133:46 - here with us now
133:48 - so
133:49 - the first thing I'm going to do is I'm
133:52 - going to
133:54 - just go to my settings and disable
133:57 - the pipeline which saves the data to
134:00 - mySQL because I don't need to do that
134:04 - for the purpose of this part H so the
134:08 - next thing is I'm going to open up my
134:11 - spider
134:12 - and
134:14 - I'm going to set a user agent so I'm
134:19 - going to do that the simplest way to do
134:21 - it is actually to go to our settings
134:23 - and in the settings we can directly set
134:27 - a user agent
134:29 - so
134:31 - just like I had shown you in the browser
134:35 - it's user agent and then it contains all
134:40 - the available information about
134:43 - the user who is requesting the web data
134:48 - so here you can see Mac OS X
134:51 - you can see it's an iPad
134:53 - etc etc etc so this is just an example
134:56 - if you wanted to send this user agent
134:58 - with every single request you can set it
135:01 - in the settings now obviously that
135:04 - doesn't make sense because
135:06 - it's not changing So within 10 or 20
135:10 - requests the website is going to say hey
135:13 - this is the same person every time
135:16 - they're making a lot of requests they're
135:18 - probably web scraping and they'll ask
135:20 - for capture audio blockers so this is
135:24 - not sufficient this is just if you want
135:26 - to set one specific user agent for every
135:30 - single request so for now we'll remove
135:32 - that again
135:33 - and we will look at how we can create a
135:38 - list and rotate through that list
135:42 - so what we're going to do is go back to
135:45 - our spider and we are going to create a
135:52 - list of user agents
135:56 - so here's one I've pasted in from our
135:59 - article you can check out our article
136:01 - and paste it in yourself as well
136:04 - and we also have this available in the
136:07 - GitHub repo so you don't have to type
136:09 - out everything here yourself
136:12 - so we've got a list the next thing we
136:15 - want to do is
136:16 - add the user agent into every single
136:20 - request that we make
136:22 - so to do that
136:25 - we can go to where we make the requests
136:30 - every time
136:31 - and we can specify that we want our user
136:36 - agent to be overwritten to do that we
136:41 - just
136:42 - go to where we have our callback and our
136:44 - book URL
136:46 - and we would do something like this
136:49 - so we would have our headers and we're
136:52 - saying
136:53 - overwrite the user agent part of our
136:56 - headers
136:57 - and we'll do this with our user agent
137:02 - list
137:03 - so we just need to specify self dot user
137:07 - agent list
137:09 - and then
137:10 - we'll import random so we can
137:14 - switch between different things at
137:17 - random and we'll do self.user agent list
137:21 - here and I think that's all we need
137:25 - so what this does is we're saying
137:29 - add this user agent to our headers when
137:32 - we make the request and pick
137:35 - a random user agent from between 0 and
137:39 - the length of the user agent so it's
137:42 - going to pick
137:43 - one of these guys at random and insert
137:46 - it in to our header
137:48 - now obviously you need to add this
137:51 - line that we just added to everywhere
137:54 - that is
137:57 - making a response dot follow
138:03 - so we can add it here as well
138:06 - and I think that's the only two places
138:09 - we have them
138:10 - so
138:11 - we should be able to run that
138:13 - and it should send a different user
138:16 - agent every time
138:20 - but as you might guess
138:22 - this isn't really enough to spoof a
138:26 - large-scale website they'll see if
138:30 - you're doing thousands of requests okay
138:32 - there's only
138:34 - five different user agents and they'll
138:39 - say we need to block this user
138:42 - so that brings us on to how we can use a
138:46 - fake user agent API to give us a massive
138:50 - list of thousands of user agents and
138:52 - then we can Loop through that list of
138:56 - thousands of user agents but instead of
138:59 - having them all here directly in our
139:01 - spider what we would do is we would
139:03 - Implement and middleware
139:06 - and we would add it into our
139:08 - middlewares.py and in this middleware is
139:11 - where we would rotate through all the
139:14 - different fake user agents that we would
139:17 - be getting from a fake user agent API
139:22 - so that's what we're going to look at
139:23 - next we're going to create a middleware
139:26 - and we're going to ask for those fake
139:29 - user Agents from the third-party website
139:32 - get those user agents returned to us
139:35 - again pass those in to our request
139:38 - headers
139:40 - so to get those request headers we can
139:43 - go back to our browser and go to
139:45 - scrapeups.io
139:48 - where you can sign up for a free account
139:50 - and then using the API key that you get
139:53 - you can use the free headers generator
139:57 - API
139:59 - so this is what happens when I use their
140:01 - headers API I can specify that I want
140:06 - user agents I put in my API key
140:10 - there make a request and it gives me
140:13 - back a result with a bunch of different
140:17 - randomly generated user agents so once
140:22 - you're actually logged in
140:24 - you can go to their
140:27 - fake headers API section and that's
140:29 - where it shows you your API key and if
140:33 - you want to generate browser headers
140:36 - you use this URL if you want to generate
140:40 - user agents
140:41 - you use that URL and then the response
140:46 - comes back like this just as I showed
140:49 - you here
140:50 - so I can just
140:54 - stick that in and it gives me the
140:56 - results back so you can specify the
140:59 - number of headers you want and it sends
141:02 - you back all the user agents our browser
141:05 - headers that we can then use in our
141:07 - middleware
141:08 - so depending on the language you're
141:10 - using as well you can specify different
141:13 - examples so
141:14 - this is where I got the URL to use
141:18 - for here
141:20 - and if you're using node or PHP a ruby
141:24 - you could use the other tabs to see the
141:28 - examples but we're using python with
141:30 - scraping so we have everything we need
141:33 - here
141:35 - so now that we have an API endpoint
141:37 - where we can get our fake user agents
141:41 - and fake headers
141:43 - we can go back to our middlewares file
141:46 - and we can start creating our middleware
141:49 - that will handle everything to do with
141:51 - the fake user agents so I'm just going
141:55 - to scroll down to the bottom and I'm
141:57 - going to start
141:58 - a new class
142:00 - and I'm going to be importing a couple
142:02 - of things that we're going to need
142:04 - so I'm going to import your link code
142:06 - which will encode our urls
142:09 - ran into pick a random integer so we can
142:13 - use that to pick one from the list and
142:15 - requests as well
142:17 - so I've created a new class called
142:19 - scrape UPS fake user agent middleware
142:21 - that can be obviously whatever you want
142:24 - it to be
142:25 - and then we're going to set it up
142:29 - so again we have our initial
142:32 - function which gets kicked off when the
142:35 - class is initialized
142:38 - and in here we first off set some of our
142:42 - settings
142:43 - so we're going to want the scrape UPS
142:45 - API key which is going to be the API key
142:49 - that we get for free from
142:54 - here so that's where we have our API key
142:56 - we also have the URL
142:59 - which you can see
143:02 - here
143:03 - and
143:05 - so we've got our endpoint
143:07 - and what else oh yeah do we want it to
143:10 - be enabled or not and the number of
143:12 - results we want to specify to come back
143:16 - so
143:17 - we go ahead and set those all up in our
143:20 - settings
143:22 - so I'm going to set my
143:26 - API key
143:28 - obviously you guys set that to whatever
143:30 - your one is
143:33 - set up the endpoint
143:37 - and the end point is going to be user
143:40 - agents
143:44 - paste static
143:47 - and then we have our
143:50 - if we have enabled
143:53 - so that's true
143:55 - there you go
143:57 - and
143:59 - the num requests
144:02 - which we can set to to 50.
144:06 - you can save that and
144:09 - save that as well so this part up here
144:13 - just makes sure that we have access to
144:16 - our crawler settings when d-class is
144:19 - initialized
144:21 - so as you can see here and here we've
144:24 - got two functions
144:26 - and that means when the class is
144:28 - initialized get the user agents list and
144:32 - enable it
144:34 - so I'm going to first off get the user
144:37 - agents list so I'm gonna add that one in
144:41 - so that function looks like
144:43 - following
144:46 - we've got the payload set which is the
144:50 - API key
144:52 - and then we're saying if the number of
144:55 - results is not none set the payload
144:59 - number results
145:01 - and then we want to make a get request
145:04 - to the API endpoint with our
145:09 - parameters which have been URL encoded
145:12 - here using the URL encode function then
145:15 - that goes off goes to the script ops
145:18 - endpoint and then gets the user agents
145:22 - and comes what comes back gets put into
145:24 - the response
145:26 - then we're using dot Json just to parse
145:30 - it into a Json response
145:32 - and then we can
145:35 - have our user agents list saved into
145:38 - user agents list
145:42 - so once we've got that we're going to
145:44 - just
145:45 - create two more simple functions
145:48 - underneath
145:50 - the first one just get random user agent
145:54 - fairly self-explanatory
145:56 - let's get one and getting one user agent
145:59 - from the list that's been returned and
146:02 - returned that selected user agent
146:06 - and then we have
146:08 - just this check to see if the fake user
146:11 - agents is either active or inactive
146:15 - and then once all that's set we can
146:18 - actually
146:19 - put that into practice with our process
146:23 - requests which is one of the scrapey
146:27 - functions that's it's one of the
146:29 - functions that scrapey will look for
146:30 - when you're using middlewares
146:32 - and
146:33 - it then sees that we've specified
146:36 - something to happen when it goes to
146:39 - processor request and then it executes
146:42 - the following
146:43 - so when it sees process request it goes
146:46 - in here it gets a random user agent
146:49 - sticks it in here and then it sets the
146:53 - header user agent to be the random user
146:57 - agent so I hope that makes sense I think
147:00 - it's fairly self-explanatory we're just
147:02 - getting a list of user agents
147:05 - and then with process request we are
147:08 - getting a random user agent and we are
147:11 - assigning that to our header request
147:14 - header so when we go off and ask our
147:17 - books to scrape site
147:20 - for the book or the list of books it
147:24 - sticks in dash random user agent
147:27 - into the user agent of the request
147:32 - the only thing to do then is to stick
147:35 - our middleware this is a step you
147:38 - mustn't forget it's very easy to forget
147:40 - it I've forgotten it loads of times and
147:42 - then sometimes you can be trying to
147:44 - debug things after
147:46 - so you want to go to your downloader
147:49 - middlewares
147:50 - open that
147:52 - and add in your new middleware
147:58 - so again as we did in the last one the
148:01 - lower number has higher priority
148:04 - so we don't need to have
148:07 - the other downloader middleware just
148:09 - want our
148:11 - a middleware we just created our scrape
148:15 - Ops fake user agent middleware
148:17 - because this middleware up here is just
148:19 - the generated one that we don't need at
148:23 - the moment
148:25 - okay we'll go ahead run our spider now
148:28 - check that it's working
148:31 - should be working fine
148:33 - and then we will look at
148:36 - just doing the same thing but instead of
148:39 - having
148:40 - a user agent come back from scrape-ups
148:44 - We'll be asking for a list of fake
148:47 - headers so we'll create a second
148:49 - middleware
148:50 - and we'll just have the whole fake
148:53 - header as opposed to just the subsection
148:55 - user agent part
148:57 - so just to make sure that the headers
149:00 - are being attached correctly because you
149:02 - might say oh I know for sure that it's
149:04 - working we can just add in a very simple
149:07 - print to our process request
149:11 - so
149:12 - I'm just going to stick this in here two
149:14 - print statements one sync new header
149:16 - attached and the other one saying the
149:19 - new user agent so it'll print out the
149:22 - user agent that we've just attached the
149:24 - random user agent so that should print
149:27 - that out then to our console and we
149:29 - should be able to see that a new user
149:32 - agent is being attached every time that
149:34 - request is being processed you out the
149:36 - only other thing we have to do is go
149:38 - back and remove what we added in earlier
149:41 - which was this header's part here
149:43 - because it's being done in middleware so
149:46 - we don't have to specify it for every
149:48 - request here so I'm just going to remove
149:51 - that
149:53 - and I'm going to remove it here as well
149:56 - so as you can see there's always
149:58 - multiple ways of doing it either kind of
150:00 - manually adding things into the spider
150:02 - setting them in the settings if you just
150:05 - want it once off in the settings if you
150:07 - want a simple case The Spider and if you
150:09 - want something more complex in the
150:10 - middleware so you've kind of got your
150:12 - three different ways of adding
150:15 - your user agents are headers in
150:19 - so now that we've removed that we don't
150:22 - need this user agents list up here
150:25 - we can remove that let's save it
150:28 - and
150:29 - we should be able to just run our spider
150:34 - again so
150:35 - Scrappy crawl
150:38 - and
150:39 - book Spider
150:42 - and if everything's gone to plan we
150:45 - should see I'll just close the spider
150:48 - straight away
150:49 - and scroll up so we can have a look and
150:52 - see to everything so it looks like all
150:53 - the data is coming back as it was before
150:56 - we don't expect anything to change there
150:58 - because we we're not going to be getting
151:00 - blocked by the books to scrape.com site
151:04 - anyway but if we scroll up I think we
151:07 - should see
151:09 - here you go so here is where we can see
151:12 - the all the new headers that are coming
151:15 - in
151:17 - so
151:19 - you can see the Chrome version there is
151:22 - different
151:24 - sometimes it looks like it's using Edge
151:26 - as well
151:28 - so you can see there's multiple
151:31 - different headers coming back
151:33 - and then they are being attached in with
151:37 - the process request
151:39 - so that all seems to be working fine
151:42 - just as we wanted to and we made sure
151:45 - that we had it enabled in the settings
151:48 - as well
151:50 - in the downloader middlewares so we have
151:52 - an enabled there
151:54 - as well as enabled up here and the
151:57 - number of
151:59 - user agents coming back from the API set
152:01 - here
152:02 - so everything is
152:05 - as it should be one other thing to note
152:07 - is that this robot's txt underscore a
152:12 - bay is set to true if we're starting to
152:14 - do more complex sites we would set this
152:17 - to false
152:18 - so every site or most sites have a
152:23 - robots.txt file
152:26 - which is usually one of the first things
152:28 - that a spider will look for so scrapey
152:31 - does this automatically every time it
152:33 - looks at a site it first goes off and
152:37 - checks does the site have a robot dot
152:39 - txt file and in that robots.txt file is
152:43 - usually specified things like the pages
152:46 - on the site is this site open to being
152:49 - scraped and if it's open to being
152:52 - scraped watch pages are not allowed to
152:55 - be scraped are what pages are allowed to
152:57 - be scraped now obviously any crawlers
153:00 - that go out crawl different websites
153:02 - don't have to obey this robots.txt file
153:06 - you know it's it's a piece of code it's
153:09 - going to go off and do what you tell us
153:10 - this is up to you to decide you know do
153:13 - you want your spider to obey it or not a
153:17 - lot of big sites will have like okay if
153:21 - you're a Google spider you're allowed
153:23 - crawl and scrape our data if you're not
153:26 - a Google spider don't scrape our data so
153:30 - if they have that in their robots.txt
153:32 - and you have it set to obey is equal to
153:35 - True your spider will go see it's not
153:38 - supposed to scrape the site and it'll
153:40 - shut down so if you're having issues and
153:43 - you have this set to True try setting it
153:45 - to false
153:46 - okay so now that we've gone through what
153:49 - robots.txt entails let's go and create
153:53 - our next middleware
153:56 - so this time instead of
153:58 - having middleware that just replaces the
154:00 - user agent every time we're going to
154:03 - create a new middleware so go to
154:05 - middlewares.py
154:07 - and this new middleware is going to
154:11 - create a new header every time using the
154:16 - data that it gets back from the fake
154:20 - browser header endpoint
154:23 - so I'm just going to paste in the code
154:25 - here and then talk through it for you
154:26 - guys
154:28 - so
154:31 - if we go up to the top we start off
154:33 - again with just a simple class name
154:36 - scrape UPS fake browser header agent
154:38 - middleware that can be whatever you want
154:42 - we pull in the settings that we need
154:46 - this will get everything from the
154:48 - settings file
154:49 - and then it kicks off get headers list
154:52 - which calls out to the API endpoint
154:56 - it does a get request here
154:58 - to the scrape UPS endpoint
155:01 - Returns the response converts it to Json
155:04 - and then we've got a list of headers
155:09 - and then in the process request which
155:12 - gets processed with every request
155:14 - we have this get random browser header
155:16 - function which will get the random
155:21 - browser header from the list that we
155:23 - just asked for from scrape UPS so then
155:26 - we have this random browser header and
155:29 - we can assign
155:30 - all the other headers not just user
155:33 - agent like we did in the middleware
155:34 - above but we can also modify all these
155:37 - other parameters in the header
155:40 - so you don't have to modify all of them
155:42 - you can modify certain ones again this
155:45 - is up to you to play around with and
155:46 - decide which ones you need which ones
155:48 - you don't need
155:49 - with scraping everything is really a
155:51 - case-by-case basis because every website
155:53 - is different
155:55 - but we're giving you here everything you
155:57 - need to play around with
155:59 - so you might find certain headers need
156:02 - to be modified more than others we have
156:05 - our request.headers
156:09 - being updated
156:10 - and then that is just like the one above
156:16 - everything we need
156:17 - let's go ahead now and add the settings
156:20 - that we need
156:22 - so we have some of the settings already
156:25 - set here so we have the browser endpoint
156:29 - there's a default set there we can set
156:31 - our fake browser header enabled to True
156:34 - here to make sure it runs and then the
156:38 - num requests is going to be the same as
156:42 - here it's already set and the API key is
156:46 - already set as well so again you get
156:48 - your own API key for that
156:51 - and I think we should be able to just go
156:55 - to settings now and make sure we have
156:58 - the middleware enabled so I'm copying
157:00 - the class name
157:02 - going to settings going down to
157:05 - downloader middlewares and where we had
157:08 - the fake user agent middleware I'm just
157:12 - going to overwrite that so we have our
157:14 - fake browser header agent middleware
157:16 - so I'm going to save that and then just
157:20 - go and run the spider again
157:24 - and it should work correctly
157:28 - so yeah there seems to be
157:31 - lots of books getting scraped
157:34 - so it seems to be working correctly I
157:36 - can stop it and just as we did before
157:40 - to double check that the
157:43 - headers are being
157:46 - set correctly
157:48 - we can just stick underneath
157:50 - a simple print statement
157:53 - that shows the headers are set
157:56 - to what we wanted them to be set to
158:00 - so
158:02 - if we run our spider it should show that
158:06 - there's multiple different headers
158:10 - so I'm stopping it again and looking at
158:12 - the output
158:14 - once we get past the book data
158:19 - okay so here there's some headers so new
158:22 - header attached which is what we have
158:23 - here and then
158:25 - we can see we have things like let's see
158:29 - accept
158:32 - the user agent so okay so we have
158:36 - the user agent Mozilla 5
158:41 - do we have accept
158:43 - so we have accept the text HTML
158:48 - everything we wanted there
158:51 - I'm just trying to see can we see in
158:54 - here that both of them are different to
158:56 - each other
158:57 - yeah so here for example you can see
159:00 - this user agent is using Chrome 103.0
159:05 - 50 60.134
159:08 - and up here it's using a different
159:11 - version of Chrome
159:13 - so it's using one zero three point zero
159:16 - point fifty sixty one one four
159:20 - so you can see it is
159:23 - changing in each request
159:26 - so that's exactly what we want to show
159:29 - you how to do
159:31 - so I think that concludes part eight
159:34 - in part nine we'll be going into how to
159:37 - use proxies
159:39 - to bypass the anti-bot blockers that
159:43 - websites have as well so instead of kind
159:45 - of handling everything ourselves and
159:47 - trying to bypass the antibots by
159:52 - updating our own headers we can see that
159:54 - there's commercial things out there for
159:57 - you to do that with and there's also
159:59 - things like proxy lists that are free to
160:03 - use as well and we look at how to
160:05 - integrate those into your scrapey
160:07 - project as well
160:09 - so see you in part nine guys
160:16 - so in part nine of the scrapey beginners
160:18 - course we're going to be looking at
160:19 - everything to do with proxies so we're
160:22 - going to be going through what are
160:24 - proxies and why do we need them and then
160:26 - we're going to be looking at the three
160:28 - most popular ways to integrate proxies
160:30 - into your projects
160:33 - so let's get started
160:35 - so in part 8 we're looking at user
160:38 - agents and requests and the headers we
160:41 - pass in when we're making the requests
160:44 - to the website you're looking to scrape
160:46 - we discussed and looked at how if you
160:49 - change your headers and change your user
160:53 - agents you can basically make it look as
160:56 - if you were multiple people accessing
160:59 - the website you're trying to scrape
161:01 - now there's one thing we also mentioned
161:04 - in part eight which is that the data
161:06 - that also is being transmitted with your
161:09 - request is usually your IP address
161:11 - so this IP address is it's your unique
161:15 - identifier
161:16 - and that's what is used to make sure the
161:19 - data comes back to your machine
161:22 - so every machine will have an IP address
161:24 - and that's how the requests get to and
161:28 - from your machine think of it as like
161:30 - your house as an address your computer
161:32 - must also need an address and this is
161:34 - your IP address so if we change the user
161:39 - agents every time when we're sending the
161:41 - requests that's fine but if we changing
161:43 - the user agents every time but we still
161:45 - have the same IP address then the site
161:47 - that we're scraping is very likely to
161:50 - know that we are the same machine that
161:54 - is requesting their data every time so
161:56 - they're very likely to block us straight
161:57 - away
161:58 - so that's why changing our IP address as
162:03 - well as our user agent and headers is
162:06 - very important so just the user agent
162:09 - and headers might work if it's not very
162:12 - sophisticated type of website that
162:14 - you're trying to scrape but if you're
162:15 - going to anything that's complex at all
162:18 - you will need to rotate your IP address
162:21 - and that's where proxies come into play
162:26 - so
162:28 - let's first off look at the first method
162:31 - is that we're going to be looking at is
162:34 - using proxy lists so these are lists of
162:38 - IP addresses and ports that belong to
162:42 - multiple different servers all over the
162:45 - world so there's lots of these machines
162:48 - that are available to bypass our
162:51 - requests will go via that machine before
162:54 - it goes to the website we're trying to
162:57 - scrape and then it'll come all the way
163:00 - back via that machine as well now
163:02 - there's pros and cons to every one of
163:05 - the of the three integration methods
163:07 - we're going to look at so the pros of
163:09 - proxy lists like this are that obviously
163:12 - the proxies that list that you can get
163:15 - online are last number free so like this
163:17 - one here
163:18 - freeproxylist.net you can go there and
163:21 - you can select from a list of different
163:23 - countries select your protocols and you
163:27 - can check the uptime there's also
163:30 - another list which is very handy on
163:33 - juno.com forward slash free proxy list
163:37 - here you also have IP address Port
163:40 - country uptime response Etc
163:44 - so here there is 9 000 proxies online in
163:49 - 136 countries but the downside is of
163:52 - using these lists is that because
163:53 - they're free so many people are using
163:56 - them that they're very likely to either
163:59 - have very poor response times and take
164:01 - very long time to actually root your
164:04 - traffic through them or else they can be
164:07 - already blacklisted because think of it
164:09 - if someone has already used them to
164:11 - scrape millions of pages from maybe the
164:13 - same site that you're going to look to
164:15 - scrape data from then there's a very
164:18 - high likelihood that that website could
164:21 - already have discovered this IP address
164:24 - and blocked it
164:26 - so the pro is that it's free the cons
164:28 - are that it can take a long time and
164:30 - there's a very high likelihood that if
164:33 - it's free that it's already been used
164:35 - too much and it could be blocked
164:38 - so we're going to go ahead anyway and
164:40 - try with a few of these IP addresses and
164:43 - a few ports from these two sites and the
164:47 - way we want to integrate them into our
164:48 - project is we're going to use this
164:51 - GitHub project which integrates with
164:54 - scrapey and it's called scrippy rotating
164:57 - proxies
164:58 - so we'll have a link to this
165:00 - available but you can just do pip
165:03 - install scrapey Dash rotating Dash
165:06 - proxies
165:08 - and go to your terminal
165:11 - and
165:12 - paste that in and run it
165:14 - now I already have it installed so it's
165:17 - going to say requirement already
165:18 - satisfied for me but for you guys you
165:20 - should see it installed in there
165:22 - now we're continuing on part nine from
165:27 - part eight so if you're looking for the
165:29 - code for where we are starting at now
165:31 - we'll have that available in a GitHub
165:33 - repo which we link to
165:35 - so you can continue on from here with us
165:38 - so now that we have that installed we
165:41 - can go ahead and
165:43 - we can add our proxy list in so as you
165:49 - probably guess everything is going to go
165:51 - into our settings file
165:53 - as with everything else that's part of
165:56 - our project
165:58 - and these are just dummy
166:02 - domain IP addresses
166:04 - but this is the idea so you can have
166:07 - this many you could have a hundred
166:09 - different IP address and ports in here
166:12 - but we're just going to put three or
166:14 - four
166:15 - just for the purpose of showing you how
166:18 - it works
166:19 - so let's go back to our free proxy list
166:23 - and take
166:26 - two of these guys
166:31 - so we want the IP address in the port
166:43 - now obviously depending on your use case
166:46 - you might need
166:47 - a
166:49 - and you might need a proxy from a
166:52 - specific country
166:53 - or you might need
166:56 - a proxy with a very good uptime or
166:59 - response time
167:02 - so that's for you guys to search in the
167:08 - search boxes on the site here
167:14 - okay so I've got three of them there
167:18 - and the next thing I want to do is
167:20 - enable the middleware so this project
167:24 - that I've just installed this scrapey
167:26 - rotating proxies will have installed a
167:29 - middleware
167:30 - but to actually make sure that the
167:32 - middleware works we need to add it to
167:35 - our downloader middlewares
167:37 - and that's where we can
167:39 - make sure it's enabled
167:42 - so I've just done gone ahead and done
167:44 - that there I've added them in so
167:48 - and as you can see I've left the other
167:51 - two middlewares that we had from part 8
167:53 - in here as well so obviously they don't
167:56 - have to be here I can also remove them
167:59 - but I might as well leave them in for
168:00 - now they're not going to do any harm
168:02 - they're just adding in
168:04 - a different request header
168:07 - so let's save that
168:10 - and then the other thing I wanted to
168:13 - quickly show you is that if you had all
168:16 - the proxies already in a file you could
168:20 - do something as simple as
168:22 - just saying the rotating proxy list path
168:25 - is equal to and then the path to your
168:28 - file obviously we don't have a file with
168:31 - a bunch of ips and ports but that's
168:35 - where you would put it if that's what
168:37 - you're wanting to do
168:39 - so let's just quickly remove that
168:42 - and now we can go ahead and we can run
168:45 - our spider and see the results so I'm
168:49 - just going to do scrippy crawl and the
168:51 - name of the spider
168:53 - I'm just going to make sure and in my
168:55 - project
168:57 - it's great big crawl
168:59 - the book Spider
169:03 - so it's going to go ahead and run you
169:05 - can see
169:06 - the header that was attached from part 8
169:10 - where we were adding the new header
169:13 - and now this can take a good bit of time
169:16 - so as you can see here this rotating
169:20 - proxies Dot middlewares
169:23 - has printed out that
169:26 - it has zero good proxies
169:29 - zero dead proxies and three unchecked
169:32 - proxies
169:33 - so that means that it's going to go
169:35 - ahead and it's first going to try and
169:37 - see can it actually send any requests
169:40 - through the proxies that we've listed
169:44 - here so this can take a good bit of time
169:46 - depending on the quality of the proxies
169:49 - that we've got obviously I have no idea
169:52 - how good the ones in that free list are
169:55 - because they change every day every hour
169:58 - there's new ones added and there's ones
170:00 - removed and then as soon as they're
170:02 - added they're being used by hundreds if
170:05 - not thousands of other users
170:08 - so
170:09 - this is the good thing about this
170:11 - middleware is that it checks cannot
170:14 - actually use it and as you can see here
170:16 - it's just retrying our books to scrape
170:20 - URL with another proxy because one of
170:23 - them failed to work
170:26 - so it's just a process of waiting so
170:29 - it's moved one of the proxies into our
170:32 - dead pool and it's still got two that it
170:36 - wants to check
170:37 - so this is just a process of waiting and
170:40 - letting the middleware do its work so
170:43 - I'm gonna leave that run for a few
170:45 - minutes and come back to it and we'll
170:47 - see did it actually manage to use any of
170:50 - those
170:51 - free proxies that were on that free
170:54 - proxy list
170:56 - so I'll come back in one second and
170:59 - we'll see where we are okay so I've just
171:02 - come back a few minutes later and
171:04 - it still hasn't managed to
171:07 - get any of our three proxies in the list
171:10 - to work
171:11 - he's got two dead now one it's trying to
171:15 - reanimate
171:16 - it's not looking good so obviously the
171:21 - ones I've picked were probably already
171:22 - overly used already could be blocked by
171:26 - the site we're trying to scrape
171:28 - so here you can see obviously the major
171:31 - disadvantage of using free proxy lists
171:34 - online now there's lots of different
171:36 - places to get them so depending on your
171:39 - source of the proxies you may have much
171:42 - better luck at getting them to work but
171:45 - it's really a process of trial and error
171:46 - and while it's free it can be painful to
171:50 - actually get up and running consistently
171:53 - and correctly
171:55 - so
171:57 - we've had a look at how we can just
171:59 - plug in a bunch of different IP
172:02 - addresses and ports into our rotating
172:04 - proxy list and how we can use this
172:07 - middleware to use our proxies in our
172:12 - scrapey project but another way we can
172:15 - do this is
172:17 - using a proxy port so what we would be
172:21 - using is a service which is provided by
172:25 - a proxy provider
172:27 - and they would give us a proxy IP
172:32 - address and port and they would handle
172:36 - changing the IP address every time and
172:40 - we wouldn't have to worry about
172:41 - compiling a list of proxies ourselves so
172:45 - that's what we're going to look at next
172:47 - so we can still be looking after our own
172:51 - user agents and our own headers but the
172:56 - proxy provider would be dealing with
172:58 - everything to do with rotating a proxy
173:01 - list for us and making sure that the
173:03 - proxy list is of good quality and
173:07 - available all the time so we wouldn't
173:10 - have to worry about that now there's
173:12 - lots of them out there and we're just
173:14 - going to look at one of them now
173:18 - the one I've just going to show you now
173:21 - is called smart proxy you can check them
173:25 - out at smartproxy.com
173:27 - and as they say effortlessly scrape web
173:31 - data you need so they've got some great
173:33 - deals and offers and as you can see they
173:38 - you know do things like bypassing
173:39 - captures IP bands they've got millions
173:42 - of proxies from millions of locations
173:45 - and the plans they do entail residential
173:50 - and data center proxies so we haven't
173:53 - talked about that yet but residential
173:55 - proxies would be basically the data will
173:59 - be forwarded through residential IP
174:02 - addresses so these are IP addresses that
174:04 - are mainly used by people's homes so
174:08 - think of it if someone is watching
174:10 - Netflix and browsing Facebook and
174:14 - looking at Google search and then
174:17 - one or two of your requests are going
174:20 - via that IP address then the website
174:24 - you're trying to scrape let's say Amazon
174:26 - is going to say oh well I saw that IP
174:29 - address yesterday they just bought
174:30 - something from me so they're much more
174:32 - likely just to let that request go
174:35 - through without any issue so that would
174:38 - be what residential proxies are then
174:40 - data center proxies would be think of
174:43 - you know your traditional data centers
174:44 - with thousands of servers in a big room
174:48 - and your requests will be routed through
174:51 - a Data Center and through the IP address
174:53 - that is belonging to one of the machines
174:56 - in the data center
174:58 - so you have access to a lot more IP
175:02 - addresses in the residential side but
175:05 - then the data center side are much
175:07 - quicker and there tends to be not as
175:09 - many data limits and they tend to be a
175:11 - bit cheaper as well so that's the
175:14 - difference between residential and data
175:17 - center proxies so you can sign up with
175:20 - them most proxy providers also give you
175:23 - a week or two of a free trial or a
175:27 - certain amount of free credits that you
175:28 - can use to test out their service so if
175:32 - you guys go ahead you can click get
175:34 - started there sign up for an account and
175:36 - then once you're logged in
175:40 - if you go to the residential tab
175:43 - because we're going to be using
175:44 - residential proxies for our next part
175:48 - now so click the residential Tab and
175:51 - then you can either check out a pay as
175:53 - you go plan where you pay per gigabytes
175:55 - of data that's transferred or you can go
175:58 - into regular or Enterprise as well so I
176:01 - already have a plan set up with them so
176:04 - I'm going to go directly to the proxy
176:06 - setup next
176:07 - and this is where we will get our
176:10 - details which we'll then put into our
176:13 - spider first off we want to generate our
176:16 - username and password so we can put in
176:19 - any kind of combination of
176:23 - letters and numbers here a password
176:26 - and click create and it creates a
176:29 - username and password once you have your
176:31 - username and password you can grab these
176:36 - and put those into the username and
176:39 - password field here then our proxy
176:43 - location if it's important for your
176:46 - spider that you are scraping from a
176:48 - certain country for example if you're
176:51 - scraping an e-commerce site that will
176:54 - only show you specific products if
176:57 - you're living in a certain country then
176:59 - it is important to select the country
177:01 - here from this list so for us it doesn't
177:04 - matter so we can leave it at random
177:07 - then for the session type we want
177:10 - rotating because we don't want a fixed
177:13 - session every request can come from a
177:17 - different IP address and that doesn't
177:20 - matter for us right now and for the
177:23 - output format we're just going to pick
177:26 - http
177:27 - so that's going to then give us this
177:30 - string here which we can copy and use in
177:34 - our project so this is the endpoint
177:37 - where we're going to send our requests
177:40 - to Smart proxy is going to handle all
177:43 - the IP address rotation and all that
177:46 - stuff and it's going to then send us
177:49 - back the response from the website we
177:51 - are trying to scrape
177:53 - now that we have our endpoint
177:56 - from Smart proxy the next thing we want
177:58 - to do is go back to our project
178:02 - we want to disable the middleware we
178:05 - were using because that will no longer
178:07 - be needed because
178:09 - smart proxy is going to be looking after
178:11 - rotating our proxies and it's going to
178:15 - be looking after some band detection
178:17 - stuff as well
178:18 - so we can disable the two of them
178:21 - and the next thing we can do is go to
178:24 - our spider
178:26 - and we can go to where we have
178:30 - response.follow and in here we'll simply
178:34 - add in
178:36 - one more field which
178:39 - is going to be meta and then the proxy
178:44 - information so matter is equal to
178:47 - proxy and then our proxy details will go
178:51 - in here
178:52 - so I can go back
178:55 - grab my proxy endpoint and paste it in
178:59 - here
179:01 - so
179:03 - that looks correct
179:06 - and then I can also copy this
179:10 - and put it down where we also have
179:15 - response dot follow
179:17 - below
179:19 - so I'll add it in here too
179:22 - and
179:24 - that should be the two main places I
179:26 - needed for now
179:28 - the other example which I'll show you in
179:30 - a second is that we can create a custom
179:32 - middleware which would insert the
179:36 - endpoint and as well so we'll do that
179:39 - once we get this to run correctly
179:42 - we can now do Scrapy crawl book spider
179:45 - and it should work for us so Scrapy
179:49 - crawl book Spider
179:52 - and hopefully we have no issues
179:57 - as I can see there's some things coming
179:59 - through
180:00 - I can go to my books data.json
180:05 - and I can see that there is
180:08 - the data coming through so it looks like
180:11 - it's working correctly and it's all
180:13 - going via
180:14 - the
180:16 - smart proxy endpoint so I can
180:18 - close down my spider and the next thing
180:21 - we can do is we can create a custom
180:23 - middleware version so just adding it in
180:27 - to our
180:29 - meta
180:31 - value here and adding in our proxy
180:33 - endpoint is fine if you've got a small
180:35 - project
180:37 - but if you've got a larger project it
180:39 - probably makes more sense just to make a
180:40 - custom middleware for it so I'm going to
180:43 - show you how to do that next
180:45 - so we'll scroll down to the bottom
180:47 - because we have our other middle words
180:50 - in here already
180:52 - and we'll create a new middleware
180:56 - where we will be adding our endpoint
180:59 - details
181:01 - so
181:03 - we just make a new class called my proxy
181:06 - middleware
181:07 - it's going to again pull in our crawler
181:11 - settings and then it's going to get our
181:13 - proxy user proxy password proxy endpoint
181:16 - and proxy port from our settings so we
181:19 - need to go ahead and set those in our
181:21 - settings
181:22 - and then once it's got those it makes
181:25 - the user credentials it puts those
181:28 - credentials into
181:30 - a proxy authorization header for the
181:33 - request
181:34 - and then it has the URL which is made
181:39 - with the endpoint and port
181:41 - and that then goes into the request dot
181:46 - meta
181:48 - so let's go ahead now and in our
181:51 - settings fill out our proxy user
181:53 - password endpoint and port
181:56 - so let me just go here
182:00 - and add them in now
182:04 - so username password endpoint and Port
182:07 - so I just need to change the password
182:10 - obviously your username and password are
182:12 - going to be whatever you guys have made
182:15 - in your own dashboards with smart proxy
182:19 - I'm just copying my details from here
182:24 - and that looks fine so I should be able
182:27 - to save that
182:29 - and the next thing I need to do is to
182:33 - make sure my middleware is enabled so
182:36 - again
182:37 - going to my
182:39 - downloader middlewares
182:41 - and I'm going to add in my
182:45 - new
182:47 - middleware
182:49 - so I can add that in there and save it
182:53 - the next thing we want to do is try and
182:56 - run our spider again and see does it
182:58 - work but we will obviously remove
183:02 - what we did a second ago
183:04 - so that we can show it's all going via
183:08 - r
183:09 - new middleware
183:13 - so let's just
183:15 - remove that
183:17 - save it
183:20 - and then
183:21 - try run or spider again
183:25 - and it looks like the
183:28 - book details are coming through again so
183:31 - I'm just going to close my spider
183:35 - now we can have a look at smart proxy
183:38 - and see
183:40 - the
183:41 - traffic usage
183:43 - and as you can see
183:46 - weave requests coming through
183:51 - so there you go
183:54 - we've got the user
184:00 - and we've got our
184:03 - usage by gigabyte
184:05 - so
184:07 - it's working just
184:09 - as we wanted it to work
184:12 - our requests are going through the
184:15 - inspired proxy endpoint smart proxy is
184:17 - looking after the IP address rotation
184:20 - and it is sending us back the request
184:24 - then scrapey is able to take the
184:27 - information out of the HTML and we have
184:31 - the data that we need
184:33 - I think that's given you a very good
184:35 - overview of how
184:37 - we would use proxy port endpoints
184:42 - so there's just one last thing I wanted
184:46 - to show you guys which is
184:47 - proxy API endpoints
184:50 - so this is if you want to go just a step
184:52 - further and not have to deal with
184:56 - the browser headers are the user agents
185:00 - or any things like that and maybe you
185:03 - are scraping something which requires a
185:06 - headless browser to do JavaScript
185:08 - running for you
185:11 - we can get that by using a proxy API so
185:16 - again it's a service where there's an
185:18 - endpoint we're sending our request
185:20 - through that service and then that
185:23 - service is making sure that certain
185:26 - things are enabled to make sure that the
185:29 - request gets us the page data
185:34 - so what we're going to do is we are
185:37 - going to
185:38 - show you how to use that now
185:42 - and that is also going to be a paid
185:45 - service
185:46 - and you can sign up for that by going to
185:51 - scrapeups.io clicking get free account
185:55 - signing up for it you've got
185:58 - a thousand free credits there and if you
186:01 - then once you're logged in
186:03 - go to the proxy aggregator page go to
186:08 - the request Builder
186:09 - and you then have an API
186:13 - key which you can use and you've got the
186:18 - proxy endpoint which you can use as well
186:21 - in your spider
186:23 - so once you've got your API key we can
186:26 - move back to our book Spider
186:29 - file and we will start adding in a new
186:33 - function which will help us send the
186:37 - traffic first to our new proxy provider
186:41 - so this new function is going to be
186:43 - called get underscore proxy underscore
186:46 - URL and we're going to pass in a URL to
186:49 - that function
186:51 - and then we're going to have an API key
186:54 - as part of this payload object and
186:58 - we're obviously going to put in
187:00 - an API key where we
187:02 - have our own API key that we got from
187:05 - scrape UPS so I'm going to add mine in
187:07 - quickly now
187:08 - I'm just going to copy and paste that in
187:13 - and
187:14 - that then is going to slot in here this
187:17 - payload is going to get URL encoded so I
187:20 - need to import this URL encode from URL
187:24 - lib and then it's going to create this
187:27 - new proxy URL and then it's going to
187:30 - return that proxy URL so this function
187:34 - is going to
187:36 - get the URL of the site that we want to
187:39 - scrape and it's going to encode it along
187:41 - with our API key and it's going to send
187:43 - it to this API endpoint
187:47 - once we've got that function created the
187:50 - next thing we want to do is we want to
187:52 - add it in
187:54 - to where we use
187:58 - our current scrapey Dash request
188:00 - function so we will have URL is equal to
188:07 - and then get proxy URL and then the same
188:12 - down here
188:14 - we'll be doing get proxy URL
188:17 - with our next page URL to and the only
188:22 - other thing we need to add in now
188:25 - is
188:26 - a new function called start requests
188:31 - so we'll add this in under our custom
188:33 - settings
188:35 - and
188:39 - I'll explain now what this does so
188:41 - scrapey looks for this function when you
188:44 - start up your spider if you don't have
188:46 - it it doesn't need it to run it'll work
188:49 - off of your start URLs list here but if
188:54 - you do have it it will go in and work
188:58 - off of what you have in here
189:00 - so what I've asked it to do is I'm
189:03 - saying okay when the spider starts up
189:06 - run this function
189:08 - and inside this function run our guess
189:12 - proxy URL the same as we do down here
189:15 - because we want the very first URL to
189:18 - also go to our proxy so if we didn't
189:21 - have this function in here what would
189:24 - happen is that the very first URL would
189:27 - actually not be sent to our proxy
189:30 - provider endpoint URL so that would mean
189:34 - that there's a chance that the first
189:35 - very first request would get blocked
189:38 - so that's why we have this function so
189:41 - that the very first URL is properly
189:44 - encoded and sent off using this get
189:47 - proxy URL and once so that's where we
189:50 - have start URLs and we're taking the
189:52 - first string inside of our start urls
189:56 - and then we're doing the Callback is
189:58 - going to be our parse function and then
190:01 - it's going to go on and it's going to
190:03 - work perfectly because it'll be going
190:06 - through get proxy URL for the rest of
190:08 - the
190:09 - requests as well
190:12 - so that takes care of the very first
190:15 - call and this get proxy URL function get
190:19 - takes care of making sure that all the
190:22 - requests are going to go via this proxy
190:25 - API endpoint so then the request will
190:30 - come back with the response and the
190:33 - response will be able to be parsed in
190:36 - our parse book page like it was before
190:40 - so we should be able to go ahead and run
190:42 - that
190:43 - if we just do scrapey crawl book Spider
190:46 - again
190:49 - and
190:50 - run this
190:53 - oh there's one other thing so it just
190:56 - did one request and stopped straight
190:58 - away and that's because it's a very easy
191:01 - mistake to make the allowed domains does
191:04 - not contain our proxy dot scrapups.io so
191:09 - let's just add that in
191:12 - and
191:14 - if we rerun it it should
191:18 - there you go so we just close our spider
191:22 - and I'll show you that we have all the
191:25 - data is coming through so we've got our
191:26 - product type books
191:29 - the title
191:31 - the description it's all there
191:33 - so that's working perfectly
191:36 - it's going via our proxy API endpoint
191:40 - and the next I want to do is I want to
191:43 - show you guys how instead of integrating
191:45 - this directly into our spider we can use
191:48 - a proxy middleware
191:50 - that's been created especially by scrape
191:53 - UPS so we can just quickly pip install
191:55 - it and it makes things a bit easier if
191:59 - you are adding it to a project and you
192:01 - don't want to have to add this special
192:04 - get proxy URL function
192:07 - so what we would do in this case is
192:10 - we would just pip install and then our
192:15 - new python module scrape up stash
192:19 - scrapey Dash proxy SDK and install that
192:25 - and then the next thing we would want to
192:28 - do is we would go to our settings and
192:31 - like we always do add more settings
192:34 - so let's just go down here
192:38 - and add in the settings we want so it
192:40 - would be again our API key
192:43 - if the scriptop's proxy is enabled yes
192:46 - and we'd be adding this line to our
192:49 - downloader middleware
192:51 - so I already have downloader middlewares
192:55 - so I'm just going to add it here
192:59 - so if I save that
193:03 - and I add my API key
193:07 - from here into my settings
193:12 - okay so I just wanted to hear perfect
193:16 - and I should just remove
193:21 - the get proxy URL
193:25 - from the places that we've been using
193:28 - that function because we don't need it
193:30 - anymore
193:33 - and then it should run fine going
193:36 - through our
193:39 - middleware
193:42 - so let's try and run that one more time
193:45 - and see does it work going by the scrape
193:49 - UPS proxy middleware
193:55 - and it looks like
193:57 - you have lots of requests going through
194:01 - so if I just cancel that
194:04 - and we can check
194:08 - in our
194:10 - dashboard
194:13 - if we have
194:15 - so we have 123 requests so that looks
194:20 - like they all went to our books to
194:23 - scrape.com site
194:25 - so we have 123 requests made so it's
194:28 - working like it should
194:31 - so that makes it very easy if you want
194:33 - to get started with it all you need to
194:34 - do is do that pip install
194:37 - and scrape UPS Scrappy Dash proxy SDK
194:41 - add to
194:42 - the two lines in here and the one line
194:45 - into your downloader middlewares
194:47 - and then it will just send all your URLs
194:53 - via the scrape UPS proxy endpoint
194:57 - now obviously you can make your own
194:59 - custom download or middleware as well
195:01 - and like we did for our smart proxy
195:06 - example
195:07 - that might be a bit more complex than
195:10 - needs be for for this because there's
195:13 - already the middleware that you can just
195:15 - pip install
195:17 - so we will leave the example with our
195:21 - downloader middleware in our article so
195:24 - if you want to check out a long version
195:26 - of how to implement the downloader
195:28 - middleware using the scrape UPS proxy
195:32 - API endpoint we'll have that in our
195:34 - article and you can copy and paste that
195:36 - into your code and play around with that
195:40 - so there's just one other thing if you
195:43 - wanted to add some more and
195:47 - functionality to the scrape-ups proxy
195:50 - endpoint you could add in
195:53 - for example the following so scrape Ops
195:56 - underscore proxy underscore settings is
195:58 - equal to Country us so this would send
196:03 - all the traffic
196:05 - via US IP addresses so
196:09 - if you're for example scraping an
196:12 - e-commerce website that needed to be
196:13 - only loaded via the US so we'd show us
196:16 - items only you would do something like
196:19 - this country
196:21 - us in your scriptops underscore proxy
196:24 - underscore settings
196:25 - they also have other functions such as
196:28 - you can pass in if you wanted to be the
196:30 - page to be JavaScript rendered and
196:33 - there's many other different parameters
196:35 - that you can pass in which will mean
196:37 - that certain things are switched on on
196:40 - your proxy provider site
196:42 - so that way instead of you having to do
196:45 - all this stuff on your side the proxy
196:48 - provider will take care of it as long as
196:50 - you pass in the correct parameters
196:53 - and each proxy provider will have their
196:56 - own page with all the different
196:58 - parameters that they allow you to pass
196:59 - in to them
197:02 - okay so that's everything I wanted to
197:04 - cover in
197:06 - part nine
197:07 - in part 10 11 and 12 we'll be looking at
197:11 - how you deploy so basically get your
197:16 - spiders to run on a on a server in the
197:19 - cloud
197:20 - so how you deploy the code to your
197:22 - server in the cloud and then how you can
197:24 - schedule and run your spiders to scrape
197:29 - at certain times of the day or the week
197:31 - so you can collect data on a periodic
197:34 - basis without having to have everything
197:36 - running off of your home so we're going
197:38 - to go through some different options
197:40 - there what's available we're going to
197:42 - look at some open sourced options free
197:45 - options and paid options and just have a
197:48 - look at the different uis and give you a
197:52 - bunch of different options and we'll go
197:53 - through the pros and cons of why you
197:56 - should pick one service over another
197:58 - service and we look at how complex and
198:01 - easy they are to use as well so that's
198:04 - what we'll be doing in the next three
198:06 - sections okay
198:09 - see you in the next part 10 News
198:16 - so in part 10 we're going to look at the
198:19 - different tools we can use to deploy and
198:22 - schedule our spiders online and the
198:25 - tools we can use to monitor how well our
198:28 - jobs are doing how much data is being
198:30 - scraped and if we're missing any Pages
198:32 - or items when we are actually running
198:35 - our spiders
198:37 - so you might be asking yourselves what
198:39 - is this deployment in scheduling so
198:42 - deployment is basically us putting the
198:45 - spider that we've just created onto a
198:49 - server that's always going to be online
198:50 - so that we don't have to have our own
198:52 - machine our own laptop or computer
198:55 - running 24 7 at home we can just put
198:58 - that onto a virtual machine somewhere on
199:00 - cloud and then we can actually schedule
199:02 - it to run at a certain point of time
199:05 - once a week once a day once every hour
199:07 - once every minute however often we want
199:10 - to run our spider to collect the data
199:13 - so that's the deployment is the act of
199:15 - getting the spider on the machine and
199:19 - the scheduling is scheduling to run at a
199:22 - certain time of day or time of week and
199:25 - then the monitoring is just seeing how
199:27 - well our scraping is doing either seeing
199:31 - have the jobs actually completed did the
199:33 - Spider Run correctly did the Spider Run
199:35 - for the correct amount of time that we
199:36 - thought it was going to run for did it
199:38 - get all the pages that we thought it
199:41 - should be getting obviously if you see
199:43 - Zero Pages script you know there's an
199:45 - issue
199:46 - so that's where the monitoring comes in
199:48 - and it's very important that we do have
199:50 - some monitoring setup because obviously
199:52 - if you don't your spider can be running
199:55 - every day and you could be missing huge
199:57 - amounts of data
199:59 - so that's deployment scheduling and
200:01 - Mantra that's the first part we're going
200:04 - to do in this part 10 is we're just
200:06 - going to look at the different tools
200:07 - available we look at free tools open
200:09 - source tools and paid tools
200:11 - so the first is square PD which is free
200:14 - and open sourced anyone can download it
200:17 - and contribute to it as well on GitHub
200:20 - so the pros of this are it's obviously
200:24 - free and open source there's
200:25 - plenty of third-party libraries for it
200:28 - as well there is optional uis from
200:31 - different providers
200:33 - and the downsides to it are things like
200:37 - you need your own server ideally because
200:40 - if you're running it on your own
200:41 - computer or laptop you would have to
200:44 - have your computer laptop online at all
200:46 - times if you want to have this for
200:49 - example running something every day at a
200:52 - set time
200:53 - so it also doesn't actually have a
200:55 - scheduler so some of the other tools we
200:58 - look at such as using scraps or scrapey
201:01 - Cloud there with those tools you can
201:03 - actually set a scheduled job to run at a
201:08 - specific time every day
201:10 - but with Scrapy D you'd have to use this
201:12 - Cron job to hit the API endpoint to get
201:16 - scrapey to run your job at a specific
201:19 - time so Scrapy D is good because it's
201:22 - free and open sourced but the downside
201:24 - is there's a bit more configuration it's
201:27 - a bit harder to install but we'll show
201:29 - you exactly how to install it if you
201:31 - want to install it
201:32 - so the second option just using scrape
201:35 - Ops to deploy a schedule and monitor
201:38 - your jobs the upsides of data is it's
201:42 - got a good a UI interface to use
201:45 - simple to use and understand it's got
201:48 - built-in monitoring for your jobs and
201:51 - spiders it's easy to schedule stuff but
201:54 - the downside would be you'd need your
201:56 - own server as well like with square PD
201:59 - and the third option with scrapey cloud
202:02 - is it's a paid service they have a
202:05 - freemium kind of version so you can just
202:07 - check it out if you want with that it's
202:10 - easy to set up you can just download
202:12 - their CLI tool use that to deploy your
202:16 - spider into their Scrappy cloud service
202:19 - and then once it's deployed there you
202:21 - can quickly and easily run it and you
202:24 - don't need to have your own server set
202:26 - up with another third-party provider
202:28 - so they're the three main options we're
202:31 - going to look at
202:32 - scrapeyd
202:33 - scrape ups and Scrappy cloud
202:36 - so let's first off have a look at
202:39 - scrapeyd
202:40 - and then we look at two different UI
202:44 - dashboards that we can install so we
202:46 - don't have to control everything using
202:48 - their API endpoints because well that
202:51 - can be useful for some people most
202:53 - people want to interact with their
202:55 - spiders and run them and schedule them
202:58 - using a nice front-end UI
203:02 - okay so Scrapy D is available to
203:06 - download as I said we need a third-party
203:09 - server set up first to install scrapeyd
203:13 - on so we're going to go ahead and create
203:15 - that with digitalocean now so
203:19 - digitalocean is a server provider which
203:22 - enables you to quickly set up virtual
203:25 - machines and then install everything you
203:27 - need on them so you can also use any
203:30 - other VM provider such as
203:33 - vulture for example these are another
203:35 - good provider and they have very cheap
203:38 - servers as well
203:40 - so
203:42 - if you go off and create your own
203:45 - account with vulture or digitalocean or
203:48 - AWS go off create your virtual machine
203:51 - I'm going to do that right now with
203:54 - digitalocean and if you want to use this
203:56 - you can just follow the steps that I'm
203:58 - using so you just log in
204:01 - go up to the create click droplets
204:04 - select the country or region you want to
204:08 - select it usually works best when you
204:10 - select a region that you're close to
204:13 - select Ubuntu for the operating system
204:16 - version 22.10
204:20 - we can select
204:21 - the cheapest virtual machine they have
204:24 - available which if you just click basic
204:26 - and then go to regular for the SSD type
204:29 - and then they have a four dollar a month
204:32 - server there
204:34 - so once you've selected the server you
204:36 - want
204:37 - you can either add an SSH key or a
204:40 - password to log in that's not that
204:42 - important now because for this you can
204:44 - also log in Via their console which can
204:48 - be accessed via the browser so that's
204:49 - how we're going to do everything now
204:51 - we're just going to use the browser to
204:53 - log into the machine and install
204:55 - everything we need makes it very simple
204:57 - and easy to use
204:59 - so that's all we need to do
205:01 - once we get to the bottom we just click
205:04 - create droplet and that will go ahead
205:06 - and create the droplet for us
205:08 - the droplet is just their term for
205:10 - virtual machine I can see it's
205:13 - creating
205:15 - so I just give it a minute or two to
205:17 - finish creating and then we can access
205:19 - the console over here
205:22 - so as you can see the droplets being
205:24 - created now
205:25 - we can click the console button which
205:28 - will open up
205:29 - a new window for us where we can access
205:32 - the console and type in all the
205:35 - instructions to get everything installed
205:37 - correctly
205:39 - so as you can see we're logged in to the
205:42 - virtual machine and now we can start
205:44 - running our commands
205:46 - so first things first is we want to run
205:49 - sudo apt update that just updates all
205:53 - the packages on the machine to make sure
205:55 - everything we install will be the most
205:58 - up-to-date versions of things
206:02 - so we'll give that a second to run
206:06 - that's finished and the next thing we
206:08 - want is to install python pip so we can
206:11 - pip install all the packages we need for
206:15 - python
206:17 - so that's just sudo
206:20 - apt install Python 3 Dash pip
206:24 - this Command right here
206:26 - we'll have all these commands easily
206:28 - available for you to copy and paste from
206:31 - our article as well so you don't have to
206:34 - be pausing the video at every point in
206:37 - time
206:39 - Okay so ill office you ask do you want
206:42 - to install this x amount of space will
206:45 - be used we're just going to say yes
206:49 - and sometimes it'll ask as well to
206:51 - restart certain Services which we can
206:53 - also say yes to
206:55 - so while that installs I'm just going to
206:58 - show you the project that we're going to
207:00 - be using so the project is the part 6
207:04 - code so obviously you may or may not
207:08 - have done that part if you haven't you
207:10 - can just git clone our project from here
207:13 - and just type git space clone space this
207:18 - URL so that this is the project that
207:20 - we're going to be using from part six of
207:22 - this course
207:25 - okay so let's see is
207:27 - wait it's just asking us to restart the
207:30 - services
207:31 - and now we'll go ahead and get clone our
207:34 - project so as I said it's just git clone
207:37 - and then the free code Camp Dash part-6
207:44 - so that's installed
207:46 - we'll now just see the into our project
207:51 - and we will install a virtual
207:54 - environment
207:56 - using pip install virtualenf
208:00 - so that's after installing virtual EnV
208:02 - now we can actually create our own
208:05 - VN folder where all the python packages
208:09 - can be installed into
208:11 - so we'll do that with virtual ends vnf
208:15 - and as you can see this folder has been
208:18 - created
208:19 - so now we just need to activate it
208:23 - so we do that with Source vnf bin
208:25 - activate
208:28 - it's activated now
208:29 - and now we can install the project
208:33 - requirements
208:34 - so
208:36 - this requirements.txt file contains all
208:41 - a list of all the things we need to get
208:43 - this project running so we can just do
208:45 - pip install space Dash r space
208:49 - requirements.txt and it'll go ahead and
208:53 - install all the
208:55 - packages that are needed to run the
208:58 - project correctly
209:00 - so we just
209:01 - give it a minute or two to install
209:03 - everything and then we should be able to
209:05 - run our scrippy spider
209:08 - next thing we do we just CD into our
209:11 - book scraper
209:13 - and we'll see if we can run scrapey list
209:19 - so that ran correctly
209:22 - and now we can run scrapey crawl book
209:26 - Spider
209:27 - so if we run that
209:32 - should see Scrapy starting up and as you
209:34 - can see
209:35 - all our pages are being scraped
209:39 - so our pages are being scraped and the
209:41 - data is being extracted from the page
209:43 - just like we were doing in part six
209:47 - so that's perfect we don't have to wait
209:50 - for that to run and complete the next
209:52 - thing we're going to look at is how we
209:55 - can install scrape PD
209:57 - okay so we just pip install scrapeyd
210:01 - to install that
210:06 - and then the next thing we can do is
210:09 - just run scrapeyd so to do that it's
210:13 - just Scrapy D now I've added on a bit
210:16 - extra after it just Scrapy D because I
210:19 - want all the output that usually gets
210:23 - displayed to the screen to go into this
210:25 - scrapeyd logs.txt file
210:28 - so put all the logs into scripty
210:31 - logs.txt and run this command in the
210:35 - background
210:36 - so we can go ahead and do that and now
210:40 - let's check that it's actually up and
210:42 - running so to do that we will be using
210:45 - curl to Ping the Damon status dot Json
210:50 - endpoint that lets us know if Scrappy D
210:55 - is running correctly so when we run that
210:58 - command
210:59 - it says status is okay there is zero
211:03 - jobs pending zero jobs running zero jobs
211:06 - finished because obviously we've just
211:09 - ran Square PD we haven't run any spiders
211:11 - yet
211:13 - we have scripted setup
211:15 - we can hit the end point using curl
211:19 - the next thing we want to do is we want
211:21 - to package up our spider
211:24 - and then deploy it to scrapeyd because
211:28 - if we don't do that square PD will not
211:31 - have access to our project and will not
211:33 - be able to run the spider
211:35 - so to do that we can install scrapeyd
211:39 - client
211:40 - so again using pip we just do pip
211:43 - install
211:44 - Scrapy D client
211:48 - that will go off and install the
211:50 - scrapeyd client
211:52 - the next thing we need to do is we need
211:54 - to
211:56 - go into our
211:58 - scrapey.cfg file
212:01 - so that should be
212:05 - here this guy
212:07 - scrapey.cfg so we want to edit that we
212:10 - can use
212:12 - Vim or VI
212:16 - and we can
212:18 - all we need to do for this is just
212:21 - uncomment out this line so that it
212:24 - deploys it to
212:25 - Scrapy D which is running on localhost
212:29 - Port 6800
212:33 - so it's handy it's already there all we
212:35 - have to do is go in
212:37 - comment that out
212:39 - so to save this
212:42 - we just I'll show you one second
212:45 - we just type in
212:48 - the double dots to get up
212:51 - so we can actually save it correctly and
212:54 - then WQ exclamation
212:58 - enter and that saves it
213:00 - so
213:02 - now that it's saved
213:03 - we can do a scrape Ed deploy
213:09 - and default is the name I've just picked
213:13 - for the project
213:14 - because scrapeyd works with the concept
213:18 - of projects
213:19 - so it needs a project name and then as
213:22 - you can see it's deployed okay
213:25 - and it now has
213:27 - one spider available so we can now
213:31 - go ahead and run our spider right now
213:35 - again using curl so curl it's going to
213:38 - hit this localhost
213:40 - 6800 port and the forward slash
213:45 - schedule.json endpoint
213:48 - we're adding the project name of default
213:51 - and the spider name of book Spider
213:54 - so because we've deployed it we should
213:57 - be able to run this
213:59 - and
214:00 - it comes back with a job ID if it has
214:04 - run correctly so it said status okay and
214:08 - we're giving you back a job ID to show
214:10 - that the job has been started now this
214:14 - doesn't mean that it finished running so
214:15 - if there is ever issues sometimes you
214:19 - need to do further investigation
214:22 - so
214:24 - we've gone through how to use Scrapy D
214:27 - and Scrapy D deploy to package up and
214:31 - deploy our spider to scrape Ed and then
214:34 - how we can use Curl to schedule our
214:38 - spider using the curl command followed
214:42 - by the schedule
214:44 - endpoint so obviously if you wanted to
214:47 - just run this yourself you could just
214:50 - set up a cron and using your crown you
214:54 - could say schedule this to run every day
214:57 - at whatever time you want and then you
215:00 - would just be running this command
215:03 - obviously we want to make this easier
215:06 - for people to use
215:07 - so now we're going to look at the two
215:10 - dashboards that we can install
215:12 - the first being Scrappy dweb
215:15 - and The Following being the scrape pups
215:18 - Scrapy D integration
215:20 - so
215:21 - for scrape edweb scripted web is also a
215:25 - third party open sourced application
215:28 - that you can install
215:30 - and we're going to go ahead now and
215:32 - install that
215:34 - so I'm just going to go back up to the
215:38 - top level
215:39 - that we were at and
215:41 - just pip install
215:44 - and then the Scrapy d-web
215:47 - so that's going to go ahead and install
215:49 - that
215:52 - now
215:54 - it may need a certain
215:57 - specific version of specific packages to
216:00 - be installed so I've gone ahead and
216:03 - found out that
216:05 - when I was making this video that four
216:08 - specific packages needs to be installed
216:10 - with a specific version number are the
216:13 - installation wouldn't go and work
216:14 - correctly on the version that of Ubuntu
216:17 - operating system that we're using right
216:18 - now
216:19 - so it's easy enough all we're going to
216:22 - do is specify
216:24 - for four different packages the version
216:26 - that we need
216:28 - so the first one is just flask SQL
216:31 - Alchemy
216:33 - so we're just going to pip install that
216:34 - specific version
216:36 - the next is
216:39 - we're going to do pip install SQL
216:41 - Alchemy
216:44 - then we're just going to install a
216:47 - specific version of flask
216:50 - and finally we're going to pip install a
216:55 - specific version of work zoic
216:58 - once those are all installed we can now
217:01 - check and see the scripted web
217:04 - run correctly so we need to do is type
217:06 - in scrippity web
217:08 - okay it's giving us an issue is it
217:12 - let's just try and rerun
217:15 - scripted web again
217:18 - yeah so this time it ran correctly so
217:24 - I think it just needed to create the
217:27 - settings file initially
217:30 - so you can you know it's running
217:33 - correctly when it stays up and you have
217:36 - the URLs where you can access it showing
217:40 - here so we can just go ahead
217:43 - and copy the URL that it has given us
217:47 - so this is the IP address of our server
217:51 - which you can also get in the
217:53 - digitalocean dashboard followed by 5000
217:56 - which is the port that scrippy web is
218:01 - running on if we copy that go up paste
218:05 - that into a browser
218:07 - we should see
218:09 - the scrapery web dashboard showing up
218:12 - correctly
218:15 - so
218:16 - we can see the jobs that I've already
218:20 - run
218:21 - so we can see the job that was run
218:23 - earlier that we ran when we ran it
218:25 - manually via the command line we had the
218:28 - default project and the spider book
218:31 - Spider and the job ID that was returned
218:33 - to us
218:35 - so
218:36 - it doesn't have the pages and the items
218:39 - because it needs this log parser
218:43 - module to be installed so we're going to
218:46 - go ahead install the log parser so we
218:49 - can see the pages and items and more
218:52 - statistics and we're also going to add
218:56 - in a username and password just some
218:59 - basic authentication right now anyone
219:02 - can hit this endpoint and start running
219:04 - my jobs I don't want that to be
219:07 - happening and I'm pretty sure you guys
219:09 - don't either so you're paying for the
219:12 - server you don't want anyone to be able
219:14 - to come on and start messing around with
219:16 - your dashboard so we're going to quickly
219:18 - go and do that now
219:20 - so we'll start by just copying and
219:22 - pasting this pip install log parser
219:25 - command
219:27 - I'm just going to shut down scrape
219:30 - Beauty web for a second
219:32 - so
219:34 - run pip install log parser
219:39 - then once that's installed I want to
219:42 - edit my scriptd web settings
219:45 - so I can use
219:47 - VI again
219:49 - and I'm going to first enable the off
219:55 - so as you can see here it's currently
219:57 - set to false
219:59 - I'm going to set it to true
220:02 - and then I'm going to set a username of
220:05 - test
220:06 - and a password of one two three four
220:08 - five six seven eight obviously set
220:11 - better username and passwords than that
220:13 - please for your own projects
220:15 - and your own servers the next thing we
220:17 - need to do is come to our script e
220:20 - servers list
220:21 - and just comment out
220:24 - this line here
220:25 - because we don't have a server running
220:28 - on Port 6801 we just have our Square PD
220:33 - running on
220:35 - this
220:36 - Port 6800 on localhost
220:40 - so once that's done
220:42 - the next thing to do is to add in our
220:45 - logs directory and to enable the log
220:48 - parser here
220:50 - so let's just enable the loud parser by
220:53 - setting that to true
220:55 - true and for us
220:58 - the
221:00 - directory is going to be
221:04 - just root then the name of the project
221:09 - frequent Camp part 6 and the name of
221:12 - these spiders the folder containing the
221:15 - spider which is book scraper and then
221:17 - it's got a logs folder in there which is
221:20 - where the log parser is going to read
221:22 - the logs from
221:24 - so obviously if you've got a different
221:26 - project name and a different spider name
221:28 - you need to just make sure that that is
221:30 - correct
221:31 - but it will always have a folder in
221:34 - there with logs already so just find out
221:36 - where your logs folder is and paste in
221:39 - the directory here
221:42 - and then the last thing we need to do is
221:44 - just set our scripted server which is
221:48 - just the default again which you can see
221:50 - from above
221:51 - 127.0.0.1 is usually the default and
221:54 - it's running on Port 6800 so now that we
221:58 - have all that I'm just going to save the
222:00 - file
222:01 - and now that is saved
222:04 - we should be able to run
222:08 - scripted web again
222:09 - this time I'm going to do like I did
222:12 - with scrape Ed and get the logs to save
222:15 - into a separate log file so they're not
222:18 - coming up on the screen
222:20 - and I'm going to run it on the
222:21 - background
222:22 - so that should be running we can check
222:25 - if everything is running correctly using
222:28 - the following using the sudo SS command
222:35 - so we can see
222:37 - that we have scrape PD running on Port
222:40 - 6800 of localhost
222:43 - and we can see that Scrapy web is
222:46 - running on localhost Port 5000.
222:50 - so we can see both of them are running
222:52 - we can see the ports they're running on
222:55 - and we have
222:56 - the process ID which we can use to kill
222:59 - the process so if we wanted to stop one
223:03 - of them from running for example I might
223:05 - as well just stop Scrapy D from running
223:07 - and show you guys
223:08 - you can just type in Kill and then we
223:11 - will get the scrappy D process ID from
223:14 - here
223:15 - and we will just paste that in and press
223:19 - enter
223:20 - and then you guys can see that
223:23 - Scrapy D is no longer running so that is
223:27 - how you can kill them if they're running
223:29 - in the background as a process like that
223:32 - so I'll just start up Scrappy D again
223:39 - so if we check
223:43 - we should see Scrapy D running and
223:45 - scripted web running
223:46 - perfect okay so because we killed and
223:50 - restarted Scrapy D we need to just
223:53 - re-deploy our project again using the
223:56 - Scrapy D deploy because if we go back to
223:59 - our Scrapy dashboard we won't be able to
224:02 - see our spider
224:04 - and we won't be able to run our spider
224:06 - so we need to package up and redeploy
224:10 - the project again
224:12 - so we can just do that with square PD
224:15 - deploy
224:16 - so it's just Scrappy Dash deploy and
224:19 - we're just picking the project name of
224:20 - default again
224:22 - so that'll just package up our spider
224:24 - and add it again to scrippy D
224:29 - so that's been added again and we can go
224:33 - back to our endpoint now it asks us to
224:35 - sign in so if I try and sign in it says
224:37 - no you need to add in the username and
224:40 - password so
224:42 - I'll add in my username and password
224:43 - that I set in the config file
224:46 - and sign in
224:48 - if I go to run spider
224:54 - we can then see the default
225:00 - default latest version
225:02 - book Spider
225:04 - if you need to set any specific settings
225:06 - and Arguments for your project you can
225:09 - do it there if you want to set it to run
225:12 - at a specific day of the week
225:15 - hour our minute you can do it here
225:18 - so I'm not going to set it for a
225:21 - specific time in the future I want it to
225:23 - run right now the next thing I need to
225:25 - do is Click just check command
225:28 - that will paste in a default command
225:30 - which as you can see here is just going
225:33 - to do the curl
225:35 - to the endpoint that we did earlier so
225:39 - all scripted web is doing is running
225:41 - this command so everything is correctly
225:44 - set the project the version The Spider
225:46 - and we just want it to run switch click
225:49 - run spider and that's going to go off
225:51 - and kick off our s our job and it's
225:54 - going to start running
225:57 - so that should be running now and we
226:02 - should soon be able to see the
226:04 - statistics coming back for a number of
226:07 - pages and items
226:08 - so let's give that a minute or two to
226:10 - run
226:12 - so as you can see it has finished
226:16 - and it took 24 seconds to run and you
226:19 - can see some other
226:22 - stats and pieces like that now we're
226:24 - still missing the pages and items and
226:27 - this sign is still up here saying that
226:29 - we need to install loud parser so I
226:32 - think I might have actually put in the
226:34 - incorrect path to where the logs are
226:37 - stored so let's fix that and then we can
226:40 - show you how the pages and items show up
226:44 - so if we go back
226:45 - I've discovered that I need to change my
226:51 - path in the scripty web settings
226:54 - so I can open up my scripty web settings
226:58 - so here you go so
227:02 - I think we just need to fix this so as I
227:06 - said depending on your project you just
227:08 - need to find where the log folder is
227:11 - kept so I think that should do it it's
227:15 - just forward slash root forward slash
227:17 - Frankel cam part six and then locks so
227:20 - I'm just going to save that
227:23 - and I'm going to restart
227:27 - scrivity web unless it's after fixing
227:31 - itself but I think we need to restart it
227:33 - yep so just go back
227:37 - run the sudo
227:40 - ss-t-u-n-l-p command again and we will
227:44 - kill our scripty web
227:46 - using the kill command
227:49 - and we'll run scriptyweb again
227:53 - so yep so as you can see
227:57 - the log parser ran eight seconds ago
228:01 - and it was last updated
228:04 - at this time
228:06 - so if we re-run our
228:10 - spider
228:12 - we're going back to run spider
228:15 - and
228:17 - we'll put the default project the latest
228:20 - version book Spider put in the command
228:23 - run spider and it'll run again
228:26 - in the meantime it's gone through and
228:29 - it's parsed the last logs that we ran a
228:33 - couple of minutes ago and you can see
228:35 - the pages and items have actually been
228:37 - populated here so it scraped 1051 pages
228:40 - and there was a thousand items and we
228:44 - can see some more stats by clicking the
228:46 - stats button
228:48 - so you can see
228:50 - what else to have warnings and errors so
228:53 - there's one warning and you can see the
228:56 - latest item that was scraped and as we
228:59 - can see it was this URL
229:02 - this was the book title
229:05 - we can see the price tax and all the
229:08 - other stuff that we've already selected
229:11 - in part five and six so
229:15 - this is kind of the basics of scribdy
229:18 - web and how you would install the
229:21 - scribby web dashboard to work with
229:23 - scrapyt
229:25 - obviously you can also
229:27 - see the full logs
229:29 - and you know
229:32 - if you need to
229:33 - see exactly a specific error or you need
229:37 - to nail down further into the logs
229:38 - you've got the full logs available there
229:41 - as well
229:42 - and it's picked out the warnings and if
229:45 - there was any errors it would show them
229:48 - as well so it's very handy it's free
229:51 - it's open sourced you can install it
229:54 - yourself
229:55 - and as you can see there's a bit of
229:57 - configuring in the settings and there's
230:00 - a little bit more knowledge required
230:02 - into okay you need to deploy this
230:05 - project like this you need to run the
230:07 - spider like that
230:09 - there is some help pieces as well
230:12 - available for example if you forget how
230:15 - to deploy it they do have a help section
230:17 - here and you can
230:20 - follow the instructions as to oh how do
230:23 - I deploy my project so that scripty web
230:27 - and Scrappy D can use it again they've
230:29 - got the commands you need to run here
230:31 - and the steps you need to follow are you
230:34 - can put in your project directory and
230:38 - they can Auto package it up
230:41 - using scriptyweb as well so
230:44 - the next dashboard we look at is the
230:47 - scrape UPS integration with scrapeyd so
230:50 - there's two different dashboards for
230:51 - scrapeyd Scrapy web and the scraps
230:54 - dashboard
230:56 - so
230:57 - for that you need to go off and create a
230:59 - scrape UPS account so you can just go to
231:03 - scrapeups.io
231:04 - and if you've been using it for any of
231:08 - the other parts we've gone through
231:10 - already in our course you can use the
231:13 - existing API key you have if you're just
231:15 - joining us for this section you can sign
231:17 - up register for free and get your own
231:20 - API key
231:21 - so you will just need the API key
231:25 - and then you will just need to follow
231:28 - the monitoring steps so
231:31 - we'll just click monitoring it should be
231:33 - scrapey
231:34 - so I need to do pip install scrape up
231:37 - scrapey
231:40 - and we'll do pip install scrape up stash
231:43 - scrapey
231:46 - so we've installed the scrape off SDK we
231:52 - just need to add our API key to the
231:54 - scrapey project settings
231:57 - so
231:58 - I can copy this line
232:01 - go now into
232:05 - my folder
232:08 - and
232:10 - I want to edit my settings.py file
232:14 - and I'm going to just add
232:17 - the API key in
232:20 - here
232:22 - and check what else I need to install I
232:26 - also need to install
232:28 - the extension and the downloader
232:32 - middlewares
232:34 - so
232:35 - go
232:37 - down to my
232:40 - downloader middlewares
232:42 - and
232:44 - I'll add those in here
232:48 - so
232:49 - copy that
232:52 - paste that in
232:54 - and
232:55 - to the extensions
232:58 - I'll just put that under the existing
233:01 - extensions there
233:04 - so
233:09 - paste that in as well so we've got the
233:11 - download the Download Remember words the
233:14 - extension and the API key
233:17 - so I should be able to now save that
233:22 - and
233:25 - just check that there's nothing else to
233:28 - do there
233:31 - I think that's all correct so that's to
233:34 - install the monitoring so everything
233:36 - will show up in the dashboard but we now
233:39 - want to install the actual
233:43 - scheduling side of things so for that we
233:46 - will go to the
233:48 - servers and deployments section
233:52 - and we will
233:54 - add a new Scrapy d
233:57 - server so we just need the name of our
234:01 - server
234:02 - we just do test
234:04 - obviously you can name your server
234:06 - whatever you want we need the server IP
234:08 - address so
234:10 - we will go to
234:12 - our digitalocean
234:15 - dashboard and copy the IP address here
234:18 - so the ipv4 so copy that
234:23 - and
234:26 - just need to paste that in
234:28 - and save the details
234:32 - and then
234:34 - it's saying you should sh into your
234:36 - server we're already in on our console
234:39 - and it says run the command in your
234:42 - terminal so we will copy this command
234:46 - and go back to our console
234:49 - and I'll just go back up to
234:52 - the top
234:56 - and paste that in
234:59 - it's installing
235:02 - everything it needs it might need to
235:04 - restart some Services again we can say
235:08 - yes to that
235:10 - once it's finished we will say yes to
235:15 - that and okay so everything seems to be
235:18 - finished
235:19 - so we can now
235:22 - go back and check our
235:24 - servers list and we can see our server
235:27 - name is there and it's connected
235:31 - and we can
235:34 - check our server perfect so if we need
235:37 - to edit or delete the details there and
235:40 - it says your server is now set up
235:41 - correctly you can schedule your jobs on
235:43 - the scheduler page here I'll click that
235:47 - and click schedule job
235:51 - and I have my server name
235:53 - I've got my spider book Spider
235:57 - I will run the spider now and I can
236:01 - click if I wanted to run it every
236:03 - month every day all I can select
236:07 - you know a specific month or whatever
236:10 - we'll do that in a second
236:12 - for now I just want to run it now and I
236:16 - don't have any settings and arguments to
236:18 - add in so I can submit the job and the
236:21 - job is scheduled
236:22 - so
236:24 - in a few seconds that should
236:28 - show up in the jobs list
236:32 - in the meantime let's go and schedule
236:36 - a job to run let's say once a week
236:42 - so let's say
236:44 - every Monday
236:46 - at
236:48 - 7am
236:50 - and obviously that's in UTC time zone
236:54 - crons are usually run in UTC
236:57 - so you need to make sure that that
236:58 - corresponds to your own time zone
237:00 - correctly so we're saying every Monday
237:03 - at 7am
237:05 - please run the book spider spider
237:08 - on the test server
237:11 - and if we submit that
237:14 - it should then show up in our list so
237:18 - there you go at 7am only on Mondays
237:21 - it'll run that's very useful if you need
237:24 - to set up your spider to run every day
237:27 - every hour or whatever you can
237:30 - View and edit them here and you can
237:32 - enable them disable them
237:34 - so if we go back to our dashboard
237:37 - we don't have any details coming through
237:40 - yet
237:41 - there might have been an issue with
237:42 - running it as well
237:45 - but we'll explore that now so when there
237:48 - is an issue like this where we can't see
237:50 - any data coming into a dashboard the
237:51 - best thing to do is to try and just run
237:54 - the scrapey list are the square B crawl
237:58 - command manually from our server so I'm
238:02 - just going to go back to
238:04 - the console and go back into the project
238:11 - and
238:14 - from inside
238:16 - the free code Camp part 6 book scraper
238:18 - folder I'm going to run a scrapey list
238:22 - first
238:23 - and if there's an issue with scrapey or
238:26 - there is an issue with the settings
238:28 - it'll show up here
238:31 - so
238:32 - Scrappy list
238:33 - and we have an error
238:35 - escape the error and scrape is saying
238:37 - that there is an indentation error in
238:40 - the downloader middleware so that's
238:42 - probably what's causing the issue
238:44 - so we can just
238:47 - uh
238:48 - edit that
238:50 - so open up our settings again
238:53 - and check
238:57 - it's a download middlewares so I think
238:59 - it's just
239:01 - that there is a space here
239:06 - and in case it's also this guy I'll just
239:11 - remove the indentations there and do the
239:14 - same for the extensions and then
239:16 - hopefully
239:18 - there's no more issues
239:21 - so save that and try run scripture list
239:24 - again
239:25 - at this time it worked and book spider
239:28 - is returned perfect so if we go back to
239:33 - our scrapups dashboard
239:35 - and try and run that again
239:39 - so it's got the server and splatter we
239:41 - need selected
239:42 - and we can submit job
239:46 - so this time
239:48 - within hopefully a couple of seconds
239:51 - it's just
239:53 - go back to our servers page and go back
239:56 - to our jobs page we can see it's now
239:58 - running so it's after kicking off
240:01 - at 6 23.
240:04 - I'm going to give that a couple of
240:05 - seconds to run and then we should also
240:07 - have the stats available for
240:10 - this job
240:14 - so we can see on Monday
240:17 - one job run
240:20 - and it's in the middle of running now
240:23 - so we'll give it a couple of seconds to
240:25 - run and then we should see
240:27 - things like the runtime the pages missed
240:31 - Pages items
240:33 - and all the other stats coming through
240:38 - okay so on our jobs page now we can see
240:42 - that the status is changed to finished
240:45 - it took 25 seconds and
240:49 - we can see the number of pages items
240:51 - coverage
240:53 - and everything there so we can actually
240:56 - click into that now and we can see all
240:59 - the pages scrape there
241:01 - the runtime and everything in more
241:03 - detail
241:05 - so we also have things like number of
241:09 - items
241:10 - fields that were scraped so this pulls
241:13 - in stuff like the we can see if there
241:16 - was any stuff that was missed
241:18 - so things like the number of stars the
241:21 - price everything most things seem to be
241:22 - at 100 percent
241:24 - and obviously the description there
241:26 - there was one or two descriptions that
241:28 - were missed for some reason so this is
241:30 - can be useful to see which Fields were
241:35 - missed are which Fields came through
241:37 - correctly you can also see the amount of
241:39 - bandwidth that was used
241:41 - and we can see that there was one
241:43 - warning as well
241:45 - so obviously when you have
241:47 - multiple runs of the same spider then
241:51 - you can actually compare
241:53 - if you're running this daily you could
241:54 - compare the stats every day and then if
241:58 - on one day suddenly you see a major
242:00 - Divergence in the stats you can say okay
242:03 - there must be an issue with my spider I
242:05 - need to investigate and go in and
242:07 - investigate further
242:09 - so
242:10 - that's very useful we also have the
242:12 - status codes if there's 500s coming back
242:15 - are 404s the page isn't found so maybe
242:19 - they the links are broken so you can use
242:22 - these status codes as well to diagnose
242:26 - any other issues
242:28 - so that's how you would use the scrape
242:31 - UPS dashboards to integrate in with
242:34 - Scrapy D which would also be running on
242:36 - your server
242:39 - so you've two different dashboards that
242:42 - you can use with scrapeyd scriptyweb are
242:45 - the script UPS dashboard and integration
242:48 - if you guys have any other questions
242:49 - with that you can stick it in the
242:52 - comments as well
242:54 - so in the next section we'll be looking
242:57 - at how we can
242:58 - instead of using scrapeyd
243:01 - to
243:03 - degrade with scrape UPS we'll be looking
243:05 - at using the complete scrape UPS
243:09 - integration
243:10 - to integrate in directly with your
243:15 - scrapey project instead of using scrape
243:18 - PD as a kind of a middle layer which
243:21 - gets integrated with your project and
243:24 - then other things have to integrate in
243:27 - and hit the API endpoints this just goes
243:30 - directly and integrates with your server
243:32 - and your project so that's what we'll be
243:34 - looking at in part 11 and then Part 12
243:37 - we'll be looking at using the scrapey
243:40 - cloud
243:41 - so that's it for now and see you in part
243:44 - 11.
243:49 - so in part 11 of the scrappy beginners
243:52 - course we're going to be looking at
243:53 - using scrape UPS to manage deploy and
243:56 - monitor our spiders
244:00 - so we'll jump straight into it first
244:02 - things first we're going to need to set
244:05 - up a virtual machine so you can do it
244:09 - with AWS if you already have an account
244:11 - with them you can do it with
244:12 - digitalocean most of these different
244:15 - companies have free credits so you can
244:17 - just sign up use their free credits try
244:20 - it out and then go from there
244:22 - so I've already got a digitalocean
244:24 - account and I'm going to be using that
244:26 - so you guys can follow along with that
244:28 - are you guys if you already have an AWS
244:31 - or Azure account you can follow on from
244:34 - the step where we log in to the actual
244:37 - virtual machine
244:39 - so I'm just going to quickly set up a
244:41 - server here I'm going to go on to the
244:43 - cheapest ones they have which are four
244:45 - dollars a month
244:47 - and I think that's all I need to select
244:51 - so I can just create droplets
244:54 - now that the droplets been created I
244:57 - have the dashboard available where it
245:00 - shows things like the IP address and a
245:02 - few different graphs
245:04 - I'm just going to
245:06 - open the console now
245:08 - so that's going to open up a new window
245:11 - and it's going to
245:13 - SSH onto a machine and then we can then
245:16 - run commands on our virtual machine
245:21 - so while that's getting set up if you
245:24 - guys haven't already
245:25 - set up a account with scrape UPS so you
245:29 - can get a free account there and we'll
245:31 - be using them now to integrate with our
245:33 - server
245:34 - so go ahead create an account I have one
245:38 - already set up
245:39 - once your account is set up
245:42 - go to servers and deployments on the
245:46 - site
245:46 - where it says add servers click add
245:49 - and then we're going to name our server
245:53 - free code camp
245:57 - and we're going to put in the IP address
245:59 - of the server
246:01 - so we get that from here
246:04 - copy their P address
246:06 - paste it in save the details
246:09 - and now it says to
246:12 - provision a server we need to run this
246:15 - script on our server so we copy the
246:19 - details there go in to our terminal our
246:23 - console for the server
246:25 - and
246:27 - paste in
246:28 - the script details so that's going to go
246:31 - off run the script and provision the
246:34 - server
246:36 - and
246:37 - we can see that it is running through
246:40 - installing the dependencies it needs
246:42 - installing a new user and the authorized
246:45 - keys and installing the required
246:48 - libraries so that's just going to run
246:50 - through those different steps
246:52 - and once it's complete we should be able
246:54 - to then go on and in Clone our
246:58 - spider onto our server using the
247:01 - dashboard so that can take an integer
247:03 - two we let it just run through the
247:05 - different steps
247:09 - so now our provisioning has completed
247:11 - and it's brought us into the server
247:14 - dashboard and here we have options like
247:17 - clone repository add spider delete the
247:20 - server edit the details schedule jobs
247:23 - and the SSH keys for the server so we're
247:27 - going to go ahead and go to the Clone
247:29 - Repository
247:30 - that's where it's going to be getting
247:33 - the details of the repository that we
247:36 - paste in and then it's going to clone
247:37 - the spider directly onto our server so
247:41 - we don't need to actually do it manually
247:42 - ourselves we can do it through this UI
247:45 - here
247:46 - so for that we're going to first
247:49 - go to the free code comp part 6 that
247:53 - we've been using in the part 10 video so
247:58 - we're going to use part six and the next
248:00 - thing you want to do is you want to Fork
248:02 - your own copy obviously if you have your
248:05 - own spider and you're using that that's
248:07 - fine if you're following along with me
248:09 - now the best thing to do is Fork your
248:11 - own copy so click the four button follow
248:15 - the steps that's just going to copy over
248:17 - the free code cam part 6 onto your own
248:20 - repo and then from there like I have
248:24 - here I've just done it myself I now have
248:27 - it under my own name and in here the
248:30 - next step will be to add the
248:33 - deploy key so this deploy key will
248:36 - enable us to do commands like git clone
248:41 - and pull the repo directly from our
248:45 - GitHub onto the machine
248:47 - so we just need to add that key we go to
248:50 - settings deploy keys
248:52 - add deploy key
248:54 - I'm going to call this
248:56 - free code cam VM and I'm going to paste
248:59 - in my deploy key so you get this deploy
249:02 - key in here you copy that
249:05 - you add it in here
249:08 - remove any spaces and we don't need to
249:11 - allow right access for the moment
249:12 - because we're just going to be pulling
249:13 - and we can add the key
249:16 - so once the key is added
249:20 - we can then go back to our main UI here
249:25 - the next thing we need to do is
249:27 - just go and grab our URL
249:32 - from the main page here
249:35 - grab our URL
249:37 - so this is my repository and this is my
249:39 - own copy of the free code cam part 6.
249:43 - the branch name
249:44 - is main so you can see it here it's Main
249:49 - so I need to make sure to put in the
249:51 - right Branch name and then the language
249:54 - is Python and the framework is scrapey
249:57 - so that's all correct
250:00 - so this is the install script that's
250:02 - going to run
250:03 - when we click on repo it's going to go
250:05 - in to our virtual machine it's going to
250:09 - then get clonus and then once it's get
250:13 - cloned the repo it's going to go into
250:15 - the repo install a python virtual
250:17 - environment activate the virtual
250:18 - environment and then install the modules
250:22 - that are in the requirements.txt so if
250:25 - you
250:26 - look in here it's going to install the
250:30 - different modules that are listed in
250:33 - here
250:34 - which is everything that the project
250:37 - needs to run
250:39 - so
250:41 - once it's done that it's just going to
250:43 - make sure that scrape is installed and
250:45 - while we're at it we're going to add the
250:49 - monitoring module first grade pops as
250:52 - well so let's just add in
250:55 - that here as well so install scrape up
250:57 - scrape B which installs the monitoring
251:01 - python module for us so once that's all
251:04 - in here we can click clone repo and it's
251:08 - going to go through the steps here so
251:10 - it's cloning the repo great
251:12 - and then it's going to run the install
251:14 - script which can take two or three
251:16 - minutes
251:18 - and then the next step is it's going to
251:20 - find the Scrapy spiders by running the
251:23 - scrapey list command
251:25 - so I'm just going to give it a minute or
251:27 - two and then we will hopefully see our
251:31 - repo in our table here and we'll see the
251:35 - spider our book spider in under the
251:38 - spiders table on the right so as you can
251:41 - see the install script ran correctly and
251:44 - it was able to find our spiders as well
251:47 - so you can see our spider automatically
251:49 - came in here and here is our cloned
251:52 - Repository
251:54 - so if you click in you can see that
251:57 - there is
251:59 - a deploy script here as well so if you
252:01 - need to deploy updates to your code
252:05 - you will
252:06 - update your own repository and then for
252:09 - the code to actually go onto the server
252:10 - you just need to go in here and click
252:12 - deploy and it will then pull the latest
252:15 - from your Repository
252:18 - so that's how you would update the code
252:20 - on your VM
252:24 - okay so we have a repository we have our
252:27 - spider so let's just go ahead and show
252:30 - you guys you can quickly run the Spider
252:32 - by just clicking the run now button
252:34 - it'll go in select the server the
252:37 - repository and the spider because you
252:39 - could have multiple
252:40 - spiders in your repository and we're
252:43 - just going to click submit job to run it
252:46 - straight away
252:48 - so the job is started
252:51 - and if you want to check the log
252:52 - straight away you just come here and
252:55 - click view logs
252:57 - so you can see it's just gonna head and
253:01 - it's running the spider correctly and
253:05 - you can see the title product type price
253:07 - everything is coming through so that's
253:10 - how simple it is to run the spider
253:13 - so the last step we want to do now is to
253:16 - activate the monitoring for our spider
253:19 - so instead of having to just look at a
253:21 - bunch of logs in a log file like that we
253:25 - can have everything displaying in the
253:26 - dashboard like we had in part 10. so to
253:29 - do that let's just open up the docs
253:35 - go to monitoring
253:37 - python scrapey
253:39 - scrapey SDK integration
253:41 - so we've already done this pip installed
253:44 - scrape up scrapey as part of the install
253:46 - script when we cloned our repository so
253:49 - we don't need to do that again
253:52 - but we do need to add in our API key and
253:55 - the extension bits here
253:58 - so it's telling us we need to add this
254:00 - to our settings.py file in our scrapey
254:03 - project
254:04 - so let's go ahead and do that now
254:07 - so I'm going to open up my Repository
254:11 - go to book scraper and go to the
254:15 - settings.py file and I can just edit it
254:18 - directly in here
254:21 - so
254:22 - I'll need to add the
254:25 - three different sections so first I'll
254:27 - add the API key and then I'll go and add
254:30 - the extension and the downloader
254:31 - middlewares
254:32 - so I'll just copy this line and go to
254:36 - GitHub
254:38 - paste this in
254:41 - here I need to get my API key which I
254:45 - can get
254:46 - from my settings
254:49 - here
254:53 - paste in my API key
254:55 - and then I want to go to my extensions
254:58 - and then the general middlewares
255:04 - extensions
255:06 - are currently comforted out so I'll just
255:08 - add it underneath
255:09 - and last of all downloader middleware's
255:14 - so obviously if you guys are using your
255:17 - own spider that has
255:19 - the downloader midwares uncommented out
255:22 - and you're just adding these two lines
255:25 - to your existing list but here because
255:28 - it's currently commented out I'm just
255:30 - pasting in the whole lot in together
255:33 - so we've got the extensions the
255:35 - downloader middlewares and the API key
255:39 - so we should be able to just commit the
255:41 - changes and now we can deploy the code
255:46 - via our dashboard so now that's
255:50 - completed
255:51 - we go back to our server
255:55 - go into our free code camp
255:57 - server
255:58 - go into our clone Repository
256:01 - and click deploy here
256:05 - so the latest has been deployed we can
256:08 - check the log as well to see
256:12 - did deployment work so we can see it
256:15 - updated the book scraper settings.py
256:18 - file one file changed with nine
256:20 - insertions so that's perfect
256:23 - great so now that that's in we should be
256:26 - able to
256:27 - run our book Spider again
256:31 - submit the job
256:34 - and
256:36 - if we check the logs again
256:40 - we can see it's
256:42 - kicked off
256:44 - and if we go to our
256:47 - jobs list we can see there's one running
256:51 - so this is the one we ran for part 10
256:54 - and this is the one that is running now
256:59 - so once it's completed running in about
257:03 - 20 more seconds we should see the pages
257:05 - items coverage and everything else fill
257:08 - in as well
257:10 - and we can also see that in our
257:12 - dashboard we can see
257:14 - under Tuesday we have this job that is
257:18 - running now
257:23 - so we'll just quickly show you how you
257:26 - can also schedule it
257:30 - so
257:32 - in case you're just joining us for part
257:34 - 11 if you want to schedule a job to run
257:38 - on This Server recurring you just go and
257:42 - click recurring
257:43 - and then you can select we want okay
257:47 - every day in March
257:49 - we want every
257:53 - every time at midnight we want this job
257:57 - to run
257:58 - so we'll submit the job
258:01 - and then we can check in our scheduled
258:04 - jobs
258:05 - we have book Spider
258:08 - which will run as 12 every day only in
258:11 - March and there it is
258:14 - so then if you need to edit that or you
258:16 - want to just disable it you can go to
258:18 - the scheduler Tab and you have the
258:22 - ability just to disable it there or
258:25 - delete it cloness view the logs whatever
258:28 - you need to do
258:30 - so if we just go back to our dashboard
258:32 - we can now see that the job is completed
258:35 - the page is scraper there we can see the
258:39 - items are there and everything looks
258:42 - like it ran correctly so we can compare
258:44 - the the two days so yesterday this many
258:47 - pages were scraped are this many status
258:50 - codes came back today this many status
258:52 - codes came back so it is useful if you
258:54 - need to compare the same job that was
258:57 - run over multiple days you can quickly
258:59 - see okay if the runtime varied or if the
259:03 - number of page varied varies are the
259:06 - items are the coverage
259:07 - you can see that very quickly in one
259:10 - page one glance so that makes it very
259:13 - useful
259:14 - and if we need to drill down into the
259:16 - individual job we can just click in and
259:21 - we can delete the job data or do
259:24 - anything else we need to do here
259:27 - so
259:29 - that brings us to the end of this
259:32 - section
259:34 - any questions you guys might have
259:36 - let us know and I hope you have an idea
259:39 - now of how to quickly get set up with a
259:42 - virtual machine and hook it up to use
259:45 - scrape UPS so a reminder that everything
259:48 - that we've used with scrape UPS here is
259:50 - free to use so there's no limitations on
259:54 - the image of servers you can hook up are
259:56 - the amount of jobs you can run
260:00 - so that's the end of part 11 guys
260:08 - so in part 12 we're going to look at
260:10 - everything to do with Scrappy cloud
260:12 - so Scrappy Cloud was made by the
260:15 - developers of scrapey and it's a way
260:18 - which you can deploy and run and
260:21 - schedule your spiders on the cloud using
260:24 - scrapey Cloud the great thing is you
260:26 - don't need to have your own third-party
260:29 - server so you don't need to have a
260:31 - server with digitalocean or vulture or
260:33 - AWS you can just deploy it directly onto
260:36 - Scrappy cloud and just run it the only
260:39 - downside is that if you want to schedule
260:42 - your jobs it's paid so you can run your
260:44 - jobs and it's free but to schedule your
260:47 - jobs on Scrappy Cloud you have to sign
260:49 - up for a monthly subscription
260:53 - so we'll show you how everything works
260:55 - and then between scrapey Cloud scrape
260:58 - ups and Scrapy D you guys will have had
261:02 - a full overview of all the different
261:04 - ways you can deploy schedule and run
261:07 - your spiders on the cloud and you can
261:10 - decide then which kind of way you want
261:12 - to go do you want to go with a
261:15 - completely open source way with using
261:17 - just scrapeyd and scripted web or do you
261:21 - want to go with a free way using scrape
261:25 - UPS or do you want to go with a paid
261:28 - solution with scrapey cloud
261:31 - so you'll have the full up array of
261:34 - options covered by the time we finish
261:36 - part 12. okay
261:39 - so let's quickly look at scrapey Cloud
261:42 - so scribby cloud is obviously made by
261:46 - size the Creator is the scrappy scalable
261:49 - Cloud hosting for your scrapey spiders
261:51 - and that's pretty much what it is host
261:53 - and monitor your Scrappy spiders in the
261:54 - cloud as we said and it's very reliable
261:58 - easy to scale as they say on demand
262:01 - scaling they have lots of other
262:03 - Integrations as well so
262:06 - what we need to do is you need to go
262:08 - ahead and create an account with them
262:10 - once you have an account you can then go
262:14 - into the dashboard and access scrapey
262:17 - Cloud here on the side
262:20 - so I'm just going to start a new project
262:23 - and I'm going to call it just free code
262:25 - camp
262:28 - and
262:30 - click Start
262:32 - so
262:34 - then it's got the instructions here of
262:38 - what we need to do to install the
262:40 - command line
262:42 - tool so we can easily deploy our spider
262:45 - into the scrapey cloud so first things
262:49 - first we're going to go ahead and we're
262:52 - going to be using the part 6 code
262:55 - example again so the code that we used
262:57 - for part six of this course we're going
263:00 - to just get clone that so I've got an
263:03 - empty folder here open in vs code and
263:06 - I'm just going to Gig clone the free
263:09 - code Camp part 6 and then I'm going to
263:12 - quickly install
263:14 - a virtual environment obviously if you
263:17 - guys are on Windows or on Linux you guys
263:19 - need to follow the steps they'll be
263:22 - covered in part two of this course to
263:25 - make sure you're installing the correct
263:27 - virtual environment
263:28 - for your operating system
263:30 - and then once the virtual environment is
263:34 - set up we can activate it
263:36 - so just do
263:39 - Source
263:41 - bin activate
263:43 - I know it's activated and now we can go
263:45 - and follow the instructions here
263:48 - so I'm just going to copy and paste them
263:50 - in directly
263:52 - so install S hub
263:55 - and then to the s-hub login
263:59 - and then it's just a matter of putting
264:01 - in my API key
264:03 - okay so that's all installed
264:07 - put in s-hub login
264:11 - it says I'm already logged in
264:13 - I'll just log out to make sure that you
264:16 - guys can see the full process
264:19 - so a sub login
264:23 - put in my API key from here
264:29 - paste in
264:31 - my API key obviously you guys will be
264:33 - putting in your API key so please don't
264:36 - use mine and then we can just do
264:40 - s-hub deploy and it should deploy
264:44 - if I'm in my
264:48 - correct project so I'm inside my project
264:52 - and I think it wants there to be a
264:55 - scrapey.cfg file so you need to make
264:58 - sure you're in the correct folder
265:02 - and if it's correct it you can see
265:06 - the deploying to Scrappy Cloud project
265:08 - and then the project ID there
265:11 - so in the meantime you can see it's
265:14 - bills the project and then it
265:18 - uploads it to the site here so here you
265:23 - can see it's been deployed
265:26 - and it's successful so if it works
265:28 - correctly this should all
265:31 - be very similar to what you see
265:33 - so now that it's deployed into the cloud
265:36 - we can go to our jobs dashboard
265:41 - and we should be able to run
265:45 - our spider
265:47 - so we have our book Spider available now
265:50 - in the drop down
265:51 - and we can leave everything else the
265:54 - same you've got priorities there if you
265:56 - want to have certain jobs running ahead
265:59 - of others so we can just click run
266:02 - and that should kick off the book spider
266:05 - and it should start scraping
266:08 - our
266:09 - books to scrape site so you can see it's
266:12 - running away there
266:14 - and
266:16 - we should see that when it's completed
266:18 - it'll be in the completed table
266:22 - and it'll also have the items and the
266:25 - requests and the errors all available
266:27 - here as well so they'll start populating
266:30 - in a second
266:33 - so
266:34 - as you can see three requests the logs
266:37 - are there if we wanted to run every day
266:42 - once a week periodically whenever we
266:45 - want we just go to periodic jobs click
266:48 - add periodic job
266:50 - select the spider and then we can select
266:53 - okay we want to run every Monday at
266:58 - 6am
267:00 - or 7am
267:02 - and
267:04 - we can just select every day
267:08 - or every Monday 7 A.M
267:10 - and we can save that
267:13 - so as you can see here it says
267:16 - the periodic jobs below will not run
267:18 - because I have not created a paid
267:22 - subscription so you just need to click
267:24 - subscribe now
267:25 - and then sign up for the paid version
267:28 - and then this job will run every Monday
267:31 - at 7.
267:34 - but if you guys just want to try it out
267:36 - then you could just schedule the jobs
267:38 - normally by clicking run here manually
267:41 - yourselves
267:42 - so
267:43 - my spider is still running away
267:47 - we'll just leave that complete and then
267:49 - we'll have a look at the items and
267:51 - requests and some of the stats that are
267:53 - available
267:53 - so as you can see our job is completed
267:57 - with the Thousand items
268:00 - the requests so the amount of pages that
268:03 - were scraped and the logs so we can
268:07 - click into the requests
268:09 - and you can see T thousand requests if
268:13 - you want so you can see all the specific
268:15 - URLs that were scraped the statuses
268:18 - etc etc
268:19 - the items then are a bit more
268:21 - interesting obviously because that's the
268:23 - actual data that we scraped so we've got
268:26 - everything set up nicely there our
268:28 - prices taxes titles URLs description
268:32 - etc etc
268:33 - so you can check quickly if the
268:36 - information that was scraped correctly
268:39 - came through or not
268:41 - if you need to look at the logs or the
268:43 - stats that's also available there so
268:46 - as you can see it's very polished very
268:49 - nice simple to use and also works and
268:54 - auto scales when you need to scale
268:57 - things as well
268:58 - so I think that's the ins and outs of
269:01 - scrapey cloud I think now you should
269:03 - have a very good idea of the different
269:05 - options you have when it comes to
269:07 - deploying your spiders and then
269:11 - scheduling and running your spiders and
269:14 - seeing all the stats in various
269:17 - dashboards so just to go through the
269:19 - three options you have first option
269:22 - Scrapy D which can run just via an API
269:27 - endpoint and you can hit the API
269:29 - endpoint to schedule things and deploy
269:31 - things
269:32 - with Scrapy D you have two uis you can
269:36 - use with it the
269:38 - scrapeydweb are the scrape Ops
269:40 - dashboards which
269:42 - we've shown you how to install so that's
269:44 - the free open source part the second
269:47 - option was using scrape ups for
269:49 - everything and using the scrape UPS
269:52 - integration directly with a server such
269:54 - as digitalocean are AWS our vulture so
269:59 - you need to set up a VM quickly there
270:01 - first
270:02 - and then the final option was just using
270:05 - the scrapey cloud for everything and
270:08 - deploying it directly to Scrappy Cloud
270:10 - but it was paid if you want to do any
270:12 - sort of periodic jobs to schedule them
270:15 - to run daily or weekly or whatever
270:19 - so there are the three main options and
270:20 - I'll leave it up to you guys to decide
270:22 - what works best for you so that brings
270:25 - us to the end of Part 12 and in part 13
270:28 - we'll just go through everything and do
270:30 - quick recap of the entire course
270:33 - so see you in part 13 guys
270:39 - so guys we've come to the end of our
270:41 - Scrappy beginners course this is the
270:44 - last part so we'll just do a quick wrap
270:47 - up and then I'll give you a small bit of
270:50 - an overview of some extra skills you
270:52 - might find useful if you want to
270:54 - continue on and get better at scraping
270:56 - using scraping
270:58 - so we've built an end-to-end scrapey
271:00 - project that scrapes all the books from
271:02 - books to scrape and then cleans the data
271:05 - and stores the extracted data in
271:08 - different file formats and different
271:11 - places such as a database
271:13 - we then looked at
271:15 - optimizing our headers and user agents
271:18 - so that websites would let us get
271:22 - through any antibiot software that they
271:25 - have on their site and we also looked at
271:27 - how we can use proxies in the different
271:29 - proxy provider options that are out
271:31 - there if we want to have something
271:33 - that's a little less Hands-On
271:36 - and then finally we looked at how you
271:39 - can deploy your spider
271:41 - to the cloud onto a server and then how
271:44 - you can schedule this to run
271:46 - periodically and then how you can view
271:48 - the results of your running spiders
271:53 - but obviously that isn't everything
271:55 - we've only gone through the kind of
271:57 - Basics there's still a lot more
272:00 - different edge cases that we haven't
272:02 - covered and the number one thing is
272:05 - probably scraping Dynamic websites so
272:08 - there's a lot of websites out there that
272:10 - are rendered in the browser so that
272:13 - means that they're using a front-end
272:15 - framework which will actually only
272:18 - display the page once all the data is
272:21 - received by the browser so in that case
272:23 - if you were to ask for a URL what you
272:28 - get back might not contain the data
272:30 - you're looking for because it hasn't had
272:31 - a chance to render inside in a browser
272:34 - so
272:36 - things I would recommend that you look
272:38 - at in in those cases would be looking at
272:41 - things like Scrappy Puppeteer or Scrappy
272:44 - selenium which then use scrapey with a
272:48 - headless browser integration to actually
272:50 - render the website in a headless browser
272:54 - so then that way using scrapey Puppeteer
272:58 - or scrapey selenium you can actually
273:00 - render the page and get the data you
273:03 - need
273:04 - the other option would be to find the
273:07 - API endpoint because most of these sites
273:09 - that are front and rendered have API
273:11 - endpoints and you can find the data
273:13 - there
273:13 - so I'll just give you one example of
273:16 - what it would look like to see the data
273:19 - coming back from an API endpoint so I
273:22 - would recommend if you guys want to work
273:25 - through a couple of different challenges
273:26 - site have put together this great site
273:28 - which we've been using for our books to
273:31 - scrape.com but they also have a bunch of
273:34 - different other examples using this
273:37 - quotes dot to scrape.com where you can
273:40 - for example have infinite scrolling so
273:43 - how would you get around a page where
273:44 - you have infinite scrolling like
273:46 - something like Instagram or Facebook and
273:48 - you can practice having a JavaScript
273:51 - rendered site so there's all these kind
273:53 - of different options which you can
273:55 - practice with and they have all these
273:57 - different pages available here for you
273:59 - to practice on so
274:02 - one example would be
274:04 - this one see we're scrolling down and if
274:08 - you go to your network tab you can see
274:10 - there is data being asked for every time
274:13 - we scroll down so more pages of data are
274:16 - being requested and they come back but
274:19 - instead of it being HTML it comes back
274:22 - as Json and then this Json data we've
274:25 - got the quotes which then the front end
274:27 - framework then goes off and populates
274:30 - the page with
274:32 - so
274:33 - this is an example of where you can
274:35 - actually
274:36 - directly query an API endpoint instead
274:40 - of actually scraping the HTML you can
274:43 - ask for the API endpoint to give you
274:47 - back the data directly so that even
274:49 - makes your life easier
274:51 - so this is one example of what I was
274:53 - talking about if you come into contact
274:55 - with a front-end rendered
274:58 - page that gets rendered in the browser
275:01 - so I really would recommend you guys
275:03 - checking out and working your way
275:05 - through these different challenges that
275:07 - are available on the twoscribe.com site
275:11 - now another very important thing is
275:14 - obviously getting through a login
275:16 - endpoint which we didn't do in this
275:18 - course but which is something which a
275:20 - lot of websites would have so that is
275:23 - something as well I would really
275:24 - recommend that you guys go off and
275:27 - explore how to do
275:29 - so the last major thing I think would be
275:32 - looking at how you can scrape at scale
275:35 - if you really want to imagine you're
275:37 - scraping millions of pages a day you
275:40 - know there's different ways which you
275:42 - can use things like scrapey redis to use
275:46 - redis to store all the URLs you want to
275:49 - scrape in one central place and then you
275:51 - could have multiple different servers
275:53 - pulling URLs off this queue and then the
275:57 - old URLs can be scraped by multiple
276:00 - different worker machines at the same
276:02 - time again this is all using scraping so
276:05 - that is something I would highly
276:06 - recommend you guys to look at as well if
276:09 - you're interested in scraping at scale
276:11 - now all this stuff we also have as
276:15 - articles and videos so if you guys want
276:18 - to check out some more videos and
276:21 - in-depth articles I would recommend you
276:23 - guys checking that out like we have
276:25 - scraping behind logins and we have
276:28 - separate articles on using Scrappy
276:31 - Puppeteer Scrapy selenium
276:33 - Etc
276:34 - and also using scrapey redis if you want
276:37 - to do a distributed worker architecture
276:40 - so all that is up there for you guys if
276:42 - you want to continue on your journey and
276:45 - learn more about scrapey and the kind of
276:48 - more challenging parts of using scrapey
276:51 - to scrape the web
276:53 - so I think that comes to the end of our
276:56 - course I'd like to thank you for
276:59 - following along and if you have any
277:01 - questions just reach out put a comment
277:04 - on the video and we'll do our best to
277:06 - get back to you thanks guys

Cleaned transcript:

dive into the world of python web scraping with this scrapey course for beginners learn to create scrapey spiders crawl websites clean and save data avoid getting blocked and deploy your scraper to the cloud all while using python scrapey Joe Kearney developed this course he is an expert in web scraping and he's the cofounder of scrape Ops hey everyone welcome to the free code campus crepey beginners course my name is Joe I'm a cofounder at scrape UPS a platform that gives developers the tooling they need to monitor schedule and run their web scrapers also the cofounder of the Python scrapey Playbook a complete collection of guides and tutorials that teaches you everything you need to know to become a scrapey developer in this free code Camp scrapey course we're going to take you from complete scrapey beginner to being able to build deploy and scale your own scrapey spiders so you can scrape the data you need you can find the code and written guides for this course over at the python scrapey Playbook book free code Camp course page so you can easily follow along with this video the link will be in the description we've broken this course down into 13 Parts which cover all the scrapey basics you need to go from never having used scraping before to being a competent Scrappy developer if you want to dive deeper into any of these topics then check out the python scrapey Playbook itself or the scrape Ops YouTube channel which is linked in the description here you will find guides that dive deeper into the topics we've discussed in this course and cover some more advanced grippy topics too such as scraping Dynamic websites and scaling your web scrapers using redis so what is scrapey so I think the best little summary for that is directly on the scrapey.org website so scrapey is an open source and collaborative framework for extracting the data you need from websites in a fast simple yet extensible way so it's an open source framework that anyone can use with python and it makes scraping websites much much easier so it helps you do things like retrieve a Pages HTML parse and then process the data and then store that data in the file formats you want and in the location that you want so that's what scrapey is so now the next question you probably have is well why should we choose scraping what does scraping have to offer over anything else so some of you might have already done a bit of scraping with python and you might have used things like just straight python requests to request the page and then get the response and then you might have parsed that response using something like beautiful soup which helps you parse HTML so this is perfect if you're doing just very simple scraping and you just want to scrape a couple of pages of a website or you just have something like a oneoff site that you want to scrape by all means use something like python requests and beautiful soup if you're looking to do anything from small medium to large scale it's much better to use something like scrapey because it comes with a load of features built in that you don't have to worry about so it helps you do things such as data extraction from the HTML using CSS selectors you can do automatic data formatting so it'll format the data you scraped into things like CSV Json XML and many other formats you can save the stuff directly into things like S3 buckets onto your local machine and use middlewares to save the data into databases so so much of that is already taken care for you so what else does that have that you don't have to worry about automatic retrace for example if you look for a page and the page comes back with an error it'll Auto retry it for you and you don't even have to worry about all the logic that goes into things like Auto retrace it looks after concurrency so you can scrape from one page all the way up to thousands of pages at the exact same time using the same piece of code so with all this stuff so much is taken off your plate that you can just focus on doing what you want to do which is okay this is the data I want from the website and this is the format that I want to save it into and the other great thing the fact that scrapey is an open sourced framework means that there's many thousands of developers all over the world who've made great plugins and extensions for it almost every single kind of question we might have is probably already answered so it's very easy to find the questions and answers online and things like stack Overflow for the use cases you'll be going through and it makes it very easy if you don't find the question just ask another one so I would really recommend scrapey if you're looking to do anything that is more than Justice very very very simple use case so if you're looking to scrape any kind of website scrapey would be the place to start off your scripting Journey okay so this course will be delivered through video of course which you're watching right now but along with the video we have an accompanying article that goes with each section so for example part two we have a full article here with all the commands you need to run code Snippets and more indepth explanations so this makes things much easier if you're not someone who likes to watch video and you prefer to read things and take things step by step that way we will also have all the codes that we use if you want to just jump into a part further down the line you can go and you can download whatever part you need we will have an accompanying git repo where you can just download the code at that point in time and follow on so hopefully that should make your learning that bit easier okay so what we're going to cover on this course so we're in part one now part two will be setting up your virtual environment and setting up scrapey on your computer part three we look at how to create a scrappy project part four creating your first scrapey spider and navigating through different pages getting the HTML from the page and then extracting what we need from the HTML we look at crawling through multiple Pages then how to clean the data that you've just scraped using item pipelines and Scrappy then in part 7 we'll be looking at saving the data to files and databases and all the different ways we can do that part 8 looking at everything to do with headers and user agents and how we can use user agents and headers to bypass certain restrictions that websites might be putting on us when we try and collect the data part nine we'll be looking at rotating proxies and proxy apis and how they can help us bypass some of those issues about getting blocked part 10 we'll be looking at deploying and scheduling our spiders using scrapeyd so when we want to run a spider on a regular basis we want to have something set up that we don't have to worry about and kick off manually but that will be programmatically run on a daily basis hourly basis every 10 minutes or whatever so we look at everything to do with that in Parts 10 11 and 12 and we're looking at the different options that are out there in terms of the free options open source options paid options the pros and cons of those options that we have and then that brings us to our recap at the end which is part 13. and part 13 we'll just go through everything we've learned and recap and talk about what there is left to do if you want to take your scripting to the next level so I think that's everything I wanted to talk about and we will see you in part two so in part two of this course we're going to be looking at how to install python setting up your price in Virtual environment on Mac Windows analytics and then finally how to install scraping okay so let's get started with that so first things first how to install python so it's fairly easy the first thing we want to do is we want to just go to python.org go to the downloads section and then you can click the download python 3.11 or whatever version it will be when you're looking at it so obviously I'm doing it I'm on the Mac so it automatically detects that and it automatically proposes that I download the version for Mac OS you guys if you're on Windows you'd be wanting to download the latest version for Windows so go ahead do that if you don't already have python we can quickly check if you do have python by going to your terminal or Powershell so open that up and then I'm just going to open one up quickly here in Visual Studio code and what you just want to do then is just type in Python and dash dash version and as you can see here python version 3.9 is installed for me so I don't need to go under loaders because I know it's already installed so go ahead and check if you have python installed if you do have it installed you can move on to the next section and if you don't just go ahead download Python and install it okay so the next thing we want to do is install pip if it's not already installed so pip is just a package manager for python so we can download thirdparty packages for our python project so what we do again we just check is it installed so it's just pip version again and as I can see I have Pip version 22 installed here so if you don't have it installed we link to it in the documentation in the article that we have for this and in the video as well so to install pip you go to pip.pypa.io and go to installation and they have supported methods for installing on Linux Mac OS and windows and to give you the commands you need to run once you have a python installed to install pip so it's very selfexplanatory and very easy to do all you need to do is copy this line and paste it into your terminal and hit enter so if I do it here it will just tell me that I already have it installed so as you can see it says requirement already satisfied pip in and then it gives the path so we have Pip installed and the next part is we want to install a virtual environment so VNV which comes with Python 3 the latest version of python will already be installed if you have python 3.3 and above if you have a lower version of python you might need to install the VNS manually and if you're on Windows you may need to install it manually as well so to do that you just pip install virtual ends if you're on Windows and I'll do that right now pip install virtual ends and that will go ahead and install virtual end for you if you're on Mac you don't need to do this or if you're on Ubuntu you more than likely won't need to do this either so we have python installed we have Pip installed we have virtual lens rvm installed so the next thing we can go ahead and do is actually create our virtual environment so a virtual environment is just think of it as a folder that sits on top of python where you can add all these thirdparty libraries and modules and they'll only be specific to the project you're currently running because what can happen is if you've got multiple python projects you can also often have multiple of the same packages but different versions to run your code and you don't want if you for example upgrade some thirdparty package that it breaks one of your other projects because one of your other projects needed an older version of that thirdparty package so by using virtual environments it just means that each project you have the third party libraries you installed are specific to that project so let's go ahead now and we'll just do python minus n and we're going to call the folder that we want VM also I just want to make sure I'm in the correct um folder so I've just made a part two folder with nothing in it as you can see and I'm just going to to thought no minus m v n f the end so that's gone ahead and let's create this VN folder with these items in it here and it's installed correctly if you're on Windows you're just going to be using the virtual end command instead okay so now that we have our virtual environment installed and you can see it here we want to just activate it so that any thirdparty package we installed after this will also be installed into this VN folder so to do that we just type in source and then VM bin activate so then you can see it's activated because we have the folder name VNV in Brackets here so that means anything we installed from now on using the package installer pip will be installed into this folder and be specific only to this project so we can go ahead now and install scrapey so we just do pip install scrapey and you can also get this command from the scrapey website itself as you can see pip install scrapey will install the latest version of scrapey 2.7.1 so I'm just going ahead and hit enter and as you can see it's downloading everything it needs for scrape B to run so depending on your connection and your computer it can take a minute or two okay so that's installed correctly as far as I know check it's installed correctly we can just run scrapey and it should give us a list of commands so if you see this output here where it's lists the available commands you know that scrapey is installed correctly as you can see from this line here scrapey has detected that there's no scrapey project created yet so it just says no active project so that's going to be the next step in part three is setting up our scrippy project so let's get going into part three so in part three we're going to be looking at how to create a scrappy project using scrapey then we're going to have an overview look at the project files that are generated when you create a new project and then after that we're going to go into detail on all the different parts of a scrapey project so that entails scrapey spiders items item pipelines it's created middlewares and settings so part three is really going to be a kind of a theory heavy part of this course so if you already know a bit of python this is probably going to be a lot more interesting than if you don't so you can feel free to dip around and and have a look at what parts of this would be most interesting to you we also have an article that goes along with this that might be a bit easier to digest so let's get going and create our project so to do that I've got a folder here part three and inside that folder I've got just the full project we're going to go through in a second and I've got my virtual environment that I've already activated so you guys if you've followed on from part two you should have already activated your virtual environment and you should already have scrapey installed so if you don't have that done just hop back to part two and make sure scrape is installed and your virtual environment is activated okay so now we can go ahead and use the scrapey start project command to create our new project so it's just simply scrapey space start project space and then the name of your project we're going to call this one book scraper because we're going to be scraping a site with books in it so if I hit enter it's gone ahead and created a new folder here book scraper you can see and if I do an LS you can see book scraper is there as well if and if we go into book scraper itself we can see we have book scraper and scrapey.cfg so I'm just going to open up the folder here and we can see inside of that we have several different files and folders so first off we have our spiders folder that at the moment has no spiders in it but we'll be doing that in part four we'll be generating spiders that go in there then we have items middlewares pipelines and settings so your basic scrapery project will contain these parts now you don't have to use items you don't have to create middlewares you don't have to touch Pipelines but you will always have a spider so you can think of items middlewares and pipelines are optional but we will be using them because if you're scraping anything more than just one page it becomes a lot easier just to use the pipelines items in middlewares and instead of trying to have everything custom made in a spider Okay so the next thing we're going to quickly look at is a fully fleshed out spider so just to give you an idea of what would go into these things like what is in items what does middlewares mean what we put in pipelines that's going to go through some code and give you an example so don't be too scared if you don't understand any of this stuff right now we're going to be going through all of it in Parts four five six seven eight so we start off with our spider so in here in our spiders folder there's just a simple spider called book Spider it's just a simple class it's got a name it's got some functions there and inside it has things like items which link into our items.py file here as we can see we're importing it and this is just a basic spider so I'll just give you a quick overview of what it does once you run the spider it goes to start requests and it puts this URL into this URL variable and then it returns scraping that request function with the URL and once the request comes back from the page with the HTML in it it goes to the next function which is to find in the parse function so this parse function then lets us use the response that contains the HTML and we can then manipulate this HTML and extract the things that we want such as the title category description price once we've got those those pieces of data are put into our book item and that is then returned to us in the console are if you've got other things set up such as feeds into a file so that might all be completely overwhelming for you but don't worry we're going to be going through all of this in extreme detail and showing you exactly how to do everything that is already here this is just to give you an idea of what's what Okay so you might have seen this book item that I mentioned so book items then links into our items.py file and in that file we just describe how we want the item to be set up so we want our book should contain title category description and price so then using this we can then use this book item both in our spiders when we fill the book item with the different pieces of data and return it and also in our Pipelines so in our Pipelines we have a simple test one set up here which goes through mimicking how you would then get the data that is returned um in the book Spider this book item with all the details and it goes through how it would save the item in a database so think of it we extract the data The Next Step would be to push the data into the item and then to put the item into a database so Pipelines are what happens once you've extracted and you're yielding returning the data from your spider so here we have for example process item it's fairly selfexplanatory it takes the item with the title the category description and it inserts it into our books database table so that's what gets put into items and item Pipelines again this could be all very confusing for you if you know a bit of python hopefully it shouldn't be too confusing but we'll be going into it in a lot more detail later on okay so then we have our middlewares so middlewares are where you can get into the nittygritty of how you want the spider to operate so it gives you control over lots of different things such as timing out requests how long you want the request to go on for what headers you want to send when you make a request what user agents should be used when you make a request if you want to do things like multiple retries you can mess around with that in the middlewares section and as you can see it comes with several kind of defaults that are there that you can either update to what you want or you can create your own ones that go in here too so you also have managing cookies caches there's everything like that would be dealt with in your middlewares now there's two types of middlewares there is downloader middlewares and spider middlewares most of what we'd be doing would probably go into the downloader middlewares but spider middlewares can also do things such as adding or removing requests or items handling different exceptions that crop up if there's an error with your spider handling things like that so all these middlewares go in the middlewares.py file and then last of all we have our settings so settings is fairly selfexplanatory it's where you put all your settings so you've got basic things like do we obey our robots.txt file when initial request is made to a website do we check that first and if it says don't scrape this site do we obey that yes or no it said here the number of concurrent requests we make so if we're scraping a website do we send one request at a time or do we send 10 or 100 requests at a time that's also set here so everything to do with how your spider and crawling operates will be either enabled or disabled in this settings.py file now we also have going back to what we were talking about our middlewares we have our spider middlewares as you can see here and our downloader middlewares as you can see here so this is where you can if you create a new middleware so this one directly links to the book scraper spider middleware that is right here so you need to make sure if you create a new middleware that you then enable it in settings also and also Verizon pipelines that's also where you need to enable if you create a new item pipeline that it is enabled in here also okay so I think we've gone through the basics of a full scrapey project and what's contained in there we've gone through what's usually in a spider gone through items and item pipelines how they can process the data once we've scraped the data from a page and then we've looked at middlewares and how in settings we can turn everything on or off so I think that's everything we wanted to cover in this part now again don't be too overwhelmed by this it does get a lot easier trust me so stick with it and in part four we'll be creating our first spider and extracting some data from a web page in part four of our scrapey beginners course we're going to look at how to create a scrapey spider using the Scrapy shell to find the CSS selectors we need using those CSS selectors in our spider to extract the data we want from the page and then finally we're going to get our spider to go through multiple pages and extract data from multiple pages so let's get going so I've got my terminal open here I've already activated my virtual environment I'm continuing on from part three so if you're just joining us here make sure you already have everything set up as we have done in part three I want to go all the way down into my spider's folder so at the moment it's empty it's just got an init.py file in it so we want to go down the level and down into the spiders so now I'm in my spiders folder and I can see there's just a new py there so in this spider's folder I'm going to run this command scrapey gen spider the name of my spider which I'm going to call book spider and then the URL of the website that we're going to be scraping and in this case it's going to be the books.2 scrape.com site which is a site that is there for people to practice their scraping up so if you go ahead and go down to your spiders folder and type this command into your terminal and hit enter scrapey will then create this spider as you can see here created spider book Spider using template basic in module and then it gives this so if we check that out now we can see that book spider.py is there and if I open this up in my vs code we can see it created book Spider here so this is just a very very basic spider we'll be adding a lot more to this but we'll just go through a few bits of what were generated here so obviously the name of our spider is book Spider so when we do Scrapy crawl to actually kick the spider off using scrapey we'll be doing scrapey crawl book Spider the allow domains list is books.2script.com this is important because later on when we're going to be doing crawling our spider is going to be going through multiple different links and having this allowed domains here listing only the domain we want to scrape prevents our spider from going off and scraping hundreds of different websites across the internet because as you can imagine URLs link from one page to another and sometimes a website might link to an outside website and in this case you would not want your spider to start crawling and scraping the entire internet so that's why we have allowed domains here next we have start URLs so this is usually just the first URL that the spider starts scraping but you can actually have multiple URLs here as well for it to go through one after the other then we have our parse function our parse function is the function that gets called once the response comes back so we'll be filling this parse function with all the different pieces we want to extract the data from the page itself Okay so we've gone through the basics of this generated spider the next thing we're going to do is we're going to use the scrapey shell to find the CSS selectors we want to get the data from the page so what I mean by CSS selectors for those of you who aren't familiar so first of all if you just open your developer tools you can do this by right clicking in Chrome or Safari or Firefox and it's usually inspect or sometimes it's called developer tools so you do that and this comes up here if you go to the elements tab you'll see then all the makeup of the page in HTML and CSS so here for example we've got a H3 tag and an a tag for links and this is the link to the page of this book here so we'll be looking now at how we can actually pick out these tags so that scrapey knows which pieces of data we want to extract from the page itself okay so let's just go back to our terminal and just to make using the Scrapy shell a little bit easier we're going to do just a pip install and then I python which just is a different shell would help if I spelled it correctly this is just a different shell which is a bit easier to read so I just did pip install IPython and then to activate this we want to go to settings and we I know we want to go to sorry not settings this scrapey.cfg and we're going to add the shell as a separate line here so now that that's done I can close Dash and we can run scrapey shell Scrappy shell then gives us this so what we want to do now we have scrape your shell open is as you can see we've got a list of the commands that it gives us that are available so we can do useful shortcuts fetch which is the command that we're going to be using so this fetches a URL and updates the local objects so we'll just run this fetch command now and I'll show you exactly what I mean so we want to fetch this books to scrape.com so I'm just going to paste in this URL here and it's going to go off it's going to fetch this and it's going to put the resulting HTML everything in here that we see into a variable inside in the scrapey Shell so we can access it and run different commands on it so enables us to kind of practice the code we want to then put into our spider so what we want to do is it put everything in from the page into this response variables so now we can just do response.css and let's say we're going to look for something specific on the page so what we can do is move our Mouse over these different tags on the page we can see article contains one book on the page so we can just say okay give me article and we'll just have the class name in as well so any class name needs to have just dot in front of it when we're referring to it like this so that has given us all the different books that are on the page now let's say we want to just get the first book we can just do this with a guess and it's giving us just the HTML that is for that first book if we want to then put all the books into a different variable so we can run some other commands on them within the scrapey shell we can do something like books is equal to response.cssarticle dot product pod if we do that it's after putting all the different books into this books variable so then if we run Len on books then in Python gets us the length so it gives us that there's 20 books if we go back to our page we can indeed see there is 20 books so there's four in each Road there's one two three four five rows showing one of 20 so that's correct Okay so for the purposes of this part four we're going to extract the name of the book the price of the book and the URL so we can actually go in and get further details later so the name the price and the URL so now that we've got our books what we can do is we can put the first book of the list of books so we'll make a new variable called book and we'll say that's equal to equal to books and the first item in the list of books that we have so now if I do book.css and then I go back and okay we want to get the the title of the book here so I can see from this that we've got a H3 tag and we've got an A tag and I want the text that is within this a tag here so I'm going to do H3 and a and that should get me the text that I'm looking for the title of the book so go back here and I do H3 a and then we just need a little bit extra which is just this text and I do get that gets me exactly what I was looking for which is a light in the dot dot dot this corresponds exactly to this so I've got the title of my book next I want to get the price of my book so I'm going to just remove these two pieces here and I'm going to inspect the price okay so if we look at dot product price and then dot price underscore color should give us the price so let's do that now so that's Dash product price color let's try and run that okay it didn't exactly what I I know because I S I did an extra double dot there you go so that got us exactly the price we're looking for and finally we want to get the URL so the URL is interesting because it's also part of this h3a tag but we want instead of the text within it we want this href attribute here which contains the part of the link to more information on the actual book itself so if we open this up in a new tab we'll see the full page that we're looking for and here you can see full project description and lots more details there Okay so we still want to do H3 and a but instead of text we're going to say a a trib e hatred that gives us our href attributes that was contained in this a tag that we were looking at a second ago so using the Scrapy shell we've managed to see how we can use the CSS selectors to extract the title the price and the URL for one book so now that we know that we can add these into our parse function and we can also Loop through all that list of books and get all the details for the 20 books that are on the page Okay so let's start adding things to our parse function so first I'm just going to add in what we initially had to get all the books that were there and that is books equals to response dot CSS article and then product underscore pod so we had that up here so I'm just taking this line here that we used in our Scrapy shell and I'm putting it in to our parse function okay the next thing we want to do is we're just going to Loop through it so we just want four book in books and then we are going to type yield so yield is like return and then what we want is scrapey to return to us is going to be the name the price and URL so we'll start with the name and then we're going to go up to where we got our text and we're going to use this exact piece here and then we're going to get our price and we're going to go to where we got our price and then last of all our URL and for that we have our href attributes okay now that we have that we should be able to go ahead and run our spider and see what happens so first let's exit our scrapey Shell by typing exit and then we might need to go up a level to our book scraper folder and we should be able to run scrapey crawl book Spider which is the name of our spider so if that goes according to plan we should see item script count of 20 there are the 20 books on the page and you can see what was returned here the name there is a book name a price there is a price and URL there's the URL and if we just scroll up we can see that the 20 books that were on the page all the data was script and output to our terminal so that worked exactly how we wanted it to work now as you've seen we have multiple Pages there's not just this one page of 20 books there is actually a lot more than that so we're going to look at how we can go to the next page if there is one and then scrape all the books on the next page and then keep looping through all the pages of books until there are no more pages of books to scrape so as you can see here we have a next page at the bottom of every page of books so if we click the next page button it goes to catalog page2.html and then we have a new page of 20 different books and as you can see it's going through all the different pages page three and there's a previous there as well to go back a page so we're going to want scrapey to bring us to page Dash three our page4.html if there is another one to scrape so we're going to go back and we're going to do scrape your shell again to open our shell we're going to again roll in our fetch command to fetch our website URL and then we are going to try and get the link so to do that we're going to inspect the next button and as we can see here it's in An Li tag and then it's got a class name of next and then within that we want the link which is contained in this href attribute and that's contained in an a tag for links so let's see if we can get that now using our scripture so we do response.css and then allies dot next so Ally for the Ally tag dot next for the class name and then a and then we want the href attribute so so that c can me to Dash and that gives us exactly our catalog forward slash page 2.html which corresponds to well this was not the exact one we're looking at we're looking at page one so I can just remove that and go down here and this one should have catalog for slash page 2.html and that corresponds to this so now that we know how we can get the next page we can just put in under our Loop we're just going to paste what we had here to get our link and we're just going to do next page is equal to and this is going to contain our next page link so the next thing we need to check for is if we get to the last page there's going to be no more next page link so that's how we can know when we've reached the end so we can check that by going to page 50. so if I type in page 50 and go to the bottom I should see that there is no more next button there's a previous button but there's no next button so I've reached the end so that's what our test is going to be we're going to put in an if statement and we're going to say if the next page URL is not none then we know there's another page so we can continue going until there is no more pages left to scrape so let's add that in now okay we're just going to do if next page is not none and then next page URL is equal to and here we're going to create the full URL because next page doesn't contain the full URL it's only a relative URL so we need to get this part of the URL plus the catalog forward slash whatever the next part of the pages so let's add that in save that and then the important part is we need to do yield response dot follow next page URL and then call back is equal to self Dot parse okay so what this does is it obviously creates our next page URL and then we tell scrapey to go to this next page URL using response.follow and the Callback is the function that's going to get executed once the response comes back from the URL that we've gone to so once we get the response from that URL it's going to kick off self.parse and self.parse is this function again so it's going to keep going through and keep going through and keep going through and calling itself until there is no more pages and then in that case it's going to stop let's try and run that now so let's exit out of our scrapey shell and let's just do a scrapey crawl again scrape your crawl and book Spider and see what we get okay so with an item script count of 40 so it scraped four pages but obviously four pages is not 50 pages so there's a bit of a bug here which we're going to have to get to the bottom of let's start looking at the next page URL because that's obviously where it's going wrong if it's only finding four pages there must be an issue with the URL here so if we go back and we again inspect the element so here we can see it's page50.html and I think we had and here it's 49 but then if we were on the initial page and we check here it's got catalog forward slash page two so sometimes it's got just page Dash 2 and sometimes it's got the catalog in t href so that's obviously why it only scraped four pages so the fifth page only has page five so we're gonna have to just modify our next page and if statement here just to check that we have catalog in the href if we do then we do a slightly different next page URL than if we don't so let's just add that in now so if the catalog forward slash is in next page then the next page URL is going to just be what we currently have and but if it's not we want an else and we're going to say add in the next page URL but we're going to add in catalog here so this should ensure that the next page URL is correct so if it contains catalog we don't need to have catalog in this part if it doesn't contain catalog we do have to have it and then that should make the correct URL so hopefully that's fixed that bug so let's try run the scripty crawl one more time okay so this seems to be going through a lot more pages which is a good sign and let's give it another minute or two to finish up and we can see the total item counts at the end and the total page of script so this is kind of the process that you have to do when you are creating a spider to scrape data there'll be small bugs like this that pop up and you need to do a small bit of detective work to find out why your spider is failing at certain parts or not able to extract certain pieces of data from the page so we can see here we have response received count to 51 I sub scraped count of a thousand so if we go back there is a thousand results so it scraped all the books that we were looking for that's pretty much everything we wanted to cover in this part four in part five we're going to go through and each book we're going to click into the book and then we're going to extract more product data from the product page itself so right now we're just kind of doing the easy thing of just going through one page one page one page just the name and extracting just the name the URL and the price but in real life scenarios most of the time we want to get a lot more data and that involves doing things like going in clicking into the actual product we're looking at and actually getting more indepth data so we'll be looking at how we can do that in part five so in part five we're going to look at how to crawl Pages using the scrapey spider using CSS selectors and expats to extract more complicated pieces of data from Pages such as from tables from breadcrumbs things like that and then we're going to move on to saving the data into certain file formats such as CSV or Json format so let's get started we're continuing on from part four so if you need to get the code for that we'll have that available for to for you to download and follow on at this point or if you've got your part 4 already completed you can just continue on from there so in part four we just ran our spider and it went through and got us the details of the Thousand books that are on the books to scrape.com site so the next thing we're going to be doing is we're going to be instead of just scraping the URL the price and the name of the book we're actually going to be going into the book page itself and we're going to be taking things such as the rating the product description the product type the price excluding that including that the category that it's in such as poetry in this case so we're going to be looping through all the different books looping through every single page and getting all the specific data for each book that is on this site okay so let's go back to our Spyder code and the first thing we're going to look at doing is we want to start going into each page individually and so to do that we're going to do something that's very similar to going to each next page URL so we'll copy this code from the bottom and we're going to do this for every book in the list of books so instead of just yielding the data we're going to be going into the URL so we're just going to paste that in over our yield right here we're going to remove the next page nonsection we're going to move our next page URL Pier and we're going to have to just modify where we get the URL for the each individual book so if we just go back into inspect the elements again and check out okay so that was H3 and a and we want the href for that H3 a tag so we just want h3a attribute href that should get us the next page and then it should create the correct URL for us instead let's call of next page because it's not the next page it's the relative URL of the book lets us call it relative URL and let's just put this here and the next thing we want to do is we want the Callback function instead of being parse we're going to do parse book page so we're going to make a new function to parse the book page individually one by one so let's go down here let's do Def passbook page self and response and at the moment let's just put pass in there so fat is going to Loop through instead of next page URL I'm going to call this one book URL and Okay so the only other thing to change that is incorrect is that obviously we need to Loop through the list of books so this book needs to be used and we're going to get the book.css and that's where we're going to get our link from with the this link here so that goes to relative URL and variable then we make the correct book URL and then using this book URL we yield um so we basically go into this um book URL and then the response HTML that comes back from this URL will get parsed by the parse underscore book underscore page function which is the one we made down here okay so I think we can save that for now and next thing we want to do is start flushing out our parse book page so what we're going to do first is we're going to open up our scrapey shell again like we did in part four and we're going to look at the different CSS selectors and expat selectors for the different items that we want to scrape on the book page itself so let's click into one of the books and we're going to see what if we want to extract from this page let's go back open up our scrapey shell in our terminal again so just Scrapy shell and when that opens we're just going to use our fetch function again to fetch the full URL from one of the book pages which in this case is just the very first book I've picked in the list so I can just put in the URL in here hit enter that's going to go off get the HTML of that page and stick it in the response variable so just like we did in part 4 we can see what works and what doesn't work with our CSS selectors so let's just do response dot CSS and then let's inspect the page again and just okay so we have product description there is an ID and there is a P tag underneath that so product pay underscore page there so that gives us the whole page so let's just try and see what happens if we do product on the score page that seems to give us back the whole page now let's look at getting the title of the book for example so in this case on this page it's in the product underscore Main and it's H1 so let's go ahead and just do so Dosh product underscore Main H1 text and there we have a Light in the Attic which matches to our Title Here so that's very simple just as we've done before so now let's do something a little more complex let's get the category up here so we have poetry in this case so for things that are a little bit more complicated like this sometimes it can be easier just to use expats instead of CSS selectors so expats are very similar but instead of using class names directly the format of how we write the expats is just a little different to how we would use CSS selectors so I've got one prewritten out which I'll just paste in here so paste in my XPath and that gives me poetry so I'll just explain to you how this gosh poetry so it went to the UL HTML tag it's put in the class breadcrumb so if we go back to the top here we should see it's in a UL HTML tag and the class is breadcrumb and then we have several Li tags and then we have an a tag within the LI tag and we have the href so here we can see that's where tli tag comes into it and then the active class is on the grayed out section here so it's then going from it's going to here and then it's saying preceding sibling so get me the preceding Li tag before the one that has the active equals class in it so it's going to here and then it's going back one to the preceding sibling and it's getting the text within here and it's doing that with the text at the end here so as you can see proceeding sibling Li one and then a and text at the end so expats are quite similar to CSS selectors in not every case will you have a class name or an ID tag on a HTML tag so in the case of the product description which I showed you a second ago we'll just look at it again there is no class name or no CSS ID on on this P for paragraph tag here so in that case as well we can say right go to the product description using the expats and then get me the following sibling that's a P tag so I can just show you that one so it's go to the product description ID get me the following sibling with the P tag and then within that get me the text so that's how expats work for getting some of these Corner cases where you might not have a simple class name or a simple ID on the HTML tag okay so we know how we can get the product description and the category tag up here we know how we can get the price and the title next let's look at extracting data from tables so if we inspect element again we can see that this is all contained within a table and this table has several rows which have TR as the HTML tag and then within the table row we have th and TD and that goes the whole way down so each row has one more th and TD so what we can do is we can specify okay get me all the rows in this table and then we can say okay we know that the product type is always going to be the second row so let's always look for the text that is within the TD of the second row if we wanted for example to see the product type so first we want to get all the table rows so we're going to look at table and then all the TRS that are in that table if so then let's assign that to a table underscore rows in our Scrapy shell so if you just use table underscore rows equals to response dot CSS and then have table space TR within the brackets that will make sure that all those table rows are within table rows we can quickly check the length of that that gives us seven rows and there's one two three four five six seven rows so now we can do something as simple as table underscore rows let's look at what we said the second ago the second one the dot CSS again and we want the TD and we wanted the text within that and I guess again we use get and we get books so again the numbering starts at zero and then the second one is second line is number one and then we look at the T D here the TD and that gets us books so that's how this line corresponds to here in this table so knowing that we can then go ahead and get things like the price excluding tax we can just put in something very similar the next row down and that should give us the price excluding tax so we now can get all the data we need from this table and the only last thing to look at is looking at how we can guess the Stars so if we just look at inspect element we can see we have several Stars here it has icon star icon star and star rating of three so that's where we can see the number of stars it's within this class that they've written three so we need to do something slightly different for this so first off we're going to get the star region so we're going to do response.css then p star rating for the class and then we're going to ask for the attribute of class so this is the attribute the attribute name is class and then it should give us our three that we're looking for so let's just do that now so response dot CSS P star rating attribute class and that gives us star rating of three so using our scrippy shell we've looked at how we can get all the different pieces of data from the page so let's start filling that into our parse book page function here so we can actually get all the book data to be scraped correctly okay so let's just exit out of our Scrappy show and let's first just get the table rows so table rolls are going to be equals to response dot CSS and then we're gonna have what we had up here where you can see table rows is table TR so table TR is a table rows and we can work with that now to fill in the rest of the details so we want to remove pass and then we're just going to do yield and we're going to have our details inside here so let's start off with the URL that's easy because we can just use the response sponsor URL so the URL of the page is contained within this response object then let's get the title so we had Dash up here here so can just copy this directly don't forget to add commas at the end and let's get the product type the price excluding tax including tax the tax so these are all to do with the table so we're going to be doing table row one for the second row and then TD text get for the product type and so on for the price excluding tax and including tax as you can see price excluding tax price including tax they're all one after the other here so all we're doing is incrementing the number here and the tax itself and we might as well add in the availability and the number of reviews as well since that's just a continuation of the same thing so add those two in we'll add in the Stars by doing this so let's copy we have them here and we'll just call that Stars and we can just paste that directly and let's also get the category and the description like we had a second ago so category or with that using the expats so that was this guy here let's just paste Dash let me have to just get it all in the same line perfect and let's get the description okay and we just want this XPath that we were using earlier in our Scrapy shell as well again just making sure that we have it all in the same line and that we add in our commas finally the only thing missing is the price and we can do that by getting the response and then dot CSS P dot price color and then the text from that so the price is up here and that's the class of price color within the P tag so if we save all that that should be everything we need to parse the individual book pages and the thing that's just missing here is the next page which we just need to add back in we deleted that earlier by mistake so that's how we get the next page URL okay so everything else looks correct so just a quick recap the spider is going to kick off go to this start URL the response that comes back the first time around we'll go into this parse function this parse function then we get all the books on the main page so that is starting with all these different books here then the next thing that happens is we get the relative URL and we turn that into the book URL we do that by getting each of these URLs here once we get the first books URL we then go into that book page so what happens is the code basically goes in here it's then goes to the Callback function which is parse book page down here it goes gets all these details here that we specified and then is Loops to the next book on the page because this so it comes back out here and then Loops back up to the start and goes to the second book on the page goes Clicks in gets the data comes back does the third book in the page so Loops through all the books on the pages keeps getting all the data for each book and then it goes to the next page and then once all the pages are done it finishes so if we've done everything correctly we should be able to now run our spider and see does that work so let's try and do a scrapey crawl the thing we're going to do that's slightly different this time is we're going to have the output go to a file so instead of having the output come into just our terminal we're going to also get it to save to a file so we do this by doing minus all uh our Dash o and then we're just going to call this book data and we'll do two um CSV CSV so CSV file formats can be opened in Excel or can be put into you know Google Sheets and different applications like that so it's CS face just stands for comma separated values so if we run that hopefully there's no issues and as you can see there's book data.csv here you can open that and we can see we have nodes of data so all the stuff that we were looking for price description everything else seems to be there so I'm just going to stop that now before it gets the end because that seemed like it's working correctly it was already on page 15 here let's see okay and I'll run that one more time except this time and do it in instead of book data.csv we're going to get her to Output to Json so I'm just going to delete that one and get it to a push to Json format instead Json format can just be a bit easier to read and if you're doing further coding it can be easier to parse as well so if we opening up as you can see it has all the data nicely formatted the title price including tax availability the number of reviews all the data is all there so that's working nicely obviously it's going to take a minute or two to scrape all a thousand books but I think that's everything we wanted to go through in part five in part six we're just going to be looking at how we can use items and item Pipelines to better structure and clean our data before we start saving it into things like a database so it'll just put a bit more structure on our code and it'll enable us through things such as for example we could change the prices from pounds to dollars four gets saved we could you know remove any trailing white space lots of different examples we'll go through in part six of how to clean up the data okay see in part six guys so in part six of the scrappy beginners course we're going to be looking at scrapey items and Scrappy Pipelines so first off we're going to go through what scrapey items are then we're going to use scrapey items to structure our existing spider a bit better then we're going to go into what scrapey pipelines are and what they do and then we're going to use the scrapey pipelines to clean our data so let's get started if you're continuing on from part five you should have everything already set up if not you can download the code from our repo and continue on from where we are now so I'm presuming you already have your book scraper project set up with your spider set up and you've got your environment activated and you have screen be installed and python installed and everything else is running okay so items so when you generate a Scrapy project it generates this items dot py file and this is where you put your items so items just help us Define what we want in a block of data that we're scraping and that were returning so for example here you can see in our book Spider we have no specific item declared we're not using an item that's been created in relationship is that py but instead we just have a yield with all the different pieces of data we're extracting from the page so that works fine but just to clean things up and to make things a bit less ambiguous the best thing to do is to use the items.py and declare a specific item in there so let's go ahead and do that now so I'm just going to copy and paste the one I've already got in called book item the book scraper item is just the default one you can leave that there for the moment so book item just has everything that we already have used in our book Spider so URL title product type all these different things but instead were declaring them specifically here so you might say well what's the point of that well one example is that if I do a Miss type and reviews goes in like this this might then not go into my database or might not go and be processed further down the line and I might not even notice it but if I'm using an item scraper will throw an error and say this Norm underscore reviews with two r's does not exist and it alert you to the fact that there is a typo here so that's one very good reason as to why we will use our items and actually Define the item first so now that we've got the item to find we've got our item class created let's actually start using that so first off we want to import that into our spider so we go up to the top and we're importing book item as you can see it brings us directly to the book item now the next thing we want to do is we just want to specify a book underscore item is equal to book item and then we're just going to yield book item at the bottom so instead of yielding just start texturing there we're going to yield book item and then we're going to remove those two brackets and we're going to say book item URL is equal to response to the URL and so on all the way down so change all these into using our item and then once that's done we'll start looking at item Pipelines so let's look at the data that have been saved into our file so this book data dot Json was what we did in part five that was the output from our spider that ran so as you can see we had things like the URL the title so on so forth but if you noticed we have the price excluding tax for example has this encoded value here so it looks like the pound sign did not go in correctly so you can specify a specific serializer that you want to use on a specific field so for example if it was like the price I've a serialized price function I can write and I can then use that serialize price to stick a dollar or a parent sign in front of the value so for example I can stick serialized price and I'll put it in front of the price excluding tax so I could just do something like serializer serializer is equal to and then serialized price so that would make the value go in here and then have the pen size applied to it before it gets put into price excluding tax so that's also a cool way that you can use items with serializers so I'm just going to remove that one for now because we actually end up processing the data from this in item pipelines in a second anyway I just wanted to show you how you could use this if you didn't want to do pipelines and you're you're only going to scrape a small bit of data and you didn't want to do a lot of postprocessing there's no point using pipelines and you could just stick to using just items and have a serializer if you needed to but if you're going to do anything more complex and you want to do a lot more processing of your data you're better off using pipelines instead of just using serial lasers the next thing we want to do is look at our Pipelines so in our pipelines again scrapey defines a book scraper pipeline when you create the project this is just here to give you an idea of what you can get started with so using pipelines you can clean your data for example you can remove the currency signs if you want you could convert the price from pounds to dollars you can format strings to integers if you're going to save it into a database that becomes very important and you can do things like converting your relative URLs to full URLs you can validate your data check if the price is actually a price or is it sold out and then in that case you can you know put in a price of zero and you can also use the pipelines to store the data so instead of having all the data going into a file like we've done in part five we could have it we could use a pipeline to get the data to go directly into a database which we will be doing in future parts of this series so let's clean up our data a bit now what do we need to clean well straight away this is not good for our data this encoded value here so we need to sort that is another thing we need to sort out could be the availability of the stock so you might say okay in stock 19 available is fine but if I needed to run a piece of code later on this data that's not very useful because I just want to know that there's 19 books I don't want to have this extra text here and here and brackets so if I just wanted availability to be 19 I could use the pipeline to remove the in stock and the available parts of the string and just convert that 19 into an integer okay so we'll do that also and I think I saw in some places that things like the title had a trailing white space are the descriptions had trailing white space so that's also something that we could remove and another thing would be changing the category we could change the category instead of it being Thriller with a capital we could change that to Thriller with lowercase so this kind of standardization of data before it gets saved into a file or into a database is important especially when you start scripting at scale and doing larger projects so we're just going to go through a bunch of different processing in our process item in our pipeline so we will just add everything in here and then the item will be returned so let's start with just removing the white space so I'll just paste in the code I've already got and talk you through it okay we straight away get our item which gets passed in to our process underscore item so we've got the item available we pass it into the item adapter so as you can see up here useful for handling different item types with a single interface with this adapter we can get all the field names and then we can Loop through using our for loop loop through all the field names and if it's not the description we want to use the strip function to strip the white space from the strings so we're just getting the field name and then stripping the value and putting that back into what was initially there for that value okay now let's quickly look at converting the product types uppercase to lowercase if there is an uppercase value for the for example thriller or poetry values we can specify specific keys that we're looking for in this as I mentioned we'd look at category you can also do things like product type and we're going to just do the same thing except we're doing the lower function on the value now let's look at cleaning the price data as I mentioned earlier and as part of that make sure that the price data is saved as a float which can be important all the prices aren't always going to be rounded up to the nearest dollar or pound or Euro for that kind of data so here we Loop through the different price Keys which because we're saving several different pieces of data we've got price price excluding tax price including tax and the tax and for each one of these we're replacing the parent sign with nothing and we can also do something like replacing the for example Unicode with a specific value the other one I wanted to do was to change the availability to remove the extra text that was in there so let me quickly add that in to do that we're just doing the split function on the bracket if it sees that there's break there's no bracket there then we'll just set the availability to zero if there is a bracket there then we will split the second piece of the array that is returned from this function and we will say okay the second piece of this we'll split that again using the split function and we then know that the first item in this availability array is going to be the availability number that we had here so this is going to be the first ISO in that availability array and this is going to be the second so that should save just the number for US of the availability and we'll save that back into our item let's just look at two other ones quickly so just converting the review to a integer so we'll just convert Dash so the number of reviews make sure that it's an INT so we're just going to adapter.get and then we're using our int and putting the string in inside the brackets and saving that back into the number of views variable and last of all we mentioned the star rating and we want to turn the star rating into an integer also so to do that we can just get the Stars split the string using the split function again we've got the array we take the second value in the array converted to lowercase and then depending on what the value is in that variable is it zero one two three four five then we save the Stars value as 0 1 2 3 4 5. so pretty easy nothing too complicated there so that's everything I wanted to cover for for pipelines so as you can see there's a huge amount of data processing that you can do on pipelines and it's a good idea to have a look at your data do one run of it like we did in part five and then have a look at your data and actually see what you can fix what needs to be fixed up what looks okay what doesn't look okay sometimes you'll get a missing piece of data there'll be blanks but this is a process of refinement so the first time around you might only you know add in two things to your item pipeline you run it again and you notice something else is wrong and you add in another piece into this pipeline so the next thing you want to do we talk about this in part three is if you've got a pipeline you want to go into your settings and you want to make sure that the pipeline is enabled so we've got our spider middlewares our downloader middlewares extensions and as you can see here we've got our item Pipelines so this book scraper pipeline should correspond to the name of our class here and if I put that in you can see they're the same so that should work because this is also generated by scrape when you generate the project so it generally works as long as you uncomment this section here so if everything was done correctly we should be able to now run our spider and see the results with all the data processed just as we want it to be processed here if there's any errors they'll pop up and we can fix them and run it again so I'm just going to make sure I'm in my project and then just you want scrape your list to make sure everything's working and then one Scrapy crawl book Spider hopefully there is no issues okay straight away I can see there's an error being returned so I'm just going to stop my spider so none time none type object has no attribute next call and we can just scroll up and double check this so spider must return request item are none because ice and meta in guess okay so let's sort out this error so if we just go back to our book spider.py file you can see the error is because I'm returning book item and yielding book item and instead it should be book underscore item so that should fix the issue and if I do a Scrapy crawl again this time I'll actually get it to go into another file we loot call clean data dot Json so it's the hyphen capital O clean data.json and hopefully there's no other errors there it does look like there's another error because if I check clean data.json there's nothing there okay so I'll just close it again and you can see okay error processing availability to give us anything else so it says pipelines.pyline 21. topple object has no attribute strip so we can go to our pipelines line 21 okay so we have our value.strip and it's saying Tuple object has no attribute strip so let's just print out the value of value let's just add in something above it so we can just see where it is in the output and try runish one more time and if we stop it again and scroll up we should be able to see that we've got our stars and we've got in stock available at 19. and it is indeed being returned in a topple uh with the second value there's nothing there so obviously we need to reference the first value in the toggle so we need to just do that so adding the square brackets since zero should return just the string that we're looking for and then dot strip can act on this string so if we remove our print statements save that and build Traverse again and it looks like there's some errors coming in there we can just check our file there's nothing in here yet so I'll go ahead and stop the spider from running and we can see an error here pipelines.py line 21 in process item so that's still giving it a bit this line but this time it's saying type error none type object is not subscriptable so I know what this error is I've had it before so this is coming up because we're getting all the field names which are a from our items.py so it's getting all these different field names here it's looping through them and one of these field names is not being found so if we look at our spider and compare all these guys here versus what we have here I think I've spotted the one already so I think it's this UPC field unique product code I think it stands for and if you look here we don't have book item UPC so I can just add that in now so I'll add that in here and save that so now we should have this which should correspond to this and we should have no more errors so let's run that again and this time we should see our clean data.json file filling up so everything looks good there open up the clean data chart Json we've got what looks like all the data we wanted so we can go ahead and just stop the spider don't need it to collect all 1000 records you can just double check that everything did go in correctly so you can see just by either checking the file or scrolling up did everything get processed the way you wanted to get processed so did the price get get processed correctly is it now double the product type is the first part lower case yes it is the number of stars is now an integer so it looks like everything that went through our pipelines.py got processed correctly we can scroll up and check the category as well and the availability so everything worked out so that's just how we go through using pipelines and items I hope that's given you a good idea of how you can use items and pipelines yourselves to clean the data that you're scraping and in part seven which we'll be looking at next we'll be looking at how we can use pipelines to save our data into databases and also how to use feed exporters in a bit more detail so see you in part seven guys so for part 7 of our scrapey beginners course we're going to look at all the different ways we can save data so all the data we've scraped in the last few parts we're just going to see how can we save it to different file formats and then eventually look at databases so first off we're going to look at Via the command line how what commands we need to run to save it to different file formats then we're going to look at how we can do that instead via the feed settings which we can set in our settings are in our main spider file and then once we've done that we're going to go on and look at saving data directly into a database using the Pipelines so if you've done part six with us you know all about pipelines by now and we'll be using those pipelines in part 7 to save the item and data into the database directly if you're just joining us now you can download the code from our GitHub repo we'll have links for that and you can follow on from just this part 7. to get going I'm just going to go into my book scraper folder make sure I'm in the right place and then run Scrappy crawl and the name of our spider crawl book Spider Dash capital O and then book data.cs V so this is going to Output the data into a CSV format which is comma separated values so that can be opened in Excel and as you can see the data is all there correctly okay so we can stop that now and if we scroll to the bottom we can see we have 321 rows so if you want to append data onto a file instead of The Hyphen or Dash capital O you can do a lowercase o and then if we do the same name again book data.csv and enter it should start pending on the data here so instead of overwriting the file every time if we close the file open it back up you can see we're already up to over 500 records so the file doesn't update automatically sometimes it can take a couple of seconds or you have to close it and reopen it so as you can see we're up to 700 records there and if I relish once more and do a capital O it will overwrite that there you go so it's after wiping the file and now filling it again okay so that's the difference with the overwriting are appending and you've seen just by changing the file format at the end of your file name is how you can specify types of files that you want to write into so here we're going to do it again but we're going to do it in Json file format so we have a new file is created book data.json as you can see it's in Json format and the other one is comma separated values format now let's move on and look at how we can specify where the data will be saved in our settings file so if we open up our settings file what we can do is use the feeds so what we do is add in a feed section and here we're saying save the data into a file called data.json so that's the scholars books data dot Json and the format is going to be Json so if I delete the two files we've just been using there and save what I have in my settings and rerun it except this time remove the Dash o and the name of the file and if I just go ahead and run my spider we've seen it's just created at books data.json because I've specified it here and it's all in the correct format so I'm just going to go ahead and stop my spider and the next thing you want to do is just show you guys how you can specify the feed data in your spider so to do that we can use the custom settings so this just enables you to overwrite anything you have in your settings file and you can just specify it in your spider so just need to specify what we want to overwrite and we're going to overwrite our feed and we would then push our feed settings in here so if it sees that the feeds are set here it will overwrite what we have in our settings.py file so this is just an easy way that if you guys want to specify certain settings you can do them here they don't all have to be in your settings.py file one important thing to note as well with our feeds when we either set it in settings or in custom settings is we need to set the overwriting so like we did earlier we just have overrice true our overwrite false because it depends on where you're storing the data what the default is for that setting so it's better just to specify that we want to or write the file or not we can just have it there and run it and it'll overwrite our current file so now that we have that the next thing we want to look at is how to save our data into databases using our Pipelines so I've gone ahead and I've already installed MySQL so MySQL is a very popular database which you can get yourselves just by going to mysql.com site and their download section and you can just choose your operating system so if you've got windows obviously have it for Windows click download and install it then they have for many other operating systems the available downloads there too so once you've downloaded and installed that you should be able to then make sure it's installed correctly just by running MySQL and then dash dash version so as you can see here I have version 8.0.32 for Mac OS 11. and that's the latest 8.0.32 so that's installed for me so the next thing I'm going to do is just Connect into my MySQL so I can just type MySQL and then if it's a simple install and you've just installed this you should be able to just hit enter and it brings you straight in and you know you're connected in because you've got MySQL here and then you can just say show databases and it shows the databases so I've already gone ahead and created one called books that's there but you obviously won't have that if you just installed it so you want to create a database so you just do create database books and then it'll say created I've already got the database already there so it says database exists for me so we need a database to actually Savor things into and once it's set up it'll be there in your list and you just do show databases to get the list of databases that are available so we can exit out of that once we have our database created you might have to connect in to your MySQL if you've set up a username and password are with a different host if you've so you could do host localhost minus U for user root and minus P if you've got a password and then it'll prompt you for a password so depending on what you have so that also works to connect in if I'd set up a password it would have asked me for a password beforehand so if you just type MySQL you can usually get in if you ever already haven't got a password set up or if you're using a different host like digitalocean or some other thirdparty provider you can stick the URL to where your database is hosted there so we've got our database set up and the next thing we want to do is we want to install our MySQL connector so just to make sure that python is able to connect to our database so I'm just going to paste in the command for that and you guys can have a look at my screen there and type it in so it's going to install the MySQL and MySQL connector python packages with using pip again so go ahead and run that and now that I have that I should be able to start working on the pipeline so we can just go directly under our existing book scraper pipeline and we're going to create a new class and we're just going to call that save to mySQL pipeline and then we're going to import our MySQL connector to help us connect in and we're going to initially just when this pipeline is initialized we're going to set up our connection and then set up our cursor so I'll show you guys now what that entails so we have this init function here so this is going to start up when our spider initializes this class we have we're using the MySQL connector.connect here to set up the connection we've got our host our username password if you have a password you have to add it in here and then the database that we just created books so we can save all that and then we have the cursor which is used to execute the commands so I have that set up here and that's saved into self dot Cur so we can use it further in other functions so the next thing we want to do is we're going to add in that we want a new table to be created if there is no table to store the data so this can just be handy in case you are running this over and over again or you're testing you might want to go in drop the table and if you don't want to remember did I just create that is a table there or not you would have this there so that will just make sure that there's a table there so it creates a table if it doesn't exist called books and that table will have the following columns ID URL title UPC product type everything that we've already been scraping from the page all the different data points so it will set up all these different columns including a primary key called ID and then all the data will be able to saved into the columns that we want it to be saved into so we'll have the table set up if it's not already set up so that's we don't have to go in and manually set it up in MySQL ourselves and the next thing we'd want to look at is we're going to again use the process item function so we've already had that in our other pipeline but we're going to add it in here and this is where we're going to have our insert statement so it's going to insert our data that we have in our item so here it is pretty simple so using the cursor that we've already defined above I'm going to say please execute this command insert into books URL title UPC product type all the pieces of data that we've already scraped so once that's insert statement just there we have to use commit to make sure that the insert is actually executed correctly and then we just return the item so that if we add one more layer to our pipeline that the item is returned and the next piece of our pipeline can also continue the only other thing we need to add in now is we want the connection to our database to be closed once the spider is finished so to do that we just add in that enclosed spider so this is just a function that's scrapey looks for if close spider is there it executes closed spider once the spider is ready to close at the end so inside enclosed spider we just add in cursor close and connection close so we're just closing the cursor and the connection just so that this stuff isn't kept open and using memory if we're executing this lots and lots of times we don't want all this memory to be taken up with cursors and connections that are not being used so now that we have that we need to go to our settings and we need to enable our new pipeline so we're just going to copy the existing line and we're going to say execute our pipeline after this and existing one so we want the data to be clean first and the second step is save the data into our mySQL database so we're going to just copy the class name go to settings paste that in here and then the only thing we need to do is we need to change this number here so I don't think I've talked about this number yet so what this number is is that it's just the order in which the items in the item pipeline have precedence so the lower the number the higher the order of importance the first thing that's going to be executed so in this case number 300 is going to be executed first and then number 400 is going to be executed after that so this is an easy way for us to say please execute this pipeline first and this pipeline second and if you had multiple pipelines you can just use these numbers it doesn't have to be three or four hundred it can be any number you want I've just picked three and four hundred for now okay so now that we have that we should be able to go ahead and check our database to see did the items get saved into the database correctly so let's do that now so as before we're just going to do Scrappy crawl book Spider and should kick off so we have several books after being scraped so that's let's stop our spider the next thing we want to do is we want to log back into our MySQL console and then we want to show database this is and then we want to use books this just enables us to select from that database so once we're using the books database we want to just show tables and we can see that the table was created that we asked to create books and then we can do select all from books and we can see that there's 138 rows there and Dash D data looks like it's safe correctly we've got the name of the books all the other pieces of data that we had the description the price the tax the availability the category it all looks like it's saved there correctly so we can drop the table so dropping the table just basically removes the table so that it won't exist drop table books if we want to start again because otherwise what it's going to do is it's going to keep appending on to the database so we can see if we show tables that there's no more tables in the database now but our pipeline creates a new table anyway so that's fine so that's how we create a simple database get it set up and have a simple pipeline script that once the data is cleaned up with our first pipeline that we did in part six it then inserts it into a mySQL database in this tutorial that we've just done again using our Pipelines so obviously if you're more familiar with using postgres databases or other types of databases you can just modify the pipeline slightly we will have available articles where it'll show you the exact code you need to use a pipeline to insert the database into a postgres database also so you guys can have a look at the articles that we'll attach and we'll also have the code repos there for you guys to just download and play around with two so I think that's it for part seven in part eight we're going to be looking at how we can use the user agents and headers to get around issues with being blocked when we're trying to scrape different websites so we're going to be looking at user agents and headers in detail what they are how to use them so see you in partake guys thanks for watching welcome to part 8 of the scrapey beginners course for a free code camp so in part 8 we're going to be looking at why we get blocked when we're scraping the web what types of websites might block us and then how to use user agents and headers to bypass instances where we're getting blocked while scraping so we'll start off by going straight into what headers are so if you go to the site we've been scraping in the last two parts books.2scrape.com you open inspect an element on the page go to the networking Tab and then simply refresh the page if you've got doc selected or all selected you will see what we want to see so because this is a just a simple website the HTML is sent to us and we can see it returned in the preview so you can see all the HTML is there and that's what we end up scraping when we are using scrapey but if we look at the headers tab we can see everything that is sent when we request this page and you've got the request URL which is just the URL of the site we're trying to scrape you've got the method are we trying to get the page are we posting data to the page and then you've got things like the status code and so on so on now the important stuff for us are the request headers so this is everything that we send when we make a request to books.2script.com and as part of this the most important part for us in this part eight of the tutorial is the user agent so the user agent gives you all the important information about who you are to the server that you're requesting the web page from so here we can see if I copy this string and I've got this site user agent string.com and it lets you paste in and analyze user agents so this is all the stuff that is is contained when we make a request to a website straight away it knows that we're using Chrome what version of Chrome we're using the render engine that we're using your operating system so I'm using OS X I'm using an Intel CPU so all this kind of data is sent with every request you make automatically now this is fine when you're browsing the web or you're building your spiders and you're doing a bit of testing but if you're doing any sort of largescale scraping on any kind of commercial sites you're more than likely going to start to get blocked so a lot of sites think I don't know Amazon Walmart any kind of big eCommerce sites most of them will have some form of antibots that stop you from scraping now you might say why do they want to stop me I'm not doing anything bad I just want to collect their data well they'll say well this is our data we own the website this is only for our customers so on so forth now obviously you need to look at the terms conditions of the site you're scraping and judge for yourself if it's legal or illegal the rule of thumb to go by is that if it's publicly available and you don't have to log in and give your details then it's more than likely okay to scrape the data of the website if you have to log in first and by logging in you might be agreeing to certain terms and conditions then more than likely the website will have that in their terms and conditions that you are not allowed to scrape their data so that's up to you guys to decide in a casebycase basis but for a lot of simpler sites like the one where we're scraping in our tutorial series here this site has no antibots on it so there is nothing which will block us even if we have the same user agent so therefore it knows if if it gets a thousand requests from this chrome and my Mac that it knows that it's me then it's it's not going to do anything about it it's not going to block me when I'm trying to scrape all these books off this website but that's obviously because this website is there for people to learn on so that's kind of why we get blocked so the other things that they look for is they look at the IP address of the machine that you're using so that's also a a very simple way for websites to block who the requests because they can see your IP address every time you make a request so they normally look for the IP address and they might set something in your cookies in your session so they might set some kind of flag or counter there so they know that it's you coming back every time so it's mainly IP address the cookies or the sessions and then the headers in general and as part of that the user agents so the difference between headers and user agents is that headers is everything that we have here so it it encompasses things like the accepted things that are returned does it take HTML or does it take images you know what do we accept back what does the browser accept back as a as a response to things like the language encoding and then the user agent is just one subset of the overall request headers so for some sites that are not too complex if we change the user agent each time we make a request the website will think that it is a different browser and a different computer looking for the data on the site every time so it'll let the request go through however for more complicated sites they'll look at everything in the request headers and they'll want everything to be different or at least slightly different so for example I have Mac OS here so if Mac OS is coming every single time and they match that plus they can see my Google Chrome version here as well and the version of chromium then they might say okay this looks too similar even though their user agent is changing every time this looks suspicious and they might flag my requests and block my requests or at least they might not even block them but they might throw up a capture page so that if you're not actively solving the capture the requests are being blocked so for the most comp for the more complicated sites we need to also be changing the entirety of the request headers not just the user agents so they're the kind of main things we need to look at is how can we change your IP address to stop getting blocked how can we change our user agents and also the entirety of the request headers for the more complicated sites so that's what we're going to be doing in part 8 and part nine is looking how to bypass being blocked by changing these different parts that are sent when we make a request for a web page okay so now that we've gone through the theory of it of what is in a header and user agent and what details the websites are looking at when we make a request let's go to our spider and Implement different user agents every time how we can get multiple different user agents insert them into our spider use middlewares to do that and then also do it for request headers okay so if we go back to our spider we're continuing on from part seven if you're just joining us for this part eight we have a link to download the code so you can hop in and start right here with us now so the first thing I'm going to do is I'm going to just go to my settings and disable the pipeline which saves the data to mySQL because I don't need to do that for the purpose of this part H so the next thing is I'm going to open up my spider and I'm going to set a user agent so I'm going to do that the simplest way to do it is actually to go to our settings and in the settings we can directly set a user agent so just like I had shown you in the browser it's user agent and then it contains all the available information about the user who is requesting the web data so here you can see Mac OS X you can see it's an iPad etc etc etc so this is just an example if you wanted to send this user agent with every single request you can set it in the settings now obviously that doesn't make sense because it's not changing So within 10 or 20 requests the website is going to say hey this is the same person every time they're making a lot of requests they're probably web scraping and they'll ask for capture audio blockers so this is not sufficient this is just if you want to set one specific user agent for every single request so for now we'll remove that again and we will look at how we can create a list and rotate through that list so what we're going to do is go back to our spider and we are going to create a list of user agents so here's one I've pasted in from our article you can check out our article and paste it in yourself as well and we also have this available in the GitHub repo so you don't have to type out everything here yourself so we've got a list the next thing we want to do is add the user agent into every single request that we make so to do that we can go to where we make the requests every time and we can specify that we want our user agent to be overwritten to do that we just go to where we have our callback and our book URL and we would do something like this so we would have our headers and we're saying overwrite the user agent part of our headers and we'll do this with our user agent list so we just need to specify self dot user agent list and then we'll import random so we can switch between different things at random and we'll do self.user agent list here and I think that's all we need so what this does is we're saying add this user agent to our headers when we make the request and pick a random user agent from between 0 and the length of the user agent so it's going to pick one of these guys at random and insert it in to our header now obviously you need to add this line that we just added to everywhere that is making a response dot follow so we can add it here as well and I think that's the only two places we have them so we should be able to run that and it should send a different user agent every time but as you might guess this isn't really enough to spoof a largescale website they'll see if you're doing thousands of requests okay there's only five different user agents and they'll say we need to block this user so that brings us on to how we can use a fake user agent API to give us a massive list of thousands of user agents and then we can Loop through that list of thousands of user agents but instead of having them all here directly in our spider what we would do is we would Implement and middleware and we would add it into our middlewares.py and in this middleware is where we would rotate through all the different fake user agents that we would be getting from a fake user agent API so that's what we're going to look at next we're going to create a middleware and we're going to ask for those fake user Agents from the thirdparty website get those user agents returned to us again pass those in to our request headers so to get those request headers we can go back to our browser and go to scrapeups.io where you can sign up for a free account and then using the API key that you get you can use the free headers generator API so this is what happens when I use their headers API I can specify that I want user agents I put in my API key there make a request and it gives me back a result with a bunch of different randomly generated user agents so once you're actually logged in you can go to their fake headers API section and that's where it shows you your API key and if you want to generate browser headers you use this URL if you want to generate user agents you use that URL and then the response comes back like this just as I showed you here so I can just stick that in and it gives me the results back so you can specify the number of headers you want and it sends you back all the user agents our browser headers that we can then use in our middleware so depending on the language you're using as well you can specify different examples so this is where I got the URL to use for here and if you're using node or PHP a ruby you could use the other tabs to see the examples but we're using python with scraping so we have everything we need here so now that we have an API endpoint where we can get our fake user agents and fake headers we can go back to our middlewares file and we can start creating our middleware that will handle everything to do with the fake user agents so I'm just going to scroll down to the bottom and I'm going to start a new class and I'm going to be importing a couple of things that we're going to need so I'm going to import your link code which will encode our urls ran into pick a random integer so we can use that to pick one from the list and requests as well so I've created a new class called scrape UPS fake user agent middleware that can be obviously whatever you want it to be and then we're going to set it up so again we have our initial function which gets kicked off when the class is initialized and in here we first off set some of our settings so we're going to want the scrape UPS API key which is going to be the API key that we get for free from here so that's where we have our API key we also have the URL which you can see here and so we've got our endpoint and what else oh yeah do we want it to be enabled or not and the number of results we want to specify to come back so we go ahead and set those all up in our settings so I'm going to set my API key obviously you guys set that to whatever your one is set up the endpoint and the end point is going to be user agents paste static and then we have our if we have enabled so that's true there you go and the num requests which we can set to to 50. you can save that and save that as well so this part up here just makes sure that we have access to our crawler settings when dclass is initialized so as you can see here and here we've got two functions and that means when the class is initialized get the user agents list and enable it so I'm going to first off get the user agents list so I'm gonna add that one in so that function looks like following we've got the payload set which is the API key and then we're saying if the number of results is not none set the payload number results and then we want to make a get request to the API endpoint with our parameters which have been URL encoded here using the URL encode function then that goes off goes to the script ops endpoint and then gets the user agents and comes what comes back gets put into the response then we're using dot Json just to parse it into a Json response and then we can have our user agents list saved into user agents list so once we've got that we're going to just create two more simple functions underneath the first one just get random user agent fairly selfexplanatory let's get one and getting one user agent from the list that's been returned and returned that selected user agent and then we have just this check to see if the fake user agents is either active or inactive and then once all that's set we can actually put that into practice with our process requests which is one of the scrapey functions that's it's one of the functions that scrapey will look for when you're using middlewares and it then sees that we've specified something to happen when it goes to processor request and then it executes the following so when it sees process request it goes in here it gets a random user agent sticks it in here and then it sets the header user agent to be the random user agent so I hope that makes sense I think it's fairly selfexplanatory we're just getting a list of user agents and then with process request we are getting a random user agent and we are assigning that to our header request header so when we go off and ask our books to scrape site for the book or the list of books it sticks in dash random user agent into the user agent of the request the only thing to do then is to stick our middleware this is a step you mustn't forget it's very easy to forget it I've forgotten it loads of times and then sometimes you can be trying to debug things after so you want to go to your downloader middlewares open that and add in your new middleware so again as we did in the last one the lower number has higher priority so we don't need to have the other downloader middleware just want our a middleware we just created our scrape Ops fake user agent middleware because this middleware up here is just the generated one that we don't need at the moment okay we'll go ahead run our spider now check that it's working should be working fine and then we will look at just doing the same thing but instead of having a user agent come back from scrapeups We'll be asking for a list of fake headers so we'll create a second middleware and we'll just have the whole fake header as opposed to just the subsection user agent part so just to make sure that the headers are being attached correctly because you might say oh I know for sure that it's working we can just add in a very simple print to our process request so I'm just going to stick this in here two print statements one sync new header attached and the other one saying the new user agent so it'll print out the user agent that we've just attached the random user agent so that should print that out then to our console and we should be able to see that a new user agent is being attached every time that request is being processed you out the only other thing we have to do is go back and remove what we added in earlier which was this header's part here because it's being done in middleware so we don't have to specify it for every request here so I'm just going to remove that and I'm going to remove it here as well so as you can see there's always multiple ways of doing it either kind of manually adding things into the spider setting them in the settings if you just want it once off in the settings if you want a simple case The Spider and if you want something more complex in the middleware so you've kind of got your three different ways of adding your user agents are headers in so now that we've removed that we don't need this user agents list up here we can remove that let's save it and we should be able to just run our spider again so Scrappy crawl and book Spider and if everything's gone to plan we should see I'll just close the spider straight away and scroll up so we can have a look and see to everything so it looks like all the data is coming back as it was before we don't expect anything to change there because we we're not going to be getting blocked by the books to scrape.com site anyway but if we scroll up I think we should see here you go so here is where we can see the all the new headers that are coming in so you can see the Chrome version there is different sometimes it looks like it's using Edge as well so you can see there's multiple different headers coming back and then they are being attached in with the process request so that all seems to be working fine just as we wanted to and we made sure that we had it enabled in the settings as well in the downloader middlewares so we have an enabled there as well as enabled up here and the number of user agents coming back from the API set here so everything is as it should be one other thing to note is that this robot's txt underscore a bay is set to true if we're starting to do more complex sites we would set this to false so every site or most sites have a robots.txt file which is usually one of the first things that a spider will look for so scrapey does this automatically every time it looks at a site it first goes off and checks does the site have a robot dot txt file and in that robots.txt file is usually specified things like the pages on the site is this site open to being scraped and if it's open to being scraped watch pages are not allowed to be scraped are what pages are allowed to be scraped now obviously any crawlers that go out crawl different websites don't have to obey this robots.txt file you know it's it's a piece of code it's going to go off and do what you tell us this is up to you to decide you know do you want your spider to obey it or not a lot of big sites will have like okay if you're a Google spider you're allowed crawl and scrape our data if you're not a Google spider don't scrape our data so if they have that in their robots.txt and you have it set to obey is equal to True your spider will go see it's not supposed to scrape the site and it'll shut down so if you're having issues and you have this set to True try setting it to false okay so now that we've gone through what robots.txt entails let's go and create our next middleware so this time instead of having middleware that just replaces the user agent every time we're going to create a new middleware so go to middlewares.py and this new middleware is going to create a new header every time using the data that it gets back from the fake browser header endpoint so I'm just going to paste in the code here and then talk through it for you guys so if we go up to the top we start off again with just a simple class name scrape UPS fake browser header agent middleware that can be whatever you want we pull in the settings that we need this will get everything from the settings file and then it kicks off get headers list which calls out to the API endpoint it does a get request here to the scrape UPS endpoint Returns the response converts it to Json and then we've got a list of headers and then in the process request which gets processed with every request we have this get random browser header function which will get the random browser header from the list that we just asked for from scrape UPS so then we have this random browser header and we can assign all the other headers not just user agent like we did in the middleware above but we can also modify all these other parameters in the header so you don't have to modify all of them you can modify certain ones again this is up to you to play around with and decide which ones you need which ones you don't need with scraping everything is really a casebycase basis because every website is different but we're giving you here everything you need to play around with so you might find certain headers need to be modified more than others we have our request.headers being updated and then that is just like the one above everything we need let's go ahead now and add the settings that we need so we have some of the settings already set here so we have the browser endpoint there's a default set there we can set our fake browser header enabled to True here to make sure it runs and then the num requests is going to be the same as here it's already set and the API key is already set as well so again you get your own API key for that and I think we should be able to just go to settings now and make sure we have the middleware enabled so I'm copying the class name going to settings going down to downloader middlewares and where we had the fake user agent middleware I'm just going to overwrite that so we have our fake browser header agent middleware so I'm going to save that and then just go and run the spider again and it should work correctly so yeah there seems to be lots of books getting scraped so it seems to be working correctly I can stop it and just as we did before to double check that the headers are being set correctly we can just stick underneath a simple print statement that shows the headers are set to what we wanted them to be set to so if we run our spider it should show that there's multiple different headers so I'm stopping it again and looking at the output once we get past the book data okay so here there's some headers so new header attached which is what we have here and then we can see we have things like let's see accept the user agent so okay so we have the user agent Mozilla 5 do we have accept so we have accept the text HTML everything we wanted there I'm just trying to see can we see in here that both of them are different to each other yeah so here for example you can see this user agent is using Chrome 103.0 50 60.134 and up here it's using a different version of Chrome so it's using one zero three point zero point fifty sixty one one four so you can see it is changing in each request so that's exactly what we want to show you how to do so I think that concludes part eight in part nine we'll be going into how to use proxies to bypass the antibot blockers that websites have as well so instead of kind of handling everything ourselves and trying to bypass the antibots by updating our own headers we can see that there's commercial things out there for you to do that with and there's also things like proxy lists that are free to use as well and we look at how to integrate those into your scrapey project as well so see you in part nine guys so in part nine of the scrapey beginners course we're going to be looking at everything to do with proxies so we're going to be going through what are proxies and why do we need them and then we're going to be looking at the three most popular ways to integrate proxies into your projects so let's get started so in part 8 we're looking at user agents and requests and the headers we pass in when we're making the requests to the website you're looking to scrape we discussed and looked at how if you change your headers and change your user agents you can basically make it look as if you were multiple people accessing the website you're trying to scrape now there's one thing we also mentioned in part eight which is that the data that also is being transmitted with your request is usually your IP address so this IP address is it's your unique identifier and that's what is used to make sure the data comes back to your machine so every machine will have an IP address and that's how the requests get to and from your machine think of it as like your house as an address your computer must also need an address and this is your IP address so if we change the user agents every time when we're sending the requests that's fine but if we changing the user agents every time but we still have the same IP address then the site that we're scraping is very likely to know that we are the same machine that is requesting their data every time so they're very likely to block us straight away so that's why changing our IP address as well as our user agent and headers is very important so just the user agent and headers might work if it's not very sophisticated type of website that you're trying to scrape but if you're going to anything that's complex at all you will need to rotate your IP address and that's where proxies come into play so let's first off look at the first method is that we're going to be looking at is using proxy lists so these are lists of IP addresses and ports that belong to multiple different servers all over the world so there's lots of these machines that are available to bypass our requests will go via that machine before it goes to the website we're trying to scrape and then it'll come all the way back via that machine as well now there's pros and cons to every one of the of the three integration methods we're going to look at so the pros of proxy lists like this are that obviously the proxies that list that you can get online are last number free so like this one here freeproxylist.net you can go there and you can select from a list of different countries select your protocols and you can check the uptime there's also another list which is very handy on juno.com forward slash free proxy list here you also have IP address Port country uptime response Etc so here there is 9 000 proxies online in 136 countries but the downside is of using these lists is that because they're free so many people are using them that they're very likely to either have very poor response times and take very long time to actually root your traffic through them or else they can be already blacklisted because think of it if someone has already used them to scrape millions of pages from maybe the same site that you're going to look to scrape data from then there's a very high likelihood that that website could already have discovered this IP address and blocked it so the pro is that it's free the cons are that it can take a long time and there's a very high likelihood that if it's free that it's already been used too much and it could be blocked so we're going to go ahead anyway and try with a few of these IP addresses and a few ports from these two sites and the way we want to integrate them into our project is we're going to use this GitHub project which integrates with scrapey and it's called scrippy rotating proxies so we'll have a link to this available but you can just do pip install scrapey Dash rotating Dash proxies and go to your terminal and paste that in and run it now I already have it installed so it's going to say requirement already satisfied for me but for you guys you should see it installed in there now we're continuing on part nine from part eight so if you're looking for the code for where we are starting at now we'll have that available in a GitHub repo which we link to so you can continue on from here with us so now that we have that installed we can go ahead and we can add our proxy list in so as you probably guess everything is going to go into our settings file as with everything else that's part of our project and these are just dummy domain IP addresses but this is the idea so you can have this many you could have a hundred different IP address and ports in here but we're just going to put three or four just for the purpose of showing you how it works so let's go back to our free proxy list and take two of these guys so we want the IP address in the port now obviously depending on your use case you might need a and you might need a proxy from a specific country or you might need a proxy with a very good uptime or response time so that's for you guys to search in the search boxes on the site here okay so I've got three of them there and the next thing I want to do is enable the middleware so this project that I've just installed this scrapey rotating proxies will have installed a middleware but to actually make sure that the middleware works we need to add it to our downloader middlewares and that's where we can make sure it's enabled so I've just done gone ahead and done that there I've added them in so and as you can see I've left the other two middlewares that we had from part 8 in here as well so obviously they don't have to be here I can also remove them but I might as well leave them in for now they're not going to do any harm they're just adding in a different request header so let's save that and then the other thing I wanted to quickly show you is that if you had all the proxies already in a file you could do something as simple as just saying the rotating proxy list path is equal to and then the path to your file obviously we don't have a file with a bunch of ips and ports but that's where you would put it if that's what you're wanting to do so let's just quickly remove that and now we can go ahead and we can run our spider and see the results so I'm just going to do scrippy crawl and the name of the spider I'm just going to make sure and in my project it's great big crawl the book Spider so it's going to go ahead and run you can see the header that was attached from part 8 where we were adding the new header and now this can take a good bit of time so as you can see here this rotating proxies Dot middlewares has printed out that it has zero good proxies zero dead proxies and three unchecked proxies so that means that it's going to go ahead and it's first going to try and see can it actually send any requests through the proxies that we've listed here so this can take a good bit of time depending on the quality of the proxies that we've got obviously I have no idea how good the ones in that free list are because they change every day every hour there's new ones added and there's ones removed and then as soon as they're added they're being used by hundreds if not thousands of other users so this is the good thing about this middleware is that it checks cannot actually use it and as you can see here it's just retrying our books to scrape URL with another proxy because one of them failed to work so it's just a process of waiting so it's moved one of the proxies into our dead pool and it's still got two that it wants to check so this is just a process of waiting and letting the middleware do its work so I'm gonna leave that run for a few minutes and come back to it and we'll see did it actually manage to use any of those free proxies that were on that free proxy list so I'll come back in one second and we'll see where we are okay so I've just come back a few minutes later and it still hasn't managed to get any of our three proxies in the list to work he's got two dead now one it's trying to reanimate it's not looking good so obviously the ones I've picked were probably already overly used already could be blocked by the site we're trying to scrape so here you can see obviously the major disadvantage of using free proxy lists online now there's lots of different places to get them so depending on your source of the proxies you may have much better luck at getting them to work but it's really a process of trial and error and while it's free it can be painful to actually get up and running consistently and correctly so we've had a look at how we can just plug in a bunch of different IP addresses and ports into our rotating proxy list and how we can use this middleware to use our proxies in our scrapey project but another way we can do this is using a proxy port so what we would be using is a service which is provided by a proxy provider and they would give us a proxy IP address and port and they would handle changing the IP address every time and we wouldn't have to worry about compiling a list of proxies ourselves so that's what we're going to look at next so we can still be looking after our own user agents and our own headers but the proxy provider would be dealing with everything to do with rotating a proxy list for us and making sure that the proxy list is of good quality and available all the time so we wouldn't have to worry about that now there's lots of them out there and we're just going to look at one of them now the one I've just going to show you now is called smart proxy you can check them out at smartproxy.com and as they say effortlessly scrape web data you need so they've got some great deals and offers and as you can see they you know do things like bypassing captures IP bands they've got millions of proxies from millions of locations and the plans they do entail residential and data center proxies so we haven't talked about that yet but residential proxies would be basically the data will be forwarded through residential IP addresses so these are IP addresses that are mainly used by people's homes so think of it if someone is watching Netflix and browsing Facebook and looking at Google search and then one or two of your requests are going via that IP address then the website you're trying to scrape let's say Amazon is going to say oh well I saw that IP address yesterday they just bought something from me so they're much more likely just to let that request go through without any issue so that would be what residential proxies are then data center proxies would be think of you know your traditional data centers with thousands of servers in a big room and your requests will be routed through a Data Center and through the IP address that is belonging to one of the machines in the data center so you have access to a lot more IP addresses in the residential side but then the data center side are much quicker and there tends to be not as many data limits and they tend to be a bit cheaper as well so that's the difference between residential and data center proxies so you can sign up with them most proxy providers also give you a week or two of a free trial or a certain amount of free credits that you can use to test out their service so if you guys go ahead you can click get started there sign up for an account and then once you're logged in if you go to the residential tab because we're going to be using residential proxies for our next part now so click the residential Tab and then you can either check out a pay as you go plan where you pay per gigabytes of data that's transferred or you can go into regular or Enterprise as well so I already have a plan set up with them so I'm going to go directly to the proxy setup next and this is where we will get our details which we'll then put into our spider first off we want to generate our username and password so we can put in any kind of combination of letters and numbers here a password and click create and it creates a username and password once you have your username and password you can grab these and put those into the username and password field here then our proxy location if it's important for your spider that you are scraping from a certain country for example if you're scraping an ecommerce site that will only show you specific products if you're living in a certain country then it is important to select the country here from this list so for us it doesn't matter so we can leave it at random then for the session type we want rotating because we don't want a fixed session every request can come from a different IP address and that doesn't matter for us right now and for the output format we're just going to pick http so that's going to then give us this string here which we can copy and use in our project so this is the endpoint where we're going to send our requests to Smart proxy is going to handle all the IP address rotation and all that stuff and it's going to then send us back the response from the website we are trying to scrape now that we have our endpoint from Smart proxy the next thing we want to do is go back to our project we want to disable the middleware we were using because that will no longer be needed because smart proxy is going to be looking after rotating our proxies and it's going to be looking after some band detection stuff as well so we can disable the two of them and the next thing we can do is go to our spider and we can go to where we have response.follow and in here we'll simply add in one more field which is going to be meta and then the proxy information so matter is equal to proxy and then our proxy details will go in here so I can go back grab my proxy endpoint and paste it in here so that looks correct and then I can also copy this and put it down where we also have response dot follow below so I'll add it in here too and that should be the two main places I needed for now the other example which I'll show you in a second is that we can create a custom middleware which would insert the endpoint and as well so we'll do that once we get this to run correctly we can now do Scrapy crawl book spider and it should work for us so Scrapy crawl book Spider and hopefully we have no issues as I can see there's some things coming through I can go to my books data.json and I can see that there is the data coming through so it looks like it's working correctly and it's all going via the smart proxy endpoint so I can close down my spider and the next thing we can do is we can create a custom middleware version so just adding it in to our meta value here and adding in our proxy endpoint is fine if you've got a small project but if you've got a larger project it probably makes more sense just to make a custom middleware for it so I'm going to show you how to do that next so we'll scroll down to the bottom because we have our other middle words in here already and we'll create a new middleware where we will be adding our endpoint details so we just make a new class called my proxy middleware it's going to again pull in our crawler settings and then it's going to get our proxy user proxy password proxy endpoint and proxy port from our settings so we need to go ahead and set those in our settings and then once it's got those it makes the user credentials it puts those credentials into a proxy authorization header for the request and then it has the URL which is made with the endpoint and port and that then goes into the request dot meta so let's go ahead now and in our settings fill out our proxy user password endpoint and port so let me just go here and add them in now so username password endpoint and Port so I just need to change the password obviously your username and password are going to be whatever you guys have made in your own dashboards with smart proxy I'm just copying my details from here and that looks fine so I should be able to save that and the next thing I need to do is to make sure my middleware is enabled so again going to my downloader middlewares and I'm going to add in my new middleware so I can add that in there and save it the next thing we want to do is try and run our spider again and see does it work but we will obviously remove what we did a second ago so that we can show it's all going via r new middleware so let's just remove that save it and then try run or spider again and it looks like the book details are coming through again so I'm just going to close my spider now we can have a look at smart proxy and see the traffic usage and as you can see weave requests coming through so there you go we've got the user and we've got our usage by gigabyte so it's working just as we wanted it to work our requests are going through the inspired proxy endpoint smart proxy is looking after the IP address rotation and it is sending us back the request then scrapey is able to take the information out of the HTML and we have the data that we need I think that's given you a very good overview of how we would use proxy port endpoints so there's just one last thing I wanted to show you guys which is proxy API endpoints so this is if you want to go just a step further and not have to deal with the browser headers are the user agents or any things like that and maybe you are scraping something which requires a headless browser to do JavaScript running for you we can get that by using a proxy API so again it's a service where there's an endpoint we're sending our request through that service and then that service is making sure that certain things are enabled to make sure that the request gets us the page data so what we're going to do is we are going to show you how to use that now and that is also going to be a paid service and you can sign up for that by going to scrapeups.io clicking get free account signing up for it you've got a thousand free credits there and if you then once you're logged in go to the proxy aggregator page go to the request Builder and you then have an API key which you can use and you've got the proxy endpoint which you can use as well in your spider so once you've got your API key we can move back to our book Spider file and we will start adding in a new function which will help us send the traffic first to our new proxy provider so this new function is going to be called get underscore proxy underscore URL and we're going to pass in a URL to that function and then we're going to have an API key as part of this payload object and we're obviously going to put in an API key where we have our own API key that we got from scrape UPS so I'm going to add mine in quickly now I'm just going to copy and paste that in and that then is going to slot in here this payload is going to get URL encoded so I need to import this URL encode from URL lib and then it's going to create this new proxy URL and then it's going to return that proxy URL so this function is going to get the URL of the site that we want to scrape and it's going to encode it along with our API key and it's going to send it to this API endpoint once we've got that function created the next thing we want to do is we want to add it in to where we use our current scrapey Dash request function so we will have URL is equal to and then get proxy URL and then the same down here we'll be doing get proxy URL with our next page URL to and the only other thing we need to add in now is a new function called start requests so we'll add this in under our custom settings and I'll explain now what this does so scrapey looks for this function when you start up your spider if you don't have it it doesn't need it to run it'll work off of your start URLs list here but if you do have it it will go in and work off of what you have in here so what I've asked it to do is I'm saying okay when the spider starts up run this function and inside this function run our guess proxy URL the same as we do down here because we want the very first URL to also go to our proxy so if we didn't have this function in here what would happen is that the very first URL would actually not be sent to our proxy provider endpoint URL so that would mean that there's a chance that the first very first request would get blocked so that's why we have this function so that the very first URL is properly encoded and sent off using this get proxy URL and once so that's where we have start URLs and we're taking the first string inside of our start urls and then we're doing the Callback is going to be our parse function and then it's going to go on and it's going to work perfectly because it'll be going through get proxy URL for the rest of the requests as well so that takes care of the very first call and this get proxy URL function get takes care of making sure that all the requests are going to go via this proxy API endpoint so then the request will come back with the response and the response will be able to be parsed in our parse book page like it was before so we should be able to go ahead and run that if we just do scrapey crawl book Spider again and run this oh there's one other thing so it just did one request and stopped straight away and that's because it's a very easy mistake to make the allowed domains does not contain our proxy dot scrapups.io so let's just add that in and if we rerun it it should there you go so we just close our spider and I'll show you that we have all the data is coming through so we've got our product type books the title the description it's all there so that's working perfectly it's going via our proxy API endpoint and the next I want to do is I want to show you guys how instead of integrating this directly into our spider we can use a proxy middleware that's been created especially by scrape UPS so we can just quickly pip install it and it makes things a bit easier if you are adding it to a project and you don't want to have to add this special get proxy URL function so what we would do in this case is we would just pip install and then our new python module scrape up stash scrapey Dash proxy SDK and install that and then the next thing we would want to do is we would go to our settings and like we always do add more settings so let's just go down here and add in the settings we want so it would be again our API key if the scriptop's proxy is enabled yes and we'd be adding this line to our downloader middleware so I already have downloader middlewares so I'm just going to add it here so if I save that and I add my API key from here into my settings okay so I just wanted to hear perfect and I should just remove the get proxy URL from the places that we've been using that function because we don't need it anymore and then it should run fine going through our middleware so let's try and run that one more time and see does it work going by the scrape UPS proxy middleware and it looks like you have lots of requests going through so if I just cancel that and we can check in our dashboard if we have so we have 123 requests so that looks like they all went to our books to scrape.com site so we have 123 requests made so it's working like it should so that makes it very easy if you want to get started with it all you need to do is do that pip install and scrape UPS Scrappy Dash proxy SDK add to the two lines in here and the one line into your downloader middlewares and then it will just send all your URLs via the scrape UPS proxy endpoint now obviously you can make your own custom download or middleware as well and like we did for our smart proxy example that might be a bit more complex than needs be for for this because there's already the middleware that you can just pip install so we will leave the example with our downloader middleware in our article so if you want to check out a long version of how to implement the downloader middleware using the scrape UPS proxy API endpoint we'll have that in our article and you can copy and paste that into your code and play around with that so there's just one other thing if you wanted to add some more and functionality to the scrapeups proxy endpoint you could add in for example the following so scrape Ops underscore proxy underscore settings is equal to Country us so this would send all the traffic via US IP addresses so if you're for example scraping an ecommerce website that needed to be only loaded via the US so we'd show us items only you would do something like this country us in your scriptops underscore proxy underscore settings they also have other functions such as you can pass in if you wanted to be the page to be JavaScript rendered and there's many other different parameters that you can pass in which will mean that certain things are switched on on your proxy provider site so that way instead of you having to do all this stuff on your side the proxy provider will take care of it as long as you pass in the correct parameters and each proxy provider will have their own page with all the different parameters that they allow you to pass in to them okay so that's everything I wanted to cover in part nine in part 10 11 and 12 we'll be looking at how you deploy so basically get your spiders to run on a on a server in the cloud so how you deploy the code to your server in the cloud and then how you can schedule and run your spiders to scrape at certain times of the day or the week so you can collect data on a periodic basis without having to have everything running off of your home so we're going to go through some different options there what's available we're going to look at some open sourced options free options and paid options and just have a look at the different uis and give you a bunch of different options and we'll go through the pros and cons of why you should pick one service over another service and we look at how complex and easy they are to use as well so that's what we'll be doing in the next three sections okay see you in the next part 10 News so in part 10 we're going to look at the different tools we can use to deploy and schedule our spiders online and the tools we can use to monitor how well our jobs are doing how much data is being scraped and if we're missing any Pages or items when we are actually running our spiders so you might be asking yourselves what is this deployment in scheduling so deployment is basically us putting the spider that we've just created onto a server that's always going to be online so that we don't have to have our own machine our own laptop or computer running 24 7 at home we can just put that onto a virtual machine somewhere on cloud and then we can actually schedule it to run at a certain point of time once a week once a day once every hour once every minute however often we want to run our spider to collect the data so that's the deployment is the act of getting the spider on the machine and the scheduling is scheduling to run at a certain time of day or time of week and then the monitoring is just seeing how well our scraping is doing either seeing have the jobs actually completed did the Spider Run correctly did the Spider Run for the correct amount of time that we thought it was going to run for did it get all the pages that we thought it should be getting obviously if you see Zero Pages script you know there's an issue so that's where the monitoring comes in and it's very important that we do have some monitoring setup because obviously if you don't your spider can be running every day and you could be missing huge amounts of data so that's deployment scheduling and Mantra that's the first part we're going to do in this part 10 is we're just going to look at the different tools available we look at free tools open source tools and paid tools so the first is square PD which is free and open sourced anyone can download it and contribute to it as well on GitHub so the pros of this are it's obviously free and open source there's plenty of thirdparty libraries for it as well there is optional uis from different providers and the downsides to it are things like you need your own server ideally because if you're running it on your own computer or laptop you would have to have your computer laptop online at all times if you want to have this for example running something every day at a set time so it also doesn't actually have a scheduler so some of the other tools we look at such as using scraps or scrapey Cloud there with those tools you can actually set a scheduled job to run at a specific time every day but with Scrapy D you'd have to use this Cron job to hit the API endpoint to get scrapey to run your job at a specific time so Scrapy D is good because it's free and open sourced but the downside is there's a bit more configuration it's a bit harder to install but we'll show you exactly how to install it if you want to install it so the second option just using scrape Ops to deploy a schedule and monitor your jobs the upsides of data is it's got a good a UI interface to use simple to use and understand it's got builtin monitoring for your jobs and spiders it's easy to schedule stuff but the downside would be you'd need your own server as well like with square PD and the third option with scrapey cloud is it's a paid service they have a freemium kind of version so you can just check it out if you want with that it's easy to set up you can just download their CLI tool use that to deploy your spider into their Scrappy cloud service and then once it's deployed there you can quickly and easily run it and you don't need to have your own server set up with another thirdparty provider so they're the three main options we're going to look at scrapeyd scrape ups and Scrappy cloud so let's first off have a look at scrapeyd and then we look at two different UI dashboards that we can install so we don't have to control everything using their API endpoints because well that can be useful for some people most people want to interact with their spiders and run them and schedule them using a nice frontend UI okay so Scrapy D is available to download as I said we need a thirdparty server set up first to install scrapeyd on so we're going to go ahead and create that with digitalocean now so digitalocean is a server provider which enables you to quickly set up virtual machines and then install everything you need on them so you can also use any other VM provider such as vulture for example these are another good provider and they have very cheap servers as well so if you go off and create your own account with vulture or digitalocean or AWS go off create your virtual machine I'm going to do that right now with digitalocean and if you want to use this you can just follow the steps that I'm using so you just log in go up to the create click droplets select the country or region you want to select it usually works best when you select a region that you're close to select Ubuntu for the operating system version 22.10 we can select the cheapest virtual machine they have available which if you just click basic and then go to regular for the SSD type and then they have a four dollar a month server there so once you've selected the server you want you can either add an SSH key or a password to log in that's not that important now because for this you can also log in Via their console which can be accessed via the browser so that's how we're going to do everything now we're just going to use the browser to log into the machine and install everything we need makes it very simple and easy to use so that's all we need to do once we get to the bottom we just click create droplet and that will go ahead and create the droplet for us the droplet is just their term for virtual machine I can see it's creating so I just give it a minute or two to finish creating and then we can access the console over here so as you can see the droplets being created now we can click the console button which will open up a new window for us where we can access the console and type in all the instructions to get everything installed correctly so as you can see we're logged in to the virtual machine and now we can start running our commands so first things first is we want to run sudo apt update that just updates all the packages on the machine to make sure everything we install will be the most uptodate versions of things so we'll give that a second to run that's finished and the next thing we want is to install python pip so we can pip install all the packages we need for python so that's just sudo apt install Python 3 Dash pip this Command right here we'll have all these commands easily available for you to copy and paste from our article as well so you don't have to be pausing the video at every point in time Okay so ill office you ask do you want to install this x amount of space will be used we're just going to say yes and sometimes it'll ask as well to restart certain Services which we can also say yes to so while that installs I'm just going to show you the project that we're going to be using so the project is the part 6 code so obviously you may or may not have done that part if you haven't you can just git clone our project from here and just type git space clone space this URL so that this is the project that we're going to be using from part six of this course okay so let's see is wait it's just asking us to restart the services and now we'll go ahead and get clone our project so as I said it's just git clone and then the free code Camp Dash part6 so that's installed we'll now just see the into our project and we will install a virtual environment using pip install virtualenf so that's after installing virtual EnV now we can actually create our own VN folder where all the python packages can be installed into so we'll do that with virtual ends vnf and as you can see this folder has been created so now we just need to activate it so we do that with Source vnf bin activate it's activated now and now we can install the project requirements so this requirements.txt file contains all a list of all the things we need to get this project running so we can just do pip install space Dash r space requirements.txt and it'll go ahead and install all the packages that are needed to run the project correctly so we just give it a minute or two to install everything and then we should be able to run our scrippy spider next thing we do we just CD into our book scraper and we'll see if we can run scrapey list so that ran correctly and now we can run scrapey crawl book Spider so if we run that should see Scrapy starting up and as you can see all our pages are being scraped so our pages are being scraped and the data is being extracted from the page just like we were doing in part six so that's perfect we don't have to wait for that to run and complete the next thing we're going to look at is how we can install scrape PD okay so we just pip install scrapeyd to install that and then the next thing we can do is just run scrapeyd so to do that it's just Scrapy D now I've added on a bit extra after it just Scrapy D because I want all the output that usually gets displayed to the screen to go into this scrapeyd logs.txt file so put all the logs into scripty logs.txt and run this command in the background so we can go ahead and do that and now let's check that it's actually up and running so to do that we will be using curl to Ping the Damon status dot Json endpoint that lets us know if Scrappy D is running correctly so when we run that command it says status is okay there is zero jobs pending zero jobs running zero jobs finished because obviously we've just ran Square PD we haven't run any spiders yet we have scripted setup we can hit the end point using curl the next thing we want to do is we want to package up our spider and then deploy it to scrapeyd because if we don't do that square PD will not have access to our project and will not be able to run the spider so to do that we can install scrapeyd client so again using pip we just do pip install Scrapy D client that will go off and install the scrapeyd client the next thing we need to do is we need to go into our scrapey.cfg file so that should be here this guy scrapey.cfg so we want to edit that we can use Vim or VI and we can all we need to do for this is just uncomment out this line so that it deploys it to Scrapy D which is running on localhost Port 6800 so it's handy it's already there all we have to do is go in comment that out so to save this we just I'll show you one second we just type in the double dots to get up so we can actually save it correctly and then WQ exclamation enter and that saves it so now that it's saved we can do a scrape Ed deploy and default is the name I've just picked for the project because scrapeyd works with the concept of projects so it needs a project name and then as you can see it's deployed okay and it now has one spider available so we can now go ahead and run our spider right now again using curl so curl it's going to hit this localhost 6800 port and the forward slash schedule.json endpoint we're adding the project name of default and the spider name of book Spider so because we've deployed it we should be able to run this and it comes back with a job ID if it has run correctly so it said status okay and we're giving you back a job ID to show that the job has been started now this doesn't mean that it finished running so if there is ever issues sometimes you need to do further investigation so we've gone through how to use Scrapy D and Scrapy D deploy to package up and deploy our spider to scrape Ed and then how we can use Curl to schedule our spider using the curl command followed by the schedule endpoint so obviously if you wanted to just run this yourself you could just set up a cron and using your crown you could say schedule this to run every day at whatever time you want and then you would just be running this command obviously we want to make this easier for people to use so now we're going to look at the two dashboards that we can install the first being Scrappy dweb and The Following being the scrape pups Scrapy D integration so for scrape edweb scripted web is also a third party open sourced application that you can install and we're going to go ahead now and install that so I'm just going to go back up to the top level that we were at and just pip install and then the Scrapy dweb so that's going to go ahead and install that now it may need a certain specific version of specific packages to be installed so I've gone ahead and found out that when I was making this video that four specific packages needs to be installed with a specific version number are the installation wouldn't go and work correctly on the version that of Ubuntu operating system that we're using right now so it's easy enough all we're going to do is specify for four different packages the version that we need so the first one is just flask SQL Alchemy so we're just going to pip install that specific version the next is we're going to do pip install SQL Alchemy then we're just going to install a specific version of flask and finally we're going to pip install a specific version of work zoic once those are all installed we can now check and see the scripted web run correctly so we need to do is type in scrippity web okay it's giving us an issue is it let's just try and rerun scripted web again yeah so this time it ran correctly so I think it just needed to create the settings file initially so you can you know it's running correctly when it stays up and you have the URLs where you can access it showing here so we can just go ahead and copy the URL that it has given us so this is the IP address of our server which you can also get in the digitalocean dashboard followed by 5000 which is the port that scrippy web is running on if we copy that go up paste that into a browser we should see the scrapery web dashboard showing up correctly so we can see the jobs that I've already run so we can see the job that was run earlier that we ran when we ran it manually via the command line we had the default project and the spider book Spider and the job ID that was returned to us so it doesn't have the pages and the items because it needs this log parser module to be installed so we're going to go ahead install the log parser so we can see the pages and items and more statistics and we're also going to add in a username and password just some basic authentication right now anyone can hit this endpoint and start running my jobs I don't want that to be happening and I'm pretty sure you guys don't either so you're paying for the server you don't want anyone to be able to come on and start messing around with your dashboard so we're going to quickly go and do that now so we'll start by just copying and pasting this pip install log parser command I'm just going to shut down scrape Beauty web for a second so run pip install log parser then once that's installed I want to edit my scriptd web settings so I can use VI again and I'm going to first enable the off so as you can see here it's currently set to false I'm going to set it to true and then I'm going to set a username of test and a password of one two three four five six seven eight obviously set better username and passwords than that please for your own projects and your own servers the next thing we need to do is come to our script e servers list and just comment out this line here because we don't have a server running on Port 6801 we just have our Square PD running on this Port 6800 on localhost so once that's done the next thing to do is to add in our logs directory and to enable the log parser here so let's just enable the loud parser by setting that to true true and for us the directory is going to be just root then the name of the project frequent Camp part 6 and the name of these spiders the folder containing the spider which is book scraper and then it's got a logs folder in there which is where the log parser is going to read the logs from so obviously if you've got a different project name and a different spider name you need to just make sure that that is correct but it will always have a folder in there with logs already so just find out where your logs folder is and paste in the directory here and then the last thing we need to do is just set our scripted server which is just the default again which you can see from above 127.0.0.1 is usually the default and it's running on Port 6800 so now that we have all that I'm just going to save the file and now that is saved we should be able to run scripted web again this time I'm going to do like I did with scrape Ed and get the logs to save into a separate log file so they're not coming up on the screen and I'm going to run it on the background so that should be running we can check if everything is running correctly using the following using the sudo SS command so we can see that we have scrape PD running on Port 6800 of localhost and we can see that Scrapy web is running on localhost Port 5000. so we can see both of them are running we can see the ports they're running on and we have the process ID which we can use to kill the process so if we wanted to stop one of them from running for example I might as well just stop Scrapy D from running and show you guys you can just type in Kill and then we will get the scrappy D process ID from here and we will just paste that in and press enter and then you guys can see that Scrapy D is no longer running so that is how you can kill them if they're running in the background as a process like that so I'll just start up Scrappy D again so if we check we should see Scrapy D running and scripted web running perfect okay so because we killed and restarted Scrapy D we need to just redeploy our project again using the Scrapy D deploy because if we go back to our Scrapy dashboard we won't be able to see our spider and we won't be able to run our spider so we need to package up and redeploy the project again so we can just do that with square PD deploy so it's just Scrappy Dash deploy and we're just picking the project name of default again so that'll just package up our spider and add it again to scrippy D so that's been added again and we can go back to our endpoint now it asks us to sign in so if I try and sign in it says no you need to add in the username and password so I'll add in my username and password that I set in the config file and sign in if I go to run spider we can then see the default default latest version book Spider if you need to set any specific settings and Arguments for your project you can do it there if you want to set it to run at a specific day of the week hour our minute you can do it here so I'm not going to set it for a specific time in the future I want it to run right now the next thing I need to do is Click just check command that will paste in a default command which as you can see here is just going to do the curl to the endpoint that we did earlier so all scripted web is doing is running this command so everything is correctly set the project the version The Spider and we just want it to run switch click run spider and that's going to go off and kick off our s our job and it's going to start running so that should be running now and we should soon be able to see the statistics coming back for a number of pages and items so let's give that a minute or two to run so as you can see it has finished and it took 24 seconds to run and you can see some other stats and pieces like that now we're still missing the pages and items and this sign is still up here saying that we need to install loud parser so I think I might have actually put in the incorrect path to where the logs are stored so let's fix that and then we can show you how the pages and items show up so if we go back I've discovered that I need to change my path in the scripty web settings so I can open up my scripty web settings so here you go so I think we just need to fix this so as I said depending on your project you just need to find where the log folder is kept so I think that should do it it's just forward slash root forward slash Frankel cam part six and then locks so I'm just going to save that and I'm going to restart scrivity web unless it's after fixing itself but I think we need to restart it yep so just go back run the sudo sstunlp command again and we will kill our scripty web using the kill command and we'll run scriptyweb again so yep so as you can see the log parser ran eight seconds ago and it was last updated at this time so if we rerun our spider we're going back to run spider and we'll put the default project the latest version book Spider put in the command run spider and it'll run again in the meantime it's gone through and it's parsed the last logs that we ran a couple of minutes ago and you can see the pages and items have actually been populated here so it scraped 1051 pages and there was a thousand items and we can see some more stats by clicking the stats button so you can see what else to have warnings and errors so there's one warning and you can see the latest item that was scraped and as we can see it was this URL this was the book title we can see the price tax and all the other stuff that we've already selected in part five and six so this is kind of the basics of scribdy web and how you would install the scribby web dashboard to work with scrapyt obviously you can also see the full logs and you know if you need to see exactly a specific error or you need to nail down further into the logs you've got the full logs available there as well and it's picked out the warnings and if there was any errors it would show them as well so it's very handy it's free it's open sourced you can install it yourself and as you can see there's a bit of configuring in the settings and there's a little bit more knowledge required into okay you need to deploy this project like this you need to run the spider like that there is some help pieces as well available for example if you forget how to deploy it they do have a help section here and you can follow the instructions as to oh how do I deploy my project so that scripty web and Scrappy D can use it again they've got the commands you need to run here and the steps you need to follow are you can put in your project directory and they can Auto package it up using scriptyweb as well so the next dashboard we look at is the scrape UPS integration with scrapeyd so there's two different dashboards for scrapeyd Scrapy web and the scraps dashboard so for that you need to go off and create a scrape UPS account so you can just go to scrapeups.io and if you've been using it for any of the other parts we've gone through already in our course you can use the existing API key you have if you're just joining us for this section you can sign up register for free and get your own API key so you will just need the API key and then you will just need to follow the monitoring steps so we'll just click monitoring it should be scrapey so I need to do pip install scrape up scrapey and we'll do pip install scrape up stash scrapey so we've installed the scrape off SDK we just need to add our API key to the scrapey project settings so I can copy this line go now into my folder and I want to edit my settings.py file and I'm going to just add the API key in here and check what else I need to install I also need to install the extension and the downloader middlewares so go down to my downloader middlewares and I'll add those in here so copy that paste that in and to the extensions I'll just put that under the existing extensions there so paste that in as well so we've got the download the Download Remember words the extension and the API key so I should be able to now save that and just check that there's nothing else to do there I think that's all correct so that's to install the monitoring so everything will show up in the dashboard but we now want to install the actual scheduling side of things so for that we will go to the servers and deployments section and we will add a new Scrapy d server so we just need the name of our server we just do test obviously you can name your server whatever you want we need the server IP address so we will go to our digitalocean dashboard and copy the IP address here so the ipv4 so copy that and just need to paste that in and save the details and then it's saying you should sh into your server we're already in on our console and it says run the command in your terminal so we will copy this command and go back to our console and I'll just go back up to the top and paste that in it's installing everything it needs it might need to restart some Services again we can say yes to that once it's finished we will say yes to that and okay so everything seems to be finished so we can now go back and check our servers list and we can see our server name is there and it's connected and we can check our server perfect so if we need to edit or delete the details there and it says your server is now set up correctly you can schedule your jobs on the scheduler page here I'll click that and click schedule job and I have my server name I've got my spider book Spider I will run the spider now and I can click if I wanted to run it every month every day all I can select you know a specific month or whatever we'll do that in a second for now I just want to run it now and I don't have any settings and arguments to add in so I can submit the job and the job is scheduled so in a few seconds that should show up in the jobs list in the meantime let's go and schedule a job to run let's say once a week so let's say every Monday at 7am and obviously that's in UTC time zone crons are usually run in UTC so you need to make sure that that corresponds to your own time zone correctly so we're saying every Monday at 7am please run the book spider spider on the test server and if we submit that it should then show up in our list so there you go at 7am only on Mondays it'll run that's very useful if you need to set up your spider to run every day every hour or whatever you can View and edit them here and you can enable them disable them so if we go back to our dashboard we don't have any details coming through yet there might have been an issue with running it as well but we'll explore that now so when there is an issue like this where we can't see any data coming into a dashboard the best thing to do is to try and just run the scrapey list are the square B crawl command manually from our server so I'm just going to go back to the console and go back into the project and from inside the free code Camp part 6 book scraper folder I'm going to run a scrapey list first and if there's an issue with scrapey or there is an issue with the settings it'll show up here so Scrappy list and we have an error escape the error and scrape is saying that there is an indentation error in the downloader middleware so that's probably what's causing the issue so we can just uh edit that so open up our settings again and check it's a download middlewares so I think it's just that there is a space here and in case it's also this guy I'll just remove the indentations there and do the same for the extensions and then hopefully there's no more issues so save that and try run scripture list again at this time it worked and book spider is returned perfect so if we go back to our scrapups dashboard and try and run that again so it's got the server and splatter we need selected and we can submit job so this time within hopefully a couple of seconds it's just go back to our servers page and go back to our jobs page we can see it's now running so it's after kicking off at 6 23. I'm going to give that a couple of seconds to run and then we should also have the stats available for this job so we can see on Monday one job run and it's in the middle of running now so we'll give it a couple of seconds to run and then we should see things like the runtime the pages missed Pages items and all the other stats coming through okay so on our jobs page now we can see that the status is changed to finished it took 25 seconds and we can see the number of pages items coverage and everything there so we can actually click into that now and we can see all the pages scrape there the runtime and everything in more detail so we also have things like number of items fields that were scraped so this pulls in stuff like the we can see if there was any stuff that was missed so things like the number of stars the price everything most things seem to be at 100 percent and obviously the description there there was one or two descriptions that were missed for some reason so this is can be useful to see which Fields were missed are which Fields came through correctly you can also see the amount of bandwidth that was used and we can see that there was one warning as well so obviously when you have multiple runs of the same spider then you can actually compare if you're running this daily you could compare the stats every day and then if on one day suddenly you see a major Divergence in the stats you can say okay there must be an issue with my spider I need to investigate and go in and investigate further so that's very useful we also have the status codes if there's 500s coming back are 404s the page isn't found so maybe they the links are broken so you can use these status codes as well to diagnose any other issues so that's how you would use the scrape UPS dashboards to integrate in with Scrapy D which would also be running on your server so you've two different dashboards that you can use with scrapeyd scriptyweb are the script UPS dashboard and integration if you guys have any other questions with that you can stick it in the comments as well so in the next section we'll be looking at how we can instead of using scrapeyd to degrade with scrape UPS we'll be looking at using the complete scrape UPS integration to integrate in directly with your scrapey project instead of using scrape PD as a kind of a middle layer which gets integrated with your project and then other things have to integrate in and hit the API endpoints this just goes directly and integrates with your server and your project so that's what we'll be looking at in part 11 and then Part 12 we'll be looking at using the scrapey cloud so that's it for now and see you in part 11. so in part 11 of the scrappy beginners course we're going to be looking at using scrape UPS to manage deploy and monitor our spiders so we'll jump straight into it first things first we're going to need to set up a virtual machine so you can do it with AWS if you already have an account with them you can do it with digitalocean most of these different companies have free credits so you can just sign up use their free credits try it out and then go from there so I've already got a digitalocean account and I'm going to be using that so you guys can follow along with that are you guys if you already have an AWS or Azure account you can follow on from the step where we log in to the actual virtual machine so I'm just going to quickly set up a server here I'm going to go on to the cheapest ones they have which are four dollars a month and I think that's all I need to select so I can just create droplets now that the droplets been created I have the dashboard available where it shows things like the IP address and a few different graphs I'm just going to open the console now so that's going to open up a new window and it's going to SSH onto a machine and then we can then run commands on our virtual machine so while that's getting set up if you guys haven't already set up a account with scrape UPS so you can get a free account there and we'll be using them now to integrate with our server so go ahead create an account I have one already set up once your account is set up go to servers and deployments on the site where it says add servers click add and then we're going to name our server free code camp and we're going to put in the IP address of the server so we get that from here copy their P address paste it in save the details and now it says to provision a server we need to run this script on our server so we copy the details there go in to our terminal our console for the server and paste in the script details so that's going to go off run the script and provision the server and we can see that it is running through installing the dependencies it needs installing a new user and the authorized keys and installing the required libraries so that's just going to run through those different steps and once it's complete we should be able to then go on and in Clone our spider onto our server using the dashboard so that can take an integer two we let it just run through the different steps so now our provisioning has completed and it's brought us into the server dashboard and here we have options like clone repository add spider delete the server edit the details schedule jobs and the SSH keys for the server so we're going to go ahead and go to the Clone Repository that's where it's going to be getting the details of the repository that we paste in and then it's going to clone the spider directly onto our server so we don't need to actually do it manually ourselves we can do it through this UI here so for that we're going to first go to the free code comp part 6 that we've been using in the part 10 video so we're going to use part six and the next thing you want to do is you want to Fork your own copy obviously if you have your own spider and you're using that that's fine if you're following along with me now the best thing to do is Fork your own copy so click the four button follow the steps that's just going to copy over the free code cam part 6 onto your own repo and then from there like I have here I've just done it myself I now have it under my own name and in here the next step will be to add the deploy key so this deploy key will enable us to do commands like git clone and pull the repo directly from our GitHub onto the machine so we just need to add that key we go to settings deploy keys add deploy key I'm going to call this free code cam VM and I'm going to paste in my deploy key so you get this deploy key in here you copy that you add it in here remove any spaces and we don't need to allow right access for the moment because we're just going to be pulling and we can add the key so once the key is added we can then go back to our main UI here the next thing we need to do is just go and grab our URL from the main page here grab our URL so this is my repository and this is my own copy of the free code cam part 6. the branch name is main so you can see it here it's Main so I need to make sure to put in the right Branch name and then the language is Python and the framework is scrapey so that's all correct so this is the install script that's going to run when we click on repo it's going to go in to our virtual machine it's going to then get clonus and then once it's get cloned the repo it's going to go into the repo install a python virtual environment activate the virtual environment and then install the modules that are in the requirements.txt so if you look in here it's going to install the different modules that are listed in here which is everything that the project needs to run so once it's done that it's just going to make sure that scrape is installed and while we're at it we're going to add the monitoring module first grade pops as well so let's just add in that here as well so install scrape up scrape B which installs the monitoring python module for us so once that's all in here we can click clone repo and it's going to go through the steps here so it's cloning the repo great and then it's going to run the install script which can take two or three minutes and then the next step is it's going to find the Scrapy spiders by running the scrapey list command so I'm just going to give it a minute or two and then we will hopefully see our repo in our table here and we'll see the spider our book spider in under the spiders table on the right so as you can see the install script ran correctly and it was able to find our spiders as well so you can see our spider automatically came in here and here is our cloned Repository so if you click in you can see that there is a deploy script here as well so if you need to deploy updates to your code you will update your own repository and then for the code to actually go onto the server you just need to go in here and click deploy and it will then pull the latest from your Repository so that's how you would update the code on your VM okay so we have a repository we have our spider so let's just go ahead and show you guys you can quickly run the Spider by just clicking the run now button it'll go in select the server the repository and the spider because you could have multiple spiders in your repository and we're just going to click submit job to run it straight away so the job is started and if you want to check the log straight away you just come here and click view logs so you can see it's just gonna head and it's running the spider correctly and you can see the title product type price everything is coming through so that's how simple it is to run the spider so the last step we want to do now is to activate the monitoring for our spider so instead of having to just look at a bunch of logs in a log file like that we can have everything displaying in the dashboard like we had in part 10. so to do that let's just open up the docs go to monitoring python scrapey scrapey SDK integration so we've already done this pip installed scrape up scrapey as part of the install script when we cloned our repository so we don't need to do that again but we do need to add in our API key and the extension bits here so it's telling us we need to add this to our settings.py file in our scrapey project so let's go ahead and do that now so I'm going to open up my Repository go to book scraper and go to the settings.py file and I can just edit it directly in here so I'll need to add the three different sections so first I'll add the API key and then I'll go and add the extension and the downloader middlewares so I'll just copy this line and go to GitHub paste this in here I need to get my API key which I can get from my settings here paste in my API key and then I want to go to my extensions and then the general middlewares extensions are currently comforted out so I'll just add it underneath and last of all downloader middleware's so obviously if you guys are using your own spider that has the downloader midwares uncommented out and you're just adding these two lines to your existing list but here because it's currently commented out I'm just pasting in the whole lot in together so we've got the extensions the downloader middlewares and the API key so we should be able to just commit the changes and now we can deploy the code via our dashboard so now that's completed we go back to our server go into our free code camp server go into our clone Repository and click deploy here so the latest has been deployed we can check the log as well to see did deployment work so we can see it updated the book scraper settings.py file one file changed with nine insertions so that's perfect great so now that that's in we should be able to run our book Spider again submit the job and if we check the logs again we can see it's kicked off and if we go to our jobs list we can see there's one running so this is the one we ran for part 10 and this is the one that is running now so once it's completed running in about 20 more seconds we should see the pages items coverage and everything else fill in as well and we can also see that in our dashboard we can see under Tuesday we have this job that is running now so we'll just quickly show you how you can also schedule it so in case you're just joining us for part 11 if you want to schedule a job to run on This Server recurring you just go and click recurring and then you can select we want okay every day in March we want every every time at midnight we want this job to run so we'll submit the job and then we can check in our scheduled jobs we have book Spider which will run as 12 every day only in March and there it is so then if you need to edit that or you want to just disable it you can go to the scheduler Tab and you have the ability just to disable it there or delete it cloness view the logs whatever you need to do so if we just go back to our dashboard we can now see that the job is completed the page is scraper there we can see the items are there and everything looks like it ran correctly so we can compare the the two days so yesterday this many pages were scraped are this many status codes came back today this many status codes came back so it is useful if you need to compare the same job that was run over multiple days you can quickly see okay if the runtime varied or if the number of page varied varies are the items are the coverage you can see that very quickly in one page one glance so that makes it very useful and if we need to drill down into the individual job we can just click in and we can delete the job data or do anything else we need to do here so that brings us to the end of this section any questions you guys might have let us know and I hope you have an idea now of how to quickly get set up with a virtual machine and hook it up to use scrape UPS so a reminder that everything that we've used with scrape UPS here is free to use so there's no limitations on the image of servers you can hook up are the amount of jobs you can run so that's the end of part 11 guys so in part 12 we're going to look at everything to do with Scrappy cloud so Scrappy Cloud was made by the developers of scrapey and it's a way which you can deploy and run and schedule your spiders on the cloud using scrapey Cloud the great thing is you don't need to have your own thirdparty server so you don't need to have a server with digitalocean or vulture or AWS you can just deploy it directly onto Scrappy cloud and just run it the only downside is that if you want to schedule your jobs it's paid so you can run your jobs and it's free but to schedule your jobs on Scrappy Cloud you have to sign up for a monthly subscription so we'll show you how everything works and then between scrapey Cloud scrape ups and Scrapy D you guys will have had a full overview of all the different ways you can deploy schedule and run your spiders on the cloud and you can decide then which kind of way you want to go do you want to go with a completely open source way with using just scrapeyd and scripted web or do you want to go with a free way using scrape UPS or do you want to go with a paid solution with scrapey cloud so you'll have the full up array of options covered by the time we finish part 12. okay so let's quickly look at scrapey Cloud so scribby cloud is obviously made by size the Creator is the scrappy scalable Cloud hosting for your scrapey spiders and that's pretty much what it is host and monitor your Scrappy spiders in the cloud as we said and it's very reliable easy to scale as they say on demand scaling they have lots of other Integrations as well so what we need to do is you need to go ahead and create an account with them once you have an account you can then go into the dashboard and access scrapey Cloud here on the side so I'm just going to start a new project and I'm going to call it just free code camp and click Start so then it's got the instructions here of what we need to do to install the command line tool so we can easily deploy our spider into the scrapey cloud so first things first we're going to go ahead and we're going to be using the part 6 code example again so the code that we used for part six of this course we're going to just get clone that so I've got an empty folder here open in vs code and I'm just going to Gig clone the free code Camp part 6 and then I'm going to quickly install a virtual environment obviously if you guys are on Windows or on Linux you guys need to follow the steps they'll be covered in part two of this course to make sure you're installing the correct virtual environment for your operating system and then once the virtual environment is set up we can activate it so just do Source bin activate I know it's activated and now we can go and follow the instructions here so I'm just going to copy and paste them in directly so install S hub and then to the shub login and then it's just a matter of putting in my API key okay so that's all installed put in shub login it says I'm already logged in I'll just log out to make sure that you guys can see the full process so a sub login put in my API key from here paste in my API key obviously you guys will be putting in your API key so please don't use mine and then we can just do shub deploy and it should deploy if I'm in my correct project so I'm inside my project and I think it wants there to be a scrapey.cfg file so you need to make sure you're in the correct folder and if it's correct it you can see the deploying to Scrappy Cloud project and then the project ID there so in the meantime you can see it's bills the project and then it uploads it to the site here so here you can see it's been deployed and it's successful so if it works correctly this should all be very similar to what you see so now that it's deployed into the cloud we can go to our jobs dashboard and we should be able to run our spider so we have our book Spider available now in the drop down and we can leave everything else the same you've got priorities there if you want to have certain jobs running ahead of others so we can just click run and that should kick off the book spider and it should start scraping our books to scrape site so you can see it's running away there and we should see that when it's completed it'll be in the completed table and it'll also have the items and the requests and the errors all available here as well so they'll start populating in a second so as you can see three requests the logs are there if we wanted to run every day once a week periodically whenever we want we just go to periodic jobs click add periodic job select the spider and then we can select okay we want to run every Monday at 6am or 7am and we can just select every day or every Monday 7 A.M and we can save that so as you can see here it says the periodic jobs below will not run because I have not created a paid subscription so you just need to click subscribe now and then sign up for the paid version and then this job will run every Monday at 7. but if you guys just want to try it out then you could just schedule the jobs normally by clicking run here manually yourselves so my spider is still running away we'll just leave that complete and then we'll have a look at the items and requests and some of the stats that are available so as you can see our job is completed with the Thousand items the requests so the amount of pages that were scraped and the logs so we can click into the requests and you can see T thousand requests if you want so you can see all the specific URLs that were scraped the statuses etc etc the items then are a bit more interesting obviously because that's the actual data that we scraped so we've got everything set up nicely there our prices taxes titles URLs description etc etc so you can check quickly if the information that was scraped correctly came through or not if you need to look at the logs or the stats that's also available there so as you can see it's very polished very nice simple to use and also works and auto scales when you need to scale things as well so I think that's the ins and outs of scrapey cloud I think now you should have a very good idea of the different options you have when it comes to deploying your spiders and then scheduling and running your spiders and seeing all the stats in various dashboards so just to go through the three options you have first option Scrapy D which can run just via an API endpoint and you can hit the API endpoint to schedule things and deploy things with Scrapy D you have two uis you can use with it the scrapeydweb are the scrape Ops dashboards which we've shown you how to install so that's the free open source part the second option was using scrape ups for everything and using the scrape UPS integration directly with a server such as digitalocean are AWS our vulture so you need to set up a VM quickly there first and then the final option was just using the scrapey cloud for everything and deploying it directly to Scrappy Cloud but it was paid if you want to do any sort of periodic jobs to schedule them to run daily or weekly or whatever so there are the three main options and I'll leave it up to you guys to decide what works best for you so that brings us to the end of Part 12 and in part 13 we'll just go through everything and do quick recap of the entire course so see you in part 13 guys so guys we've come to the end of our Scrappy beginners course this is the last part so we'll just do a quick wrap up and then I'll give you a small bit of an overview of some extra skills you might find useful if you want to continue on and get better at scraping using scraping so we've built an endtoend scrapey project that scrapes all the books from books to scrape and then cleans the data and stores the extracted data in different file formats and different places such as a database we then looked at optimizing our headers and user agents so that websites would let us get through any antibiot software that they have on their site and we also looked at how we can use proxies in the different proxy provider options that are out there if we want to have something that's a little less HandsOn and then finally we looked at how you can deploy your spider to the cloud onto a server and then how you can schedule this to run periodically and then how you can view the results of your running spiders but obviously that isn't everything we've only gone through the kind of Basics there's still a lot more different edge cases that we haven't covered and the number one thing is probably scraping Dynamic websites so there's a lot of websites out there that are rendered in the browser so that means that they're using a frontend framework which will actually only display the page once all the data is received by the browser so in that case if you were to ask for a URL what you get back might not contain the data you're looking for because it hasn't had a chance to render inside in a browser so things I would recommend that you look at in in those cases would be looking at things like Scrappy Puppeteer or Scrappy selenium which then use scrapey with a headless browser integration to actually render the website in a headless browser so then that way using scrapey Puppeteer or scrapey selenium you can actually render the page and get the data you need the other option would be to find the API endpoint because most of these sites that are front and rendered have API endpoints and you can find the data there so I'll just give you one example of what it would look like to see the data coming back from an API endpoint so I would recommend if you guys want to work through a couple of different challenges site have put together this great site which we've been using for our books to scrape.com but they also have a bunch of different other examples using this quotes dot to scrape.com where you can for example have infinite scrolling so how would you get around a page where you have infinite scrolling like something like Instagram or Facebook and you can practice having a JavaScript rendered site so there's all these kind of different options which you can practice with and they have all these different pages available here for you to practice on so one example would be this one see we're scrolling down and if you go to your network tab you can see there is data being asked for every time we scroll down so more pages of data are being requested and they come back but instead of it being HTML it comes back as Json and then this Json data we've got the quotes which then the front end framework then goes off and populates the page with so this is an example of where you can actually directly query an API endpoint instead of actually scraping the HTML you can ask for the API endpoint to give you back the data directly so that even makes your life easier so this is one example of what I was talking about if you come into contact with a frontend rendered page that gets rendered in the browser so I really would recommend you guys checking out and working your way through these different challenges that are available on the twoscribe.com site now another very important thing is obviously getting through a login endpoint which we didn't do in this course but which is something which a lot of websites would have so that is something as well I would really recommend that you guys go off and explore how to do so the last major thing I think would be looking at how you can scrape at scale if you really want to imagine you're scraping millions of pages a day you know there's different ways which you can use things like scrapey redis to use redis to store all the URLs you want to scrape in one central place and then you could have multiple different servers pulling URLs off this queue and then the old URLs can be scraped by multiple different worker machines at the same time again this is all using scraping so that is something I would highly recommend you guys to look at as well if you're interested in scraping at scale now all this stuff we also have as articles and videos so if you guys want to check out some more videos and indepth articles I would recommend you guys checking that out like we have scraping behind logins and we have separate articles on using Scrappy Puppeteer Scrapy selenium Etc and also using scrapey redis if you want to do a distributed worker architecture so all that is up there for you guys if you want to continue on your journey and learn more about scrapey and the kind of more challenging parts of using scrapey to scrape the web so I think that comes to the end of our course I'd like to thank you for following along and if you have any questions just reach out put a comment on the video and we'll do our best to get back to you thanks guys
