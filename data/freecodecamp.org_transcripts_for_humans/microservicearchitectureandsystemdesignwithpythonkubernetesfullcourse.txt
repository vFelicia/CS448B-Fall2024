With timestamps:

00:00 - in this course you will learn about
00:01 - microservice architecture and
00:04 - distributed systems using a Hands-On
00:07 - approach a microservices architecture is
00:10 - a type of application architecture where
00:13 - the application is developed as a
00:15 - collection of services Giorgio from
00:18 - Canton coding teaches this course he
00:21 - does a great job teaching how to combine
00:23 - a bunch of different Technologies into a
00:25 - single application hey what's up
00:27 - everybody and welcome to this video on
00:30 - microservice architectures where we will
00:32 - be applying this architecture to an
00:35 - application that will convert video
00:37 - files to MP3 files in this Hands-On
00:40 - tutorial we'll be making use of python
00:43 - rabbitmq mongodb Docker kubernetes and
00:47 - MySQL to build this microservice
00:50 - architecture which is admittedly a lot
00:53 - but don't worry I'll walk you through
00:55 - every step so let's go over what this
00:58 - application is going to to look like
01:00 - from a top-down perspective so when a
01:03 - user uploads a video to be converted to
01:06 - MP3 that request will first hit our
01:08 - Gateway our Gateway will then store the
01:11 - video in mongodb and then put a message
01:14 - on this queue here which is our rapidm
01:16 - queue letting Downstream Services know
01:19 - that there is a video to be processed in
01:21 - mongodb the video to MP3 converter
01:24 - service will consume messages from the
01:26 - queue it will then get the ID of the
01:28 - video from the message pull that video
01:31 - from mongodb convert the video to MP3
01:34 - then store the MP3 on mongodb then put a
01:38 - new message on the Queue to be consumed
01:39 - by the notification service that says
01:42 - that the conversion job is done the
01:44 - notification service consumes those
01:46 - messages from the queue and sends an
01:48 - email notification to the client
01:50 - informing the client that the MP3 for
01:52 - the video that he or she uploaded is
01:55 - ready for download the client will then
01:57 - use a unique ID acquired from the
01:59 - notification education plus his or her
02:01 - JWT to make a request to the API Gateway
02:04 - to download the MP3 and the API Gateway
02:08 - will pull the MP3 from mongodb and serve
02:11 - it to the client and that is the overall
02:14 - conversion flow and how rapid mq is
02:17 - integrated with the overall system okay
02:20 - so the first thing that we're going to
02:22 - need to do before we get started with
02:24 - writing any code or setting up any of
02:26 - our services is we're going to need to
02:29 - install a few things now the Links for
02:32 - all of these pages are going to be in
02:35 - the description and it's critical that
02:37 - we correctly install all of these
02:39 - prerequisites prior to moving forward
02:41 - with the course so please take the time
02:44 - to make sure that you've correctly
02:46 - installed all of these things so to
02:49 - start we're going to install Docker and
02:52 - I'm using Max so everything that I do
02:55 - like everything that I install is going
02:56 - to be for Mac so if you have Windows or
02:59 - a Linux system you're going to need to
03:01 - do some additional research to figure
03:03 - out how to install on those platforms
03:05 - because I'm not going to spend the time
03:07 - diving too deep into that but if you're
03:09 - able to successfully install everything
03:11 - on this list then that's probably one of
03:14 - the most difficult parts of this course
03:15 - and the rest of it should be a pretty
03:17 - smooth sailing so to start we're going
03:19 - to install Docker so I would hit this
03:22 - install Docker desktop and I'm using an
03:25 - M1 MacBook Pro so I would install with
03:28 - apple chip if you're using an Intel
03:30 - MacBook Pro or MacBook then you would
03:32 - use the Intel chip installation and of
03:34 - course you're just going to click the
03:36 - link and install it that way
03:39 - and following the successful
03:41 - installation of Docker you should be
03:43 - able to do Docker version and get a
03:46 - Docker version here now once you've
03:48 - gotten Docker installed we'll move on to
03:51 - installing the kubernetes manline tool
03:54 - and as you can see the kubernetes
03:56 - command line tool allows you to run
03:57 - commands against kubernetes clusters and
04:00 - we're going to be deploying our services
04:02 - within a kubernetes cluster so that's
04:04 - why we need this command line tool so
04:07 - again I'm going to choose to install for
04:10 - Mac
04:11 - and there are a couple of options to
04:14 - install for Mac you can use Homebrew or
04:16 - you can install the binary using curl I
04:19 - believe I installed the binary using
04:21 - curl and I selected the Apple chip and I
04:25 - just ran this command
04:29 - and I didn't validate the binary
04:35 - but you do have to make the binary
04:37 - executable by running this command
04:40 - and of course you're going to need to
04:41 - move the binary to a location that's
04:44 - within your path now I'm not going to
04:46 - get into details on how to do this this
04:48 - documentation is pretty detailed and
04:50 - installing binaries and adding them to
04:53 - your path isn't within the scope of this
04:54 - video so if that's a little bit too
04:56 - advanced for you you can just use the
04:58 - Homebrew installation which pretty much
05:00 - automates all of this for you
05:03 - and once you've finished with the
05:04 - installation you should be able to type
05:06 - Cube CTL or cube cuddle or whatever you
05:09 - like to call it
05:11 - and if you type that you should get the
05:14 - output that provides information about
05:16 - this command line utility
05:19 - now following the installation of
05:21 - cubecto you can move on to installing
05:24 - minicube and minicube is a local
05:26 - kubernetes focusing on making it easy to
05:29 - learn and develop for kubernetes so
05:31 - basically this is going to allow us to
05:34 - have a kubernetes cluster on our local
05:37 - machine so this way we can make a
05:40 - microservice architecture on our local
05:42 - machine without having to actually have
05:44 - a kubernetes cluster deployed to like a
05:47 - production environment or something like
05:48 - that so this goes on to explain the
05:51 - requirements to use minicube
05:54 - and the installation instructions are
05:57 - here and you're just going to select for
06:00 - your operating system so I would select
06:03 - Mac OS and I would select arm 64 as the
06:06 - architecture and of course we'll select
06:08 - stable and I used the binary download
06:12 - for this one as well but again if you're
06:14 - not familiar with how to install
06:16 - binaries and configure your path just go
06:19 - with the Homebrew installation
06:22 - and then once it's installed we should
06:24 - be able to start a cluster from our
06:27 - local machine
06:29 - and your Cube CTL will automatically be
06:33 - configured to work with mini cubes so
06:36 - you don't need to do anything there so
06:38 - once mini cube is installed you should
06:39 - be able to do mini Cube start
06:45 - and everything should start up for you
06:47 - and there's actually another thing that
06:50 - I forgot to add to the list of what we
06:52 - need to install so we're going to
06:53 - install this K9s
06:58 - and we're just going to use this to sort
07:01 - of help manage our kubernetes cluster so
07:04 - the installation instructions for this
07:06 - are just here in the documentation you
07:09 - can just do Brew install canines of
07:11 - course if you're on Mac but of course
07:13 - they have the installation instructions
07:14 - for other operating systems as well so
07:17 - just figure out which one of these is
07:19 - best for you
07:21 - and once you've finished installing
07:23 - canines you should be able to type
07:25 - canines in the command line interface
07:27 - and it should pull up our cluster which
07:30 - is just our mini Cube cluster which you
07:32 - can see here and you can quit by just
07:34 - using Ctrl C
07:35 - and let's go ahead and clear this now
07:38 - the next thing that we're going to need
07:39 - to install is python3 more specifically
07:42 - because we're going to use Python 3 to
07:44 - create our first service a very simple
07:47 - authentication Service and all of these
07:49 - services are going to be relatively
07:51 - simple because the focus of this
07:53 - tutorial isn't necessarily the services
07:55 - but how all of the services
07:57 - intercommunicate and how a distributed
08:00 - system is integrated as a whole so we're
08:03 - not going to spend too much time on
08:05 - creating large services for this
08:08 - microservice architecture anyways to
08:10 - download python if you don't already
08:12 - have it you're just going to select for
08:14 - Mac of course you're just going to
08:15 - select this button here and just follow
08:18 - the installation instructions from there
08:20 - and lastly we're going to need to
08:22 - install MySQL because this is going to
08:25 - be the database that we use for our auth
08:28 - service so I installed MySQL using
08:31 - Homebrew so you can just do Brew install
08:33 - MySQL so let's just go ahead and copy
08:35 - this
08:36 - and we can just paste it
08:41 - now as you can see here it says we've
08:43 - installed your mySQL database without a
08:45 - root password and to secure it run
08:47 - mysql's secure installation but we're
08:49 - not going to do any of this because
08:50 - again that's not the focus of this
08:53 - tutorial so yeah in a production
08:55 - environment you're going to want to make
08:57 - sure you're securing your MySQL
09:00 - installation and you're going to want to
09:02 - follow all of the best security
09:04 - practices when working with and
09:06 - installing a database server but in our
09:09 - case we're just going to leave the
09:11 - installation the way it is which again
09:13 - isn't something that you're going to
09:14 - want to do in a production environment
09:15 - this is just so that we can get to the
09:19 - actual meat and potatoes of the tutorial
09:21 - so we can access our mySQL database by
09:26 - just running MySQL with the user root
09:28 - and we don't even need a password so we
09:30 - should just be able to do MySQL U root
09:33 - and this will give us access to the
09:35 - database
09:38 - and I think that's it for the things
09:40 - that we need to install for now we're
09:41 - probably going to need to install some
09:43 - more things later but for our first
09:45 - service I think that that's all that we
09:47 - need so as mentioned before we're going
09:49 - to start with our auth service that's
09:51 - going to be this first service that we
09:53 - create and deploy to our cluster on our
09:56 - local environment and just quick note
09:57 - everything's going to be deployed on our
10:00 - local environment in our mini Cube
10:02 - cluster we're not going to deploy
10:04 - anything to a server I might create
10:06 - another tutorial on how to actually
10:08 - deploy this to a server or to a
10:11 - production like environment but for now
10:13 - we're only focused on the actual
10:15 - architecture so everything's going to be
10:17 - done on our local system within this
10:19 - mini Cube cluster so to start we're
10:22 - going to want to create a directory so
10:24 - we'll make dur and we'll call this dir
10:27 - maybe something like system design
10:32 - and we'll just go ahead and change
10:33 - directory into this system design
10:35 - directory and we're going to be writing
10:38 - code for multiple services and actually
10:41 - all of the services are going to be
10:43 - written in Python and if you aren't
10:44 - familiar with python don't worry I
10:47 - explained enough when writing the code
10:49 - where it shouldn't really matter which
10:51 - language you're most comfortable with
10:52 - and since we're focused on the
10:55 - architecture as a whole we're not
10:57 - necessarily going to be writing any
10:58 - complicated code anyway so anyways we're
11:01 - going to make a directory for Python and
11:03 - this is going to contain our python
11:05 - services and we'll put a source
11:08 - directory but we don't need a bin
11:10 - directory so we can just do let's just
11:13 - make a python directory change directory
11:16 - to python
11:18 - make directory source
11:22 - So within this system design python
11:25 - Source directory we're going to create
11:27 - the directory for our auth service so
11:30 - we'll make their auth and this author is
11:33 - going to contain our auth service code
11:35 - so let's CD auth
11:37 - and from here to start we're going to
11:40 - want to create a virtual environment
11:44 - and we're going to write all of the code
11:46 - for the service in one file that we're
11:48 - going to call server.pi and the reason
11:51 - we're doing this in one file is because
11:53 - it's going to be less than 700 lines of
11:55 - code and like I said before the service
11:57 - is going to be relatively small it's
11:59 - just going to be a very simple auth
12:01 - service and actually I forgot to
12:03 - activate our virtual environment so
12:06 - let's go ahead and do that and we should
12:07 - see that our virtual environment is
12:10 - running if we run this command here and
12:13 - I'm going to need to install a couple of
12:15 - things for my Vim configuration I'll go
12:18 - ahead and install pylent
12:21 - and pip install Jedi let's just go ahead
12:25 - and run this command that they're
12:26 - suggesting to upgrade pip
12:31 - and let's open server.pi and the start
12:35 - of our file is just going to be to
12:37 - import JWT which is Json web token and
12:41 - we're going to get into why we're
12:42 - importing that soon we're going to
12:45 - import date time and Os and we're also
12:49 - going to import from flask
12:52 - import flask and request and we're also
12:56 - going to import from flask
12:59 - mysqldb Imports
13:02 - my SQL so basically this is going to
13:07 - allow us to query our mySQL database
13:10 - this is going to be our actual server
13:13 - we're going to be using flask to create
13:15 - our server and this is going to be what
13:18 - we're going to use for our actual auth
13:21 - we're going to use Json web tokens and
13:25 - date time is so that we can set an
13:27 - expiration date on our token and Os is
13:30 - just going to be used so that we can use
13:33 - environment variables to configure our
13:35 - MySQL connection and you will see what I
13:38 - mean by that soon
13:40 - so let's just go ahead and save this and
13:42 - we're going to need to install a couple
13:44 - of things so let's just cat server.pi so
13:48 - we can see what we need to install and
13:49 - then we can just do pip install I
13:51 - believe JWT is like Pi JWT
13:55 - and we're going to need to pip install
13:57 - flask
14:01 - and pip install flask MySQL DB
14:08 - and we can open this file back up again
14:10 - and to start we're just going to create
14:13 - a server which is just going to be a
14:15 - flask object
14:18 - so we're going to instantiate this flask
14:21 - object and we're going to create a MySQL
14:23 - object which is going to be an instance
14:26 - of this MySQL which we passed the server
14:29 - now for the purposes of this video we
14:31 - don't necessarily need to understand the
14:34 - magic Behind These two lines of code
14:36 - here
14:37 - if we go ahead and save this and we go
14:39 - to the definition here we can get a
14:41 - general understanding of what this flask
14:44 - object is doing but the main thing that
14:46 - we need to be concerned with is once it
14:48 - is created it will act as a central
14:50 - registry for the view functions the URL
14:53 - rules template configuration and much
14:55 - more so basically this is just going to
14:57 - configure our server so that requests to
15:00 - specific routes can interface with our
15:02 - code and this MySQL object is basically
15:05 - just going to make it so that our
15:08 - application can connect to our mySQL
15:10 - database and basically query the
15:12 - database so following this we're going
15:14 - to want to set up our config so our
15:17 - server object has a config attribute
15:20 - which is essentially a dictionary which
15:23 - we can use to store configuration
15:25 - variables so for example we can set the
15:29 - configuration for our MySQL host
15:33 - and we can set it equal to OS dot
15:37 - environ.get which is just going to get
15:40 - the MySQL host from our environment so
15:43 - what do I mean by that so if we save
15:45 - this and we do export MySQL host equals
15:49 - localhost and we go into server.pi
15:54 - this code here this OS dot environ.get
15:57 - MySQL host is going to resolve to
16:00 - localhost that we set in our environment
16:03 - within our shell so if we were to go and
16:06 - print this
16:07 - server.config
16:10 - MySQL host
16:13 - and if we were to just Python 3
16:15 - server.pi
16:17 - you'd see that it prints out the Local
16:19 - Host that we set in our environment
16:21 - variable
16:25 - So This Server is our application
16:29 - and server.config is just the
16:31 - configuration for our server or our
16:33 - application
16:34 - so we're going to create a couple of
16:36 - these
16:40 - all with different variables of course
16:42 - so this one's going to be
16:45 - MySQL user
16:48 - and the same here for the environment
16:51 - and this one will be MySQL password
16:59 - and this one would be MySQL DB
17:05 - and MySQL
17:08 - port
17:09 - and we don't need that one so this is
17:11 - going to be the configuration for our
17:13 - application and these are going to be
17:15 - the variables that we use to connect to
17:17 - our mySQL database and the next thing
17:20 - that we want to do is create our first
17:22 - route
17:23 - which is going to have the path login
17:26 - and the methods for login are just going
17:30 - to be post
17:31 - and this route is going to route to this
17:34 - function login
17:36 - and we can just write the code for the
17:38 - login function we're going to set off a
17:40 - variable called auth equal to request
17:42 - dot authorization
17:46 - and this request is the request that
17:49 - we're importing here and with this
17:51 - authorization attribute provides is the
17:54 - credentials from a basic authorization
17:56 - header so when we send a request to this
17:59 - login route we're going to need to
18:01 - provide a basic authorization header
18:04 - which will contain essentially a
18:05 - username and a password and this request
18:08 - object has an attribute that gives us
18:10 - access to that so once we instantiate
18:13 - this object we'd be able to do
18:15 - auth.username to get the username from
18:18 - the basic authorization header and
18:22 - auth.password to get the password from
18:25 - the basic authorization header
18:28 - and you'll see what I mean when we send
18:30 - the actual request but if we don't
18:32 - provide that header within the request
18:33 - then this auth is going to be none so
18:36 - that means that the request is going to
18:38 - be invalid so we're going to do if not
18:40 - off so if the header doesn't exist
18:43 - within the request we're going to return
18:45 - missing credentials and we're going to
18:49 - turn a status 401 which is just the
18:52 - standard status code that we would
18:53 - return in this case
18:55 - and the next thing that we're going to
18:57 - do is we're just going to check DB for
19:00 - username and password
19:03 - so the way that this login route is
19:05 - going to work is basically we're going
19:08 - to check a database so this off service
19:11 - is going to have its own mySQL database
19:13 - and we're going to check a user table
19:15 - within that database which we're going
19:17 - to create and the user table should
19:19 - contain a username and password for the
19:22 - users that are trying to log in or
19:24 - trying to access the API so actually
19:27 - before we do this part we want to go
19:29 - ahead and create a database and a user
19:32 - table and a user that we can use to
19:34 - access the API so we'll go ahead and
19:37 - save this and let's just clear and we
19:40 - just want to go ahead and create a file
19:42 - called
19:43 - init.sql So within this file we're
19:46 - basically going to create a user for our
19:49 - auth service and we're going to give
19:51 - that user a username and a password and
19:53 - then we're going to create a database a
19:56 - mySQL database called auth which is
19:58 - going to be the database for our auth
20:00 - service and we're going to Grant the
20:02 - user that we create privileges to the
20:04 - database and we're going to create a
20:06 - table within that database called user
20:08 - not to be mistaken with the user that
20:10 - we're creating within the mySQL database
20:13 - and that user table is going to be what
20:15 - we use to store users that we want to
20:18 - give access to our API and I know I'm
20:21 - using the word user a lot and the users
20:24 - that I'm mentioning here aren't
20:25 - interchangeable so let's just go ahead
20:27 - and get into it so I can show you what I
20:29 - mean so first we want to create user now
20:33 - this user is the user for our actual
20:36 - database so this isn't the user that
20:38 - we're trying to give access to our API
20:41 - and we're just going to call it auth
20:42 - user because it's the user for the auth
20:44 - service to access our database
20:48 - and we're going to say identified by and
20:51 - we're going to give it a simple password
20:53 - auth123
20:55 - so this here is creating the user to
20:57 - access the mySQL database so this isn't
21:00 - the user to access the API this is a SQL
21:03 - script so we're basically just writing
21:06 - out some SQL queries and statements in
21:09 - this script to build our database
21:10 - essentially so then we want to create
21:13 - database
21:15 - and we're going to call the database off
21:17 - and we want to give this user up here
21:20 - access to the auth database and all of
21:23 - its tables so we're going to do Grant
21:25 - all
21:27 - privileges on off and all tables to
21:33 - auth user which is the user that we just
21:35 - created at localhost
21:38 - and we want to use the auth database
21:40 - when we create the table and then we'll
21:43 - create a table called user
21:47 - and the primary key is just going to be
21:49 - ID int not null Auto increment
21:55 - primary key
21:57 - and actually there's a typo here
22:00 - Auto increment and the next column is
22:04 - going to just be email and it's going to
22:06 - be varchar and we'll just put the
22:08 - maximum 255 not null
22:11 - and lastly we're going to have a
22:13 - password which is again a parchar
22:18 - and we'll just allow it to be long
22:22 - and after we create the table we just
22:25 - want to insert the user that we're going
22:28 - to use to test our application
22:31 - so we're going to say I'm going to give
22:33 - the user an email and a password
22:35 - and the values are going to be
22:39 - I'll just use my name you can use yours
22:41 - if you want or whatever
22:44 - admin123 for the password
22:46 - and can't forget this in my colon so
22:50 - so this is going to create our initial
22:53 - user
22:54 - or the user that will represent our auth
22:57 - service so basically we're going to use
23:00 - this credential to access the database
23:02 - via our auth service and then we're
23:05 - going to create the database
23:07 - and then we're going to Grant
23:08 - permissions for our auth service to make
23:11 - changes to this auth service database
23:14 - and we're going to go ahead and create
23:16 - the table here as well as our initial
23:18 - user
23:20 - and this is going to be the user that
23:21 - goes into the database which will have
23:24 - access to our off service API
23:28 - so we can just go ahead and save that
23:30 - and now we can just go ahead and run
23:32 - this script to create our initial
23:34 - database
23:35 - so just before we do that we'll just go
23:37 - ahead and
23:39 - go into our database and we can do show
23:44 - databases here and you'll see that we
23:47 - don't have the auth database yet
23:50 - so we can go ahead and exit that and
23:52 - clear and now we're going to do the same
23:55 - thing that we would do to log in but
23:57 - this time we're just going to pass in
23:59 - our init.sql file and it appears we have
24:03 - an error in our syntax near Auto
24:06 - increment primary key
24:08 - so let's go ahead and go back in here
24:11 - and I spelled increment wrong
24:16 - should be increment
24:19 - and let's just try that again
24:22 - and actually it's failing now because
24:24 - just now when we try to run the script
24:26 - the first time it already created the
24:29 - user but then the script failed so we
24:32 - already have this user so it's trying to
24:34 - create the same user again so let's just
24:36 - go ahead and delete their user
24:40 - drop database auth as well
24:43 - and drop
24:45 - user
24:48 - so now we dropped the database and we
24:50 - dropped the user that we created in that
24:52 - script because we want the whole script
24:53 - to run not just part of it so we can go
24:56 - ahead and clear this and let's just run
24:58 - it one more time so on SQL
25:01 - you root
25:04 - innate.sql and the script ran
25:06 - successfully
25:08 - so let's go in here
25:10 - and show databases and you can see now
25:13 - that we have this auth database and if
25:16 - we use auth
25:18 - and we show tables
25:21 - you can see that we have this user table
25:23 - and we can also describe user and it
25:27 - shows all of the fields for the user
25:29 - table which include the primary key
25:31 - which is the ID the email and the
25:33 - password and we can even select all from
25:37 - user
25:39 - oops
25:41 - select all from user and you see that we
25:44 - have the one user that we created in the
25:47 - script I named it my name for the email
25:50 - and admin123 for the password you can do
25:53 - whatever you want for this part as long
25:54 - as you make sure to use those
25:56 - credentials when you actually try to
25:58 - make requests to our API
26:00 - so we can go ahead and exit that and now
26:03 - we can go back into writing our code for
26:05 - our server.pi file so here we're going
26:08 - to check the DB for the username and the
26:11 - password that we pass in our basic
26:14 - authorization header in our request to
26:16 - this login endpoint and we're going to
26:18 - do that by making use of this flask
26:21 - MySQL DB here so this is basically just
26:24 - going to allow us to interface with our
26:26 - mySQL database
26:28 - and that's why we added this application
26:31 - config here which has all of the
26:33 - configuration variables to connect to
26:35 - the database that we just created
26:39 - and we're going to need to set
26:41 - environment variables for our host and
26:44 - our user and our password in our
26:46 - database which are all things that we
26:48 - just created
26:49 - and our host is going to be localhost
26:51 - and the port is going to be the default
26:54 - port for MySQL
26:56 - so we'll go here and we're going to
26:59 - create a cursor by doing
27:01 - mysql.connection.cursor
27:06 - and we're going to use that cursor to
27:08 - execute queries so we're going to say
27:10 - the result of the query is going to be
27:13 - equal to
27:14 - cursor.execute and within this execute
27:18 - method we're going to put in our query
27:21 - so we can just do it this way
27:24 - so we'll do select email we want to
27:26 - select the email and the password from
27:29 - our user table where
27:31 - email
27:33 - equals
27:34 - whatever email is getting passed in the
27:37 - request
27:38 - so we're going to do off we're going to
27:40 - take it from that alt dictionary or
27:42 - object we're going to take the username
27:44 - and we need to pass it in as a tuple
27:48 - so remember this auth object here gives
27:50 - us access to the username and the
27:52 - password from the basic authorization
27:54 - header
27:56 - so what we're doing here is we're
27:58 - selecting from our user table in our
28:01 - auth database the email that's equal to
28:04 - the username that's passed into this
28:06 - basic authorization header so we're
28:08 - going to be using email for our username
28:13 - and if the user exists within our
28:15 - database then we should have a result so
28:18 - if result is greater than zero because
28:20 - results going to be an array of rows I
28:23 - believe so if result is greater than
28:25 - zero then that means that we have at
28:27 - least one row with that username and in
28:30 - this situation we should only have one
28:31 - row with that username because the
28:34 - username should be unique but actually I
28:37 - forgot to set the column for the
28:39 - username to be unique so maybe we should
28:41 - go ahead and do that
28:44 - so we should be able to just go over
28:46 - here and add unique for the email and
28:50 - then save it and once again we need to
28:53 - drop our database
28:55 - we're going to drop the user
28:59 - and we're also going to drop the
29:01 - database
29:04 - and now once again we can run MySQL U
29:07 - root init.sql okay so back into our
29:12 - server.pi file
29:14 - so anyways at this point if resulted is
29:17 - greater than zero then that means the
29:18 - user exists within our database
29:22 - and if that's the case we're going to
29:24 - set the row that contains our user data
29:27 - to
29:28 - cursor.fetch one
29:30 - and this is basically going to resolve
29:32 - to a tuple which is going to contain our
29:35 - email so we'll do user row 0 that's
29:39 - going to be the email and our password
29:41 - which is going to be
29:44 - user Row one
29:46 - and next we just want to check to see if
29:49 - the username and the password returned
29:52 - in the row is equal to the credentials
29:54 - passed in the request and if that's not
29:56 - the case we'll say that the credential
29:58 - is invalid and if it is the case then
30:00 - we're going to return a Json web token
30:03 - so we'll say if auth.username not equal
30:06 - to email or auth.password because we
30:11 - need both of them to be equal to
30:14 - what we get from the database not equal
30:16 - to password then if that's the case
30:19 - we're going to return invalid
30:21 - credentials and we're going to return a
30:24 - 401 status code
30:26 - else will return and we're going to
30:29 - create this function we haven't created
30:31 - it yet but we'll return the results of
30:33 - the function called create JWT and in
30:37 - this function we'll pass the auth
30:39 - username and we'll need to pass in a
30:42 - secret for the JWT so we'll just have
30:45 - that in our environment
30:48 - and we'll just call it JWT Secret
30:52 - and we're going to pass in true and I'll
30:55 - get to what this means in a second but
30:57 - we're not creating this create JWT
30:59 - function yet so just bear with me for a
31:02 - bit and lastly we get to
31:07 - if result is not greater than zero so if
31:10 - result's not greater than zero then that
31:12 - means the user doesn't exist in our
31:14 - database and if the user doesn't exist
31:16 - in our database then that means the user
31:18 - doesn't have access so we'll just return
31:21 - invalid credentials as well for this one
31:25 - and a 401.
31:33 - and that is going to be it for our login
31:37 - route and the login function so now we
31:40 - want to go ahead and create this create
31:43 - JWT method or create JWT function
31:46 - actually so I'm going to go ahead and
31:49 - explain a little bit about what a JWT
31:52 - actually is first so we're going to go
31:54 - over the overall flow using basic
31:56 - authentication and jwts and I will try
32:00 - to clear up any compiled confusion that
32:02 - you might have up to this point in the
32:04 - tutorial so let's visualize the flow
32:07 - from a top-down perspective to get an
32:09 - overall understanding of what our code
32:12 - is doing so as mentioned before our
32:15 - micro services are going to be running
32:17 - in a kubernetes cluster and that
32:19 - clusters internal network is not going
32:21 - to be accessible to or from the outside
32:24 - world or the open internet our client is
32:27 - going to be making requests from outside
32:30 - of the cluster with the intention of
32:32 - making use of our distributed system
32:34 - deployed within our private kubernetes
32:36 - cluster via our systems Gateway so our
32:39 - Gateway service is going to be the entry
32:42 - point to the overall application and the
32:44 - Gateway service is going to be the
32:46 - service that receives requests from the
32:48 - client and it is also going to be the
32:50 - service that communicates with the
32:52 - necessary internal services to fulfill
32:55 - the requests received from the client
32:57 - our Gateway is also going to be where we
33:00 - Define the functionality of our overall
33:02 - application for example if we want to
33:05 - add functionality to upload a file we
33:08 - need to Define an upload endpoint in our
33:10 - Gateway service source that initiates
33:13 - all of the necessary internal services
33:15 - to make that happen so if our internal
33:18 - Services live within a private Network
33:20 - how do we determine when we should allow
33:23 - requests in from the open internet this
33:25 - is where our auth service comes in we
33:28 - can give clients access to our
33:30 - application by creating credentials for
33:32 - them with in our office database any
33:35 - user password combination that exists
33:37 - within our MySQL DBS user table is a
33:41 - user password combination that will be
33:43 - granted access to our application's
33:45 - endpoints this is where the
33:47 - authentication scheme called basic
33:49 - authentication or basic access
33:51 - authentication comes in this
33:53 - authentication scheme requires the
33:55 - client to provide a username and
33:58 - password in their request which should
33:59 - be contained within a header field of
34:02 - the form authorization basic credentials
34:05 - where credentials is the base64 encoding
34:08 - of the username and password joined by a
34:11 - single colon in the context of our off
34:14 - flow we are going to make use of this
34:16 - authentication scheme by taking the
34:18 - username and password from the
34:20 - authorization header when a client sends
34:22 - a request to our login endpoint and
34:25 - comparing them to what we have in our
34:27 - mysqldb if we find a match for the
34:30 - credentials we know that the user has
34:32 - access so we will return a Json web
34:34 - token to the client which the client
34:36 - will use for subsequent requests to our
34:39 - gateways upload and download endpoints
34:41 - which brings us to the other critical
34:44 - part of our off flow Json web tokens so
34:47 - what are Json web tokens a Json web
34:50 - token is basically just two Json
34:52 - formatted strings and a signature which
34:55 - comprise three parts each part being
34:58 - base64 encoded all three parts are
35:00 - merged together separated by a single
35:03 - dot which is how we end up with
35:05 - something that looks like this but let's
35:07 - break this down so what are these three
35:09 - parts well the first part is the header
35:12 - the header contains a key value pair for
35:14 - both the signing algorithm and the type
35:16 - of token which is of course JWT the
35:19 - signing algorithm is the algorithm that
35:22 - was used to sign the token which will
35:24 - allow us to later verify that the sender
35:26 - of the token is who it says it is and to
35:29 - ensure that the message wasn't changed
35:31 - along the way now there are both
35:33 - asymmetric signing algorithms with two
35:35 - keys a public and private key and there
35:38 - are symmetric signing algorithms which
35:40 - use just one private key we aren't going
35:42 - to go into detail about signing
35:44 - algorithms because it is not within the
35:46 - scope of this course but what you do
35:48 - need to know is that our auth service is
35:50 - going to be using the symmetric signing
35:52 - algorithm
35:53 - hs-256 for example our auth service is
35:57 - going to be the only entity that knows
35:59 - our single private key and when a user
36:02 - logs in using basic auth or auth service
36:04 - will create a JWT or Json web token and
36:08 - sign it using that private key that JWT
36:11 - will then be returned to the user or the
36:14 - client that way when the user makes
36:16 - following requests to our API it will
36:19 - send its JWT in the requests and our
36:22 - auth service can validate the token
36:24 - using the single private key if the
36:26 - token has been tampered with in any way
36:28 - or was signed using another key then our
36:31 - auth service will know that the token is
36:33 - invalid it's important that we know that
36:36 - the Json formatted data in the token
36:38 - hasn't been tampered with because that
36:40 - data is going to contain the access
36:42 - permissions for the user so without this
36:44 - signing algorithm if the client were to
36:46 - alter the Json data and upgrade its
36:49 - permissions to increase its permissions
36:51 - allowing itself access to resources that
36:53 - shouldn't be available to that
36:55 - particular user then at that point our
36:57 - entire system would be compromised this
37:00 - brings me to the next part of our JWT
37:03 - the payload the payload contains the
37:05 - claims for the user or the bearer of the
37:08 - token what are claims you ask or maybe
37:10 - you didn't ask but I'll explain it
37:12 - anyway simply put claims are just pieces
37:15 - of information about the user for the
37:17 - most part these claims are defined by us
37:19 - although there are predefined claims as
37:22 - well for things like the issuer of the
37:24 - token the expiration of the token Etc
37:26 - the claims that we're going to Define on
37:28 - our own are who the user is for example
37:31 - the username and whether or not the user
37:34 - has admin privileges which in our case
37:36 - is just going to be true or false now
37:38 - the last part of the token is the
37:40 - signature the signature is created by
37:43 - taking the base64 encoded header the
37:46 - encoded payload and our private key and
37:49 - signing them using the signing algorithm
37:51 - which would in our case be hs256 at the
37:55 - end of all this we are left with a token
37:57 - that looks like this and now whenever
38:00 - the client makes requests to our API and
38:02 - provides this token within the request
38:04 - we can determine if the client's token
38:06 - was indeed signed with our private key
38:08 - and our signing algorithm and if so we
38:11 - can determine the client's access level
38:13 - by checking the claims in the payload
38:15 - portion of the token so in our case
38:17 - we're simply going to allow the client
38:19 - access to all of our endpoints if a
38:22 - claim that we're going to call admin has
38:24 - a value of true when we decode the
38:26 - payload portion of the token and that's
38:28 - going to be our off flow so we're going
38:31 - to Define and create JWT and it's going
38:35 - to take in a username a secret
38:38 - and auth's
38:42 - and auth is just going to tell us
38:44 - whether or not the user is an
38:46 - administrator so we're going to keep the
38:48 - permissions simple we're going to either
38:50 - have true or false either the user is an
38:53 - administrator or the user isn't an
38:55 - administrator
38:57 - and this is just going to return JWT dot
39:00 - encode now this JWT comes from up here
39:04 - we're importing this JWT here and it's
39:07 - from this Pi JWT module
39:11 - So within this in code we're going to
39:13 - need to pass a dictionary containing our
39:16 - claims a secret and an algorithm so
39:19 - we'll start with the dictionary
39:22 - it's going to contain username and the
39:24 - username is going to be the username
39:26 - that we pass into the function and it's
39:28 - going to have expiration and expiration
39:31 - is going to be date time dot date time
39:35 - dot now and time zone is going to be
39:39 - equal to date time Dot
39:42 - timezone.utc
39:45 - and we'll just continue this on the next
39:47 - line and we're going to add that to date
39:49 - time dot time Delta
39:53 - days
39:54 - equals one
39:56 - this is just going to set the expiration
39:58 - of this token to one day so this token
40:01 - is going to expire in 24 hours
40:05 - and this IAT is just issued at so like
40:08 - this is when the token is issued so
40:10 - we're just going to do date time
40:12 - date time
40:15 - UTC now
40:17 - and actually we could have done the same
40:19 - thing above for it now but we'll just
40:21 - leave it
40:22 - and then we're going to have whether or
40:23 - not this user is an administrator
40:27 - and that's just going to be
40:30 - the bull that we pass here for alts
40:34 - and next we need to pass in the secret
40:36 - and we also need to pass in the
40:39 - algorithm and this is basically just the
40:42 - signing algorithm for our JWT
40:46 - and we'll just use hs256
40:50 - which I believe is the default but a
40:53 - little verbosity never hurt anybody
41:01 - so actually
41:05 - so now we have our login route and our
41:09 - login function and we have the function
41:12 - to create the Json web token
41:17 - so essentially the flow is going to be a
41:20 - user is going to make a request to our
41:23 - login route using his or her credentials
41:26 - a username and a password and then we're
41:29 - going to check to
41:31 - see if that user data exists within our
41:34 - database if it does then we can consider
41:36 - the user to be authenticated and we'll
41:39 - return a Json web token which is going
41:42 - to be used by that user to make requests
41:45 - to the API and the endpoints that that
41:48 - user will have access to will be
41:50 - determined by this permissions here so
41:53 - whether or not the user is an admin
41:55 - we're going to keep it simple if the
41:57 - user is an admin will make the user have
41:59 - access to all the endpoints and yeah
42:01 - that's going to be the flow for logging
42:04 - in so we also need to create an endpoint
42:07 - for this auth service to actually
42:09 - validate the jwts
42:15 - so the way that we're going to do it is
42:17 - we're going to basically we're using
42:20 - this secret when we create the JWT and
42:23 - that same secret is going to be used to
42:25 - actually decode the token as well so
42:28 - that's how we know that this is a valid
42:30 - token for our API okay so while we're
42:33 - down here let's just go ahead and
42:36 - configure our entry point so we'll just
42:39 - do if name equals Main and all this
42:44 - means is basically when we run this file
42:48 - using the python command then this name
42:51 - variable will resolve to Main
42:54 - so let's just leave here and clear this
42:56 - so if we do Python 3 server.pi whenever
43:00 - we run this file this way the name
43:01 - variable resolves to Main
43:05 - so if we go back in here
43:07 - we can actually just print name
43:12 - and if we run this we'll see that the
43:15 - result is Main
43:17 - so that's all this is for so whenever we
43:20 - run our file using the python command we
43:23 - want our server to start so we'll do
43:25 - server run and we want our server to run
43:28 - on Port 5000 and we want to configure
43:32 - the host parameter like so which
43:34 - essentially is going to allow our
43:37 - application to listen to any IP address
43:41 - on our host
43:45 - so essentially if we don't set this host
43:48 - parameter like so the default is going
43:50 - to be localhost which means that our API
43:53 - wouldn't be available externally
43:55 - and as you can see here in this flask
43:58 - documentation configuring our host
44:01 - parameter this way tells our operating
44:04 - system to listen on all public IPS
44:06 - otherwise the server is only accessible
44:09 - from our own computer or from localhost
44:12 - so let me try to explain what I mean by
44:15 - that so basically any server is going to
44:18 - need an IP address to allow access from
44:21 - outside of the server so in our case our
44:24 - server would be a Docker container and
44:27 - our application will be running within
44:29 - that container when we spin up our
44:31 - Docker container it will be given its
44:34 - own IP address we can then use that IP
44:37 - address to send requests to our Docker
44:40 - container which in this case is our
44:42 - server and keep in mind when I'm
44:44 - referring to an IP address assigned to a
44:47 - Docker container I'm referring to the IP
44:49 - address assigned to that container
44:51 - within a Docker Network so when we spin
44:54 - up our Docker container it will be given
44:56 - its own IP address and we can use that
44:59 - IP address to send requests to our
45:01 - Docker container which in this case is
45:04 - our server but that alone isn't enough
45:06 - to enable our flask application to
45:09 - receive those requests we need to tell
45:11 - our flask application to listen on our
45:14 - container's IP address so that when
45:16 - request gets into our containers IP our
45:19 - application can receive those requests
45:22 - so this is where the host config comes
45:24 - in the host is the server that is
45:27 - hosting our application in our case the
45:30 - server that is hosting our flask
45:31 - application is the docker container that
45:34 - it is running in so we need to tell our
45:36 - flask app to listen on our Docker
45:39 - containers IP address but a Docker
45:42 - container's IP address is subject to
45:44 - change so instead of setting it to the
45:46 - static IP address of our Docker
45:48 - container we set it to this
45:50 - 0.0.0.0 IP address which is kind of like
45:54 - a wild card that tells our our flask app
45:56 - to listen on any and all of our Docker
45:59 - containers IP addresses that it can find
46:01 - if we don't configure this it will
46:03 - default to just localhost which is the
46:06 - loopback address and localhost is only
46:09 - accessible from within the host
46:10 - therefore outside requests sent to our
46:13 - Docker container would never actually
46:15 - make it to our flask app because the
46:18 - loopback address isn't publicly
46:19 - accessible so when we set host to
46:23 - 0.0.0.0 we are telling our flask app to
46:26 - listen on all of our Docker containers
46:28 - IPS including the loopback address or
46:31 - localhost and any other IP address
46:33 - available on the docker container for
46:35 - example if we connect our Docker
46:37 - container to two separate Docker
46:39 - networks Docker will assign a different
46:41 - IP address to our container for each
46:44 - Docker Network that means that with the
46:47 - 0.0.0.0 host configuration our flask app
46:51 - will listen to requests coming to both
46:53 - of the IP addresses assigned to the
46:55 - container here it's also possible to set
46:58 - the host config to a specific IP address
47:00 - that way our flask app will only listen
47:03 - to requests going to that IP address and
47:05 - would no longer listen to requests going
47:07 - to localhost or the other IP address
47:10 - from the other Docker Network
47:12 - so now that we've finished creating our
47:14 - login route we want to actually create
47:17 - another route to validate jwts
47:21 - and this route is going to be used by
47:24 - our API Gateway to validate jwt's synth
47:28 - within request from the client to both
47:31 - upload and receive or download MP3s or
47:36 - to upload videos and download the MP3
47:38 - version of those videos and you'll see
47:40 - what I mean by that later on in the
47:42 - tutorial when we actually start to
47:44 - implement that so we're going to do
47:47 - another route and this one's going to be
47:50 - validate
47:51 - and methods are going to be
47:54 - post
47:56 - and we're going to define a function
47:58 - called validate
47:59 - and we're going to want to pull the
48:01 - encoded JWT from our request and we're
48:05 - going to require the JWT to be in a
48:08 - headers or a header called authorization
48:13 - and if the JWT is not present in the
48:16 - authorization header we want to return
48:18 - an error
48:20 - so we'll say encoded JWT if not encoded
48:23 - JWT we'll just return missing
48:27 - credentials
48:29 - and a 401
48:31 - so if you remember from the explanation
48:34 - of the basic authentication scheme for
48:37 - that scheme we would need to have the
48:39 - word basic in our authorization header
48:41 - that contained our base64 encoded
48:44 - username and password separated by a
48:46 - colon well for our JWT we instead need
48:50 - to have the word Bearer in the
48:52 - authorization header that includes the
48:54 - token so let me quickly go over the
48:56 - format for the authorization header so
48:59 - that you understand what's happening so
49:01 - if we look at the documentation for
49:02 - authorization headers on mozilla.org we
49:06 - see that the format for the header is
49:08 - first type and then credentials here
49:11 - type represents the authentication
49:13 - scheme and credentials represents the
49:16 - credentials necessary specific to that
49:18 - type so from the perspective of the
49:20 - server handling the authorization header
49:23 - the type tells us what type of
49:25 - credential is contained within the
49:27 - header so if the type is basic from the
49:30 - server perspective we know that we are
49:32 - dealing with a credential which is a
49:34 - base64 encoded username and password
49:36 - separated by a colon if the type is
49:40 - Bearer we know that we are dealing with
49:42 - a bearer token which essentially means
49:44 - that we can assume the party in
49:46 - possession of the token or the bearer of
49:48 - the token has access to the tokens
49:51 - Associated resources now in the code for
49:54 - our validation endpoint to save some
49:57 - time we're just going to assume the
49:59 - authorization header contains a bearer
50:01 - token therefore we aren't going to check
50:03 - or validate the word that represents the
50:06 - type that comes before the credential in
50:08 - the header but in an actual production
50:10 - environment you are definitely going to
50:12 - want to spend the extra time to check
50:13 - the type or the authentication scheme
50:16 - present within the authorization header
50:19 - and within this authorization header
50:21 - we're going to require the token to be
50:23 - formatted as bear authentication so
50:26 - basically we're going to want the header
50:28 - that's synt with the JWT to look like
50:30 - this so it's going to have this Bearer
50:33 - as part of the string and then the token
50:35 - and as a result of that if the encoded
50:39 - JWT is present we're going to need to
50:42 - split the string so we're going to set
50:44 - encoded JWT equal to encoded JWT dot
50:48 - split and we're going to need to split
50:51 - it based on a space because there's
50:54 - going to be the word bear
50:56 - and then a space and then there's going
50:58 - to be the token so the array that
51:00 - results from this split is going to have
51:02 - the item with the word bear and it's
51:04 - going to have an element with the token
51:06 - so we're going to need the first index
51:09 - or the item at the first index of the
51:12 - array not the zero
51:13 - and then we're going to try and we're
51:17 - going to do decoded equals JWT dot
51:21 - decode
51:24 - and to this decode method we're going to
51:27 - need to pass the encoded JWT and we also
51:30 - need to pass our JWT secret the one that
51:33 - was used when we actually encoded the
51:36 - JWT which is going to be in an
51:38 - environment variable
51:43 - and we're also going to need the
51:45 - algorithm and the algorithm that we used
51:48 - was hs256
51:52 - and if that fails
51:54 - we're just going to return not
51:57 - authorized
51:59 - and a 403. but if it doesn't fail we
52:02 - will return the decoded token
52:06 - and a 200. and that's pretty much going
52:10 - to be it for our auth service so we can
52:13 - just
52:14 - go ahead and save this
52:23 - so now we have our actual service and we
52:27 - have our init script for our database
52:29 - and now we're going to need to start
52:31 - writing all of our infrastructure code
52:33 - for the actual deployment so we're
52:35 - basically going to deploy all of our
52:37 - services within a kubernetes cluster as
52:40 - you already know so we need to create
52:42 - Docker images that we're going to push
52:45 - to a repository and our kubernetes
52:48 - configuration is going to pull from the
52:51 - repositories for our Docker images and
52:54 - create our deployments within our
52:55 - cluster and I know that this sounds kind
52:57 - of confusing but we're going to walk
52:59 - through everything step by step so don't
53:01 - worry so we're going to start by making
53:03 - a Docker file and as our base image we
53:06 - want to use a python image so we'll use
53:08 - this python 310 slim bullseye
53:15 - and after we write out this Docker file
53:17 - I'll go over what all the lines mean in
53:20 - a little bit more detail but for now I'm
53:22 - just going to vaguely go over what we're
53:23 - doing so first we're going to run our
53:27 - apt-get update
53:30 - and we also need to apt-kit install a
53:33 - couple of dependencies
53:47 - so we're going to need build Essentials
53:50 - and default
53:52 - live MySQL
53:55 - client Dev
53:57 - and then we want to pip install upgrade
54:01 - pip
54:07 - and following that we want to set our
54:10 - working directory to app and we want to
54:13 - copy first just our requirements dot txt
54:17 - which we haven't created yet
54:21 - and the reason we're copying this
54:23 - separately from the rest of the
54:25 - application is because we want to make
54:27 - sure our requirements are in a separate
54:30 - layer so that if our application code
54:32 - changes we can still use the cached
54:35 - requirements layer we don't need to
54:36 - reinstall or recreate the layer and I'll
54:39 - probably explain that in a little bit
54:41 - more detail later on
54:43 - so then we want to run pip install our
54:46 - requirements
54:56 - and then we can copy over the rest of
54:58 - our application
55:00 - and our app is going to be running on
55:03 - Port 5000 so we'll expose that port and
55:06 - finally we need to create the command
55:08 - which is going to be python3
55:12 - server.pi
55:14 - so it's the same as when we're actually
55:17 - running this python3 server.pi from the
55:19 - command line
55:21 - and that is it for that so we'll save
55:24 - that now let's get into explaining the
55:26 - contents of our Docker file so basically
55:29 - when we build a Docker image we're
55:32 - building it on top of a base image which
55:34 - in our case is this base image python
55:37 - 310 slim Bullseye so we can think of an
55:40 - image as a file system snapshot for
55:43 - example the base image that we are
55:45 - building our image on top of is
55:47 - essentially a snapshot of a file system
55:49 - that contains all of the necessary
55:51 - dependencies to run python applications
55:54 - for instance we wouldn't be able to run
55:56 - a DOT Pi file on an OS that doesn't have
55:59 - python installed right so a base python
56:02 - image will have things pre-installed so
56:04 - that we don't need to worry about that
56:06 - so based on that understanding let's go
56:09 - a little deeper it's important to keep
56:11 - in mind when writing Docker files that
56:13 - each instruction in a Docker file
56:15 - results in a single New Image layer
56:17 - being created that means that the next
56:19 - instructions image layer will be built
56:21 - on top of the previous instructions
56:24 - image layer so that means that this from
56:26 - instruction creates a layer and then
56:28 - this run instruction creates a new layer
56:31 - on top of the previous layer that was
56:33 - built from the from instruction and this
56:35 - continues on until we reach the end of
56:38 - our Docker file this is important to
56:40 - understand because if we need to build
56:42 - our Docker image again Docker is smart
56:45 - enough to use cached image layers if
56:48 - nothing within the layer has changed and
56:50 - none of its preceding layers have
56:51 - changed so what do I mean by that so
56:54 - let's say that the dependencies for our
56:56 - application change resulting in our
56:58 - requirements.txt file changing when we
57:01 - rebuild the image we won't need to
57:03 - rebuild every layer again we only need
57:06 - to rebuild the layer that changes and
57:08 - every layer after it because of course
57:10 - every layer after it is based on its
57:13 - preceding layer therefore if the
57:14 - preceding layer changes so does it and
57:17 - the reason it's important to understand
57:18 - this is because optimizing your Docker
57:21 - file to use cached layers efficiently
57:23 - will significantly decrease the build
57:25 - time of your image and that might not
57:27 - seem so beneficial in this context but
57:30 - when we are talking about deploying
57:32 - production applications using CI CD
57:34 - pipelines the build speed is something
57:36 - that we want to consider now if you
57:38 - don't know what a CI CD pipeline is
57:40 - don't worry it's not necessary to
57:42 - understand that for this tutorial
57:43 - anyways with the understanding that each
57:46 - instruction results in its own layer and
57:49 - that if one layer changes every layer
57:50 - after that layer will also need to be
57:52 - rebuilt we can see why it's beneficial
57:54 - to separate the copy instructions for
57:57 - our requirements.txt from the copy
58:00 - instructions of the rest of our
58:01 - application because with this
58:03 - configuration as you can see if the
58:06 - dependencies for our application change
58:08 - resulting in our requirements.txt file
58:11 - changing we need to create a new layer
58:14 - to build onto with the new requirements
58:16 - being installed in this run pip install
58:19 - layer and of course every layer after
58:21 - that layer will need to be rebuilt as
58:23 - well but if we only make a code change
58:26 - and our requirements don't change we
58:28 - don't want to have to build the layer
58:30 - that installs our dependencies again
58:32 - because this is probably one of the most
58:34 - time consuming layers to build so as you
58:37 - can see we are copying the rest of our
58:39 - application to the app directory here
58:41 - and by the way this dot just means the
58:43 - current directory that we ran the docker
58:46 - builds command in so if we copy dot to
58:49 - our app directory which is our working
58:51 - directory or copying everything in the
58:54 - directory where we ran the docker build
58:56 - command in on our local machine in other
58:59 - words the directory that contains our
59:01 - Docker file on our local machine and if
59:04 - any of the code has changed since our
59:06 - source files are contained within that
59:08 - directory Docker will detect that there
59:11 - was a change and rebuild this layer and
59:13 - every layer following this layer will
59:14 - need to be rebuilt as well but as you
59:16 - can see the layers following this layer
59:18 - don't include the time consuming pip
59:20 - install command so that's just a quick
59:23 - example of why optimizing your Docker
59:25 - file to be more layer efficient is
59:27 - beneficial so now back to the rest of
59:29 - our instructions so after we build our
59:32 - base layer using the python 310 slim
59:35 - Bullseye image we then move on to
59:38 - installing our OS dependencies and all
59:41 - of these flags in purple here are to
59:43 - avoid installing unnecessary additional
59:45 - packages as well as avoiding taking up
59:48 - additional space with caching and stuff
59:50 - like that because we want our container
59:52 - to be as light as possible and we also
59:54 - don't want to introduce potential
59:56 - vulnerabilities present on packages that
59:58 - we don't even need and the reason we are
60:01 - combining all of these commands into one
60:03 - command is so that we can keep them all
60:05 - in the same run instruction therefore
60:07 - keeping them contained to one image
60:09 - layer because remember every Docker
60:12 - instruction creates a new image layer
60:13 - and then here we're just creating a
60:16 - directory to work in and this dirt is
60:19 - where our application source is going to
60:21 - live and these instructions we've
60:23 - already gone over and this expose
60:26 - instruction doesn't actually do much of
60:28 - anything other than serve as
60:30 - documentation to anybody that builds
60:32 - this image and it essentially lets them
60:34 - know what port is intended to be
60:36 - published so our app listens on Port
60:39 - 5000 so that is the port that we have
60:41 - here in the expose instruction and
60:44 - lastly we have our Command instruction
60:46 - and this instruction is the instruction
60:48 - that is going to be used to run our
60:50 - container this instruction sets the
60:53 - command to be executed when running the
60:55 - image so for example when we run our
60:57 - image the Python 3 command will be run
61:00 - on our server.pi file which is going to
61:03 - run our auth server in this case
61:05 - okay now let's go ahead and build this
61:08 - Docker file
61:15 - oh and I forgot to create our
61:18 - requirements.txt so we're going to do uh
61:21 - pip 3 freeze and we're going to freeze
61:23 - our requirements the current
61:25 - requirements for our application into a
61:27 - file called requirements.txt and if we
61:30 - go into this file you see it has all of
61:33 - the requirements that we needed to
61:35 - install for our application like it has
61:37 - this MySQL DB that we're using and of
61:39 - course flask and any of their
61:42 - dependencies as well
61:44 - so doing pip 3 freeze it basically
61:47 - freezes our dependencies into a file so
61:50 - that we know what dependencies we need
61:52 - to install to run this application
61:54 - so let's go ahead and try this again
62:00 - and now it says unable to locate package
62:03 - build Essentials so let's go back into
62:06 - our Docker file and that is a typo it's
62:09 - actually just build essential
62:12 - and let's try again
62:21 - foreign and now our image is finished
62:25 - being built so once we've finished
62:27 - building our image we actually want to
62:29 - create a Docker registry or a repository
62:32 - so you can just go to hub.docker.com
62:39 - and create an account here I already
62:42 - have an account so I will just sign in
62:44 - okay so once you've successfully created
62:47 - an account and logged into your Docker
62:49 - Hub account you should end up at a page
62:51 - that looks like this and from here you
62:53 - just want to click this repositories Tab
62:56 - and what we're going to do is we're
62:58 - going to create the repositories that
63:00 - we're going to push our container images
63:02 - to and then our kubernetes configuration
63:06 - is going to pull from this repository
63:08 - the repositories that we create for each
63:11 - individual service now your repositories
63:13 - are going to have the same suffix as
63:16 - mine because we're going to create the
63:19 - repository name using the same suffix
63:21 - but the prefix to your repository is
63:23 - going to be different mine's going to
63:25 - have this prefix and yours is going to
63:27 - be whatever the name of your account is
63:30 - so for example if we create a repository
63:33 - here
63:34 - and we name the repository auth because
63:37 - it's going to be the repository for our
63:39 - auth services images when we actually
63:41 - push to this repository from our command
63:44 - line I'm going to push to sweezytech
63:47 - auth and you're going to push to
63:48 - whatever your username is for your
63:51 - account and you'll see what I mean by
63:53 - that in a second so we're going to go
63:54 - ahead and create this repository for our
63:56 - auth services images and we're just
63:58 - going to make it public because we only
64:00 - can make one private one and then if we
64:02 - do a private one we're going to have to
64:04 - configure credentials within minicube
64:06 - which is a little bit more complicated
64:08 - so we're just going to do public and
64:10 - throughout this tutorial it will be
64:12 - possible for you to push and pull from
64:15 - my repository which can cause lots of
64:17 - issues for you as you follow along with
64:20 - this tutorial because my images may or
64:22 - may not be in the same state that they
64:24 - are at the part of the tutorial that
64:26 - you're on so just make sure you take the
64:29 - time to create this account and create
64:30 - your own repositories so we can just go
64:33 - ahead and create this and now as you can
64:35 - see here it tells you how to push to
64:37 - this repository and as you can see this
64:40 - is going to look different for you than
64:42 - it does for me specifically this part is
64:45 - going to look different for you
64:48 - so anyways now what we want to do is we
64:51 - want to tag the image that we just
64:53 - created here we just built an image
64:55 - using this Docker builds command it
64:57 - built the image based on our Docker file
64:59 - that we just created now we want to tag
65:02 - this image so we'll do Docker tag and
65:05 - we're going to just use this Shaw here
65:11 - you don't actually need to put in the
65:13 - whole thing but I'm just going to put in
65:14 - the whole thing and you're just going to
65:16 - tag it using your username from your
65:18 - Docker Hub account
65:19 - slash auth and then we're going to tag
65:22 - it as latest because it's going to be
65:24 - the most recent version of our image
65:27 - now if we do Docker image LS you can see
65:31 - that we have our tag here and you can
65:34 - compare this part of the image ID to the
65:36 - first part of this shot and just ignore
65:39 - all of these other images that I have
65:41 - you'll likely have whatever images you
65:44 - have on your system as well but none of
65:46 - that matters as long as you have this
65:48 - image tag here then you're fine so let's
65:51 - just go ahead and clear that and now
65:52 - that we've tagged it we can push it to
65:55 - our repository so we'll just do Docker
65:57 - push and again you're just going to use
66:01 - your username for your Docker Hub
66:03 - account and then auth and then latest
66:05 - and then just push that
66:12 - and once that's finished you can go to
66:15 - the repository and you can just refresh
66:18 - the page
66:21 - and you should see your image tag here
66:24 - so that means we've successfully pushed
66:27 - this image to our repository and now
66:30 - whenever we want to pull this image we
66:32 - could just do Docker pool and we could
66:34 - just do
66:38 - the name of our image and the tag or the
66:41 - URL for our image in the tag
66:43 - and we'd be able to pull it but that's
66:46 - not actually how we're going to be
66:48 - pulling these images our kubernetes
66:50 - configuration is actually going to be
66:51 - pulling the images so let's go ahead and
66:53 - clear
66:54 - and now we're going to make a directory
66:57 - called Manifest this directory is going
66:59 - to contain all of our kubernetes
67:01 - configurations so let's change directory
67:04 - to manifest
67:06 - and for all of these configuration files
67:09 - I'm going to go over them in detail
67:11 - after we write them out so if you're
67:13 - confused about the infrastructure code
67:15 - that we're writing just hang in there so
67:18 - all of our configuration files are going
67:19 - to be yaml files and we'll start with a
67:22 - file called off deploy.yaml and within
67:26 - this file we'll do API version apps V1
67:31 - foreign
67:32 - is deployment now again this is the
67:36 - configuration for our kubernetes cluster
67:39 - and our service and if you're not
67:40 - familiar with kubernetes a lot of these
67:42 - configurations you probably won't
67:44 - understand I will try to go into more
67:47 - detail for these in a little bit so
67:50 - we'll do metadata name of our service or
67:54 - our deployment is going to be off
67:56 - and labels
67:58 - app is going to be auth as well
68:02 - and then we're going to do our spec
68:04 - in replicas we're going to want two
68:07 - replicas or two instances of our service
68:10 - and selector we're going to match labels
68:14 - and the app will be off
68:17 - strategy
68:19 - type
68:20 - rolling update and the configuration for
68:24 - Rolling update
68:25 - we're going to do Max surge equals three
68:29 - then we want to do template
68:32 - metadata for template labels
68:36 - app off
68:39 - and we'll do spec again
68:42 - containers
68:45 - name auth and this is where we're going
68:48 - to configure it to pool our image we're
68:51 - going to set image to
68:54 - remember this should be your username
68:56 - and then auth
68:58 - and then ports
69:00 - the container port
69:03 - is going to be 5000 because our
69:06 - application is running on Port 5000 so
69:08 - we'll just do the same for container
69:10 - Port as well and then we want to get our
69:12 - environment variables from config map
69:15 - file we're going to create this file
69:17 - after this so we're going to do config
69:19 - map ref and we're going to name the
69:23 - config map that we're going to create
69:24 - auth config map and again we haven't
69:27 - created this yet we're going to create
69:28 - it soon and here we're going to do
69:31 - Secret ref and we're going to store our
69:34 - secrets in a secret we're going to name
69:37 - it auth secret
69:40 - and we're going to create this file as
69:42 - well
69:44 - and let's just check formatting and
69:47 - looks fine
69:50 - so we can just go ahead and save that so
69:52 - now let's create the config map so we'll
69:55 - just do Vim
69:57 - configmap.yaml this config map is going
69:59 - to set environment variables within our
70:02 - container so for this one we'll do API
70:04 - version
70:06 - B1 kind config map
70:10 - metadata name is going to be auth config
70:13 - map and data are going to be our
70:16 - environment variables so we'll do MySQL
70:19 - post and since we're going to be using
70:23 - our local MySQL server we're going to
70:26 - need to reference that server from
70:29 - within our kubernetes cluster and
70:31 - luckily minicube gives us a way to
70:33 - access our host the Clusters hosts via
70:36 - this host dot miniq dot internal because
70:40 - basically within the cluster we're kind
70:43 - of in our own isolated Network so since
70:47 - our MySQL server is just deployed to our
70:50 - local host from within the cluster we
70:52 - wouldn't be able to just use localhost
70:54 - we need to access the system that's
70:57 - hosting the cluster and that's what this
70:59 - host mini Cube internal is for
71:02 - so our MySQL user is going to be off
71:06 - user we created and our MySQL DB
71:10 - is going to be auth which we also
71:12 - created and default port for MySQL is
71:16 - 3306 and actually we should do it as a
71:20 - string so these are going to be the
71:22 - environment variables that will
71:24 - automatically be exported within our
71:26 - Shale when we do the deployment so in
71:29 - other words if we were to run the
71:31 - environment command within that
71:33 - container all of these variables and
71:36 - their values are going to be present
71:37 - within the container
71:39 - so that's what this config map file is
71:41 - for and configmap is for environment
71:44 - variables that aren't necessarily
71:46 - sensitive data like passwords so we're
71:48 - also going to need to do a similar file
71:50 - for our secrets or sensitive data like
71:53 - our password to our database and of
71:55 - course in a production environment you
71:57 - would never push your secrets
72:00 - configuration to like a git repository
72:03 - or something because then your passwords
72:05 - would be easily visible at the
72:07 - repository so just keep that in mind
72:09 - when we're creating this file so we're
72:11 - just going to call it secret.yaml
72:14 - and API version once again V1 and this
72:18 - time kind is going to be secret and
72:21 - metadata we're going to do name auth
72:24 - Secret
72:25 - and we're going to do string data
72:28 - and we'll do MySQL password is going to
72:31 - be auth123 and our JWT secret is another
72:36 - secret that we need for this application
72:38 - and we're just going to make it a random
72:40 - name sarcasm
72:42 - and we need to set type to opaque
72:47 - and this is going to be our environment
72:50 - variables for our secrets and we can
72:53 - just go ahead and check if the
72:55 - formatting is okay and save that and
72:57 - lastly we need to create our
73:00 - service.yaml and we'll do API version
73:04 - V1 and kind is going to be service
73:09 - metadata will be name is auth that's
73:13 - going to be the name of the overall
73:14 - service and spec
73:17 - we'll do selector
73:20 - app off and we'll do type
73:24 - cluster IP and this cluster IP basically
73:27 - just means that the IP address assigned
73:30 - to this service is only going to be
73:32 - accessible within our cluster but again
73:35 - I'll go into a little bit more details
73:36 - soon
73:38 - and our Port is going to be 5000
73:42 - our Target Port 5000 as well
73:45 - and protocol is going to be TCP
73:51 - and then we can just save that so once
73:54 - we have all of our info code for our
73:57 - kubernetes deployment we can actually
73:59 - start to deploy this off service to our
74:02 - cluster
74:03 - so let's go ahead and take a look at
74:05 - canines So currently we have nothing in
74:08 - canines there's no cluster and there's
74:10 - no context because our mini Cube isn't
74:13 - running right now
74:14 - so we can actually do
74:17 - any Cube start
74:23 - and once mini cube is started we should
74:25 - be able to go back into canines
74:28 - and if we change the namespace to All by
74:32 - hitting zero
74:34 - here
74:38 - we can see
74:40 - that we have our mini Cube pods running
74:43 - within the cube system namespace
74:47 - and you can see here our cluster is mini
74:49 - Cube so let's go ahead and Ctrl C out of
74:52 - there and we can clear this
74:55 - okay so let me just briefly go over what
74:57 - we're doing here so basically within
75:00 - this manifest directory we wrote the
75:02 - infrastructure code for our auth
75:05 - deployment so if we change directory
75:07 - back to our main directory we wrote the
75:10 - code for our auth service and we created
75:13 - a Docker file to build that source code
75:16 - into a Docker image and we then pushed
75:19 - that Docker image to a repository on the
75:22 - internet and within our manifest
75:25 - infrastructure code
75:30 - we're actually pulling that image from
75:33 - the internet and deploying it to
75:35 - kubernetes and that image contains our
75:38 - code
75:41 - so all of these files within this
75:43 - manifest directory when applied will
75:46 - interface with the kubernetes API which
75:49 - is the API for our kubernetes cluster to
75:52 - interface with our kubernetes cluster so
75:54 - these files are going to interface with
75:56 - that API to create our service and its
75:59 - corresponding resources like its config
76:01 - map and its secret and to do that all we
76:04 - need to do is do Cube CTL
76:07 - apply and then we're going to use this F
76:10 - flag for file and we're just going to
76:12 - apply all the files in the current
76:14 - directory this manifest directory
76:18 - and as you can see here our config map
76:21 - resource was created and our secret was
76:24 - created and our service was created but
76:26 - we actually had an error here for our
76:28 - deployment so it's saying that let's see
76:32 - a known field template
76:34 - so this seems like there's an issue with
76:37 - our auth deploy.yaml file let's go ahead
76:39 - and have a look
76:40 - so a template shouldn't be unknown
76:43 - so the spacing is really important in
76:47 - yaml files so we'll put this back one
76:53 - because strategy is actually part of
76:56 - spec
76:58 - but the way we had it before
77:01 - we had strategy a part of selector
77:04 - so we need to make sure that spacing is
77:06 - correct in these files
77:10 - so we'll go ahead and
77:13 - put this back
77:20 - and let's give it a try now
77:22 - so we can just apply again and it will
77:25 - only apply the files that have changed
77:29 - and now we're getting unknown field name
77:31 - so I'm assuming that it's an issue with
77:34 - the spacing again so let's just go over
77:36 - this
77:40 - so the issue here is that our config map
77:43 - reference the name shouldn't be at the
77:45 - same spacing it should be
77:48 - there and the same for the secret ref
77:51 - name
77:52 - so we can go ahead and save and let's
77:55 - try to apply again and now we were able
77:58 - to create our deployment as well so now
78:02 - that we've created these resources we
78:04 - can go into K9s
78:08 - and we can see that we have two
78:10 - instances of our auth service being
78:14 - created
78:16 - and now both instances are running and
78:20 - if we go into the logs by just pressing
78:22 - enter on first the Pod and then the
78:26 - container we can see the logs within the
78:28 - container and we see that our server is
78:30 - running
78:31 - and that's for both of these replicas
78:35 - and also within canines we can use the
78:38 - shell within the container by just
78:40 - entering on the Pod and then pressing s
78:43 - on the container to access the shell
78:46 - so now we're in a shell within our
78:49 - container and within this shell we can
78:51 - do environment and we can see our
78:54 - environment variables so you can see we
78:56 - have our secret here our MySQL password
78:59 - and we have our MySQL user here actually
79:03 - we can just environment grep MySQL and
79:07 - we can see all of our MySQL environment
79:09 - variables from both our config map and
79:12 - our secret
79:13 - and we can just exit the shell here and
79:16 - leave canines okay so to explain our
79:19 - kubernetes configuration I will need to
79:22 - explain a little bit more about
79:24 - kubernetes in general so throughout this
79:26 - course there's been a lot of mention of
79:28 - deploying our micro services to a
79:30 - kubernetes cluster but what does that
79:33 - actually mean let's first briefly go
79:35 - over what kubernetes is in simple terms
79:38 - kubernetes eliminates many of the manual
79:41 - processes involved in deploying and
79:43 - scaling containerized applications for
79:46 - example if we configure a service to
79:48 - have four pods kubernetes will keep
79:51 - track of how many pods are up and
79:52 - running and if any of the pods go down
79:55 - for any reason kubernetes will
79:57 - automatically scale the deployment so
79:59 - that the number of PODS matches the
80:00 - configured amount so there's no need to
80:03 - manually deploy individual pods when a
80:06 - pod crashes kubernetes also makes
80:08 - manually scaling pods more streamlined
80:11 - for example say I have a service that
80:13 - load balances requests to individual
80:16 - pods using round robin and that service
80:18 - is experiencing more traffic than the
80:20 - number of available pods can handle as a
80:23 - result of this I decide to scale my
80:25 - service up from two to five pods without
80:28 - kubernetes in a situation like this I'd
80:30 - likely need to go manually deploy each
80:33 - individual additional pod and then I'd
80:36 - need to reconfigure the load balancer to
80:38 - include the new pods in the round robin
80:40 - algorithm but kubernetes can handle all
80:43 - of this for you and it's as simple as
80:45 - running this command with this simple
80:47 - command kubernetes will scale up your
80:50 - service which includes maintaining the
80:52 - newly scaled number of PODS if a pod
80:54 - happens to crash and it will auto
80:56 - configure the load balancer to include
80:58 - the new pods basically with kubernetes
81:01 - weekend cluster together a bunch of
81:03 - containerized services and easily
81:05 - orchestrate the deployment and
81:07 - management of these services within the
81:09 - cluster using what we call kubernetes
81:12 - Objects which are persisted did entities
81:14 - in the kubernetes system I know that
81:17 - sounds a bit complicated so let me
81:18 - explain so for this part of the
81:20 - explanation let's go to the kubernetes
81:23 - documentation
81:24 - so it's explained here that a kubernetes
81:26 - object is a record of intent once you
81:29 - create the object the kubernetes system
81:32 - will constantly work to ensure that
81:34 - object exists by creating an object
81:37 - you're effectively telling the
81:38 - kubernetes system what you want your
81:40 - cluster's workload to look like this is
81:43 - your cluster's desired State now this
81:45 - sounds complicated but we've actually
81:47 - already done this multiple times for
81:50 - example we created a deployment object
81:52 - here in this yaml file this file is the
81:55 - above mentioned record of intent we are
81:58 - telling kubernetes that we want this
82:00 - deployment object to exist in our
82:02 - cluster in the state specified in our
82:05 - spec here for example we want two
82:08 - replicas to be deployed once this
82:10 - configuration is applied as explained
82:12 - here the kubernetes control plane
82:15 - continually and actively manages every
82:17 - object's actual state to match the
82:20 - desired State you supplied that means
82:22 - that kubernetes will keep track of the
82:25 - actual status or state of your
82:26 - deployment and make sure that it matches
82:29 - your record of intent in other words
82:31 - your yaml specification
82:33 - so bringing it all together we can say
82:35 - that our kubernetes cluster is comprised
82:38 - of a bunch of objects that we've
82:40 - configured that describe our cluster's
82:42 - intended state from there kubernetes
82:45 - will continually compare the current
82:47 - status or state of those objects to the
82:50 - specification or desired state from our
82:52 - original configuration and if that
82:55 - comparison ever differs kubernetes will
82:57 - automatically make adjustments to match
82:59 - the current status with our original
83:01 - record of intent in other words our
83:04 - original specification so how do we
83:06 - communicate with kubernetes to configure
83:09 - and or create these objects well let's
83:12 - once again take a look at the
83:13 - documentation it's explained here that
83:16 - to work with kubernetes objects whether
83:18 - to create modify or delete them you will
83:21 - need to use the kubernetes API when you
83:24 - use the cube CTL command line interface
83:27 - for example the CLI makes the necessary
83:30 - kubernetes API calls for you so
83:32 - basically the Q CTL CLI that we
83:35 - installed is interfacing with the
83:37 - kubernetes API to essentially run crud
83:40 - operations on our clusters objects in
83:43 - our case we are running our cluster
83:45 - locally using mini Cube so the end point
83:47 - for the kubernetes API in this case is
83:49 - on our local machine but in the real
83:52 - world your cluster will usually be
83:54 - deployed on some server and on your
83:56 - local machine you'll have a kubernetes
83:59 - configuration for the cluster on that
84:01 - server which will enable your local Cube
84:03 - CTL CLI to interface with the remote
84:06 - server but we don't need to go into the
84:09 - details for that in this video so now
84:11 - that we have a general understanding of
84:13 - what kubernetes is and how it is working
84:15 - we can now get into explaining our
84:17 - actual yaml configuration files so if we
84:20 - have a look at the documentation here we
84:23 - see that there are some required fields
84:25 - necessary when creating kubernetes
84:26 - objects using the dot yaml files those
84:29 - are API version kind metadata and spec
84:33 - we can also see a description for each
84:36 - field API version is which version of
84:38 - the kubernetes API we are using to
84:40 - create this object kind is what kind of
84:43 - object we want to create for example
84:46 - deployment config map secret
84:49 - Etc metadata is just data that helps
84:52 - uniquely identify the object and lastly
84:55 - spec is the desired state or record of
84:58 - intent for the object which we explained
85:00 - before as mentioned here spec format is
85:03 - different for every kubernetes object
85:05 - type for example the spec format for an
85:08 - object of kind deployment will be
85:11 - different from the spec format for an
85:13 - object of kind service and to see how to
85:16 - configure the spec for specific types of
85:19 - objects we can use the kubernetes API
85:21 - reference
85:22 - so let's go over the spec format for our
85:26 - deployment object configuration so first
85:29 - as you can see here we have all of the
85:31 - required fields we have API version
85:34 - which is the required field here we have
85:37 - kind which is the required field here
85:39 - and we have metadata which is the
85:42 - required field here and as mentioned in
85:44 - this dock the precise format of the
85:46 - object spec is different for every
85:49 - kubernetes object so anything within
85:51 - this spec block is our deployment spec
85:54 - so to see the actual format for the spec
85:57 - for a deployment we can just go to the
86:00 - kubernetes API reference here
86:04 - and a deployment is a workload resource
86:07 - so we would click this workload resource
86:10 - and we see we have deployment here so we
86:13 - can select deployment
86:15 - so this here is basically giving us the
86:18 - overall configuration for a deployment
86:20 - so it's showing the API version and the
86:23 - kind and the metadata as well and it
86:26 - also tells us the object metadata
86:29 - because as you can see here our metadata
86:31 - has its own nested fields
86:34 - so if we were to hit that object
86:36 - metadata we'd get a description for each
86:39 - field that we have within our metadata
86:41 - so name and it tells us that name must
86:44 - be unique within the namespace and that
86:47 - it's required when creating a resource
86:49 - and if we scroll down here we see that
86:52 - we have additional fields that we're not
86:54 - using currently but that we can use
86:56 - within our metadata block
86:59 - and then we have labels which we're
87:01 - using here and it tells us the format
87:05 - for the labels is it's a map of string
87:08 - keys and string values so for example
87:11 - this app would be the key and auth would
87:14 - be the value for our labels
87:17 - so let's go back and get into the spec
87:20 - configuration so as you can see here is
87:22 - spec and then there's a deployment spec
87:25 - link that we can select that will give
87:27 - us the details of the actual spec format
87:30 - for a deployment so we have our spec
87:32 - here and any of the fields nested within
87:36 - this spec block are going to be present
87:38 - here so we can get a detailed
87:40 - explanation of each individual field so
87:42 - for example the selector here is the
87:45 - selector that we have here and if we
87:48 - click label selector
87:50 - we can find this matched labels that
87:53 - we're using here down here
87:55 - and basically what match labels is doing
87:58 - it says here a label selector is a label
88:00 - query over a set of resources and to
88:03 - understand what I mean by that let's go
88:05 - back to what a selector is here so as
88:08 - you can see here it says label selector
88:10 - for pods existing replica sets whose
88:12 - pods are selected by this will be the
88:14 - ones affected by this deployment and it
88:16 - must match the Pod templates labels so
88:19 - as you can see within our template which
88:22 - we're going to get to for our replicas
88:24 - we're setting the label with key app and
88:28 - value auth so we're going to get into it
88:31 - but this template is basically going to
88:33 - be the configuration for each individual
88:35 - pod and our deployment is basically
88:39 - going to know what pods are part of the
88:42 - overall deployment because this selector
88:44 - is going to match the labels that are
88:47 - assigned to each individual pod in our
88:50 - template here
88:52 - so simply put our deployment knows what
88:55 - pods are part of the deployment because
88:57 - based on this template each pod is going
89:00 - to be deployed with a label that's a key
89:02 - value pair where the key is going to be
89:05 - app and the value is going to be auth
89:07 - and our deployment is going to select
89:09 - pods using the same label app off as the
89:13 - key and value
89:15 - and then we can go here to replicas and
89:18 - as you can see here replicas is the
89:21 - number of desired pods and you already
89:23 - saw how this replicas is working you saw
89:26 - that when we actually applied our
89:28 - configuration there were two auth pods
89:30 - deployed so if we were to increase this
89:32 - to say four then four auth pods would be
89:34 - deployed when we apply the configuration
89:36 - and then we can head over here to
89:38 - strategy which is here and this is just
89:42 - the deployment strategy to use to
89:44 - replace existing pods with new ones and
89:46 - basically this is the difference between
89:48 - here killing all of the existing pods
89:51 - before creating new ones which would
89:54 - essentially mean that our service is
89:56 - unavailable during the creation of new
89:58 - pods or replacing our old replica sets
90:02 - by new ones using rolling update which
90:05 - basically gradually scales down the old
90:07 - replica sets and scales up new ones and
90:10 - in our case we're actually configuring
90:12 - the max surge here to three which is
90:15 - this here and this is the maximum number
90:18 - of PODS that can be scheduled above the
90:20 - desired number of PODS so for example if
90:23 - our desired number of PODS is two and we
90:26 - need to do an update a rolling update it
90:29 - might be necessary to exceed the number
90:31 - of replicas while some pods are shutting
90:34 - down and newer pods are spinning up so
90:37 - this Max surge is just to give us some
90:39 - extra Headroom when we're actually
90:41 - needing to update our deployment and
90:44 - lastly we get into our template here
90:46 - and simply put template describes the
90:50 - pods that will be created we've already
90:52 - gone over this a little bit but let's
90:53 - actually go into the Pod template spec
90:57 - so everything nested under this template
91:00 - field is within the Pod template spec so
91:03 - we have here our metadata and metadata
91:06 - is the same for all of the types so
91:08 - object metadata is going to be the same
91:10 - regardless of the type
91:12 - so if we click this we see that it's
91:14 - still just name and all of the other
91:16 - fields that are possible within metadata
91:22 - and same thing with spec but this time
91:26 - the spec here isn't the same spec as
91:28 - deployment because remember each type
91:31 - has its own spec format so this spec is
91:34 - going to be pod spec here so if we
91:37 - select pod spec
91:39 - we get a different set of fields and
91:41 - descriptions for a spec for a template
91:44 - which is a pod spec because the template
91:47 - is the template for pods so as you can
91:49 - see here we have containers then we have
91:51 - containers here as well
91:53 - and this is where we Define our
91:56 - container so we need to name the
91:58 - container and we need to set an image
92:00 - for our container as well and remember
92:03 - we're pulling our image for our
92:05 - container from our Docker repository and
92:08 - that's the image that's going to be used
92:11 - for our container within our pod so
92:13 - that's image here and then we have ports
92:19 - which we can find here and this ports is
92:22 - actually similar to the expose
92:24 - instruction that we did in our Docker
92:26 - file doesn't actually serve as anything
92:28 - other than documentation so as you can
92:30 - see here it says list of ports to expose
92:33 - from the container exposing a port here
92:36 - gives the system additional information
92:37 - about the network connection a container
92:39 - uses but is primarily informational so
92:42 - not specifying a port here does not
92:44 - prevent that Port from being exposed so
92:47 - any port which is listening on the
92:49 - default
92:50 - 0.0.0.0 address inside a container will
92:53 - be accessible from the network so we've
92:55 - already gone over all of this when we
92:57 - were configuring our Docker file so you
93:00 - should be familiar so yeah this
93:02 - container Port 5000 is similar to our
93:05 - expose instruction in our Docker file it
93:08 - serves as documentation essentially and
93:10 - let's see if we can just search for
93:13 - environment from
93:16 - foreign
93:18 - so we have environment from here which
93:21 - is this configuration here and it's just
93:24 - list of resources to populate
93:26 - environment variables in the container
93:28 - so as I explained to you and showed to
93:30 - you our config map is where we Define
93:33 - our environment variables for our
93:36 - container which you were able to see
93:37 - when we went into the shell for our
93:40 - container so we're using an additional
93:42 - resource config map
93:45 - which can be seen here and it's the
93:48 - config map to select from for the
93:50 - environment variables for the container
93:52 - and down here we also have our secret
93:55 - ref or this one for that matter the
93:58 - contents of the target secret data field
94:00 - will represent the key value pairs as
94:02 - environment variables so essentially the
94:04 - secrets are being stored as environment
94:06 - variables as well and the secrets come
94:08 - from our secrets configuration and both
94:11 - this config map and the secret are their
94:15 - own individual kubernetes objects as
94:17 - well so basically whenever you see kind
94:20 - and a name that means that's an
94:22 - individual kubernetes object so as you
94:25 - can see for our configuration file for
94:27 - our config map it has its own kind and
94:30 - config map type so this is going to
94:32 - create another object in our kubernetes
94:36 - cluster and as you can see each object
94:39 - configuration is going to have
94:41 - essentially similar Fields overall like
94:45 - they're all going to need an API version
94:47 - a kind metadata but some things might
94:51 - differ like this here is a data field
94:54 - which we don't use in our actual
94:57 - deployment object configuration and
95:00 - here's another example when we created
95:02 - Our Kind service we still have the
95:05 - metadata field with its nested field
95:07 - name and again we have the spec for the
95:11 - kind service but of course this spec
95:13 - format is going to be specific to the
95:16 - kind service so it'll be different from
95:19 - our deployment spec format and this API
95:23 - reference documentation is very
95:24 - important and I will have a link to it
95:27 - in the description of this video so now
95:30 - we can start to write the code for our
95:32 - Gateway service so we can just change
95:35 - directory
95:38 - back to our python Source directory and
95:41 - right now we only have our auth service
95:43 - but we can also make their Gateway
95:47 - and we'll go ahead and change directory
95:50 - into Gateway and to start we want to
95:52 - create a virtual environment as usual
95:57 - and then we want to start our virtual
95:59 - environment
96:00 - and as you can see our virtual
96:02 - environment variable is the Gateway
96:05 - virtual environment now the next thing
96:07 - that we want to do is create a file
96:09 - called server.pi so our Gateway service
96:12 - is going to have a few dependencies so
96:15 - we're going to need to import
96:19 - and actually let me go ahead and install
96:21 - some Vim dependencies
96:34 - so we're going to need to import OS
96:39 - we're going to need to import grid FS
96:41 - pica
96:43 - and Json and we're also going to need to
96:46 - import flask
96:49 - and request and we're going to need to
96:52 - import flask Pi
96:58 - and we're going to create an auth
97:00 - package
97:02 - and we're going to create a validate
97:04 - module within that package and we're
97:07 - also going to create an auth service
97:09 - package and we're going to import module
97:12 - access from that package
97:15 - and we're also going to need to create a
97:18 - storage package and we'll import util
97:22 - from the storage package
97:25 - so these we haven't created yet but
97:27 - we're going to create soon
97:29 - and flask by we're going to use
97:32 - mongodb to store our files
97:36 - and this grid FS is basically going to
97:40 - allow us to store larger files in
97:43 - mongodb and I'll explain a little bit
97:45 - more about that when we get to it and
97:47 - this Pika here is going to be what we
97:50 - use to interface with our queue we're
97:53 - going to be using a rabbitmq service to
97:56 - store our messages and I'll go over that
97:59 - more once we get to it as well
98:02 - so to start we just want to create our
98:06 - server so server is going to be equal to
98:08 - flask
98:12 - so in our server config
98:16 - we can do
98:18 - URI
98:20 - and set it equal to
98:23 - mongodb at
98:26 - host.minicube dot internal
98:29 - at Port
98:31 - 27017 which is the default mongodb port
98:36 - and at database that we're going to call
98:39 - videos so if you remember this host mini
98:41 - Cube internal just gives us access to
98:44 - our local host from within our
98:46 - kubernetes cluster and this is just a
98:49 - mongodb URI that's going to be the
98:52 - endpoint to interface with our mongodb
98:55 - so our config is going to have the
98:57 - configuration to our mongodb that's on
99:00 - the Local Host that we haven't yet
99:02 - installed but we will
99:04 - then we're just going to create a
99:06 - variable called and we're going to
99:08 - do PI
99:10 - server
99:12 - and I will explain what this is doing in
99:15 - a second but first let's just go ahead
99:16 - and install all of our dependencies
99:19 - so we can go ahead and save this and
99:22 - we'll just cat server.pi so we can see
99:24 - what we need to install and we'll do
99:26 - pip3 install
99:29 - pika
99:30 - and pip3 install flask
99:34 - and let's cut that again so we can see
99:38 - pip3 install I believe it's high
99:42 - and once we've installed those
99:44 - dependencies we can go back in here
99:48 - foreign
99:53 - [Music]
99:54 - flask Pi
99:57 - which is flask
99:59 - hi
100:02 - and now let's go back in here and see
100:06 - and we have a type over here this should
100:08 - be a small l
100:11 - so now here what this line of code is
100:13 - doing
100:14 - so what we need to know about this line
100:16 - of code is that this Pi is going
100:19 - to wrap our flask server which is going
100:22 - to allow us to interface with our
100:24 - mongodb so if we go to the definition
100:27 - here we see that it manages mongodb
100:30 - connections for our flask app
100:33 - so this essentially is abstracting the
100:36 - handling of the mongodb connections away
100:38 - from us so beyond that we don't really
100:40 - need to understand what's actually
100:41 - happening for the purposes of our use
100:45 - case
100:45 - and once we've created that
100:48 - variable or the pi instance we're
100:52 - going to create FS for grid fs and we're
100:56 - going to create an instance of this grid
100:58 - FS class and we need to pass in our DB
101:03 - from our database
101:06 - which is going to be this video's DB
101:10 - so grid FS is going to wrap our mongodb
101:14 - which is going to enable us to use
101:16 - mongodb's grid FS so let me quickly
101:19 - explain what grid FS is okay so we're
101:23 - going to very quickly go over what grid
101:25 - FS is in relation to our mongodb so
101:29 - we're using mongodb to store our files
101:33 - files being both our MP3 files and our
101:36 - video files and if we go to mongodb's
101:40 - limits and thresholds documentation we
101:43 - can see that a binary Json document size
101:46 - has a maximum size of 16 megabytes and
101:50 - it's explained here that the maximum
101:53 - document size helps ensure that a single
101:55 - document cannot use an excessive amount
101:57 - of Ram or during transmission excessive
102:00 - amounts of bandwidth so they're
102:03 - basically saying that handling files
102:06 - over 16 megabytes in memory will result
102:09 - in Prof performance degradation so what
102:12 - they provide as an alternative is grid
102:15 - FS which essentially allows us to work
102:18 - with files larger than 16 megabytes by
102:21 - sharding the files for example if we go
102:24 - to the gridfs documentation here we see
102:27 - that grid FS is a specification for
102:29 - storing and retrieving files that exceed
102:31 - the binary Json document size limit of
102:34 - 16 megabytes and it says instead of
102:36 - storing a file in a single document
102:38 - gridfs divides files into parts or
102:41 - chunks and it stores each chunk as a
102:44 - separate document so in this case we'd
102:46 - no longer be dealing with files larger
102:49 - than 16 megabytes in memory because a
102:52 - file larger than that size would be
102:54 - separated into chunks and we're only
102:55 - dealing with the individual chunks at
102:57 - that point which avoids the performance
103:00 - degradation issue so when using grid FS
103:03 - gridfs uses two collections to store
103:07 - files and you can just think of
103:09 - collections in mongodb as tables so it's
103:13 - explained here that one of those
103:14 - collections stores the file chunks and
103:16 - then there's another collection that
103:18 - stores the files metadata so this
103:21 - collection that stores the file's
103:23 - metadata basically contains the
103:25 - information necessary to reassemble the
103:28 - chunks to create or reform the original
103:32 - file and if you're interested in more
103:34 - details about this gridfs or mongodb for
103:38 - that matter has very good documentation
103:40 - so you can come and read additionally
103:44 - about this but you'll see later on in
103:47 - the tutorial these are the two
103:49 - collections that we'll be working with
103:50 - to work with these files and the reason
103:53 - we're actually using gridfs is because
103:56 - working with video files there's a high
103:59 - probability that will eventually be
104:01 - working with files larger than 16
104:03 - megabytes so this is essentially going
104:05 - to Future proof our application but for
104:08 - the purposes of this tutorial you don't
104:10 - really need to know much beyond what I
104:12 - just explained but again if you're the
104:14 - type of person that likes to dive deeper
104:16 - into these types of details like me then
104:19 - I recommend reading this page as well
104:22 - which is actually pretty interesting now
104:25 - the next thing that we're going to want
104:26 - to do is configure our rabbitmq
104:29 - connection
104:30 - so we'll just create a variable called
104:32 - connection and we're going to use Pica
104:35 - dot blocking connection
104:38 - which is essentially going to make our
104:41 - communication with our rapidm QQ
104:43 - synchronous and again the details of how
104:46 - this is working are abstracted away from
104:48 - us so we don't need to worry too much
104:50 - about that so in here we're going to add
104:53 - our connection parameters
104:58 - and we're going to pass to our
105:01 - connection parameters the host for our
105:04 - rapidm QQ and we're going to deploy our
105:07 - queue as a stateful set in our
105:09 - kubernetes cluster and it's going to be
105:11 - accessible via just the name rabbitmq
105:14 - and we haven't configured this yet we're
105:16 - going to configure it later but just
105:18 - know that this rabbit mq string is
105:20 - referencing our rabbitmq host and once
105:24 - we create this instance of a blocking
105:26 - connection we want to create a channel
105:28 - with just the connection that we just
105:31 - created dot Channel
105:35 - so let's briefly go over how rabbitmq is
105:39 - going to integrate with our overall
105:41 - architecture so we already know how the
105:44 - off flow works so we don't need to go
105:46 - over that again but now let's go over
105:48 - how rabbitmq integrates with our overall
105:51 - architecture so when a user uploads a
105:54 - video to be converted to MP3 that
105:56 - request will first hit our Gateway our
105:59 - Gateway will then store the video in
106:01 - mongodb and then put a message on this
106:04 - queue here which is our rapid in Q
106:06 - letting Downstream Services know that
106:08 - there is a video to be processed in
106:10 - mongodb the video to MP3 converter
106:13 - service will consume messages from the
106:16 - queue it will then get the ID of the
106:18 - video from the message pull that video
106:20 - from mongodb convert the video to MP3
106:23 - then store the MP3 on mongodb then put a
106:27 - new message on the Queue to be consumed
106:29 - by the notification service that says
106:31 - that the conversion job is done the
106:34 - notification service this consumes those
106:36 - messages from the queue and sends an
106:38 - email notification to the client
106:40 - informing the client that the MP3 for
106:42 - the video that he or she uploaded is
106:45 - ready for download the client will then
106:47 - use a unique ID acquired from the
106:49 - notification plus his or her JWT to make
106:52 - a request to the API Gateway to download
106:55 - the MP3 and the API Gateway will pull
106:58 - the MP3 from mongodb and serve it to the
107:01 - client and that is the overall
107:04 - conversion flow and how rapidmq is
107:07 - integrated with the overall system okay
107:09 - so now that we have a clear
107:11 - understanding of the overall flow of our
107:13 - system we can now use that understanding
107:16 - to familiarize ourself with some key
107:19 - terms when considering microservice
107:21 - architectures those terms are
107:23 - asynchronous and synchronous
107:25 - inter-service communication and strong
107:27 - and eventual consistency let's start
107:30 - with synchronous inter-service
107:32 - communication because understanding that
107:34 - will make it easy easier to understand
107:36 - everything else so synchronous
107:38 - inter-service communication put simply
107:41 - means that the client service sending
107:43 - the request awaits the response from the
107:45 - service that it is sending the request
107:47 - to the client service can't do anything
107:49 - while it waits for this response so it
107:51 - is essentially blocked so this request
107:54 - is considered a blocking request for
107:57 - example our Gateway service communicates
107:59 - with our auth service synchronously so
108:02 - when the Gateway service sends an HTTP
108:05 - post request to our auth service to log
108:08 - in a user and retrieve a JWT for that
108:11 - user our Gateway service is blocked
108:13 - until the auth service either Returns
108:15 - the JWT or an error so communication
108:18 - between our API Gateway and our off
108:21 - service is synchronous which makes those
108:24 - two Services tightly coupled now on the
108:27 - other end of the spectrum we have
108:28 - asynchronous inter-service communication
108:30 - so with asynchronous inter-service
108:33 - communication the client service does
108:35 - not need to await the response of the
108:38 - downstream service therefore this is
108:40 - considered a non-blocking request this
108:42 - is achieved in our case by using a cue
108:45 - for example in our architecture our
108:48 - Gateway service needs to communicate
108:50 - with our converter service but if our
108:52 - Gateway were to communicate with our
108:54 - converter service in a synchronous
108:56 - manner the performance of our Gateway
108:58 - would take a hit because if the Gateway
109:00 - were to get many requests to convert
109:03 - large videos the processes that make
109:05 - requests to the converter service would
109:07 - be blocked until the converter service
109:09 - finishes processing the videos so let's
109:12 - say that hypothetically our Gateway
109:14 - service processes one request per thread
109:17 - concurrently or at the same time if we
109:19 - have two processes with four threads
109:21 - each that's eight concurrent requests if
109:24 - our Gateway were to get more than eight
109:26 - requests to process large videos the
109:28 - entirety of our gateways threads would
109:31 - be blocked awaiting the completion of
109:33 - the processing of each request so in
109:35 - this case synchronous communication
109:37 - between our Gateway and our converter
109:39 - service would not be scalable and this
109:42 - is where our queue comes into the
109:43 - picture as explained before our Gateway
109:46 - doesn't communicate directly with our
109:48 - converter service therefore it does not
109:51 - depend on the converter Services
109:52 - response this means that in our current
109:55 - architecture our Gateway and our
109:58 - converter service are Loosely coupled
110:00 - this decoupling is done by using the
110:03 - queue our Gateway just stores the video
110:05 - on mongodb and throws a message on the
110:07 - queue for a downstream service to
110:09 - process the video at its convenience so
110:11 - in this case the only thing holding up
110:14 - the threads on our Gateway service is
110:16 - the uploading of the video to mongodb
110:18 - and putting the message on the Queue
110:20 - which means that our Gateway Services
110:21 - threads will be freed up much quicker
110:24 - allowing for our gateway to handle more
110:26 - incoming requests so with the current
110:28 - architecture our Gateway service is
110:30 - asynchronously communicating with our
110:32 - converter service that is it sends
110:34 - requests to the converter service in the
110:37 - form of messages on the Queue but it
110:39 - doesn't need to wait for nor does it
110:41 - care about a response from the converter
110:43 - service it essentially just sends and
110:45 - forgets the message and this same thing
110:47 - is happening with the communication
110:49 - between the converter service and the
110:51 - notification service now let's get into
110:53 - strong consistency versus eventual
110:56 - consistency let's start with an example
110:58 - of what our application flow would look
111:00 - like if it were strongly consistent so
111:03 - let's say that hypothetically whenever a
111:05 - user uploads a video to our gateway to
111:07 - be converted to an MP3 we make a
111:10 - synchronous request to our converter
111:11 - service waiting for the conversion to
111:14 - complete and then in response the
111:15 - converter sends an ID for the MP3 back
111:18 - to the user once the conversion is
111:20 - complete at the point that the user
111:22 - received that ID it's certain that the
111:24 - video has been processed and converted
111:26 - into an MP3 and that the data is
111:28 - consistent with the update so at that
111:30 - point if the user were to request to
111:33 - download the data based on that ID the
111:36 - user is guaranteed to get the most
111:37 - recent update of that data so that is
111:40 - strong consistency eventual consistency
111:43 - on the other hand is a bit different so
111:45 - let's use our actual architecture as an
111:48 - example in this case for the sake of
111:50 - example let's say that hypothetically
111:52 - when our Gateway uploads the video to
111:55 - mongodb and puts the message on the
111:57 - queue for it to be processed we return a
111:59 - download ID to the user at that moment I
112:02 - know that we don't return a download ID
112:04 - to the user at that moment in our actual
112:06 - application this is just to help you to
112:08 - understand if the user were to upload a
112:11 - video that takes one minute to process
112:12 - but immediately after receiving the ID
112:15 - the user tried to download the MP3 the
112:17 - MP3 would not yet be available because
112:19 - it would still be processing in that
112:21 - case but the MP3 eventually will be
112:24 - available so if the user were to wait
112:26 - one minute and then request to download
112:28 - the MP3 with that same ID at that point
112:31 - the MP3 would be available therefore the
112:34 - data is eventually consistent and that
112:37 - is eventual consistency
112:39 - okay so the first route that we're going
112:42 - to make for our Gateway is going to be
112:44 - our login route
112:52 - and we're going to define a function
112:54 - called login
112:56 - and what this route is going to do is
112:58 - it's going to communicate with our auth
113:01 - service to log the user in and assign a
113:05 - token to that user so we'll set token
113:08 - error equal to
113:11 - access Dot Login
113:14 - and we're going to pass in the request
113:16 - and this request is from flask and we're
113:19 - importing it here
113:26 - and we're going to create a module
113:28 - called access that's going to contain
113:30 - this login function so let's just go
113:33 - ahead and save this and let's clear and
113:36 - in our Gateway directory we want to
113:39 - create another directory that we'll call
113:41 - auth service
113:43 - and we'll CD into that directory and
113:46 - we'll create a file called init.pi which
113:50 - is essentially going to mark this
113:52 - directory as a package
113:56 - and we'll also create a file called
113:58 - access.pi which is going to be the
114:00 - module that contains our login function
114:03 - and we're going to need to import OS and
114:08 - we also need to import requests and this
114:11 - request is different from the request
114:13 - that we import from flask this request
114:16 - is going to be the module that we use to
114:20 - make HTTP calls to our auth service
114:24 - so we can go ahead and Define log in and
114:27 - we take in request which is not to be
114:29 - confused with this requests
114:32 - and we're going to set auth equal to
114:34 - request Dot
114:37 - authorization and as we write the code
114:39 - for this just pay close attention to
114:42 - requests versus requests with an S at
114:45 - the end because they're different so
114:47 - this request object has this
114:49 - authorization attribute so if when we
114:52 - create this variable this variable
114:55 - resolves To None So if not auth that
114:58 - means that there's no authorization
115:00 - parameters in our request so that means
115:03 - that we need to return none for our
115:06 - token and we need to return for our
115:09 - error missing credentials
115:13 - and a 401
115:17 - so if we go ahead and save this
115:20 - and quit and go back into our
115:24 - server.pi file
115:29 - we see that we're setting token and
115:32 - error equal to
115:34 - access.login so access.login needs to
115:37 - return a tuple and the first item in the
115:39 - Tuple will go to token and the second
115:41 - item in the Tuple will go to error so if
115:44 - we go back to the definition in this
115:46 - case when we're missing credentials
115:48 - we're going to return none for the token
115:50 - and we're going to return an error but
115:52 - upon the successful login of a user
115:55 - we're going to return none for the error
115:57 - and we're going to return an actual
115:59 - token
116:00 - so we'll set basic auth equal to
116:04 - auth.username and auth.password
116:09 - and we've already gone over basic auth
116:12 - so you should already be familiar with
116:14 - this
116:15 - and we're going to set our response
116:18 - equal to
116:19 - requests dot post
116:24 - now this request is going to be the
116:27 - request that's going to make the HTTP
116:29 - call to our auth service so let's save
116:32 - this actually and we need to install
116:36 - requests
116:40 - and let's go back into our access file
116:44 - so this request.post is going to make a
116:47 - post request to our auth service and the
116:50 - parameters or the arguments that we need
116:51 - to pass are the string the URL endpoint
116:55 - string and our auth header and the way
116:57 - that we do that is we can just create a
117:00 - formatted string
117:03 - and we'll do OS Dot environ.get
117:07 - and we'll just get our auth service
117:11 - address which we're going to create so
117:14 - we're going to create this off service
117:16 - address environment variable which is
117:18 - going to be the address for our auth
117:21 - service and we're going to need to
117:23 - access the login endpoint
117:26 - and to create the basic auth header
117:28 - we're just going to do auth equals basic
117:30 - auth
117:31 - and once this request completes this
117:35 - response is going to contain the result
117:38 - and we're going to check if response dot
117:41 - status code
117:43 - equals 200
117:46 - that means that we're good we're going
117:48 - to return response dot text which is
117:50 - going to be our token and none for the
117:52 - error otherwise
117:55 - we'll return none if we don't get a 200
117:59 - response it means we didn't get our
118:00 - token so we'll return none and we'll
118:02 - just return response.txt
118:05 - and response dot status code
118:11 - and we can go ahead and save that
118:14 - and this should be
118:16 - double equal
118:19 - and let's format
118:27 - and this is spelled wrong
118:31 - and we can save this file and let's just
118:34 - clear this
118:36 - let's just change directory back to our
118:39 - main directory and go back into
118:41 - server.pi
118:44 - so once we call this access.login we're
118:47 - going to check if not error and that's
118:50 - because if there is an error this is
118:52 - going to contain an error but if there
118:54 - is no error this is going to be null or
118:57 - none and if that's the case we can just
118:59 - return our token
119:01 - otherwise if there is an error then
119:03 - we're just going to return the error
119:06 - and that's going to be it for our login
119:08 - function
119:13 - now the next route that we want to
119:15 - create is our upload route and this is
119:19 - going to be the route that we use to
119:21 - upload our video that we want to convert
119:23 - into an MP3
119:25 - so we're just going to call it upload
119:28 - and methods will be just post
119:33 - so we'll Define a function called upload
119:37 - and now for this route we need to make
119:40 - sure that the user has a token from our
119:42 - login route so we need to validate our
119:45 - user so we're going to set access and
119:49 - error equal to validate dot token
119:53 - and we're going to create this validate
119:55 - module as well and we're going to pass
119:57 - in the request so let's go ahead and
119:59 - create this validate module with this
120:02 - token function
120:04 - so we'll go ahead and save and let's
120:06 - clear and we'll make dur and this dirt
120:09 - will just call auth the other one we
120:11 - call it auth service because we're
120:13 - communicating with the auth service on
120:16 - behalf of the user or the client but
120:19 - auth is going to just be used internally
120:21 - so our Gateway is going to use this auth
120:24 - package to validate tokens that it
120:26 - receives from the client so we'll make
120:28 - their auth
120:30 - and let's change their auth and once
120:32 - again we need to create this init.pi
120:34 - file to mark this directory as a package
120:37 - and we'll create a file called
120:39 - validate.pi and we will import OS and
120:43 - requests once again
120:45 - and we'll Define a function called token
120:49 - which takes in a request because we're
120:51 - validating a token
120:55 - so remember the flow is the client's
120:57 - going to access our internal services or
121:01 - our endpoints by first logging in and
121:04 - getting a JWT and then for all
121:06 - subsequent requests the client is going
121:08 - to have an authorization header
121:10 - containing that JWT which tells our API
121:14 - Gateway that that client has access to
121:17 - the endpoints of our overall application
121:20 - so this validate token function that
121:23 - we're creating here is going to be the
121:24 - function that validates that JWT sent by
121:27 - the client so we need to first check to
121:30 - see if the client has the authorization
121:32 - header in his request or his or her
121:35 - request
121:37 - foreign
121:41 - is not in request.headers we're going to
121:45 - return none for our access and we're
121:48 - going to return an error that says
121:50 - missing credentials
121:52 - and it's going to be a 401. otherwise
121:55 - we're going to set our token equal to
121:58 - request dot headers
122:01 - authorization
122:03 - and if that token does not exist we'll
122:07 - return none as well and the same error
122:10 - missing credentials
122:13 - 401
122:14 - but if the token does exist and the
122:18 - authorization header exists we'll set
122:19 - response equal to request dot post
122:23 - and this should be requests so don't
122:27 - make the same mistake I did and once
122:29 - again we're going to send a post request
122:32 - via HTTP to our auth service so once
122:36 - again we'll do a formatted string
122:40 - and we're going to get our host from the
122:43 - environment
122:46 - using the environment variable auth
122:49 - service address
122:52 - and we're going to access the validate
122:56 - endpoint
122:57 - and headers will be equal to
123:00 - authorization
123:02 - token
123:04 - so we're basically just passing along
123:06 - the authorization token to our validate
123:09 - request of our auth service
123:12 - and we want to check the response if
123:14 - response dot status code
123:17 - equals 200 then we're good so that means
123:20 - we're going to return
123:22 - response.txt and none and response.text
123:26 - is going to contain the body which will
123:29 - be the axis that the bearer of this
123:32 - token has and you'll see what I mean by
123:34 - that when we parse this
123:37 - else if the response from our auth
123:40 - service isn't 200 we're going to return
123:42 - none and we're going to return an error
123:45 - so response Dot txt and response dot
123:50 - status code
123:54 - and we can save that and let's go back
123:57 - to our root directory and back into our
124:01 - server.pi file okay so let's take a
124:03 - second to understand what's happening
124:05 - here with our validation before moving
124:08 - forward so let's quickly go back into
124:11 - our auth service and into the server.pi
124:15 - file
124:17 - and let's go over our login route so if
124:20 - you remember our login route here is
124:23 - going to take a username and a password
124:25 - or an email and a password and return a
124:28 - token and the token is going to be a
124:31 - Json web token and that token is being
124:34 - created here in this create JWT function
124:37 - and as you can see in this function
124:40 - we're encoding a payload and that
124:43 - payload is here and this payload
124:46 - contains our claims which is basically
124:49 - these data points within the payload so
124:52 - our username is a claim the expiration
124:55 - date is a claim and also contained
124:58 - within this payload is this claim here
125:00 - for admin which is just a bull that's
125:02 - going to be true or false and as I said
125:05 - earlier we're just going to allow
125:06 - anybody with admin equal to True access
125:09 - to all of the endpoints of our services
125:13 - so the token that's returned to the
125:17 - logged in client is going to contain
125:19 - this payload but the payload is going to
125:21 - be encrypted
125:23 - and when that client sends their token
125:25 - in their request and we validate it in
125:28 - using the validate endpoint for our off
125:32 - service we're going to first check to
125:34 - see if the token exists in the request
125:37 - and then we're going to decode that
125:40 - token and when we're decoding the token
125:42 - we're using the same key that we signed
125:45 - the token with this JWT secret which is
125:48 - how we know that this is a valid token
125:50 - because our auth service is the service
125:52 - that signed the token using this key and
125:55 - when we decode the token we're using the
125:57 - same key the service is key so if
125:59 - somebody were to send a token that was
126:01 - signed with a different secret key then
126:03 - of course it wouldn't work and when we
126:05 - decode this token with this decoded
126:07 - variable is going to include that
126:10 - payload that tells us who the user is
126:12 - via their username which is their email
126:14 - and their privileges which is the auth's
126:18 - claim which is true or false so now
126:20 - let's go back into our Gateway code code
126:24 - and here when a user tries to upload
126:27 - they need to have a token header which
126:30 - we're going to validate using our
126:33 - function to validate the token and the
126:35 - Gateway is just going to forward this
126:38 - token to our auth services validate
126:40 - endpoint and the response that we expect
126:43 - from our auth service is that decoded
126:46 - body
126:47 - so here when we get a successful
126:50 - response a 200 response what we're
126:52 - returning here in this response.txt is
126:55 - going to be the body of that token
126:57 - containing the claims so it's
126:58 - essentially it's going to be the decoded
127:01 - token where the body is visible and it's
127:03 - going to be a string which is going to
127:05 - be Json formatted
127:08 - and that means that here this access
127:11 - variable is going to resolve to that
127:14 - Json string that contains our payload
127:17 - with our claims so here we'll set access
127:20 - equal to Json dot loads
127:24 - access
127:27 - and if we go to the definition of this
127:29 - you see that it deserializes an instance
127:33 - containing a Json document to a python
127:36 - object so we're essentially just
127:37 - converting this Json string to a python
127:40 - object so that we can work with it in
127:42 - code
127:43 - and what's being converted into a python
127:46 - object is this here
127:50 - so our python object is going to look
127:52 - like this once we've decoded the Json
127:57 - so let's just go back
127:59 - and we can go ahead and close this
128:01 - so this access is going to contain that
128:04 - object and that object has the admin
128:07 - claim which is going to be a bull which
128:09 - is true or false and actually just to be
128:11 - clear let me show what I mean by that
128:17 - so this admin claim here is going to
128:20 - contain the auths which is a wool which
128:22 - is true or false and if it's true we'll
128:25 - give the user access to all of the
128:27 - endpoints
128:29 - so we're going to check for that claim
128:31 - so we're going to say if access which is
128:34 - the object that was converted from the
128:36 - Json so we're going to say if access
128:38 - admin which is essentially saying if the
128:42 - admin claim resolves to true then we're
128:44 - going to give the user access
128:47 - so let's go ahead and close that again
128:50 - so if the user does have access we're
128:53 - going to make sure that there's a file
128:56 - to be uploaded in the first place
128:58 - so if there's a file being uploaded the
129:01 - request should contain a dictionary in
129:04 - this request.files so we're going to say
129:06 - if the length of request.files is
129:09 - greater than one because we're only
129:10 - going to allow the uploading of one file
129:12 - at a time for now
129:13 - or the length of request.files is less
129:18 - than one because we want there to be
129:20 - exactly one file so we don't want more
129:22 - than one file then we don't want less
129:23 - than one file so if one of these is true
129:26 - then we're going to return
129:28 - exactly one file required
129:32 - and a 400.
129:35 - and this
129:36 - request.files dictionary is going to
129:39 - have a key for the file which will be
129:42 - defined when we send the request and the
129:44 - actual file as the value
129:48 - so that means that
129:50 - we should iterate through the key values
129:52 - in the request.files dictionary so we'll
129:55 - do for key which we don't need to use
129:57 - and file which is the value in
129:59 - request.files.items
130:03 - for every file we're going to upload it
130:05 - where we're going to do an upload and
130:08 - there should only be one file so this
130:09 - should only happen once
130:11 - so we'll do util dot upload and we
130:14 - haven't created this function yet but we
130:16 - will and this function is going to take
130:18 - us parameters to file our grid FS
130:21 - instance our rabbitmq Channel and the
130:26 - axis which was just explained above and
130:29 - this function is going to return an
130:30 - error if something goes wrong but if
130:32 - nothing goes wrong then it won't return
130:34 - anything it'll return none
130:37 - so to check if something went wrong
130:39 - we're just going to check if error and
130:41 - if there is an error we're just going to
130:43 - return error
130:44 - and after this for Loop completes if
130:47 - we've never returned an error then that
130:49 - means it was successful so we'll just
130:51 - return
130:52 - success
130:54 - with a 200 and that's what's going to be
130:57 - happening if the user is authorized but
131:00 - if the user isn't authorized so we need
131:03 - to go down here and do else
131:05 - so this is the block that's going to get
131:07 - executed if the user isn't authorized
131:09 - and in that case we're just going to
131:11 - return not
131:13 - authorized
131:15 - and a 401
131:22 - and that's going to be our upload route
131:24 - and remember we still need to go and
131:26 - create this upload function but I'm
131:28 - going to get to that in a second because
131:30 - that function is going to be a little
131:32 - bit involved
131:34 - so before we do that let's just finish
131:36 - up our template here for our Gateway
131:39 - server so we'll do the final endpoint
131:41 - which is going to be server.route
131:43 - and this is going to be the download
131:45 - endpoint which is the endpoint that is
131:47 - going to be used to download the MP3
131:50 - that was created from the video and this
131:53 - is going to be methods and we're only
131:55 - going to do git and we're going to
131:57 - Define download
132:00 - and for now we'll just pass this is just
132:02 - a template portion for this function or
132:04 - this endpoint and lastly we need to set
132:08 - if name
132:09 - equals
132:11 - Main
132:14 - we're going to run our server and again
132:17 - we're going to set our host to 0.0.0.0
132:22 - and our port in this case is going to be
132:25 - 880.
132:29 - and this is going to be our Gateway
132:32 - service template including both the
132:36 - login endpoint and the upload endpoint
132:38 - and to be continued on the download
132:41 - endpoint so let's go ahead and save this
132:43 - and actually let's go back in here so I
132:46 - don't confuse you
132:47 - so now what we're going to need to
132:49 - create is this upload function here and
132:52 - this util as you can see is coming from
132:56 - this here so from Storage import util so
133:00 - we need to create a storage package and
133:02 - within that package we need to create a
133:04 - util module
133:06 - so we'll make their storage
133:09 - change there into storage and we'll
133:11 - create our init.pi file
133:14 - and now we'll do util.pi
133:17 - and we'll start by importing Pica and
133:21 - Json
133:24 - and we'll Define a function called
133:26 - upload and remember the parameters are
133:28 - the file our grid FS instance our
133:32 - rabbitmq Channel and our access which is
133:36 - the user's access
133:38 - now this function is going to be a
133:40 - little bit complicated so try to keep up
133:43 - and pay attention I'll try to explain
133:45 - things the best that I can so basically
133:47 - with this upload function needs to do is
133:50 - it needs to First upload the file to our
133:54 - mongodb database using gridfs and once
133:57 - the file has been successfully uploaded
133:59 - we need to put a message in our rabbitmq
134:02 - so that a downstream service when they
134:05 - pull that message from the queue can
134:07 - process the upload by pulling it from
134:10 - the mongodb and this queue is allowing
134:13 - us to create an asynchronous
134:15 - communication flow between our Gateway
134:18 - service and the service that actually
134:19 - processes our videos and this
134:22 - asynchronicity is going to allow us to
134:25 - avoid the need for our Gateway service
134:28 - to wait for an internal service to
134:31 - process the video before being able to
134:33 - return a response to the client so the
134:35 - first thing that we want to do is we
134:37 - want to try to put our file into the
134:39 - mongodb so when we put a file in the
134:42 - mongodb we're going to use this FS dot
134:45 - put function and it's going to be the
134:47 - file that we want to put
134:50 - and if this put is successful a file ID
134:53 - is going to be returned a file ID object
134:57 - to be more specific so mongodb is going
134:59 - to return a file ID and that's if it's
135:02 - successful but if it's not successful
135:04 - then we want to catch the error
135:08 - and for now we're just going to return
135:11 - internal server error
135:17 - and if this returns this means our file
135:20 - wasn't uploaded successfully and the
135:22 - function just returned so we don't need
135:24 - to do anything else after that but if
135:26 - the file was successfully uploaded we
135:28 - need to create a message to put onto our
135:30 - queue so we'll set message equal to
135:35 - a dictionary and it's going to contain
135:38 - the video file ID which is going to be
135:41 - the file ID object converted into a
135:44 - string
135:46 - and we also need to create an empty MP3
135:50 - file ID within this same dictionary and
135:54 - for now it's going to be none but
135:56 - Downstream it's going to end up being
135:59 - set to the mp3s file ID in the database
136:02 - but you don't need to think too much
136:03 - about this right now we'll get to it
136:05 - later
136:05 - and we need to also have the username to
136:08 - identify who owns the file and the
136:11 - username is going to come from our
136:13 - access from our auth service and
136:15 - remember there was a claim for username
136:17 - there which contains our user's email
136:19 - and remember in our auth DB the email
136:23 - must be unique so this is a way to
136:26 - uniquely identify our user so that's
136:29 - going to be our message and now we need
136:31 - to put that message on the Queue so
136:32 - we're going to try and put this message
136:34 - on the queue
136:35 - using the channel that's passed to the
136:37 - function and we're going to do basic
136:39 - publish
136:42 - and we're going to set exchange equal to
136:46 - an empty string and this is just going
136:49 - to mean we're going to use the default
136:50 - exchange so I will explain a bit more
136:53 - about how rabbitmq works so for our
136:56 - purposes we're going to use a very basic
136:58 - rapidmq configuration and setup but we
137:01 - need to go over a couple of things so
137:03 - that we have a clear understanding of
137:05 - what's Happening Here let's start with
137:07 - the top level overview of how rabbitmq
137:10 - integrates with our system the first
137:12 - thing that is important to understand is
137:15 - that our producer which is the service
137:17 - that is putting the message on the Queue
137:19 - isn't publishing the message directly to
137:21 - the queue it actually sends messages
137:23 - through an exchange The Exchange is
137:26 - basically a middleman that allocates
137:28 - messages to their correct queue
137:30 - throughout the video I've been referring
137:32 - to rabbitmq as if it were a single cue
137:35 - but under the hood we actually can and
137:38 - do configure multiple cues within one
137:40 - rabbitmq instance for example in our
137:44 - case we'll make use of both a cue that
137:46 - we'll call video and a queue that we'll
137:48 - call MP3 so when our producer publishes
137:51 - a message to The Exchange The Exchange
137:54 - will route the message to the correct
137:56 - queue based on some criteria so how does
137:59 - our Exchange Route messages to the
138:00 - correct queue in our case well since
138:03 - we're going with a simple rabbitmq
138:05 - configuration for the sake of brevity
138:07 - you'll remember that we are using the
138:09 - default exchange by setting the exchange
138:12 - to an empty string and if we take a look
138:14 - at the rabbitmq documentation here the
138:17 - default exchange is a direct exchange
138:20 - with no name EG the empty string
138:23 - pre-declared by the broker and broker
138:26 - just being our rapidm queue instance and
138:29 - this default exchange has one special
138:31 - property that makes it very useful for
138:33 - simple applications and that is that
138:36 - every queue that is created is
138:38 - automatically bound to the default
138:40 - exchange with a routing key which is the
138:42 - same as the Q name so what does that
138:45 - mean exactly simply put that just means
138:47 - that we can set our routing key to the
138:50 - name of the queue that we want our
138:52 - message to be directed to and set the
138:54 - exchange to the default exchange and
138:57 - that will result in our message going to
138:59 - the queue specified by the routing key
139:01 - so with this overview we can see our
139:04 - video queue the exchange our producer
139:07 - and our consumer so let's say that this
139:09 - producer is our Gateway service and this
139:12 - queue is our video queue and this
139:14 - consumer is our video to MP3 converter
139:17 - when the user uploads a video our
139:20 - Gateway stores the video and then
139:21 - publishes a message to The Exchange that
139:24 - is designated for the video queue The
139:27 - Exchange will route that message to the
139:29 - video queue and the downstream service
139:31 - which is the consumer of the video queue
139:34 - will process the message the consumer in
139:36 - this case is our video to MP3 converter
139:39 - service so it will process the message
139:41 - by pulling the video from mongodb
139:44 - converting it to MP3 storing the MP3 on
139:47 - mongodb and then publishing a message to
139:49 - The Exchange that is intended for the
139:52 - MP3 queue but let's not focus on the
139:54 - mp3q just yet we'll get to that later
139:57 - let's just focus on the video queue for
139:59 - now let's say for example our producer
140:02 - is paddling on more messages than our
140:04 - one consumer can process in a timely
140:07 - manner this is where the capability to
140:09 - scale up our video to MP3 consumer comes
140:12 - into the picture but if we're going to
140:14 - scale up our queue actually needs to be
140:17 - able to accommodate multiple instances
140:19 - of our consumer without bottlenecking
140:22 - the entire flow so how do we manage that
140:24 - we manage that by making use of a
140:27 - pattern called the competing consumers
140:29 - pattern this pattern simply enables
140:31 - multiple concurrent consumers to process
140:34 - messages received on the same messaging
140:36 - channel that way if our queue is packed
140:39 - full of messages we can scale up our
140:41 - consumers to process the messages
140:43 - concurrently resulting in more
140:45 - throughput luckily by default our rapidm
140:48 - QQ will dispatch messages to our
140:51 - consuming services using the round robin
140:54 - algorithm which satisfies our needs this
140:57 - basically just means that the messages
140:59 - will be distributed more or less evenly
141:02 - amongst our consumers for example if we
141:05 - have two instances of our consuming
141:07 - service the first message would go to
141:10 - this instance and if another message
141:12 - were to come in it would not go to the
141:14 - same instance it would go to the next
141:16 - one and the same goes for if we already
141:18 - have a bunch of messages on the Queue
141:21 - the distribution of the messages will
141:23 - essentially go in a sequence from one
141:26 - instance of the consumer to the next to
141:28 - the next and then back to the first Etc
141:30 - so basically the messages will be
141:32 - distributed evenly in a round robin
141:34 - fashion so that they can be processed
141:36 - concurrently and that is going to be
141:39 - that so now that that's out of the way
141:41 - let's get back to right adding our code
141:44 - and then we're going to set our routing
141:46 - key equal to video
141:48 - so the routing key is actually going to
141:51 - be the name of our queue and we're going
141:53 - to have a queue called video where we
141:55 - put the video messages on
141:58 - and then the body of the message is
142:01 - going to be Json dot dumps
142:04 - message
142:05 - now similar to
142:07 - json.loads this dumps converts a python
142:10 - object into a Json string
142:13 - so as you can see it serializes the
142:16 - object a python object to a Json
142:18 - formatted string so it's basically doing
142:20 - the opposite of Json loads because the
142:23 - message we need to have a string
142:25 - contained within it not a python object
142:29 - and the python object I'm talking about
142:31 - is this here this is being converted
142:33 - into a Json string
142:35 - which is going to be the body of our
142:37 - message which gives our Downstream
142:40 - service all of the information that it
142:42 - needs to process the video conversion
142:46 - and we need to also set properties equal
142:50 - to Pika dot basic properties
142:56 - and within these properties we need to
142:59 - set the delivery mode
143:01 - and that's going to be set to pica.spec
143:04 - dot persistent
143:07 - delivery
143:09 - mode
143:11 - and this part is very important to make
143:13 - sure that our messages are persisted in
143:17 - our queue in the event of a pod crash or
143:21 - a restart of our pod so since our pod
143:24 - for our rabbitmq is a stateful pod
143:28 - within the kubernetes cluster we need to
143:30 - make sure that when messages are added
143:32 - to the queue they're actually persisted
143:34 - so that if the Pod is reset or the Pod
143:37 - fails when it comes back up or spins
143:39 - back up the messages are still there
143:41 - because if we don't set this if the Pod
143:44 - crashes or is reset then the messages
143:47 - are all going to be gone once the Pod is
143:50 - restored back to its original state so
143:52 - what we're essentially going to need to
143:54 - do is we're going to need to make our
143:56 - cue durable which means that the queue
143:58 - is going to be retained even after a pod
144:02 - restart and we need to make sure the
144:03 - messages within the queue are also
144:05 - durable which means that the messages
144:07 - are going to be retained even in the
144:09 - event of a pod restart or crash so when
144:12 - we actually create the queue we can
144:14 - configure the queue to be durable
144:16 - but that doesn't mean that the messages
144:18 - are going to also be persisted it just
144:21 - means that the queue will be durable so
144:23 - each individual message that we send to
144:25 - the queue we need to set this
144:27 - configuration here to tell rabbitmq that
144:30 - the message should be persisted until
144:32 - the message has been removed from the
144:34 - queue
144:35 - so anyways we're going to try to put our
144:38 - message onto our queue and if that
144:41 - doesn't work out
144:42 - we need to first delete the file from
144:45 - our mongodb because if there's no
144:48 - message on the queue for the file but
144:50 - the file still exists in the DB that
144:53 - file is never going to get processed
144:54 - because the downstream service doesn't
144:57 - know that that file exists if it never
144:58 - receives a message telling it to process
145:01 - that file so we'll just end up with a
145:03 - bunch of stale files in the database if
145:05 - we don't delete them in the event that a
145:07 - message can't be put onto our queue so
145:09 - if the message is unsuccessfully added
145:11 - to the queue we'll do fs.delete and we
145:14 - need to delete using the file ID
145:17 - and the file ID is the file ID object
145:20 - not the string it's defined here
145:23 - and once we've deleted the file we can
145:25 - then return internal server
145:28 - error
145:30 - so in this case in the event of a
145:32 - failure
145:35 - we will neither have a message on the
145:37 - Queue telling the downstream service to
145:39 - process the file and we will also not
145:42 - have a file in our database so it's
145:45 - going to be a complete failure so we can
145:47 - just return internal server error and at
145:49 - that point the user could just upload
145:51 - the file again if they want to try again
145:53 - and this is going to be it for our
145:56 - upload function
146:00 - and we're not using this error variable
146:03 - yet but we might use it later on down
146:06 - the line Depending on time constraints
146:08 - so we'll just leave it for now
146:10 - so let's go ahead and save and quit and
146:12 - we can clear okay so at this point we
146:15 - can go back to our root directory and we
146:18 - can start creating the deployment for
146:21 - this Gateway service so to start we just
146:24 - want to go ahead and freeze our
146:27 - requirements our dependencies into a
146:29 - requirements.txt file and we can start
146:33 - to create our Docker file now our Docker
146:36 - file is going to be quite similar to the
146:38 - one that we used for our auth service so
146:41 - we can actually just go to our auth
146:44 - service
146:45 - and get this Docker file and paste it
146:49 - into this one
146:53 - and everything is pretty much going to
146:55 - remain the same except for we're going
146:57 - to change the exposed port to 8080 and
147:01 - we also don't need this here but other
147:05 - than that the file is going to be
147:07 - identical so we can just go ahead and
147:10 - save this so now let's go ahead and
147:13 - build our image so we'll do Docker build
147:25 - and once we've built the image similar
147:27 - to before we can go ahead and tag our
147:29 - image so we'll do Docker tag and we'll
147:32 - go ahead and take the beginning of this
147:35 - and paste it in and your username for
147:39 - your Docker Hub account and this time
147:41 - instead of auth we're going to say
147:43 - Gateway
147:44 - and latest
147:47 - now at this point on your Docker Hub
147:49 - account you'll only have one repository
147:52 - which is the repository for our auth
147:55 - service and if we want to we can just
147:57 - create the Gateway repository manually
148:00 - but it'll actually create itself once we
148:02 - push to our username with the suffix
148:06 - Gateway
148:07 - so if we just do docker
148:09 - push
148:13 - and username slash forward slash Gateway
148:16 - latest
148:25 - you'll see that if you refresh the page
148:28 - here
148:30 - it automatically created the Gateway
148:33 - repository for us and if we go in here
148:37 - we'll see that we pushed the tag latest
148:40 - a few seconds ago so now we have our
148:43 - Gateway repository in our Docker Hub
148:45 - account
148:46 - so now that we can pull the image
148:48 - containing our source for our Gateway
148:50 - service from the internet we can go
148:52 - ahead and create our kubernetes manifest
148:55 - directory and we can go ahead and change
148:57 - there into there and similar again to
149:00 - our auth service we need to create a
149:03 - Gateway deployment yaml file
149:07 - and this configuration is also going to
149:10 - be very similar to our off service
150:01 - and this time we're going to pull our
150:03 - image from the Gateway Repository
150:07 - so remember your username and then
150:09 - Gateway
150:12 - and similar to the auth service we're
150:14 - going to create a config map
150:21 - called Gateway config map and let's not
150:24 - make the same mistake as last time this
150:26 - should be indented
150:27 - and for our secret ref we're going to
150:31 - create a secret as well
150:37 - and that's going to be it for our
150:40 - deployment
150:42 - and now let's go ahead and make our
150:44 - config map
150:58 - and the data that we need for our
151:01 - environment variables we need our auth
151:03 - service address
151:06 - and in kubernetes the service name will
151:09 - resolve to that service's host IP
151:12 - address so we can just put auth and then
151:14 - the port that the service is available
151:16 - at so we'll do all 5000 and that's going
151:20 - to be the address of our auth service
151:23 - within our kubernetes cluster and we can
151:26 - go ahead and save that and now let's
151:28 - create our secret
151:41 - and as of right now I don't think we
151:43 - have secrets for our Gateway service so
151:46 - let's just put a placeholder here for
151:48 - now and later we can add any secret
151:50 - variables if we need to
151:58 - and let's save that and we need to
152:01 - create service.yaml
152:12 - and the name of our service will just be
152:13 - Gateway
152:24 - and this service will have an internal
152:26 - IP address which will only be available
152:29 - within our cluster but our Gateway API
152:32 - needs to be able to be accessed from
152:35 - outside of our cluster so we're actually
152:36 - going to need to create another
152:38 - configuration called an Ingress to Route
152:41 - traffic to our actual Gateway service
152:43 - which I will get into in a second so we
152:46 - need to set ports equal to
152:49 - 80 80
152:51 - Target port 8080 as well
152:55 - and protocol is of course TCP
153:02 - so now we'll create the ingress.yaml
153:05 - which is going to allow traffic to
153:08 - access our Gateway endpoint so let's
153:10 - take a second to understand what an
153:12 - Ingress is in the context of a
153:15 - kubernetes cluster okay so in order to
153:18 - understand what an Ingress is in the
153:20 - context of kubernetes you first need to
153:22 - understand what a service is in the
153:24 - context of kubernetes so with this
153:26 - configuration file we're creating a
153:29 - service and you can really just think of
153:31 - the service as a group of PODS so in our
153:34 - case we want to create the Gateway
153:36 - service and we want that service to be
153:38 - able to scale up to multiple instances
153:41 - or multiple pods so the service comes in
153:44 - and groups all of the instances of our
153:46 - Gateway service together and it does
153:48 - this by using a selector so in our case
153:51 - we're using this label selector to tell
153:54 - our service what pods are part of its
153:57 - group or under its umbrella so this
154:00 - label selector essentially by lines our
154:03 - pods to the service so any pod with this
154:06 - label will be recognized by the service
154:08 - as being part of its group so now we
154:11 - don't have to think about individual IPS
154:13 - for each individual pod and we don't
154:16 - have to worry about keeping track of the
154:18 - IPS of PODS that go down or are
154:21 - recreated we also don't have to think
154:23 - about how requests to our service are
154:25 - load balanced to individual pods the
154:28 - service abstracts all of this away from
154:30 - us so now we can just send requests to
154:33 - the services cluster IP which remember
154:35 - is its internal IP and we can assume
154:38 - that these requests will be distributed
154:40 - logically amongst our pods based on
154:43 - something like round robin for example
154:45 - which we already went over when
154:47 - explaining webinimq so now that we have
154:50 - a clear picture of a service we can get
154:52 - into what an Ingress is so we have our
154:55 - service with its pods and that service
154:57 - sits in our cluster which is our private
155:00 - network but we need to allow quests from
155:03 - outside of our cluster to hit our
155:05 - Gateway Services endpoints we do this by
155:08 - making use of an Ingress simply put an
155:11 - Ingress consists of a load balancer that
155:13 - is essentially the entry point to our
155:16 - cluster and a set of rules those rules
155:19 - basically say which requests go where
155:21 - for example we have a rule that says
155:23 - that any request that hits our load
155:25 - balancer via the domain name
155:27 - mp3converter.com should be routed to our
155:30 - Gateway service so since this load
155:32 - balancer is the entry point to our
155:34 - cluster it can actually Route traffic to
155:37 - the cluster IPS within the cluster so in
155:40 - this case it would route requests going
155:42 - to the configured MP3 converter.com
155:45 - domain to our Gateway Services internal
155:47 - cluster IP and if we wanted to we could
155:50 - even add a rule to our Ingress that says
155:52 - to Route requests to apples.com to a
155:55 - different service in our cluster for
155:57 - example so that's pretty much everything
156:00 - you need to know about Ingress for the
156:02 - purposes of this video so let's get into
156:04 - writing our Ingress configuration file
156:07 - so for the API version we're going to
156:11 - set
156:13 - networking.kh dot IO V1 and kind is
156:18 - going to be Ingress this time
156:22 - and we're going to name it
156:24 - Gateway Ingress and we're going to use
156:27 - the default Ingress which is basically
156:30 - an nginx Ingress and we can set some
156:33 - configurations for our nginx Ingress
156:36 - using this annotations key and we want
156:39 - to make sure that our Ingress allows the
156:42 - upload of some relatively large files so
156:45 - we're just going to set our Ingress
156:53 - body size to zero which is essentially
156:58 - going to allow any body size and of
157:01 - course you want to fine-tune
157:02 - configurations like these in a
157:03 - production application but again our
157:06 - focus is the overall architecture so
157:08 - we're basically just configuring this in
157:10 - the easy way possible to get things done
157:12 - and this is a typo
157:16 - and we're going to do two more
157:17 - configurations
157:20 - proxy
157:22 - read time out and we're going to set
157:25 - that equal to 600
157:29 - and proxy
157:31 - same time out and we'll also set that
157:34 - one to 600. now for our spec
157:39 - and the rules we're going to Route
157:41 - request to the host MP3 converter
157:45 - .com to our Gateway service
158:03 - so remember our service name is Gateway
158:06 - and our service is available at Port
158:10 - 8080.
158:12 - so you're probably wondering how our
158:15 - kubernetes cluster knows if we're making
158:17 - a request to this host and basically
158:20 - what we're going to do is we're going to
158:22 - make it so that requests to this host on
158:25 - our local machine gets routed to the
158:27 - Local Host so we're just going to map
158:29 - this hostname to localhost on our local
158:32 - machine and we're going to Tunnel
158:34 - requests to our Local Host to minicube
158:36 - which sounds a bit complicated but just
158:39 - bear with me and we'll get to it so
158:41 - we're going to save this file and first
158:44 - we need to make sure that the MP3
158:46 - converter.com gets routed to localhost
158:49 - so you need to open a file called
158:52 - Etc hosts
158:54 - and you're going to have to have pseudo
158:57 - permissions to do that
159:00 - so once in this directory you want to
159:02 - map this address
159:05 - 127.0.0.1 which is the loopback address
159:09 - which localhost also resolves to we want
159:13 - to map it to
159:15 - MP3 converter.com
159:18 - so once we do this whenever we enter
159:20 - This MP3 converter.com into our browser
159:23 - or if we send a request to this host
159:26 - it's going to resolve to localhost
159:29 - so we can go ahead and save that
159:32 - and now we need to configure a mini Cube
159:35 - add-on to allow Ingress so we'll do mini
159:38 - Cube add-ons and let's list the add-ons
159:42 - that are available
159:45 - and as you can see there's an Ingress
159:48 - add-on here that's currently disabled
159:51 - so we can do mini Cube add-ons Ingress
159:55 - enable I believe
159:58 - or we can do
160:01 - enable Ingress
160:04 - and once that's done as you can see here
160:07 - it says that after the add-on is enabled
160:09 - please run minicube tunnel and your
160:12 - Ingress resources would be available at
160:15 - this loopback address that we mapped to
160:18 - mp3converter.com so basically whenever
160:20 - we want to use our microservice
160:23 - architecture or test this overall
160:25 - architecture we're going to run this
160:28 - mini Cube tunnel command
160:33 - and while this is running whenever we
160:35 - send requests to our loopback address
160:38 - they're going to go to our mini Cube
160:41 - cluster via the Ingress and since we
160:44 - mapped mp3converter.com to our loopback
160:47 - address if we send requests to MP3
160:50 - converter.com they're going to go to
160:52 - this mini Cube tunnel so just keep in
160:55 - mind so we control C out of this
160:57 - whenever we
160:59 - cancel this process then we're no longer
161:02 - tunneling the Ingress so as you can see
161:05 - here it says please do not close this
161:07 - terminal as this process must stay alive
161:09 - for the tunnel to be accessible so
161:11 - whenever we test we're going to have to
161:12 - run this mini Cube tunnel basically and
161:15 - we'll get to end-to-end testing a little
161:17 - bit later we're not going to test yet
161:19 - until we configure our q and our
161:22 - consumer service as well as our Gateway
161:25 - service which will be the producer
161:26 - service and that is how we are going to
161:29 - Route requests into our cluster and
161:32 - directly to our API Gateway so if we go
161:35 - and check K9s
161:37 - you may or may not see these two new
161:40 - items here but if you remember we've
161:42 - only deployed our auth service so far so
161:45 - we still need to deploy this Gateway
161:47 - service and similar to the off service
161:49 - we're going to have two replicas of our
161:51 - Gateway deployed so we can quit this and
161:56 - we can attempt to apply our
161:58 - configuration that we have here
162:03 - and it looks like we have an issue here
162:06 - so let's go into our secrets file and it
162:09 - looks like we forgot to capitalize
162:12 - Secret
162:13 - so let's once again try to apply
162:16 - and as you can see all of the resources
162:19 - or the objects were created successfully
162:21 - so let's go into canines
162:25 - and we're having an error with pulling
162:27 - the image here so let's see
162:30 - so it's actually not in error with
162:32 - pulling the image so the Gateway is
162:34 - failing because we basically don't have
162:37 - the rabbitmq queue deployed yet and it's
162:41 - trying to connect to this host here that
162:44 - we haven't yet created so that's fine
162:46 - for now so for now it's not going to be
162:49 - able to deploy so actually so it's not
162:52 - continuously running let's just scale
162:54 - that down for now until we create our
162:56 - rabbit mqq deployment so we can do Cube
163:00 - CTL scale and we can do deployment and
163:04 - we want to change the replicas to zero
163:06 - and we want to do that for the Gateway
163:08 - service so it says that our Gateway
163:11 - service was scaled so now you can see
163:13 - the the Gateway service is gone now it's
163:16 - not trying to be scaled up currently
163:18 - because we need to create that rabbitmq
163:20 - so let's go ahead and quit and from here
163:23 - we can start to get into the rabbitmq
163:25 - stuff so we need to create and deploy a
163:28 - rabbit and Q container to our kubernetes
163:31 - cluster so we're just going to change
163:33 - directory back to our source directory
163:36 - and currently we have our auth directory
163:38 - and our Gateway directory now we need to
163:40 - make a directory for our rabbitmq so
163:43 - we're just going to call it rabbit
163:45 - and we'll CD into rabbit and for rabbit
163:48 - mq instead of making a deployment like
163:50 - we did for the other two Services we're
163:53 - going to need to make a stateful set
163:55 - because we want our cue to remain intact
163:58 - even if the Pod crashes or restarts we
164:01 - want the messages within the queue to
164:03 - stay persistent until they've been
164:05 - pulled from the queue so let me go ahead
164:07 - and explain what a stateful set is okay
164:10 - so a staple set is similar to a
164:13 - deployment in that it manages the
164:15 - deployment and scaling of a set of PODS
164:17 - and these pods are based on an identical
164:19 - container spec but unlike a deployment
164:22 - with a staple set each pod has a
164:24 - persistent identifier that it maintains
164:27 - across any rescheduling this means that
164:30 - if a pod fails the persistent pod
164:32 - identifiers make it easier to match
164:34 - existing volumes to the new pods that
164:37 - replace any that have failed and this is
164:39 - important because if we were to have
164:41 - multiple instances of say for example a
164:44 - MySQL server each individual instance
164:47 - would reference its own physical storage
164:49 - and actually there would be a master pod
164:52 - that is able to actually persist data to
164:54 - its physical storage and the rest of the
164:56 - pods would be slaves and they would only
164:58 - be able to read the data from their
165:01 - physical storage and the physical
165:03 - storage that the slave pods use
165:04 - basically continuously syncs with the
165:07 - master pods physical storage because
165:10 - that's where all of the data persistence
165:12 - happens and most of the details
165:14 - surrounding this are not related to our
165:16 - architecture so I'll spare you the
165:18 - details actually to be honest there's
165:20 - probably a better way to configure our
165:21 - rapidmq broker within our cluster but
165:24 - this configuration will work just fine
165:25 - for our purposes we'll only be making
165:28 - use of one replica to achieve the before
165:30 - mentioned competing consumers pattern
165:32 - anyways the most important configuration
165:34 - that you need to understand for this
165:36 - particular service is how we are going
165:38 - to be persisting the data in our cues
165:40 - remember that if our instance fails we
165:43 - don't want to lose all of the messages
165:44 - that haven't been processed because then
165:47 - the users that uploaded those videos
165:49 - that produce those messages would just
165:51 - never hear back from us so basically
165:53 - what we want to do is we want to mount
165:55 - the physical storage on our local to The
165:57 - Container instance and if the container
165:59 - instance happens to die for whatever
166:01 - reason of course the storage volume that
166:03 - was mounted would remain intact then
166:05 - when a new pod is redeployed it will
166:07 - once again have that same physical
166:09 - storage mounted to it so let me show you
166:11 - what I mean by that to show you this
166:13 - I'll need to show you what the
166:14 - configuration file for our staple set is
166:17 - going to look like although we haven't
166:18 - written the code for this file yet just
166:21 - follow along so that you can understand
166:22 - where we're going with this so as you
166:24 - can see this configuration file follows
166:26 - a similar pattern to what all of the
166:28 - other configuration files did so I'm not
166:30 - going to go into detail about every
166:32 - single line if you need to please refer
166:35 - to the kubernetes API documentation that
166:37 - I introduced to you earlier but let's go
166:39 - ahead and have a look at containers here
166:42 - similar to our deployments this is going
166:44 - to determine the contain painter that
166:46 - gets spun up so the image we're using
166:48 - here is a rapidmq image but the part
166:50 - that we need to pay attention to starts
166:52 - here at this volume mounts Mount path so
166:55 - we want to mount a storage volume to our
166:58 - container right this Mount path is
167:00 - configuring where in our container we
167:02 - want the physical storage volume to
167:04 - mount to so basically anything that is
167:06 - saved in this VAR lib rabbitmq directory
167:09 - within our container we'll actually be
167:11 - getting saved to the physical storage
167:13 - volume that will persist even if the
167:15 - container fails and this particular
167:17 - directory is configured as the mount
167:19 - path for a reason this is actually where
167:21 - rabbitmq is going to store the cues when
167:24 - we create a durable cue and the messages
167:26 - when we configure them to be persistent
167:28 - and I'm going to go into detail about
167:30 - how we make the queue durable and the
167:32 - messages persistent a little bit later
167:33 - for now you just need to understand that
167:36 - we're mounting physical storage to this
167:38 - path and this is the path where rabbitmq
167:40 - will save cues and messages okay so now
167:43 - that that's out of the way the rest of
167:45 - the configure duration here under
167:46 - volumes is basically just the
167:48 - configuration for the physical volume to
167:50 - be mounted to the container for example
167:52 - we need to create an additional resource
167:54 - called a persistent volume claim and
167:57 - this config here basically links this
168:00 - staple set to the persistent volume
168:01 - claim that we're going to create and
168:04 - we're going to call that persistent
168:05 - volume claim rabbitmq PVC so what is a
168:09 - persistent volume claim in simple terms
168:12 - the persistent volume claim or PVC is
168:15 - going to be bound to a persistent volume
168:17 - and within the configuration for the
168:19 - persistent volume claim we'll set how
168:22 - much storage we want to make available
168:24 - to it from the persistent volume and the
168:27 - persistent volume will actually be what
168:29 - interacts with the actual physical
168:31 - storage and I know there are many layers
168:34 - of abstraction here but again for our
168:36 - purposes we really don't need to go into
168:38 - too much detail here all you really need
168:40 - to understand is that this configuration
168:43 - is going to make it so that the dura
168:45 - rabbitmq stores cues and messages will
168:48 - actually be persistent storage so
168:50 - whenever the Pod dies those cues and
168:53 - messages will be retained and the new
168:55 - pod will just reconnect to that
168:56 - persistent volume so let's go ahead and
168:59 - write up this configuration so now that
169:01 - we have an understanding what a staple
169:03 - set is we can go ahead and create a file
169:06 - called stateful set.yaml and actually
169:09 - we're going to want to make this in a
169:12 - manifest directory so we'll just make
169:14 - their manifest
169:17 - and CD into manifests and then we'll
169:20 - create the stateful set.yaml file
169:22 - in the here we'll set API version to
169:26 - apps V1 and this time kind is going to
169:29 - be stateful set
169:32 - and metadata
169:34 - will name
169:35 - rabbitmq and our spec configuration so
169:40 - the service name we're not going to use
169:42 - this
169:44 - so we'll just set it to not applicable
169:48 - and selector match labels as usual app
169:53 - rabbitmq now for our template and this
169:58 - is similar to the deployment it's going
170:00 - to be the template for our pods and
170:02 - we'll do metadata
170:04 - labels app is rabbit mq and our template
170:09 - spec or the Pod spec to be more specific
170:13 - through containers
170:15 - name
170:17 - rabbitmq and image is going to be rabbit
170:21 - mq
170:23 - three management and this is the
170:26 - official rabbitmq image and we're adding
170:29 - this management we're using the one that
170:31 - contains management because we want to
170:33 - have the graphical user interface to
170:35 - manage our cues as well included in the
170:37 - image
170:38 - and then we need to set ports and this
170:42 - container is going to need to include
170:44 - two ports we need the port to access the
170:46 - graphical user interface and we also
170:49 - need the port that handles the actual
170:51 - messages for example the messages that
170:54 - we send to the queue are going to be
170:55 - handled on a separate Port from the port
170:58 - that handles our connection to the
171:00 - graphical user interface and you'll see
171:02 - what I mean by that so we're going to
171:04 - set the name of this port the first port
171:07 - to http
171:09 - because we're going to use HTTP to
171:11 - access the graphical user interface
171:14 - and protocol
171:16 - will be TCP and container Port is going
171:21 - to be one five six seven two
171:24 - and we're also going to need a port for
171:27 - amqp which stands for advanced message
171:31 - queuing protocol and this is just the
171:33 - protocol that we use to send messages to
171:36 - the queue
171:39 - and container Port here is going to just
171:43 - be five six seven two
171:45 - and after ports we'll do environment
171:47 - from and we're still going to use config
171:50 - map
171:54 - and we'll call it rabbitmq config map
171:58 - and secret ref name will be rabbitmq
172:03 - secret
172:05 - and we're also going to need a key
172:07 - called volume mounts
172:10 - and we need to specify the mount path
172:13 - and this is going to be the path within
172:16 - the container that we want mounted
172:19 - so we'll do VAR
172:21 - lib
172:22 - rapid mq and the rabbitmq server is
172:26 - going to store the persisted data like
172:29 - the messages and the cues in this
172:31 - directory or at this path so we want to
172:34 - mount this path to our volume and our
172:37 - volume is essentially going to be
172:39 - storage that we connect to our
172:42 - kubernetes cluster which is where
172:44 - persisted data is going to be stored
172:47 - so name will be rabbitmq volume
172:55 - So within spec at the same level as
172:58 - containers
173:00 - we're going to do volumes
173:03 - name
173:05 - rabbit mq volume
173:08 - and we're going to use persistent
173:12 - volume claim and claim name will be
173:17 - rabbitmq PVC which we need to create and
173:21 - that's going to be it for this stateful
173:24 - set configuration
173:26 - so we can save that so now we need to
173:29 - create our persistent volume claim so
173:31 - we're going to do pvc.yaml
173:34 - and we'll do API version
173:37 - P1 and kind this time is going to be
173:40 - persistent
173:42 - volume claim
173:45 - and metadata
173:47 - we're going to do name rabbit mq PVC
173:51 - which we just referenced in our stateful
173:54 - set file and spec and access modes is
173:59 - going to be read write once
174:03 - resources requests
174:07 - storage
174:09 - one gigabyte and storage class
174:13 - name will be standard
174:17 - and we can save that and as usual we
174:21 - need to create our service.yaml
174:24 - API version V1 kind service
174:30 - metadata name of the service will be
174:34 - rabbitmq
174:35 - and our spec so type again is going to
174:39 - be cluster IP our service is only going
174:42 - to have an internal IP address which is
174:45 - only accessible within our cluster and
174:47 - selector
174:50 - app rabbitmq
174:52 - and ports
174:54 - remember that uh ports we're going to
174:57 - need to have the port for our graphical
174:59 - user interface so basically the rabbitmq
175:02 - like Management console and then we need
175:04 - the port for actual message transmission
175:06 - so we'll do ports name http
175:12 - protocol
175:13 - TCP and we're just going to do port
175:16 - 15672
175:21 - and we'll do Target Port is the same
175:25 - and then we'll do our port for our
175:28 - message transmission which will be amqp
175:31 - again and we'll do protocol
175:34 - TCP and this one's going to be Port 5672
175:40 - and Target board is the same port and we
175:43 - can save that and actually one second we
175:45 - need to go back in here so as you can
175:47 - see we're going to need to allow access
175:50 - to this port from a web browser so we're
175:53 - going to need to allow access from
175:55 - outside of the cluster directly to this
175:58 - port so that we can access rabbitmq's
176:00 - Management console so to do that we need
176:02 - to create an Ingress for this port as
176:05 - well
176:07 - so let's go ahead and quit and do Vim
176:09 - Ingress and we'll do
176:12 - API version
176:14 - networking.kh.iov1
176:19 - and kind is going to be Ingress
176:23 - and a data name rabbit mq Ingress
176:29 - and spec
176:31 - rules and host and we're going to do it
176:35 - at
176:37 - rabbitmqmanager.com which we need to
176:39 - configure in our Etc host file
176:43 - and it's going to be http
176:46 - aths and path
176:50 - type prefix
176:54 - back end
176:56 - service and the back end service is
176:59 - going to be rabbit in queue
177:01 - more specifically rabbitmq's port number
177:05 - 15672 which is the port number for the
177:08 - Management console
177:10 - so we can save that and we need to open
177:13 - this file again
177:17 - and we're basically going to do the same
177:19 - configuration but it's going to be for
177:22 - rabbit
177:24 - mqmanager.com I think that's what we
177:27 - called it
177:29 - yeah
177:30 - rabbitmqmanager.com
177:32 - so we can close that save this and close
177:34 - it
177:35 - and let's make a config map and I don't
177:38 - think we currently need any
177:40 - environment variables but let's just
177:43 - make a template
177:45 - just in case we need to add some
177:59 - and we'll do the same thing for a secret
178:23 - and that should be everything so let's
178:26 - go ahead and try and apply this
178:29 - so we have a couple of Errors the first
178:31 - one here is just a spelling error so
178:34 - config map I misspelled the key metadata
178:37 - so let's go in there and fix that
178:42 - meta data
178:44 - and let's apply again and in the
178:47 - service.yaml it's saying that the API
178:49 - version is not set so let's go and
178:53 - service.yaml and that's because I put
178:55 - ape version
178:57 - so API version and let's apply again
179:01 - and it says that
179:05 - stapleset.spec.templetunknown field
179:06 - volumes and that's because volumes
179:08 - should be at the same level as spec so
179:11 - let's go into stapleset.yaml
179:27 - actually the issue is the opposite of
179:29 - what I said volumes should be at the
179:31 - same level as container
179:33 - so it's under spec
179:36 - so we need to move this in one
179:42 - so let's save that
179:44 - and let's apply again and now it seems
179:47 - all of the objects were created
179:49 - successfully so let's go into canines
179:52 - and we see that our rabbitmq pod is
179:56 - pending and let's go have a look
180:00 - and it seems something's not working as
180:03 - expected so let's do
180:06 - subscribe pod rabbitmq and we have an
180:11 - event here warning failed scheduling one
180:14 - pod has Unbound immediate persistent
180:17 - volume claims
180:19 - okay so it seems there's an issue with
180:22 - our PVC so let's go ahead and do qctl
180:26 - describe
180:28 - PVC
180:30 - and we have another event here warning
180:33 - provisioning failed it says storage
180:36 - class storage uh stranded
180:39 - so it can't find the storage class
180:43 - because the storage class is standard
180:45 - and I have a typo so let's go ahead and
180:49 - Vim into our PVC file and here is the
180:54 - error it should be standard so let's
180:56 - save that
180:58 - and let's apply again
181:00 - and actually for persistent volume
181:03 - claims as said here the spec for this is
181:07 - immutable after creation so we're
181:09 - actually going to have to delete the
181:11 - resources created using these files and
181:14 - we only really need to delete the
181:16 - persistent volumes claim but it doesn't
181:17 - matter we'll just delete all of the
181:20 - resources created with this file or
181:23 - created with these files so basically
181:25 - we're going to use this Cube CTL delete
181:27 - command
181:28 - and we're going to have the flag f for
181:31 - files and we're going to delete all the
181:33 - resources created with the files within
181:35 - this directory so let's just delete them
181:37 - all and now that we're done deleting
181:39 - them we can just go ahead and apply them
181:41 - again
181:42 - and it seems they're created
181:44 - successfully so we can go into canines
181:47 - and it looks like the container is
181:50 - creating
181:54 - and it looks like everything is going as
181:57 - expected so let's leave the logs and
182:01 - leave the container so yeah now we have
182:03 - our rapidmq instance running within our
182:06 - kubernetes cluster
182:08 - let's go ahead and quit since we
182:09 - configured an Ingress for this and we
182:11 - configured this route
182:14 - in our Etc hosts file
182:17 - we should be able to access
182:19 - rabbitmqmanager.com and that should take
182:22 - us to the rabbitmq manager so let's try
182:26 - that
182:27 - so let's go ahead and go to
182:30 - rabbitmqmanager.com
182:33 - it's not working because we forgot to do
182:35 - mini Cube tunnel so let's go ahead and
182:38 - clear this and do mini Cube tunnel
182:41 - and as you can see here it's trying to
182:44 - start a tunnel service for both our
182:47 - Gateway Ingress and our rabbit mq
182:49 - Ingress so let's see if we can access
182:51 - the Management console now
182:54 - so let's just refresh this page and I'm
182:58 - just going to accept the risk and there
183:01 - we go we have access to our Management
183:04 - console and let me just go ahead and
183:06 - zoom in here so the username and the
183:09 - password for this should just be admin
183:13 - and login failed so maybe that's not the
183:17 - correct credentials
183:20 - so let's just go to Google and type in
183:24 - rabbitmq Management
183:26 - console default
183:29 - credentials
183:33 - and here it actually says the username
183:35 - and password are both guest so let's try
183:37 - that
183:38 - so we'll do guest and guest and there we
183:43 - go we are logged in zoomed in a little
183:45 - bit too much
183:46 - and this is what the Management console
183:49 - is going to look like
183:52 - and you don't need to get overwhelmed by
183:54 - all of this we're going to limit our
183:56 - Focus to just this cues section so for
184:00 - example we're going to create our cues
184:02 - here using this add a new queue
184:05 - and yeah so we're going to make use of
184:07 - two cues one of them is going to be
184:09 - called video which is going to be the
184:11 - queue where we put our video messages
184:13 - and let me just show you to give you a
184:16 - bit of a refresher so remember while
184:18 - we're actually using our Ingress we
184:21 - can't exit this so we're going to need
184:23 - to open up a new terminal or a new tab
184:26 - so I'll just open a new tab
184:28 - and I'll zoom in here
184:31 - and I'll just change directory to
184:34 - system design python Source rabbits
184:40 - and actually what I wanted to show you
184:42 - is in the Gateway directory so
184:48 - in our server.pi when we upload
184:55 - we use this util.upload and in here
184:59 - we're putting the message on the Queue
185:01 - called video so routing key is just the
185:04 - queue so in the console here we actually
185:07 - need to create that cue so we're going
185:09 - to create a cue called video and this
185:11 - durability needs to be set to durable
185:13 - because if it's transient then that
185:16 - means that if the container is restarted
185:19 - or shut down or anything the queues no
185:21 - longer going to exist you're going to
185:22 - need to create it again so durable means
185:25 - that the queue will be essentially
185:27 - persisted so if the container restarts
185:29 - or something the queue will still exist
185:31 - afterwards
185:32 - so we'll just add this cue
185:36 - so now we have our video queue here
185:39 - so now let's see if we can start up our
185:41 - Gateway server
185:45 - so we can just quit here
185:48 - and canines
185:50 - quickly
185:52 - so yeah we want to spit up our Gateway
185:55 - service so we'll go ahead and let's just
186:00 - change directory into Gateway manifest
186:04 - and we'll do Cube CTL apply we'll apply
186:07 - all of the files in this directory
186:09 - and let's do canines again
186:12 - and as you can see now our Gateway
186:14 - service is able to start up with no
186:17 - issues
186:19 - so at this point we have our Gateway
186:21 - service and our off service and our
186:24 - rabbitmq queue service up and running
186:26 - within our cluster so that means that
186:28 - right now we can upload files and
186:31 - messages for those uploads will be added
186:33 - to the queue and at this point we have
186:35 - no consumer service to consume the
186:38 - messages from the queue to actually
186:39 - convert the files so we need to create
186:41 - an additional service and this service
186:43 - is going to pull messages off of the
186:45 - queue that the Gateway adds to the queue
186:48 - and it's going to convert them into MP3
186:51 - and then it's going to store the MP3 and
186:53 - mongodb and put a message onto another
186:55 - queue called MP3 which will have another
186:58 - Downstream service pull from but I don't
187:00 - want to confuse you all too much so
187:02 - let's just do this step by step
187:05 - so we can leave this
187:07 - and we need to go back to our source
187:12 - directory because we need to create
187:13 - another service so we're going to make
187:16 - dur converter and this converter service
187:20 - is going to convert videos to MP3 so
187:23 - this is going to be the consumer service
187:25 - that pulls the messages off the queue so
187:28 - it knows which videos it needs to
187:30 - convert and where they're stored Etc so
187:33 - we'll make this directory and we'll just
187:35 - CD into that directory and let's clear
187:37 - and we're going to make a file called
187:40 - consumer.pi
187:42 - and in this file we're going to import
187:45 - Pica of course because we need to pull
187:47 - the messages off the queue sys OS time
187:51 - and from PI we need to import
187:55 -  client
187:59 - and that's not how you spell import
188:03 - and we also need to import grid FS
188:06 - because we need to take the files from
188:08 - mongodb to video files and we also need
188:11 - to upload the MP3 files to mongodb and
188:14 - from convert which is a package that
188:17 - we're going to create ourselves we're
188:18 - going to import to MP3 which is going to
188:21 - be of a module within that package
188:24 - and we're just going to define a
188:26 - function called Main and our client is
188:29 - going to be equal to mongodb client and
188:32 - it's going to be our mongodb host which
188:36 - is on our local machine it's not
188:38 - deployed in our cluster remember so we
188:40 - need to use this host
188:42 - mini Cube internal which basically gives
188:45 - us access to our host systems local
188:48 - environment and the port for mongodb is
188:53 - 27017
188:55 - and we're going to do DB Videos equals
189:00 - client dot videos so this instance of
189:03 -  client is going to give us access
189:05 - to the DBS that we have in our
189:09 - database so we can do DB MP3s
189:13 - equals client Dot
189:15 - MP3s so these databases will exist
189:19 - within our mongodb
189:22 - and then we need our grid FS stuff
189:25 - so we'll do FS videos
189:29 - equals an instance of grid FS
189:34 - which we need to pass our DB Videos to
189:39 - and fsmp3s we need to do the same thing
189:49 - and now we need to configure our rabbit
189:52 - mq connection
189:55 - so connection will equal
189:58 - Pika dot blocking connection just like
190:00 - before
190:10 - and we'll do Pica dot connection
190:13 - [Music]
190:15 - parameters
190:17 - host equals rabbit mq
190:21 - and this is possible because our service
190:24 - name is rabbit and Q and our service
190:26 - name will resolve to the host IP for our
190:30 - rabbitmq service
190:33 - and channel will equal
190:36 - connection.channel
190:38 - so what we need to do is we need to
190:41 - create a configuration to consume the
190:44 - messages from our video queue and to do
190:47 - that we're going to use this Channel and
190:50 - basic consume
190:53 - and let's save this
190:59 - and the arguments that we need to pass
191:01 - to this basic consume is our q and we're
191:04 - going to get the Q name from the
191:06 - environment so the queue that we want to
191:07 - consume from in this case is the video
191:09 - queue but just in case we want to change
191:12 - it in the future we're going to just
191:13 - have an environment variable that
191:15 - contains our Q configuration so we'll do
191:17 - OS Dot
191:19 - environ.git and we'll name the
191:22 - environment variable video queue
191:25 - so that's going to be our q and we're
191:27 - going to need to create a callback
191:29 - function that gets executed whenever a
191:32 - message is pulled off of the queue so
191:34 - we'll say on message
191:36 - callback equals callback and we need to
191:40 - create this callback function so we'll
191:42 - go up here and Define callback
191:45 - so whenever a message is taken off the
191:49 - queue by this consumer service this
191:51 - callback function is going to be
191:52 - executed
191:54 - and it's going to be executed with the
191:56 - parameters Channel method
192:00 - properties and body
192:04 - and what we want to do when we get the
192:06 - message is we want to convert the video
192:08 - to MP3 so we'll do two MP3 dot start
192:13 - so there's going to be a function in our
192:15 - two MP3 module called start and we're
192:19 - going to pass in the body of our message
192:21 - and we're going to pass in FS videos and
192:24 - Fs mp3s and the channel and when we
192:29 - create this function you're going to see
192:30 - why we're doing all of this but for now
192:32 - we're just creating the Callback
192:34 - function that's going to call this
192:36 - function so we're going to set the
192:38 - result to error and if there is an error
192:41 - so if error so if there is an error we
192:43 - want to send a negative acknowledgment
192:46 - to the channel
192:48 - so we'll do basic
192:50 - Knack which stands for negative
192:52 - acknowledgment which basically means
192:54 - that we're not going to acknowledge that
192:57 - we've received and processed the message
192:58 - so the message won't be removed from the
193:01 - queue because we want to keep messages
193:04 - on the Queue if there's a failure to
193:06 - process them so they can be processed
193:08 - later by another process and here we're
193:10 - going to do delivery
193:12 - tag equals method dot delivery
193:18 - tag
193:19 - and this delivery tag uniquely
193:22 - identifies the delivery on a channel so
193:25 - when we send this negative
193:26 - acknowledgment to the Channel with the
193:29 - delivery tag rabbitmq knows which
193:31 - delivery tag or which message hasn't
193:34 - been acknowledged so it'll know not to
193:36 - remove that message from the queue but
193:39 - on the other hand if the error is none
193:41 - then that means there wasn't an issue
193:43 - with the conversion
193:44 - so we'll just go ahead and acknowledge
193:47 - the message so we'll do basic
193:50 - pack for acknowledgment and same thing
193:53 - we're going to do
193:55 - delivery Tech is method.deliverytag and
193:59 - if you see here method is one of the
194:01 - parameters that's passed to the Callback
194:03 - function and that's how we're keeping
194:06 - track of the delivery tag so that's it
194:08 - for our callback function and let's go
194:11 - ahead and format
194:12 - and we'll go ahead and print a message
194:16 - that just says waiting for messages
194:20 - and we can also put to exit press
194:24 - control plus c
194:29 - and basically once we run this uh main
194:32 - function here this is going to be
194:34 - printed and then we're going to do
194:36 - Channel dot start consuming
194:40 - and this start consuming is essentially
194:43 - going to run our consumer so our
194:45 - consumer is going to be listening to the
194:47 - queue or listening on that channel where
194:50 - our video messages are being put so we
194:53 - need to do if name equals
194:57 - Main
194:59 - we're going to try and run our main
195:02 - function
195:03 - and we're going to do accept
195:06 - keyboard interrupt
195:08 - so our main function is going to run
195:10 - until we press Ctrl C and interrupt the
195:14 - process
195:16 - and once that process is interrupted
195:18 - with bias pressing Ctrl C then that
195:21 - event is going to be captured in this
195:23 - try except and we're going to catch the
195:26 - keyboard interrupt which is US pressing
195:28 - Ctrl C to cancel the consumer process
195:31 - and in that case we'll just print
195:34 - interrupted
195:37 - and we'll try sis.exit
195:42 - and
195:44 - we'll accept
195:46 - system exit and then we'll do OS dot
195:50 - exit
195:53 - zero and this is basically us just
195:55 - gracefully shutting down the actual
195:58 - service in the event that we do a
196:00 - keyboard interrupt
196:02 - and that is going to be it for our
196:05 - consumer so we need to go and create
196:08 - this convert package and this two MP3
196:11 - module and we also need to install some
196:14 - things and we also forgot to create a
196:16 - virtual environment so let's go ahead
196:18 - and save this
196:20 - and let's go ahead and do python
196:23 - 3
196:24 - p e and B
196:27 - and now we can activate our virtual
196:30 - environment
196:31 - so
196:34 - Source then activate
196:37 - and we now are using our converter
196:40 - virtual environment and now we can just
196:43 - cat consumer
196:45 - dot pi
196:49 - or maybe it's better to do cat
196:52 - head
196:53 - in 10. so we need to install some of
196:57 - these dependencies so we'll do fifth
197:01 - three install
197:02 - Pica and Pi
197:06 - and actually I don't need the comma
197:08 - there so pip3 install Pika and Pi
197:13 - and let's go in here
197:16 - and it looks like we're good to go so
197:18 - let's clear so now we need to create our
197:21 - convert package so we'll just make dare
197:23 - convert
197:25 - and change directory into convert and we
197:28 - need to create the init.pi file and we
197:31 - need to create a module called to MP3
197:35 - and then we're going to need to import a
197:38 - couple of things so we'll import Pica as
197:40 - usual and we need to import Json as well
197:43 - and we need to import temp file which I
197:46 - will show you what that's going to be
197:47 - used for in a second and Os as well and
197:50 - we need to import this binary Json dot
197:54 - object ID
197:57 - and from there we need to import object
198:01 - ID and I'll show you what this is doing
198:03 - or what this is going to be used for
198:05 - soon as well
198:07 - and lastly we need to import this movie
198:10 - Pi editor
198:12 - which we need to install
198:15 - and we're going to define a function
198:18 - called start and it's going to take in a
198:21 - message a gridfs videos instance a grid
198:26 - FS MP3s instance
198:29 - and a channel
198:31 - and for now let's pass so we can go
198:34 - ahead and recap
198:37 - so just to recap if we go into our
198:40 - consumer.pi file
198:42 - we're importing this module that we just
198:44 - created
198:46 - to MP3 and this two MP3 module contains
198:50 - the start function which is the one that
198:52 - we're creating now
199:02 - so we're going to be using this movie
199:05 - pie
199:06 - to convert our videos to MP3 so we're
199:10 - going to need to install pip install
199:11 - movie pi
199:22 - and from there we can start writing the
199:24 - code for our start function
199:27 - so the first thing that we're going to
199:29 - need to do is we're going to need to
199:31 - load our message
199:37 - which is essentially going to make it
199:40 - into a python object
199:43 - so let me just install something really
199:45 - quick
199:55 - so we're going to deserialize an
199:58 - instance containing a Json document to a
200:01 - python object
200:02 - so at this point our message contains
200:05 - the python object version of our message
200:07 - and the first thing that we want to do
200:09 - before converting the file is we want to
200:12 - create an empty temporary file and we're
200:15 - going to write our video contents to
200:18 - that temporary file so we'll do empty
200:21 - temp file is TF so we'll do TF equals
200:25 - temp file dot named temporary file and
200:29 - as you can see here it says create and
200:32 - return a temporary file and this
200:34 - temporary file is going to be a named
200:36 - temporary file as opposed to a temporary
200:39 - file that does not have a name
200:42 - so if we go to this definition here
200:46 - you can see that this temporary file has
200:49 - a name attribute where you can access
200:51 - the file's name so we're going to create
200:54 - this named temporary file and it's
200:57 - essentially going to create a temp file
200:59 - in attempt directory and we can use that
201:02 - temp file to write our video data to
201:06 - so we'll do video contents
201:08 - and we'll set out equal to FS videos dot
201:13 - get so now we're going to get our video
201:16 - file from grid fs and we're going to
201:19 - have it in this out variable or this out
201:22 - object is going to have a read method so
201:25 - we'll be able to write the data that's
201:27 - stored in this out variable to the file
201:30 - so we need to do object ID
201:34 - and message
201:37 - video fid
201:40 - which if you remember from our Gateway
201:43 - service
201:45 - in the actual storage util function
201:52 - we have video FID set to the file ID
201:55 - that was given to us after we put the
201:58 - video file into mongodb
202:02 - but if you also remember we have to
202:04 - convert that into a string because the
202:07 - FID that comes from the return value of
202:10 - this fs.put is actually an ID object
202:14 - so we needed to convert it into a string
202:16 - to put it into our message
202:19 - so we're taking our string version of
202:22 - our FID and converting it into an object
202:24 - and then we need to use that object
202:26 - version to get the file from our mongodb
202:30 - we can't get the file using the string
202:32 - version of the ID
202:33 - so just really quick let's go ahead and
202:36 - save that actually let's go back in here
202:39 - and for some reason it says no name
202:42 - object ID so
202:45 - let's see
202:56 - and I guess that's just an error so
203:02 - so once we have the video file data we
203:06 - can add video contents to empty file and
203:11 - we're going to do that by taking the
203:13 - empty file which is the TF variable and
203:16 - we're going to write to that file the
203:19 - data that's returned from this read
203:21 - method and this is the read method on
203:25 - the out variable or the out object as
203:29 - this read method which allows us to read
203:31 - the data stored in out
203:35 - so if we go to the definition here we
203:37 - can see that we can read the bytes from
203:39 - the file so the bytes that are returned
203:41 - from this read method here are going to
203:44 - be written to our temporary file here
203:48 - and then what we want to do is we want
203:50 - to convert our video file into audio so
203:54 - our temp file currently has the video
203:56 - file so we'll do create audio from temp
204:00 - video file
204:02 - so audio is going to equal moviepi dot
204:07 - editor Dot video file clip and then
204:12 - we're going to take the TF or the
204:14 - temporary file name
204:15 - which is actually going to resolve to
204:18 - the path of the temporary file and we're
204:21 - going to extract the audio from that
204:23 - file so all of this is going to resolve
204:28 - to our audio file being stored in this
204:30 - audio variable
204:32 - and the last thing we need to do is we
204:34 - need to close our temp file and with
204:37 - this temp file module here the temp file
204:40 - will automatically be deleted after
204:42 - we're done with it so basically after we
204:45 - close the file it will automatically be
204:47 - deleted so we don't need to worry about
204:48 - cleaning that up so now that we've
204:50 - extracted the audio into the audio
204:52 - variable we need to write the audio to
204:56 - the file or to its own file and we're
204:59 - going to do that by first creating a
205:02 - path for our audio file so we'll do temp
205:05 - file path equals temp file dot get
205:09 - tempter and this is going to give us the
205:11 - directory on our OS where the temp files
205:14 - are being stored by this temp file
205:16 - module so we'll take that dur and we'll
205:19 - add it to our desired file name
205:22 - so we're going to name it the video file
205:25 - ID which we'll take from message
205:28 - video file ID
205:31 - .mp3 so what we're doing here is we're
205:34 - first taking the path to our temp
205:37 - directory and we're appending our
205:40 - desired MP3 file name to that path so
205:43 - we're left with the full path to the
205:45 - file and we want to name the MP3 file
205:49 - just the file ID of the video because
205:51 - it's going to be a unique ID so in this
205:53 - case we won't have to worry about
205:55 - collisions with file names because every
205:58 - video file is going to have a unique ID
206:00 - and that's going to be the name of the
206:02 - MP3 file as well so then we want to do
206:05 - audio Dot right audio file and we're
206:09 - going to write the file to the path that
206:12 - we just created so we're taking our
206:14 - audio file that was created using this
206:16 - movie pi and this object is going to
206:19 - have a method called Write audio file
206:21 - and we basically just need to tell this
206:24 - right audiophile method where we want to
206:27 - write the file and what we want to name
206:29 - the file so that's why we're just
206:30 - passing this path that we created here
206:33 - and once the audio file is created the
206:36 - temporary audio file we can save the
206:39 - file to
206:40 - so first we need to open the file
206:43 - so we'll open the file at the path we
206:46 - just created and we just want to read
206:48 - the file and we'll set data equal to
206:51 - that file that we opened dot read and
206:55 - then we'll set file ID equal to
206:58 - fsmp3s dot put and then we're just going
207:02 - to put that data that we extracted from
207:04 - the file so we're storing our MP3 file
207:08 - in our grid fs and at this point we
207:10 - don't need that temp file anymore so
207:12 - when we do F Dot close
207:15 - we also need to go ahead and do OS dot
207:19 - remove
207:21 - TF path because remember in this case
207:24 - this write audio file method created the
207:27 - temporary file and not this temp file
207:29 - module so we have to actually delete
207:31 - this temp file manually ourselves
207:34 - and lastly we need to update our message
207:37 - and remember we have this MP3 FID in our
207:43 - message and we want to set that equal to
207:46 - string version of the FID object that we
207:50 - got from uploading the MP3 to mongodb
207:54 - and lastly we need to put this message
207:57 - on a new cue or a different queue that
207:59 - we need to create called the MP3 queue
208:03 - so we'll do try Channel Dot
208:07 - basic publish
208:15 - and we'll use the default exchange Again
208:17 - by just putting an empty string and our
208:20 - routing key it's going to be mp3 for the
208:23 - MP3 queue but remember we're going to
208:25 - get those from the environment the names
208:28 - of our cues from the environment so
208:30 - we'll just call this environment
208:31 - variable MP3 Q
208:36 - and our body of course is going to be
208:39 - Json dot dumps because we need to
208:42 - convert the python object into Json and
208:46 - our message will be the input
208:49 - and we need to make sure the message is
208:52 - persisted until it's processed so we'll
208:55 - do
208:56 - pica.basic properties
209:00 - and we're going to once again like
209:02 - before set the delivery mode equal to
209:06 - pica.spec dot persistent delivery mode
209:13 - and if we can't put the message on our
209:16 - queue
209:17 - and let's just catch the exception as an
209:20 - error
209:22 - just in case we need that variable and
209:25 - then we'll do FS MP3s dot delete
209:30 - FID so basically if we can't
209:33 - successfully put the message on the
209:35 - Queue saying that there's an MP3
209:37 - available for that message then we want
209:40 - to delete the actual MP3 from mongodb as
209:43 - well because if we don't put the message
209:45 - on the Queue then the file in mongodb
209:48 - the MP3 will never get processed anyway
209:50 - so we need to make sure we remove the
209:52 - MP3 from mongodb if we can't add the
209:56 - message to the queue and in that case
209:58 - we're just going to return
210:00 - failed to publish message
210:04 - and the reason this will work is because
210:08 - let's go ahead and
210:10 - save this
210:11 - if we go back into our consumer.pi file
210:16 - if you remember
210:19 - if this start function here fails then
210:22 - we're going to return an error and that
210:25 - error is going to be stored in this
210:27 - error variable and if there is an error
210:30 - then we're going to send a negative
210:32 - acknowledgment for the actual message
210:35 - that's on our video queue so that means
210:37 - that that message will not be removed
210:40 - from the queue and it can be processed
210:41 - again later so that's why we need the
210:44 - actual start method to fail completely
210:46 - because we're going to basically attempt
210:48 - this whole function again if something
210:51 - goes wrong
210:53 - so that's why we need to delete the file
210:56 - from mongodb if we can't put the message
210:59 - onto the queue
211:05 - and let's just go ahead and format
211:08 - and we can save that and that's going to
211:11 - be it for our consumer
211:13 - so what we need to do now is we need to
211:16 - create our Docker file and our
211:19 - kubernetes configuration to create the
211:22 - service within our cluster so let's pip
211:25 - freeze our requirements into a
211:28 - requirements.txt file as usual and then
211:32 - we'll create a Docker file and this
211:34 - Docker file is once again going to be
211:37 - the same as the previous Docker file
211:39 - so we'll just copy this and paste it
211:43 - here
211:45 - and let's just save that
211:49 - and we're going to need to add something
211:51 - additional here called
211:53 - ffmpeg which is just a dependency for
211:56 - the movie Pi module and we need to
212:00 - change this to consumer.pi
212:03 - and we need to change our exposed to I
212:06 - think Port 3000
212:08 - and let's go ahead and save that
212:10 - actually I made a mistake since this is
212:13 - a consumer we're not going to expose any
212:15 - port
212:16 - because it's not going to be a service
212:19 - that we're making requests to this
212:21 - service is going to consume messages
212:23 - from a queue
212:24 - so it's going to act on its own so we'll
212:27 - save that and let's do Docker build
212:46 - and once that's done we can do Docker
212:48 - tag
212:50 - and we'll just take a piece of this
212:53 - and we're going to use your username for
212:57 - your Docker Hub account and this time
213:00 - we'll call it converter
213:01 - and latest
213:03 - and then we can do Docker push
213:08 - converter latest
213:21 - and now if you go to your Docker Hub
213:24 - account
213:25 - foreign
213:29 - you should see that you now have a
213:31 - converter repository as well
213:36 - and you should have the tag latest here
213:38 - as well
213:39 - so now we can make our manifest
213:41 - directory
213:45 - and for this one we just need to create
213:47 - a yaml file for
213:50 - the deployment
213:52 - and the secret and the config map so
213:55 - we're not going to need to create a
213:57 - service configuration for this one
213:59 - so we'll do API version
214:02 - apps V1 kind deployment
214:06 - metadata
214:08 - name is converter
214:11 - labels app converter
214:15 - spec
214:17 - and we'll do four replicas for this one
214:19 - and selector
214:21 - match labels
214:24 - app converter
214:27 - and strategy is going to be type rolling
214:31 - update
214:33 - and rolling update
214:35 - Max surge we'll just double the number
214:39 - of replicas
214:42 - and now for the template
214:46 - labels app converter
214:55 - and we'll do spec
214:57 - actually spec should go back here
215:03 - and containers
215:06 - name equals converter
215:09 - and image is going to be your username
215:13 - converter
215:14 - and environment from will be
215:18 - config map reference
215:21 - and name is going to be converter config
215:24 - map which we'll create
215:27 - and we'll also do Secret ref
215:30 - which will have the name Secret
215:35 - or converter
215:37 - Secret
215:39 - and that's going to be it for that
215:44 - so let's save and we'll create our
215:47 - config map
215:49 - and we'll set API version to V1
215:54 - kind config map
215:57 - metadata name
215:59 - converter config map
216:02 - and data so we need our mp3q name
216:07 - because remember we're using the
216:10 - environment variable to select the queue
216:12 - in our code and we need our video queue
216:16 - name
216:18 - and actually while we're doing this we
216:20 - need to go create our mp3q
216:23 - so back in our Management console
216:27 - we have our video Cube but we need to
216:30 - create another queue so add new q and
216:33 - we're just going to name it MP3 and it's
216:35 - going to be a durable queue as well and
216:37 - we'll just add Q
216:38 - so now we have both the MP3 queue and
216:41 - the video queue
216:42 - and we can save this now we need to just
216:45 - create our secret.yaml and we don't have
216:48 - any secrets for this service so this is
216:50 - just going to be a template file for now
217:13 - foreign
217:16 - let's save that and let's make some
217:18 - space
217:19 - so now let's go into canines and check
217:23 - to see what we have running so far so we
217:25 - have our auth service our Gateway and
217:28 - our queue
217:30 - so now we're trying to deploy our
217:33 - consumer which is the converter
217:35 - so we'll just apply all the files in the
217:39 - current directory and it seems
217:41 - everything was created so let's see if
217:43 - we run into any issues
217:57 - let's see if we can get a better view of
217:59 - the logs so we'll do Cube CTL
218:03 - logs follow
218:05 - and just select one of these converters
218:09 - and we're getting no module named binary
218:12 - Json dot object ID so I must be pretty
218:16 - sleepy because I don't know why I didn't
218:18 - see this before but object ID here is
218:21 - clearly missing a b
218:23 - so let's go into our
218:27 - convert
218:29 - to MP3 file
218:32 - and go over here and change this to
218:36 - object ID
218:39 - and we'll save that
218:41 - change directory and just to double
218:46 - check let's check to see if there's a
218:49 - linting error
218:51 - and go to definition so we're good to go
218:54 - so we need to rebuild the docker file so
218:56 - we'll do Docker build
219:04 - and we need to update the repository so
219:07 - we need to tag it again first of all
219:13 - foreign
219:18 - Docker push converter latest
219:30 - and now that that's pushed let's once
219:33 - again try to apply our configuration
219:35 - that's in our manifest directory
219:40 - and let's check canines
219:47 - hmm actually let's try to let's first
219:52 - um delete
219:54 - and let's see if we can clear this out
219:57 - really quick
219:58 - and now let's
220:00 - let's check to see if they're closing or
220:03 - shutting down
220:04 - okay we're good so now let's try Cube
220:07 - CTL apply
220:11 - and let's check canines
220:14 - and let's see
220:18 - well we got one two and three running
220:22 - and four running so that's good news all
220:26 - right so at this point we have our auth
220:29 - service deployed we have uh multiple
220:32 - instances of the converter service
220:34 - deployed to pull from our rabbitmq which
220:37 - we have deployed as well and we have our
220:38 - Gateway deployed so at this point we can
220:41 - see if uploading files results in
220:44 - messages getting put on the Queue and we
220:47 - can also see if those messages are being
220:49 - pulled off of the queue by our converter
220:51 - service and that'll probably be the most
220:55 - difficult part to get configured because
220:57 - once we have our uploads resulting in
221:00 - the proper messages being put on the
221:02 - Queue and the converter service
221:04 - consuming those messages and converting
221:06 - the videos and storing them in mongodb
221:08 - that's pretty much the entire
221:10 - functionality of this microservice
221:12 - application so at that point we'll just
221:15 - be creating a service to send
221:17 - notifications when in p3s are finished
221:19 - being converted or when videos are
221:21 - finished being converted to MP3 so let's
221:24 - test the end-to-end functionality of
221:26 - uploading and having those uploads be
221:29 - converted
221:30 - so we can quit this and let's clear and
221:34 - we still want our tunnel to be running
221:36 - so make sure that your mini Cube tunnel
221:39 - command is still running and you have
221:41 - another tab opened to work with whatever
221:44 - it is that we're working with at the
221:46 - moment and we want to test by uploading
221:49 - a video file and when we upload that
221:51 - video file we want to see that the
221:55 - message gets added to our video queue
221:57 - and then gets removed from our video
221:59 - queue and another message gets added to
222:02 - our MP3 queue and we don't have a
222:05 - consuming service to consume the mp3q
222:08 - messages so all the messages should just
222:10 - be piling up at the mp3q if our
222:13 - end-to-end functionality is working as
222:15 - expected and also if it's working as
222:17 - expected we should be able to download a
222:20 - converted video file which would just be
222:23 - an MP3 file from our mongodb so let's go
222:26 - ahead and try to test that
222:29 - and I actually don't have Postman or
222:32 - anything like that installed on this
222:34 - laptop at the moment so I'm just going
222:35 - to use Curl to test this but if you're
222:38 - familiar with Postman you can use that
222:39 - to test as well if not just follow along
222:42 - with the commands that I use and make
222:45 - sure you have curl installed of course
222:48 - and at this point we can just go on
222:50 - YouTube and download a video and we
222:54 - don't want the video to take too long to
222:55 - download so we'll just do something
222:57 - short so this one's 37 seconds
223:00 - Mark Zuckerberg says he's not a lizard
223:04 - so we'll just go ahead and take this and
223:08 - copy the link to the video and I'm going
223:12 - to use this YouTube download tool to
223:15 - actually download the video from YouTube
223:17 - so if you want you can just do Brew
223:20 - install YouTube DL
223:26 - and of course it's already installed for
223:28 - me but that's just in case you want to
223:30 - do the same thing to get a video or you
223:32 - can use whatever video you have on your
223:33 - machine already so I'll just do a
223:36 - YouTube download and then paste in the
223:39 - URL for the video
223:42 - and for some reason that video doesn't
223:45 - work so let's just try a different one
223:47 - so yeah sorry Mark
223:50 - and this video is relatively short so
223:54 - let's go ahead and do this one
223:56 - we'll just take the URL
224:10 - and once that's finished downloading you
224:13 - should have the file in your current
224:15 - directory and just really quick we're
224:18 - going to go into our mySQL database
224:25 - and we're going to show tables
224:31 - actually we need to use database auth
224:35 - and I spelled database wrong
224:38 - and actually it should just pu's auth so
224:42 - now we can just show tables and let's
224:44 - just select all from user
224:52 - select all from user now when you do
224:56 - this you should have credentials here
224:58 - from when we created the database and we
225:01 - used our SQL script to create the user
225:05 - in the beginning of this video so you
225:07 - should have an email and a password and
225:10 - these are the credentials that we're
225:12 - going to use in our basic auth when we
225:15 - send a request to our login endpoint to
225:17 - get a Json web token to upload the video
225:20 - so I'm just going to exit this and I'm
225:23 - going to send a curl request and it's
225:26 - going to be a post request and I'm going
225:28 - to send it to http
225:32 - MP3
225:33 - converter.com because remember our
225:36 - Gateway Ingress resolves this host name
225:38 - and we configured this hostname to
225:41 - resolve to our local host or our
225:44 - loopback address so we're just going to
225:46 - use this when we send requests to our
225:48 - Gateway and it's going to be the login
225:50 - endpoint
225:52 - and in curl you can just do this u-flag
225:55 - to do basic authentication credentials
225:58 - and I'll just do Giorgio
226:00 - email.com
226:02 - and the password is admin123 for me
226:07 - and we're actually getting an internal
226:09 - server error so let's go into canines
226:12 - and check our Gateway logs and that's
226:16 - too small so I'll just do Cube CTL logs
226:20 - f
226:21 - Gateway
226:23 - and it's saying in alt service access
226:26 - dot pylon 16 object has no attribute txt
226:29 - so we can just go into that file so we
226:33 - need to change actually we can just go
226:36 - into the file from here
226:42 - so it's line 16.
226:54 - yeah so I'm just going to change
226:56 - directory
227:08 - and I'll activate this virtual
227:10 - environment
227:17 - foreign
227:33 - and it's because the response is
227:36 - response.txt
227:38 - and not response.txt
227:44 - and let's just see if there's any more
227:49 - and let's check the directory for txt
227:53 - and we should exclude our virtual
227:57 - environment
227:59 - so invalidate.pi
228:09 - so we're going to need to change all of
228:11 - these
228:12 - so
228:14 - I hope there's not too many sorry about
228:16 - that but of course this requirements.txt
228:19 - is supposed to be txt and maybe we
228:24 - should check all of the service
228:26 - directories because I don't remember
228:30 - it should be ignoring the virtual
228:32 - environment there so yeah let's just see
228:35 - how it goes
228:37 - so we need to go back into the gateway
228:40 - and we need to do Docker build again
228:50 - and then we need to do
228:52 - cker tag
229:00 - and this is for the Gateway
229:03 - and then Docker push
229:15 - and we'll just delete all of our Gateway
229:19 - resources
229:21 - and just recreate them
229:31 - and it looks like those are up and
229:33 - running
229:36 - so let's try to get our token again
229:40 - and now we were able to successfully get
229:43 - our JWT
229:45 - so we can just copy this JWT
229:48 - and let's go back to the directory where
229:50 - our video file is so that's in
229:59 - converter
230:00 - and here we need to do curl and this is
230:03 - going to be a post request as well
230:06 - but this time we need to add our file
230:09 - and actually to make this easier let's
230:11 - change the name of this file so we'll
230:13 - move this
230:15 - to test.nkv
230:19 - now we can do curl x post
230:24 - file equals at test.nkv
230:31 - and we can't forget the header which
230:34 - needs to be authorization
230:37 - and remember it has to be a bear token
230:40 - and then we'll paste in the token
230:43 - and that's going to go to our
230:47 - MP3
230:49 - converter.com
230:51 - upload and let's see what happens
230:54 - and we are getting 403 Forbidden so
230:59 - let's go ahead and go into canines
231:03 - and maybe that's coming from our auth
231:05 - service
231:08 - let's just check the Gateway nothing
231:12 - hmm so actually it looks like we're
231:16 - using the wrong uh host name here we
231:19 - have MP3 convert but it should be
231:23 - mp3converter.com let's go ahead and sudo
231:26 - Vim our hosts
231:31 - yeah so it should be mp3converter.com so
231:34 - let's go ahead and try it with that
231:39 - and let's make sure our tunnel is still
231:42 - running
231:44 - and let's see
231:47 - okay so now we're getting an actual
231:50 - error from the actual Services I believe
231:53 - so let's go into canines
231:57 - and we can assume that that error came
232:00 - from the Gateway server all right so
232:03 - we're hitting the server now with our
232:05 - upload and maybe our Gateway server is
232:10 - getting an error from auth
232:15 - but it seems auth is returning a 200 for
232:18 - the validate so if we're getting the 200
232:20 - from the validate endpoint on the auth
232:23 - service
232:25 - then that means that our Gateway
232:30 - is returning a an internal server error
232:34 - when
232:36 - we get a 200 from validate
232:38 - so let's change directory into our
232:42 - Gateway
232:44 - and go into server.pi
232:47 - and let's go to validate so for the
232:50 - upload endpoint
232:52 - we validate and at some point
232:55 - I guess we're returning a 500 but
233:00 - hmm
233:01 - that's a bit strange
233:04 - try and
233:07 - let's get another token really quick
233:16 - and let's try with the new token
233:32 - uh and that's because we aren't in the
233:35 - directory of our file so let's change
233:37 - directory back to converter where our
233:40 - test file is and let's try this again
233:44 - and still an internal server error
234:09 - so yeah it's a debug this so I'm just
234:11 - going to scale all of our services down
234:13 - to just one replica so that we don't
234:15 - have to check multiple replicas for the
234:19 - logs
234:20 - so we can just do Cube CTL scale
234:24 - and we're going to scale the deployment
234:28 - Gateway
234:30 - down to one
234:33 - and we also need to scale our converter
234:38 - converter down to one and we also need
234:42 - to scale our auth service down to one
234:46 - so when we go into canines now
234:50 - you can see that we're terminating all
234:52 - of these extra replicas and we're going
234:55 - to have one auth service running and one
234:58 - converter running in one Gateway running
235:04 - so once that's done terminating
235:07 - we can go ahead and I'm just going to
235:10 - open some extra tabs here and run the
235:14 - logs for our Gateway
235:21 - and
235:24 - same for our auth service
235:30 - and the same for our converter
235:40 - that's strange there's nothing printing
235:43 - for the converter so let me see
235:45 - something
236:14 - foreign
236:50 - [Music]
236:53 - and check the logs so the Gateway is
236:57 - what's returning the 500
237:00 - validates returning a 200 so the
237:02 - validation is going through
237:05 - and we're not getting anything on the
237:07 - converter
237:10 - so if the validation is completing
237:13 - but the Gateway is still returning to
237:15 - 500 then that means that something's
237:17 - happening between validation and
237:21 - actually adding the message and
237:23 - uploading the file so if we go into our
237:27 - Gateway server.pi file
237:31 - we can go to validate and our validation
237:34 - happens here and it's successful
237:36 - and we can let's just assume that we're
237:39 - making it to the upload
237:42 - and here's where we're returning a 500
237:45 - so in this case we're catching the error
237:48 - and we're returning a 500 and we're not
237:51 - doing anything with the error so we
237:53 - can't really see what's happening so
237:54 - let's go ahead and print the error here
237:59 - and here as well we'll print the error
238:04 - so then we can see if we're either
238:06 - getting an error when we're trying to
238:08 - publish the message
238:10 - or if we're getting an error when we try
238:12 - to upload the file
238:15 - so hopefully we can get some more
238:17 - information by printing these errors so
238:20 - we can go ahead and save this and we're
238:22 - going to need to change directory to
238:25 - Gateway
238:28 - and we need to do Docker build and tag
238:31 - and push again
238:32 - so we'll do Docker build
238:35 - because we need to add the code changes
238:37 - with the print statements and then we'll
238:40 - do Docker tag and it's going to be the
238:43 - Gateway repository that we're pushing to
238:47 - so we'll do username
238:50 - Gateway latest
238:55 - and then Docker push
238:57 - Gateway
239:03 - and once we've pushed we can just delete
239:06 - all of our
239:09 - resources for the Gateway using our
239:13 - manifest files and then we can just
239:15 - recreate them
239:23 - on actually
239:25 - we don't want to use we don't want it to
239:29 - scale up more than one right now because
239:31 - we're debugging so let's do Cube CTL
239:34 - scale
239:36 - deployment
239:38 - to one
239:39 - and for the Gateway
239:51 - and let's change directory back to where
239:54 - our file is
239:56 - and our logs since we shut down that
240:00 - container that we were following the
240:02 - logs for we need to do Cube CTL logs
240:04 - again for the new container
240:09 - and let's check our tunnel it's asking
240:12 - for the password again so we need to do
240:14 - that
240:16 - yeah
240:18 - and let's try this again
240:24 - so it was a success that time
240:27 - so I think what was happening was the
240:30 - old Gateway deployment like the old
240:32 - replicas weren't connected to the host
240:34 - because the host variable wasn't
240:38 - resolving to the actual rabbit and Q
240:40 - host in the container and that's
240:42 - something that happens sometimes so
240:44 - basically like huh let's see if we can
240:47 - let's see if I can prove what I'm what I
240:49 - suspect is happening
240:52 - so for example if we have our rapidm QQ
240:57 - here
240:57 - our Gateway is connecting to the
241:00 - rabbitmq using the name rabbitmq which
241:03 - is the name of the stateful set so let
241:07 - me try and clarify
241:09 - so let's change directory to the Gateway
241:12 - and if we go into
241:16 - storage and util.pi
241:24 - actually not storage util it's just in
241:26 - server.pi
241:29 - our connection to rabbitmq we're using
241:33 - this name rabbitmq which is the name of
241:36 - the service
241:38 - because if we
241:40 - check our rapidmq
241:43 - manifestservice.yaml you see the name is
241:45 - rabbitmq
241:47 - so in kubernetes the service name will
241:51 - resolve to that Services host
241:54 - so in server.pi
241:57 - we're depending on this name resolving
242:01 - to the host for abadin Q but it seems
242:04 - that in kubernetes if a container is
242:08 - connecting to a host via this name if
242:11 - that host changes or restarts it seems
242:15 - that the containers that reference that
242:17 - service name still reference in older
242:20 - host I believe
242:22 - so for example if I were to go in here
242:28 - so first let's send that again uh let's
242:31 - change directory
242:33 - to converter and let's send that curl
242:35 - request again
242:38 - hmm and we're getting an internal server
242:41 - error again
242:44 - but this time
242:46 - it's not our internal server error so
242:49 - let's see what's happening
242:54 - so it says we're referencing
242:57 - this
242:59 - error before the assignment hmm
243:03 - so let's go into Gateway where we did
243:07 - that
243:09 - storage util.pi
243:12 - print error
243:15 - print
243:17 - ah here we're not catching the error
243:21 - for example up here we're doing accept
243:23 - exception as error
243:26 - but here we're just doing accept so we
243:28 - need to do exception as error
243:32 - foreign
243:35 - Docker build
243:39 - and Docker tag
243:52 - and Docker push
243:57 - oh actually
244:00 - that's pretty bad so actually I just did
244:03 - that in the converter directory so
244:06 - I basically built the converter image
244:09 - and pushed it to our Gateway image so I
244:12 - pretty much just overwrote the Gateway
244:14 - image with the converter image uh let's
244:17 - see if we can just cancel that
244:19 - and let's change directory to Gateway
244:23 - and now here we can do Docker build but
244:26 - let me just double check to see what I
244:28 - did
244:31 - so yeah I did Docker build in the
244:32 - converter directory so it built the
244:34 - docker file within this directory which
244:37 - is our converter Docker file and then I
244:40 - pushed it to
244:43 - the
244:45 - actual Gateway repository and what did I
244:49 - change
244:53 - yeah I made changes to the Gateway
244:54 - though so Docker build in the Gateway
244:57 - directory to build our Docker file for
245:00 - Gateway
245:02 - Docker tag
245:15 - Docker push
245:20 - and now the latest image for our Gateway
245:23 - repository is going to be this most
245:25 - recent one so that should resolve the
245:28 - issue or accidentally pushed the
245:30 - converter image
245:32 - and we'll do Cube CTL delete again for
245:37 - our Gateway resources
245:40 - and apply
245:43 - and actually once again I forgot to
245:45 - scale so scale it to one
245:49 - foreign
245:52 - so it looks like that time we got to
245:54 - success it worked so
246:09 - so we're still terminating one of the
246:10 - replicas
246:13 - mini cubes asking for a password again
246:15 - because we had to redeploy the Ingress
246:18 - as well
246:27 - so
246:30 - all right so let's go ahead and
246:34 - set up the logs again for our Gateway
246:41 - and let's send the request again for the
246:43 - upload
246:45 - and again we need to go to where the
246:47 - file is
246:51 - and we get a success
246:56 - auth is successful and the converter is
247:00 - writing the audio so that's successful
247:02 - as well
247:05 - so back to my explanation of what I
247:07 - think the issue was before
247:09 - so our Gateway is using the service name
247:12 - for this rabbitmq service to connect to
247:16 - that host but I think that if for
247:17 - example we restart this rabbitmq the
247:20 - Gateway will still be resolving to the
247:23 - old host using that rapidmq service name
247:26 - so the Gateway won't actually be able to
247:28 - connect if we were to restart this
247:30 - unless we restart the Gateway which
247:33 - would refresh its reference to the host
247:35 - I believe
247:36 - so let's just try and test that theory
247:39 - because I don't really like not knowing
247:41 - what happened so if we send again
247:44 - we should get a success and then if we
247:47 - go into canines
247:51 - and we just delete this pod
247:56 - so we're going to delete our rabbitmq
247:57 - pod and it's going to make it restart
248:03 - and the converter breaks because it's
248:05 - it needs to connect to the rapidmq host
248:08 - as well
248:12 - foreign
248:13 - so once the rabbit mq pod restarts let's
248:17 - just restart this one refresh the
248:19 - converter
248:20 - now at this point if my theory is
248:23 - correct the Gateway is still referencing
248:25 - the older rabbitmq host using the
248:29 - rabbitmq service name so it's it should
248:32 - fail if we try to upload right now
248:36 - so if we do this again
248:39 - we get an internal server error so if we
248:43 - reset the Gateway it will reset its
248:46 - variable for rabbitmq or it will reset
248:50 - what it resolves rabbit in Q2 which
248:52 - would be the new host so resetting this
248:55 - should actually make it work
249:01 - so now if we do curl upload
249:05 - we get success so yeah my theory is
249:09 - pretty much correct and it's kind of
249:12 - annoying
249:14 - but yeah just keep that in mind so for
249:18 - example
249:20 - let me just
249:22 - recap what I said
249:28 - in server.pi we're connecting to our
249:31 - rabbitmq using the service name
249:35 - so the service name in kubernetes
249:38 - resolves to the host so yeah the theory
249:41 - is that the cluster IP address that this
249:43 - service name resolves to which is the
249:46 - cluster IP address of the service the
249:49 - rabbitmq service the theory is that IP
249:52 - address changes when we restart the
249:54 - rabbitm coupon but the Gateway pod still
249:57 - references the old IP address using this
250:00 - service name which is why when we
250:03 - restart the Gateway pod this rabbitmq
250:06 - its reference for this rabbitmq service
250:09 - name gets updated which is why it works
250:12 - after doing that
250:14 - but anyways the good news is it seems
250:17 - that everything is working so when we
250:20 - upload a file we're successfully
250:25 - if we were to clear this and do
250:29 - logs for our converter
250:34 - it seems that we're successfully writing
250:36 - the audio so in order to confirm that
250:40 - the end-to-end functionality is working
250:42 - correctly we can check for the existence
250:45 - of a audio file from our video
250:48 - conversion in the DB and that's how
250:51 - we'll check to see if everything is
250:52 - working from end to end and also if we
250:55 - go to our queue let's refresh we can see
250:58 - that our mp3q has four messages ready
251:02 - and remember we don't have a consumer
251:04 - service pulling from the MP3 queue so
251:07 - all of the messages are just going to
251:09 - pile up here so at this point we should
251:11 - have four MP3s in our database and we
251:16 - uploaded the same video every time so
251:18 - all four MP3s should be the same audio
251:20 - file but at this point we should be able
251:22 - to download one of these four MP3s from
251:26 - our mongodb and test it to see if it's
251:28 - working correctly and as you see our
251:30 - video queue is empty because any message
251:33 - that we put on the video queue was
251:35 - processed by our consumer service which
251:38 - converts them to MP3 and then that
251:40 - service then puts them onto the MP3
251:42 - queue so to confirm that everything is
251:44 - working correctly let's just check our
251:47 - mongodb to see if we have four MP3 files
251:51 - which should all be the audio file for
251:54 - the same video that we uploaded four
251:56 - times and we can also go ahead and do it
251:59 - one more time
252:02 - and we're getting an internal server
252:05 - error so
252:06 - let's go ahead and
252:09 - actually don't remember if we restarted
252:12 - the Gateway so let's just restart that
252:24 - just wait till this terminates and we'll
252:27 - do logs again
252:29 - for Gateway
252:33 - and we'll upload that again and we get a
252:36 - success
252:38 - so now there should be five messages in
252:41 - our queue the MP3 queue so as you can
252:44 - see we now have five messages in the
252:46 - queue so now we should have five MP3s in
252:49 - our mongodb database and to test that if
252:53 - you installed mongodb earlier in this
252:56 - course you should have shell
253:00 - which should put you directly into the
253:03 - mongodb that's running on our local host
253:05 - or that's running on our local machine
253:07 - and we should be able to do show
253:09 - databases and it shows that we have this
253:12 - MP3s database and this videos database
253:15 - so this database should have five audio
253:18 - files stored so let's see if we can just
253:21 - use MP3s
253:25 - and now let's try and show collections
253:29 - and as explained before with grid FS the
253:33 - actual file data is stored in these
253:35 - chunks and the files will have a
253:38 - reference or the file is essentially
253:41 - like the metadata for a collection of
253:43 - chunks so if we do DB dot FS dot not
253:47 - chunks fs.files.find
253:52 - this will show all of the objects that
253:55 - we have stored
253:58 - and actually I forgot that when I was
254:01 - testing this prior to making this
254:03 - tutorial I uploaded a bunch of videos
254:05 - that were converted to mp3s so my
254:08 - database is going to have more than five
254:10 - yours should only have the five or it
254:13 - should only have as many MP3s as you
254:16 - sent upload requests that were
254:18 - successful so let's actually do it this
254:20 - way so we can go to the queue the MP3
254:22 - queue
254:23 - and we can actually let's see get a
254:27 - message from the queue so let's just do
254:30 - get message
254:32 - and we see that there's an MP3 file ID
254:36 - that has this ID
254:39 - so let's go ahead and copy this
254:43 - and here we can do DB Dot actually let's
254:48 - uh show collections again
254:51 - and we can do db.fs.files.find
254:58 - and remember we want to use the actual
255:00 - object version of the ID to find it and
255:03 - to do that we'll do
255:07 - underscore ID for the key and then we'll
255:11 - do object
255:12 - ID and inside of there we can put our
255:17 - actual ID and as you can see that actual
255:21 - object ID is stored successfully inside
255:25 - of our MP3s database so now what we want
255:28 - to do is we want to download this and
255:31 - see if it's an actual audio file and to
255:34 - do that we can go ahead and exit this
255:37 - and let's clear and again if you
255:40 - installed mongodb using the instructions
255:42 - earlier in this video you should have
255:44 - this files and you can put for the
255:48 - DB mp3s and we want to get by ID and we
255:52 - want the local file to be called we'll
255:55 - just do test
255:57 - dot MP3 and we need to use this object
256:02 - to get the ID same way that we have to
256:04 - do that within our database so
256:09 - we'll put that in
256:10 - which is the string version of the ID
256:12 - within this object syntax and we should
256:15 - be able to download the file that way
256:19 - so it was able to connect to our mongodb
256:22 - on the local host and it was able to
256:24 - finish writing to test MP3
256:27 - so now this test MP3 file should contain
256:31 - the sound for our video file so it
256:35 - should contain the sound for this file
256:36 - so let me just go into this directory on
256:39 - the user interface okay so now in this
256:42 - directory which is the directory of our
256:44 - converter service we have this video
256:47 - file which is the test MKV file
256:54 - so here's a burger again the double
256:57 - double and I'm just going to start
256:58 - eating away with a bag here and uh
257:01 - and yeah it's cut that a bit short don't
257:03 - know about copyright or whatever so this
257:07 - audio file should be the sound for that
257:10 - video file so let's go ahead and just as
257:13 - you can see it's an audio file then we
257:14 - can just go ahead and play this so
257:16 - here's the burger again the double
257:18 - double and I'm just going to start
257:19 - eating away so yeah it looks like our
257:22 - end-to-end functionality is working up
257:25 - to the point where we put the message on
257:27 - the MP3 queue so at this point we are
257:31 - uploading our video and adding the
257:34 - message to a queue so we're essentially
257:36 - when we upload a video it gets put onto
257:38 - mongodb then we create a message and add
257:41 - it to this video queue and then our
257:43 - consumer converter service is going to
257:46 - pull off of this video queue convert the
257:49 - video into an MP3 and then put a new
257:51 - message on this MP3 queue saying that in
257:54 - mp3 for a specific file ID exists within
257:57 - mongodb so the last thing that we need
257:59 - to create is a service that's going
258:01 - going to consume This MP3 cue and that
258:05 - service is just going to be a
258:07 - notification service that's going to
258:09 - tell our user that a video is done or a
258:13 - video conversion to MP3 process is done
258:15 - so the service is essentially going to
258:18 - pull the message off the queue and it's
258:19 - going to have the ID and the email of
258:22 - the user and it's going to send an email
258:24 - to the user saying hey this ID is
258:28 - available for download as an MP3 and
258:30 - then from there there's going to be a
258:32 - download endpoint that we create on our
258:34 - Gateway where the user can use his or
258:37 - her token to basically request to
258:40 - download a specific MP3 using the file
258:44 - ID that's sent in the notification
258:46 - service email so essentially if you've
258:48 - gotten to this point where you're
258:50 - actually having the messages put on the
258:52 - MP3 queue and the mp3s stored in mongodb
258:56 - then you've essentially completed the
258:58 - most difficult part of this entire
259:00 - tutorial because from there we're just
259:03 - going to pull messages off this queue
259:04 - and send an email and that's pretty much
259:06 - it so if you've gotten this far this is
259:08 - like a major checkpoint so from here
259:10 - onward we're just going to create that
259:13 - additional service and add the code for
259:15 - the download endpoint on our Gateway
259:18 - okay so at this point in the tutorial
259:20 - what we have left is our notification
259:23 - service and we need to update our
259:26 - gateway to have a download endpoint so
259:29 - we can just start by updating our
259:31 - Gateway and then we'll move on to
259:33 - creating our notification service after
259:36 - that so we're just going to change
259:38 - directory into our Gateway directory and
259:41 - let's go ahead and clear this and we can
259:43 - actually close these other tabs that had
259:45 - our logs
259:48 - foreign
259:50 - we can just leave that running for now
259:53 - and we're going to need to update our
259:56 - server.pi file so let's go ahead and
259:59 - open that file and from here we're going
260:01 - to need to import a couple of additions
260:04 - so we need to import send file which is
260:08 - going to be the method that we use to
260:11 - send files back to the user that
260:13 - downloads them
260:14 - and we're going to be pulling that file
260:17 - from mongodb so just like before we're
260:19 - going to need to use the binary Json
260:22 - object ID so we'll import
260:26 - object ID
260:31 - and make sure I spelled it right this
260:33 - time and we're also going to need to
260:35 - change our configuration for mongodb so
260:39 - right now we're setting the
260:40 - configuration for our URI to
260:42 - include the videos database but that
260:45 - kind of limits us to just using the
260:47 - videos database but we need to use both
260:50 - databases the MP3 and the videos one so
260:53 - instead of using this configuration this
260:56 - way we're just going to create two
260:59 - separate instances of Pi for each
261:02 - database and if we have a look at the
261:04 - flask Pi documentation here
261:07 - essentially what we need to do is this
261:09 - here
261:10 - and it says you can create multiple Pi
261:12 -  instances to connect to multiple
261:14 - databases or database servers so we're
261:17 - going to do two of these and one's going
261:20 - to be for our videos database and one's
261:23 - going to be for our MP3s database
261:26 - so we're just going to change this to
261:28 -  video
261:30 - and we can go ahead and
261:34 - include the URI when creating the
261:37 - instance
261:42 - and can't forget the comma here and
261:45 - we're also going to remove this config
261:48 - here so we're not going to use this
261:50 - anymore because we're going to configure
261:52 - the URI for each instance within the
261:56 - instantiation of the pi class
261:58 - so we'll remove that
262:01 - and for MP3 it's going to be
262:04 - similar
262:05 - we'll change this to MP3
262:09 - and we'll change this to mp3s
262:12 - and now for grid FS we need to create
262:15 - two separate FS instances
262:19 - so this one can be videos
262:21 - and this will need to be set to
262:24 - video because we're using the
262:27 - video instance for the videos fs and
262:29 - this one can be MP3s
262:33 - and this will be changed to MP3
262:36 - and since the old grid FS instance was
262:40 - referenced by the variable FS we need to
262:43 - update that so here we're uploading a
262:46 - video and we were uploading it to the
262:49 - videos mongodb instance but now we need
262:53 - to change this to FS videos specifically
262:58 - and that is all that we need to do for
263:01 - that
263:04 - and now we can go to our download
263:06 - template endpoint that we created and we
263:09 - can actually write the code for the
263:11 - download function so we can remove this
263:14 - pass and we're going to do the same
263:17 - validation that we do in the upload
263:19 - endpoint so we can just copy this so
263:22 - we're going to copy the validate token
263:25 - request and we're going to copy loading
263:27 - the access from that variable that
263:29 - validate token resolves to so we can
263:32 - just go ahead and copy that part and go
263:34 - back to our download and paste that in
263:39 - and we need to add the else here
263:44 - and actually I just realized we're not
263:46 - checking the error here in our upload
263:50 - so let's go back to upload and we need
263:53 - to do if error so if there's an error
263:56 - when we try to validate
263:58 - we need to return
264:00 - error and let me just check something
264:02 - really quick
264:08 - actually no we should be good
264:11 - foreign
264:14 - we don't need to have an else
264:17 - so basically if there is no access then
264:20 - we're just going to return
264:23 - not authorized
264:34 - anyway so the download function is going
264:37 - to be pretty straightforward we first
264:39 - need to make sure the file ID exists in
264:42 - the request so basically the
264:44 - notification service is going to send an
264:47 - email to the user informing them that
264:49 - the conversion job is done and in that
264:52 - email it's going to give them a file ID
264:54 - which is the file they should download
264:56 - when they send a request to the download
264:58 - endpoint and that file ID is required so
265:01 - we're going to do
265:04 - FID string equals request Dot args.get
265:13 - and it's going to be fid
265:16 - and basically if this parameter doesn't
265:20 - exist in the request then it's going to
265:22 - return none so if we go here we see that
265:26 - it Returns the default value if the
265:29 - request data doesn't exist and the
265:32 - defaults for the default value is none
265:34 - so we can set this if we want but the
265:37 - none is the behavior that we want so if
265:41 - FID doesn't exist in the request then
265:43 - this FID string will equal none so that
265:47 - means that we can just do if not FID
265:51 - string then we can just return FID is
265:54 - required
265:58 - oh and I updated the upload endpoint to
266:02 - handle the error but I didn't update
266:04 - this one so we'll do if error
266:07 - return error
266:12 - and anyways if the FID string does exist
266:15 - then we're going to use it to get our
266:18 - file from mongodb so we'll do try
266:23 - and we'll set out equal to FS MP3s so
266:28 - this is the mongodb instance for our MP3
266:31 - database dot get
266:33 - and remember we need to use the object
266:35 - ID
266:37 - which is here and it's a mongodb object
266:40 - ID and we're importing that here
266:45 - and to that we're going to pass the FID
266:47 - string which is essentially going to
266:50 - convert our FID string to a object ID
266:54 - which is what's needed to get the object
266:57 - from mongodb so the data for our MP3
267:01 - will be referenced by this out variable
267:04 - so then we can just return
267:07 - send file
267:09 - and we'll return out and we'll do the
267:11 - download name which is going to be the
267:14 - name of the file and we're just going to
267:16 - set it to FID string plus MP3
267:22 - and this is also going to be able to
267:25 - determine the mime type for the file so
267:28 - this is all we need to do to return the
267:30 - file to the client except
267:33 - exception as error
267:36 - we'll print the air and we'll return
267:39 - internal server error
267:47 - and that's pretty much going to be it
267:49 - for our download endpoint
267:52 - so we're just going to get the access
267:53 - via the validate endpoint from our alt
267:56 - service
267:57 - and then we're just going to check to
267:59 - see if admin is true in that user's
268:01 - access and if so we're just going to
268:03 - return the file for the past file ID and
268:06 - we're not going to check the user's
268:08 - email or anything in this case we're
268:09 - just going to assume if they have the
268:11 - file ID then they have access to the
268:13 - file but of course feel free to expand
268:16 - upon this however you like so let's go
268:18 - ahead and save that and since we changed
268:21 - the code we need to build this Docker
268:24 - image again and push it to our
268:26 - Repository
268:27 - so we'll do Docker build
268:38 - and then we'll do Docker tag
268:48 - and Docker push
268:57 - and now we can redeploy this so I'm just
269:00 - going to delete all of the resources for
269:02 - Gateway just in case so I'll just delete
269:05 - everything in the Manifest directory
269:08 - and then I'll just apply them again
269:19 - and we'll check that later for now we
269:21 - can just go into creating our
269:23 - notification service so let's change
269:26 - directory to our source directory and we
269:29 - need to make their notification
269:32 - and we'll CD into notification
269:34 - so this notification service similar to
269:37 - our converter service is going to be a
269:40 - consumer service so we're going to have
269:43 - some similar code to the converter
269:45 - service so we're going to copy some
269:47 - things from our converter consumer.pi
269:50 - file into our notification consumer.pi
269:53 - file so let's go ahead and create a file
269:56 - called consumer.pi
269:58 - and we're just going to go to our
270:02 - converter consumer.pi file
270:05 - and we'll just copy everything over
270:12 - but we're actually not going to need any
270:14 - of that and we're also now going to need
270:16 - any of this but we're going to create a
270:19 - package called send and a module called
270:23 - email that we're going to need to import
270:26 - and this is where we're going to write
270:27 - the code to send the email
270:30 - and our callback functions basically
270:33 - going to stay the same but we are going
270:35 - to change this to email dot notification
270:39 - we're going to create a notification
270:41 - function
270:43 - and it's only going to take in the body
270:46 - of the message but other than that the
270:49 - Callback function is going to be the
270:51 - same and of course we need to change
270:53 - this to mp3q because this consumer
270:57 - service is listening or consuming from
271:00 - the mp3q
271:04 - and everything else is going to stay the
271:06 - same
271:10 - so now we need to go and create this
271:13 - function here this email.notification
271:15 - function
271:16 - so let's save this file and we'll make a
271:19 - directory called send and change
271:22 - directory into send and we'll touch
271:25 - init.pi
271:27 - and we're going to create a file called
271:30 - email.pi
271:32 - so in this file we're going to write all
271:34 - of the code to send in email
271:35 - notification to the client
271:38 - and we're going to just use this python
271:41 - documentation here which gives an
271:44 - example of how to send simple email
271:46 - messages which is all we really need to
271:48 - send so our code is going to be similar
271:51 - to this here
271:52 - but instead of sending the message via
271:55 - our own SMTP server we're going to use
271:57 - Google's SMTP server so we're just going
272:00 - to send it using the same SMTP server
272:03 - that Gmail uses and I will show you how
272:06 - I'm going to do that now so first we
272:08 - want to import
272:11 - SMTP lib and Os and for this tutorial
272:15 - you don't really need to know what an
272:17 - SMTP server is in simple terms it's just
272:20 - a server to send receive and relay
272:22 - outgoing mail between email senders and
272:24 - receivers so we're essentially just
272:27 - going to be using the same SMTP server
272:29 - that Gmail uses within our application
272:32 - to send emails
272:34 - so we need to import the SMTP lib and Os
272:39 - and we also need to import from
272:40 - email.message
272:43 - email message
272:46 - and this is just going to allow us to
272:48 - create an instance of an email message
272:51 - and you'll see what I mean by that in a
272:53 - second so we'll do Define notification
272:59 - and notification will take in message
273:01 - from our queue
273:03 - and then from there we will try and
273:06 - we'll set message equal to json.loads
273:10 - the message we're going to turn our
273:13 - message from our queue into a python
273:15 - object
273:16 - and then we'll do MP3 FID equals the FID
273:21 - in our message
273:24 - and we need a sender address which is
273:28 - going to be the address that we're using
273:30 - to send the email and I'm going to
273:33 - recommend that you create a dummy Gmail
273:35 - account to send the email because in
273:38 - order for this to work within that Gmail
273:39 - account you're going to need to enable
273:42 - or authorize non-google applications to
273:45 - log into your Gmail account so like
273:48 - basically the default setting when you
273:49 - create a Gmail is like only Google
273:52 - applications can log into that account
273:54 - so for example like the Gmail
273:56 - application on your phone can log into
273:58 - that Gmail account but in order for this
274:01 - application to log into that account
274:02 - you're going to have to authorize
274:04 - non-gmail or non-google applications and
274:07 - that means any non-google application
274:09 - can log into your Gmail account if they
274:11 - have your credentials so it's not
274:13 - recommended that you use your actual
274:15 - Gmail account for this because I
274:17 - wouldn't recommend allowing non-google
274:20 - applications to log into your primary
274:22 - Gmail account if you want to be safe you
274:24 - should create a dummy account so the
274:27 - sender address is going to be the email
274:29 - address of the Gmail account that you
274:32 - want to use to send the email so I'm
274:34 - going to store it in an environment
274:36 - variable
274:38 - and the environment variable is going to
274:40 - be called Gmail address
274:42 - and then we're going to also do sender
274:44 - password
274:46 - and it's going to be in an environment
274:49 - variable called Gmail password
274:52 - and I too am going to create a dummy
274:55 - account for these credentials and I'll
274:58 - get into that in a little bit and then
275:00 - the receiver address
275:03 - which is who we're sending the email to
275:05 - is going to be the user that's
275:10 - using
275:16 - is going to be the user who's associated
275:19 - with the JWT
275:23 - because that's the user who uploaded the
275:26 - original file and then we're going to
275:28 - create the message that's going to be in
275:30 - the email it's going to be an instance
275:32 - of email message and we'll do message
275:35 - dot set content
275:38 - and it's just going to be a simple
275:40 - message it's just going to say
275:42 - MP3 file ID
275:47 - and then MP3 fid
275:51 - which is coming from here
275:55 - and then is now ready
275:59 - so then the receiver of this email can
276:02 - just take the file ID that we give them
276:04 - and they can send a request to our
276:05 - download endpoint using this file ID to
276:08 - download that file and we can just say
276:10 - the message subject
276:13 - which is just the same subject that you
276:16 - would put when you're writing an email
276:17 - in Gmail and that's just going to be MP3
276:20 - download
276:21 - and message from is going to be
276:25 - sender address
276:28 - and message two is going to be receiver
276:31 - address
276:33 - and lastly we need to create an SMTP
276:37 - session for sending the mail so
276:39 - essentially we need to connect to
276:41 - Google's SMTP server and then log into
276:44 - our Gmail account and then send the
276:46 - email so we'll set session equal to SMTP
276:50 - lib dot SMTP
276:54 - and we'll put in the Gmail SMTP server
276:56 - which is smtp.gmail.com
277:00 - and then we'll do session dot start TLS
277:06 - and what this does is it puts the
277:09 - connection to the SMTP server into TLS
277:12 - mode and TLS is transport layer security
277:15 - so it's essentially going to make sure
277:17 - our communication between the SMTP
277:20 - server is encrypted and this is
277:22 - essentially to secure our packets in
277:25 - transit so that they can't be
277:27 - intercepted and the data within them
277:29 - read so in simple terms it's just to
277:32 - secure our communication between our
277:35 - application and the service you don't
277:36 - really need to know details about how
277:38 - this is working either just know that
277:40 - it's necessary and then after that we're
277:43 - going to do session.login
277:45 - and we need to log in using our Cinder
277:48 - address and sender password
277:51 - and once we've done that we can do
277:53 - session dot send message
277:57 - and in there we're going to put the
277:59 - message which is the message that we're
278:02 - instantiating here and we're customizing
278:05 - that object here and then we're sending
278:08 - it here
278:09 - and we're going to put the sender
278:11 - address and the receiver address and
278:14 - then once we've sent the message we want
278:15 - to close the session so we can do
278:17 - session dot quit and once that's sent we
278:20 - can just print
278:21 - male scent
278:24 - just so that we can see that it's done
278:25 - and if any of this fails we want to just
278:28 - print the error so we're in this try
278:30 - block here
278:33 - so we'll just do accept exception as
278:38 - error and then we'll print the error
278:41 - and then we'll return the error
278:44 - and the reason we're catching the error
278:46 - here and returning it is because
278:48 - the call to this function is expecting
278:51 - an error so the error should either be
278:53 - none or it should contain an error
278:56 - for example so if we go back up One
278:59 - Directory into our consumer file
279:03 - we see that notification here
279:06 - let me close that notification here is
279:09 - expecting an error and if there is an
279:12 - error then we're going to send a
279:14 - negative acknowledgment so the message
279:17 - can stay on the Queue and be processed
279:19 - by another process but if there's no
279:21 - error we're going to send a basic
279:24 - acknowledgment which means that the
279:26 - message can be removed from the queue
279:29 - so that's going to be it for that
279:33 - and there's a typo here this should just
279:35 - be session
279:39 - and there's also no import for Json so
279:44 - let's do that
279:48 - and we haven't installed these
279:50 - dependencies yet either so that's going
279:53 - to be a problem so we'll just go ahead
279:54 - and save this
279:56 - and we can just cat the head of our file
280:00 - and this file is called email dot by and
280:04 - we'll do
280:05 - actually we didn't start a virtual
280:08 - environment yet so let's change
280:09 - directory and do Python 3 Dash MV and V
280:15 - and start our virtual environment
280:19 - and now we can do fip3 install
280:22 - actually one second I don't think we
280:24 - need to install that I think that's
280:25 - already part of
280:29 - yeah we don't need to install that
280:34 - uh let me install this for my Vim
280:44 - so yeah this is part of Python's
280:46 - standard Library so we don't need to
280:47 - install that and let's see if we need
280:50 - anything installed from consumer.pi we
280:53 - need to install Pica and that's it so
280:56 - we'll do pip3 install Pico and that's
280:59 - going to be it for that so now we can go
281:01 - ahead and we can actually just copy the
281:05 - docker file from our Gateway
281:08 - into this directory
281:10 - and then we have Docker file and we
281:13 - don't need to expose anything and this
281:16 - needs to be changed to consumer.pi and
281:18 - everything else is going to be exactly
281:20 - the same so we can just save that
281:22 - and let's make some space here and we
281:25 - can also copy the Manifest directory
281:28 - from our converter.pi
281:33 - so now we have the Manifest directory
281:35 - from our converter dot pi and we'll
281:38 - change the name of converter deploy to
281:42 - notification deploy.yaml
281:47 - and we can go into that
281:49 - notificationdeploy.yaml file and we'll
281:52 - change all occurrences of converter to
281:57 - notification
282:00 - so I just did it using Vim said command
282:02 - but you can do it however you want just
282:04 - make sure you change all occurrences of
282:06 - converter to notification so we want our
282:09 - deployment name to be notification and
282:12 - then we want the app label to be
282:13 - notification and we'll leave replicas
282:16 - the same and we need to match label
282:18 - notification as well
282:20 - and Max surge is going to be the same
282:22 - and the templates app label is also
282:24 - going to be notification
282:27 - and container name also notification and
282:30 - we're also going to create a Docker
282:32 - repository called notification as well
282:36 - and the same for the config map and the
282:38 - secret
282:40 - so we can just save that and then we
282:42 - need to go into our config map and we
282:44 - need to change this to notification as
282:47 - well
282:49 - and the mp3q is MP3 which is correct
282:54 - and then we need to go into our secret
282:56 - and this changes to notification and we
283:00 - don't have a secret here
283:02 - so let's save that
283:04 - and let's clear change directory back to
283:08 - our root now let's go ahead and dock our
283:10 - build
283:17 - oh and we forgot to do pip freeze
283:21 - now Docker build
283:29 - and Docker tag
283:34 - foreign
283:36 - and then notification latest
283:42 - and now Docker push and notification
283:45 - latest
283:46 - and this is going to create the
283:48 - notification repository in your Docker
283:51 - Hub account
283:56 - and now let's check the docker Hub
283:58 - account
284:11 - and you should have a repository that
284:15 - was last pushed a few seconds ago
284:18 - and if you go to it you should have a
284:20 - tag latest so the next thing that we
284:22 - need to do is we need to configure our
284:25 - Gmail account to allow non-google
284:28 - accounts to log in okay so I've already
284:31 - created a dummy Google account or a
284:34 - Gmail account here so once you've done
284:36 - the same you want to click on this and
284:39 - go to manage your Google account
284:42 - and from here you want to go to security
284:47 - and you can just go down to the bottom
284:50 - here and there's this box here that says
284:53 - less secure app access and I've already
284:55 - turned this on but you're going to want
284:57 - to click this and then change it from
285:00 - off to on
285:03 - foreign
285:09 - I'm allowing apps and devices that use
285:11 - less secure sign-in technology to access
285:13 - my account
285:14 - so make sure that this is set to on
285:17 - before continuing with the tutorial and
285:20 - another thing that we need to do is well
285:22 - me specifically if I go to
285:26 - mySQL U root
285:28 - and I use auth
285:31 - and show tables
285:34 - select all from user
285:39 - you can see that I'm actually using a
285:41 - fake email here so the notification
285:43 - service will try to send an email to
285:45 - this fake email so if you used a fake
285:47 - email for your credentials here you
285:49 - should update it to an actual existing
285:52 - email I'm just going to send it to my
285:55 - dummy email so I'll essentially be
285:57 - sending the notification to myself but
286:00 - you can send the email to whatever email
286:02 - you want to use because at that point
286:04 - that email is just receiving a message
286:06 - so it's not insecure to use an actual
286:09 - legitimate email for the actual sent
286:12 - message
286:13 - so I'm just going to update mine
286:31 - so if I go ahead and select all from
286:34 - user again
286:38 - you can see that my email has now
286:40 - changed
286:42 - and that's actually the email that I'm
286:44 - using here
286:46 - so I can just exit this and that means
286:49 - that I'm going to have to get another
286:51 - token when we test this later on and you
286:54 - will have to do the same if you've
286:55 - changed the email
286:56 - and let's just go ahead and clear so at
286:59 - this point we can go ahead and deploy
287:01 - the notification service so we can just
287:03 - do Cube CTL apply everything in the
287:06 - Manifest directory
287:08 - and we can check this
287:16 - and it looks like it's working as
287:18 - expected
287:20 - and we could just change directory to
287:23 - our source directory and actually let's
287:27 - scale up our Gateway and our auth
287:30 - service as well
287:31 - so I'll just apply all
287:35 - manifests
287:39 - and that's going to scale it up to the
287:42 - two replicas that we have configured in
287:44 - our manifest directory for the all
287:46 - service
287:47 - and we can do the same thing for Gateway
287:53 - and Gateway has two instances deployed
287:57 - as well
287:58 - and the same thing for converter
288:04 - so that's scaling our converter up to
288:07 - four instances
288:09 - so at this point we have everything we
288:11 - have two instances of our auth service
288:14 - four instances of our converter two
288:16 - instances of our Gateway four instances
288:19 - of our notification service and one
288:23 - instance of our rapid mq service so now
288:26 - let's just check to see if our tunnel is
288:28 - running it's asking for the password
288:30 - again because I redeployed the Gateway
288:33 - Ingress so I'll put the password in and
288:36 - our tunnel is running and now I'm just
288:38 - going to go ahead and log in again
288:43 - but this time I'm going to change the
288:46 - email so gmail.com
288:52 - 63 and let's see if we can get the token
288:56 - okay so now we have a token which I will
289:00 - just copy
289:02 - and now I'm going to send and upload
289:05 - request so let me see if I have I need
289:08 - to go to converter
289:10 - and I'll delete the test MP3 that I had
289:13 - from last time
289:15 - and I'll upload the
289:19 - test MKV file
289:22 - but I'm going to change this token
289:26 - so this is going to be the curl request
289:28 - to upload the video file
289:32 - so what we're doing right now is we're
289:33 - testing the end-to-end functionality
289:36 - including the notification service so
289:38 - we're first uploading a video so this is
289:41 - the curl request to upload this video
289:44 - and the header authorization bear is
289:47 - going to be the token that I just got
289:48 - from the login into point
289:51 - and then our upload URL
289:56 - and I'm assuming this is because we need
289:58 - to restart our Gateway service
290:02 - so I'll just delete both of these
290:06 - so they can restart
290:10 - and let's try again
290:12 - and we have a success
290:15 - so if everything's working as expected
290:20 - the message should be being consumed
290:22 - from our MP3 queue as well
290:25 - so let's just restart this and it says
290:28 - we have six unacknowledged now let me
290:30 - check this email
290:43 - Ah that's because I forgot a very
290:46 - important part
290:48 - so let me change directory
290:53 - to the notification service
291:00 - actually I didn't forget so what is the
291:03 - issue
291:06 - okay so for some reason we're
291:07 - experiencing an issue so
291:10 - let's just check the logs for our
291:14 - notification service
291:37 - just send it again
291:39 - okay
291:45 - I'm going to scale everything down
291:49 - Gateway converter
291:52 - all
291:53 - notification
292:02 - actually I'm just going to scale
292:04 - everything down to zero and restart it
292:12 - so the only thing I have up right now is
292:15 - the rabbit in QQ
292:20 - and now I'll scale everything up to one
292:37 - okay so let's try and send the request
292:41 - and now
292:43 - so it seems that somehow the messages
292:45 - aren't being acknowledged
292:48 - actually there's one more thing that I
292:50 - forgot so
292:52 - I need to go into our notifications
292:57 - manifest secret file and we need to set
293:01 - up our Gmail address environment
293:03 - variable and Gmail password
293:06 - so we can just use this placeholder and
293:08 - change it to Gmail address
293:13 - and Gmail
293:16 - password
293:18 - and that's because if you remember
293:22 - in our notifications send email dot Pi
293:26 - file
293:29 - here we set our sender address to an
293:34 - environment variable and we also set the
293:36 - sender password here to an environment
293:39 - variable
293:40 - so we need to provide those credentials
293:43 - here so I'm going to do my credentials
293:46 - and you need to do the ones for the
293:49 - account that you're using
293:52 - foreign
293:57 - and make space and just to be safe I'm
294:00 - just going to scale everything down
294:07 - so I'm scaling everything aside from the
294:09 - queue down to zero
294:15 - and then I will scale it back up
294:24 - actually
294:25 - for the notification service
294:28 - I need to reapply
294:35 - so I'll just remove the resources first
294:40 - and this is for notification
294:45 - and then I will reapply those
295:04 - and let me check to see if those
295:07 - variables are working as expected
295:13 - and they are working as expected so
295:16 - let's see if that resolved the issue
295:20 - so let's just go ahead and send again
295:23 - but
295:24 - it's in our converter directory
295:52 - foreign
295:57 - so it seems that it's still not getting
295:59 - acknowledged so it seems that the
296:02 - messages are being consumed from the
296:05 - queue so the issue has to be in our
296:07 - notification service
296:12 - within the consumer.pi code
296:15 - there's probably an issue
296:20 - so we're consuming the message but we're
296:23 - never acknowledging the message but
296:27 - we're never acknowledging that we've
296:28 - processed the message
296:30 - so our email that notification
296:35 - we basically have everything in this try
296:37 - block
296:38 - and if there's an issue we're printing
296:40 - the error and returning the error
296:43 - [Music]
296:47 - okay so we need to make a small change
296:51 - to our notification
296:55 - send email dot Pi file
296:58 - so here we need to add in the port 587
297:03 - and this is the port for TLS and start
297:07 - TLS so I'm thinking that this should
297:10 - resolve the issue that we're having so
297:12 - I'll go ahead and save this
297:15 - and then we could just do Docker build
297:17 - and actually we're in the wrong
297:19 - directory so
297:22 - let's do Docker build
297:26 - and Docker tag
297:35 - and Docker push
297:45 - and while that's pushing let's check the
297:47 - queue So currently since we had those
297:49 - unacknowledged messages in our queue and
297:52 - our notification consumer service isn't
297:55 - currently consuming because we scaled it
297:57 - down we had messages that were ready but
298:00 - they just got swallowed up by the
298:01 - service
298:03 - and it seems that they're getting
298:05 - acknowledged at this point so this
298:07 - should go down to zero all right so they
298:09 - were all processed so that means that we
298:11 - should have emails for all of those
298:13 - messages so if I go to the inbox here
298:17 - you can see that we have the emails for
298:20 - the messages
298:22 - and let me check spam
298:27 - and I think some of those messages had
298:29 - the old email the one that wasn't a real
298:32 - email so that's why I only we're not
298:34 - getting 11 messages we're only getting
298:35 - these ones
298:38 - but anyways the email contains the MP3
298:42 - file ID string here so we should be able
298:45 - to take this and use it to download the
298:48 - actual file
298:49 - so we can clear this and to download the
298:52 - file we're going to do curl and we want
298:55 - the output of the file to go to a file
298:58 - so we'll just name that file download
299:01 - MP3 download.mp3
299:04 - and it's going to be a get request and
299:07 - the header is going to contain our
299:10 - authorization and it's going to be the
299:13 - authorization token
299:17 - and the URL should be mp3converter.com
299:22 - and don't need the port
299:26 - and it should be the download endpoint
299:28 - and you should put a URL query parameter
299:31 - here that's called FID and set it equal
299:34 - to the ID from the email
299:38 - so my email ID is
299:43 - this ID here which is the same ID that
299:47 - I'm using here
299:49 - and then if you hit enter on that it
299:52 - should download the file
299:54 - and now you should have a file in the
299:56 - directory called MP3 download.mp3 and we
300:01 - should be able to open this file up or
300:03 - listen to this file and it should be the
300:05 - video that we uploaded to audio so let
300:08 - me just go to that in the user interface
300:10 - so as you can see I have this file here
300:12 - called MP3 download.mp3 and if I play it
300:17 - so here's a burger again the double
300:20 - double it will play the sound from the
300:23 - video that we uploaded so that means
300:25 - that our end-to-end application is
300:28 - working and let's just clear here and we
300:31 - don't need this
300:33 - so let's quickly go into canines and we
300:37 - have everything scaled down so let's
300:39 - just reapply the initial configuration
300:44 - so I'm just going to delete the
300:47 - configuration
300:49 - for all of our services
300:59 - except for the rapidmq of course because
301:02 - no need to scale that up or down there's
301:05 - only one pod for that
301:07 - and then now we can just apply
301:10 - auth manifests
301:14 - converter manifests
301:19 - Gateway
301:21 - and notification
301:27 - and now if we go into here
301:31 - we have all of the instances that we
301:33 - have configured for our services
301:38 - so let's just move that video file from
301:42 - converter
301:45 - let's just move that here and let's
301:47 - check our cue really quick so currently
301:50 - our queue is empty
301:52 - and let's go ahead and upload this file
301:58 - actually let me make sure I have a valid
302:01 - token so
302:03 - I'll just do
302:06 - a request to the login endpoint to get
302:08 - the token
302:19 - and there's the new token so I'll just
302:22 - try to upload
302:23 - and let's just Spam it a couple of times
302:31 - so it seems that our cues are processing
302:33 - both the videos and the MP3 messages and
302:36 - at this point it seems that they're all
302:38 - done and if we go into our email we have
302:41 - a few more downloads here so we can just
302:45 - go ahead and copy the file ID for one of
302:48 - those
302:49 - and now we can just go ahead and attempt
302:51 - to download that
302:54 - we'll do a different name for the file
302:57 - so I'll just put something random just
302:59 - something dot MP3
303:04 - and let's change this to the file ID
303:07 - that we just copied from the email
303:09 - whoops that's the JWT let me go copy
303:13 - that again
303:16 - and let's download
303:19 - and we have this file something.pi so
303:22 - let me go to the user interface and play
303:24 - that so in our source folder we have
303:27 - something.pi
303:29 - so here's a burger and as you can see it
303:33 - is working as expected and that is going
303:36 - to be it for this tutorial I think that
303:40 - if you were able to make it to the end
303:41 - of this tutorial and you were able to
303:43 - get everything working you should
303:45 - definitely be proud of yourself because
303:47 - this one was a difficult one and yeah if
303:50 - you have any questions feel free to post
303:52 - them in the comment section I'll try to
303:54 - help you as best I can if you're having
303:55 - troubles getting everything working and
303:57 - I hope that this has been helpful to you
303:59 - and if you've made it this far in the
304:01 - video and if you haven't already please
304:03 - don't forget to like the video and
304:06 - subscribe to the channel for more
304:07 - content like this and yeah I'll see you
304:10 - in the next one

Cleaned transcript:

in this course you will learn about microservice architecture and distributed systems using a HandsOn approach a microservices architecture is a type of application architecture where the application is developed as a collection of services Giorgio from Canton coding teaches this course he does a great job teaching how to combine a bunch of different Technologies into a single application hey what's up everybody and welcome to this video on microservice architectures where we will be applying this architecture to an application that will convert video files to MP3 files in this HandsOn tutorial we'll be making use of python rabbitmq mongodb Docker kubernetes and MySQL to build this microservice architecture which is admittedly a lot but don't worry I'll walk you through every step so let's go over what this application is going to to look like from a topdown perspective so when a user uploads a video to be converted to MP3 that request will first hit our Gateway our Gateway will then store the video in mongodb and then put a message on this queue here which is our rapidm queue letting Downstream Services know that there is a video to be processed in mongodb the video to MP3 converter service will consume messages from the queue it will then get the ID of the video from the message pull that video from mongodb convert the video to MP3 then store the MP3 on mongodb then put a new message on the Queue to be consumed by the notification service that says that the conversion job is done the notification service consumes those messages from the queue and sends an email notification to the client informing the client that the MP3 for the video that he or she uploaded is ready for download the client will then use a unique ID acquired from the notification education plus his or her JWT to make a request to the API Gateway to download the MP3 and the API Gateway will pull the MP3 from mongodb and serve it to the client and that is the overall conversion flow and how rapid mq is integrated with the overall system okay so the first thing that we're going to need to do before we get started with writing any code or setting up any of our services is we're going to need to install a few things now the Links for all of these pages are going to be in the description and it's critical that we correctly install all of these prerequisites prior to moving forward with the course so please take the time to make sure that you've correctly installed all of these things so to start we're going to install Docker and I'm using Max so everything that I do like everything that I install is going to be for Mac so if you have Windows or a Linux system you're going to need to do some additional research to figure out how to install on those platforms because I'm not going to spend the time diving too deep into that but if you're able to successfully install everything on this list then that's probably one of the most difficult parts of this course and the rest of it should be a pretty smooth sailing so to start we're going to install Docker so I would hit this install Docker desktop and I'm using an M1 MacBook Pro so I would install with apple chip if you're using an Intel MacBook Pro or MacBook then you would use the Intel chip installation and of course you're just going to click the link and install it that way and following the successful installation of Docker you should be able to do Docker version and get a Docker version here now once you've gotten Docker installed we'll move on to installing the kubernetes manline tool and as you can see the kubernetes command line tool allows you to run commands against kubernetes clusters and we're going to be deploying our services within a kubernetes cluster so that's why we need this command line tool so again I'm going to choose to install for Mac and there are a couple of options to install for Mac you can use Homebrew or you can install the binary using curl I believe I installed the binary using curl and I selected the Apple chip and I just ran this command and I didn't validate the binary but you do have to make the binary executable by running this command and of course you're going to need to move the binary to a location that's within your path now I'm not going to get into details on how to do this this documentation is pretty detailed and installing binaries and adding them to your path isn't within the scope of this video so if that's a little bit too advanced for you you can just use the Homebrew installation which pretty much automates all of this for you and once you've finished with the installation you should be able to type Cube CTL or cube cuddle or whatever you like to call it and if you type that you should get the output that provides information about this command line utility now following the installation of cubecto you can move on to installing minicube and minicube is a local kubernetes focusing on making it easy to learn and develop for kubernetes so basically this is going to allow us to have a kubernetes cluster on our local machine so this way we can make a microservice architecture on our local machine without having to actually have a kubernetes cluster deployed to like a production environment or something like that so this goes on to explain the requirements to use minicube and the installation instructions are here and you're just going to select for your operating system so I would select Mac OS and I would select arm 64 as the architecture and of course we'll select stable and I used the binary download for this one as well but again if you're not familiar with how to install binaries and configure your path just go with the Homebrew installation and then once it's installed we should be able to start a cluster from our local machine and your Cube CTL will automatically be configured to work with mini cubes so you don't need to do anything there so once mini cube is installed you should be able to do mini Cube start and everything should start up for you and there's actually another thing that I forgot to add to the list of what we need to install so we're going to install this K9s and we're just going to use this to sort of help manage our kubernetes cluster so the installation instructions for this are just here in the documentation you can just do Brew install canines of course if you're on Mac but of course they have the installation instructions for other operating systems as well so just figure out which one of these is best for you and once you've finished installing canines you should be able to type canines in the command line interface and it should pull up our cluster which is just our mini Cube cluster which you can see here and you can quit by just using Ctrl C and let's go ahead and clear this now the next thing that we're going to need to install is python3 more specifically because we're going to use Python 3 to create our first service a very simple authentication Service and all of these services are going to be relatively simple because the focus of this tutorial isn't necessarily the services but how all of the services intercommunicate and how a distributed system is integrated as a whole so we're not going to spend too much time on creating large services for this microservice architecture anyways to download python if you don't already have it you're just going to select for Mac of course you're just going to select this button here and just follow the installation instructions from there and lastly we're going to need to install MySQL because this is going to be the database that we use for our auth service so I installed MySQL using Homebrew so you can just do Brew install MySQL so let's just go ahead and copy this and we can just paste it now as you can see here it says we've installed your mySQL database without a root password and to secure it run mysql's secure installation but we're not going to do any of this because again that's not the focus of this tutorial so yeah in a production environment you're going to want to make sure you're securing your MySQL installation and you're going to want to follow all of the best security practices when working with and installing a database server but in our case we're just going to leave the installation the way it is which again isn't something that you're going to want to do in a production environment this is just so that we can get to the actual meat and potatoes of the tutorial so we can access our mySQL database by just running MySQL with the user root and we don't even need a password so we should just be able to do MySQL U root and this will give us access to the database and I think that's it for the things that we need to install for now we're probably going to need to install some more things later but for our first service I think that that's all that we need so as mentioned before we're going to start with our auth service that's going to be this first service that we create and deploy to our cluster on our local environment and just quick note everything's going to be deployed on our local environment in our mini Cube cluster we're not going to deploy anything to a server I might create another tutorial on how to actually deploy this to a server or to a production like environment but for now we're only focused on the actual architecture so everything's going to be done on our local system within this mini Cube cluster so to start we're going to want to create a directory so we'll make dur and we'll call this dir maybe something like system design and we'll just go ahead and change directory into this system design directory and we're going to be writing code for multiple services and actually all of the services are going to be written in Python and if you aren't familiar with python don't worry I explained enough when writing the code where it shouldn't really matter which language you're most comfortable with and since we're focused on the architecture as a whole we're not necessarily going to be writing any complicated code anyway so anyways we're going to make a directory for Python and this is going to contain our python services and we'll put a source directory but we don't need a bin directory so we can just do let's just make a python directory change directory to python make directory source So within this system design python Source directory we're going to create the directory for our auth service so we'll make their auth and this author is going to contain our auth service code so let's CD auth and from here to start we're going to want to create a virtual environment and we're going to write all of the code for the service in one file that we're going to call server.pi and the reason we're doing this in one file is because it's going to be less than 700 lines of code and like I said before the service is going to be relatively small it's just going to be a very simple auth service and actually I forgot to activate our virtual environment so let's go ahead and do that and we should see that our virtual environment is running if we run this command here and I'm going to need to install a couple of things for my Vim configuration I'll go ahead and install pylent and pip install Jedi let's just go ahead and run this command that they're suggesting to upgrade pip and let's open server.pi and the start of our file is just going to be to import JWT which is Json web token and we're going to get into why we're importing that soon we're going to import date time and Os and we're also going to import from flask import flask and request and we're also going to import from flask mysqldb Imports my SQL so basically this is going to allow us to query our mySQL database this is going to be our actual server we're going to be using flask to create our server and this is going to be what we're going to use for our actual auth we're going to use Json web tokens and date time is so that we can set an expiration date on our token and Os is just going to be used so that we can use environment variables to configure our MySQL connection and you will see what I mean by that soon so let's just go ahead and save this and we're going to need to install a couple of things so let's just cat server.pi so we can see what we need to install and then we can just do pip install I believe JWT is like Pi JWT and we're going to need to pip install flask and pip install flask MySQL DB and we can open this file back up again and to start we're just going to create a server which is just going to be a flask object so we're going to instantiate this flask object and we're going to create a MySQL object which is going to be an instance of this MySQL which we passed the server now for the purposes of this video we don't necessarily need to understand the magic Behind These two lines of code here if we go ahead and save this and we go to the definition here we can get a general understanding of what this flask object is doing but the main thing that we need to be concerned with is once it is created it will act as a central registry for the view functions the URL rules template configuration and much more so basically this is just going to configure our server so that requests to specific routes can interface with our code and this MySQL object is basically just going to make it so that our application can connect to our mySQL database and basically query the database so following this we're going to want to set up our config so our server object has a config attribute which is essentially a dictionary which we can use to store configuration variables so for example we can set the configuration for our MySQL host and we can set it equal to OS dot environ.get which is just going to get the MySQL host from our environment so what do I mean by that so if we save this and we do export MySQL host equals localhost and we go into server.pi this code here this OS dot environ.get MySQL host is going to resolve to localhost that we set in our environment within our shell so if we were to go and print this server.config MySQL host and if we were to just Python 3 server.pi you'd see that it prints out the Local Host that we set in our environment variable So This Server is our application and server.config is just the configuration for our server or our application so we're going to create a couple of these all with different variables of course so this one's going to be MySQL user and the same here for the environment and this one will be MySQL password and this one would be MySQL DB and MySQL port and we don't need that one so this is going to be the configuration for our application and these are going to be the variables that we use to connect to our mySQL database and the next thing that we want to do is create our first route which is going to have the path login and the methods for login are just going to be post and this route is going to route to this function login and we can just write the code for the login function we're going to set off a variable called auth equal to request dot authorization and this request is the request that we're importing here and with this authorization attribute provides is the credentials from a basic authorization header so when we send a request to this login route we're going to need to provide a basic authorization header which will contain essentially a username and a password and this request object has an attribute that gives us access to that so once we instantiate this object we'd be able to do auth.username to get the username from the basic authorization header and auth.password to get the password from the basic authorization header and you'll see what I mean when we send the actual request but if we don't provide that header within the request then this auth is going to be none so that means that the request is going to be invalid so we're going to do if not off so if the header doesn't exist within the request we're going to return missing credentials and we're going to turn a status 401 which is just the standard status code that we would return in this case and the next thing that we're going to do is we're just going to check DB for username and password so the way that this login route is going to work is basically we're going to check a database so this off service is going to have its own mySQL database and we're going to check a user table within that database which we're going to create and the user table should contain a username and password for the users that are trying to log in or trying to access the API so actually before we do this part we want to go ahead and create a database and a user table and a user that we can use to access the API so we'll go ahead and save this and let's just clear and we just want to go ahead and create a file called init.sql So within this file we're basically going to create a user for our auth service and we're going to give that user a username and a password and then we're going to create a database a mySQL database called auth which is going to be the database for our auth service and we're going to Grant the user that we create privileges to the database and we're going to create a table within that database called user not to be mistaken with the user that we're creating within the mySQL database and that user table is going to be what we use to store users that we want to give access to our API and I know I'm using the word user a lot and the users that I'm mentioning here aren't interchangeable so let's just go ahead and get into it so I can show you what I mean so first we want to create user now this user is the user for our actual database so this isn't the user that we're trying to give access to our API and we're just going to call it auth user because it's the user for the auth service to access our database and we're going to say identified by and we're going to give it a simple password auth123 so this here is creating the user to access the mySQL database so this isn't the user to access the API this is a SQL script so we're basically just writing out some SQL queries and statements in this script to build our database essentially so then we want to create database and we're going to call the database off and we want to give this user up here access to the auth database and all of its tables so we're going to do Grant all privileges on off and all tables to auth user which is the user that we just created at localhost and we want to use the auth database when we create the table and then we'll create a table called user and the primary key is just going to be ID int not null Auto increment primary key and actually there's a typo here Auto increment and the next column is going to just be email and it's going to be varchar and we'll just put the maximum 255 not null and lastly we're going to have a password which is again a parchar and we'll just allow it to be long and after we create the table we just want to insert the user that we're going to use to test our application so we're going to say I'm going to give the user an email and a password and the values are going to be I'll just use my name you can use yours if you want or whatever admin123 for the password and can't forget this in my colon so so this is going to create our initial user or the user that will represent our auth service so basically we're going to use this credential to access the database via our auth service and then we're going to create the database and then we're going to Grant permissions for our auth service to make changes to this auth service database and we're going to go ahead and create the table here as well as our initial user and this is going to be the user that goes into the database which will have access to our off service API so we can just go ahead and save that and now we can just go ahead and run this script to create our initial database so just before we do that we'll just go ahead and go into our database and we can do show databases here and you'll see that we don't have the auth database yet so we can go ahead and exit that and clear and now we're going to do the same thing that we would do to log in but this time we're just going to pass in our init.sql file and it appears we have an error in our syntax near Auto increment primary key so let's go ahead and go back in here and I spelled increment wrong should be increment and let's just try that again and actually it's failing now because just now when we try to run the script the first time it already created the user but then the script failed so we already have this user so it's trying to create the same user again so let's just go ahead and delete their user drop database auth as well and drop user so now we dropped the database and we dropped the user that we created in that script because we want the whole script to run not just part of it so we can go ahead and clear this and let's just run it one more time so on SQL you root innate.sql and the script ran successfully so let's go in here and show databases and you can see now that we have this auth database and if we use auth and we show tables you can see that we have this user table and we can also describe user and it shows all of the fields for the user table which include the primary key which is the ID the email and the password and we can even select all from user oops select all from user and you see that we have the one user that we created in the script I named it my name for the email and admin123 for the password you can do whatever you want for this part as long as you make sure to use those credentials when you actually try to make requests to our API so we can go ahead and exit that and now we can go back into writing our code for our server.pi file so here we're going to check the DB for the username and the password that we pass in our basic authorization header in our request to this login endpoint and we're going to do that by making use of this flask MySQL DB here so this is basically just going to allow us to interface with our mySQL database and that's why we added this application config here which has all of the configuration variables to connect to the database that we just created and we're going to need to set environment variables for our host and our user and our password in our database which are all things that we just created and our host is going to be localhost and the port is going to be the default port for MySQL so we'll go here and we're going to create a cursor by doing mysql.connection.cursor and we're going to use that cursor to execute queries so we're going to say the result of the query is going to be equal to cursor.execute and within this execute method we're going to put in our query so we can just do it this way so we'll do select email we want to select the email and the password from our user table where email equals whatever email is getting passed in the request so we're going to do off we're going to take it from that alt dictionary or object we're going to take the username and we need to pass it in as a tuple so remember this auth object here gives us access to the username and the password from the basic authorization header so what we're doing here is we're selecting from our user table in our auth database the email that's equal to the username that's passed into this basic authorization header so we're going to be using email for our username and if the user exists within our database then we should have a result so if result is greater than zero because results going to be an array of rows I believe so if result is greater than zero then that means that we have at least one row with that username and in this situation we should only have one row with that username because the username should be unique but actually I forgot to set the column for the username to be unique so maybe we should go ahead and do that so we should be able to just go over here and add unique for the email and then save it and once again we need to drop our database we're going to drop the user and we're also going to drop the database and now once again we can run MySQL U root init.sql okay so back into our server.pi file so anyways at this point if resulted is greater than zero then that means the user exists within our database and if that's the case we're going to set the row that contains our user data to cursor.fetch one and this is basically going to resolve to a tuple which is going to contain our email so we'll do user row 0 that's going to be the email and our password which is going to be user Row one and next we just want to check to see if the username and the password returned in the row is equal to the credentials passed in the request and if that's not the case we'll say that the credential is invalid and if it is the case then we're going to return a Json web token so we'll say if auth.username not equal to email or auth.password because we need both of them to be equal to what we get from the database not equal to password then if that's the case we're going to return invalid credentials and we're going to return a 401 status code else will return and we're going to create this function we haven't created it yet but we'll return the results of the function called create JWT and in this function we'll pass the auth username and we'll need to pass in a secret for the JWT so we'll just have that in our environment and we'll just call it JWT Secret and we're going to pass in true and I'll get to what this means in a second but we're not creating this create JWT function yet so just bear with me for a bit and lastly we get to if result is not greater than zero so if result's not greater than zero then that means the user doesn't exist in our database and if the user doesn't exist in our database then that means the user doesn't have access so we'll just return invalid credentials as well for this one and a 401. and that is going to be it for our login route and the login function so now we want to go ahead and create this create JWT method or create JWT function actually so I'm going to go ahead and explain a little bit about what a JWT actually is first so we're going to go over the overall flow using basic authentication and jwts and I will try to clear up any compiled confusion that you might have up to this point in the tutorial so let's visualize the flow from a topdown perspective to get an overall understanding of what our code is doing so as mentioned before our micro services are going to be running in a kubernetes cluster and that clusters internal network is not going to be accessible to or from the outside world or the open internet our client is going to be making requests from outside of the cluster with the intention of making use of our distributed system deployed within our private kubernetes cluster via our systems Gateway so our Gateway service is going to be the entry point to the overall application and the Gateway service is going to be the service that receives requests from the client and it is also going to be the service that communicates with the necessary internal services to fulfill the requests received from the client our Gateway is also going to be where we Define the functionality of our overall application for example if we want to add functionality to upload a file we need to Define an upload endpoint in our Gateway service source that initiates all of the necessary internal services to make that happen so if our internal Services live within a private Network how do we determine when we should allow requests in from the open internet this is where our auth service comes in we can give clients access to our application by creating credentials for them with in our office database any user password combination that exists within our MySQL DBS user table is a user password combination that will be granted access to our application's endpoints this is where the authentication scheme called basic authentication or basic access authentication comes in this authentication scheme requires the client to provide a username and password in their request which should be contained within a header field of the form authorization basic credentials where credentials is the base64 encoding of the username and password joined by a single colon in the context of our off flow we are going to make use of this authentication scheme by taking the username and password from the authorization header when a client sends a request to our login endpoint and comparing them to what we have in our mysqldb if we find a match for the credentials we know that the user has access so we will return a Json web token to the client which the client will use for subsequent requests to our gateways upload and download endpoints which brings us to the other critical part of our off flow Json web tokens so what are Json web tokens a Json web token is basically just two Json formatted strings and a signature which comprise three parts each part being base64 encoded all three parts are merged together separated by a single dot which is how we end up with something that looks like this but let's break this down so what are these three parts well the first part is the header the header contains a key value pair for both the signing algorithm and the type of token which is of course JWT the signing algorithm is the algorithm that was used to sign the token which will allow us to later verify that the sender of the token is who it says it is and to ensure that the message wasn't changed along the way now there are both asymmetric signing algorithms with two keys a public and private key and there are symmetric signing algorithms which use just one private key we aren't going to go into detail about signing algorithms because it is not within the scope of this course but what you do need to know is that our auth service is going to be using the symmetric signing algorithm hs256 for example our auth service is going to be the only entity that knows our single private key and when a user logs in using basic auth or auth service will create a JWT or Json web token and sign it using that private key that JWT will then be returned to the user or the client that way when the user makes following requests to our API it will send its JWT in the requests and our auth service can validate the token using the single private key if the token has been tampered with in any way or was signed using another key then our auth service will know that the token is invalid it's important that we know that the Json formatted data in the token hasn't been tampered with because that data is going to contain the access permissions for the user so without this signing algorithm if the client were to alter the Json data and upgrade its permissions to increase its permissions allowing itself access to resources that shouldn't be available to that particular user then at that point our entire system would be compromised this brings me to the next part of our JWT the payload the payload contains the claims for the user or the bearer of the token what are claims you ask or maybe you didn't ask but I'll explain it anyway simply put claims are just pieces of information about the user for the most part these claims are defined by us although there are predefined claims as well for things like the issuer of the token the expiration of the token Etc the claims that we're going to Define on our own are who the user is for example the username and whether or not the user has admin privileges which in our case is just going to be true or false now the last part of the token is the signature the signature is created by taking the base64 encoded header the encoded payload and our private key and signing them using the signing algorithm which would in our case be hs256 at the end of all this we are left with a token that looks like this and now whenever the client makes requests to our API and provides this token within the request we can determine if the client's token was indeed signed with our private key and our signing algorithm and if so we can determine the client's access level by checking the claims in the payload portion of the token so in our case we're simply going to allow the client access to all of our endpoints if a claim that we're going to call admin has a value of true when we decode the payload portion of the token and that's going to be our off flow so we're going to Define and create JWT and it's going to take in a username a secret and auth's and auth is just going to tell us whether or not the user is an administrator so we're going to keep the permissions simple we're going to either have true or false either the user is an administrator or the user isn't an administrator and this is just going to return JWT dot encode now this JWT comes from up here we're importing this JWT here and it's from this Pi JWT module So within this in code we're going to need to pass a dictionary containing our claims a secret and an algorithm so we'll start with the dictionary it's going to contain username and the username is going to be the username that we pass into the function and it's going to have expiration and expiration is going to be date time dot date time dot now and time zone is going to be equal to date time Dot timezone.utc and we'll just continue this on the next line and we're going to add that to date time dot time Delta days equals one this is just going to set the expiration of this token to one day so this token is going to expire in 24 hours and this IAT is just issued at so like this is when the token is issued so we're just going to do date time date time UTC now and actually we could have done the same thing above for it now but we'll just leave it and then we're going to have whether or not this user is an administrator and that's just going to be the bull that we pass here for alts and next we need to pass in the secret and we also need to pass in the algorithm and this is basically just the signing algorithm for our JWT and we'll just use hs256 which I believe is the default but a little verbosity never hurt anybody so actually so now we have our login route and our login function and we have the function to create the Json web token so essentially the flow is going to be a user is going to make a request to our login route using his or her credentials a username and a password and then we're going to check to see if that user data exists within our database if it does then we can consider the user to be authenticated and we'll return a Json web token which is going to be used by that user to make requests to the API and the endpoints that that user will have access to will be determined by this permissions here so whether or not the user is an admin we're going to keep it simple if the user is an admin will make the user have access to all the endpoints and yeah that's going to be the flow for logging in so we also need to create an endpoint for this auth service to actually validate the jwts so the way that we're going to do it is we're going to basically we're using this secret when we create the JWT and that same secret is going to be used to actually decode the token as well so that's how we know that this is a valid token for our API okay so while we're down here let's just go ahead and configure our entry point so we'll just do if name equals Main and all this means is basically when we run this file using the python command then this name variable will resolve to Main so let's just leave here and clear this so if we do Python 3 server.pi whenever we run this file this way the name variable resolves to Main so if we go back in here we can actually just print name and if we run this we'll see that the result is Main so that's all this is for so whenever we run our file using the python command we want our server to start so we'll do server run and we want our server to run on Port 5000 and we want to configure the host parameter like so which essentially is going to allow our application to listen to any IP address on our host so essentially if we don't set this host parameter like so the default is going to be localhost which means that our API wouldn't be available externally and as you can see here in this flask documentation configuring our host parameter this way tells our operating system to listen on all public IPS otherwise the server is only accessible from our own computer or from localhost so let me try to explain what I mean by that so basically any server is going to need an IP address to allow access from outside of the server so in our case our server would be a Docker container and our application will be running within that container when we spin up our Docker container it will be given its own IP address we can then use that IP address to send requests to our Docker container which in this case is our server and keep in mind when I'm referring to an IP address assigned to a Docker container I'm referring to the IP address assigned to that container within a Docker Network so when we spin up our Docker container it will be given its own IP address and we can use that IP address to send requests to our Docker container which in this case is our server but that alone isn't enough to enable our flask application to receive those requests we need to tell our flask application to listen on our container's IP address so that when request gets into our containers IP our application can receive those requests so this is where the host config comes in the host is the server that is hosting our application in our case the server that is hosting our flask application is the docker container that it is running in so we need to tell our flask app to listen on our Docker containers IP address but a Docker container's IP address is subject to change so instead of setting it to the static IP address of our Docker container we set it to this 0.0.0.0 IP address which is kind of like a wild card that tells our our flask app to listen on any and all of our Docker containers IP addresses that it can find if we don't configure this it will default to just localhost which is the loopback address and localhost is only accessible from within the host therefore outside requests sent to our Docker container would never actually make it to our flask app because the loopback address isn't publicly accessible so when we set host to 0.0.0.0 we are telling our flask app to listen on all of our Docker containers IPS including the loopback address or localhost and any other IP address available on the docker container for example if we connect our Docker container to two separate Docker networks Docker will assign a different IP address to our container for each Docker Network that means that with the 0.0.0.0 host configuration our flask app will listen to requests coming to both of the IP addresses assigned to the container here it's also possible to set the host config to a specific IP address that way our flask app will only listen to requests going to that IP address and would no longer listen to requests going to localhost or the other IP address from the other Docker Network so now that we've finished creating our login route we want to actually create another route to validate jwts and this route is going to be used by our API Gateway to validate jwt's synth within request from the client to both upload and receive or download MP3s or to upload videos and download the MP3 version of those videos and you'll see what I mean by that later on in the tutorial when we actually start to implement that so we're going to do another route and this one's going to be validate and methods are going to be post and we're going to define a function called validate and we're going to want to pull the encoded JWT from our request and we're going to require the JWT to be in a headers or a header called authorization and if the JWT is not present in the authorization header we want to return an error so we'll say encoded JWT if not encoded JWT we'll just return missing credentials and a 401 so if you remember from the explanation of the basic authentication scheme for that scheme we would need to have the word basic in our authorization header that contained our base64 encoded username and password separated by a colon well for our JWT we instead need to have the word Bearer in the authorization header that includes the token so let me quickly go over the format for the authorization header so that you understand what's happening so if we look at the documentation for authorization headers on mozilla.org we see that the format for the header is first type and then credentials here type represents the authentication scheme and credentials represents the credentials necessary specific to that type so from the perspective of the server handling the authorization header the type tells us what type of credential is contained within the header so if the type is basic from the server perspective we know that we are dealing with a credential which is a base64 encoded username and password separated by a colon if the type is Bearer we know that we are dealing with a bearer token which essentially means that we can assume the party in possession of the token or the bearer of the token has access to the tokens Associated resources now in the code for our validation endpoint to save some time we're just going to assume the authorization header contains a bearer token therefore we aren't going to check or validate the word that represents the type that comes before the credential in the header but in an actual production environment you are definitely going to want to spend the extra time to check the type or the authentication scheme present within the authorization header and within this authorization header we're going to require the token to be formatted as bear authentication so basically we're going to want the header that's synt with the JWT to look like this so it's going to have this Bearer as part of the string and then the token and as a result of that if the encoded JWT is present we're going to need to split the string so we're going to set encoded JWT equal to encoded JWT dot split and we're going to need to split it based on a space because there's going to be the word bear and then a space and then there's going to be the token so the array that results from this split is going to have the item with the word bear and it's going to have an element with the token so we're going to need the first index or the item at the first index of the array not the zero and then we're going to try and we're going to do decoded equals JWT dot decode and to this decode method we're going to need to pass the encoded JWT and we also need to pass our JWT secret the one that was used when we actually encoded the JWT which is going to be in an environment variable and we're also going to need the algorithm and the algorithm that we used was hs256 and if that fails we're just going to return not authorized and a 403. but if it doesn't fail we will return the decoded token and a 200. and that's pretty much going to be it for our auth service so we can just go ahead and save this so now we have our actual service and we have our init script for our database and now we're going to need to start writing all of our infrastructure code for the actual deployment so we're basically going to deploy all of our services within a kubernetes cluster as you already know so we need to create Docker images that we're going to push to a repository and our kubernetes configuration is going to pull from the repositories for our Docker images and create our deployments within our cluster and I know that this sounds kind of confusing but we're going to walk through everything step by step so don't worry so we're going to start by making a Docker file and as our base image we want to use a python image so we'll use this python 310 slim bullseye and after we write out this Docker file I'll go over what all the lines mean in a little bit more detail but for now I'm just going to vaguely go over what we're doing so first we're going to run our aptget update and we also need to aptkit install a couple of dependencies so we're going to need build Essentials and default live MySQL client Dev and then we want to pip install upgrade pip and following that we want to set our working directory to app and we want to copy first just our requirements dot txt which we haven't created yet and the reason we're copying this separately from the rest of the application is because we want to make sure our requirements are in a separate layer so that if our application code changes we can still use the cached requirements layer we don't need to reinstall or recreate the layer and I'll probably explain that in a little bit more detail later on so then we want to run pip install our requirements and then we can copy over the rest of our application and our app is going to be running on Port 5000 so we'll expose that port and finally we need to create the command which is going to be python3 server.pi so it's the same as when we're actually running this python3 server.pi from the command line and that is it for that so we'll save that now let's get into explaining the contents of our Docker file so basically when we build a Docker image we're building it on top of a base image which in our case is this base image python 310 slim Bullseye so we can think of an image as a file system snapshot for example the base image that we are building our image on top of is essentially a snapshot of a file system that contains all of the necessary dependencies to run python applications for instance we wouldn't be able to run a DOT Pi file on an OS that doesn't have python installed right so a base python image will have things preinstalled so that we don't need to worry about that so based on that understanding let's go a little deeper it's important to keep in mind when writing Docker files that each instruction in a Docker file results in a single New Image layer being created that means that the next instructions image layer will be built on top of the previous instructions image layer so that means that this from instruction creates a layer and then this run instruction creates a new layer on top of the previous layer that was built from the from instruction and this continues on until we reach the end of our Docker file this is important to understand because if we need to build our Docker image again Docker is smart enough to use cached image layers if nothing within the layer has changed and none of its preceding layers have changed so what do I mean by that so let's say that the dependencies for our application change resulting in our requirements.txt file changing when we rebuild the image we won't need to rebuild every layer again we only need to rebuild the layer that changes and every layer after it because of course every layer after it is based on its preceding layer therefore if the preceding layer changes so does it and the reason it's important to understand this is because optimizing your Docker file to use cached layers efficiently will significantly decrease the build time of your image and that might not seem so beneficial in this context but when we are talking about deploying production applications using CI CD pipelines the build speed is something that we want to consider now if you don't know what a CI CD pipeline is don't worry it's not necessary to understand that for this tutorial anyways with the understanding that each instruction results in its own layer and that if one layer changes every layer after that layer will also need to be rebuilt we can see why it's beneficial to separate the copy instructions for our requirements.txt from the copy instructions of the rest of our application because with this configuration as you can see if the dependencies for our application change resulting in our requirements.txt file changing we need to create a new layer to build onto with the new requirements being installed in this run pip install layer and of course every layer after that layer will need to be rebuilt as well but if we only make a code change and our requirements don't change we don't want to have to build the layer that installs our dependencies again because this is probably one of the most time consuming layers to build so as you can see we are copying the rest of our application to the app directory here and by the way this dot just means the current directory that we ran the docker builds command in so if we copy dot to our app directory which is our working directory or copying everything in the directory where we ran the docker build command in on our local machine in other words the directory that contains our Docker file on our local machine and if any of the code has changed since our source files are contained within that directory Docker will detect that there was a change and rebuild this layer and every layer following this layer will need to be rebuilt as well but as you can see the layers following this layer don't include the time consuming pip install command so that's just a quick example of why optimizing your Docker file to be more layer efficient is beneficial so now back to the rest of our instructions so after we build our base layer using the python 310 slim Bullseye image we then move on to installing our OS dependencies and all of these flags in purple here are to avoid installing unnecessary additional packages as well as avoiding taking up additional space with caching and stuff like that because we want our container to be as light as possible and we also don't want to introduce potential vulnerabilities present on packages that we don't even need and the reason we are combining all of these commands into one command is so that we can keep them all in the same run instruction therefore keeping them contained to one image layer because remember every Docker instruction creates a new image layer and then here we're just creating a directory to work in and this dirt is where our application source is going to live and these instructions we've already gone over and this expose instruction doesn't actually do much of anything other than serve as documentation to anybody that builds this image and it essentially lets them know what port is intended to be published so our app listens on Port 5000 so that is the port that we have here in the expose instruction and lastly we have our Command instruction and this instruction is the instruction that is going to be used to run our container this instruction sets the command to be executed when running the image so for example when we run our image the Python 3 command will be run on our server.pi file which is going to run our auth server in this case okay now let's go ahead and build this Docker file oh and I forgot to create our requirements.txt so we're going to do uh pip 3 freeze and we're going to freeze our requirements the current requirements for our application into a file called requirements.txt and if we go into this file you see it has all of the requirements that we needed to install for our application like it has this MySQL DB that we're using and of course flask and any of their dependencies as well so doing pip 3 freeze it basically freezes our dependencies into a file so that we know what dependencies we need to install to run this application so let's go ahead and try this again and now it says unable to locate package build Essentials so let's go back into our Docker file and that is a typo it's actually just build essential and let's try again foreign and now our image is finished being built so once we've finished building our image we actually want to create a Docker registry or a repository so you can just go to hub.docker.com and create an account here I already have an account so I will just sign in okay so once you've successfully created an account and logged into your Docker Hub account you should end up at a page that looks like this and from here you just want to click this repositories Tab and what we're going to do is we're going to create the repositories that we're going to push our container images to and then our kubernetes configuration is going to pull from this repository the repositories that we create for each individual service now your repositories are going to have the same suffix as mine because we're going to create the repository name using the same suffix but the prefix to your repository is going to be different mine's going to have this prefix and yours is going to be whatever the name of your account is so for example if we create a repository here and we name the repository auth because it's going to be the repository for our auth services images when we actually push to this repository from our command line I'm going to push to sweezytech auth and you're going to push to whatever your username is for your account and you'll see what I mean by that in a second so we're going to go ahead and create this repository for our auth services images and we're just going to make it public because we only can make one private one and then if we do a private one we're going to have to configure credentials within minicube which is a little bit more complicated so we're just going to do public and throughout this tutorial it will be possible for you to push and pull from my repository which can cause lots of issues for you as you follow along with this tutorial because my images may or may not be in the same state that they are at the part of the tutorial that you're on so just make sure you take the time to create this account and create your own repositories so we can just go ahead and create this and now as you can see here it tells you how to push to this repository and as you can see this is going to look different for you than it does for me specifically this part is going to look different for you so anyways now what we want to do is we want to tag the image that we just created here we just built an image using this Docker builds command it built the image based on our Docker file that we just created now we want to tag this image so we'll do Docker tag and we're going to just use this Shaw here you don't actually need to put in the whole thing but I'm just going to put in the whole thing and you're just going to tag it using your username from your Docker Hub account slash auth and then we're going to tag it as latest because it's going to be the most recent version of our image now if we do Docker image LS you can see that we have our tag here and you can compare this part of the image ID to the first part of this shot and just ignore all of these other images that I have you'll likely have whatever images you have on your system as well but none of that matters as long as you have this image tag here then you're fine so let's just go ahead and clear that and now that we've tagged it we can push it to our repository so we'll just do Docker push and again you're just going to use your username for your Docker Hub account and then auth and then latest and then just push that and once that's finished you can go to the repository and you can just refresh the page and you should see your image tag here so that means we've successfully pushed this image to our repository and now whenever we want to pull this image we could just do Docker pool and we could just do the name of our image and the tag or the URL for our image in the tag and we'd be able to pull it but that's not actually how we're going to be pulling these images our kubernetes configuration is actually going to be pulling the images so let's go ahead and clear and now we're going to make a directory called Manifest this directory is going to contain all of our kubernetes configurations so let's change directory to manifest and for all of these configuration files I'm going to go over them in detail after we write them out so if you're confused about the infrastructure code that we're writing just hang in there so all of our configuration files are going to be yaml files and we'll start with a file called off deploy.yaml and within this file we'll do API version apps V1 foreign is deployment now again this is the configuration for our kubernetes cluster and our service and if you're not familiar with kubernetes a lot of these configurations you probably won't understand I will try to go into more detail for these in a little bit so we'll do metadata name of our service or our deployment is going to be off and labels app is going to be auth as well and then we're going to do our spec in replicas we're going to want two replicas or two instances of our service and selector we're going to match labels and the app will be off strategy type rolling update and the configuration for Rolling update we're going to do Max surge equals three then we want to do template metadata for template labels app off and we'll do spec again containers name auth and this is where we're going to configure it to pool our image we're going to set image to remember this should be your username and then auth and then ports the container port is going to be 5000 because our application is running on Port 5000 so we'll just do the same for container Port as well and then we want to get our environment variables from config map file we're going to create this file after this so we're going to do config map ref and we're going to name the config map that we're going to create auth config map and again we haven't created this yet we're going to create it soon and here we're going to do Secret ref and we're going to store our secrets in a secret we're going to name it auth secret and we're going to create this file as well and let's just check formatting and looks fine so we can just go ahead and save that so now let's create the config map so we'll just do Vim configmap.yaml this config map is going to set environment variables within our container so for this one we'll do API version B1 kind config map metadata name is going to be auth config map and data are going to be our environment variables so we'll do MySQL post and since we're going to be using our local MySQL server we're going to need to reference that server from within our kubernetes cluster and luckily minicube gives us a way to access our host the Clusters hosts via this host dot miniq dot internal because basically within the cluster we're kind of in our own isolated Network so since our MySQL server is just deployed to our local host from within the cluster we wouldn't be able to just use localhost we need to access the system that's hosting the cluster and that's what this host mini Cube internal is for so our MySQL user is going to be off user we created and our MySQL DB is going to be auth which we also created and default port for MySQL is 3306 and actually we should do it as a string so these are going to be the environment variables that will automatically be exported within our Shale when we do the deployment so in other words if we were to run the environment command within that container all of these variables and their values are going to be present within the container so that's what this config map file is for and configmap is for environment variables that aren't necessarily sensitive data like passwords so we're also going to need to do a similar file for our secrets or sensitive data like our password to our database and of course in a production environment you would never push your secrets configuration to like a git repository or something because then your passwords would be easily visible at the repository so just keep that in mind when we're creating this file so we're just going to call it secret.yaml and API version once again V1 and this time kind is going to be secret and metadata we're going to do name auth Secret and we're going to do string data and we'll do MySQL password is going to be auth123 and our JWT secret is another secret that we need for this application and we're just going to make it a random name sarcasm and we need to set type to opaque and this is going to be our environment variables for our secrets and we can just go ahead and check if the formatting is okay and save that and lastly we need to create our service.yaml and we'll do API version V1 and kind is going to be service metadata will be name is auth that's going to be the name of the overall service and spec we'll do selector app off and we'll do type cluster IP and this cluster IP basically just means that the IP address assigned to this service is only going to be accessible within our cluster but again I'll go into a little bit more details soon and our Port is going to be 5000 our Target Port 5000 as well and protocol is going to be TCP and then we can just save that so once we have all of our info code for our kubernetes deployment we can actually start to deploy this off service to our cluster so let's go ahead and take a look at canines So currently we have nothing in canines there's no cluster and there's no context because our mini Cube isn't running right now so we can actually do any Cube start and once mini cube is started we should be able to go back into canines and if we change the namespace to All by hitting zero here we can see that we have our mini Cube pods running within the cube system namespace and you can see here our cluster is mini Cube so let's go ahead and Ctrl C out of there and we can clear this okay so let me just briefly go over what we're doing here so basically within this manifest directory we wrote the infrastructure code for our auth deployment so if we change directory back to our main directory we wrote the code for our auth service and we created a Docker file to build that source code into a Docker image and we then pushed that Docker image to a repository on the internet and within our manifest infrastructure code we're actually pulling that image from the internet and deploying it to kubernetes and that image contains our code so all of these files within this manifest directory when applied will interface with the kubernetes API which is the API for our kubernetes cluster to interface with our kubernetes cluster so these files are going to interface with that API to create our service and its corresponding resources like its config map and its secret and to do that all we need to do is do Cube CTL apply and then we're going to use this F flag for file and we're just going to apply all the files in the current directory this manifest directory and as you can see here our config map resource was created and our secret was created and our service was created but we actually had an error here for our deployment so it's saying that let's see a known field template so this seems like there's an issue with our auth deploy.yaml file let's go ahead and have a look so a template shouldn't be unknown so the spacing is really important in yaml files so we'll put this back one because strategy is actually part of spec but the way we had it before we had strategy a part of selector so we need to make sure that spacing is correct in these files so we'll go ahead and put this back and let's give it a try now so we can just apply again and it will only apply the files that have changed and now we're getting unknown field name so I'm assuming that it's an issue with the spacing again so let's just go over this so the issue here is that our config map reference the name shouldn't be at the same spacing it should be there and the same for the secret ref name so we can go ahead and save and let's try to apply again and now we were able to create our deployment as well so now that we've created these resources we can go into K9s and we can see that we have two instances of our auth service being created and now both instances are running and if we go into the logs by just pressing enter on first the Pod and then the container we can see the logs within the container and we see that our server is running and that's for both of these replicas and also within canines we can use the shell within the container by just entering on the Pod and then pressing s on the container to access the shell so now we're in a shell within our container and within this shell we can do environment and we can see our environment variables so you can see we have our secret here our MySQL password and we have our MySQL user here actually we can just environment grep MySQL and we can see all of our MySQL environment variables from both our config map and our secret and we can just exit the shell here and leave canines okay so to explain our kubernetes configuration I will need to explain a little bit more about kubernetes in general so throughout this course there's been a lot of mention of deploying our micro services to a kubernetes cluster but what does that actually mean let's first briefly go over what kubernetes is in simple terms kubernetes eliminates many of the manual processes involved in deploying and scaling containerized applications for example if we configure a service to have four pods kubernetes will keep track of how many pods are up and running and if any of the pods go down for any reason kubernetes will automatically scale the deployment so that the number of PODS matches the configured amount so there's no need to manually deploy individual pods when a pod crashes kubernetes also makes manually scaling pods more streamlined for example say I have a service that load balances requests to individual pods using round robin and that service is experiencing more traffic than the number of available pods can handle as a result of this I decide to scale my service up from two to five pods without kubernetes in a situation like this I'd likely need to go manually deploy each individual additional pod and then I'd need to reconfigure the load balancer to include the new pods in the round robin algorithm but kubernetes can handle all of this for you and it's as simple as running this command with this simple command kubernetes will scale up your service which includes maintaining the newly scaled number of PODS if a pod happens to crash and it will auto configure the load balancer to include the new pods basically with kubernetes weekend cluster together a bunch of containerized services and easily orchestrate the deployment and management of these services within the cluster using what we call kubernetes Objects which are persisted did entities in the kubernetes system I know that sounds a bit complicated so let me explain so for this part of the explanation let's go to the kubernetes documentation so it's explained here that a kubernetes object is a record of intent once you create the object the kubernetes system will constantly work to ensure that object exists by creating an object you're effectively telling the kubernetes system what you want your cluster's workload to look like this is your cluster's desired State now this sounds complicated but we've actually already done this multiple times for example we created a deployment object here in this yaml file this file is the above mentioned record of intent we are telling kubernetes that we want this deployment object to exist in our cluster in the state specified in our spec here for example we want two replicas to be deployed once this configuration is applied as explained here the kubernetes control plane continually and actively manages every object's actual state to match the desired State you supplied that means that kubernetes will keep track of the actual status or state of your deployment and make sure that it matches your record of intent in other words your yaml specification so bringing it all together we can say that our kubernetes cluster is comprised of a bunch of objects that we've configured that describe our cluster's intended state from there kubernetes will continually compare the current status or state of those objects to the specification or desired state from our original configuration and if that comparison ever differs kubernetes will automatically make adjustments to match the current status with our original record of intent in other words our original specification so how do we communicate with kubernetes to configure and or create these objects well let's once again take a look at the documentation it's explained here that to work with kubernetes objects whether to create modify or delete them you will need to use the kubernetes API when you use the cube CTL command line interface for example the CLI makes the necessary kubernetes API calls for you so basically the Q CTL CLI that we installed is interfacing with the kubernetes API to essentially run crud operations on our clusters objects in our case we are running our cluster locally using mini Cube so the end point for the kubernetes API in this case is on our local machine but in the real world your cluster will usually be deployed on some server and on your local machine you'll have a kubernetes configuration for the cluster on that server which will enable your local Cube CTL CLI to interface with the remote server but we don't need to go into the details for that in this video so now that we have a general understanding of what kubernetes is and how it is working we can now get into explaining our actual yaml configuration files so if we have a look at the documentation here we see that there are some required fields necessary when creating kubernetes objects using the dot yaml files those are API version kind metadata and spec we can also see a description for each field API version is which version of the kubernetes API we are using to create this object kind is what kind of object we want to create for example deployment config map secret Etc metadata is just data that helps uniquely identify the object and lastly spec is the desired state or record of intent for the object which we explained before as mentioned here spec format is different for every kubernetes object type for example the spec format for an object of kind deployment will be different from the spec format for an object of kind service and to see how to configure the spec for specific types of objects we can use the kubernetes API reference so let's go over the spec format for our deployment object configuration so first as you can see here we have all of the required fields we have API version which is the required field here we have kind which is the required field here and we have metadata which is the required field here and as mentioned in this dock the precise format of the object spec is different for every kubernetes object so anything within this spec block is our deployment spec so to see the actual format for the spec for a deployment we can just go to the kubernetes API reference here and a deployment is a workload resource so we would click this workload resource and we see we have deployment here so we can select deployment so this here is basically giving us the overall configuration for a deployment so it's showing the API version and the kind and the metadata as well and it also tells us the object metadata because as you can see here our metadata has its own nested fields so if we were to hit that object metadata we'd get a description for each field that we have within our metadata so name and it tells us that name must be unique within the namespace and that it's required when creating a resource and if we scroll down here we see that we have additional fields that we're not using currently but that we can use within our metadata block and then we have labels which we're using here and it tells us the format for the labels is it's a map of string keys and string values so for example this app would be the key and auth would be the value for our labels so let's go back and get into the spec configuration so as you can see here is spec and then there's a deployment spec link that we can select that will give us the details of the actual spec format for a deployment so we have our spec here and any of the fields nested within this spec block are going to be present here so we can get a detailed explanation of each individual field so for example the selector here is the selector that we have here and if we click label selector we can find this matched labels that we're using here down here and basically what match labels is doing it says here a label selector is a label query over a set of resources and to understand what I mean by that let's go back to what a selector is here so as you can see here it says label selector for pods existing replica sets whose pods are selected by this will be the ones affected by this deployment and it must match the Pod templates labels so as you can see within our template which we're going to get to for our replicas we're setting the label with key app and value auth so we're going to get into it but this template is basically going to be the configuration for each individual pod and our deployment is basically going to know what pods are part of the overall deployment because this selector is going to match the labels that are assigned to each individual pod in our template here so simply put our deployment knows what pods are part of the deployment because based on this template each pod is going to be deployed with a label that's a key value pair where the key is going to be app and the value is going to be auth and our deployment is going to select pods using the same label app off as the key and value and then we can go here to replicas and as you can see here replicas is the number of desired pods and you already saw how this replicas is working you saw that when we actually applied our configuration there were two auth pods deployed so if we were to increase this to say four then four auth pods would be deployed when we apply the configuration and then we can head over here to strategy which is here and this is just the deployment strategy to use to replace existing pods with new ones and basically this is the difference between here killing all of the existing pods before creating new ones which would essentially mean that our service is unavailable during the creation of new pods or replacing our old replica sets by new ones using rolling update which basically gradually scales down the old replica sets and scales up new ones and in our case we're actually configuring the max surge here to three which is this here and this is the maximum number of PODS that can be scheduled above the desired number of PODS so for example if our desired number of PODS is two and we need to do an update a rolling update it might be necessary to exceed the number of replicas while some pods are shutting down and newer pods are spinning up so this Max surge is just to give us some extra Headroom when we're actually needing to update our deployment and lastly we get into our template here and simply put template describes the pods that will be created we've already gone over this a little bit but let's actually go into the Pod template spec so everything nested under this template field is within the Pod template spec so we have here our metadata and metadata is the same for all of the types so object metadata is going to be the same regardless of the type so if we click this we see that it's still just name and all of the other fields that are possible within metadata and same thing with spec but this time the spec here isn't the same spec as deployment because remember each type has its own spec format so this spec is going to be pod spec here so if we select pod spec we get a different set of fields and descriptions for a spec for a template which is a pod spec because the template is the template for pods so as you can see here we have containers then we have containers here as well and this is where we Define our container so we need to name the container and we need to set an image for our container as well and remember we're pulling our image for our container from our Docker repository and that's the image that's going to be used for our container within our pod so that's image here and then we have ports which we can find here and this ports is actually similar to the expose instruction that we did in our Docker file doesn't actually serve as anything other than documentation so as you can see here it says list of ports to expose from the container exposing a port here gives the system additional information about the network connection a container uses but is primarily informational so not specifying a port here does not prevent that Port from being exposed so any port which is listening on the default 0.0.0.0 address inside a container will be accessible from the network so we've already gone over all of this when we were configuring our Docker file so you should be familiar so yeah this container Port 5000 is similar to our expose instruction in our Docker file it serves as documentation essentially and let's see if we can just search for environment from foreign so we have environment from here which is this configuration here and it's just list of resources to populate environment variables in the container so as I explained to you and showed to you our config map is where we Define our environment variables for our container which you were able to see when we went into the shell for our container so we're using an additional resource config map which can be seen here and it's the config map to select from for the environment variables for the container and down here we also have our secret ref or this one for that matter the contents of the target secret data field will represent the key value pairs as environment variables so essentially the secrets are being stored as environment variables as well and the secrets come from our secrets configuration and both this config map and the secret are their own individual kubernetes objects as well so basically whenever you see kind and a name that means that's an individual kubernetes object so as you can see for our configuration file for our config map it has its own kind and config map type so this is going to create another object in our kubernetes cluster and as you can see each object configuration is going to have essentially similar Fields overall like they're all going to need an API version a kind metadata but some things might differ like this here is a data field which we don't use in our actual deployment object configuration and here's another example when we created Our Kind service we still have the metadata field with its nested field name and again we have the spec for the kind service but of course this spec format is going to be specific to the kind service so it'll be different from our deployment spec format and this API reference documentation is very important and I will have a link to it in the description of this video so now we can start to write the code for our Gateway service so we can just change directory back to our python Source directory and right now we only have our auth service but we can also make their Gateway and we'll go ahead and change directory into Gateway and to start we want to create a virtual environment as usual and then we want to start our virtual environment and as you can see our virtual environment variable is the Gateway virtual environment now the next thing that we want to do is create a file called server.pi so our Gateway service is going to have a few dependencies so we're going to need to import and actually let me go ahead and install some Vim dependencies so we're going to need to import OS we're going to need to import grid FS pica and Json and we're also going to need to import flask and request and we're going to need to import flask Pi and we're going to create an auth package and we're going to create a validate module within that package and we're also going to create an auth service package and we're going to import module access from that package and we're also going to need to create a storage package and we'll import util from the storage package so these we haven't created yet but we're going to create soon and flask by we're going to use mongodb to store our files and this grid FS is basically going to allow us to store larger files in mongodb and I'll explain a little bit more about that when we get to it and this Pika here is going to be what we use to interface with our queue we're going to be using a rabbitmq service to store our messages and I'll go over that more once we get to it as well so to start we just want to create our server so server is going to be equal to flask so in our server config we can do URI and set it equal to mongodb at host.minicube dot internal at Port 27017 which is the default mongodb port and at database that we're going to call videos so if you remember this host mini Cube internal just gives us access to our local host from within our kubernetes cluster and this is just a mongodb URI that's going to be the endpoint to interface with our mongodb so our config is going to have the configuration to our mongodb that's on the Local Host that we haven't yet installed but we will then we're just going to create a variable called and we're going to do PI server and I will explain what this is doing in a second but first let's just go ahead and install all of our dependencies so we can go ahead and save this and we'll just cat server.pi so we can see what we need to install and we'll do pip3 install pika and pip3 install flask and let's cut that again so we can see pip3 install I believe it's high and once we've installed those dependencies we can go back in here foreign flask Pi which is flask hi and now let's go back in here and see and we have a type over here this should be a small l so now here what this line of code is doing so what we need to know about this line of code is that this Pi is going to wrap our flask server which is going to allow us to interface with our mongodb so if we go to the definition here we see that it manages mongodb connections for our flask app so this essentially is abstracting the handling of the mongodb connections away from us so beyond that we don't really need to understand what's actually happening for the purposes of our use case and once we've created that variable or the pi instance we're going to create FS for grid fs and we're going to create an instance of this grid FS class and we need to pass in our DB from our database which is going to be this video's DB so grid FS is going to wrap our mongodb which is going to enable us to use mongodb's grid FS so let me quickly explain what grid FS is okay so we're going to very quickly go over what grid FS is in relation to our mongodb so we're using mongodb to store our files files being both our MP3 files and our video files and if we go to mongodb's limits and thresholds documentation we can see that a binary Json document size has a maximum size of 16 megabytes and it's explained here that the maximum document size helps ensure that a single document cannot use an excessive amount of Ram or during transmission excessive amounts of bandwidth so they're basically saying that handling files over 16 megabytes in memory will result in Prof performance degradation so what they provide as an alternative is grid FS which essentially allows us to work with files larger than 16 megabytes by sharding the files for example if we go to the gridfs documentation here we see that grid FS is a specification for storing and retrieving files that exceed the binary Json document size limit of 16 megabytes and it says instead of storing a file in a single document gridfs divides files into parts or chunks and it stores each chunk as a separate document so in this case we'd no longer be dealing with files larger than 16 megabytes in memory because a file larger than that size would be separated into chunks and we're only dealing with the individual chunks at that point which avoids the performance degradation issue so when using grid FS gridfs uses two collections to store files and you can just think of collections in mongodb as tables so it's explained here that one of those collections stores the file chunks and then there's another collection that stores the files metadata so this collection that stores the file's metadata basically contains the information necessary to reassemble the chunks to create or reform the original file and if you're interested in more details about this gridfs or mongodb for that matter has very good documentation so you can come and read additionally about this but you'll see later on in the tutorial these are the two collections that we'll be working with to work with these files and the reason we're actually using gridfs is because working with video files there's a high probability that will eventually be working with files larger than 16 megabytes so this is essentially going to Future proof our application but for the purposes of this tutorial you don't really need to know much beyond what I just explained but again if you're the type of person that likes to dive deeper into these types of details like me then I recommend reading this page as well which is actually pretty interesting now the next thing that we're going to want to do is configure our rabbitmq connection so we'll just create a variable called connection and we're going to use Pica dot blocking connection which is essentially going to make our communication with our rapidm QQ synchronous and again the details of how this is working are abstracted away from us so we don't need to worry too much about that so in here we're going to add our connection parameters and we're going to pass to our connection parameters the host for our rapidm QQ and we're going to deploy our queue as a stateful set in our kubernetes cluster and it's going to be accessible via just the name rabbitmq and we haven't configured this yet we're going to configure it later but just know that this rabbit mq string is referencing our rabbitmq host and once we create this instance of a blocking connection we want to create a channel with just the connection that we just created dot Channel so let's briefly go over how rabbitmq is going to integrate with our overall architecture so we already know how the off flow works so we don't need to go over that again but now let's go over how rabbitmq integrates with our overall architecture so when a user uploads a video to be converted to MP3 that request will first hit our Gateway our Gateway will then store the video in mongodb and then put a message on this queue here which is our rapid in Q letting Downstream Services know that there is a video to be processed in mongodb the video to MP3 converter service will consume messages from the queue it will then get the ID of the video from the message pull that video from mongodb convert the video to MP3 then store the MP3 on mongodb then put a new message on the Queue to be consumed by the notification service that says that the conversion job is done the notification service this consumes those messages from the queue and sends an email notification to the client informing the client that the MP3 for the video that he or she uploaded is ready for download the client will then use a unique ID acquired from the notification plus his or her JWT to make a request to the API Gateway to download the MP3 and the API Gateway will pull the MP3 from mongodb and serve it to the client and that is the overall conversion flow and how rapidmq is integrated with the overall system okay so now that we have a clear understanding of the overall flow of our system we can now use that understanding to familiarize ourself with some key terms when considering microservice architectures those terms are asynchronous and synchronous interservice communication and strong and eventual consistency let's start with synchronous interservice communication because understanding that will make it easy easier to understand everything else so synchronous interservice communication put simply means that the client service sending the request awaits the response from the service that it is sending the request to the client service can't do anything while it waits for this response so it is essentially blocked so this request is considered a blocking request for example our Gateway service communicates with our auth service synchronously so when the Gateway service sends an HTTP post request to our auth service to log in a user and retrieve a JWT for that user our Gateway service is blocked until the auth service either Returns the JWT or an error so communication between our API Gateway and our off service is synchronous which makes those two Services tightly coupled now on the other end of the spectrum we have asynchronous interservice communication so with asynchronous interservice communication the client service does not need to await the response of the downstream service therefore this is considered a nonblocking request this is achieved in our case by using a cue for example in our architecture our Gateway service needs to communicate with our converter service but if our Gateway were to communicate with our converter service in a synchronous manner the performance of our Gateway would take a hit because if the Gateway were to get many requests to convert large videos the processes that make requests to the converter service would be blocked until the converter service finishes processing the videos so let's say that hypothetically our Gateway service processes one request per thread concurrently or at the same time if we have two processes with four threads each that's eight concurrent requests if our Gateway were to get more than eight requests to process large videos the entirety of our gateways threads would be blocked awaiting the completion of the processing of each request so in this case synchronous communication between our Gateway and our converter service would not be scalable and this is where our queue comes into the picture as explained before our Gateway doesn't communicate directly with our converter service therefore it does not depend on the converter Services response this means that in our current architecture our Gateway and our converter service are Loosely coupled this decoupling is done by using the queue our Gateway just stores the video on mongodb and throws a message on the queue for a downstream service to process the video at its convenience so in this case the only thing holding up the threads on our Gateway service is the uploading of the video to mongodb and putting the message on the Queue which means that our Gateway Services threads will be freed up much quicker allowing for our gateway to handle more incoming requests so with the current architecture our Gateway service is asynchronously communicating with our converter service that is it sends requests to the converter service in the form of messages on the Queue but it doesn't need to wait for nor does it care about a response from the converter service it essentially just sends and forgets the message and this same thing is happening with the communication between the converter service and the notification service now let's get into strong consistency versus eventual consistency let's start with an example of what our application flow would look like if it were strongly consistent so let's say that hypothetically whenever a user uploads a video to our gateway to be converted to an MP3 we make a synchronous request to our converter service waiting for the conversion to complete and then in response the converter sends an ID for the MP3 back to the user once the conversion is complete at the point that the user received that ID it's certain that the video has been processed and converted into an MP3 and that the data is consistent with the update so at that point if the user were to request to download the data based on that ID the user is guaranteed to get the most recent update of that data so that is strong consistency eventual consistency on the other hand is a bit different so let's use our actual architecture as an example in this case for the sake of example let's say that hypothetically when our Gateway uploads the video to mongodb and puts the message on the queue for it to be processed we return a download ID to the user at that moment I know that we don't return a download ID to the user at that moment in our actual application this is just to help you to understand if the user were to upload a video that takes one minute to process but immediately after receiving the ID the user tried to download the MP3 the MP3 would not yet be available because it would still be processing in that case but the MP3 eventually will be available so if the user were to wait one minute and then request to download the MP3 with that same ID at that point the MP3 would be available therefore the data is eventually consistent and that is eventual consistency okay so the first route that we're going to make for our Gateway is going to be our login route and we're going to define a function called login and what this route is going to do is it's going to communicate with our auth service to log the user in and assign a token to that user so we'll set token error equal to access Dot Login and we're going to pass in the request and this request is from flask and we're importing it here and we're going to create a module called access that's going to contain this login function so let's just go ahead and save this and let's clear and in our Gateway directory we want to create another directory that we'll call auth service and we'll CD into that directory and we'll create a file called init.pi which is essentially going to mark this directory as a package and we'll also create a file called access.pi which is going to be the module that contains our login function and we're going to need to import OS and we also need to import requests and this request is different from the request that we import from flask this request is going to be the module that we use to make HTTP calls to our auth service so we can go ahead and Define log in and we take in request which is not to be confused with this requests and we're going to set auth equal to request Dot authorization and as we write the code for this just pay close attention to requests versus requests with an S at the end because they're different so this request object has this authorization attribute so if when we create this variable this variable resolves To None So if not auth that means that there's no authorization parameters in our request so that means that we need to return none for our token and we need to return for our error missing credentials and a 401 so if we go ahead and save this and quit and go back into our server.pi file we see that we're setting token and error equal to access.login so access.login needs to return a tuple and the first item in the Tuple will go to token and the second item in the Tuple will go to error so if we go back to the definition in this case when we're missing credentials we're going to return none for the token and we're going to return an error but upon the successful login of a user we're going to return none for the error and we're going to return an actual token so we'll set basic auth equal to auth.username and auth.password and we've already gone over basic auth so you should already be familiar with this and we're going to set our response equal to requests dot post now this request is going to be the request that's going to make the HTTP call to our auth service so let's save this actually and we need to install requests and let's go back into our access file so this request.post is going to make a post request to our auth service and the parameters or the arguments that we need to pass are the string the URL endpoint string and our auth header and the way that we do that is we can just create a formatted string and we'll do OS Dot environ.get and we'll just get our auth service address which we're going to create so we're going to create this off service address environment variable which is going to be the address for our auth service and we're going to need to access the login endpoint and to create the basic auth header we're just going to do auth equals basic auth and once this request completes this response is going to contain the result and we're going to check if response dot status code equals 200 that means that we're good we're going to return response dot text which is going to be our token and none for the error otherwise we'll return none if we don't get a 200 response it means we didn't get our token so we'll return none and we'll just return response.txt and response dot status code and we can go ahead and save that and this should be double equal and let's format and this is spelled wrong and we can save this file and let's just clear this let's just change directory back to our main directory and go back into server.pi so once we call this access.login we're going to check if not error and that's because if there is an error this is going to contain an error but if there is no error this is going to be null or none and if that's the case we can just return our token otherwise if there is an error then we're just going to return the error and that's going to be it for our login function now the next route that we want to create is our upload route and this is going to be the route that we use to upload our video that we want to convert into an MP3 so we're just going to call it upload and methods will be just post so we'll Define a function called upload and now for this route we need to make sure that the user has a token from our login route so we need to validate our user so we're going to set access and error equal to validate dot token and we're going to create this validate module as well and we're going to pass in the request so let's go ahead and create this validate module with this token function so we'll go ahead and save and let's clear and we'll make dur and this dirt will just call auth the other one we call it auth service because we're communicating with the auth service on behalf of the user or the client but auth is going to just be used internally so our Gateway is going to use this auth package to validate tokens that it receives from the client so we'll make their auth and let's change their auth and once again we need to create this init.pi file to mark this directory as a package and we'll create a file called validate.pi and we will import OS and requests once again and we'll Define a function called token which takes in a request because we're validating a token so remember the flow is the client's going to access our internal services or our endpoints by first logging in and getting a JWT and then for all subsequent requests the client is going to have an authorization header containing that JWT which tells our API Gateway that that client has access to the endpoints of our overall application so this validate token function that we're creating here is going to be the function that validates that JWT sent by the client so we need to first check to see if the client has the authorization header in his request or his or her request foreign is not in request.headers we're going to return none for our access and we're going to return an error that says missing credentials and it's going to be a 401. otherwise we're going to set our token equal to request dot headers authorization and if that token does not exist we'll return none as well and the same error missing credentials 401 but if the token does exist and the authorization header exists we'll set response equal to request dot post and this should be requests so don't make the same mistake I did and once again we're going to send a post request via HTTP to our auth service so once again we'll do a formatted string and we're going to get our host from the environment using the environment variable auth service address and we're going to access the validate endpoint and headers will be equal to authorization token so we're basically just passing along the authorization token to our validate request of our auth service and we want to check the response if response dot status code equals 200 then we're good so that means we're going to return response.txt and none and response.text is going to contain the body which will be the axis that the bearer of this token has and you'll see what I mean by that when we parse this else if the response from our auth service isn't 200 we're going to return none and we're going to return an error so response Dot txt and response dot status code and we can save that and let's go back to our root directory and back into our server.pi file okay so let's take a second to understand what's happening here with our validation before moving forward so let's quickly go back into our auth service and into the server.pi file and let's go over our login route so if you remember our login route here is going to take a username and a password or an email and a password and return a token and the token is going to be a Json web token and that token is being created here in this create JWT function and as you can see in this function we're encoding a payload and that payload is here and this payload contains our claims which is basically these data points within the payload so our username is a claim the expiration date is a claim and also contained within this payload is this claim here for admin which is just a bull that's going to be true or false and as I said earlier we're just going to allow anybody with admin equal to True access to all of the endpoints of our services so the token that's returned to the logged in client is going to contain this payload but the payload is going to be encrypted and when that client sends their token in their request and we validate it in using the validate endpoint for our off service we're going to first check to see if the token exists in the request and then we're going to decode that token and when we're decoding the token we're using the same key that we signed the token with this JWT secret which is how we know that this is a valid token because our auth service is the service that signed the token using this key and when we decode the token we're using the same key the service is key so if somebody were to send a token that was signed with a different secret key then of course it wouldn't work and when we decode this token with this decoded variable is going to include that payload that tells us who the user is via their username which is their email and their privileges which is the auth's claim which is true or false so now let's go back into our Gateway code code and here when a user tries to upload they need to have a token header which we're going to validate using our function to validate the token and the Gateway is just going to forward this token to our auth services validate endpoint and the response that we expect from our auth service is that decoded body so here when we get a successful response a 200 response what we're returning here in this response.txt is going to be the body of that token containing the claims so it's essentially it's going to be the decoded token where the body is visible and it's going to be a string which is going to be Json formatted and that means that here this access variable is going to resolve to that Json string that contains our payload with our claims so here we'll set access equal to Json dot loads access and if we go to the definition of this you see that it deserializes an instance containing a Json document to a python object so we're essentially just converting this Json string to a python object so that we can work with it in code and what's being converted into a python object is this here so our python object is going to look like this once we've decoded the Json so let's just go back and we can go ahead and close this so this access is going to contain that object and that object has the admin claim which is going to be a bull which is true or false and actually just to be clear let me show what I mean by that so this admin claim here is going to contain the auths which is a wool which is true or false and if it's true we'll give the user access to all of the endpoints so we're going to check for that claim so we're going to say if access which is the object that was converted from the Json so we're going to say if access admin which is essentially saying if the admin claim resolves to true then we're going to give the user access so let's go ahead and close that again so if the user does have access we're going to make sure that there's a file to be uploaded in the first place so if there's a file being uploaded the request should contain a dictionary in this request.files so we're going to say if the length of request.files is greater than one because we're only going to allow the uploading of one file at a time for now or the length of request.files is less than one because we want there to be exactly one file so we don't want more than one file then we don't want less than one file so if one of these is true then we're going to return exactly one file required and a 400. and this request.files dictionary is going to have a key for the file which will be defined when we send the request and the actual file as the value so that means that we should iterate through the key values in the request.files dictionary so we'll do for key which we don't need to use and file which is the value in request.files.items for every file we're going to upload it where we're going to do an upload and there should only be one file so this should only happen once so we'll do util dot upload and we haven't created this function yet but we will and this function is going to take us parameters to file our grid FS instance our rabbitmq Channel and the axis which was just explained above and this function is going to return an error if something goes wrong but if nothing goes wrong then it won't return anything it'll return none so to check if something went wrong we're just going to check if error and if there is an error we're just going to return error and after this for Loop completes if we've never returned an error then that means it was successful so we'll just return success with a 200 and that's what's going to be happening if the user is authorized but if the user isn't authorized so we need to go down here and do else so this is the block that's going to get executed if the user isn't authorized and in that case we're just going to return not authorized and a 401 and that's going to be our upload route and remember we still need to go and create this upload function but I'm going to get to that in a second because that function is going to be a little bit involved so before we do that let's just finish up our template here for our Gateway server so we'll do the final endpoint which is going to be server.route and this is going to be the download endpoint which is the endpoint that is going to be used to download the MP3 that was created from the video and this is going to be methods and we're only going to do git and we're going to Define download and for now we'll just pass this is just a template portion for this function or this endpoint and lastly we need to set if name equals Main we're going to run our server and again we're going to set our host to 0.0.0.0 and our port in this case is going to be 880. and this is going to be our Gateway service template including both the login endpoint and the upload endpoint and to be continued on the download endpoint so let's go ahead and save this and actually let's go back in here so I don't confuse you so now what we're going to need to create is this upload function here and this util as you can see is coming from this here so from Storage import util so we need to create a storage package and within that package we need to create a util module so we'll make their storage change there into storage and we'll create our init.pi file and now we'll do util.pi and we'll start by importing Pica and Json and we'll Define a function called upload and remember the parameters are the file our grid FS instance our rabbitmq Channel and our access which is the user's access now this function is going to be a little bit complicated so try to keep up and pay attention I'll try to explain things the best that I can so basically with this upload function needs to do is it needs to First upload the file to our mongodb database using gridfs and once the file has been successfully uploaded we need to put a message in our rabbitmq so that a downstream service when they pull that message from the queue can process the upload by pulling it from the mongodb and this queue is allowing us to create an asynchronous communication flow between our Gateway service and the service that actually processes our videos and this asynchronicity is going to allow us to avoid the need for our Gateway service to wait for an internal service to process the video before being able to return a response to the client so the first thing that we want to do is we want to try to put our file into the mongodb so when we put a file in the mongodb we're going to use this FS dot put function and it's going to be the file that we want to put and if this put is successful a file ID is going to be returned a file ID object to be more specific so mongodb is going to return a file ID and that's if it's successful but if it's not successful then we want to catch the error and for now we're just going to return internal server error and if this returns this means our file wasn't uploaded successfully and the function just returned so we don't need to do anything else after that but if the file was successfully uploaded we need to create a message to put onto our queue so we'll set message equal to a dictionary and it's going to contain the video file ID which is going to be the file ID object converted into a string and we also need to create an empty MP3 file ID within this same dictionary and for now it's going to be none but Downstream it's going to end up being set to the mp3s file ID in the database but you don't need to think too much about this right now we'll get to it later and we need to also have the username to identify who owns the file and the username is going to come from our access from our auth service and remember there was a claim for username there which contains our user's email and remember in our auth DB the email must be unique so this is a way to uniquely identify our user so that's going to be our message and now we need to put that message on the Queue so we're going to try and put this message on the queue using the channel that's passed to the function and we're going to do basic publish and we're going to set exchange equal to an empty string and this is just going to mean we're going to use the default exchange so I will explain a bit more about how rabbitmq works so for our purposes we're going to use a very basic rapidmq configuration and setup but we need to go over a couple of things so that we have a clear understanding of what's Happening Here let's start with the top level overview of how rabbitmq integrates with our system the first thing that is important to understand is that our producer which is the service that is putting the message on the Queue isn't publishing the message directly to the queue it actually sends messages through an exchange The Exchange is basically a middleman that allocates messages to their correct queue throughout the video I've been referring to rabbitmq as if it were a single cue but under the hood we actually can and do configure multiple cues within one rabbitmq instance for example in our case we'll make use of both a cue that we'll call video and a queue that we'll call MP3 so when our producer publishes a message to The Exchange The Exchange will route the message to the correct queue based on some criteria so how does our Exchange Route messages to the correct queue in our case well since we're going with a simple rabbitmq configuration for the sake of brevity you'll remember that we are using the default exchange by setting the exchange to an empty string and if we take a look at the rabbitmq documentation here the default exchange is a direct exchange with no name EG the empty string predeclared by the broker and broker just being our rapidm queue instance and this default exchange has one special property that makes it very useful for simple applications and that is that every queue that is created is automatically bound to the default exchange with a routing key which is the same as the Q name so what does that mean exactly simply put that just means that we can set our routing key to the name of the queue that we want our message to be directed to and set the exchange to the default exchange and that will result in our message going to the queue specified by the routing key so with this overview we can see our video queue the exchange our producer and our consumer so let's say that this producer is our Gateway service and this queue is our video queue and this consumer is our video to MP3 converter when the user uploads a video our Gateway stores the video and then publishes a message to The Exchange that is designated for the video queue The Exchange will route that message to the video queue and the downstream service which is the consumer of the video queue will process the message the consumer in this case is our video to MP3 converter service so it will process the message by pulling the video from mongodb converting it to MP3 storing the MP3 on mongodb and then publishing a message to The Exchange that is intended for the MP3 queue but let's not focus on the mp3q just yet we'll get to that later let's just focus on the video queue for now let's say for example our producer is paddling on more messages than our one consumer can process in a timely manner this is where the capability to scale up our video to MP3 consumer comes into the picture but if we're going to scale up our queue actually needs to be able to accommodate multiple instances of our consumer without bottlenecking the entire flow so how do we manage that we manage that by making use of a pattern called the competing consumers pattern this pattern simply enables multiple concurrent consumers to process messages received on the same messaging channel that way if our queue is packed full of messages we can scale up our consumers to process the messages concurrently resulting in more throughput luckily by default our rapidm QQ will dispatch messages to our consuming services using the round robin algorithm which satisfies our needs this basically just means that the messages will be distributed more or less evenly amongst our consumers for example if we have two instances of our consuming service the first message would go to this instance and if another message were to come in it would not go to the same instance it would go to the next one and the same goes for if we already have a bunch of messages on the Queue the distribution of the messages will essentially go in a sequence from one instance of the consumer to the next to the next and then back to the first Etc so basically the messages will be distributed evenly in a round robin fashion so that they can be processed concurrently and that is going to be that so now that that's out of the way let's get back to right adding our code and then we're going to set our routing key equal to video so the routing key is actually going to be the name of our queue and we're going to have a queue called video where we put the video messages on and then the body of the message is going to be Json dot dumps message now similar to json.loads this dumps converts a python object into a Json string so as you can see it serializes the object a python object to a Json formatted string so it's basically doing the opposite of Json loads because the message we need to have a string contained within it not a python object and the python object I'm talking about is this here this is being converted into a Json string which is going to be the body of our message which gives our Downstream service all of the information that it needs to process the video conversion and we need to also set properties equal to Pika dot basic properties and within these properties we need to set the delivery mode and that's going to be set to pica.spec dot persistent delivery mode and this part is very important to make sure that our messages are persisted in our queue in the event of a pod crash or a restart of our pod so since our pod for our rabbitmq is a stateful pod within the kubernetes cluster we need to make sure that when messages are added to the queue they're actually persisted so that if the Pod is reset or the Pod fails when it comes back up or spins back up the messages are still there because if we don't set this if the Pod crashes or is reset then the messages are all going to be gone once the Pod is restored back to its original state so what we're essentially going to need to do is we're going to need to make our cue durable which means that the queue is going to be retained even after a pod restart and we need to make sure the messages within the queue are also durable which means that the messages are going to be retained even in the event of a pod restart or crash so when we actually create the queue we can configure the queue to be durable but that doesn't mean that the messages are going to also be persisted it just means that the queue will be durable so each individual message that we send to the queue we need to set this configuration here to tell rabbitmq that the message should be persisted until the message has been removed from the queue so anyways we're going to try to put our message onto our queue and if that doesn't work out we need to first delete the file from our mongodb because if there's no message on the queue for the file but the file still exists in the DB that file is never going to get processed because the downstream service doesn't know that that file exists if it never receives a message telling it to process that file so we'll just end up with a bunch of stale files in the database if we don't delete them in the event that a message can't be put onto our queue so if the message is unsuccessfully added to the queue we'll do fs.delete and we need to delete using the file ID and the file ID is the file ID object not the string it's defined here and once we've deleted the file we can then return internal server error so in this case in the event of a failure we will neither have a message on the Queue telling the downstream service to process the file and we will also not have a file in our database so it's going to be a complete failure so we can just return internal server error and at that point the user could just upload the file again if they want to try again and this is going to be it for our upload function and we're not using this error variable yet but we might use it later on down the line Depending on time constraints so we'll just leave it for now so let's go ahead and save and quit and we can clear okay so at this point we can go back to our root directory and we can start creating the deployment for this Gateway service so to start we just want to go ahead and freeze our requirements our dependencies into a requirements.txt file and we can start to create our Docker file now our Docker file is going to be quite similar to the one that we used for our auth service so we can actually just go to our auth service and get this Docker file and paste it into this one and everything is pretty much going to remain the same except for we're going to change the exposed port to 8080 and we also don't need this here but other than that the file is going to be identical so we can just go ahead and save this so now let's go ahead and build our image so we'll do Docker build and once we've built the image similar to before we can go ahead and tag our image so we'll do Docker tag and we'll go ahead and take the beginning of this and paste it in and your username for your Docker Hub account and this time instead of auth we're going to say Gateway and latest now at this point on your Docker Hub account you'll only have one repository which is the repository for our auth service and if we want to we can just create the Gateway repository manually but it'll actually create itself once we push to our username with the suffix Gateway so if we just do docker push and username slash forward slash Gateway latest you'll see that if you refresh the page here it automatically created the Gateway repository for us and if we go in here we'll see that we pushed the tag latest a few seconds ago so now we have our Gateway repository in our Docker Hub account so now that we can pull the image containing our source for our Gateway service from the internet we can go ahead and create our kubernetes manifest directory and we can go ahead and change there into there and similar again to our auth service we need to create a Gateway deployment yaml file and this configuration is also going to be very similar to our off service and this time we're going to pull our image from the Gateway Repository so remember your username and then Gateway and similar to the auth service we're going to create a config map called Gateway config map and let's not make the same mistake as last time this should be indented and for our secret ref we're going to create a secret as well and that's going to be it for our deployment and now let's go ahead and make our config map and the data that we need for our environment variables we need our auth service address and in kubernetes the service name will resolve to that service's host IP address so we can just put auth and then the port that the service is available at so we'll do all 5000 and that's going to be the address of our auth service within our kubernetes cluster and we can go ahead and save that and now let's create our secret and as of right now I don't think we have secrets for our Gateway service so let's just put a placeholder here for now and later we can add any secret variables if we need to and let's save that and we need to create service.yaml and the name of our service will just be Gateway and this service will have an internal IP address which will only be available within our cluster but our Gateway API needs to be able to be accessed from outside of our cluster so we're actually going to need to create another configuration called an Ingress to Route traffic to our actual Gateway service which I will get into in a second so we need to set ports equal to 80 80 Target port 8080 as well and protocol is of course TCP so now we'll create the ingress.yaml which is going to allow traffic to access our Gateway endpoint so let's take a second to understand what an Ingress is in the context of a kubernetes cluster okay so in order to understand what an Ingress is in the context of kubernetes you first need to understand what a service is in the context of kubernetes so with this configuration file we're creating a service and you can really just think of the service as a group of PODS so in our case we want to create the Gateway service and we want that service to be able to scale up to multiple instances or multiple pods so the service comes in and groups all of the instances of our Gateway service together and it does this by using a selector so in our case we're using this label selector to tell our service what pods are part of its group or under its umbrella so this label selector essentially by lines our pods to the service so any pod with this label will be recognized by the service as being part of its group so now we don't have to think about individual IPS for each individual pod and we don't have to worry about keeping track of the IPS of PODS that go down or are recreated we also don't have to think about how requests to our service are load balanced to individual pods the service abstracts all of this away from us so now we can just send requests to the services cluster IP which remember is its internal IP and we can assume that these requests will be distributed logically amongst our pods based on something like round robin for example which we already went over when explaining webinimq so now that we have a clear picture of a service we can get into what an Ingress is so we have our service with its pods and that service sits in our cluster which is our private network but we need to allow quests from outside of our cluster to hit our Gateway Services endpoints we do this by making use of an Ingress simply put an Ingress consists of a load balancer that is essentially the entry point to our cluster and a set of rules those rules basically say which requests go where for example we have a rule that says that any request that hits our load balancer via the domain name mp3converter.com should be routed to our Gateway service so since this load balancer is the entry point to our cluster it can actually Route traffic to the cluster IPS within the cluster so in this case it would route requests going to the configured MP3 converter.com domain to our Gateway Services internal cluster IP and if we wanted to we could even add a rule to our Ingress that says to Route requests to apples.com to a different service in our cluster for example so that's pretty much everything you need to know about Ingress for the purposes of this video so let's get into writing our Ingress configuration file so for the API version we're going to set networking.kh dot IO V1 and kind is going to be Ingress this time and we're going to name it Gateway Ingress and we're going to use the default Ingress which is basically an nginx Ingress and we can set some configurations for our nginx Ingress using this annotations key and we want to make sure that our Ingress allows the upload of some relatively large files so we're just going to set our Ingress body size to zero which is essentially going to allow any body size and of course you want to finetune configurations like these in a production application but again our focus is the overall architecture so we're basically just configuring this in the easy way possible to get things done and this is a typo and we're going to do two more configurations proxy read time out and we're going to set that equal to 600 and proxy same time out and we'll also set that one to 600. now for our spec and the rules we're going to Route request to the host MP3 converter .com to our Gateway service so remember our service name is Gateway and our service is available at Port 8080. so you're probably wondering how our kubernetes cluster knows if we're making a request to this host and basically what we're going to do is we're going to make it so that requests to this host on our local machine gets routed to the Local Host so we're just going to map this hostname to localhost on our local machine and we're going to Tunnel requests to our Local Host to minicube which sounds a bit complicated but just bear with me and we'll get to it so we're going to save this file and first we need to make sure that the MP3 converter.com gets routed to localhost so you need to open a file called Etc hosts and you're going to have to have pseudo permissions to do that so once in this directory you want to map this address 127.0.0.1 which is the loopback address which localhost also resolves to we want to map it to MP3 converter.com so once we do this whenever we enter This MP3 converter.com into our browser or if we send a request to this host it's going to resolve to localhost so we can go ahead and save that and now we need to configure a mini Cube addon to allow Ingress so we'll do mini Cube addons and let's list the addons that are available and as you can see there's an Ingress addon here that's currently disabled so we can do mini Cube addons Ingress enable I believe or we can do enable Ingress and once that's done as you can see here it says that after the addon is enabled please run minicube tunnel and your Ingress resources would be available at this loopback address that we mapped to mp3converter.com so basically whenever we want to use our microservice architecture or test this overall architecture we're going to run this mini Cube tunnel command and while this is running whenever we send requests to our loopback address they're going to go to our mini Cube cluster via the Ingress and since we mapped mp3converter.com to our loopback address if we send requests to MP3 converter.com they're going to go to this mini Cube tunnel so just keep in mind so we control C out of this whenever we cancel this process then we're no longer tunneling the Ingress so as you can see here it says please do not close this terminal as this process must stay alive for the tunnel to be accessible so whenever we test we're going to have to run this mini Cube tunnel basically and we'll get to endtoend testing a little bit later we're not going to test yet until we configure our q and our consumer service as well as our Gateway service which will be the producer service and that is how we are going to Route requests into our cluster and directly to our API Gateway so if we go and check K9s you may or may not see these two new items here but if you remember we've only deployed our auth service so far so we still need to deploy this Gateway service and similar to the off service we're going to have two replicas of our Gateway deployed so we can quit this and we can attempt to apply our configuration that we have here and it looks like we have an issue here so let's go into our secrets file and it looks like we forgot to capitalize Secret so let's once again try to apply and as you can see all of the resources or the objects were created successfully so let's go into canines and we're having an error with pulling the image here so let's see so it's actually not in error with pulling the image so the Gateway is failing because we basically don't have the rabbitmq queue deployed yet and it's trying to connect to this host here that we haven't yet created so that's fine for now so for now it's not going to be able to deploy so actually so it's not continuously running let's just scale that down for now until we create our rabbit mqq deployment so we can do Cube CTL scale and we can do deployment and we want to change the replicas to zero and we want to do that for the Gateway service so it says that our Gateway service was scaled so now you can see the the Gateway service is gone now it's not trying to be scaled up currently because we need to create that rabbitmq so let's go ahead and quit and from here we can start to get into the rabbitmq stuff so we need to create and deploy a rabbit and Q container to our kubernetes cluster so we're just going to change directory back to our source directory and currently we have our auth directory and our Gateway directory now we need to make a directory for our rabbitmq so we're just going to call it rabbit and we'll CD into rabbit and for rabbit mq instead of making a deployment like we did for the other two Services we're going to need to make a stateful set because we want our cue to remain intact even if the Pod crashes or restarts we want the messages within the queue to stay persistent until they've been pulled from the queue so let me go ahead and explain what a stateful set is okay so a staple set is similar to a deployment in that it manages the deployment and scaling of a set of PODS and these pods are based on an identical container spec but unlike a deployment with a staple set each pod has a persistent identifier that it maintains across any rescheduling this means that if a pod fails the persistent pod identifiers make it easier to match existing volumes to the new pods that replace any that have failed and this is important because if we were to have multiple instances of say for example a MySQL server each individual instance would reference its own physical storage and actually there would be a master pod that is able to actually persist data to its physical storage and the rest of the pods would be slaves and they would only be able to read the data from their physical storage and the physical storage that the slave pods use basically continuously syncs with the master pods physical storage because that's where all of the data persistence happens and most of the details surrounding this are not related to our architecture so I'll spare you the details actually to be honest there's probably a better way to configure our rapidmq broker within our cluster but this configuration will work just fine for our purposes we'll only be making use of one replica to achieve the before mentioned competing consumers pattern anyways the most important configuration that you need to understand for this particular service is how we are going to be persisting the data in our cues remember that if our instance fails we don't want to lose all of the messages that haven't been processed because then the users that uploaded those videos that produce those messages would just never hear back from us so basically what we want to do is we want to mount the physical storage on our local to The Container instance and if the container instance happens to die for whatever reason of course the storage volume that was mounted would remain intact then when a new pod is redeployed it will once again have that same physical storage mounted to it so let me show you what I mean by that to show you this I'll need to show you what the configuration file for our staple set is going to look like although we haven't written the code for this file yet just follow along so that you can understand where we're going with this so as you can see this configuration file follows a similar pattern to what all of the other configuration files did so I'm not going to go into detail about every single line if you need to please refer to the kubernetes API documentation that I introduced to you earlier but let's go ahead and have a look at containers here similar to our deployments this is going to determine the contain painter that gets spun up so the image we're using here is a rapidmq image but the part that we need to pay attention to starts here at this volume mounts Mount path so we want to mount a storage volume to our container right this Mount path is configuring where in our container we want the physical storage volume to mount to so basically anything that is saved in this VAR lib rabbitmq directory within our container we'll actually be getting saved to the physical storage volume that will persist even if the container fails and this particular directory is configured as the mount path for a reason this is actually where rabbitmq is going to store the cues when we create a durable cue and the messages when we configure them to be persistent and I'm going to go into detail about how we make the queue durable and the messages persistent a little bit later for now you just need to understand that we're mounting physical storage to this path and this is the path where rabbitmq will save cues and messages okay so now that that's out of the way the rest of the configure duration here under volumes is basically just the configuration for the physical volume to be mounted to the container for example we need to create an additional resource called a persistent volume claim and this config here basically links this staple set to the persistent volume claim that we're going to create and we're going to call that persistent volume claim rabbitmq PVC so what is a persistent volume claim in simple terms the persistent volume claim or PVC is going to be bound to a persistent volume and within the configuration for the persistent volume claim we'll set how much storage we want to make available to it from the persistent volume and the persistent volume will actually be what interacts with the actual physical storage and I know there are many layers of abstraction here but again for our purposes we really don't need to go into too much detail here all you really need to understand is that this configuration is going to make it so that the dura rabbitmq stores cues and messages will actually be persistent storage so whenever the Pod dies those cues and messages will be retained and the new pod will just reconnect to that persistent volume so let's go ahead and write up this configuration so now that we have an understanding what a staple set is we can go ahead and create a file called stateful set.yaml and actually we're going to want to make this in a manifest directory so we'll just make their manifest and CD into manifests and then we'll create the stateful set.yaml file in the here we'll set API version to apps V1 and this time kind is going to be stateful set and metadata will name rabbitmq and our spec configuration so the service name we're not going to use this so we'll just set it to not applicable and selector match labels as usual app rabbitmq now for our template and this is similar to the deployment it's going to be the template for our pods and we'll do metadata labels app is rabbit mq and our template spec or the Pod spec to be more specific through containers name rabbitmq and image is going to be rabbit mq three management and this is the official rabbitmq image and we're adding this management we're using the one that contains management because we want to have the graphical user interface to manage our cues as well included in the image and then we need to set ports and this container is going to need to include two ports we need the port to access the graphical user interface and we also need the port that handles the actual messages for example the messages that we send to the queue are going to be handled on a separate Port from the port that handles our connection to the graphical user interface and you'll see what I mean by that so we're going to set the name of this port the first port to http because we're going to use HTTP to access the graphical user interface and protocol will be TCP and container Port is going to be one five six seven two and we're also going to need a port for amqp which stands for advanced message queuing protocol and this is just the protocol that we use to send messages to the queue and container Port here is going to just be five six seven two and after ports we'll do environment from and we're still going to use config map and we'll call it rabbitmq config map and secret ref name will be rabbitmq secret and we're also going to need a key called volume mounts and we need to specify the mount path and this is going to be the path within the container that we want mounted so we'll do VAR lib rapid mq and the rabbitmq server is going to store the persisted data like the messages and the cues in this directory or at this path so we want to mount this path to our volume and our volume is essentially going to be storage that we connect to our kubernetes cluster which is where persisted data is going to be stored so name will be rabbitmq volume So within spec at the same level as containers we're going to do volumes name rabbit mq volume and we're going to use persistent volume claim and claim name will be rabbitmq PVC which we need to create and that's going to be it for this stateful set configuration so we can save that so now we need to create our persistent volume claim so we're going to do pvc.yaml and we'll do API version P1 and kind this time is going to be persistent volume claim and metadata we're going to do name rabbit mq PVC which we just referenced in our stateful set file and spec and access modes is going to be read write once resources requests storage one gigabyte and storage class name will be standard and we can save that and as usual we need to create our service.yaml API version V1 kind service metadata name of the service will be rabbitmq and our spec so type again is going to be cluster IP our service is only going to have an internal IP address which is only accessible within our cluster and selector app rabbitmq and ports remember that uh ports we're going to need to have the port for our graphical user interface so basically the rabbitmq like Management console and then we need the port for actual message transmission so we'll do ports name http protocol TCP and we're just going to do port 15672 and we'll do Target Port is the same and then we'll do our port for our message transmission which will be amqp again and we'll do protocol TCP and this one's going to be Port 5672 and Target board is the same port and we can save that and actually one second we need to go back in here so as you can see we're going to need to allow access to this port from a web browser so we're going to need to allow access from outside of the cluster directly to this port so that we can access rabbitmq's Management console so to do that we need to create an Ingress for this port as well so let's go ahead and quit and do Vim Ingress and we'll do API version networking.kh.iov1 and kind is going to be Ingress and a data name rabbit mq Ingress and spec rules and host and we're going to do it at rabbitmqmanager.com which we need to configure in our Etc host file and it's going to be http aths and path type prefix back end service and the back end service is going to be rabbit in queue more specifically rabbitmq's port number 15672 which is the port number for the Management console so we can save that and we need to open this file again and we're basically going to do the same configuration but it's going to be for rabbit mqmanager.com I think that's what we called it yeah rabbitmqmanager.com so we can close that save this and close it and let's make a config map and I don't think we currently need any environment variables but let's just make a template just in case we need to add some and we'll do the same thing for a secret and that should be everything so let's go ahead and try and apply this so we have a couple of Errors the first one here is just a spelling error so config map I misspelled the key metadata so let's go in there and fix that meta data and let's apply again and in the service.yaml it's saying that the API version is not set so let's go and service.yaml and that's because I put ape version so API version and let's apply again and it says that stapleset.spec.templetunknown field volumes and that's because volumes should be at the same level as spec so let's go into stapleset.yaml actually the issue is the opposite of what I said volumes should be at the same level as container so it's under spec so we need to move this in one so let's save that and let's apply again and now it seems all of the objects were created successfully so let's go into canines and we see that our rabbitmq pod is pending and let's go have a look and it seems something's not working as expected so let's do subscribe pod rabbitmq and we have an event here warning failed scheduling one pod has Unbound immediate persistent volume claims okay so it seems there's an issue with our PVC so let's go ahead and do qctl describe PVC and we have another event here warning provisioning failed it says storage class storage uh stranded so it can't find the storage class because the storage class is standard and I have a typo so let's go ahead and Vim into our PVC file and here is the error it should be standard so let's save that and let's apply again and actually for persistent volume claims as said here the spec for this is immutable after creation so we're actually going to have to delete the resources created using these files and we only really need to delete the persistent volumes claim but it doesn't matter we'll just delete all of the resources created with this file or created with these files so basically we're going to use this Cube CTL delete command and we're going to have the flag f for files and we're going to delete all the resources created with the files within this directory so let's just delete them all and now that we're done deleting them we can just go ahead and apply them again and it seems they're created successfully so we can go into canines and it looks like the container is creating and it looks like everything is going as expected so let's leave the logs and leave the container so yeah now we have our rapidmq instance running within our kubernetes cluster let's go ahead and quit since we configured an Ingress for this and we configured this route in our Etc hosts file we should be able to access rabbitmqmanager.com and that should take us to the rabbitmq manager so let's try that so let's go ahead and go to rabbitmqmanager.com it's not working because we forgot to do mini Cube tunnel so let's go ahead and clear this and do mini Cube tunnel and as you can see here it's trying to start a tunnel service for both our Gateway Ingress and our rabbit mq Ingress so let's see if we can access the Management console now so let's just refresh this page and I'm just going to accept the risk and there we go we have access to our Management console and let me just go ahead and zoom in here so the username and the password for this should just be admin and login failed so maybe that's not the correct credentials so let's just go to Google and type in rabbitmq Management console default credentials and here it actually says the username and password are both guest so let's try that so we'll do guest and guest and there we go we are logged in zoomed in a little bit too much and this is what the Management console is going to look like and you don't need to get overwhelmed by all of this we're going to limit our Focus to just this cues section so for example we're going to create our cues here using this add a new queue and yeah so we're going to make use of two cues one of them is going to be called video which is going to be the queue where we put our video messages and let me just show you to give you a bit of a refresher so remember while we're actually using our Ingress we can't exit this so we're going to need to open up a new terminal or a new tab so I'll just open a new tab and I'll zoom in here and I'll just change directory to system design python Source rabbits and actually what I wanted to show you is in the Gateway directory so in our server.pi when we upload we use this util.upload and in here we're putting the message on the Queue called video so routing key is just the queue so in the console here we actually need to create that cue so we're going to create a cue called video and this durability needs to be set to durable because if it's transient then that means that if the container is restarted or shut down or anything the queues no longer going to exist you're going to need to create it again so durable means that the queue will be essentially persisted so if the container restarts or something the queue will still exist afterwards so we'll just add this cue so now we have our video queue here so now let's see if we can start up our Gateway server so we can just quit here and canines quickly so yeah we want to spit up our Gateway service so we'll go ahead and let's just change directory into Gateway manifest and we'll do Cube CTL apply we'll apply all of the files in this directory and let's do canines again and as you can see now our Gateway service is able to start up with no issues so at this point we have our Gateway service and our off service and our rabbitmq queue service up and running within our cluster so that means that right now we can upload files and messages for those uploads will be added to the queue and at this point we have no consumer service to consume the messages from the queue to actually convert the files so we need to create an additional service and this service is going to pull messages off of the queue that the Gateway adds to the queue and it's going to convert them into MP3 and then it's going to store the MP3 and mongodb and put a message onto another queue called MP3 which will have another Downstream service pull from but I don't want to confuse you all too much so let's just do this step by step so we can leave this and we need to go back to our source directory because we need to create another service so we're going to make dur converter and this converter service is going to convert videos to MP3 so this is going to be the consumer service that pulls the messages off the queue so it knows which videos it needs to convert and where they're stored Etc so we'll make this directory and we'll just CD into that directory and let's clear and we're going to make a file called consumer.pi and in this file we're going to import Pica of course because we need to pull the messages off the queue sys OS time and from PI we need to import client and that's not how you spell import and we also need to import grid FS because we need to take the files from mongodb to video files and we also need to upload the MP3 files to mongodb and from convert which is a package that we're going to create ourselves we're going to import to MP3 which is going to be of a module within that package and we're just going to define a function called Main and our client is going to be equal to mongodb client and it's going to be our mongodb host which is on our local machine it's not deployed in our cluster remember so we need to use this host mini Cube internal which basically gives us access to our host systems local environment and the port for mongodb is 27017 and we're going to do DB Videos equals client dot videos so this instance of client is going to give us access to the DBS that we have in our database so we can do DB MP3s equals client Dot MP3s so these databases will exist within our mongodb and then we need our grid FS stuff so we'll do FS videos equals an instance of grid FS which we need to pass our DB Videos to and fsmp3s we need to do the same thing and now we need to configure our rabbit mq connection so connection will equal Pika dot blocking connection just like before and we'll do Pica dot connection parameters host equals rabbit mq and this is possible because our service name is rabbit and Q and our service name will resolve to the host IP for our rabbitmq service and channel will equal connection.channel so what we need to do is we need to create a configuration to consume the messages from our video queue and to do that we're going to use this Channel and basic consume and let's save this and the arguments that we need to pass to this basic consume is our q and we're going to get the Q name from the environment so the queue that we want to consume from in this case is the video queue but just in case we want to change it in the future we're going to just have an environment variable that contains our Q configuration so we'll do OS Dot environ.git and we'll name the environment variable video queue so that's going to be our q and we're going to need to create a callback function that gets executed whenever a message is pulled off of the queue so we'll say on message callback equals callback and we need to create this callback function so we'll go up here and Define callback so whenever a message is taken off the queue by this consumer service this callback function is going to be executed and it's going to be executed with the parameters Channel method properties and body and what we want to do when we get the message is we want to convert the video to MP3 so we'll do two MP3 dot start so there's going to be a function in our two MP3 module called start and we're going to pass in the body of our message and we're going to pass in FS videos and Fs mp3s and the channel and when we create this function you're going to see why we're doing all of this but for now we're just creating the Callback function that's going to call this function so we're going to set the result to error and if there is an error so if error so if there is an error we want to send a negative acknowledgment to the channel so we'll do basic Knack which stands for negative acknowledgment which basically means that we're not going to acknowledge that we've received and processed the message so the message won't be removed from the queue because we want to keep messages on the Queue if there's a failure to process them so they can be processed later by another process and here we're going to do delivery tag equals method dot delivery tag and this delivery tag uniquely identifies the delivery on a channel so when we send this negative acknowledgment to the Channel with the delivery tag rabbitmq knows which delivery tag or which message hasn't been acknowledged so it'll know not to remove that message from the queue but on the other hand if the error is none then that means there wasn't an issue with the conversion so we'll just go ahead and acknowledge the message so we'll do basic pack for acknowledgment and same thing we're going to do delivery Tech is method.deliverytag and if you see here method is one of the parameters that's passed to the Callback function and that's how we're keeping track of the delivery tag so that's it for our callback function and let's go ahead and format and we'll go ahead and print a message that just says waiting for messages and we can also put to exit press control plus c and basically once we run this uh main function here this is going to be printed and then we're going to do Channel dot start consuming and this start consuming is essentially going to run our consumer so our consumer is going to be listening to the queue or listening on that channel where our video messages are being put so we need to do if name equals Main we're going to try and run our main function and we're going to do accept keyboard interrupt so our main function is going to run until we press Ctrl C and interrupt the process and once that process is interrupted with bias pressing Ctrl C then that event is going to be captured in this try except and we're going to catch the keyboard interrupt which is US pressing Ctrl C to cancel the consumer process and in that case we'll just print interrupted and we'll try sis.exit and we'll accept system exit and then we'll do OS dot exit zero and this is basically us just gracefully shutting down the actual service in the event that we do a keyboard interrupt and that is going to be it for our consumer so we need to go and create this convert package and this two MP3 module and we also need to install some things and we also forgot to create a virtual environment so let's go ahead and save this and let's go ahead and do python 3 p e and B and now we can activate our virtual environment so Source then activate and we now are using our converter virtual environment and now we can just cat consumer dot pi or maybe it's better to do cat head in 10. so we need to install some of these dependencies so we'll do fifth three install Pica and Pi and actually I don't need the comma there so pip3 install Pika and Pi and let's go in here and it looks like we're good to go so let's clear so now we need to create our convert package so we'll just make dare convert and change directory into convert and we need to create the init.pi file and we need to create a module called to MP3 and then we're going to need to import a couple of things so we'll import Pica as usual and we need to import Json as well and we need to import temp file which I will show you what that's going to be used for in a second and Os as well and we need to import this binary Json dot object ID and from there we need to import object ID and I'll show you what this is doing or what this is going to be used for soon as well and lastly we need to import this movie Pi editor which we need to install and we're going to define a function called start and it's going to take in a message a gridfs videos instance a grid FS MP3s instance and a channel and for now let's pass so we can go ahead and recap so just to recap if we go into our consumer.pi file we're importing this module that we just created to MP3 and this two MP3 module contains the start function which is the one that we're creating now so we're going to be using this movie pie to convert our videos to MP3 so we're going to need to install pip install movie pi and from there we can start writing the code for our start function so the first thing that we're going to need to do is we're going to need to load our message which is essentially going to make it into a python object so let me just install something really quick so we're going to deserialize an instance containing a Json document to a python object so at this point our message contains the python object version of our message and the first thing that we want to do before converting the file is we want to create an empty temporary file and we're going to write our video contents to that temporary file so we'll do empty temp file is TF so we'll do TF equals temp file dot named temporary file and as you can see here it says create and return a temporary file and this temporary file is going to be a named temporary file as opposed to a temporary file that does not have a name so if we go to this definition here you can see that this temporary file has a name attribute where you can access the file's name so we're going to create this named temporary file and it's essentially going to create a temp file in attempt directory and we can use that temp file to write our video data to so we'll do video contents and we'll set out equal to FS videos dot get so now we're going to get our video file from grid fs and we're going to have it in this out variable or this out object is going to have a read method so we'll be able to write the data that's stored in this out variable to the file so we need to do object ID and message video fid which if you remember from our Gateway service in the actual storage util function we have video FID set to the file ID that was given to us after we put the video file into mongodb but if you also remember we have to convert that into a string because the FID that comes from the return value of this fs.put is actually an ID object so we needed to convert it into a string to put it into our message so we're taking our string version of our FID and converting it into an object and then we need to use that object version to get the file from our mongodb we can't get the file using the string version of the ID so just really quick let's go ahead and save that actually let's go back in here and for some reason it says no name object ID so let's see and I guess that's just an error so so once we have the video file data we can add video contents to empty file and we're going to do that by taking the empty file which is the TF variable and we're going to write to that file the data that's returned from this read method and this is the read method on the out variable or the out object as this read method which allows us to read the data stored in out so if we go to the definition here we can see that we can read the bytes from the file so the bytes that are returned from this read method here are going to be written to our temporary file here and then what we want to do is we want to convert our video file into audio so our temp file currently has the video file so we'll do create audio from temp video file so audio is going to equal moviepi dot editor Dot video file clip and then we're going to take the TF or the temporary file name which is actually going to resolve to the path of the temporary file and we're going to extract the audio from that file so all of this is going to resolve to our audio file being stored in this audio variable and the last thing we need to do is we need to close our temp file and with this temp file module here the temp file will automatically be deleted after we're done with it so basically after we close the file it will automatically be deleted so we don't need to worry about cleaning that up so now that we've extracted the audio into the audio variable we need to write the audio to the file or to its own file and we're going to do that by first creating a path for our audio file so we'll do temp file path equals temp file dot get tempter and this is going to give us the directory on our OS where the temp files are being stored by this temp file module so we'll take that dur and we'll add it to our desired file name so we're going to name it the video file ID which we'll take from message video file ID .mp3 so what we're doing here is we're first taking the path to our temp directory and we're appending our desired MP3 file name to that path so we're left with the full path to the file and we want to name the MP3 file just the file ID of the video because it's going to be a unique ID so in this case we won't have to worry about collisions with file names because every video file is going to have a unique ID and that's going to be the name of the MP3 file as well so then we want to do audio Dot right audio file and we're going to write the file to the path that we just created so we're taking our audio file that was created using this movie pi and this object is going to have a method called Write audio file and we basically just need to tell this right audiophile method where we want to write the file and what we want to name the file so that's why we're just passing this path that we created here and once the audio file is created the temporary audio file we can save the file to so first we need to open the file so we'll open the file at the path we just created and we just want to read the file and we'll set data equal to that file that we opened dot read and then we'll set file ID equal to fsmp3s dot put and then we're just going to put that data that we extracted from the file so we're storing our MP3 file in our grid fs and at this point we don't need that temp file anymore so when we do F Dot close we also need to go ahead and do OS dot remove TF path because remember in this case this write audio file method created the temporary file and not this temp file module so we have to actually delete this temp file manually ourselves and lastly we need to update our message and remember we have this MP3 FID in our message and we want to set that equal to string version of the FID object that we got from uploading the MP3 to mongodb and lastly we need to put this message on a new cue or a different queue that we need to create called the MP3 queue so we'll do try Channel Dot basic publish and we'll use the default exchange Again by just putting an empty string and our routing key it's going to be mp3 for the MP3 queue but remember we're going to get those from the environment the names of our cues from the environment so we'll just call this environment variable MP3 Q and our body of course is going to be Json dot dumps because we need to convert the python object into Json and our message will be the input and we need to make sure the message is persisted until it's processed so we'll do pica.basic properties and we're going to once again like before set the delivery mode equal to pica.spec dot persistent delivery mode and if we can't put the message on our queue and let's just catch the exception as an error just in case we need that variable and then we'll do FS MP3s dot delete FID so basically if we can't successfully put the message on the Queue saying that there's an MP3 available for that message then we want to delete the actual MP3 from mongodb as well because if we don't put the message on the Queue then the file in mongodb the MP3 will never get processed anyway so we need to make sure we remove the MP3 from mongodb if we can't add the message to the queue and in that case we're just going to return failed to publish message and the reason this will work is because let's go ahead and save this if we go back into our consumer.pi file if you remember if this start function here fails then we're going to return an error and that error is going to be stored in this error variable and if there is an error then we're going to send a negative acknowledgment for the actual message that's on our video queue so that means that that message will not be removed from the queue and it can be processed again later so that's why we need the actual start method to fail completely because we're going to basically attempt this whole function again if something goes wrong so that's why we need to delete the file from mongodb if we can't put the message onto the queue and let's just go ahead and format and we can save that and that's going to be it for our consumer so what we need to do now is we need to create our Docker file and our kubernetes configuration to create the service within our cluster so let's pip freeze our requirements into a requirements.txt file as usual and then we'll create a Docker file and this Docker file is once again going to be the same as the previous Docker file so we'll just copy this and paste it here and let's just save that and we're going to need to add something additional here called ffmpeg which is just a dependency for the movie Pi module and we need to change this to consumer.pi and we need to change our exposed to I think Port 3000 and let's go ahead and save that actually I made a mistake since this is a consumer we're not going to expose any port because it's not going to be a service that we're making requests to this service is going to consume messages from a queue so it's going to act on its own so we'll save that and let's do Docker build and once that's done we can do Docker tag and we'll just take a piece of this and we're going to use your username for your Docker Hub account and this time we'll call it converter and latest and then we can do Docker push converter latest and now if you go to your Docker Hub account foreign you should see that you now have a converter repository as well and you should have the tag latest here as well so now we can make our manifest directory and for this one we just need to create a yaml file for the deployment and the secret and the config map so we're not going to need to create a service configuration for this one so we'll do API version apps V1 kind deployment metadata name is converter labels app converter spec and we'll do four replicas for this one and selector match labels app converter and strategy is going to be type rolling update and rolling update Max surge we'll just double the number of replicas and now for the template labels app converter and we'll do spec actually spec should go back here and containers name equals converter and image is going to be your username converter and environment from will be config map reference and name is going to be converter config map which we'll create and we'll also do Secret ref which will have the name Secret or converter Secret and that's going to be it for that so let's save and we'll create our config map and we'll set API version to V1 kind config map metadata name converter config map and data so we need our mp3q name because remember we're using the environment variable to select the queue in our code and we need our video queue name and actually while we're doing this we need to go create our mp3q so back in our Management console we have our video Cube but we need to create another queue so add new q and we're just going to name it MP3 and it's going to be a durable queue as well and we'll just add Q so now we have both the MP3 queue and the video queue and we can save this now we need to just create our secret.yaml and we don't have any secrets for this service so this is just going to be a template file for now foreign let's save that and let's make some space so now let's go into canines and check to see what we have running so far so we have our auth service our Gateway and our queue so now we're trying to deploy our consumer which is the converter so we'll just apply all the files in the current directory and it seems everything was created so let's see if we run into any issues let's see if we can get a better view of the logs so we'll do Cube CTL logs follow and just select one of these converters and we're getting no module named binary Json dot object ID so I must be pretty sleepy because I don't know why I didn't see this before but object ID here is clearly missing a b so let's go into our convert to MP3 file and go over here and change this to object ID and we'll save that change directory and just to double check let's check to see if there's a linting error and go to definition so we're good to go so we need to rebuild the docker file so we'll do Docker build and we need to update the repository so we need to tag it again first of all foreign Docker push converter latest and now that that's pushed let's once again try to apply our configuration that's in our manifest directory and let's check canines hmm actually let's try to let's first um delete and let's see if we can clear this out really quick and now let's let's check to see if they're closing or shutting down okay we're good so now let's try Cube CTL apply and let's check canines and let's see well we got one two and three running and four running so that's good news all right so at this point we have our auth service deployed we have uh multiple instances of the converter service deployed to pull from our rabbitmq which we have deployed as well and we have our Gateway deployed so at this point we can see if uploading files results in messages getting put on the Queue and we can also see if those messages are being pulled off of the queue by our converter service and that'll probably be the most difficult part to get configured because once we have our uploads resulting in the proper messages being put on the Queue and the converter service consuming those messages and converting the videos and storing them in mongodb that's pretty much the entire functionality of this microservice application so at that point we'll just be creating a service to send notifications when in p3s are finished being converted or when videos are finished being converted to MP3 so let's test the endtoend functionality of uploading and having those uploads be converted so we can quit this and let's clear and we still want our tunnel to be running so make sure that your mini Cube tunnel command is still running and you have another tab opened to work with whatever it is that we're working with at the moment and we want to test by uploading a video file and when we upload that video file we want to see that the message gets added to our video queue and then gets removed from our video queue and another message gets added to our MP3 queue and we don't have a consuming service to consume the mp3q messages so all the messages should just be piling up at the mp3q if our endtoend functionality is working as expected and also if it's working as expected we should be able to download a converted video file which would just be an MP3 file from our mongodb so let's go ahead and try to test that and I actually don't have Postman or anything like that installed on this laptop at the moment so I'm just going to use Curl to test this but if you're familiar with Postman you can use that to test as well if not just follow along with the commands that I use and make sure you have curl installed of course and at this point we can just go on YouTube and download a video and we don't want the video to take too long to download so we'll just do something short so this one's 37 seconds Mark Zuckerberg says he's not a lizard so we'll just go ahead and take this and copy the link to the video and I'm going to use this YouTube download tool to actually download the video from YouTube so if you want you can just do Brew install YouTube DL and of course it's already installed for me but that's just in case you want to do the same thing to get a video or you can use whatever video you have on your machine already so I'll just do a YouTube download and then paste in the URL for the video and for some reason that video doesn't work so let's just try a different one so yeah sorry Mark and this video is relatively short so let's go ahead and do this one we'll just take the URL and once that's finished downloading you should have the file in your current directory and just really quick we're going to go into our mySQL database and we're going to show tables actually we need to use database auth and I spelled database wrong and actually it should just pu's auth so now we can just show tables and let's just select all from user select all from user now when you do this you should have credentials here from when we created the database and we used our SQL script to create the user in the beginning of this video so you should have an email and a password and these are the credentials that we're going to use in our basic auth when we send a request to our login endpoint to get a Json web token to upload the video so I'm just going to exit this and I'm going to send a curl request and it's going to be a post request and I'm going to send it to http MP3 converter.com because remember our Gateway Ingress resolves this host name and we configured this hostname to resolve to our local host or our loopback address so we're just going to use this when we send requests to our Gateway and it's going to be the login endpoint and in curl you can just do this uflag to do basic authentication credentials and I'll just do Giorgio email.com and the password is admin123 for me and we're actually getting an internal server error so let's go into canines and check our Gateway logs and that's too small so I'll just do Cube CTL logs f Gateway and it's saying in alt service access dot pylon 16 object has no attribute txt so we can just go into that file so we need to change actually we can just go into the file from here so it's line 16. yeah so I'm just going to change directory and I'll activate this virtual environment foreign and it's because the response is response.txt and not response.txt and let's just see if there's any more and let's check the directory for txt and we should exclude our virtual environment so invalidate.pi so we're going to need to change all of these so I hope there's not too many sorry about that but of course this requirements.txt is supposed to be txt and maybe we should check all of the service directories because I don't remember it should be ignoring the virtual environment there so yeah let's just see how it goes so we need to go back into the gateway and we need to do Docker build again and then we need to do cker tag and this is for the Gateway and then Docker push and we'll just delete all of our Gateway resources and just recreate them and it looks like those are up and running so let's try to get our token again and now we were able to successfully get our JWT so we can just copy this JWT and let's go back to the directory where our video file is so that's in converter and here we need to do curl and this is going to be a post request as well but this time we need to add our file and actually to make this easier let's change the name of this file so we'll move this to test.nkv now we can do curl x post file equals at test.nkv and we can't forget the header which needs to be authorization and remember it has to be a bear token and then we'll paste in the token and that's going to go to our MP3 converter.com upload and let's see what happens and we are getting 403 Forbidden so let's go ahead and go into canines and maybe that's coming from our auth service let's just check the Gateway nothing hmm so actually it looks like we're using the wrong uh host name here we have MP3 convert but it should be mp3converter.com let's go ahead and sudo Vim our hosts yeah so it should be mp3converter.com so let's go ahead and try it with that and let's make sure our tunnel is still running and let's see okay so now we're getting an actual error from the actual Services I believe so let's go into canines and we can assume that that error came from the Gateway server all right so we're hitting the server now with our upload and maybe our Gateway server is getting an error from auth but it seems auth is returning a 200 for the validate so if we're getting the 200 from the validate endpoint on the auth service then that means that our Gateway is returning a an internal server error when we get a 200 from validate so let's change directory into our Gateway and go into server.pi and let's go to validate so for the upload endpoint we validate and at some point I guess we're returning a 500 but hmm that's a bit strange try and let's get another token really quick and let's try with the new token uh and that's because we aren't in the directory of our file so let's change directory back to converter where our test file is and let's try this again and still an internal server error so yeah it's a debug this so I'm just going to scale all of our services down to just one replica so that we don't have to check multiple replicas for the logs so we can just do Cube CTL scale and we're going to scale the deployment Gateway down to one and we also need to scale our converter converter down to one and we also need to scale our auth service down to one so when we go into canines now you can see that we're terminating all of these extra replicas and we're going to have one auth service running and one converter running in one Gateway running so once that's done terminating we can go ahead and I'm just going to open some extra tabs here and run the logs for our Gateway and same for our auth service and the same for our converter that's strange there's nothing printing for the converter so let me see something foreign and check the logs so the Gateway is what's returning the 500 validates returning a 200 so the validation is going through and we're not getting anything on the converter so if the validation is completing but the Gateway is still returning to 500 then that means that something's happening between validation and actually adding the message and uploading the file so if we go into our Gateway server.pi file we can go to validate and our validation happens here and it's successful and we can let's just assume that we're making it to the upload and here's where we're returning a 500 so in this case we're catching the error and we're returning a 500 and we're not doing anything with the error so we can't really see what's happening so let's go ahead and print the error here and here as well we'll print the error so then we can see if we're either getting an error when we're trying to publish the message or if we're getting an error when we try to upload the file so hopefully we can get some more information by printing these errors so we can go ahead and save this and we're going to need to change directory to Gateway and we need to do Docker build and tag and push again so we'll do Docker build because we need to add the code changes with the print statements and then we'll do Docker tag and it's going to be the Gateway repository that we're pushing to so we'll do username Gateway latest and then Docker push Gateway and once we've pushed we can just delete all of our resources for the Gateway using our manifest files and then we can just recreate them on actually we don't want to use we don't want it to scale up more than one right now because we're debugging so let's do Cube CTL scale deployment to one and for the Gateway and let's change directory back to where our file is and our logs since we shut down that container that we were following the logs for we need to do Cube CTL logs again for the new container and let's check our tunnel it's asking for the password again so we need to do that yeah and let's try this again so it was a success that time so I think what was happening was the old Gateway deployment like the old replicas weren't connected to the host because the host variable wasn't resolving to the actual rabbit and Q host in the container and that's something that happens sometimes so basically like huh let's see if we can let's see if I can prove what I'm what I suspect is happening so for example if we have our rapidm QQ here our Gateway is connecting to the rabbitmq using the name rabbitmq which is the name of the stateful set so let me try and clarify so let's change directory to the Gateway and if we go into storage and util.pi actually not storage util it's just in server.pi our connection to rabbitmq we're using this name rabbitmq which is the name of the service because if we check our rapidmq manifestservice.yaml you see the name is rabbitmq so in kubernetes the service name will resolve to that Services host so in server.pi we're depending on this name resolving to the host for abadin Q but it seems that in kubernetes if a container is connecting to a host via this name if that host changes or restarts it seems that the containers that reference that service name still reference in older host I believe so for example if I were to go in here so first let's send that again uh let's change directory to converter and let's send that curl request again hmm and we're getting an internal server error again but this time it's not our internal server error so let's see what's happening so it says we're referencing this error before the assignment hmm so let's go into Gateway where we did that storage util.pi print error print ah here we're not catching the error for example up here we're doing accept exception as error but here we're just doing accept so we need to do exception as error foreign Docker build and Docker tag and Docker push oh actually that's pretty bad so actually I just did that in the converter directory so I basically built the converter image and pushed it to our Gateway image so I pretty much just overwrote the Gateway image with the converter image uh let's see if we can just cancel that and let's change directory to Gateway and now here we can do Docker build but let me just double check to see what I did so yeah I did Docker build in the converter directory so it built the docker file within this directory which is our converter Docker file and then I pushed it to the actual Gateway repository and what did I change yeah I made changes to the Gateway though so Docker build in the Gateway directory to build our Docker file for Gateway Docker tag Docker push and now the latest image for our Gateway repository is going to be this most recent one so that should resolve the issue or accidentally pushed the converter image and we'll do Cube CTL delete again for our Gateway resources and apply and actually once again I forgot to scale so scale it to one foreign so it looks like that time we got to success it worked so so we're still terminating one of the replicas mini cubes asking for a password again because we had to redeploy the Ingress as well so all right so let's go ahead and set up the logs again for our Gateway and let's send the request again for the upload and again we need to go to where the file is and we get a success auth is successful and the converter is writing the audio so that's successful as well so back to my explanation of what I think the issue was before so our Gateway is using the service name for this rabbitmq service to connect to that host but I think that if for example we restart this rabbitmq the Gateway will still be resolving to the old host using that rapidmq service name so the Gateway won't actually be able to connect if we were to restart this unless we restart the Gateway which would refresh its reference to the host I believe so let's just try and test that theory because I don't really like not knowing what happened so if we send again we should get a success and then if we go into canines and we just delete this pod so we're going to delete our rabbitmq pod and it's going to make it restart and the converter breaks because it's it needs to connect to the rapidmq host as well foreign so once the rabbit mq pod restarts let's just restart this one refresh the converter now at this point if my theory is correct the Gateway is still referencing the older rabbitmq host using the rabbitmq service name so it's it should fail if we try to upload right now so if we do this again we get an internal server error so if we reset the Gateway it will reset its variable for rabbitmq or it will reset what it resolves rabbit in Q2 which would be the new host so resetting this should actually make it work so now if we do curl upload we get success so yeah my theory is pretty much correct and it's kind of annoying but yeah just keep that in mind so for example let me just recap what I said in server.pi we're connecting to our rabbitmq using the service name so the service name in kubernetes resolves to the host so yeah the theory is that the cluster IP address that this service name resolves to which is the cluster IP address of the service the rabbitmq service the theory is that IP address changes when we restart the rabbitm coupon but the Gateway pod still references the old IP address using this service name which is why when we restart the Gateway pod this rabbitmq its reference for this rabbitmq service name gets updated which is why it works after doing that but anyways the good news is it seems that everything is working so when we upload a file we're successfully if we were to clear this and do logs for our converter it seems that we're successfully writing the audio so in order to confirm that the endtoend functionality is working correctly we can check for the existence of a audio file from our video conversion in the DB and that's how we'll check to see if everything is working from end to end and also if we go to our queue let's refresh we can see that our mp3q has four messages ready and remember we don't have a consumer service pulling from the MP3 queue so all of the messages are just going to pile up here so at this point we should have four MP3s in our database and we uploaded the same video every time so all four MP3s should be the same audio file but at this point we should be able to download one of these four MP3s from our mongodb and test it to see if it's working correctly and as you see our video queue is empty because any message that we put on the video queue was processed by our consumer service which converts them to MP3 and then that service then puts them onto the MP3 queue so to confirm that everything is working correctly let's just check our mongodb to see if we have four MP3 files which should all be the audio file for the same video that we uploaded four times and we can also go ahead and do it one more time and we're getting an internal server error so let's go ahead and actually don't remember if we restarted the Gateway so let's just restart that just wait till this terminates and we'll do logs again for Gateway and we'll upload that again and we get a success so now there should be five messages in our queue the MP3 queue so as you can see we now have five messages in the queue so now we should have five MP3s in our mongodb database and to test that if you installed mongodb earlier in this course you should have shell which should put you directly into the mongodb that's running on our local host or that's running on our local machine and we should be able to do show databases and it shows that we have this MP3s database and this videos database so this database should have five audio files stored so let's see if we can just use MP3s and now let's try and show collections and as explained before with grid FS the actual file data is stored in these chunks and the files will have a reference or the file is essentially like the metadata for a collection of chunks so if we do DB dot FS dot not chunks fs.files.find this will show all of the objects that we have stored and actually I forgot that when I was testing this prior to making this tutorial I uploaded a bunch of videos that were converted to mp3s so my database is going to have more than five yours should only have the five or it should only have as many MP3s as you sent upload requests that were successful so let's actually do it this way so we can go to the queue the MP3 queue and we can actually let's see get a message from the queue so let's just do get message and we see that there's an MP3 file ID that has this ID so let's go ahead and copy this and here we can do DB Dot actually let's uh show collections again and we can do db.fs.files.find and remember we want to use the actual object version of the ID to find it and to do that we'll do underscore ID for the key and then we'll do object ID and inside of there we can put our actual ID and as you can see that actual object ID is stored successfully inside of our MP3s database so now what we want to do is we want to download this and see if it's an actual audio file and to do that we can go ahead and exit this and let's clear and again if you installed mongodb using the instructions earlier in this video you should have this files and you can put for the DB mp3s and we want to get by ID and we want the local file to be called we'll just do test dot MP3 and we need to use this object to get the ID same way that we have to do that within our database so we'll put that in which is the string version of the ID within this object syntax and we should be able to download the file that way so it was able to connect to our mongodb on the local host and it was able to finish writing to test MP3 so now this test MP3 file should contain the sound for our video file so it should contain the sound for this file so let me just go into this directory on the user interface okay so now in this directory which is the directory of our converter service we have this video file which is the test MKV file so here's a burger again the double double and I'm just going to start eating away with a bag here and uh and yeah it's cut that a bit short don't know about copyright or whatever so this audio file should be the sound for that video file so let's go ahead and just as you can see it's an audio file then we can just go ahead and play this so here's the burger again the double double and I'm just going to start eating away so yeah it looks like our endtoend functionality is working up to the point where we put the message on the MP3 queue so at this point we are uploading our video and adding the message to a queue so we're essentially when we upload a video it gets put onto mongodb then we create a message and add it to this video queue and then our consumer converter service is going to pull off of this video queue convert the video into an MP3 and then put a new message on this MP3 queue saying that in mp3 for a specific file ID exists within mongodb so the last thing that we need to create is a service that's going going to consume This MP3 cue and that service is just going to be a notification service that's going to tell our user that a video is done or a video conversion to MP3 process is done so the service is essentially going to pull the message off the queue and it's going to have the ID and the email of the user and it's going to send an email to the user saying hey this ID is available for download as an MP3 and then from there there's going to be a download endpoint that we create on our Gateway where the user can use his or her token to basically request to download a specific MP3 using the file ID that's sent in the notification service email so essentially if you've gotten to this point where you're actually having the messages put on the MP3 queue and the mp3s stored in mongodb then you've essentially completed the most difficult part of this entire tutorial because from there we're just going to pull messages off this queue and send an email and that's pretty much it so if you've gotten this far this is like a major checkpoint so from here onward we're just going to create that additional service and add the code for the download endpoint on our Gateway okay so at this point in the tutorial what we have left is our notification service and we need to update our gateway to have a download endpoint so we can just start by updating our Gateway and then we'll move on to creating our notification service after that so we're just going to change directory into our Gateway directory and let's go ahead and clear this and we can actually close these other tabs that had our logs foreign we can just leave that running for now and we're going to need to update our server.pi file so let's go ahead and open that file and from here we're going to need to import a couple of additions so we need to import send file which is going to be the method that we use to send files back to the user that downloads them and we're going to be pulling that file from mongodb so just like before we're going to need to use the binary Json object ID so we'll import object ID and make sure I spelled it right this time and we're also going to need to change our configuration for mongodb so right now we're setting the configuration for our URI to include the videos database but that kind of limits us to just using the videos database but we need to use both databases the MP3 and the videos one so instead of using this configuration this way we're just going to create two separate instances of Pi for each database and if we have a look at the flask Pi documentation here essentially what we need to do is this here and it says you can create multiple Pi instances to connect to multiple databases or database servers so we're going to do two of these and one's going to be for our videos database and one's going to be for our MP3s database so we're just going to change this to video and we can go ahead and include the URI when creating the instance and can't forget the comma here and we're also going to remove this config here so we're not going to use this anymore because we're going to configure the URI for each instance within the instantiation of the pi class so we'll remove that and for MP3 it's going to be similar we'll change this to MP3 and we'll change this to mp3s and now for grid FS we need to create two separate FS instances so this one can be videos and this will need to be set to video because we're using the video instance for the videos fs and this one can be MP3s and this will be changed to MP3 and since the old grid FS instance was referenced by the variable FS we need to update that so here we're uploading a video and we were uploading it to the videos mongodb instance but now we need to change this to FS videos specifically and that is all that we need to do for that and now we can go to our download template endpoint that we created and we can actually write the code for the download function so we can remove this pass and we're going to do the same validation that we do in the upload endpoint so we can just copy this so we're going to copy the validate token request and we're going to copy loading the access from that variable that validate token resolves to so we can just go ahead and copy that part and go back to our download and paste that in and we need to add the else here and actually I just realized we're not checking the error here in our upload so let's go back to upload and we need to do if error so if there's an error when we try to validate we need to return error and let me just check something really quick actually no we should be good foreign we don't need to have an else so basically if there is no access then we're just going to return not authorized anyway so the download function is going to be pretty straightforward we first need to make sure the file ID exists in the request so basically the notification service is going to send an email to the user informing them that the conversion job is done and in that email it's going to give them a file ID which is the file they should download when they send a request to the download endpoint and that file ID is required so we're going to do FID string equals request Dot args.get and it's going to be fid and basically if this parameter doesn't exist in the request then it's going to return none so if we go here we see that it Returns the default value if the request data doesn't exist and the defaults for the default value is none so we can set this if we want but the none is the behavior that we want so if FID doesn't exist in the request then this FID string will equal none so that means that we can just do if not FID string then we can just return FID is required oh and I updated the upload endpoint to handle the error but I didn't update this one so we'll do if error return error and anyways if the FID string does exist then we're going to use it to get our file from mongodb so we'll do try and we'll set out equal to FS MP3s so this is the mongodb instance for our MP3 database dot get and remember we need to use the object ID which is here and it's a mongodb object ID and we're importing that here and to that we're going to pass the FID string which is essentially going to convert our FID string to a object ID which is what's needed to get the object from mongodb so the data for our MP3 will be referenced by this out variable so then we can just return send file and we'll return out and we'll do the download name which is going to be the name of the file and we're just going to set it to FID string plus MP3 and this is also going to be able to determine the mime type for the file so this is all we need to do to return the file to the client except exception as error we'll print the air and we'll return internal server error and that's pretty much going to be it for our download endpoint so we're just going to get the access via the validate endpoint from our alt service and then we're just going to check to see if admin is true in that user's access and if so we're just going to return the file for the past file ID and we're not going to check the user's email or anything in this case we're just going to assume if they have the file ID then they have access to the file but of course feel free to expand upon this however you like so let's go ahead and save that and since we changed the code we need to build this Docker image again and push it to our Repository so we'll do Docker build and then we'll do Docker tag and Docker push and now we can redeploy this so I'm just going to delete all of the resources for Gateway just in case so I'll just delete everything in the Manifest directory and then I'll just apply them again and we'll check that later for now we can just go into creating our notification service so let's change directory to our source directory and we need to make their notification and we'll CD into notification so this notification service similar to our converter service is going to be a consumer service so we're going to have some similar code to the converter service so we're going to copy some things from our converter consumer.pi file into our notification consumer.pi file so let's go ahead and create a file called consumer.pi and we're just going to go to our converter consumer.pi file and we'll just copy everything over but we're actually not going to need any of that and we're also now going to need any of this but we're going to create a package called send and a module called email that we're going to need to import and this is where we're going to write the code to send the email and our callback functions basically going to stay the same but we are going to change this to email dot notification we're going to create a notification function and it's only going to take in the body of the message but other than that the Callback function is going to be the same and of course we need to change this to mp3q because this consumer service is listening or consuming from the mp3q and everything else is going to stay the same so now we need to go and create this function here this email.notification function so let's save this file and we'll make a directory called send and change directory into send and we'll touch init.pi and we're going to create a file called email.pi so in this file we're going to write all of the code to send in email notification to the client and we're going to just use this python documentation here which gives an example of how to send simple email messages which is all we really need to send so our code is going to be similar to this here but instead of sending the message via our own SMTP server we're going to use Google's SMTP server so we're just going to send it using the same SMTP server that Gmail uses and I will show you how I'm going to do that now so first we want to import SMTP lib and Os and for this tutorial you don't really need to know what an SMTP server is in simple terms it's just a server to send receive and relay outgoing mail between email senders and receivers so we're essentially just going to be using the same SMTP server that Gmail uses within our application to send emails so we need to import the SMTP lib and Os and we also need to import from email.message email message and this is just going to allow us to create an instance of an email message and you'll see what I mean by that in a second so we'll do Define notification and notification will take in message from our queue and then from there we will try and we'll set message equal to json.loads the message we're going to turn our message from our queue into a python object and then we'll do MP3 FID equals the FID in our message and we need a sender address which is going to be the address that we're using to send the email and I'm going to recommend that you create a dummy Gmail account to send the email because in order for this to work within that Gmail account you're going to need to enable or authorize nongoogle applications to log into your Gmail account so like basically the default setting when you create a Gmail is like only Google applications can log into that account so for example like the Gmail application on your phone can log into that Gmail account but in order for this application to log into that account you're going to have to authorize nongmail or nongoogle applications and that means any nongoogle application can log into your Gmail account if they have your credentials so it's not recommended that you use your actual Gmail account for this because I wouldn't recommend allowing nongoogle applications to log into your primary Gmail account if you want to be safe you should create a dummy account so the sender address is going to be the email address of the Gmail account that you want to use to send the email so I'm going to store it in an environment variable and the environment variable is going to be called Gmail address and then we're going to also do sender password and it's going to be in an environment variable called Gmail password and I too am going to create a dummy account for these credentials and I'll get into that in a little bit and then the receiver address which is who we're sending the email to is going to be the user that's using is going to be the user who's associated with the JWT because that's the user who uploaded the original file and then we're going to create the message that's going to be in the email it's going to be an instance of email message and we'll do message dot set content and it's just going to be a simple message it's just going to say MP3 file ID and then MP3 fid which is coming from here and then is now ready so then the receiver of this email can just take the file ID that we give them and they can send a request to our download endpoint using this file ID to download that file and we can just say the message subject which is just the same subject that you would put when you're writing an email in Gmail and that's just going to be MP3 download and message from is going to be sender address and message two is going to be receiver address and lastly we need to create an SMTP session for sending the mail so essentially we need to connect to Google's SMTP server and then log into our Gmail account and then send the email so we'll set session equal to SMTP lib dot SMTP and we'll put in the Gmail SMTP server which is smtp.gmail.com and then we'll do session dot start TLS and what this does is it puts the connection to the SMTP server into TLS mode and TLS is transport layer security so it's essentially going to make sure our communication between the SMTP server is encrypted and this is essentially to secure our packets in transit so that they can't be intercepted and the data within them read so in simple terms it's just to secure our communication between our application and the service you don't really need to know details about how this is working either just know that it's necessary and then after that we're going to do session.login and we need to log in using our Cinder address and sender password and once we've done that we can do session dot send message and in there we're going to put the message which is the message that we're instantiating here and we're customizing that object here and then we're sending it here and we're going to put the sender address and the receiver address and then once we've sent the message we want to close the session so we can do session dot quit and once that's sent we can just print male scent just so that we can see that it's done and if any of this fails we want to just print the error so we're in this try block here so we'll just do accept exception as error and then we'll print the error and then we'll return the error and the reason we're catching the error here and returning it is because the call to this function is expecting an error so the error should either be none or it should contain an error for example so if we go back up One Directory into our consumer file we see that notification here let me close that notification here is expecting an error and if there is an error then we're going to send a negative acknowledgment so the message can stay on the Queue and be processed by another process but if there's no error we're going to send a basic acknowledgment which means that the message can be removed from the queue so that's going to be it for that and there's a typo here this should just be session and there's also no import for Json so let's do that and we haven't installed these dependencies yet either so that's going to be a problem so we'll just go ahead and save this and we can just cat the head of our file and this file is called email dot by and we'll do actually we didn't start a virtual environment yet so let's change directory and do Python 3 Dash MV and V and start our virtual environment and now we can do fip3 install actually one second I don't think we need to install that I think that's already part of yeah we don't need to install that uh let me install this for my Vim so yeah this is part of Python's standard Library so we don't need to install that and let's see if we need anything installed from consumer.pi we need to install Pica and that's it so we'll do pip3 install Pico and that's going to be it for that so now we can go ahead and we can actually just copy the docker file from our Gateway into this directory and then we have Docker file and we don't need to expose anything and this needs to be changed to consumer.pi and everything else is going to be exactly the same so we can just save that and let's make some space here and we can also copy the Manifest directory from our converter.pi so now we have the Manifest directory from our converter dot pi and we'll change the name of converter deploy to notification deploy.yaml and we can go into that notificationdeploy.yaml file and we'll change all occurrences of converter to notification so I just did it using Vim said command but you can do it however you want just make sure you change all occurrences of converter to notification so we want our deployment name to be notification and then we want the app label to be notification and we'll leave replicas the same and we need to match label notification as well and Max surge is going to be the same and the templates app label is also going to be notification and container name also notification and we're also going to create a Docker repository called notification as well and the same for the config map and the secret so we can just save that and then we need to go into our config map and we need to change this to notification as well and the mp3q is MP3 which is correct and then we need to go into our secret and this changes to notification and we don't have a secret here so let's save that and let's clear change directory back to our root now let's go ahead and dock our build oh and we forgot to do pip freeze now Docker build and Docker tag foreign and then notification latest and now Docker push and notification latest and this is going to create the notification repository in your Docker Hub account and now let's check the docker Hub account and you should have a repository that was last pushed a few seconds ago and if you go to it you should have a tag latest so the next thing that we need to do is we need to configure our Gmail account to allow nongoogle accounts to log in okay so I've already created a dummy Google account or a Gmail account here so once you've done the same you want to click on this and go to manage your Google account and from here you want to go to security and you can just go down to the bottom here and there's this box here that says less secure app access and I've already turned this on but you're going to want to click this and then change it from off to on foreign I'm allowing apps and devices that use less secure signin technology to access my account so make sure that this is set to on before continuing with the tutorial and another thing that we need to do is well me specifically if I go to mySQL U root and I use auth and show tables select all from user you can see that I'm actually using a fake email here so the notification service will try to send an email to this fake email so if you used a fake email for your credentials here you should update it to an actual existing email I'm just going to send it to my dummy email so I'll essentially be sending the notification to myself but you can send the email to whatever email you want to use because at that point that email is just receiving a message so it's not insecure to use an actual legitimate email for the actual sent message so I'm just going to update mine so if I go ahead and select all from user again you can see that my email has now changed and that's actually the email that I'm using here so I can just exit this and that means that I'm going to have to get another token when we test this later on and you will have to do the same if you've changed the email and let's just go ahead and clear so at this point we can go ahead and deploy the notification service so we can just do Cube CTL apply everything in the Manifest directory and we can check this and it looks like it's working as expected and we could just change directory to our source directory and actually let's scale up our Gateway and our auth service as well so I'll just apply all manifests and that's going to scale it up to the two replicas that we have configured in our manifest directory for the all service and we can do the same thing for Gateway and Gateway has two instances deployed as well and the same thing for converter so that's scaling our converter up to four instances so at this point we have everything we have two instances of our auth service four instances of our converter two instances of our Gateway four instances of our notification service and one instance of our rapid mq service so now let's just check to see if our tunnel is running it's asking for the password again because I redeployed the Gateway Ingress so I'll put the password in and our tunnel is running and now I'm just going to go ahead and log in again but this time I'm going to change the email so gmail.com 63 and let's see if we can get the token okay so now we have a token which I will just copy and now I'm going to send and upload request so let me see if I have I need to go to converter and I'll delete the test MP3 that I had from last time and I'll upload the test MKV file but I'm going to change this token so this is going to be the curl request to upload the video file so what we're doing right now is we're testing the endtoend functionality including the notification service so we're first uploading a video so this is the curl request to upload this video and the header authorization bear is going to be the token that I just got from the login into point and then our upload URL and I'm assuming this is because we need to restart our Gateway service so I'll just delete both of these so they can restart and let's try again and we have a success so if everything's working as expected the message should be being consumed from our MP3 queue as well so let's just restart this and it says we have six unacknowledged now let me check this email Ah that's because I forgot a very important part so let me change directory to the notification service actually I didn't forget so what is the issue okay so for some reason we're experiencing an issue so let's just check the logs for our notification service just send it again okay I'm going to scale everything down Gateway converter all notification actually I'm just going to scale everything down to zero and restart it so the only thing I have up right now is the rabbit in QQ and now I'll scale everything up to one okay so let's try and send the request and now so it seems that somehow the messages aren't being acknowledged actually there's one more thing that I forgot so I need to go into our notifications manifest secret file and we need to set up our Gmail address environment variable and Gmail password so we can just use this placeholder and change it to Gmail address and Gmail password and that's because if you remember in our notifications send email dot Pi file here we set our sender address to an environment variable and we also set the sender password here to an environment variable so we need to provide those credentials here so I'm going to do my credentials and you need to do the ones for the account that you're using foreign and make space and just to be safe I'm just going to scale everything down so I'm scaling everything aside from the queue down to zero and then I will scale it back up actually for the notification service I need to reapply so I'll just remove the resources first and this is for notification and then I will reapply those and let me check to see if those variables are working as expected and they are working as expected so let's see if that resolved the issue so let's just go ahead and send again but it's in our converter directory foreign so it seems that it's still not getting acknowledged so it seems that the messages are being consumed from the queue so the issue has to be in our notification service within the consumer.pi code there's probably an issue so we're consuming the message but we're never acknowledging the message but we're never acknowledging that we've processed the message so our email that notification we basically have everything in this try block and if there's an issue we're printing the error and returning the error okay so we need to make a small change to our notification send email dot Pi file so here we need to add in the port 587 and this is the port for TLS and start TLS so I'm thinking that this should resolve the issue that we're having so I'll go ahead and save this and then we could just do Docker build and actually we're in the wrong directory so let's do Docker build and Docker tag and Docker push and while that's pushing let's check the queue So currently since we had those unacknowledged messages in our queue and our notification consumer service isn't currently consuming because we scaled it down we had messages that were ready but they just got swallowed up by the service and it seems that they're getting acknowledged at this point so this should go down to zero all right so they were all processed so that means that we should have emails for all of those messages so if I go to the inbox here you can see that we have the emails for the messages and let me check spam and I think some of those messages had the old email the one that wasn't a real email so that's why I only we're not getting 11 messages we're only getting these ones but anyways the email contains the MP3 file ID string here so we should be able to take this and use it to download the actual file so we can clear this and to download the file we're going to do curl and we want the output of the file to go to a file so we'll just name that file download MP3 download.mp3 and it's going to be a get request and the header is going to contain our authorization and it's going to be the authorization token and the URL should be mp3converter.com and don't need the port and it should be the download endpoint and you should put a URL query parameter here that's called FID and set it equal to the ID from the email so my email ID is this ID here which is the same ID that I'm using here and then if you hit enter on that it should download the file and now you should have a file in the directory called MP3 download.mp3 and we should be able to open this file up or listen to this file and it should be the video that we uploaded to audio so let me just go to that in the user interface so as you can see I have this file here called MP3 download.mp3 and if I play it so here's a burger again the double double it will play the sound from the video that we uploaded so that means that our endtoend application is working and let's just clear here and we don't need this so let's quickly go into canines and we have everything scaled down so let's just reapply the initial configuration so I'm just going to delete the configuration for all of our services except for the rapidmq of course because no need to scale that up or down there's only one pod for that and then now we can just apply auth manifests converter manifests Gateway and notification and now if we go into here we have all of the instances that we have configured for our services so let's just move that video file from converter let's just move that here and let's check our cue really quick so currently our queue is empty and let's go ahead and upload this file actually let me make sure I have a valid token so I'll just do a request to the login endpoint to get the token and there's the new token so I'll just try to upload and let's just Spam it a couple of times so it seems that our cues are processing both the videos and the MP3 messages and at this point it seems that they're all done and if we go into our email we have a few more downloads here so we can just go ahead and copy the file ID for one of those and now we can just go ahead and attempt to download that we'll do a different name for the file so I'll just put something random just something dot MP3 and let's change this to the file ID that we just copied from the email whoops that's the JWT let me go copy that again and let's download and we have this file something.pi so let me go to the user interface and play that so in our source folder we have something.pi so here's a burger and as you can see it is working as expected and that is going to be it for this tutorial I think that if you were able to make it to the end of this tutorial and you were able to get everything working you should definitely be proud of yourself because this one was a difficult one and yeah if you have any questions feel free to post them in the comment section I'll try to help you as best I can if you're having troubles getting everything working and I hope that this has been helpful to you and if you've made it this far in the video and if you haven't already please don't forget to like the video and subscribe to the channel for more content like this and yeah I'll see you in the next one
