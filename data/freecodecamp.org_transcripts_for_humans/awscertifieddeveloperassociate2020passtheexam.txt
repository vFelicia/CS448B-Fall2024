With timestamps:

00:00 - Hey, this is Andrew Brown from exam Pro, and
I'm bringing you another animal certification
00:04 - course. And this one happens to be the hardest
at the associate track, which is the developer
00:08 - associate. So if you're looking to gain hands
on knowledge, building, securing, and deploying
00:15 - web applications to AWS, this is the course
for you. And remember, if you really want
00:20 - to pass, make sure you do all the follow ons
that we provide to you here in your own database
00:25 - account. And we definitely welcome any type
of feedback you have. So go on any social
00:29 - media, and we will find you. And if you do
pass, definitely tell me on Twitter or LinkedIn,
00:34 - I love to hear that people are passing, and
good luck on your exam. So I'm sure you have
00:44 - a lot of questions about the developer associate.
I'm hoping that I can answer some of these
00:48 - before you get started on your journey to
make sure that you're making the right choice
00:51 - about gaining developer associate. And the
first question we're going to ask is, who
00:55 - is this certification for. And I would say
that if you're a web developer, and you're
01:00 - looking to add cloud computing skills, to
your developer toolkit, this is the certification
01:05 - for you. If you if you want to know how to
build web applications, while thinking about
01:11 - how to build them cloud first, this allows
you to push a lot of your web application
01:15 - complexity into managed services. And then
you have easier, more modular web applications.
01:23 - That is another goal or sorry, another thing
you'll achieve getting the Dell developer
01:27 - associate, you're going to learn how to deploy
web applications for a variety of different
01:31 - cloud architectures. So serverless, microservices,
traditional applications, there's a variety
01:38 - of different ways to deploy to the cloud.
And the last thing is that if you are a web
01:42 - developer, and you're looking to transition
into a cloud engineering role, this is a certification
01:48 - for you. Now, what value does the developer
associate hold? What's it going to do for
01:53 - you? And I would say first, that the this
certification is the hardest eight associate
02:00 - certification. So you have the solutions architect
and the sysops. This one is absolutely, brutally
02:06 - hard. And the reason why is that you have
to have practical knowledge of AWS to pass
02:12 - this exam, it's very hands on. But that the
great advantage of that is that it's going
02:17 - to directly help you get a job as a cloud
engineer, because it's, it really is about
02:22 - doing the actual work. Okay. And the last
thing I want to point out is that it will
02:28 - help you stand out on resumes. It's not likely
to increase your salary unless your company
02:33 - really values cloud engineers. But you're
definitely get more job opportunities. And
02:39 - eventually, this is going to be the de facto
of a web developer having cloud skills. So
02:44 - it's important you get this now, because you're
going to get ahead of everybody else. So another
02:49 - question people ask me is how long to study
to pass as developer associate, and you just
02:54 - heard me say that it is the hardest associate
certification. So this is a little bit longer
03:01 - than standard. So if you're a developer, I'm
going to say it's going to take 1.5 months
03:05 - of study, I generally say for the sysops,
or the solution architect associate one to
03:10 - two months, but this one is definitely going
to be 1.5 months, it going definitely into
03:16 - two months, if you are a developer, if you
are a bootcamp grad, you don't know anything
03:21 - about AWS, and you're doing this from scratch,
you're looking at at least two months study
03:26 - pushing on to three months. If you are already
a cloud engineer, and you're just trying to
03:31 - add the certification to around out your resume,
you're looking at 30 hours of study. So if
03:38 - you really sat down and took the time, you
could get this out in a week. So you know,
03:45 - that is the scope that I would say there.
And the last thing here is, is just figure
03:49 - out how much how long and how many questions.
So the cost of this certification, just like
03:54 - all the other associates is $150 USD, it's
130 minutes. So it's a you get a lot of Apple
04:01 - time there, there's 65 questions in this exam.
And to pass, you have to have a 70, around
04:09 - 72%. It's not an exact number, that number
can float around there. I think one more point
04:15 - is that this certification will be valid for
three years. So it's definitely you know,
04:20 - $150 seems like a lot but it's gonna last
you for quite a while. So, you know, hopefully
04:24 - that answers your, your questions, and we'll
move on to the exam guide. Alright, so we're
04:33 - taking a look here at the exam guide breakdown.
The first thing I want you to know, is the
04:38 - course code, which is dva cs 01. This might
be the future and you're booking your exam
04:43 - and you might be presented with two versions
of the exam. This happens when a new version
04:47 - of the exam is out. And there's some overlap
between the old one and the new one. So the
04:52 - solution architect associate is sa c 02. That's
the new one and the old one is C 01. There
05:00 - has there, they have yet to release a co2
version of the developer, I can tell you right
05:05 - now that there isn't a huge change between
CL one and co2, at least at this, Susan artech
05:11 - associate, all they've really done is rebalanced,
the domains percentages, and revise the questions.
05:18 - But the bulk of the content for course, material
is the same. So if this is the future, and
05:23 - you're looking for co2, or you're going to
be totally fine using this course content
05:27 - here, we'll move on the next plot part, which
is the passing grade is 720 points out of
05:32 - 1000. I don't know how those points are distributed,
it's not that important to know, all you really
05:37 - need to know is that you need to get 72% to
pass. And that's not necessarily exactly 72%.
05:45 - AWS can adjust that value based on how many
people are passing and failing. So you can
05:50 - go on an exam get 73% and still fail. So you
do have to consider that there are 65 questions
05:56 - in this exam, that means you can afford to
get 18 questions wrong. So you have a great
06:00 - margin of error here for this particular exam.
And the duration is 130 minutes. So that means
06:06 - you get two minutes per question. For this
exam, for the developer associate, it's very
06:12 - unlikely that you'll run out of time. Well,
you'll end up with surplus time at the pro
06:18 - levels, you always run out like you're always
running against the clock. So you have to
06:22 - know your your pacing per question for developer
associate, that's not the case. And if you
06:28 - have surplus time, you're definitely going
to want to go back and review all your questions
06:32 - and utilize 100% of that time. Now what we'll
talk about is just the type of questions you'll
06:39 - encounter. So you'll see multiple choice,
this is where you choose one out of four.
06:43 - And then you have multiple responses. And
this is where you choose two or more out of
06:48 - five. So those are the two typical formats
you're going to see on the exam here. And
06:54 - now just getting to the actual breakdown of
the exam, it's broken down into domains. And
06:59 - it also has sub domains. And we'll look at
that in greater detail when we actually pull
07:03 - up the exam guide. But the first domain is
deployment worth 22% deployment is extremely
07:09 - important to know for the developer associate,
and you're going to end up with around 14
07:14 - to 15 questions that you'll be presented with.
The next domain is security. And so security
07:20 - is becoming super important in all the certifications.
So we're seeing that really across the board
07:27 - everywhere. And here. It's super important
with 26%. And you're gonna see between 16
07:34 - to 17 questions, then we have development
with AWS services. So this is at 30%. So you're
07:41 - gonna see between 19 to 20 questions, then
you have refactoring, which is worth 10%.
07:46 - So this is between six to seven questions.
And the last domain is monitoring and troubleshooting
07:51 - at 12%. So we have questions between seven
to eight that you'll see on the exam. So you
07:58 - can see that development with Ada services
is the highest percentage. And that makes
08:02 - sense, because it's the developer associate,
you should learn how to develop with a diverse
08:07 - services. So in the exam guide recommends
white papers that you should read. And so
08:14 - if you're not familiar with white papers,
what these are, are PDFs that are published
08:18 - by AWS, which are 100%, free for you to download.
And they're kind of like Ava's documentation,
08:25 - but they have a sales perspective to help
you adopt AWS. And when they create the exams,
08:32 - they actually base a lot of information from
the contents of these white papers. So it's
08:35 - important for you to read some of these white
papers, when you're studying for professionals,
08:40 - it becomes absolutely essential to know these
white papers inside and out at the associate
08:44 - level, not so much. But if there are white
papers that you should absolutely read, it's
08:50 - these ones in red, you absolutely absolutely
need to read those white papers, the ones
08:55 - in black are things I suggest for you to read.
And the the ones in gray are the ones that
09:01 - I would say, it would not matter whatsoever
if you read them. So that is my recommendation
09:06 - for white papers. And if you're looking for
them, they're free to download, you just got
09:08 - to go to aws.amazon.com forward slash white
papers. And I believe that they also have
09:13 - the hyperlinks in the actual exam Guide, which
you can download from AWS as well. But you
09:19 - know, now that we've gone through that, let's
actually open up the the white paper. Alright,
09:23 - so what I've done here is I've pulled up a
dress.amazon.com, certification certified
09:28 - developer associate. So we can look at the
exam guide, maybe the sample questions and
09:32 - a little bit more about this portal here.
Just because there is really great information
09:36 - here for you. Before you go and take an exam,
or even study, just make sure if any changes
09:44 - are happening. You go to here, a recertification
go coming soon. I check this all the time,
09:48 - I have to watch it like a hawk. And it tells
you when things are changing. So here you
09:52 - can see that the solution architect associate
exam has changed. They have a bunch of information
09:58 - here. You can also take Beta exams, I think
at half price off, which is here beta exams
10:03 - are offered 50% discount for standard exam
pricing. I don't ever bother to sit beta exams.
10:11 - I don't know, I guess you get certified if
your beta. I'm not really sure about that.
10:15 - But it's definitely not something I actually
ever do. I always just wait for the official
10:19 - exams. But the reason I want you to check
this is just to see, you know, are there any
10:23 - changes coming to the exam that you are interested
in? And that might affect? You know, when
10:29 - you want to take that exam? Would you rather
want to wait for the new one, or take the
10:32 - old one, or even in the case here, the big
data one has been split into two certifications.
10:37 - So is it valuable to still get the big data
one if you if there's a ellickson database?
10:42 - Will this big data make you look dated in
terms of your certification, so just peek
10:48 - around there and consider that. But coming
back to this page, you can see that we have
10:53 - recommended knowledge and experience. And
then on the right hand side here, we can download
10:58 - the exam guide and download the sample questions.
So here I have the exam guide open, I'm just
11:05 - going to zoom in a little bit. And the first
thing I want to check is the actual course
11:10 - code, make sure that's the one that you want
to study. It's interesting here because it
11:14 - says CEO one and then over here, it says dva
001. I don't know if that's a mistake or not.
11:20 - But whatever. Then down below, we have the
recommended Eva's knowledge. And you'll notice
11:26 - this is exactly the same thing from the list
here. So they're just copied and pasted it
11:30 - there. And then for exam preparation, they
recommend a was training. So we have developing
11:34 - on AWS, an instructor led live or virtual
three day course, I believe that's like 1000
11:39 - $2,000. for registration. I don't know anybody
that uses this. And really, it's for enterprise
11:47 - companies that have a lot of money. So there's
a lot of like government programs where if
11:52 - you send someone for training, and the training
is at $2,000, the government will reimburse
11:56 - like 75% of it. And that's why you have these
really expensive training packages, which
12:03 - make no sense, would you think they'd be charged
$100. It's just kind of that scheme there.
12:07 - I haven't heard good things about these whatsoever.
But if you work for a very large company,
12:12 - and they're willing to pay for it, maybe you
want to take it, then you have the ADA, a
12:17 - digital training on the training platform,
these are actually Okay, so I would definitely
12:22 - consider checking them out as supplemental
content to this course here. Then you have
12:27 - the white paper recommendations. And as you
can see, they are all hyperlinked, I could
12:31 - open up this exam guide month over month,
and this list will slightly change because
12:36 - they're always revising or updating these
white papers. It'll be the same white paper,
12:41 - but the date will change here, maybe the title
will change. And some some of the content
12:46 - will be revised. But again, for the developer
associate, reading white papers is not a big
12:53 - deal. And you saw the recommendations I made
in my list. And I like how they've actually
12:58 - now listed as documentation here, they never
did before. But when you're going through
13:04 - my exam, all I'm doing is I mean, I've taken
the exam, and I have a lot of practical experience.
13:09 - But I'm going through the documentation and
highly, highly condensing it. If you were
13:13 - to study, just read it, it was documentation
alone, you can definitely do it. But you know,
13:19 - you're looking at three, four times longer
study, and you might be over studying on stuff
13:24 - that you should not be doing. But you definitely
want to check out the database documentation
13:29 - could become extremely important at the professional
level at the associate level, it can be a
13:34 - bit of a time sink. Then down to the actual
exam content here you can see they're talking
13:39 - about those multiple choice multiple responses
which we talked about there. Then there's
13:46 - unscored content, this happens sometimes,
so sometimes you'll end up having additional
13:51 - questions. So like that will never ever be
scored. And the reason why is that a device
13:57 - is always testing out new questions. So if
you take an exam, and you get a question that
14:02 - is like so far out there, and you're really
stressed out about you feel like you know,
14:06 - you didn't study well enough, just consider
maybe it's just a test question. And you're
14:09 - not supposed to know the answer. There's always
like two or three in there. So you know, don't
14:14 - get stressed out about that. Then we have
the exam results, they talk about this point
14:19 - system, and there it is, it says 700 720.
And then we'll go down below and we'll actually
14:26 - look at the domains and sub domain. So we
see them again, deployment, security development,
14:31 - database services, refactoring, monitoring,
and troubleshooting. And then here are our
14:35 - sub domains. So under deployment, we have
deploy written code and eight of us using
14:41 - existing ci CD pipelines, processes and patterns.
So that's why in my course, we're going to
14:46 - be talking about code pipeline code build,
code deploy, and code commit, then deploying
14:52 - applications using Elastic Beanstalk. This
is something we heavily heavily heavily covered
14:57 - in this in this course here and again. Have
you really good follow along the 100% need
15:03 - to do is really good. Prepare the application
deployment package to be deployed to AWS.
15:09 - This is kind of talking about Elastic Beanstalk
in terms of preparing them. This could be
15:12 - preparing containers for deployment. or this
could be just preparing artifacts that need
15:19 - to be deployed via code deploy. Then you have
deploy serverless applications. Here, they're
15:23 - talking about Sam the serverless application
model, or using cloud formation templates.
15:28 - Moving on to security. So make authenticated
calls to AWS services. I guess that's just
15:33 - using the COI or API or SDK. I guess it also
could be Eva's cognito. That could be sts
15:41 - tokens, implementing encryption using Ada
services, that is absolutely 100% going to
15:46 - be kms. And we're probably looking at both
in transit and encryption at rest. So also
15:54 - ACM Amazon certification manager, implement
application authentication, authorization,
16:00 - authorization, so that is specifically going
to be cognito. I set it up here, but this
16:04 - is definitely cognito. Knowing about web identity
Federation, maybe I am accounts that we have
16:14 - development with Ada services, write code
for service applications, so step functions
16:17 - lambdas, X ray API gateway, translate functional
requirements into application design. I guess
16:28 - that's giving you like, functional, it's giving
you scenarios and then you kind of like pick
16:32 - out what technologies you should use. That's
not too complicated. Implement application
16:37 - design into application code. So they give
you a design and you have to translate that
16:43 - into code. I don't know what kind of questions
that would be on the exam. And I've written
16:46 - a lot of exam questions. So that one's a little
bit vague for me write code that interacts
16:50 - with the AWS services, so API SDK COI, you
definitely will see on the exam, like they'll
16:56 - show you COI commands. So in this course,
here, I try to expose you to as many ci commands
17:02 - as possible. And even when we could use the
console, I'm making us use a cry just so you
17:07 - get those COI commands etched in your brain
and also get some experience with SDK, then
17:12 - you have refactoring. So optimize applications
to best use Eva services and features. So
17:17 - that's just making the best choice based on
the scenario, migrate existing application
17:22 - code to run on AWS. I guess it's like just
deploying code. It's weird because it says
17:29 - migration. So I'm not really sure what they
mean by that. And I've written I've written
17:35 - practice exam questions for all this. So I
mean, I definitely know. But I, sometimes
17:39 - I lie every design, and we're talking about
monitoring and troubleshooting write code
17:44 - that can be monitored. So Cloud watch X ray,
there are new features in cloud watch for
17:50 - like cloud watch insights, is a cloud watch
synthetics, there's like a new one for serverless
17:55 - applications, maybe cloud trail, perform root
cause analysis on faults found in testing
18:01 - or production code. So this would be like
knowing when a code deploy fails, and then
18:06 - it shows you errors, like how do you read
it and understand what's going on what happens
18:11 - when cloud formation rolls back, and you have
to investigate and go fix that, you know,
18:17 - logging things out into Cloud watch. So like,
if you're using a lambda and something goes
18:22 - wrong, you know, to open up cloud watch monitoring
route through the logs. So things like that.
18:27 - So that's the general breakdown of this exam
guide. And hopefully, it gives you a bit of
18:30 - perspective of what is in front of us. So
let's get to it. So just before we jump into
18:41 - the certification content, I want to remind
you about the AWS certified challenge, and
18:46 - this was officially started by Free Code Camp.
And the idea here is it allows you to get
18:52 - support from other people that are also on
the same journey as you so you don't have
18:56 - to do it alone. And all you have to do is
if you have a Twitter account, and you could
18:59 - do this on LinkedIn, as well. Tweet a photo
of yourself thumbs up, announce that you begun
19:04 - the ADA certified challenge. Tweet your daily
progress of what you learned, encourage other
19:09 - people that are taking the challenge as well.
And when you earn that certification, print
19:14 - out and pose with it. And as an added bonus
of there's also an official discord group
19:19 - that you can join. I think we're almost 1000
members there. I sit in it all day long. And
19:25 - I'm pretty good about answering questions
in real time. And there's a lot of support
19:29 - there and other resources being shared there.
So definitely if you want to maximize your
19:34 - ability to learn and get that support, join
the discord. And also, you know, join the
19:39 - ido certified challenge on Twitter or LinkedIn.
Hey, this is Angie brown from exam Pro. And
19:48 - we are looking at Elastic Beanstalk, which
you can use to quickly deploy and manage web
19:53 - applications on AWS without worrying about
the underlying infrastructure. And this is
19:58 - a platform as a service which we'll talk About
here shortly. So to understand Elastic Beanstalk,
20:04 - we're going to need to know what a platform
as a services or a pass, and that that is
20:08 - a platform allowing customers to develop,
run and manage applications without the complexity
20:13 - of building and maintaining the infrastructure
typically associated with developing and launching
20:17 - an app. So that's exactly what Elastic Beanstalk
does. You upload your code, and everything
20:23 - just runs. If you want to give a similar service
to Elastic Beanstalk. I always say think of
20:28 - Elastic Beanstalk as the Roku for AWS. And
the way we abbreviate Elastic Beanstalk is
20:34 - with EBS. So yeah, there you go. So the idea
is you choose a platform, you upload your
20:40 - code, and it runs with little knowledge of
the underlying infrastructure on Elastic Beanstalk.
20:45 - If you read the documentations, on AWS, they'll
say it's not recommended for production applications.
20:50 - But what does he mean by that, because I know
a lot of startups and sizable companies that
20:56 - run their production workloads on Elastic
Beanstalk. So they're talking to enterprise
21:01 - and large companies, you have to understand
that eight of us is a very large company.
21:05 - And so their clients can be like mega corporations,
or governments. And they'll think, Okay, I'm
21:11 - gonna run my my infrastructure on beanstalk.
But it's just not the use case. So just take
21:17 - that with a grain of salt that you can run
production applications, but AWS is just warning
21:22 - large companies not to rely on it. So Elastic
Beanstalk is powered by cloudformation template
21:28 - templates. So when you spin up an Elastic
Beanstalk environment, that's what it's doing.
21:33 - It's just a very fancy cloudformation template
with a really fancy UI. So if you go over
21:40 - to the cloudformation console, you can actually
see what it's provisioned. And you can go
21:44 - try to read through that cloudformation template.
It's very complicated. But it's just interesting
21:49 - to see what it's doing there. And so lots
of beanstalk can set up things such as elastic
21:53 - load balancer, auto scaling groups, RDS database,
and it comes with pre configured easy to instances.
21:59 - And this is the big list of it. So you can
see you can do Docker, multi container Docker,
22:04 - you have go Java, Ruby, etc. And generally,
these pre configured platforms come with the
22:12 - common technology, you need to run certain
frameworks. So if you're using Ruby, it's
22:15 - gonna be able to run rails, etc. Then you
have monitoring. So it has cloud watch, and
22:20 - SNS integrated into its dashboard, which is
really nice. It has in place and bluegreen
22:26 - deployment methodology. So you don't have
to go out and build a complex code pipeline,
22:30 - which you could spend weeks doing so it already
has it for you there. It can rotate out your
22:35 - passwords for RDS so keeps things very secure,
and it can run dockerized environments. So
22:40 - if you are, if you are employing microservices,
you can definitely use Elastic Beanstalk as
22:47 - your gateway to elastic container service,
which is what it uses under the hood to run
22:53 - Docker containers. So let's just take a quick
look here at the supported languages for Elastic
23:03 - Beanstalk. So we got go Node JS, Java, Python,
Ruby, PHP, dotnet, and Docker. And I, you
23:10 - know, I said this previously, but I'm gonna
say it again, these pre configured, platforms
23:14 - generally have the tools that you need, or
the lot of the things you need to run the
23:20 - frameworks as well. So if you're going to
be Ruby, you should be comfortable about running
23:24 - rails. If you're running the Python platform,
you can run Django if it's PHP lorelle, Tomcat
23:30 - spring no Jess Express Yes. So just keep that
in mind. So when you first create a Elastic
23:41 - Beanstalk application, you have to choose
an environment and you are choosing between
23:45 - web versus worker. And so if you're you need
to build a web application, it can be choosing
23:51 - a web environment. But if you need to run
background jobs, you can choose a work environment,
23:56 - a lot of the cases when you're building web
apps, you're going to make two environments,
23:59 - you can actually make both of these one web
and one worker, and they're going to be interconnected
24:04 - together. But let's just talk about the components
that are involved here. So we have a bit of
24:09 - an idea of how they're different. So on the
left hand side, we have our web environment.
24:14 - And this comes into variants, which we'll
talk about in another slide here. But the
24:19 - idea is that you have these EC two instances,
maybe it's one, maybe it's multiples, and
24:25 - they're running an auto scaling group. And
it also creates an elastic load balancer for
24:31 - you, which is optional. If you want to save
money, you just don't have one there. And
24:35 - that goes out to the internet. So it's a very
simple setup here on the left hand side. But
24:39 - then on the right hand side, we have our our
worker environment, and this is again for
24:44 - background jobs. So you'd have your EC two
instances, they would be in an auto scaling
24:50 - group. And you'd also create an Sq sq. So
if you didn't have a queue, we create one
24:56 - for you, and it would also install the Sq
s daemon on all Those easy two instances so
25:01 - that it can seamlessly communicate with the
Sq sq. But it also has this other setup here,
25:08 - which cloudwatch, it will watch the amount
of instances you have. So that if you're under
25:14 - capacity, it will spin up more instances and
adjust the auto scaling group there. So that's
25:19 - really nice. So there you go. That's your
web and worker environments. So in the prior
25:29 - slide there, I said there were two types of
web web environments are two variants. So
25:34 - let's look at them. The first one we have
already seen is a load balanced environment.
25:38 - And so the idea with this one is that you
have easy two instances running an auto scaling
25:44 - group. But that auto scaling group is set
to scale. So if you get a lot of traffic coming
25:48 - in, it's going to spin up more instances.
And then when the traffic declines, it's going
25:53 - to remove instances. So that means that there
could be a variable cost based on traffic,
25:59 - you have an elastic load balancer there. And
that's where traffic is coming into the lb.
26:07 - So the other case is you can set up a single
instance environment. And so this one is extremely
26:11 - cost effective, because you're only running
a single server, but you still are using an
26:15 - auto scaling group, because auto scaling groups
are great for keeping, not just for scaling
26:20 - us to add servers, but just to keep you in
a single server running. So the desired capacity
26:24 - is always set to one, there is no elastic
load balancer. And that's just to save on
26:29 - costs. But with no EOB, that means that there's
going to be a public IP address that is used.
26:35 - So if you have roughly two, three, it's going
to point to that IP address where in the load
26:39 - balance environment, it's going to be pointing
to that load balancer. So there you go. So
26:45 - I was saying earlier, that Elastic Beanstalk
comes with deployment options built in. And
26:54 - this is definitely going to save you a lot
of time. So you don't have to set up your
26:56 - own code pipeline. So let's talk about some
of the deployment policies that are available
27:00 - with Elastic Beanstalk. And so what we have
is all at once rolling, rolling with additional
27:05 - batch and immutable. And we're going to be
walking through every single one of these.
27:10 - But based on what deployment policy choose,
they're only available for specific web environments.
27:18 - And so you can see for rolling and rolling
with additional batch, they're not available
27:22 - in single instance environments. And the reason
why is that is that you need a load balancer
27:26 - in order to do that, because the load balancer
is going to attach and detach instances, in
27:31 - batches from EOB, what you're going to notice
is that there is no mention of in place, or
27:38 - blue green on this list. And the reason why
is basically this entire list is in place.
27:44 - But we're gonna explain in place in blue,
green, the context of it. So that makes a
27:49 - lot more sense. Because even myself, when
I first started learning this, I was like,
27:53 - Okay, we have this list of words in place
in blue, green. So hopefully I can clear that
27:59 - up for you. So let's take a look first at
all at once deployment. So the first thing
28:06 - we're going to do is we're going to deploy
the new app version to all the instances at
28:10 - the same time, then we're going to take all
the instances out of service while the deployment
28:15 - is processing, and then the servers are going
to become available again. So this is the
28:20 - fastest, but also the most dangerous deployment
method, it's fast, because it's doing everything
28:24 - at once. It's dangerous, because you're taking
those instances out of service, meaning that
28:29 - your services are going to become an unavailable.
And also you're you're applying updates to
28:33 - all your instances that exact same time. So
if you have a major failure here, do you might
28:40 - have to rollback your changes. But if that
rollback fails, you'll have all these instances
28:46 - that are in a broken state. And you'll have
to deploy the original version again. And
28:49 - it can get kind of messy. So that is all at
once deployment. So now we're going to look
29:01 - at a rolling deploy, it looks really complicated,
but we just need a big graphic in order to
29:06 - explain this. So the first thing is we're
going to deploy the new app version to a batch
29:11 - of instances at a time. So whereas all at
once was all of them, this one is just like
29:16 - we'll do two at a time, right. And so if we
have four servers, we're gonna do two. So
29:20 - we take in batches, those instances out of
service. And then once that, once those are
29:28 - good, we're gonna reattach them with the update
instances, we're going to move on to the next
29:32 - batch, and so forth and so forth until we've
gone through all the servers. So you can see
29:39 - that that mitigates some of the problems with
all at once deploy, it's going to be definitely
29:44 - a bit slower. But you know, if we need, we
might need to perform additional rolling updates
29:52 - in order to rollback the changes. So rollback
could be still pretty darn complicated. There.
30:03 - Let's take a look at rolling with additional
batch. So the idea here is that when you start
30:09 - to deploy, you're going to spin up new servers.
So if you're doing it in batches of two or
30:14 - whatever size, instead of taking a batch out
of service, we're just going to add new servers.
30:21 - And then we're going to apply our app version
there. And once those are good, we're going
30:26 - to then terminate our old instances or, or
another batch. And the idea here is that by
30:34 - doing this, we're never going to reduce our
capacity. And this is important for applications
30:40 - where a reduction in capacity could cause
availability issues for users, because we
30:44 - saw with rolling, we will have reduced capacity
for a short period of time, in this case,
30:48 - we'll never have a reduced capacity. But you
know, we still have the same issues where
30:54 - if you are, if you want to do a rollback,
you're going to have to perform an additional
30:59 - rolling update. So roll backs is still quite
painful in this case and slow. So let's take
31:10 - a look at immutable deploys. And this one,
it really is reliant on the auto scaling group.
31:15 - So over here, what you can see is that we
already have an elastic load balancer that
31:20 - points needs to do instance, that's inside
of an auto scaling group. But what we're going
31:24 - to do is we're going to make a new auto scaling
group with a single EC two instance in it,
31:30 - or whatever, however many servers we need
to replace. And then the next thing we're
31:35 - going to do is we're going to deploy the updated
version of our app on the new AC two instances
31:40 - in that new auto scaling group. And then what
we're going to do is we're going to point
31:44 - that elastic load balancer to the new ASC.
And then we're going to delete the old SGX,
31:49 - which is going to terminate all the old instances.
So the reason why you'd want to do this is
31:55 - that this is the safest way to deploy critical
applications. And when you want to roll back,
32:01 - if we just go over here, the idea is that
you don't have to destroy this auto scaling
32:06 - group immediately. You can wait until this
new production and auto scaling group is running
32:11 - smoothly, you could wait days or weeks however
long you want. And then you could destroy
32:16 - this old auto scaling group. Or if you had
to roll back, you could instantly move back
32:22 - to that auto scaling group because all the
infrastructure exists. So, rollbacks are really
32:26 - easy. It's super safe, you know, but there's
not a lot of downsides to it. I mean, this
32:33 - is the one I would choose to do. So we're
going to take a look at deployment methodologies
32:42 - for Elastic Beanstalk. And you're going to
notice that down below, I have blue green,
32:47 - so we never covered blue, green as of yet.
So these have all been immutable all the way
32:52 - to all at once I've been deployment policies,
these are built in deployment methods into
32:57 - Elastic Beanstalk. But we have blue, green.
And we'll explain the difference between in
33:01 - in place and blue green in the next slide
here. But let's just compare these methodologies
33:07 - and understand what the trade offs are because
this is definitely important on the exam.
33:11 - So the first one is all at once. And it has
the fastest deploy time because it updates
33:15 - all the servers at the exact same time. So
if we have four servers, it takes them out
33:21 - of service applies the updates and puts them
back into service. But when they're out of
33:24 - service, we're going to experience a downtime.
And downtime could be a bad thing. If your
33:31 - users notice. And that could impact their
experience, or if they're doing serious or
33:37 - series are critical transactions, that could
be a problem. So you have to decide whether
33:41 - that is a trade off you want to take but most
people do not want to use all at once. And
33:46 - also if you encounter an error, let's say
all your deploy fails, and you have four servers,
33:53 - now you have to roll them back manually. And
so that's kind of a pain. But also imagine
33:58 - if you encountered an error during rollback,
so the rollback fails, and now you have four
34:02 - servers all stuck in a broken state, that
could be extremely detrimental to your business,
34:07 - losing hours upon hours of time, so you have
to weigh those trade offs. Now, next, we have
34:13 - is rolling. And so rolling, mitigates this
downtime problem where we have instances that
34:19 - are out of service. So what it does is, so
if you have four servers, like the previous
34:24 - case, it's going to update these in batches.
So it's going to take the first two and take
34:29 - them out of service, right, and then put them
back into service. And then it's gonna move
34:33 - on the next one. But the trade off, there
is still a downside, which is we're going
34:37 - to have a reduced capacity. So if you have
to have always four servers to run your critical
34:42 - workload or to just handle the current usage,
this is not going to be ideal for you. So
34:48 - what you're going to want to do is you're
going to want to use rolling with additional
34:51 - batch. So rolling with additional batch is
very similar to rolling. So it's going to
34:55 - work in batches, but instead of taking a batch
out of service, it's going to add a new batch.
35:01 - And once this one is good and running, it
just kills an old one. And so this way, you
35:05 - always have at least the minimum amount of
servers you need running to meet your capacity
35:11 - needs. But you know, rolling back for these
methodologies, so can be difficult, it's still
35:18 - a manual process. And you can just imagine
having to roll each section back can be painful.
35:23 - Imagine that you're rolling this back, and
then this part, this rollback fails. And now
35:28 - this one's stuck into the state. So you have
a weird number of servers messed up here,
35:33 - this can be extremely difficult. So the last
one here is immutable, immutable, the idea
35:38 - is you just replicate your entire environment.
So like all at once, you know, if you had
35:44 - four servers, it would take them out of service
with immutable, it would just create four
35:47 - new servers. And once they're all good, then
you could move over to those four new servers.
35:52 - And if you had a rollback, you just point
back to the old ones, because they still exist,
35:56 - because they haven't yet been deleted until
you decide to do so. So immutable gives you
36:00 - the best flexibility in terms of the rollback
process. It can be more expensive, depending
36:06 - on how long he goes servers around the provisioning
time takes a long time, the actual switch
36:13 - takes very little time, because it's very
fast to rollback or switch to the new version.
36:19 - But to provision those servers take a while
because you're replicating everything at once.
36:26 - And before you can actually start routing
traffic to it, because with rollbacks, you
36:29 - could start with rolling, you can start routing
traffic to new instances, gradually remotely.
36:36 - Whereas mutable, you have to wait till all
the servers are ready. But I you know, I think
36:39 - immutable is extremely, extremely safe, and
a good a good deployment methodology to use.
36:47 - And then last, we have blue, green, and blue
green is very, very similar to immutable,
36:51 - where it will replicate the all the servers.
So if you have four servers, it's going to
36:56 - make four new servers. But also, it could
spin up other infrastructure, like elastic
37:02 - load balancers and stuff like that. But that
these are super similar. And that's why we
37:08 - have another slide to talk about them to understand
when something's in place versus blue, green.
37:13 - But just before we move on here for Elastic
Beanstalk, the bluegreen methodology uses
37:19 - a DNS way of doing bluegreen. So that means
that when you want to move over to these new
37:23 - servers, it refers to three points to a new
load balancer. So there's a new load balancer
37:29 - with new instances on it. And so the change
is happening at the DNS level. And the reason
37:33 - why this can be a negative is that DNS, DNS
changes have to propagate to DNS servers around
37:39 - the world. And the effect that could happen
is that there could you could have this new
37:46 - production server ready, but people are still
being pointed the old one or it's going nowhere.
37:51 - And so even though the server's not down,
there could, some users could experience unavailability,
37:56 - just because you know, they're being pointed
to the wrong thing. So that is a consideration
38:01 - you have to think about, though, for Elastic
Beanstalk. It's not generally that bad, but
38:06 - it does happen. So we have to consider that
as a negative. But now that we know deployment
38:11 - methodologies, let's move on to the comparison
between in place and bluegreen. So let's take
38:23 - a look at inplace versus bluegreen. Deployment
here. And these terms are confusing because
38:28 - they're not definitive in definition. And
the context can change the scope of what they
38:32 - mean. So it's important to learn these not
just for Elastic Beanstalk, but for DevOps
38:36 - in general. So we're gonna spend a little
bit time here making sure you really know
38:40 - this stuff inside and out. So Elastic Beanstalk,
by default, performs in place updates. And
38:46 - that's all the deployment policies we've been
looking at, they've all been considered in
38:50 - place. But let's change the context to see
how that affects the scope, which will change
38:56 - what is considered in place. So the first
we're going to look at, which we're already
39:00 - familiar with is with the Elastic Beanstalk
environment. So when that is our our scope,
39:05 - that means that all the policies we saw prior
all at once rolling rolling with additional
39:09 - batch and immutable are considered in place
deployment methodologies. So let's say we
39:15 - change the scope to outside of Elastic Beanstalk.
And now it's just servers. So we have these
39:22 - servers. But what's really important is that
these servers are never replaced. So they
39:26 - always have to be the same servers, the existing
servers. So that's our scope. And now the
39:31 - only deployment methodologies that are available
to us is all at once are rolling, which are
39:36 - considered in place because they never replaced
a server, they'll take a server out of service
39:41 - and make changes and put it back into service.
Now, that doesn't mean that rolling with additional
39:45 - batch immutable would be considered bluegreen.
They just wouldn't be considered in place
39:50 - for the scope of that scenario. Let's set
up another one where the scope is now we have
39:56 - a server that can never be interrupted. So
Doesn't mean like, that means we can't replace
40:02 - the server with a new server. But that also
means we can't take it out of service. So
40:05 - it should never lose traffic, it should always
be traffic should always be pointed to it.
40:12 - And so to solve this, we use zero downtime
deploys. This is where bluegreen occurs on
40:18 - the actual server itself, like in a virtual
sense where you, you have your code base,
40:23 - and you deploy the second version of the code
base on the server, and you facilitate the
40:27 - change the switch within the server virtually.
And you can't do this on Elastic Beanstalk.
40:32 - I used to do this for years with Capistrano,
Ruby on Rails and unicorn. And this allows
40:38 - for deploys that happened within minutes.
So I like in 30 seconds or one minute, I'd
40:43 - have the latest version updated, it was amazing.
But when we have to consider all these cloud
40:47 - components, you know, this, this kind of agility
is has been lost. But we get other kind of
40:54 - trade off to it. So you know, that's the stuff
that we're looking at. But when we're talking
40:59 - about the exams itself, and they're talking
about in place, they're going to be talking
41:02 - about generally in the context of Elastic
Beanstalk environment. So I just want you
41:06 - to know, the different the differences of
this, but for the exam, this is the one you're
41:11 - going to focus on, for in place, okay. Okay,
so this is the slide where everything is going
41:21 - to fall into place. And you're going to really
understand the difference between in place
41:25 - versus bluegreen deployment. So this is this
is going to be in the context of Elastic Beanstalk.
41:31 - And we're first going to look at an immutable
deployment methodology. So we know the immutable
41:37 - that what happens is, it replicates the auto
scaling group with another easy to instance,
41:44 - and then it facilitates the the transition
to the new production servers by switching
41:50 - over to this auto scaling group and then destroying
the old one. Now, this is funny, because in
41:55 - our last slide, we just saw that there was
a blue green methodology called EOB, blue
42:02 - green. And this is exactly what it did. So
why isn't this considered blue, green, and
42:05 - it's called in place. And that has to do with
the the boundaries of the environment. And
42:11 - so the environment is defined here as being
the Elastic Beanstalk container. And so because
42:18 - the mechanism of our deploys inside the boundaries
of this environment, it's considered in place
42:24 - because it's inside of the environment. So
now looking at a bluegreen deploy, for Elastic
42:31 - Beanstalk. This can only occur at the DNS
at the DNS level. So it's roughly three that's
42:37 - facilitating it. But notice that it's outside
of the environments, it's outside of it, it's
42:43 - going from a blue environment to a green environment.
And so that is why it's called Blue Green
42:49 - deploy. If it's all within the same environment,
it can't be considered blue green. So, you
42:55 - know, talking about doing a DNS, like having
the DNS level facilitate the the switch of
43:02 - servers, or having the load balancer, I was
saying the load balancer was a lot better.
43:07 - Because with DNS, we could have an interruption
in service, not because the servers aren't
43:14 - ready, but like the DNS servers have to propagate
the changes. And so there could be some unavailability
43:19 - for servers. So why would you still use this,
then if this one is better, this has to do
43:24 - with where some of your external resources
lie. So in the context of Elastic Beanstalk,
43:30 - it really has to do with your database. So
if you are using inplace, deployment, a lot
43:35 - of them are very destructive. So your your
database would be inside of the environment,
43:39 - it would be running on an EC two instance,
generally, and, you know, it would get terminated
43:45 - with the environment. And so you'd lose your
data. So that would not be good whatsoever.
43:50 - So your database would have to sit outside
of your environment will like on RDS. And
43:56 - so you know, the better place to do, it would
generally be with a bluegreen deploy with
44:01 - your database outside of it. So that is generally
the reason why we use blue green. That doesn't
44:07 - mean you can't use RDS with in place. But
generally it's more for bluegreen deploy.
44:12 - So hopefully that clears up a lot of stuff.
And you can understand and visualize those
44:16 - boundaries to see when something's in place
and when something is not considered in place.
44:26 - So if we want to change the way our Elastic
Beanstalk environment works programmatically,
44:32 - then the way we're going to do it is through
configuration files. These configuration files
44:36 - sit in a hidden folder called Eb extensions
at the root of your project. And they're going
44:41 - to have an extension that says dot config
on them. So there's going to be a variety
44:46 - of different ones. But that's what Osip Bienstock
expects. And what we can change to these configuration
44:52 - files is the option settings for our initial
environment. We can do things very specific
44:58 - to Linux and Windows. And then we can also
set up custom resources. So if we need other
45:03 - services to integrate, that's what we can
do there. But the motivation of having this
45:07 - file here is that let's say we want to hand
this project to somebody else, they can just
45:12 - provision all the things they need, with the
exact configuration they need. So this is
45:16 - something you'll definitely run into. If you're
working on Elastic Beanstalk, you'll have
45:19 - to do a little bit of configuring. So I said
that, you could configure some options settings
45:28 - for the Elastic Beanstalk environment. And
that's called the environment manifest, which
45:33 - is in a file called the E and V dot YAML.
And you're going to store that at the root
45:37 - of your project. So this file is important
because when you first create your Elastic
45:43 - Beanstalk environment, it's going to look
for the style and set up a bunch of defaults.
45:47 - So this is the way that you're really sharing
that configuration with other people or just,
45:52 - you know, saving that configuration for yourself.
So what I've done here is I pulled out a little
45:58 - example. And we're just going to look at some
of these attributes here. So the first one
46:02 - that we're looking at is the environment name.
So that's whatever you want the name to be.
46:06 - And we'll talk about what that pluses in a
moment on the end there, then there you have
46:10 - your solution stack. So that could be Ruby,
Python, Java, whatever. But that's going to
46:14 - be what ami is chosen. Then you have environments,
links, and this associates to other environments.
46:19 - So we saw that you could have a web environment,
and a worker environment. And so this is a
46:23 - way to connect them together. And then you
have some default configurations for specific
46:28 - AWS services. So here we are setting a load
balancer to be cross zone. And there is a
46:32 - lot more options here. But we don't need to
go through all of them, you just have to generally
46:37 - know that you can do this stuff. And let's
just talk about that plus there on the end.
46:42 - So that plus is used to add the environment
name to the end there. So to give these more
46:48 - unique names. So that's all that thing is
for. So there you go. So now we're going to
47:01 - take a look at Linux server configuration,
there is one for Windows server configuration.
47:06 - But I think we only need to really learn one
here, because you can pretty much apply this
47:11 - to both. And that will be good enough for
our studies. So the first thing you could
47:15 - configure is being able to download packages.
So here I have Ruby, or we have mem cache,
47:23 - I think the package manager will generally
be young, because that's what AWS uses for
47:28 - both Amazon is one Amazon likes to if you
can use some kind of other OSS that might
47:34 - change to something else. Then you can set
Linux Unix groups, something I don't do, very
47:41 - often, but something you can configure, then
you can also configure users and assign them
47:46 - to Linux groups. You can also create files
or download files from the internet using
47:52 - a URL, that URL is for public facing file.
So I don't think there's a way to download
47:57 - private files. But for the content, you just
specify content and provide what you want.
48:02 - There, I'm providing a yamo file, then you
have commands, these are commands that you
48:07 - want to run before your application has been
installed. So Elastic Beanstalk polls, your
48:12 - code base, but this is happens before that
code is actually in the environment, then
48:19 - you have services. So maybe you've installed
nginx, and you want to ensure that it is running,
48:24 - when it starts up it continuously run. So
for whatever reason it shuts down, it will
48:28 - try to make it start up again. And then the
last thing our container commands. And so
48:34 - these are commands that are are specific to
your application. So after your application
48:40 - or your source code has been downloaded to
the environment, these are what you want to
48:43 - run it. It says container commands, which
make you think that these are for Docker containers,
48:48 - that's not the case. So just be aware of that.
But you know, for for Windows, you're going
48:53 - to have similar ones who have container commands,
commands, probably something for packages
48:58 - and files. So it's more or less similar, just
as long as you conceptually understand the
49:03 - things you can set at the server level. So
Elastic Beanstalk has a COI which you can
49:13 - install. And that's going to give you more
of a Heroku like experience and going to know
49:18 - some of these commands are very similar to
Heroku commands. So to get the COI, you need
49:23 - to go to GitHub and just had an install it.
So it's just as simple as cloning the repo.
49:29 - I believe this is using Python. And so you
just execute that command. If you're on a
49:34 - Mac. This should just work for other systems.
You're gonna have to read a little bit more
49:39 - on the GitHub page itself, which you can see
is that the universe Elastic Beanstalk COI
49:44 - set up there, but let's just run through some
of the commands that are available to us starting
49:49 - with Eb a net so this configures your project
directory in Elastic Beanstalk COI This is
49:54 - the first thing you're going to want to run
because it's going to set up a bunch of defaults
49:58 - on your computer. And if you don't want to
make a project, you just delete that project
50:02 - afterwards. But you still want to run this.
When you want to create an environment you're
50:07 - going to be doing Eb create. When you want
to check the status of the environment, you're
50:11 - going to do Eb status. When you want to check
the health, you can get health on the particular
50:16 - instances and the overall health of the environment.
If you want to get that in real time, or near
50:23 - real time, you can do a refreshable update
every 10 seconds. If you need to see a bunch
50:28 - of events that are being output outputted
by the Elastic Beanstalk environment, you
50:33 - can run that if you want to see a logs from
the actual instances yourself, you can run
50:38 - Eb logs, if you want to open up the the application
in your browser, there's Eb open, but it's
50:46 - really not that hard to do, you could just
go to your browser, when you're ready to deploy
50:51 - your current version of code. You do Eb deploy,
if you want to see what kind of configuration
50:56 - setup you have for that environment, you know,
seeing whether it's running Ruby or other
51:02 - options, you can check that as well. And if
you want to terminate that instance, because
51:05 - you want to save money, you can do Eb terminate.
So, you know, go ahead and download the CLR
51:10 - and give it give it a go. So we're gonna look
at how you can use your own custom image in
51:21 - Elastic Beanstalk. And this is where you can
provide your own ami instead of the standard
51:26 - Elastic Beanstalk ami of your choice. The
reason why you'd want to do this is because
51:33 - it could improve provisioning time. So if
all if you have a lot of software like packages,
51:37 - you need to install to run your software,
it takes a long time to pull all those if
51:42 - you bake it into an ami, it's just gonna speed
things up because it's already there. So let's
51:47 - go through the process of how you'd actually
get a custom image. Because it's not hard,
51:51 - but there's a lot of steps to it. So the first
thing you want to do is go to AWS docs, there's
51:55 - a page called like supported platforms, and
you get a list of all the type of standard
52:00 - Elastic Beanstalk kaomise. And what you're
doing is you're trying to get the platform
52:04 - information there. So that you can then use
the COI to use this command called describe
52:09 - platform version. And so by using describe
platform version, and you can see over there,
52:14 - it has the platform name in it, it's going
to get us back an image ID and that image
52:20 - ID is the the AMI ID so that we can then find
the AMI and then extend that ami to do what
52:26 - we want with it. So using that ami Id go to
the EC two marketplace, in probably into the
52:32 - community section and paste in that ami, so
we would find the AMI we're looking for. And
52:37 - then we would launch a new EC two server.
Once that easy two server is launched, we
52:42 - could then log into it either using SSL or
sessions manager, I would suggest sessions
52:47 - manager to do so you would need the correct
Im role attached to it. So just be aware of
52:55 - that. But once you get into that machine,
you configure it however you want. So you'd
52:58 - go in and you install those packages manually.
And then you would go ahead and bake that
53:03 - ami and so you'd have your new ami. So once
you have the new ami, you could go in your
53:07 - configuration like your files, or you could
even do it through the console and set the
53:13 - new ami ID. And then when you create new environments,
that's what it's going to use. So that is
53:18 - the whole process to setting up a custom image.
So we need to talk about configuring your
53:27 - RDS database with Elastic Beanstalk because
you actually have two options, you can add
53:31 - a database inside or outside your Elastic
Beanstalk environment. And you might not even
53:36 - be aware that you're doing it if you're setting
it up. So it's important to know the difference.
53:42 - So let's talk about inside Elastic Beanstalk
environments. So when you go to create an
53:46 - environment through the console, you'll have
the option to create a RDS database. And if
53:51 - you are doing that, that means it's going
to be within the Elastic Beanstalk environment.
53:56 - Now, the thing is if you do this, that means
whenever this environment is terminated for
54:01 - any reason, it will take out the database
with it. So that means that generally this
54:06 - setup is for development environments. That
doesn't mean you can't use it for production.
54:12 - Because as long as you're using in place deployment
mechanisms, so like let's say use immutable
54:16 - and stuff that's gonna replace the EC two
servers, it's never going to remove the RDS
54:21 - database but if you for whatever reason, delete
that entire environment, your database is
54:25 - gone with it. Then on the other side you have
outside the Elastic Beanstalk environment
54:30 - and the way you know you're doing this as
you're creating your database first and RDS.
54:35 - And then you configure it with your EC two
instances that are in your inside your Elastic
54:39 - Beanstalk environment. Now, when the Elastic
Beanstalk environment is terminated, the database
54:44 - is going to remain because it wasn't created
part of the EB environment. And so these are
54:48 - generally suited for production environments.
And with this setup, you know you generally
54:53 - are using bluegreen deployment. You don't
have to but you totally can. I just want you
54:58 - to know that distinction. Vote Inside and
outside the environment. Hey, this is Angie
55:07 - brown from exam Pro. And we made it to the
end of the Elastic Beanstalk overview and
55:11 - that means it's time for the cheat sheet.
So let's review. Elastic Beanstalk handles
55:15 - the deployment from capacity provisioning,
load balancing, auto scaling to application
55:19 - health monitoring. So it really sets up a
lot of infrastructure for you. It's a good
55:24 - time to use Eb when you want to run a web
app. But you don't want to have to think about
55:29 - the underlying infrastructure. And we just
saw this big list of infrastructure above.
55:33 - It costs nothing to use Eb only the resources
that provision so if it spins up RDS, an E
55:38 - lb CT, you're gonna be paying for those but
EDI itself costs nothing recommended for tests
55:43 - or development apps not recommended for production
use. Remember, when AWS says this, when they
55:48 - say not for production use, they're talking
about super large enterprises who think that
55:52 - they can use Elastic Beanstalk for the production
environments. If your small to medium business
55:56 - Elastic Beanstalk is a okay. You can choose
from the following pre configured platforms
56:01 - we have Java dotnet, PHP, no GS Python, Ruby
go and Docker. You can run containers on Eb
56:07 - either in single container or multi container
mode. These containers are running on ECS.
56:12 - Instead of EC two, you can launch either a
web environment or a worker environment. Web
56:17 - environments come in two types we have single
instance at or load balance for single instance
56:22 - environments, it launches a single EC two
instance and assigns it an elastic IP address
56:28 - to that EC two instance, for a low bounce
environment, it's going to launch easy to
56:33 - instances behind nlb managed by an auto scaling
group I didn't mention in the last one. But
56:37 - for the single instance environment, it isn't
an auto scaling group as well, it's just set
56:40 - to one set to design capacity of one. But
I don't that's important for the exam, it's
56:47 - not a big deal. Then you have your work environments,
this creates an SQL queue installs the SQL
56:52 - daemon on all these instances has an auto
scaling scaling policy, which will add or
56:57 - remove instances based on the queue size.
Then we have Eb has the following deployment
57:02 - policies. So we have all at once. So this
takes all the servers out of service applies
57:06 - changes, put servers back in service, this
is super fast and has has downtime. So that
57:12 - is one condition you have to think about.
Then you have rolling update servers in batches
57:16 - reduce capacity based on batch size, rolling
with additional batch adds new servers and
57:20 - batches to replace the old never reduces capacity,
then you have immutable creates the same amount
57:26 - of servers and switches all at once to the
new servers removing old servers, you really
57:31 - really need to know these deployment policies
inside and out. So make sure you know the
57:34 - difference. And if you don't know, go back
to the lecture content. Look at those diagrams
57:38 - and make sure it clicks. And then we're on
to the last page here. So rollbacks rollback
57:43 - deployment policies require an EOB. So rollback
that it should be called rolling. So I meant
57:51 - to write rolling update this here, but for
the video, it's just gonna say rollback. So
57:55 - rolling deployments rolling or rolling with
additional additional additional batch policies
58:04 - requires an E lb so it cannot be used with
single instance web environments. Just consider
58:08 - that in place deployment is when deployment
occurs within the environment, all deployment
58:13 - policies are in place. Blue Green is when
deployment swaps environments outside an environment.
58:19 - When you have external resources such as RDS
which cannot be stored or destroyed. It's
58:24 - suited for bluegreen deployment. Eb extensions
is a folder which contains all configuration
58:28 - files. With Eb you can provide a custom image
which can improve provisioning times. If you
58:33 - let Elastic Beanstalk create the RDS instance
that means when delete your environment, it
58:37 - will delete the database. The setup is intended
for development and test environments. Really
58:42 - do consider that. And the last thing here
is Docker at most JSON is similar to ECS task
58:48 - definition file and defines multi container
configuration. So yeah, if you looked at a
58:53 - task definition, you'll understand it. We
don't have to go through the the the guts
58:57 - of that here. But this is generally what you
need to know for the exam, but really know
59:01 - those deployment models, okay. Hey, this is
Andrew Brown from exam Pro. And we are going
59:10 - to start with the Elastic Beanstalk follow
along we're going to look at how to deploy
59:14 - Elastic Beanstalk a variety different ways.
So we know it inside and out. I want to point
59:19 - out first, before we get started here, make
sure you are in the correct region. And we
59:23 - always do everything in US East one, because
that's where the most abundance of Ada services
59:29 - are available. And it just makes things a
lot easier. So just go up here and make sure
59:32 - you're in US East one. And be very careful
because 80 of us likes to switch out that
59:37 - region on you sometimes. So if you feel like
things aren't going the way that should be
59:42 - going just double check your region. So now
that we have that out of the way, let's go
59:46 - ahead and make our way over to cloud nine
because we're going to need a developer, a
59:50 - developer environment to run and test our
application and then go ahead and take that
59:55 - over and deploy the Elastic Beanstalk. So
I'm gonna go ahead here to cloud nine. I don't
59:59 - have Any region or environments created here,
so we'll go ahead and create an environment.
60:03 - I'm going to name this Dev, n, e and V was,
which is developer environment here saying,
60:12 - not to use root account, I'm definitely not
logged in as the root account. So I'm not
60:15 - sure why I'm getting that message. But we'll
go ahead here, hit next. And what we're gonna
60:20 - do is we're gonna make sure this is a TT micro,
that's part of the free tier eligibility,
60:24 - we'll scroll down here we have the choice
between Amazon Linux and Ubuntu. Amazon Linux
60:30 - one is supposed to be unsupported at some
point, because they want us Amazon Linux to.
60:36 - So if you're watching this in the future,
maybe Amazon Linux two will be here, you'll
60:39 - have to use a bun two. But if Amazon Lex one
is here, absolutely use it because it is amazing.
60:45 - We're gonna leave the default cost saving
settings here to 30 minutes. So if we're not
60:49 - using if we don't have any activity, or we
don't have the browser open here, it will
60:52 - shut down the server save us money. It looks
like it wants to create an IM role, we'll
60:59 - let it go ahead and do that, we'll go ahead
and hit next. And down below, it has some
61:02 - best practices for us. And just shows us a
confirmation of what we're creating. This
61:07 - is all great. So just hit Create environment.
And we'll just have to wait here a little
61:11 - bit. And I'll see you here in a moment. Alright,
so our cloud nine environment here is ready.
61:17 - And just before I get started, I like to use
the dark theme. So I'm gonna just switch it
61:20 - down here to the classic dark theme. And I
also like to use vim, I would recommend just
61:26 - using the default, but vim is what I use,
that rebounds all the keys for Super efficiency.
61:31 - So you know, it's just because I've been doing
it for years. But anyway, now that we have
61:36 - our cloud nine environment, let's actually
get an application going here. And since this
61:39 - is a very developer focused, I think we should
try to use the terminal as much as we can
61:44 - to get as much experience as possible. The
first thing I want you to do is I want you
61:49 - to type in NPM, i c nine hyphen G, so C nine,
which is short for cloud nine. This is a Node
61:58 - JS utility that makes it easy to open files
directly from the terminal here. So you know,
62:05 - we have this README file. But also just noticed
that C versus environment, this actually maps
62:09 - to this dev at ebn directory, if I hover over
there, you can see it'll autocomplete to that.
62:16 - So I don't know why CLOUD NINE does that.
But that's how they name it. But anyway, I
62:19 - just want to show you how c nine works. So
we have a readme in here. And if I just wanted
62:23 - to open it up, it actually is I think open
right there. But if I just typed in LS, and
62:28 - then I typed in C nine README, then it would
open up that README file. So that is going
62:33 - to give us a little bit of help along the
way. So now that we have seen I installed,
62:39 - let's go set up the actual application itself.
And so the first thing we're going to do is
62:44 - we're going to type in MK dir, which makes
a new directory, and we want to make that
62:49 - in our environment. So I'm gonna use Tilda
to make sure I'm always at home, I'm going
62:52 - to type environment study, sync is the name
of the application we are creating today.
62:57 - And you can see up here that it created a
folder. Okay, I will go ahead and we'll just
63:02 - create some additional files. So I'm just
going to CD into that folder to save myself
63:06 - some trouble. And the first thing we need
to do is initialize an empty our node projects,
63:13 - we'll do NPM and knit hyphen y. Okay, and
what that did is created a package dot JSON
63:20 - for us here, which we will adjust momentarily.
But we want to run a web app. So we're gonna
63:26 - need some kind of web framework. So we're
gonna go ahead and use Express. Okay, so we'll
63:34 - go ahead and type that in. What's, what's
that going to do, it's going to add it as
63:38 - a dependency there. So now we can use Express.
The next thing we want to do is we're going
63:43 - to need some initial files to work with here.
So I'm going to type in touch, we're going
63:47 - to type in main.js, we'll probably need index
dot HTML, actually not instead of main, we'll
63:54 - call it index. I think actually, I normally
call it index. Then we will have index dot
63:59 - HTML app.gs and style dot CSS. And so that
created all the initial files that we need
64:06 - to work with. And so now we just need to populate
those files. So the first thing I want to
64:11 - do is I want to have a way of actually running
our application here node. So we're going
64:15 - to add a new script up here called start.
Okay, and we'll just type in Node main.js
64:21 - actually going to call that index there. And
the next thing we're going to do is we're
64:27 - to start populating this file. So if you make
your way over to the GitHub, and you go to
64:30 - exam Pro, co the free database developer associate,
there's a folder here called study sync 000.
64:37 - And these are the files that we're going to
copy on over. So the first is the index.js.
64:42 - So we'll go there and just hit raw. And we'll
copy the contents here. And we'll double click
64:48 - on that and we will just paste it. Then we
will click back here and we'll go grab the
64:54 - styling. Okay, we'll hit raw It's not that
important for you to know how to program.
65:03 - But I mean, you know, we need to get as comfortable
as we can here. So we're not going to really
65:06 - need to learn all this stuff that we're doing.
Just copy paste it through. And we just need
65:13 - I think, the app GS file, yep, the JavaScript
file here. And we will grab all that data.
65:25 - And so those are the three files we need.
And just give you a very quick tour of what's
65:28 - going on here. We have this index dot HTML
file, oh, I guess we didn't populate that.
65:33 - Okay. Give me two seconds here. Sometimes
you think you do something and you don't.
65:41 - So anyway, the index HTML file loads this
style dot CSS file, which is located there.
65:48 - What we're doing is we're using a CDN to polen
mithral, which is a JavaScript front end framework,
65:54 - we are going to use App JS to load our JavaScript.
Going over to our JavaScript here, we're using
66:01 - the mythical framework. So it's very simple,
we have this app here. And the idea is it's
66:06 - we're going to have a question, and we have
multiple choices, and we can submit the answer
66:11 - somewhere. And then we just have some plain
stallion CSS. So now that we have that all
66:18 - going, the next thing we need to do is actually
preview this application, because before we
66:22 - can deploy it, and package it, we need to
make sure that it is working here. So I'm
66:26 - just going to go ahead here and close these
tabs. And there's just going to be a couple
66:30 - things that we need to do next. Okay, so what
we need to do is we need to get our application
66:42 - running to make sure that it's all in working
order before we go ahead and package it. And
66:47 - so we can preview in cloud nine, but cloud
nine, by default doesn't open up its ports
66:52 - to the internet. So we have to go ahead and
do that. This would be no different than you
66:56 - setting up a web app on an EC two instance,
you'd still have to open up ports. And so
67:01 - generally, the ports that clubnight allows
out to the internet is 80 8080 8081, and 8082.
67:09 - So what we'll do, I just want to show you
how you normally do this. So you go here,
67:14 - and you go to easy two instances, you go to
instances on the left hand side, and we find
67:20 - the one that's running, that's our cloud nine
environment. And over here, we're going to
67:23 - find security groups. And if we expand that,
and we check inbound rules, we go here at
67:29 - it, and we just add 8080, right. And then
we restricted to our IP, this is a development
67:33 - environment, oops, I gotta hit plus there.
And but we're not going to do it this way.
67:39 - Because I want you to get as much programmatic
experience because this is a developer associate.
67:45 - So we're gonna figure out how to do it completely
from the terminal. So we're gonna do the exact
67:50 - same there, I know that this is much faster,
but like, trust me, this is gonna help you
67:54 - in the long run for studying here. So let's
get to it. So what we're going to do here,
68:04 - I'm just as I play here, so my screen is nice
and clean, is we need to figure out what the
68:10 - MAC addresses for the CC two instance. And
then we use that MAC address to get the security
68:14 - group IDs. And then from that we use the COI
to update and create our own inbound rule.
68:21 - So whenever you want to get information about
an easy to instance, that is where the metadata
68:25 - service comes into play. And it's very easy
to access on a server, whether you're here
68:31 - in your SSH into an EC two instance, or you're
here in cloud nine, you just type in curl,
68:35 - hyphen s, HTTP, colon, forward slash forward
slash, and then it's 169 254 169 254. Latest
68:47 - and metadata. So here, it's showing you that
there's a way you can get a lot of data in
68:52 - here, this one is going to be for, you should
know this IP address, it should be etched
68:57 - into your brain, because it's definitely a
standard here when working with EC two. And
69:01 - as a developer, you need to know it. But we
need to find the MAC address. And here we
69:04 - have this and it says Mac. So we'll go ahead
here and type in Mac. And now we have the
69:08 - MAC address. And the next thing is we're gonna
use this MAC MAC address to find out all the
69:13 - security group IDs for the network interfaces
that use that Mac. So what we'll do is we're
69:20 - going to hit up and we're just going to back
out here a bit. And we'll do network. And
69:25 - we'll just piecemeal it because if you make
the whole link, sometimes it's a big pain
69:28 - in the butt. And it's hard to hunt down the
problems. So I'm going to just keep on doing
69:32 - this bit by bit. Oh just shows the MAC address
that's even more convenient. And we'll just
69:38 - hit Enter. And then we want our security group
IDs. And there that's so we only have one
69:48 - security group if there was multiples we like
attaches easy to instance, we probably see
69:52 - more, but we only have this single one here.
So now that we have this security group ID
69:58 - what we'll do is we will use The COI, the
COI in vcli is is already installed on this
70:04 - cloud nine environment because it's Amazon
Lex one already comes pre installed. And CLOUD
70:08 - NINE also loads your credentials from your
user account. So we don't have to play with
70:12 - the credentials file here. If you're doing
this on your local computer, you absolutely
70:15 - would have to set that up, we'll type in AWS
EC two. And we'll type in authorize security
70:23 - groups. ingress and there is a new intimacy
ally and it has autocomplete. So like you
70:30 - could hit tab and it would complete that stuff
there for you. But I don't believe I have
70:34 - the latest one installed here. So I have to
do things manually. And so we'll place in
70:38 - that security group will say what port we
want to open up. So Port 8080, we want we'll
70:44 - have to specify the protocol, it's going to
be a TCP. And then we need to supply the cider.
70:53 - So that's the IP address that we're going
to want to be accessible. So before we hit
70:56 - enter here, we actually need to go get our
IP address, because that's what we're going
71:00 - to put in here. So what we'll do is use one
of AWS services, which is called check IP.
71:08 - So it's a very useful service. Let's go use
that now. So I just open up a new tab here,
71:14 - and I'm just going to type in. I'm going to
type in check IP, Amazon AWS COMM And this
71:27 - will tell me what my IP address for my local
computer is. There's other websites like what's
71:32 - my IP, but let's use the AWS service, because
they took the time to make it for us. And
71:36 - we'll go back over here. And we will hit enter
and we want forward slash 32. The forward
71:41 - slash 32 is very, very important. Because
that says only a single only a single IP address,
71:48 - a cider cider block is a range of IP addresses,
something we definitely cover in this course.
71:54 - And you definitely need to know what networking
like cider blocks are in the associate. But
71:59 - for the time being if you don't know what
it is just understand that you need to put
72:01 - your IP address in there and type in forward
slash. And what we'll do is we'll go hit Enter.
72:08 - And actually, before we do that, no, no, we'll
just hit enter, that's fine. So it didn't
72:13 - show us anything. So I mean, I believe that
successfully created it. So we'll go over
72:18 - here and just take a look here to see if it
actually made it. And there it is. But let's
72:23 - say that we didn't want to make our way over
here. And we want to do this pragmatically,
72:29 - let's go confirm the security group, through
the COI. So what we're going to do is have
72:33 - an a to s EC to describe security groups.
We're going to put in that group ID, it takes
72:43 - a bunch of them so but we only need one here.
And we'll I'll put it this as text allows
72:52 - us to generally default Jason, but that's
just really hard to read in this case. And
72:57 - then we're going to use filters. So we'll
type in filters name equals IP, hyphen, permission.to,
73:05 - hyphen, port, values, equal 8080. So what
I'm saying here is describe all the security
73:15 - groups to me, and filter it out. So we own
or only select this security group, display
73:20 - this text, and then filter it out so that
we only see inbound rules that have Port 8080.
73:26 - We'll hit enter here. And we have a invalid
command there. So I'm just going to double
73:31 - check here. I might have typed something wrong.
permissions does not look spelt correctly
73:36 - to me. Yeah, it's gonna be P er permissions.
And I'm still having a bit of trouble here.
73:46 - IP hyphen permissions to port. Oh, you know
what it's a singular permissions is not with
74:01 - an ass, I think. There we go. So it's a little
bit hard to read. But the idea here is that
74:08 - it's going to say this is our security group
that returned it saying that Port 8080 has
74:12 - been setting that's the IP address. Now, if
it hadn't been set, and we ran this command,
74:16 - it would just show nothing. So the fact that
something shows up here means that that these
74:22 - that that inbound rule was created. But of
course, in practicality, you probably just
74:28 - use the console. So now that we have, we'll
just type clear here to clear this stuff up.
74:36 - So now that we've opened that, that port,
the next thing is actually getting the application
74:41 - running. But before we can even do that, we
need to know what the actual IP address of
74:45 - this cloud nine environment is. And I'm pretty
sure if we use our EC two instance here, we
74:51 - can go here, and we can go and check it and
that is its public IP address. I think that'd
74:57 - be the same thing. But let's again, do it
programmatically. So we'll type in curl, hyphen
75:03 - s HTTP, colon forward slash forward slash
160 9.2 54.1 69.2 54. Latest metadata. I'm
75:14 - gonna hit enter, just so I'm not having too
much trouble here. You can even see right
75:19 - here. It's public ipv4. So we'll type in public,
ipv4 and it says 385 910. I'm gonna go back
75:31 - here. Yep. So it's the same one there. So
that's what we're going to use to access the
75:36 - web application. And let's go ahead and actually
start this application up. So to start it
75:44 - up, we just have to make sure we're in that
study sync directory here. And we want to
75:49 - start it up on port 8080. So if you're wondering,
like, why are we typing Port 8080 there, the
75:56 - way this application works, if you open up
the index dot j, s, uses process AV port,
76:02 - and it's going to pass that port number to
express. So it knows to start up on that port
76:08 - number. And so what we'll do here is we'll
just go and type in NPM start. And we'll see
76:18 - if this starts up. And we have a little error
here. And that's totally okay. It's it's failed
76:23 - to parse package JSON data, you know what
happened, we forgot to comment, something
76:27 - I do all the time. So remember, we wrote this
line in here, we have to just make sure there's
76:31 - a comma on the end of it, or it's not valid
JSON. And I'm just gonna go back down here,
76:36 - hit Ctrl C, which kills that there. And we'll
hit up again, we'll see if we're in better
76:41 - shape. So now it says that it's launched on
port 8080. So what we can do now is get that
76:48 - IP address that we have earlier. So it is
somewhere in here. Um, can't seem to see it.
76:57 - So I'm just going to go here and copy this
here and make a new tab in Terminal and just
77:01 - paste it in. Of course, we could just go up
to instance. But why do that when we can try
77:05 - to do the proper way. And so what we'll need
to do is do this and say, Port 8080, we're
77:11 - not going to open it up here. But I'm just
typing it out so that I'm having less trouble
77:17 - here. I will just copy that. And we will see
if this works. And there's our application.
77:24 - So subrogation doesn't really do much, you
can select something and submit but it doesn't
77:29 - really submit to anywhere, maybe we will start
hooking this up and do more with it. I call
77:33 - this the study sync application supposed to
help you study I guess, um, but you know,
77:37 - it's just a superficial application. So now
that we have our application, running, and
77:43 - we can preview it from Cloud Nine, and we
have some COI experience, the next thing is
77:46 - to get a git repository set up. So that's
what we'll do next. Okay, so let's get going
77:57 - here. And I'm just going to go ahead and close
this bash command will go back here, and we'll
78:01 - just stop the server, I'm doing that by typing
Ctrl C on my keyboard, you can see that it
78:06 - says Ctrl C, this little icon represents control.
And we're going to need a dot Git ignore file
78:13 - because there's files that we just do not
want to include. So up here, we have no modules,
78:16 - this is how node works. They just put all
the libraries in line. And we do not want
78:21 - that in our Git repo. That's just too much
stuff. And so with git, there's a file called
78:25 - dot Git ignore. And what we'll do is we'll
just make sure we create it here in our study
78:31 - stink directory. So just make sure your environment
study sync, if you're not sure where you are
78:35 - just type in environment, studies sync as
such. And then we will just go ahead and touch
78:41 - a dot Git ignore file. And that should now
exist, I just don't see it here on the left
78:51 - hand side, it might be hiding it or we might
have to hit refresh. Well, I definitely know
78:59 - that it's there. So because if we do an ls
l hyphen, LA, there, we can see that hidden
79:04 - file, there's probably a way to turn it on
there. I'm just not sure at the current moment.
79:09 - But that's fine, because this is all about
learning how to use the terminal and see Eliza
79:13 - developers. So if we want to open up that
dot Git ignore, we're going to use our handy
79:17 - dandy c nine command, type, type Git ignore.
And if you don't want to type everything,
79:22 - just hit tab on your keyboard that saves a
lot of time when your autocompleting stuff
79:25 - works for most things we're going to do is
type in node modules. what's what's that going
79:30 - to do, it's going to ignore this file completely
just folder completely because we don't need
79:34 - to include that with Git. Now that we have
that set up, you may want to set up your Git
79:40 - config global you username and email. We're
not gonna worry about it right now, but we'll
79:45 - probably be prompted for it and it'll just
be annoying message every time we see it.
79:49 - So now that we have a git ignore, let's set
up our Git. Let's actually set up this Git
79:53 - repo. So we'll type in get a net and it's
initialized. An empty repo is create a new
79:59 - folder called dot get here. So we just do
an ls, LS hyphen LA, you can see I have a
80:06 - dot Git directory now. And we're not going
to really get into the details about Git here.
80:11 - But you know, just showing you what's going
on. The next thing we want to do is want to
80:15 - add all the stuff that we worked on so far
to our actual repos. So if we type in git
80:20 - status, it'll show that we have these untracked
files, meaning that they, they aren't going
80:24 - to be committed. So let's add them to be committed.
So we have to get add, we'll do git add period,
80:30 - that will just add them all. So we'll hit
enter, we'll type in git status. And so now
80:36 - you see one from untracked two changes to
be committed. And so now we just need to write
80:41 - our git commit message hyphen, M, initial
commit. And they are now committed. And there's
80:48 - that thing I was talking about the git, git
config username and email, generally, you
80:53 - want to set these with your name an email,
since I've just doing this for practice here,
80:57 - I'm not going to do that. And you'll probably
keep on popping up. But we've created a git
81:02 - repository, but it only lives on this cloud
nine environment. And we really want to make
81:07 - sure that this is hosted somewhere in the
cloud, you could use GitHub. But for the this
81:13 - course we are, or this project, we are going
to be using code commit so that we get some
81:17 - hands on experience with that. I think a lot
of developers already have experience with
81:20 - GitHub. But yeah, we'll get to that. Next
here. So I said, the next thing we want to
81:34 - do is get this, this local repo, that's this,
this folder here, well, we can't see it on
81:40 - the left hand side, because it's hidden. We
want to get this all the contents of this
81:44 - entire folder and the stock get file into
a repo and we're going to use code commit.
81:49 - Now when you use the, the Elastic Beanstalk
COI, you're setting up a project for the new
81:56 - time, it'll the first time it will set up
a code commit project for you. And so I figured
82:00 - that's the way we should go ahead and do it.
So we'll just go up here and make a new tab.
82:07 - Because the the CLR is not pre installed.
So the EVA CLR is pre installed on this cloud
82:12 - nine instance, but not the Elastic Beanstalk
one. So what we'll do here is we're just going
82:16 - to go ahead and type in an Elastic Beanstalk
COI GitHub. And because that's going to have
82:25 - the instructions for us to do this install
here, and so I'm just going to scroll down
82:29 - here. And based on your environment, you might
have to install additional things, you can
82:33 - see there's a bunch of things. But since we're
working cloud nine, there's not going to be
82:37 - anything that's too difficult here. And all
we have to do to install this is to run the
82:42 - git clone command. So let's go ahead and give
that a go. So that we make our way back to
82:47 - our cloud environments, I'm just gonna type
in clear so we can see what we're doing. And
82:51 - I'm just going to go back one directory, because
this is going to clone that repo like download
82:55 - this folder, and I just don't want to have
it in my study sync here. So I'm just going
82:58 - to go back and directory, which is a CD dot
dot, and we're gonna type in git clone. And
83:04 - this is already complaining. Too many arguments.
Oh, you know what, because when we copied
83:10 - it, it already typed git clone for us, it
was trying to save me some trouble. So I'm
83:15 - gonna go back in there, copy, I wrote it in
manually, I'm silly. And we'll hit Enter.
83:19 - And that's going to clone it. So that's just
going to download it to our local computer.
83:22 - And now to run it, if we go over here. It
should be probably this command. Yep, that's
83:31 - the command. So we'll go back here, and we'll
just hit Enter. And what it's going to do
83:36 - is it's going to install a bunch of stuff.
This is probably not this cloud nine environments,
83:42 - probably not using this version of Python.
If we just go over here, new tab, we're not
83:45 - gonna we're not going to mess with this one.
But if I just type in, I think it's like,
83:48 - just type in Python hyphen version here. Oops,
oops, oops, I didn't want that. hyphen, hyphen
83:59 - version, maybe. Okay, so it says version 3.6
10. And this one wants version 3.72. So, you
84:07 - know, that's just the state of Amazon Linux
one right now. And we're just going to have
84:10 - to wait, this is going to take a few minutes,
as it says here, I'm just gonna go over here,
84:14 - as I have, every time I stopped the video,
I have to, I have to hit a command key and
84:18 - messes up terminal. But we're just going to
wait for this to install. And this again,
84:22 - takes several minutes. So please be patient.
You know, it might be three, four minutes,
84:26 - and I'll see you back here in a moment. Okay,
so just waiting here a little bit here. Coming
84:30 - back to our first tab, we can see that it
has completed so it just took a little bit
84:35 - time to install Python. And there's one more
thing that we need to do and it's just to
84:40 - add this loss of Bienstock to our path so
if we were to type in Eb, it shouldn't be
84:44 - able to find it because it just doesn't know
where that binary is stored. So we just need
84:50 - to take its recommendation recommendation
Here we are using bash until reason bash says
84:53 - bash up here. We just need to echo that command
here. I'll just hit that. It also seems to
85:03 - suggest this. I don't remember having to do
this last time. And I think we can go ahead
85:08 - and do that. I think it's safe to do. So.
I hope that doesn't mess up this, this follow
85:14 - along here, but I'm pretty sure that won't.
And so now if we type in Eb, we have Elastic
85:21 - Beanstalk pop up. So that's really great.
What do I need to do is just delete this folder,
85:25 - we don't need it anymore. It's just creating
clutter. So just type in LS hyphen, LA, make
85:29 - sure you are at this environment. directory,
if you don't know just type in CD tilde forward
85:36 - slash environment. And we'll type in RM hyphen,
RF, e to us, autocomplete it, hit Enter. And
85:45 - we saw that advantage there. So it's just
a bit of housecleaning, because we don't need
85:48 - that sticking around. So now that we have
the Elastic Beanstalk environment installed,
85:53 - or the COI, let's see what we can do with
it, which will be actually setting up an application,
86:00 - I'm just enclose this other tab here. And
I'll see you here in a moment. So now that
86:09 - we have the CLR installed, we're ready to
initialize a new Elastic Beanstalk project.
86:12 - Now I want to point out that we are currently
in Tilda environment, that's our home directory,
86:19 - it's very important that we run this command
in the study sync directory, because it needs
86:22 - to find this dot Git directory in order for
it to upload our code to code commit. So just
86:28 - type in TT, or CD tilde a Ford slash environment,
study sync, and do LS hyphen LA, make sure
86:36 - you see that docket directory there. Before
we get going here, I'm just gonna open up
86:40 - a couple tabs in AWS. And we're gonna go we're
gonna go to one that's actually at the COI
86:48 - here. So we're going to make her way or sorry,
to Elastic Beanstalk. And then for this one,
86:55 - we're going to make sure that it's on code
commit. Just so we can see what's happening
87:01 - in the background here. So what I want you
to do is I want you to type in Eb will give
87:08 - you a full list of commands. We're not, we'll
probably won't end up using all these commands.
87:12 - But these are the most general ones. And they
tell you to use Eb net Eb create an EB open
87:17 - for Eb open, we don't actually have the ability
to use this command. This makes it so the
87:21 - application opens up in the URL in the browser,
which is very convenient. If you're if you're
87:26 - running Elastic Beanstalk or your local computer,
not in cloud nine, that's a great command
87:30 - to use. But here we won't be able to use it.
Let's go ahead and do Eb Annette. The first
87:35 - thing it's going to ask us is the region we
definitely want to default that to US East
87:38 - one always US East one because it makes things
easier. It's going to ask us to select the
87:43 - application it's going to be study sync and
knows that because it's picking it up from
87:46 - the dot Git folder there. It's gonna ask us
if we're using no Jess, we absolutely are.
87:52 - So yes. Do we want to use code code commit,
we'll say yes. And then enter the repository
87:59 - name, we're going to call this study think.
And then it's going to ask us if or what we
88:07 - want our our default branch to be we want
it to be Master, we'll just type in Master,
88:12 - you just hit enter there either. And what
it's going to do, it's going to go ahead here
88:16 - and in code commit, it will show us that it
created that repo so here it is. So it is
88:24 - now here in code commit Elastic Beanstalk.
We're not going to see anything here yet.
88:28 - This is just an old one here. But if I do
a refresh, I go back here. Well, you probably
88:36 - won't see this. But I had I had a older terminated
instances here. So you might not see anything
88:42 - here as of yet. And then down below, it's
gonna ask us if we want to set up SSH access,
88:47 - I'm gonna say no, we do not need that. And
so now we've initialized our project. Notice
88:54 - that it's created a new folder called Eb Elastic
Beanstalk. The period means that it's a hidden
88:59 - folder, we're going to go and open up this
config dot yamo file. And so these are some
89:04 - of the options we chose when setting up the
EB Annette. So now that we've initialized
89:10 - our project, we have our code on code commit,
we need to configure this application so we'll
89:15 - move on to are the the Elastic Beanstalk environment.
So we'll move on to that. Okay, so the next
89:23 - thing we need to do is configure this environment.
So you generally have to do this for all Elastic
89:31 - Beanstalk projects based on what environment
you're using, and configurations show up in
89:36 - the.eb extensions directory so we'll have
to create one ourselves. I'm just gonna type
89:41 - clear here I'm going to make sure that I'm
in the right place. So study sync. And I'm
89:45 - just going to type in MK dir which is going
to make a new folder.eb extensions. Okay,
89:56 - and so now I have that new folder there. I'm
just gonna double check it Eb extensions.
89:59 - That is totally cool. We're going to need
to create a couple files in here. So I'm going
90:02 - to do touch, or sorry, I'm just going to CD
into this Eb extensions folder, they'll save
90:07 - me some time, I touch a file called 001 n
var dot config. And I'm going to do another
90:15 - one called node command. I'm just going to
go back a directory. So if we open up this
90:25 - folder here, we now have n bar. And this is
where we're going to set some default environment
90:30 - variables. By default, you don't have to specify
an environment here, it would, it would just
90:35 - go to the Elastic Beanstalk environment, but
we are going to be explicit here. And this
90:41 - is what we want to type. I always always type
the word environment wrong, E and VIRO, m
90:48 - e NT, when you call it on the end there. And
so I'm just gonna set Port 8081. And we're
90:57 - gonna say node envy, production. And it's
four spaces in debt, that's totally fine.
91:05 - I'm just gonna leave it alone. And so that
is one file configured. Then we will go over
91:10 - to the next one here. I'm just gonna double
check this one here, option settings. Yep,
91:15 - that's all good. And so the next thing we
need to do is tell Elastic Beanstalk actually
91:20 - how to start up our application because it
has no idea. So we'll do AWS Elastic Beanstalk
91:29 - container. And then no, Jess. So we're going
to give it some no Jess specific configurations
91:40 - here. First, we're going to provide no command
that's going to tell what command it's going
91:45 - to run, when it first starts up, we want it
to NPM start. And then we need to specify
91:50 - the node version, this is going to be 10 point
18.1. I'd actually have to put the parentheses
91:57 - there and use double quotes, I think you use
single quotes, but I'm just gonna stick to
92:01 - what I wrote earlier, because I don't want
anything to go wrong in my follow along here.
92:06 - And if you do node hyphen version, you can
see that choosing 10 point 19.0. And it is
92:11 - better to use the latest version. But that
doesn't necessarily mean that those like Elastic
92:16 - Beanstalk can run that I definitely know that
10 point 19 is not out for Elastic Beanstalk,
92:21 - and 10. Point 18.1 is available. So we'll
stick with this one. But yeah, that's the
92:26 - configuration there. So we'll go ahead and
we'll just commit it. So get add all get commit
92:31 - hyphen m configuration for Elastic Beanstalk.
It's very important that we do commit this
92:38 - and then we push it because of these halls
aren't there and we tried to create an environment,
92:44 - it's just going to air out because it always
looks at what's in the repo here. So if I
92:49 - go into here, code commit Eb extensions, we
can see we have those files there. So that
92:57 - is the configuration phase over here. I'm
going to close a couple things here. Just
93:00 - clean up here a bit. And what we'll do next
is we'll actually create a new Elastic Beanstalk
93:09 - environment. Alright, so we are ready to create
our Elastic Beanstalk environment. And to
93:21 - do that we need to use the EB create command.
But we're not just going to write Eb create,
93:25 - we're going to write Eb create, hyphen, hyphen,
single, this hyphen, hyphen, single This is
93:29 - a command flag. What it's going to do is tell
this to create a environment an EB environment
93:35 - that is it running in single instance mode.
If you don't provide this flag, it's going
93:39 - to spin up an elastic load balancer elastic
load balancers cost money. Technically, if
93:45 - you're using the free tier, if it's if you
made this account, and you're still in your
93:48 - first year, you got one lb free running. But
you know, we just want to avoid these kind
93:52 - of problems. So let's just not use it. And
there's we're not going to really be doing
93:56 - anything with elastic load balancer anyway,
through this walkthrough. So do Eb single,
94:02 - it's going to prompt us with some options.
I'm going to name this study, sync, prod,
94:07 - even though we're in our environment, our
developer environment of my iOS account, if
94:11 - we go back up here, where it says, example,
Dev, I'm going to pretend this is a production
94:16 - application. And I'm just hit enter there,
we just wanted it to be the same. We don't
94:21 - want you to spot instances, that's a great
way to save money. And then it says it's insufficient
94:26 - Iam privileges, unable to determine if this
rule exists. Assuming that exists. It's gonna
94:31 - go ahead and spin that up. I don't know this
is gonna cause us a problem, but we're gonna
94:35 - have to wait here. And this is gonna it's
gonna take a little bit of time to get going
94:40 - here. So I'm just going to wait for this to
get started. I'm gonna open up another terminal
94:46 - here. And I'm just gonna go to study skincare.
And I'm just gonna type in AV status. And
94:54 - this actually shows us the current status
of the application. Right now the health is
94:57 - gray and the status is launching if we go
over here You can see I was trying to launch
95:01 - some stuff earlier here, those are terminated
instances. This is the new one here pending.
95:05 - And you can see it's a nice dark gray. And
this mirrors exactly what we're looking at
95:10 - over here. So this is going to take about
five to 10 minutes to launch. And I will see
95:15 - you back here momentarily. Okay, so after
waiting a little while here, I go back here,
95:21 - and it says that it successfully launched
the EC two instance. So it looks like to me
95:25 - that it's all in good working order. We go
over here and we type in Eb status. You can
95:32 - see that says, ready and Yellow Yellow is
not a great status to have. So if we come
95:38 - back over to the Elastic Beanstalk environment,
I can't tell if it's finished yet, but I'm
95:43 - just gonna go back here to study sync, click
into the yellow here. And it's giving us a
95:47 - warning, it's saying unable to assume the
role. It was asked Bienstock service role,
95:52 - so it's supposed to create that for us. But
for whatever reason, it just did not. When
95:57 - I first wrote this fall long, it definitely
created that for me. So I'm not sure why it's
96:02 - not creating it. But if we go up to this link
here, and open it up here, the application
96:07 - clearly is working. So, um, yeah, the old
commands not that great, but maybe it'll go
96:13 - away on its own, it's gonna hit refresh. So
I'm just gonna wait a little bit here and
96:18 - see if it actually does go away or not. Okay,
so our yellow eventually became a red. And
96:23 - it really has to do with this Avis Elastic
Beanstalk service role. This is confusing,
96:29 - because when you run Eb create, it's supposed
to create this for you, it's supposed to actually
96:32 - create two different Im roles. As I go over
here, I don't see them in here at all. Now,
96:38 - you could go ahead and manually create them.
I've tried to do this, and I haven't had much
96:44 - success as well. But there is another way
for us to create these roles without us having
96:49 - to do a lot of manual labor. And, again, you
might not have to do this, those those roles
96:56 - are Im roles may exist. But in my case, I'm
just having a hard time today with Elastic
97:00 - Beanstalk. So to get them created, what I'm
going to do is I'm just going to start another
97:04 - Elastic Beanstalk project. So I'll go here,
create a new app, I'll just call test, it
97:09 - doesn't matter. Because if we launch one from
here, it will absolutely create us a those
97:17 - roles. So I'm saying here, go to web server
environments. Okay, leave it as test, I'm
97:22 - gonna choose a Ruby, I'm going to go down
here and launch a sample application at launch
97:27 - rails. I'm just gonna write test in here,
check for availability, that's all good. hit
97:33 - Create environment. And so what that should
do is it should trigger to the console. That
97:41 - should go create those Im roles. So if I refresh
here, now you see they exist as Elastic Beanstalk
97:47 - aect role as Elastic Beanstalk service role.
So I have no idea why those aren't appearing,
97:53 - but now they do. But the trick is, I just
need to delete this environment now. So I
98:00 - can't stop this as it's running. So we'll
have to wait till it goes through the motions
98:03 - of it off of this test environment. And then
once it's done here, we'll just go ahead and
98:07 - terminate it. Okay, so the environment spun
up. And now we're just gonna have to go ahead
98:13 - and delete it. I know this is really silly.
But I mean, that's the only way I can get
98:17 - these roles to be created. But, you know,
the definitely, definitely should be able
98:21 - to make a manual in this definitely should
automatically happen. I'm going to go back,
98:25 - I just clicked to all applications here, then
we'll go into here on the right hand side,
98:30 - we'll see if we can go ahead and delete it
here. So I'll just put test in here. And so
98:38 - what that will do is it should automatically
start deleting this environment. So if I go
98:42 - into here, it's terminated yet. So that's
that. But this environment is just no good.
98:49 - So what we'll do is, I mean, we can terminate
it, I suppose. I guess what we'll do is we'll
98:59 - just terminate this environment as well. So
I'm just gonna go here and type this in here.
99:08 - And I'm just gonna wait for this one to delete.
And then we'll try Eb create again, and hopefully,
99:16 - we won't have any issues this time around.
Okay, so I did a refresh there and it's terminated.
99:21 - And what I'm going to do is I'm gonna go back
to cloud nine. And I'm going to try this again.
99:28 - So I'm gonna do I'm gonna go back to our first
tab here. I'm going to do is just hit up.
99:36 - It's gonna ask me for the name is going to
study, sink, fraud. Hit Enter again. We don't
99:44 - want spot here. It's saying that it was possible
inside ECG roll it can't find it. That's okay.
99:53 - When I first did this, I had that error and
it wasn't an issue. But let's just really
99:58 - make sure that it actually is there. As long
as it's there, that's all that matters. Okay,
100:04 - so the roll is there. So we shouldn't have
any problems this time around. But we'll just
100:11 - wait here and see what the result is. I'm
just gonna go over to Elastic Beanstalk. Give
100:15 - it a refresh, click on it here. And we'll
just wait a few minutes and see how it goes.
100:21 - Okay, great. So this Elastic Beanstalk has
gone green. So I was creating that temporary
100:27 - application, even though as silly as it is,
fix the issue here. Hopefully, you don't have
100:32 - to do that. And those roles just exist for
you. So if we go over to cloud nine, here,
100:37 - I'm just going to do a clear here. If you
do Eb status, that's going to show you the
100:44 - status here, so green and ready, which is
the same thing that is over here. If we wanted
100:49 - to go view this web application, it shows
the C name in here. So if I copy this out,
100:55 - we can see we have a link there. And if we
scroll up here, we can also see that it assigned
100:59 - us an elastic IP address. So that's another
way of accessing the web application here
101:03 - is that IP address. If we typed in Eb logs,
this would show us what happened actually,
101:10 - on the EC two instance, if anything was logged
out, I showed you this prior, but I'm going
101:13 - to show you quickly here again. And so you
can see that the application started up here,
101:19 - I'm not sure what's going on down below here.
I don't think that matters. But this is what
101:23 - we really do want to see, we hit q to exit
that out there. If we type in Eb events, that
101:28 - is going to show us the event history that
has happened. So if you go over here and events,
101:33 - that's the same information here. Really great
way to debug stuff. I want to point out that
101:38 - the deployment, the deployment model we're
using is all at once right now, we haven't
101:43 - actually done a deploy yet we've I mean, we
technically have deploy, but we haven't deployed
101:47 - to an existing environments. So I'm just showing
you that it's using all at once. And so the
101:53 - next thing we're going to do is switch this
over to immutable and see what the difference
101:57 - is there. But we're not going to do it in
here, we could just click immutable hit apply,
102:02 - and then do a deploy. But I want to do everything
for the console. So that's what we're going
102:06 - to do next here. So let's get to that. So
to switch this over to mutable deploys, again,
102:17 - I said that we could modify this file, but
we want to do it programmatically. And we're
102:20 - gonna do that through the configuration files.
So what I want you to do is going to Eb dot
102:25 - extensions here. Stop letting me type that
happens to you just close bash, and open a
102:35 - new window here, I already have one open.
And I'm just going to go into Eb extensions,
102:39 - I'm going to make a new file, and I'm going
to call this 000 deploy config, I just wanted
102:46 - to be ahead of those other ones. I don't think
that order really matters, but it's just what
102:50 - I want to do. Oops, it's not make we want
to do touch. So we'll touch a new file there.
102:58 - And what we're gonna do here is we're gonna
set option settings. And we're gonna do eight
103:04 - of us Elastic Beanstalk here. And we're going
to command and what we're gonna do is set
103:12 - the deployment policy to be immutable. And
then what we're also going to do is set health
103:20 - check, success, threshold, threshold, to warning.
And then we're going to set ignore health
103:35 - check to true, and we're gonna set timeout
to 600 here. So I'm just gonna read this over
103:46 - very quickly here, make sure to make mistakes
here, Elastic Beanstalk? That's correct. immutable,
103:52 - or deployment policies immutable, that looks
right to me. Health Check status threshold
103:57 - that looks good to me. Ignore health check.
So we're gonna say over here, what we're doing.
104:02 - We're just we're actually checked boxing that
off, okay. And we're pretty much setting the
104:06 - same settings that are here, except this is
going to be warning. And that should make
104:11 - the deploy really fast. And also, while we're
at it, let's just make a superficial change.
104:16 - So when we do deploy, we can actually see
if the effects have taken effect here. So
104:21 - I'm just gonna go to our actual application
here, I'm gonna just change this to study
104:26 - sink. version one, if we were to actually
check out this application before we deploy
104:35 - it here. Notice that it says hello world here.
So we're changing that to study sync of version
104:41 - one. We're gonna go back to bash here, we're
going to go back and directory, it's gonna
104:47 - type clear. Make sure you're in the environments,
study sinkin here, and we're going to add
104:53 - all the changes. So we may modify that file,
we added a new configuration file, so I'm
104:57 - going to add them both commit I found em immutable
deploys, I'm going to push those changes,
105:13 - I'm going to go ahead and deploy that. So
so we'll just type in Eb deploy. And right
105:23 - away, it should start switching over to a
mutable deploys, because it's going to pull
105:29 - those configuration files, look at them, and
then it's going to decide on what to do. So
105:37 - I think we just wait here a moment, it's actually
going to tell us, and we can see that I actually
105:41 - have an error here. So I'm going to go ahead
and abort that. So I'm just type Ed abort.
105:50 - But we can see contains invalid key option
settings. So I probably just made a mistake
105:56 - here. Oops. Yep. We'll try this again, AB
deploy, oh, well, we have to commit those
106:12 - changes. Push, do AV deploy again. So what
we're looking for, is to see if it'll actually
106:33 - say that it's doing an immutable deploy. And
there it is, it says immutable deployment
106:49 - policy enabled. So we're gonna have to wait
for that deploy to happen, I'm just gonna
106:52 - open a new tab here, because I'm going to
stop the video here. And immutable deploys
106:57 - are a lot slower than that all once. But the
advantage here is that it won't take our server
107:02 - out of service, it's going to create a new
server, and then when that new server is good,
107:06 - it's going to switch over to it. So our users
will never have an interruption in service.
107:10 - So I'll see you back here shortly. All right,
great. So our mutable deploy has completed.
107:16 - It's actually been quite a while since last
time I was here, because I actually recording
107:20 - this in the next day. But I can tell you that
that immutable deploy didn't take too long.
107:26 - So it definitely takes longer than the than
the all at once deploy, all the ones is extremely
107:33 - fast, where these immutable deploys, have
to go through health checks, and then go through
107:40 - multiple checks before it determines that
the new service is good and moves over. So
107:46 - you know, that was immutable deploy. But what
I want you to do is go back to cloud nine.
107:50 - And we are just going to undo those changes
there, because the next thing we're going
107:53 - to learn how to do is bluegreen deployment.
And I don't want these immutable deploys,
107:58 - slowing down or development here. So just
to get rid of this immutable deploy stuff,
108:02 - all we're going to do is remove that file
there. So when you type in RM, and we're going
108:07 - to do tilde to here environments, study sync,
Eb extensions, 000 config deploy. And then
108:16 - we're just going to add those changes, I'm
going to make sure that I'm in that study
108:18 - sync directory there because it looks like
I was in the wrong place there. And we will
108:23 - do git add all get commit hyphen m, revert
back to immutable deploys. Okay, git push.
108:37 - And just before we do anything else, here,
I just want to go back to the environment
108:43 - here and just show you under your configuration
that it should have switched to immutable
108:48 - deploys. So here you can see it's immutable
and the health checks are disabled. But anyway,
108:54 - now that we have that set up, all I'm going
to do here is now that I've made these changes,
108:58 - I did a push, I'm just going to do a cap deploy
or not cap deploy, Eb deploy, I'm thinking
109:04 - of Capistrano, which is for Ruby on Rails.
It's not what we're doing here. And, and what
109:09 - we'll do is we'll just revert this back to
all at once. And this isn't going to take
109:14 - too long. So I'm just gonna go back here to
my dashboard, and we can see this is updating.
109:20 - And I'll see you here when this is done. And
then we'll move on to bluegreen deployments
109:24 - which should be super exciting. Alright, so
after a short wait there are, are mutable
109:36 - deploys your back to all at once deploys,
and we can just double check here under the
109:40 - configuration. If we go down here, we should
see now it's now it's all at once. So let's
109:46 - move on to bluegreen deploy. So bluegreen
deploy is when you you switch environments.
109:52 - So right now we have this environment here
which we can consider our blue environment.
109:57 - And the idea is we're going to spin up identical
environment called our green environment,
110:02 - which will have our latest changes. And once
that environment is in good shape, what we'll
110:06 - do is we will swap the URL of the environments.
Now this option isn't available to us right
110:12 - now. Because there's nothing for it to swap
to. But once we have that other environment,
110:17 - that's how we will make that switch over.
So in order to do so we're gonna go back to
110:22 - cloud nine. And what we need to do is clone
this environment, make a copy of it, and we
110:28 - could go in here, and I think we could go
right into here, and then click Actions and
110:33 - clone the environment. But let's do it through
the CLR. Because again, this is the developer
110:38 - associate, this is the best way to learn.
So we're gonna type in Eb two, or Eb clone.
110:44 - And then it's going to prompt us for the name,
I think clone is okay, for our case here.
110:49 - So I'll hit enter, we'll keep the C name the
same. And what's going to do is, start up
110:56 - a new environment there. So we'll go back
to study sync. And I'm just gonna give it
111:02 - a refresh. and here we can see that this environment
is spinning up. So again, we're gonna have
111:07 - to wait a little bit here, and I will see
you back momentarily. Alright, so after a
111:13 - short wait there, because it's using alt once
deployment, we have our production clone environment
111:18 - up, I know, it doesn't look like it's running
here. But if we just go back here a second,
111:23 - and I do a refresh here, we can see now it's
green. So I don't always trust the universe
111:28 - console, always refresh and look around. Because
sometimes things are ready, and you're just
111:32 - waiting around for nothing. So if we take
a look here at this clone environment, Following
111:36 - this, this see name here, or the DNS, what
do they call it, I'll just say URL, this URL
111:43 - here, we can see that it's running. But we
want to make sure that we have a new version
111:47 - here. So this is version one. So let's just
make a superficial change client version two,
111:52 - and then see how we can deploy to this new
environment and then facilitate the switch.
111:58 - So what I'm going to do is go back to cloud
nine, and I'm going to make my way over to
112:02 - the app. And it's just making some complaints
here. So I'm gonna close these tabs. And what
112:07 - we want to do is we want to go ahead and open
this app js file and just change this to version
112:13 - two. All right, and so now that I've changed
the version two, I'm going to commit this
112:18 - to the repo. So I'm gonna go get add, well,
I'll do git status, we should always do that,
112:22 - we can see the file one to add git add all
get commit change to version two. We will
112:31 - do a git push. git status. Alright, great.
So our version two is there. So how would
112:41 - we go about deploying to the green environment
because we have these two environments. So
112:47 - if we ran Eb deploy, I think by default, it's
going to deploy to the original environment.
112:52 - But if we want to specify the environment
we want to deploy to, which is the green environment,
112:57 - we just have to provide its name. So that's
called study, sync, prod, clone, I'm pretty
113:02 - sure that's the name of it. Yep, study, sync,
prod, clone. And so this should now deploy
113:09 - the latest changes to this environment. So
go ahead and press that there. And it's going
113:17 - to start up again. We'll just give it a second
here. We'll flip back, we will go back to
113:27 - study sync. I'm just gonna give it a refresh
here, because something should be changing.
113:32 - Probably the prod. Yep. So it's updating.
So we will let that deploy there. We'll give
113:38 - that a little bit of time. And I'll see you
back here, when that's done. And we'll just
113:41 - double check, it's version two. And if that's
all good, we can do the slop. Alright, so
113:48 - pushing our changes, or version two changes
to the clone is done, we go to this page and
113:53 - refresh, you're going to notice that there
hasn't been any change, but actually has worked
113:58 - on this is just an issue with Chrome. Because
if you open up another browser, and refresh
114:02 - in Firefox, it says version two for the same
URL. So this is a chrome caching issue. I
114:08 - spent hours upon hours trying to solve this
problem and not realizing it's Chrome. So
114:13 - just be aware that anytime you're doing deployments
to anything, you're checking stuff, always
114:17 - just rule out your browser. And sometimes
it's not even AWS. So you know, if we want
114:21 - to get to see the latest version here, I'm
going to open up Inspector, make sure you're
114:25 - on network and have disabled cache, do a refresh
there. And now it says version two, this will
114:29 - not work unless you have this open this checkbox
and then you do the refresh. All right. Um,
114:34 - but yeah, now that we have figured out how
to deploy our second version with bluegreen
114:40 - deployment, well, we can go ahead and do is
swap over the environment URL. And so we said
114:47 - that what we can do is go to here to actions
and to go to swap environment URL. And we
114:53 - could go here and choose our other version
and swap here. However, since this is for
114:58 - the developer, associate, I really do want
you to get as much experience with the CLR.
115:02 - I'm gonna just keep on saying that. And what
we're going to do is go ahead and use cloud
115:09 - nine and use the actual ebci to do that. So
the command here is Eb, swap. And then we're
115:16 - going to say, if we hit enter, now, it would
prompt us to ask, you know, the source and
115:20 - the destination. But I just want to be very
explicit here. And I'm going to just type
115:24 - in the source. So I want to swap the prod,
with the clone. So we'll say study, sync,
115:32 - prod clone. And this will do exactly the same
thing as swapping the URL URL out here, okay.
115:41 - I'm just gonna hit enter, unrecognized argument
here, cologne. Let me just make sure that
115:49 - I spelt that name correctly, I'm just gonna
go over here and check. Study sync, prod clone.
115:59 - Study sync product clone. Oh, sorry, you know
what, I have to provide a flag here it is,
116:13 - hyphen, hyphen, destination name. And that
will hit Enter. And this is going to trigger
116:21 - that swap action there. And so we're just
going to have to wait a little bit there.
116:25 - It says it's completed the swap. Wow, that
was really fast. So we'll go back here. And
116:30 - if we are to click on prod Now, what I want
you to notice is this is the clone URL here,
116:38 - right. And we are now in the clone environment.
So this used to say clone, but now it says
116:42 - prod it's taking the original seed name from
the other environment. If we go to the first
116:49 - environment here, this one now has clone.
So that's the swap that occurred. So that's
116:56 - how we know that it worked. Now that that
swap has occurred, what we want to do is just
117:01 - get rid of our old environment, because we
know our new environment is running. With
117:05 - no problems here. If we just go to prod like
this, it's running version two, so we're all
117:09 - good. And so what we need to do is go in here
and then just delete, terminate this environment.
117:15 - But let's do it from the COI. So we'll go
back here to cloud nine. And what we're going
117:21 - to do is type in Eb, terminate, study, sync,
prod. And then it's going to just ask us to
117:30 - confirm it. Hit enter. And that's going to
terminate. Now, we're pretty much done with
117:38 - bluegreen deployment here. And with that out
of the way, we can actually move on to learning
117:43 - how to deploy a single Docker container next.
So what we're going to need to do is rip down
117:48 - everything because we do not need even this
clone anymore. So I'm going to also terminate
117:52 - this one, but I'm going to do through the
console here. And we need to type its environment
117:56 - name. So I'm just going to copy it here. paste
that in. Make sure that's right. And what
118:06 - we're gonna do is we're going to wait for
these to shut down here. And when these are
118:10 - both terminated, we'll move on to the next
step, which is deploying a single container
118:16 - Docker environment to Elastic Beanstalk. Alright,
so we are back here, it went through the whole
118:25 - process here and built everything and it's
saying that it's running on port 8080, what
118:30 - we're going to need to do is open a new tab
here, because we're going to need to get the
118:33 - IP address of this Docker or this cloud nine
environment. So we've got a terminal here,
118:38 - I'm going to type in curl, HTTP, colon forward
slash forward slash 160 9.2 54 160 9.2 54.
118:51 - Latest metadata, we'll hit enter, make sure
this works. And then we'll type forward slash
118:59 - public IP v four. And that's the IP address.
So what we can do with this, just copy this
119:07 - here, and go Port 8080. And we will see if
this works. Oh, let's just click it to open.
119:17 - There it is. So this is running in a single
Docker container. The reason you'd want to
119:21 - Docker eyes your environment is because it
allows you to ship your your configuration
119:26 - with your code base. So you saw before we
are restricted to version 10, point 18, whatever
119:33 - have no but now we are only restricted to
whatever we provide with it. So a lot more
119:37 - flexibility around that. In order to prepare
this for deployment, we are going to need
119:42 - this node modules anymore. Because we are
using a Docker container and this is just
119:47 - going to do nothing. So we'll go ahead and
remove that. So what I want you to do is just
119:54 - close this tab here. I'm going to do Ctrl
C to stop the Docker container. I'm going
119:57 - to go CD Tilda. Oh, we're already in All right
place, but this is where we need to be. And
120:02 - we're just going to do RM, Eb extension 002
and just remove that file. So now that's been
120:13 - removed, we need to make an adjustment to
this file here, I'll just hit keep open, this
120:18 - needs to be Port 8080, because that's what
we're setting in our application. And let's
120:24 - go ahead and commit our changes here. So configure
give me for Docker. So now that our pushes
120:44 - have been changed, what we can do is go ahead
and do an EB create. I've been I've been single,
120:49 - so we don't launch a load balancer. We are
going to name this one differences so we can
120:54 - identify it, we're gonna say study sync, Docker,
we'll hit enter, we'll hit No, we don't want
121:01 - spot instances. And we will make our way over
to the cloud nine, or CLOUD NINE over here,
121:08 - do a refresh. We'll give it a second here
to start up there goes. We can see that using
121:17 - the Docker platform, we'll do a refresh. And
we will just wait until this is done and see.
121:24 - So after a little while there, our environment
is now running here. Let's take a look to
121:32 - see if it's working. And there you go. We're
running on Docker, it was that easy. The thing
121:38 - with Elastic Beanstalk is that it did all
the work for us. We just had the Docker file
121:42 - in here. And when we uploaded it, it did all
the work it built the image for us. But normally,
121:47 - what you'd have to do is build the image yourself,
and then push it to an actual Docker repository
121:54 - that could be Docker Hub, or in the case of
AWS, you can use elastic Container Registry
121:59 - or ECR. And that's what we're going to do
because that's a more complex setup and the
122:04 - more common stuff that people will be using,
because most people outgrow this Docker file,
122:09 - the simple setup here. In order to do that,
what we're going to need to do is create a
122:16 - new file called a Docker run itos dot JSON
file, and we're going to have to build an
122:21 - image and push it to ECR. But before we do
that, let's just make a revision to our actual
122:26 - code base here. And we will go to App dot
j s, and we'll call this version three. And
122:36 - the next thing we will do is we'll go ahead
and build our Docker image. So I'm going to
122:41 - type in a Docker build, hyphen t study sync
period. So this is going to build a Docker
122:51 - image. And it's going to name it study sync.
So we'll just wait here a little bit. And
122:57 - there it is, it's done. That was fast. The
next thing we need to do is we need to authenticate
123:04 - to ECR. And this is a very long command, so
we'll get to it. It's AWS ECR get login, password,
123:13 - pipe, Docker, login hyphen, hyphen, username,
AWS hyphen, hyphen, password, st in, we're
123:28 - going to have to provide our account ID here.
I don't know what my account ID is for this
123:34 - account, we need to poke around, we should
be able to find it somewhere. It's generally
123:39 - under my account settings, I just don't want
to show all of my billing information here.
123:44 - So another easier way, we'll just go make
our way over to I am, I feel like that's always
123:49 - a place where we see our account number. We
should see it anywhere. We'll just go into
123:57 - even the user here. Here's one, our account
number is everywhere. I just need part of
124:04 - it there. So I'm just going to paste it over
here and then extract it out. And then what
124:08 - we need to do is provide it as such. And then
we need to type in this URL. So we need d
124:16 - k r.ec r. And then we need the region we're
operating in. So US East one dot Amazon AWS
124:27 - comm if you're wondering how I got this whole
link is in the Airbus documentation for ECR.
124:33 - So what this is going to do is log us in and
generate out a token so we can authenticate.
124:39 - So it says there's an unknown flag name user,
so I'm just going to double check that there.
124:45 - It's actually supposed to be user name. So
we'll go ahead here and type this in. And
124:56 - here it says it's created that credentials
helper file, so there you go. So notice that
125:01 - it's created a file here called Doc, or a
hidden file called Docker config dot Jason.
125:07 - And that's what storing the token which is
going to help us to authenticate. So let's
125:11 - take a look at the actual images that are
here. And we can see that we have our images
125:16 - built here. And we're going to get how to
get this image ID next. And what we need to
125:21 - do is tag this Docker image. So I'm going
to put in that image ID, we need our account
125:27 - ID again here. And it's actually the same
link here. So it would probably be easier
125:34 - if I just copy that out like that. And then
we need to specify the name. So now that it
125:46 - has been tagged, what we can do is do Docker
push. And I believe it is the same URL here.
125:54 - So let's copy this. And it says here that
the repository does not exist with this ID.
126:07 - So maybe what we should do is make our way
over to ECR. And just maybe we need to make
126:13 - the repo beforehand. I always forget this.
So I guess we'll find out. I thought we just
126:18 - create it for us. Nope, I guess not. So we'll
just type in study sync here, I'll hit Create
126:25 - repository, then we'll make our way back to
cloud nine, just hit up. And there we go.
126:30 - It's uploading our Docker images, kind of
like GitHub is incredibly small, Docker image.
126:37 - So it's not taking too long, which is really
nice. One advantage of using Node JS over
126:42 - other languages and frameworks. So I'll just
wait here a little bit, and I'll see you back
126:49 - in a moment. So our Docker image is now built
and pushed to our ECR repo here. So if we
126:57 - go in here, we can see that we have it. And
the next thing is to prepare our actual, this
127:04 - next environment here, and instead of working
with this one, because it's gonna be a lot
127:08 - of work here, we're just gonna make a new
folder. So once you go, cd cd.or, just go
127:13 - actually here to CD Tilda Ford slash environment,
we're gonna make a new directory, we're gonna
127:17 - call it study sync external. And what I want
you to do is make a new file in here. So we'll
127:23 - just CD into this. We're going to call it
Docker run dot ABS dot JSON. If you've ever
127:30 - seen a task definition file, it's extremely
similar. And in this developer associate course,
127:36 - we definitely cover how to deploy with ECS,
and fargate. So this will become extremely
127:41 - familiar to you shortly. But what we need
to do is open up that file, I made it as a
127:45 - directory, that was an accident, I should
have made that a file, so I'm just going to
127:49 - remove that. And instead of doing MK dir,
I'm going to type in touch. Okay, and then
127:56 - we can just open up that file there. And what
we're going to do is write some JSON. So the
128:03 - first thing we need to do is define the Docker
version for EBS here, so Docker run version,
128:10 - it's going to double check, make sure that
is correct. Yep, that is right. And we're
128:16 - going to specify version one. version one
is for single, single containers, when you
128:21 - do multi container, you do version two, then
we need to specify the image. And that's going
128:30 - to be the URL we were seeing there earlier.
I feel like we could grab that from ECR. Yep,
128:36 - it's pretty much the same thing here. I just
want this part of it. I don't know if I need
128:39 - to put latest in there. Until we'll put that
in there, we have to specify the ports. So
128:47 - we'll go ahead here and do that. So it knows
what to map to. And then that we will do this
128:57 - little bit of cleanup here. I'm just gonna
double check to make sure everything is right
129:14 - here. Sometimes it's easy to miss these commas.
It looks all correct to me, so we're in good
129:20 - shape. The next thing we need to do is initialize
a Git repo. Here's we're gonna do Git init.
129:28 - And we're gonna copy over a couple files.
So we want to bring over our Git ignore Eb
129:32 - extensions file, and our n var config file,
I think so we will go ahead and do that. I'm
129:41 - just trying to think the easiest way to do
this, probably just make the files again,
129:45 - so I'm gonna just type touch dot Git ignore.
And then we will do we'll make a new directory
129:51 - called Eb extensions. And then we will touch
Eb extensions. 001 dot n var config. And I
130:13 - think that's the only two files we need to
move over. So we will go to our old one here.
130:19 - And it has some Elastic Beanstalk stuff in
here, and that will just take all of it, that's
130:25 - totally fine. And we'll go to our new one
here and paste that in. And we said, we need
130:30 - to set this as well. So we'll go to our old
one, Copy that, paste that, paste that there.
130:38 - And now that we have those files in there,
I want you to do is go ahead and do a git
130:43 - status. So we have three files, that's great
git add git commit hyphen, M. Docker run.
130:52 - And we need this, we need a Git repo because
it's going to create a new one when we run
130:56 - Eb net here in a moment. So I'll just do git
status, make sure that all worked fine. Great,
131:06 - and we will do Eb in it. So we're going to
choose US East one. So number one, we are
131:13 - going to create a new application. So press
two, we are going to stick with the name that
131:18 - we are given here. So hit enter. We are definitely
using Docker. So we'll hit yes, we want to
131:24 - use code commit. Sure. So we'll hit y we need
to select a repo, we are making a new repo.
131:30 - So press to enter the repos name is going
to be called study sync external. Make sure
131:39 - you type it right. We are going to want it
to be master branch. So hit enter. We don't
131:53 - need to SSH in. That's okay. And so there
we go. So now that that is created, I'm just
132:05 - going to double check and make sure that is
the case. So we'll make our way over to code
132:09 - commit. And here we have our external repo.
So it's all in good shape. So now that that's
132:20 - all set up, we should be able to create a
new environment. So we'll do Eb create, hyphen,
132:26 - hyphen, single, we will name it pretty much
the same, I'm just going to take off the dead
132:32 - part on it doesn't matter too much as long
as you can remember what you set it to. We'll
132:40 - say no for spot instances. And what we're
gonna do here is just wait a little bit here,
132:50 - it's nice to see the message, just make sure
that it's creating what we want to create.
132:56 - And this is a Docker image. So hopefully this
works. First try, I'm going to make my way
133:02 - back to Docker here, we'll go over here. And
while that's going, we can go ahead and terminate
133:07 - this one, we don't need this one anymore.
And just to point out, like, look at this,
133:18 - this doesn't contain any of our code. So where
is our code, our code is part of the actual
133:24 - Docker container. That's why we don't see
here, because when we built it, it copied
133:29 - it and put it into the actual container. Whereas
in this setup, the Docker file is here. And
133:33 - so we can work with our source code and have
it all in one place. So you just have to decide
133:38 - you know what workflow works best for you.
And you know, if you can get away with just
133:40 - having a Docker file like that, that's definitely
better. And this is creating, I'm just going
133:44 - to go back here and do a refresh. I don't
see this new environment yet. should be called
133:55 - external write. Study, think external. Oh,
here it is. Yeah, because it's a completely
134:08 - new application. That's totally fine. So yeah,
I'll see you back here in a moment once this
134:13 - environment is done creating. Alright, so
our deploy is done, but we have helped degraded
134:16 - and it looks like we have an error here. It
turns out, Elastic Beanstalk can't authenticate
134:21 - to ECR because we didn't give it permissions
to do so. Whereas in cloud nine, we had pulled
134:26 - that all the credentials and stored it in
here so that we could read from ECR. So what
134:31 - we need to do is update the incidence profile
of the actual eccm instance that runs here.
134:38 - So what we'll need to do is make our way over
to Iam. So just type in Im here. We'll open
134:45 - this in a new tab. On the left hand side,
we'll go to roles. We'll go to EC to roll
134:52 - here for Elastic Beanstalk. We're going to
attach permissions. We're gonna type in Amazon
134:58 - EC two Read Only container will attach that
policy. So this should allow us to gain access
135:07 - to ECR. And then what we'll do is go back
to cloud nine. And we'll simply do Eb deploy.
135:19 - And so what that will do is it will just deploy
again. But now it will also update the Iam
135:24 - role, and it should have permissions this
time around. So we'll give it a second here
135:27 - to get started. We'll make our way back over
to here, we'll do a refresh. And we can see
135:35 - this is in progress. And I'll see you back
here momentarily. And our deploy is done.
135:46 - So I'm just going to close these additional
tabs here, I'm just going to open up the new
135:50 - tab here. And we are now seeing version three,
if you don't remember could be chrome caching
135:55 - it but there you go. So we went through a
lot of different variations here with Elastic
135:59 - Beanstalk. And you know, that is a lot of
stuff, but is necessary to go all through
136:05 - these things. Let's just go ahead and clean
up what we have. So I'm going to go back all
136:09 - the way to applications. And what we can do
is go ahead and just delete these applications,
136:14 - this should terminate all the environments.
So hit Delete. And we're going to also delete
136:23 - this application here. We'll say delete, click
into this, this should be a terminating. So
136:33 - our cloud nine environment was not a big issue,
it's going to shut down after 30 minutes,
136:37 - when we're not in use for our elastic containers,
we're going to go to code commits. I don't
136:45 - think this is really an issue having these
around. So I'm I don't have much motivation
136:49 - to delete them. And we might be using them
for the ECS and fargate tutorial. So we're
136:56 - gonna leave these alone, or we're gonna leave
our code commits, or, or Yeah, co committed
137:00 - ECR alone, so we'll leave this alone as well.
But yeah, that's it for the Elastic Beanstalk
137:08 - walkthrough. Hey, this is Andrew Brown from
exam Pro. And we are looking at elastic container
137:19 - service, which is a fully managed container
orchestration service. It's highly secure,
137:24 - reliable and scalable way to run containers.
So let's take a look at the components of
137:30 - ECS. And so over here, we have a technical
architectural diagram. And we'll just talk
137:35 - about all the components involved. So the
first is the cluster itself. So ECS cluster
137:40 - is, is just a grouping of EC two instances,
they call them EC two containers, which is
137:45 - a bit confusing, because inside these instances
have containers within them, so they are both
137:53 - running Docker installed on them. So you can
launch EC two or Docker containers. And so
138:00 - another thing that's really important is task
definition files, we don't have a representation
138:04 - of any of that in our architectural diagram
here. But that's just a JSON file that defines
138:08 - the configuration of up to 10 containers that
you're going to want to run, then you have
138:13 - a task, and that uses a task definition to
launch containers. And a task is a container,
138:21 - which runs for only the duration of the workload.
So the idea is, let's say you have a background
138:27 - job you want to run as soon as it's done,
the task stops or deletes itself. So it's
138:32 - really good for one off jobs, then you have
a service. And this is exactly like a task,
138:38 - except that it's long running. It's intended
for web application. So Ruby on Rails, Django,
138:43 - express j s, where you don't intend these
things to shut down. And the last thing we
138:48 - want to talk about here is the container agent.
It's not represented in the diagram here.
138:52 - But this is a binary that's installed on the
EC two instances. And this just monitors,
138:57 - as it is monitors the tasks as well as it
stops and starts. Let's talk about the options,
139:07 - we have to choose one configuring a cluster.
So the first thing we're going to do is go
139:11 - ahead and create that cluster. And you're
gonna have to choose between fargate or ECS
139:16 - clusters and whether you want it to have some
networking components involved. And then once
139:22 - you've done that, you have to go through and
choose a bunch of options. So you have to
139:25 - choose whether you want to be spot or on demand.
So with ECS, you can save money with spot
139:30 - because if you're running background tasks,
maybe they it's not a big deal for those to
139:35 - get interrupted. Then you don't want to have
to, you're going to want to choose the EC
139:39 - two instance type, and then you'll have to
choose the number of instances. You'll have
139:43 - to choose the EBS storage volume and then
you can choose whether you want use Amazon
139:48 - Lex to her Amazon Linux one which both have
Docker installed on them. So there are some
139:53 - of those options right there for you as you
can see, then you'll have to choose your VPC
139:57 - or create a new VPC. Then you You need to
decide in an IM role, then you have the option
140:03 - to turn on cloudwatch container insights,
this is going to give you richer metrics about
140:07 - the operations of your containers. And then
you can choose a key pair, which is unusual
140:15 - because you don't necessarily need to log
into your instances. And 80 of us generally
140:19 - does not recommend you to SSH into those containers.
But you totally can. So that's all the options
140:24 - for ECS. And we'll see this again, when we
go through the follow. Let's take a look at
140:34 - that task definition file that we talked about
that is used to launch our tasks or services.
140:39 - So what you do is you'd go hit the Create
new task definition, and this would actually
140:43 - have a wizard to help you get set up. But
if you had to write this by hand, this is
140:47 - what the actual file would look like. And
in this file, you can define multiple containers
140:52 - within a task, which is actually what we're
doing here on the right hand side. And the
140:57 - Docker images can be provided either by ECR,
which is elastic container repository, which
141:02 - we'll talk about the next slide, or an official
Docker repository, such as Docker Hub. So
141:07 - here, you can see that we are specifying an
image and that image is WordPress. And then
141:13 - another important thing is that you must have
at least one essential container. So this
141:17 - container fails or stops, and all the other
containers will be stopped. So this is just
141:20 - to make sure that you have at least one dependent
resource there. And if you aren't sure how
141:28 - this all works, it's okay because eight of
us has that wizard. So when you click that
141:32 - create new task definition button at the top,
they have all the fields that you fill out
141:36 - to create this, but if you wanted to create
by hand, you could totally do so. So I want
141:45 - to take a quick look here at tasks Container
Registry, which stands for ECR. And this is
141:49 - a fully managed Docker container registry
that makes it easy for developers to store
141:53 - manage and deploy Docker container images.
So just to give a representation here, you
141:58 - have a Docker image, and then you can push
that to ECR, which is like it's like a repo
142:03 - for Docker, Docker images. And then once you
have it there, you're gonna be able to access
142:08 - it from elastic Container Service, fargate,
Kubernetes, or even on premise. So it's just
142:13 - an alternative way of storing a Docker image
as opposed to Docker hub or somewhere else
142:18 - or hosting your own on it's highly secure.
Hey, this is Andrew Brown from exam Pro, and
142:27 - welcome to the ECS. Follow along. In order
to do this follow along, you have to do the
142:31 - Elastic Beanstalk one first, because in that
fall long, we build out a web application,
142:36 - and we turn into a Docker image and then we
host it on ECR. And we are going to need this
142:41 - ECR Docker image in order to complete this
follow along so please go to that one first.
142:45 - And do that follow along and you definitely
should. And once you have that done, come
142:50 - back here and we will proceed forward. So
before we can create our ECS cluster, we're
142:55 - going to need to create an IM role for our
EC two instances. Each of us has documentation
143:00 - all this the Amazon ECS instance roll, these
instructions aren't very clear, but I know
143:04 - what to do. So let's follow along. So the
first thing is we're going to want to name
143:07 - it this UCS incident roll. Well, you could
name it anything you want. But let's just
143:12 - be consistent here as because this is what
everybody else names, it will make our way
143:16 - over to I am on the left hand side, you want
to go to roles, we're going to create a new
143:20 - role. We're going to leave that here, choose
EC to go next. And then what I want you to
143:26 - do is type in EC to container, or we're looking
Container Service role for EC to this one
143:34 - here, I'm going to double check it just make
sure it is what I'm expecting it to be, I
143:38 - usually can tell what this stuff is by looking
at the services. Yep, this is the one we will
143:43 - go next, we will hit Next, we will name that
role, we will create that role. So now we
143:50 - have this role, and we're ready to go create
our cluster. So going back to our first tab
143:54 - here, I want you to make it over to ECS. So
we'll click on that left hand side, we will
144:00 - choose cluster we will create a cluster and
we'll be presented with multiple options.
144:04 - It's defaulted to fargate, which we will be
doing in the fargate Follow along but right
144:07 - now we're doing ECS and the way you know if
it's ECS is that it's not powered by fargate.
144:14 - And you do not create an empty cluster. So
we are going to use Linux. So we'll hit next
144:18 - here. And if we check box this to here, this
would make it fargate. But that's not what
144:23 - we're doing. I'm gonna call this my ECS cluster,
we'll leave it as on demand spot is a really
144:30 - great way to save money but I don't I just
don't want anything to go wrong in this fall
144:35 - long. So we'll just leave it as on demand.
I want you to go look for T to micro here.
144:41 - Because that's part of the free tier there
I found it. We're gonna only have one instance
144:47 - we want to keep this very inexpensive. Amazon
links to seems fine to me. We do not need
144:51 - to set a key pair. We do not need to set any
of the VPC settings here and we need to make
144:57 - sure our ECS instance role is there. It automatically
select Did it and then we can go ahead and
145:02 - hit Create. What's that going to do? It's
going to create an ECS cluster, then it's
145:07 - going to make sure we have that Im policy.
And now we're just waiting for this cloudformation
145:11 - stack to complete. So this won't take too
long. It's still pending right here. We're
145:17 - just waiting for the auto scaling group and
the internet gateway. I think what I'll do
145:23 - is I'll just pause the video here. Oh, no,
it looks like it's almost done. Maybe we'll
145:28 - just give it a second here. Oh, okay. It's
proceeding forward. Now, we're just waiting
145:39 - for the security group and the auto scaling
group. Okay, great. Now, we're just waiting
145:47 - for these last two here, the Virtual Private
gateway in the auto scaling group. Alright,
145:57 - now we're just waiting for the aect route.
You can see that it sets up a lot of stuff
146:03 - to make this cluster. And that looks like
it's done. It's still spinning, though. I
146:11 - think this is pending here, the the route
table subnet Association. So we're just waiting
146:14 - for the route tables to set up and there we
go. So let's go ahead and view your cluster.
146:21 - And what we need to do is want to create a
service. But before we can do that, we're
146:25 - going to need to create a task definition
to actually launch. So go to task definitions
146:29 - create a new task definition, we have the
option between far gaming c two, we obviously
146:33 - want to see two, but that's for ECS. or hit
next, I'm gonna name this study sync, we need
146:42 - to choose a task role. Optional, Im role that
the task can use to make API request to authorize
146:49 - data services, create a one here, we might
need one here for using ECR. I'm not sure
146:59 - I guess we'll find out as we go. We'll go
down here and we'll have to specify some memory.
147:07 - In order to do this, we need to know how much
memory comes with a T T two micro that is
147:13 - 500 megabytes. So that's the maximum we can
do. And with a T two micro you get one v PC,
147:19 - so I'm going to place that in there. Here
you can set the CPU units, which I have no
147:23 - idea what to do for that I don't know if one
Vc vcpus wanted you for probably is but I
147:30 - definitely know there is only one v v CPU
for virtual CPU for t twos. T two micros.
147:39 - So now that we're there, we'll go ahead and
add our container. I'm gonna name this study
147:44 - sink container. And then we need to specify
our image repos. So we'll go over to ECR,
147:51 - I'm going to copy that. Paste that there.
Um, I think we want the latest. I think we
148:00 - can place the latest in here. And we'll set
it to 256 megabytes, it's a no GS app, it
148:07 - doesn't require a lot of memory, we want to
map the port. So 80 is the is the port that
148:13 - goes to the internet. That's what we want.
And 8080 is what our container port is. That's
148:18 - what we start up our web application on. And
we will see that if like you'll see that in
148:22 - the Elastic Beanstalk follow along, but that's
what we do, we're not going to set up the
148:26 - health check. We do need to set up some environment
variables, we don't need to set an entry point
148:31 - or Command, these are set in that build image.
If we wanted to override them, we can place
148:34 - them in here. Same thing with working directory,
we're going to set up the port here, which
148:40 - will be 8080. And then the node environment
which will be production. Then we'll go down
148:47 - here. There's some there's a quite a few options
here. Not super important. We'll keep on going.
148:54 - And yeah, that should be everything that we
need. We'll hit Add. I still don't know if
148:59 - we're gonna run into an issue because I know
that ECR requires authentication. And I don't
149:04 - know if it will pull it without that task
role, but I guess we'll find out. We have
149:09 - a lot of interesting options here for service
integration. So app mesh, proxy configuration,
149:15 - log router with fire lens, I don't even know
what forlenza sounds like a new service to
149:18 - me. We can add volumes not necessary. I do
want to point out one thing I can't maybe
149:26 - I can click it here. I just didn't show to
you. But when you set environment variables
149:29 - you can do value from and this allows you
to provide the Arn of a systems parameter
149:34 - stork key that allows you to pull in secret
parameters. So if you had secrets you want
149:39 - to put in there, you could definitely do that.
I'm just going to now it might show up on
149:43 - the exam. We'll go ahead and hit Create unable
to create this task definition. It doesn't
149:48 - like something I've done. I'm just looking
through it here. looks okay to me. The only
150:00 - thing I can think of is because it doesn't
have a TAS role. But it says optional Im role.
150:11 - Alright, so I tried going ahead and creating
this task definition. And I got this here.
150:18 - And it's exactly what I thought it was, we
just don't have permissions here. And so I
150:23 - did a bit of googling, and I believe we are
missing this ECS task execution role. So here
150:30 - they have the instructions. So we will go
ahead and go ahead and create that. So luckily,
150:36 - we still have I am open here, I'm going to
create a new role. I think we need to choose
150:41 - ecso, select trust entity choose elastic Container
Service. What's interesting is that this would
150:50 - actually have been created for us if we had
launched Amazon ECS console first run experience.
150:56 - So the first time you do it automatically
creates it, but like, I don't have anything
150:59 - to launch. I've heard das definition. It's
like a catch 22. One of those things that
151:04 - I wish at best would improve upon I'm sure
if I can play it on Twitter, they will definitely
151:08 - do that. But anyway, we'll just go ahead and
manually do it. Because it's all about figuring
151:13 - stuff on your own here. And they might actually
already have a pre made one here. But we'll
151:17 - just go down here. Select your use case, choose
elastic Container Service task. Oh, there
151:24 - it is. Okay, we'll hit next. And it should
automatically add the policy, I guess not
151:30 - attached permissions policy. Sometimes when
you select those pre made ones, that automatically
151:36 - fills it in for you. But I guess in this case,
that's the one we need. We'll hit next, we'll
151:41 - hit next. Let's name it what it suggests us
to name it. Whoops, there we go. Now we have
151:51 - a roll, we'll scroll up, hit refresh. And
there it is. And then we'll go down here,
151:56 - you're given permission to ECS to create and
use the ECS. So here, it's suggesting that
152:01 - it would have created it on our behalf, but
I never did. Alright. That's fine. We'll sit
152:08 - great. Anyway. Hi, all right, I'm back here
again, sorry, for all the hard cuts, I'm just
152:12 - having a really hard time with ECS. I made
this fall along in my other iOS account, it
152:17 - works perfectly the first time, and I'm just
getting beat up at every single corner. And
152:21 - trust me, I go on Twitter. And I literally
complain to AWS about these things, just because
152:28 - they are really painful. But I just want you
to know that even myself as being an ageless
152:33 - expert. I even have hard time getting through
these follow along. So you know, just stick
152:37 - with it if you have any issues. But anyway,
I was able to create this task definition,
152:41 - I have to create a revision here to show you
what was going on there. So I'm just gonna
152:46 - hit Create revision. And it has all the same
filled in information. And the only problem
152:51 - I had, we definitely probably had to create
this task role, we should have made it anyway.
152:56 - But we needed to click on here. And I had
latest on the end of here, which I had I thought
153:01 - prior, but I guess it doesn't allow you to
do it. So just remove latest on the end of
153:05 - an end of here. And then you'll be able to
create this task definition. And also in this
153:09 - box here. I wasn't carefully reading it. But
it was actually saying that it would have
153:13 - created this for us anyway. So if we left
this blank, and we removed that latest there
153:17 - and created it, it would we would have had
that role. But anyway, we made we made it
153:22 - through there. So just go ahead and hit Create.
And you should be exactly where I am with
153:26 - the task definition. So now that we have our
task definition, I'm definitely gonna close
153:32 - these things here. Because we have a lot of
tabs open. I'm just going to keep on going
153:38 - here, all the way here, we are ready to launch
this, this task. So we'll go over to clusters,
153:45 - oops, I clicked Eks. Definitely not doing
Kubernetes. Today, we'll go to clusters here,
153:50 - we'll click on my ECS cluster. And we have
services and tasks. So services continuously
153:55 - run tasks. And as soon as the task is done.
So this is a web application, we definitely
154:00 - want to make it a service. So we'll hit Create
here, we want to be easy to because that's
154:04 - free CS, we're not making a fargate type.
We'll leave that name alone, that's totally
154:09 - fine. We'll name the service, I'm just going
to call my service very unoriginal. We're
154:13 - gonna leave it as a replica. We want one task
running here. We'll leave it as rolling update,
154:20 - that seems fine to me. Easy balance spreads
seem fine to me, I don't play around with
154:25 - these too much. We'll go ahead and hit next.
And it's gonna ask us what low bounce we want
154:30 - to use. We don't want when we want to save
money, obviously, it's recommended to use
154:34 - a load balancer and and have things in an
auto scaling group. But you know, we just
154:39 - want to be able to launch a service. That's
all that we really need to learn here. And
154:44 - we'll scroll down we have a lot of options
you don't need we don't need to read any of
154:48 - this will hit next that we have service auto
scaling is without an auto scaling group.
154:52 - We do not need one so we'll hit next and then
we will create the service. So this should
154:58 - actually be very fast. So when you log launch
an easy to instance, it takes forever, right?
155:04 - Because you have to wait for it to create
one. But there's already one running, all
155:07 - it has to do is put that task in there. So
we just have to wait a little bit on this
155:13 - will take a little bit of time here the first
time around. But I'm pretty sure that when
155:16 - we launch tasks after this initial setup here,
it's really, really fast. So we'll just wait
155:25 - here a little bit, and I'll see you back in
a moment. And that was actually really fast.
155:30 - I did not even have to stop the video, but
I did so. So that was the started, we can
155:35 - go check out that my service button, click
the big blue button down below. And here we
155:40 - can see our task is running. And it's running,
study sync colon one. So version one. Yep,
155:48 - that version is fine. And we'll click into
task because we only actually have one version.
155:54 - And in here, what we can do is we can see
the container instance, if we click into that,
155:58 - it's going to give us information such as
the public IP address, and etc. I think if
156:03 - we click into the task here, and we drop down
here, I know we just went backwards, but actually
156:08 - shows us Port 80, and 8080. We just have this
convenient link to get to it, there's tons
156:13 - of ways to get this link. And we can see our
application is running. So version three was
156:17 - the one we had last and Elastic Beanstalk.
So that's all it takes to run an ECS task.
156:23 - Now there's obviously a lot of options in
ECS. Not important for this fall long. But
156:31 - definitely, you know, read through all the
lecture content, if you are doing the DevOps,
156:35 - you definitely no need to know all those options.
So way longer follow along with ECS. For that,
156:43 - but for the developer associate, all we need
to know is how to launch that service. Now
156:48 - this cluster costs money, because it is an
easy to instance, that is constantly running.
156:52 - So I'm gonna go ahead and delete it. So we'll
just type in delete me. And it takes a little
157:00 - bit of time here. And once that's deleted,
we'll be in good shape. I'm not going to wait
157:05 - around for this video to see this delete,
it's deleting I know it will happen. But yeah,
157:10 - once this is done, that means we can move
on to the fargate follow on which is very
157:14 - similar except there is no easy to instance
running. So that's super exciting, because
157:18 - it's a serverless container. So we'll see
you then, hey, this is Andrew Brown from exam
157:27 - Pro. And we are looking at fargate, which
is serverless containers. So don't worry about
157:31 - the servers, run the containers and pay based
on the duration and consumption. So fargate
157:37 - is sometimes branded ECS fargate, or just
fargate by itself, but it is under the ECS
157:43 - console. But the difference is that when you
want to launch fargate, you go ahead and you
157:49 - create an ECS cluster, but you actually make
an empty cluster, so there's no provision
157:53 - to EC two, and then you launch the tasks as
fargate. And you could also launch services
157:58 - there as well. So you no longer have to provision
configure and scale clusters of EC two instances
158:03 - to run those containers, you're charged for
at least one minute, and then after that it's
158:08 - per second. You pay based on the duration
and consumption, which we'll look at a bit
158:13 - more. But to really understand the difference,
I just want to give you a visual comparison.
158:18 - So this is ECS, which we saw prior. And you
can see that there are you see two containers.
158:23 - And then for fargate. It's extremely similar
except there's no containers. So you know,
158:28 - hopefully that makes that clear. So now we're
going to look at how to configure a fargate
158:36 - task. And the first thing you're going to
do is using the task definition wizard in
158:42 - the fargate console, you're going to have
to choose how much memory and CPU will utilize
158:48 - entirely for all your tasks. And then what
you're going to do is you're going to add
158:52 - containers and then allocate memory in CPU
for each based on the requirements based on
158:58 - the allocation that you've defined above.
So here I have a Ruby and a Python container.
159:04 - And you can see that I've split the memory,
half and half for both. When you run the task,
159:10 - you can choose what VPC and subnet it'll run
in a lot of people think that there are no
159:14 - VPC, subnets with fargate because it's a because
it's serverless containers, but it's not true.
159:19 - So you do have flexibility flexibility around
that. And you can apply security groups to
159:24 - a task. This is a great way to you know, have
a secure security around your tasks. And this
159:32 - actually puts you up as an exam question knowing
that you can apply that the reason why there's
159:37 - some, like this is such an important question
is because, you know, would you apply the
159:42 - task to the EC two container that's running
the actual server or would you apply it to
159:48 - the tasks and this is gonna apply for both
ECS and fargate. It's always at the task level.
159:53 - You can also apply an IM role to the task
those you can say for every individual task
159:58 - delegated different policies. And just to
reiterate, here, you can apply a security
160:05 - group and an IM role for both ECS and fargate.
So that's for both tasks and services. And
160:10 - again, that might show up as an example. So
I want to do a quick comparison between fargate
160:19 - and lambda, because they're both serverless
compute. And so they seem like they solve
160:24 - the same problem. But there are a few key
differences. And so we're just going to quickly
160:29 - walk through those. The first is they both
have cold starts. And it can be argued that
160:35 - fargate cold starts are slightly shorter,
I can't really remember as to why, but in
160:41 - the documentation, I think it says that they're
a big different factor is duration. So with
160:47 - a lambda, the maximum time you can run it
is 15 minutes, where the duration with the
160:51 - fargate task can run as long as you want,
because you can just make it a service. And
160:55 - it runs indefinitely. In terms of memory,
lambda has an upper upper limit of three gigabytes,
161:03 - whereas fargate has an upper limit of 30 gigabytes.
So if you need a lot of memory, go over with
161:08 - fargate. For containers, you provide your
own containers with fargate. So you definitely
161:14 - have a lot more flexibility in terms of configuration
there. For lambda, setting up containers,
161:20 - is extremely limited. Use the standardized
containers, and then you build stuff on top
161:24 - of that. So if you really need something highly
configurable, you're going to need to go with
161:28 - fargate. For integration, lambda just has
seamless integration with a lot of serverless
161:33 - services. And it just makes it really easy
to chain stuff together. Even recently, like
161:38 - lambda has output destinations, and it's just
keeps on getting more and more easier to integrate
161:45 - with stuff fargate, you know, you can orchestrate
things together. So like with step functions,
161:49 - you can tie things together, but it's just
not as seamless as lambda. So and you know,
161:56 - you do have to do a lot of configuration to
get your cluster set up and the fargate tasks
162:02 - set up. But definitely less less than doing
an ECS cluster without you know, without fargate.
162:09 - And the last thing is pricing. So with lambda
you're paying per 100 milliseconds, where
162:14 - with fargate, you're paying at least one minute
and then every additional second is how the
162:20 - pricing works there. Obviously, the amount
of memory you use, and the CPUs also comes
162:27 - into factor there. But just the takeaway that
I want you to know is that lambda is 100 milliseconds,
162:34 - right, and fargate is one minute and every
additional second. So hopefully, that makes
162:39 - a little bit of sense to you there. And you
can kind of have an idea of like the use cases
162:43 - for each. Hey, this is Andrew Brown from exam
Pro, and we are looking at the fargate. Follow
162:53 - along if you have yet to do the ECS and Elastic
Beanstalk follow along, you have to do those
162:58 - ones first. And the reason why is that this
is all dependent on a Docker image that we
163:03 - created in Elastic Beanstalk follow along.
And it's good to do this ECS one first so
163:09 - that you get a difference between ECS and
fargate. So, you know, go ahead and do that.
163:15 - And once you have this Docker image, you can
proceed forward here. So for this one, what
163:19 - we're going to do is make our way over to
ECS. Because that's where fargate is sometimes
163:25 - atrios calls it ECS fargate, sometimes it's
just called fargate. All right, but anyway,
163:31 - fargate is ECS, we'll go over to clusters,
we're going to create a new cluster, and we're
163:36 - going to use networking only because eight
of us is going to manage the EC two instances
163:41 - for us. So we don't have the same kind of
options as we would here with the other ones.
163:46 - If you were to go here and choose an empty
cluster that is basically a fargate cluster,
163:50 - I'm going to go back here and choose networking
only, which is the default. And we're going
163:54 - to name this cluster, I'm gonna call my fargate
cluster fargate cluster, we are not going
164:03 - to create a VPC, we're gonna use the default
one here, we're gonna hit create, and it creates
164:07 - it right away. Because it's serverless. It's
super fast. There's there's no server running
164:11 - as of yet. So that's why it's super fast,
we need to create a task definition, we created
164:16 - one before for study sync for ECS. And this
time, we're going to make one for fargate.
164:23 - And we're going to name it study sync. And
I'm going to put an F on the end. That's just
164:28 - my convention for distinguishing those tasks
here. We're going to choose the ECS task execution
164:35 - role. If we had not had this here, I feel
like it would have would make it for us. We
164:39 - have the network mo which is AWS VPC, I was
on Ada support. And I was not aware of this
164:46 - until I talk to the sport engineer, I guess
is the default. And the way it works is that
164:52 - whatever port you set is what is what mapping
you're going to get and explain that in a
164:57 - moment. When we get to adding our container.
We're going to choose Test memory size and
165:01 - we're gonna choose our V CPU, I noticed that
like, I can't go up here. So there are some
165:05 - restrictions as to what you can do. A quarter
of a virtual CPU is well enough for this example,
165:11 - no GS does not require a lot of memory, or
memory or CPU power, at least our use case
165:17 - doesn't. We'll go ahead and add a container,
we're going to need the ECR image here, I'm
165:22 - going to hit copy. And look here, it says
that we can put colon tags. So in the ECS,
165:27 - follow along, I had colon latest from really
sure that we can do. But for whatever reason
165:34 - wasn't working for me, maybe I had a spelling
mistake, maybe there was something off there.
165:38 - But it'll definitely seems like I can go in
here and use this one where has colon latest,
165:43 - but I just don't want to have any problems.
So I'm going to leave that out there, we're
165:46 - going to name this container, I'm going to
call it my fargate container. Okay, not not
165:52 - really original there, we'll just select 128,
it's less than we used in the ECS. One, but
165:57 - it's totally fine. And notice, we don't have
a host mapping. So this is what I was talking
166:01 - about this AWS VPC thing. So our host port
is going to be 8080, we don't have a choice
166:08 - in the matter. That's all we get. We're going
to scroll down, we're going to make our way
166:13 - over to the environment variables, I'm going
to set one for port, and we'll be at 80. And
166:20 - then we'll set one on node E and V. production.
Now, I guess if we want to run this on port
166:26 - 80, I could change this to 80. But the thing
is, like, if you've ever ran, if you ever
166:32 - set up a server, anytime you run Port 80,
on Amazon, Linux two, or whatever throws errors,
166:36 - you got to make sure it's sudo, it's a big
pain, so we're just not going to do it. But
166:40 - for a production environment, you'd want some
80. Or we'd have to, we have to find another
166:47 - way around this like doing a proxy a container,
which we'll talk about later. In this follow
166:52 - along here. We don't need to fill out anything
else in here, there's a lot of options here
166:56 - not even worth discussing. And we're gonna
hit Add. And you can see it's only utilizing
167:00 - this amount of the actual container, I'm going
to scroll on down here all the way to the
167:05 - bottom, we're gonna hit create an ad creates
super, super fast, we're gonna view that task
167:10 - definition. And we don't really need to do
anything here. But we'll make our way over
167:14 - to the clusters. And we have my fargate cluster,
and it should say one service running, it's
167:20 - probably just starting starting. It's getting
going. So we'll click into it here. We should
167:27 - see a service Oh, no, we won't, because we
just created a task definition, we actually
167:31 - haven't launched the service yet. So we'll
hit create, we will choose the f1. We'll choose
167:37 - fargate. It's one we don't have anything else,
we have my fargate cluster, a name is my fargate.
167:44 - Service, I ran into an error, where I named
this my service, which was what we named the
167:52 - ECS one and then it errored out on me, which
was weird because it didn't exist anymore.
167:55 - So you may have to fiddle with the name here.
So I'm just changing it just so I avoid that
168:00 - issue. We only want one task to be running,
we're gonna leave it as rolling deploys, and
168:04 - we'll go next. And then we have to choose
the cluster VPC and subnet, which when you
168:09 - think of like serverless containers, you think
you would have to choose anything, we're gonna
168:13 - go ahead here and choose VPC, we're gonna
choose the first subnet, we're going to scroll
168:17 - all the way down, we're not going to use a
load balancer, we just want to save money
168:20 - and not have any complications. But for a
production environment, you definitely would
168:23 - want to set a load balancer all the way down
the bottom, we're gonna hit next, we're not
168:29 - gonna use auto scaling group. But for production,
I would absolutely do that. There's no additional
168:33 - cost there. So it's easy. And then we'll scroll
all the way down to the bottom, we get a nice
168:37 - review. And we'll hit Create. And this is
super fast, way faster than ECS. Well, actually,
168:42 - I shouldn't say that ECS is faster, because
it doesn't have to start up a new CTO instance.
168:48 - So this looks like it's fast. But when you
go here, you're gonna notice that it's provisioning.
168:51 - So it's actually not running as of yet. So
UCS is way way faster for getting your tasks
168:58 - running. So that is one trade off, you have
to consider. Now, while this is getting started,
169:04 - we can see we have this public IP. It's, it's
in pending or provisioning. So if we go here,
169:08 - we're not going to get anything. And also
it's not going to work because it's running
169:11 - on port 8080. So we're going to need to expose
that port there is a security group attached
169:17 - to this cluster or this task or service or
something. security groups are around tasks,
169:24 - so we should click into the task. Yeah, there
it is. So we'll click into this. And we will
169:33 - go into inbound rules, we will edit a rule
and we are going to add a new rule 8080 for
169:40 - everybody in the internet. Let's save and
then we'll go back and we will check. Check
169:45 - on our service here to see if if it's warmed
up. And so it's active. We'll click into it.
169:54 - We'll go to tasks. We'll click into the task,
we will copy the public IP address dress.
170:00 - we'll paste it in here. And we'll do AD AD.
And does it work? Nothing as of yet. Oh, it
170:06 - still says pending here. Okay, we don't have
any errors. Let me go back here. pending.
170:17 - I'm just trying to make sure that it's not
stopping and starting, if there was a configuration
170:21 - issue, it would stop and start. But it also
would tell us the error somewhere in here
170:25 - might be in here or in the logs. But we're
not seeing issues yet. So I think we just
170:30 - have to wait a little while it's still warming
up. So what I'm going to do is just stop the
170:34 - video here. And when it is running, I'm going
to come back or if there's an error, we'll
170:37 - talk about it. Okay, and we're back here is
moved to activating state and says it's running,
170:44 - I was looking up to see what would happen
if it was in a pending state, it could say
170:48 - that the Docker daemon is unresponsive, the
Docker image is large, the ECS container agent
170:52 - lost connectivity, these this greater agent
takes a long time to stop the existing task.
170:57 - But we're actually not having any problems,
it was just me thinking that there was more
171:01 - of a problem there than there actually was.
Because if we go here and copy this IP address,
171:07 - and then do AD AD, it works. So what if we
wanted this not to be at ages Port 80, which
171:12 - would be nothing on here on the end, what
we would need to do is either we would need
171:17 - to start like in our very variable, set it
so that it's Port 80. And, but then we might
171:24 - have to set sudo, it's a big pain to get Port
80 running for web apps. And so I just didn't
171:30 - want to go through that hassle. Another thing
you could do is you could have a, you can
171:34 - run multiple containers, and one container
could be nginx. And nginx could be used as
171:39 - a proxy, where you change Port 8080 to Port
80. So that's something that you could use
171:45 - an nginx container for. And that'd be a great
strategy, if you really have a bunch of containers,
171:53 - and then you want to then have more flexibility
on it. They're mapping. But I just want to
171:58 - show you how the like that it is hard coded
to Port 80 there. So we go back to that task
172:04 - definition, I'm going to go into here, study,
think f f one, we'll go to the JSON. And you
172:10 - can see here it says AD AD. All right, and
we didn't set that this is what it is hard
172:14 - coded into here. But we're done with this
service, what we'll do is we'll go ahead and
172:20 - delete it. And we will type and delete me,
I we don't have to do this because we never
172:25 - set up any service discovery with it. That
would be for like app mesh, which I'm not
172:29 - really familiar with. Maybe that would show
up in the interest advanced networking certification.
172:35 - And we will hit delete, and that is fargate.
Now we can go ahead and delete this cluster,
172:40 - it's not going to do anything because it's
not raining easy to instance, costs money,
172:45 - but let's just delete it because it's a good
habit to do so. And there you go. So that
172:55 - should be deleted now. It's still showing
up. Deleting Okay, sometimes you got to do
173:09 - it twice. Anyway, um, that's the fargate follow
along and we're all done. Okay, so we're on
173:19 - to the ECS and fargate cheat sheet, I group
these ones together because fargate is under
173:23 - the ECS console. And there's not a lot you
need to know about these two. So let's get
173:28 - to it. So elastic Container Service is a fully
managed container orchestration service highly
173:32 - secure, reliable, scalable, and way to run
containers. The components of VCs is as follows
173:37 - cluster multiple, you see two instances that
will host the Docker containers, a task definition
173:42 - file, a JSON file that defines the Configure
configuration up to 10 containers that you
173:46 - want to run task. Launch containers defined
in task definition tasks do not, do not remain
173:53 - running once workloads complete service ensure
tasks remain running. So I think of a web
173:58 - app last to you 100% need to know container
agent binary on each EC two instance, which
174:04 - monitors starts stops tasks. And we'll talk
about ECR for a second here, I guess it's
174:09 - part of this cheat sheet as well. It is a
fully managed Docker container registry that
174:12 - makes it easy for developers to store manage
and deploy Docker container images. So think
174:17 - of it like the Git for Docker containers.
And then we'll move on to fargate, which is
174:23 - the last thing here. So fargate is a sort
of serverless containers. You don't worry
174:27 - about servers, run containers and pay based
on duration and consumption. When you create
174:32 - a fargate cluster. It's just like making ECS
cluster except it's empty. There's no servers,
174:37 - so it's really easy to set up. fargate has
cold start. So if this is an issue, then you're
174:43 - want to consider using ECS which does not
have cold start. The duration is as long as
174:48 - you want. So you can run this forever. This
is important to note versus lambdas, which
174:53 - have a hard limit in terms of how long they
can run. Memory can go up to 30 gigabytes,
174:58 - which is pretty great. And the price pay at
least one minute and every additional second
175:03 - from then on. I didn't mention vcpus. I don't
think it really matters. But there's a limit
175:09 - to how many v CPUs you can set on it. But
it's definitely a lot more computing than
175:13 - lambda will give you. But yeah, there you
go, that is ECS fargate, I guess ecrs. Well,
175:22 - hey, this is Andrew Brown from exam Pro. And
we are looking at ABS X ray, which helps developers
175:28 - analyze and debug applications utilizing micro
service architecture. And also another way
175:33 - to think of it as x ray is a distributed tracing
system or a performance monitoring system.
175:39 - So to understand X ray, we're gonna have to
understand what micro services are. So micro
175:44 - services is an architectural or organizational
approach to software development, where software
175:49 - is composed of small, independent services
that can communicate over well defined API's.
175:55 - So these are services owned by small self
contained teams, microservice architecture
176:00 - makes apps easier to scale and faster develop
enabling innovation accelerating time to market
176:05 - for new features. And to really put this in
perspective, if you're using AWS, and you're
176:09 - using a host of different services, you might
already be using micro service architecture.
176:14 - So the idea is that you have all these isolette
services. So as you have your source storage,
176:20 - you have instead of using large GC tools to
handle all the functionality of your application,
176:25 - you break it up into containers, or serverless
functions. And then you have your databases,
176:31 - notifications, queuing and streaming. So in
the combination of all these services being
176:36 - used, utilized together is microservice architecture.
But the question is, is how do we keep track
176:42 - of or debug the communication between all
these services, because if you're using a
176:47 - lot of them, it can get a bit confusing. And
that's kind of what X ray solves. So what
176:53 - exactly is X ray? Well, it is a distributed
tracing system. So let's talk about what that
176:59 - means. So distributed tracing, also known
as distributed request tracing, is a method
177:04 - used to profile and monitor apps, especially
those built using a microservice architecture.
177:09 - Distributed tracing helps pinpoint where failures
occur, and what causes poor performance. Notice,
177:15 - at the end, they are where failures occur,
and what causes poor performance. So those
177:20 - are, that's the major thing that X ray is
doing. Now, performance monitoring, which
177:26 - is its own thing is also kind of the scope
of X ray. So traditionally, we used to have
177:32 - application performance monitors, I mean,
we still have them, but they refer traditional
177:36 - architecture where you had a single EC two.
And all of your business logic or all the
177:42 - tasks that your application was handled was
in one specific application. But let's talk
177:49 - about what performance monitoring is because
X ray basically falls under that category.
177:54 - So monitoring and management of performance
and availability of software apps APM. The
177:58 - A's for application strives to detect and
diagnose complex application performance problems
178:03 - to maintain an expected level of service.
So you could say that x ray is both of these.
178:09 - But you know, I would lean towards saying
it's a distributed tracing system. So to try
178:14 - to help you understand X ray, let's just talk
about some third party services that are kind
178:19 - of similar, where it's cloud monitoring or
application performance monitoring, I do feel
178:23 - that these services are starting to tack on,
like distributed tracing to their services.
178:28 - So some of these might not really match X
ray, but I'm sure they will catch up in time.
178:34 - The number one service I can think of is data
dog, which does a lot of stuff, it does APN,
178:39 - log monitoring, and other sorts like that,
then you have New Relic was which was traditionally
178:43 - just application monitoring for large applications.
But now we can do considerable more signal
178:48 - FX, which is supposed to be like data dog,
but is more in real time. And then you have
178:53 - lamego, which is an I don't know if I'm saying
it right. But that's that's all I'm saying
178:57 - lamego. And it is focused on serverless monitoring.
So it looks a lot like x ray. But it has a
179:04 - huge emphasis on just serverless services.
So you know, hopefully, if you go look up
179:09 - those and take a peek around there, it'll
give you a better perspective on how X ray
179:14 - fits in the market of these monitoring services.
So x ray is a distributed tracing system.
179:20 - And it collects data about requests dates
that your application serves. It can view
179:24 - filter collected data to identify issues and
avenues for optimization. For any trace request
179:31 - to your application. You can see detailed
information not only about the request and
179:35 - response, but also about the calls that your
app makes to downstream ated with resources,
179:40 - microservices, databases, HTTP web API's,
and I'm certain that there are more things
179:44 - in there but just to visualize that, here,
we can see that we have a trace and we have
179:51 - dynamodb. And it goes and it shows you all
the steps within the table that it's taking.
179:57 - So you know you can get some very detailed
information out of this. But let's move on
180:01 - and start breaking up. What are the components
to X ray? Now we're going to take a look at
180:10 - the anatomy of X ray. So this basically, this
is the overall landscape of how it works.
180:16 - So the first thing is we have the X ray console.
And this is where we are going to be able
180:20 - to visualize monitor, or a drill down into
our tracing data. That data is going to be
180:27 - coming from the X ray API. Now you think that
you would just directly send your request
180:32 - to this API, but that's not the case, you
send them to the X ray daemon. And this is
180:37 - used as a buffer, because you're going to
be sending a lot of requests, the data is
180:41 - coming from the X ray SDK. And so here we
have Ruby, Python and node, there's a lot
180:47 - more than just the ones there. We'll talk
about that later. And then we have the SDK
180:52 - and see ally. And this could as well be sending
the segment data to X ray Damon, or it can
180:59 - be directly interacting interacting with the
X ray API. Let's talk about the X ray SDK,
181:05 - because this is where all our segment data
is coming from. And that is what we're visualizing
181:09 - in the X ray console. So the X ray SDK provides
a way to intercept incoming HTTP requests.
181:16 - So that means request information around that
kind of stuff, then it has something called
181:22 - client handlers. And this is just really the,
the SDKs that are specific per language. So
181:28 - when it says SDK client, it we're talking
just about, like the Ruby client, or the Python
181:32 - Client, etc. And we're able to set up instrumenting,
I will talk about that next slide, because
181:38 - instrument is kind of a vague term, that'll
make more sense, soon enough. And then it
181:43 - also has an HTTP client. So we can use this
to instrument calls to internal external HP
181:49 - web services. And also just to deliver our
our information actually, to the X ray daemon.
181:55 - So that's another component of it. And I want
to point out that the SDK does support instrumenting
182:00 - calls for the for SQL databases. And for other
features, the one that's worth highlighting
182:07 - SQL is SQL databases, because it really lets
you drill down to see what's happening with
182:11 - your database calls for that. But anyway,
let's move on to what instrumenting is. So
182:22 - I mentioned the term instrumenting, or instrument
in the last slide, I figured that was worth
182:28 - exploring more, because even myself, I wasn't
sure what the term meant. So instrumenting
182:33 - is the ability to monitor or measure the level
of a product's performance to diagnose errors
182:37 - and to write trace information. So to me,
it just sounds like you're logging information.
182:41 - That's exactly what it is. So good. To get
an idea of what it looks like to instrument
182:46 - something in X ray, is, we have this piece
of code here. And this is for a Node JS, express
182:52 - JS application. And what we're doing is we're
including the X ray SDK, and then what we
182:58 - do is we open up a segment segments and sub
segments are the pieces of information that
183:04 - we want to capture, or to send X ray and that
is instrumenting. So we have a part where
183:10 - we open and then we have a part that we close.
And everything that is in between there is
183:15 - going to get captured, like the duration of
it or additional information. And that's what's
183:20 - gonna get passed along. Generally, in the
code, you don't have to necessarily set the
183:25 - segments the segments get captured. So using
your code, you're setting up sub segments.
183:31 - But yeah, just to give you a visualization.
So just remember that when you hear instrument,
183:35 - just think of it like logging information
and sending it to X ray. Let's take a closer
183:45 - look here at the X ray daemon. And I said
earlier, when we're looking at the anatomy
183:49 - that we do not send our segment data directly
to the kms API, what we do is we send it to
183:56 - the daemon and then the daemon buffers it
and then sends it off to the kms API. So let's
184:02 - look at that in more detail. So instead of
sending your traces directly to X ray, the
184:07 - SDK sends Jason segment documents. So that's
what the segments are made of JSON files to
184:12 - the daemon processes and listening on UDP
traffic. So here's our SDKs and other clients.
184:18 - And here it is sending that JSON document
to the X ray daemon, right. And then the X
184:23 - ray daemon buffers segments into a queue and
uploads them to X ray in batches. So the idea
184:28 - is that it's creating this buffer. This makes
sense, because if you are sending instrument,
184:34 - if you're sending logging or instrument data,
you're gonna have a lot of requests. And so
184:39 - you don't want the API to be flooded there.
So this is acts as a buffer. And then there
184:45 - it is sending the batch information to X ray
API so that it's not so burdensome. You can
184:52 - install the daemon on Windows, Linux and Mac
and it's already included on Elastic Beanstalk
184:57 - and lambda platforms. So I think when you're
using it like when you're using the serverless,
185:02 - like, you're setting up a serverless application
and you turn on X ray, there's already a daemon
185:09 - running there. I don't know where it is, but
I know it's working. It looks like you've
185:13 - also just set one up locally for development.
And x ray uses trace data from the universe
185:18 - resources that power your cloud applications
to generate a detailed service graph. So let's
185:22 - just say all this segment data turns into
that graph. So, you know, hopefully, that
185:28 - makes clear the utility of the X ray data.
Now we're going to cover all the X ray concepts.
185:38 - So the first thing that we really need to
understand are segments because that is the
185:43 - core of X ray. That's the data that X ray
receives. And x ray can group segments and
185:50 - have common requests, which would be called
traces. Okay. And then X ray processes those
185:57 - traces to generate a graph, and that provides
a visual representation for your app. So that's
186:03 - the general idea. But there are a lot of components
to it. So let's go through the list really
186:08 - quickly here. So we have segments, we have
sub segments, we have service graph, we have
186:13 - traces, we have sampling, we have the tracing
header, we have filter expressions, we have
186:18 - groups, we have annotation metadata, and then
we have errors, faults and exceptions, mostly
186:23 - just exceptions. So we're going to walk through
every single one of these components, and
186:30 - then we'll really understand how X ray works.
So let's first look at the service graph.
186:39 - And this is something that's visible when
you click into a trace. And so the service
186:44 - graph shows the client your front end services
and back end services. And then might not
186:49 - be very clear looking at this, which is the
client front end and back end. So I'm going
186:53 - to help divide this up so that we can make
sense of that. So the first one is the client.
186:58 - It's just the little person in terms of representation.
So nothing super exciting there. But then
187:04 - we still have a bunch of these other services
are running. So then we have your front end
187:09 - services. And we can divide that here. And
what we see running here is like computing
187:16 - and application integration. So that could
be SNS, lambdas, ECS, you see two Sq s. So
187:24 - anything like that. And then the last thing
is backend services. This is generally your
187:29 - databases. So here, all these calls are related
to dynamodb. So you know, hopefully that makes
187:36 - that all clear there. And the service graph,
the whole purpose of having this is help you
187:42 - perform, like identify bottlenecks, latency
spikes, and other issues. So the idea is that
187:47 - you can click into any of these and drill
down and really figure out how to optimize
187:56 - your. So let's take a look at segments, which
is the most important thing to know about
188:01 - x ray. And so if you have a bunch of compute
resources that are running your application
188:06 - logic there, you're going to want to send
data about that to X ray. And we call that
188:12 - sending work. So we're sending our work off
to specific segments. So here is an actual
188:18 - segment that we could drill down into. And
there's a lot of different information here.
188:24 - So we have things like the host information.
So that'd be hostname, alias, or IP address,
188:29 - I call this this lambda function, if you can
see the origins as as lambda, I call it in
188:35 - the console. I didn't call through API gateway,
so it doesn't have any of that data here.
188:39 - So that's why we're not seeing it. Then you'd
have request information. So the method client
188:44 - address path user agent, again, I didn't use
API gateway. So we're not seeing that information
188:48 - there. But if if I had done so we'd see a
lot more information, then you'd have the
188:52 - response. So that's the status and content
here we have a 200. So that means everything's
188:57 - good. The work done. So the start and end
times, we have the duration and sub segments.
189:04 - And then the last thing would be issues that
occurred so errors, faults and exceptions.
189:08 - And we have a dedicated tab there for that.
So that's what it looks like to drill down
189:13 - into a segment. So let's take a look at sub
segments, which allows us to get more granular
189:23 - timing information or details about downstream
calls that your app makes. So these are basically
189:28 - operations within your app that you want to
capture. So a sub segment can contain calls
189:33 - to either services external HTTP API's, or
SQL, SQL calls within a database, and also
189:41 - things within dynamodb. And that's actually
an example we're going to look at is dynamodb.
189:45 - So here, what we have is we have an Elastic
Beanstalk environment and there's a put request
189:51 - going to it. And then underneath in that application,
it's making calls to Dynamo dB. So we can
189:57 - see get item, update item, etc. So those are
the sub segments. Now, you can actually define
190:04 - your own arbitrary sub segments to instrument
specific functions or lines of code. And that's
190:08 - how I'm gonna show you the next step. But
a lot of these things, you know, if you're
190:12 - using specific native services, they might
already log these sub segments for you. But
190:16 - you know, if you need something more specific,
you can write your own. So here, we have a
190:22 - trace. And within this trace, we have the
segment and then there's a sub segment, can
190:27 - you tell where the custom sub segment is?
It's right there. So that's the one I defined.
190:32 - And I want to show you how you do that in
code. So here is the actual code example.
190:37 - And the first thing you're going to want to
do is include the SDK. So there's the X ray
190:41 - SDK core, I'm writing this, obviously, no
Gs. And it's very similar to when we defined
190:46 - a segment earlier on. But in this case, we
first have to get the current segment. And
190:51 - then we're going to add a new sub segment.
So I call this mining. And then you have some
190:57 - code that runs and then you have is you close
the sub segment, so everything in between
191:02 - the certain end will be captured. And that's
going to go to that segment there. I'm just
191:06 - doing console log. So it's 0.0 milliseconds,
nothing super exciting there. You generally
191:12 - don't define, like you don't create segments
in code. I mean, the earlier example, we saw
191:17 - it, because that was expressed yes application.
But generally, the only thing you're defining
191:22 - in code for instrumentation is sub segments.
But you know, that's just this is just an
191:28 - example to show you how to set up sub sub.
Let's just take a look here at traces. So
191:37 - here I've drilled into a trace. And the way
you can think of a trace is just, it's a way
191:41 - of grouping all your segments together. And
they, they're all started from a single request,
191:47 - so that that first request is triggered. And
then the idea is we need to keep track of
191:53 - its path through all these segments and services.
So that's why we have a trace ID. And the
191:59 - idea is when that trace ID is generated for
the first service that it interacts with,
192:06 - it's going to generate that ID and that's
going to get passed along through all this
192:09 - stuff. And that's, that's what helps build
out this, that like graph down below that
192:14 - tells us all that information. So we're going
to talk about trace IDs a bit more. But we're
192:19 - gonna first talk more about sampling, because
not every trace that happens here is actually
192:26 - recorded. So that confuses me a lot. When
I first started using X ray, where I was triggering
192:31 - stuff, I was expecting to see every single
thing that I triggered, recorded, and that's
192:36 - not the case. So I was saying the last slide,
we were looking at traces that not every single
192:45 - request is actually passed on or captured.
And the reason why is when you're using the
192:51 - X ray SDK, it uses a sampling algorithm to
determine which request will actually get
192:55 - traced. So by default, the SDK is going to
record the record the first request for each
193:02 - second, and 5% of every additional requests.
So why would we have this why won't be the
193:10 - purpose of sampling. And the reason is to
avoid incurring service charges. So when you're
193:15 - first getting started, you want the default
sampling rate to be low. Just because you're
193:21 - not maybe not definitely X ray, and you don't
realize how many traces or requests you're
193:26 - sending. And you might not be able to make
use of that. So you can actually see the options
193:31 - that are set up for this particular sampling
algorithm. And you can actually modify the
193:38 - sampling rule and add it add additional rule.
So if you were over here, you can go hit Create
193:44 - sampling rule and set a bunch of options.
So you can set it to match up on the service
193:50 - name, the service type, and a bunch of other
things. And down below, you can see it says
193:54 - limit to one requests per second, and then
five seconds per fixed rate. So the whole
194:01 - purpose of sampling is just to help you save
money, because it's gonna help you reduce
194:05 - the amount of traces for high volume and unimportant
requests, you're just going to find that there
194:09 - could be like a component within your application,
where, let's say does polling, so it's constantly
194:15 - pulling like frequently, but you don't need
to capture the information because it doesn't
194:20 - provide you any valuable information. Now
we're gonna take a look at the trace header
194:29 - itself. So all requests are traced up to a
configurable minimum, and after reaching that
194:34 - minimum, a percentage of the requests are
traced to avoid unnecessary costs. Okay, the
194:42 - sampling decision and the trace ID are added
to the HTTP requests in in tracing headers
194:46 - named x, Amazon trace ID. So we were talking
about the trace Id get set, but this is just
194:52 - what it looks like. So the first X ray integrated
service that the request hits is when the
194:59 - tracing header gets out. So here, this is
what it looks like. It has a root, and it
195:04 - has a sampled number there. And then the next
thing is, if you have a tracer header, where
195:13 - it's originating from an actual instrument
and application, it could actually have this
195:17 - parent thing here as well, you don't really
need to remember this stuff for the exam,
195:21 - I'm just showing it to you. But just understand
that the trace editor determines whether this
195:28 - will show up in the in the in the trace or
not, it's going to be based off that sample
195:33 - number that is assigned to it. So even with
sampling, a complex application still generates
195:44 - a lot of data. So sampling isn't going to
remove enough so that it's very easy for you
195:48 - to make sense of stuff. So that's where filter
expressions come in. And this helps you narrow
195:52 - down to specific paths, or users. So here's
an example. And this is we're looking at all
196:00 - traces. So there's a list of traces. And in
that Filter Expression, you can put in a thing
196:06 - in there. And that's going to filter out what
traces you want to look at. If you're wondering
196:10 - like what the actual syntax or filter expressions
is, you just click that question mark there.
196:15 - And there's a lot of information under there.
So these are a lot of attributes that you
196:19 - can use. Here, you can see that we were filtering
out based off the request URL, but there's
196:23 - a lot of stuff here. And also that you can
group these results together. So they have
196:30 - a bunch of predefined groups, for you to just
make sense of stuff a lot easier. You can
196:35 - see in the graphic there by default, it is
for URL. So let's talk about groups we just
196:47 - saw a group by but you can also assign a filter
expression to a group that you create. And
196:55 - the way it works is you're going to name that
group. And it could be just a name, or you
196:59 - can use an Arn. And the advantage of making
your groups is that it's going to generate
197:03 - on service graph, create summaries and cloudwatch
metrics. And this is gonna save you a lot
197:08 - of time when investigating very common scenarios.
So the idea is right beside where the Filter
197:15 - Expression is, you can drop down and hit Create
group. And from there, you're going to name
197:19 - that group and again, you can put an Arn in
there. And then you just provide your filter
197:22 - expression. Notice that you can only create
up to 25 groups by default. But I guess you
197:26 - can use a service limit to increase beyond
that. If you're going beyond 25, you must
197:31 - have a very complex application. But I do
want to warn you about these groups, because
197:37 - let's and this is just the the nature of it.
So when you create a group and you set that
197:41 - Filter Expression, and let's say down the
road, you know, like a week later, you want
197:47 - to adjust that Filter Expression doesn't,
it's not going to retroactively apply that
197:52 - to all the previous data, it's only going
to apply this regular expression for or this
197:58 - filter expression for future data. So that's
an issue where the data is doesn't exactly
198:05 - represent what you expect it to look like,
because you have this old expression and new
198:10 - expression. So what you can do is if it really
matters to you that the expression only represents
198:19 - data that is exactly based on that Filter
Expression, you're just gonna have to delete
198:23 - the current group and make a new one. So the
X ray, you can add annotations and metadata
198:32 - to your segment documents. So that's the JSON
files that get sent to X ray. That is generally
198:36 - information you're doing there. And you can
aggregate this at the trace level. And it
198:41 - can be added to both the segments and a sub
segments. It's not just limited to top level
198:46 - segments. And so let's talk about annotations.
So this is just a key value pair that that
198:51 - is indexed for use with filter expressions.
And you can have up to 50 annotations per
198:55 - trace. Now for metadata, you have a key value
pair that are not indexed, and the values
199:01 - can be of any type, including objects and
lists, so they sound really similar, they
199:05 - both key pair values, the only difference
is that annotation is indexed. And metadata
199:10 - is not. So just to better understand the use
case, you want to use annotations to record
199:15 - data that you want to use in group traces
in the console or when you're calling the
199:19 - get trace summary. So it's really for group
traces. And then you want use metadata to
199:25 - record data when you want to store in the
trace, but you don't need to use it for searching
199:29 - traces. So for annotations, it's help, it's
for you to help find data. And for metadata.
199:34 - It's just to have additional data there when
you need it. And you can view the annotation
199:38 - and metadata in the segment sub segment details
X ray console. So if you just click into a
199:43 - segment, you'll see annotations and met data
there. So now let's just point out errors,
199:52 - faults and exceptions. So when an exception
occurs, while your app is serving instrument
199:57 - requests, the X ray SDK records exception
details and the stack Trace if available,
200:00 - these are the type of errors you could encounter.
So we have error fault and throttle errors
200:05 - are client errors. So they're gonna be 400
fault is server errors, it's gonna be 500.
200:10 - Throttle is throttling errors, it's just too
many requests. And if you're looking for this
200:15 - data, it's going to show up under the exceptions
tab, tab for the segment details. And it also
200:20 - shows up in the actual segment graph there.
So we're going to trace and you see all the
200:26 - segments and their their duration. Sometimes
it will have like a little exclamation or
200:31 - it'll color it differently. So you know, there's
an error at some part during the path of the
200:39 - trace. Now let's take a look at what services
integrate in with X ray. And the most important
200:46 - we've highlighted in yellow because this is
the Give me the most common use cases. So
200:50 - we have lambda API gateway app mesh cloud
trail cloud, watch, Ava's config Elastic Beanstalk.
200:55 - I'm not exactly sure how that works, but Elastic
Beanstalk seems to want integrate with everything
201:01 - nowadays. Then you have EBS you have SNS Sq
s EC to elastic Container Service fargate.
201:08 - I wouldn't be surprised if it also supports
elastic Kubernetes service. But yeah, that
201:14 - is the run of it. But the ones that are highlighted
are the ones you most likely need to know.
201:19 - And probably ECS as well there, even though
I didn't highlight. For the exam, we're going
201:27 - to need to know what languages does X ray
support and supports go Java node, Python,
201:33 - Ruby, ASP. net and PHP. So it's all the culprits
here, the only thing it doesn't support is
201:39 - PowerShell, which there's no reason to support
that. But yeah, this is generally what AWS
201:44 - would support for most services for languages.
So it has the whole gambit here. Hey, this
201:54 - is Andrew Brown from exam Pro, and we've made
it to the X ray cheat sheet. And this one
201:58 - is a little bit long, but x ray is very important
to know. So we're going to be very thorough
202:02 - here in our review. So let's get to it. So
x ray helps developers analyze and debug applications
202:09 - that utilize a micro service architecture.
Remember that it's really, really keen for
202:13 - micro service architectures to be using lambdas.
Or, or you're using containers. X ray is really
202:19 - good for that. X ray is a distributed tracing
system. It is a method used to prove to profile
202:25 - and monitor apps, especially those built using
a micro service architecture, to pinpoint
202:30 - where failures occur and what causes poor
performance. The X ray daemon is a software
202:35 - application that listens for traffic on UDP,
UDP port 2000, gathers raw segment data and
202:41 - relays it to the database X ray API data is
generally not sent directly to the X ray API
202:46 - and passes through the X ray daemon, which
uploads in bulk and that's just to help create
202:51 - a buffer between the x, the X ray API, and
the actual data being sent. Will the damond
202:58 - show up on the actual exam? Probably not.
But we should really know all the components
203:02 - of X ray segments provide the resources name
details about the request and details about
203:07 - the work done. Then you have sub segments,
which provides more granular timing information
203:11 - and details about downstream calls of your
app made to fulfill the original request,
203:15 - then you have your service graph, that graph
that is that flow chart visualization for
203:19 - average response of microservices individually
pinpoint failure, then you have traces. This
203:24 - collects all segments generated by similar
requests so that you can track the path of
203:27 - request through multiple services. Then you
have sampling, which is an algorithm that
203:32 - decides which requests should be traced by
default X ray SDK records the first request
203:37 - each second, and 5% of additional requests.
So if you have a question on the exam, and
203:43 - they're asking, like, why don't you see this
information, just think about sampling, tracing
203:47 - headers, the name x, Amazon trace ID and identifies
a trace with which pass along downstream services.
203:53 - What's important to remember here is x Amazon
trace ID, you might see a question where they
203:59 - show multiple, like, like to pick the right
one. So remembering that is key. Filter expressions
204:03 - allow you to narrow down specific paths or
users groups allows allow you to save filter
204:09 - expressions, so you can quickly filter traces.
And then on to the second page, we're almost
204:13 - done here. annotation metadata allows you
to capture additional additional information
204:17 - and key value pairs. So for annotations, they
are index, for use with filter expressions
204:23 - with a limit of 50. Metadata or not index
use metadata to record data you want to store
204:28 - in the trace, but don't need to use for searching
traces. So if you need to search for traces,
204:33 - and need that metadata you're going to be
used or that additional information, you're
204:36 - going to be using annotations. errors are
400 faults or 500. Throttle is four to nine
204:43 - too many requests. You probably want to just
know the last one there. X ray supports the
204:46 - following languages. I don't feel like they'd
asked you a question but they used to, but
204:50 - I think the exam is getting a little bit harder.
So they're not they're not just asking you
204:54 - to like Choose the language that is not applicable
anymore. But X ray supports basically all
204:58 - possible language, the data Lewis Howes on
there, as long as they're not telling you,
205:02 - you're putting Perl on the list, it should
be pretty easy to figure that out. X ray supports
205:07 - a ton of service integrations with the following
lambda API gateway app mesh cloud trail cloud
205:11 - watch, Eva's config, Eb lb, MSS Sq s, easy
to ECS and fargate. So there you go, that
205:18 - is x ray. And we are done here. Hey, this
is Andrew Brown from exam Pro. And we are
205:27 - looking at Amazon certificate manager also
known as ACM, which is used to provision manage
205:32 - and deploy public and private SSL certificates
for use within your AWS services. So let's
205:38 - look at ACM a little bit more in detail. It
handles the complexity of creating and managing
205:42 - public SSL certificates for your AWS based
websites and applications. It handles two
205:48 - types of certificates we have public, those
are ones that are provided by AWS, and they
205:54 - are free. And then you have private. So these
are certificates that you import, and they
205:59 - cost $400 per month, you generally just want
to use a public certificate, if you ever use
206:04 - Let's Encrypt, those are all public. So if
you're comfortable with that you're comfortable
206:08 - with these ones, just make sure that when
you're creating that certificate, you do not
206:11 - make the wrong choice. I myself have chosen
incorrectly, and but luckily I reached out
206:18 - to to support and before they charged me they
fixed that issue. But that is a tricky one
206:22 - to get your money back on if you make that
mistake. So ACM can handle multiple subdomains
206:28 - and wildcard domains. So you can see them
entering exam Pro is the naked domain. And
206:32 - then I have a wildcard one. That is the setup,
I always recommend that you use because otherwise,
206:37 - you'd have to create a bunch of domains, or
sorry certificates later. And that's kind
206:42 - of a pain. And ACM is attached to very specific
Eva services or resources. So you can attach
206:48 - it to elastic load balancer CloudFront API
gateway, and you can apparently use it with
206:52 - Elastic Beanstalk. But I'm imagining that
is through the ELD. So those are the three
206:56 - services you need to know that ACM attaches
to so remember that, there you go. So we're
207:06 - gonna look at a couple of ACM examples. And
the point of this is to understand SSL termination.
207:11 - So the first one here we have is we're using
ACM, and we're attaching that certificate
207:18 - to our load balancer, which is application
load bouncer. And if you see that line, the
207:24 - idea is that this red line represents the
traffic that is encrypted. And so once it
207:30 - hits the alrb, the certificate is going to
decrypt that, that traffic, and then everything
207:35 - between the alrb to the CPU instance, is now
unencrypted. And that's totally fine, because
207:41 - it's within your network. So it's still secure.
But you know, for someone to take advantage
207:48 - of that, they'd have to break into your AWS
account, and they'd have to be able to intercept
207:54 - that traffic. So it's a very low risk. But
but at the ACM can only really be attached
208:00 - to lots of little bits, or CloudFront, or
API gateway. So it's not easy to protect this
208:09 - traffic here. But the advantage of attaching
your certificates at this level here is that
208:14 - you can add as many easy to instances as you
want, you don't have to configure each one
208:18 - of them to be able to handle a certificate.
So that makes it a lot easier to manage certificates.
208:26 - Now, the other case is terming SSL end to
end. And this is where the traffic from the
208:33 - start to the finish is encrypted. So even
within your network, it's going to be encrypted.
208:37 - And the way you do that. I don't know how
to do with ACM, I don't even think you can
208:42 - deal with ACM, ACM, because I've only noted
being able to attach to resources over here.
208:47 - But you, I guess you could use Let's Encrypt.
And so you'd have to set that up on every
208:51 - single server, and then rotate them out. And
that's kind of a bit of a hassle to maintain.
208:58 - But a lot of people are used to doing that.
And this is if you need end to end encryption,
209:02 - this is going to be dependent on your compliancy.
So if you're a large corporation, maybe you
209:07 - have like a rule that says you have to encrypt
end to end. But I have for 99% of other use
209:14 - cases, this is more ideal terminating SSL
at the load balancer. So there you go. Hey,
209:22 - this is Angie brown from exam Pro, and we
are looking at remedy three, which is a highly
209:26 - available and scalable domain name service.
So whenever you think about revenue three,
209:32 - the easiest way to remember what it does is
think of GoDaddy or Namecheap, which are both
209:36 - DNS providers. But the difference is that
revenue three has more synergies with AWS
209:42 - services, so you have a lot more rich functionality
that you could do on on AWS than you could
209:46 - with one of these other DNS providers. So
what can you do with revenue three, you can
209:51 - register and manage domains, you can create
various record sets on a domain, you can implement
209:56 - complex traffic flows such as bluegreen, deploys,
or failing You can continuously monitor records
210:02 - via health checks and resolve epcs outside
of AWS. So here I have a use case. And this
210:13 - is actually how we use it at exam Pro is that
we have our domain name, you can purchase
210:19 - it, or you can have revenue three managed
the name servers, which allow you to then
210:25 - set your record sets within route 53. And
so here we have a bunch of different record
210:30 - sets for sub domains. And we want those sub
domains to point to different resources on
210:36 - AWS. So for our app, our app runs behind elastic
load balancer, if we need to work on an ami
210:42 - image, we could launch a single EC two instance
and point that subdomain there for our API,
210:46 - if it was powered by API gateway, we use that
subdomain for that for our static website
210:51 - hosting, we would probably want to point to
CloudFront. So the WW dot points to CloudFront
210:55 - distribution. And for fun and for learning,
we might run a minecraft server on a very
210:59 - specific IP, probably would be elastic IP
because we wouldn't want it to change. And
211:04 - that could be Minecraft exam pro Co. So there's
a basic example. But we're going to jump into
211:09 - all the different complex rules that we can
do in revenue three here. So in the previous
211:20 - use case, we saw a bunch of sub domains, which
were pointing to AWS resources, well, how
211:25 - do we create that link so that a revenue three
will point to those resources, and that is
211:30 - by creating record sets. So here, I just have
the form for record sets. So you can see the
211:36 - kind of the types of records that you can
create. But it's very simple, you just fill
211:40 - in your sub domain, or even leave the naked
domain, and then you choose the type. And
211:43 - in the case for a this is allows you to point
this sub domain to a specific IP address,
211:49 - you just fill it in, that's all there is to
it. Okay, now, I do need to make note of this
211:54 - alias option here, which is a special option
created by AWS. So here in the next slide
212:00 - here, we've set alias to true. And what it
allows us to do is directly select specific
212:07 - AWS resources. So we could select CloudFront,
Elastic Beanstalk, EOB, s3 VPC API gateway.
212:15 - And why would you want to do this over making
a traditional type record? Well, the idea
212:21 - here is that this alias has the ability to
detect changes of IP addresses. So it continuously
212:27 - keeps pointing that endpoint to the correct
resource. Okay. So that's normally when if,
212:34 - if and whenever you can use alias always use
alias because it just makes it easier to manage
212:39 - the connections between resources via roughly
three record sets. And the limitations are
212:45 - listed here as follows. The major advantage
of Route 53 is it's seven types of routing
212:58 - policies. And we're going to go through every
single one here. So we understand the use
213:01 - case, for all seven, before we get into that
a really good way to visualize how to work
213:08 - with these different routing policies is through
traffic flow. And so traffic flow is a visual
213:12 - editor that lets you create sophisticated
routing configurations within route 53. Another
213:18 - advantage of traffic flow is that we conversion,
these policy routes, so if you created of
213:23 - complex routing policy and you wanted to change
it tomorrow, you could save it as version
213:27 - one, version two, and roll, roll this one
out or roll back to that. And just to play
213:31 - around traffic flow, it does cost $2 per policy
record. So this whole thing is one policy
213:36 - record. But they don't charge you until you
create it. So if you do want to play around
213:41 - with it, just just create a new traffic flow,
and name it and it will get you'll get to
213:46 - this visual editor. And it's not until you
save this. So you can play around with this
213:50 - to get an idea of like all the different routing
rules and how you can come up with creative
213:54 - solutions. But now that we've covered traffic
flow, and we know that there are seven routing
213:58 - rules, let's go deep and look at what we can
do. So we're gonna look at our first routing
214:08 - policy, which is the simple routing policy.
And it's also the default routing policies.
214:13 - So when you create a record set, and here
I have one called random, and we're on the
214:17 - a type here, down below, you're gonna see
that routing policy box that's always by default
214:23 - set to simple Okay, so what can we do with
simple The idea is that you have one record,
214:29 - which is here, and you can provide either
a single IP address or multiple IP addresses.
214:36 - And if it's just a single, that just means
that random is going to go to that first IP
214:40 - address every single time. But if you have
multiples, it's going to pick one at or at
214:44 - random. So it's good way to make like a if
you wanted some kind of random thing made
214:50 - for a to b testing, you could do this and
that is as simple as it is. So there you go.
215:00 - So now we're looking at weighted routing policies.
And so what a weighting routing policy lets
215:05 - you do is allows you to split up traffic based
on different weights assigned. Okay, so down
215:10 - below, we have app.example.co. And we would
create two record sets in roughly three, and
215:17 - they'd be the exact same thing, we both say
after example car, but we'd set them both
215:20 - to waited, and we give them two different
weights. So for this one, we would name it
215:24 - stable. So we've named that one stable, give
it 85%. And then we make a new record set
215:29 - with the exact same sub domain, and set this
one to 15%. And call experiment, okay. And
215:35 - the idea is that when ever traffic, any traffic
hits app.example.co, it's going to look at
215:40 - the two way two values, a 5% is going to go
to the stable one. And for the 15%, it's going
215:45 - to go to the experimental one. And a good
use case for that is to test a small amount
215:50 - of traffic to minimize impact when you're
testing out new experimental features. So
215:54 - that's a very good use case for a weighted
routing. So now we're going to take a look
216:03 - at latency based routing. Okay, so layer based
routing allows you to direct traffic based
216:09 - on the lowest network latency possible for
your end user based on a region. Okay, so
216:14 - the idea is, let's say people want to hit
app dot exam pro.co. And they're coming from
216:20 - Toronto. Alright, so coming from Toronto.
And the idea is that we have, we've created
216:24 - two records, which have latency with this
sub domain, and one is set to us West. So
216:31 - that's on the west coast. And then we have
one central Canada, I believe that's located
216:36 - in Montreal. And so the idea is that it's
going to look here and say, Okay, which one
216:40 - produces the least amount of latency, it doesn't
necessarily mean that it has to be the closest
216:44 - one geographically, just whoever has the lowest
return in milliseconds is the one that it's
216:50 - going to route traffic to. And so in this
case, it's 12 milliseconds. And logically,
216:55 - things that are closer by should be faster,
and so that, so it's going to route it to
217:00 - this a lb, as opposed to that one. So that's,
that's how latency based routing works. So
217:08 - now we're looking at another routing policy,
this one is for failover. So failover allows
217:15 - you to create an Active Passive setup in situations
where you want a primary site in one location,
217:20 - and a secondary data recovery site and another
one. Okay, another thing to note is that revenue
217:26 - three automatically monitors via health checks
from your primary site to determine if that
217:31 - that endpoint is healthy. If it determines
that it's in a failed state, then all the
217:36 - traffic will be automatically redirected to
that secondary location. So here we have down
217:42 - below an example. So we have apt out exam
pro CO, and we have a primary location and
217:48 - a secondary one. Alright. And so the idea
is that roughly three, it's going to check.
217:55 - And if it determines that this one is unhealthy
based on a health check, it's going to then
218:00 - reroute the traffic to our secondary location.
So you'd have to create you know, two routing
218:04 - policies with the exact same. The exact same
domain, you just said which one is the primary
218:11 - and which one is the secondary, and it's that
simple. So here, we are looking at the geolocation
218:20 - routing policy. And it allows you to direct
traffic based on the geolocation, geographical
218:26 - location of where the request is originating
from. So down below, we have a request from
218:31 - the US hitting app dot exam pro.co. And we
have a a record set for a geolocation that's
218:38 - set for North America. So since the US is
in North America, it's going to go to this
218:44 - record set. Alright. And that's as simple
as that. So we're going to look at geo proximity
218:54 - routing policy, which is probably the most
complex routing policy is a bit confusing,
218:59 - because it sounds a lot like geolocation,
but it's not. And we'll see shortly here,
219:04 - you cannot create this using record sets,
you have to use traffic flow, because it is
219:09 - a lot more complicated, and you need to visually
see what you're doing. And so it's gonna be
219:14 - crystal clear, we're just going to go through
here and look at what it does. So the idea
219:17 - is that you are choosing a region. So you
can choose one of the existing Eva's regions,
219:24 - or you can give your own set of coordinates.
And the idea is that you're giving it a bias
219:28 - around this location, and then it's going
to draw boundaries. So the idea is that if
219:32 - we created a geo proximity routing for these
regions, this is what it would look like.
219:38 - But if we were to give this 120 5% more bias,
you're going to see that here it was a bit
219:43 - smaller, now it's a bit larger, but if we
minus it, it's going to reduce it. So this
219:46 - is the idea behind a geo proximity where you
have these boundaries, okay. Now, just to
219:52 - look at in more detail here, the idea is that
you can set as many regions or points as Do
220:00 - you want here and so here, I just have two
as an example. So I have China chosen over
220:05 - here. And this looks like we have Dublin chose.
So just an idea to show you a simple example.
220:11 - Here's a really complicated one here, I chose
every single region just so you have an idea
220:14 - of splits. So the idea is you can choose as
little or as many as you want. And then you
220:20 - can also give it custom coordinates. So here
I chose Hawaii. So I looked at the Hawaii
220:23 - coordinates, plugged it in, and then I turned
the bias down to 80%. So that it would have
220:28 - exactly around here, and I could have honed
it in more. So it just gives you a really
220:32 - clear picture of how geo proximity works.
And it really is boundary bays, and you have
220:37 - to use traffic flow for that. So the last
routing policy we're gonna look at is multivalue.
220:48 - And multivalue is exactly like simple routing
policy. The only difference is that it uses
220:55 - a health check. Okay, so the idea is that
if it picks one by random, it's going to check
220:59 - if it's healthy. And if it's not, it's just
going to pick another one by random. So that
221:03 - is the only difference between multivalue
and simple. So there you go. Another really
221:13 - powerful feature of Route 53 is the ability
to do health checks. Okay, so the idea is
221:19 - that you can go create a health check, and
I can say for app.exam.pro.co, it will check
221:23 - on a regular basis to see whether it is healthy
or not. And that's a good way to see at the
221:29 - DNS level if something's wrong with your instance,
or if you want to failover. So let's get into
221:34 - the details of here. So we can check health
every 30 seconds by default, and we can it
221:39 - can be reduced down to 10 seconds, okay, eye
health checking, initiate a failover. If status
221:44 - is returned, unhealthy, a cloudwatch alarm
can be created to alert you of status unhealthy,
221:50 - a health check can monitor other health checks
to create a chain of reactions, you can have
221:56 - up to 50 in a single AWS account. And the
pricing is pretty affordable. So it's 50 cents.
222:04 - So that's two quarters for per endpoint on
AWS. And there are some additional features,
222:09 - which is $1 per feature. Okay. So if you're
using route 53, you might wonder well, how
222:20 - do I route traffic to my on premise environment.
And that's where revenue three resolver comes
222:26 - into play, formerly known as dot two. resolver
is a regional service that lets you connect
222:31 - route DNS queries between your VBC and your
network. So it is a tool for hybrid environments
222:37 - on premises and cloud. And we have some options
here, if we just want to do inbound and outbound
222:41 - inbound only or outbound only. So that's all
you really need to know about it. And that's
222:46 - how you do hybrid networks. So now we're taking
a look at revenue three cheat sheet, and just
222:57 - we're going to summarize everything that we
have learned about revenue three, so revenue
223:00 - three as a DNS provider to register and manage
domains create record sets, think GoDaddy
223:05 - or namecheap. Okay, there's seven different
types of routing policies, starting with simple
223:09 - routing policy, which allows you to input
a single or multiple IP addresses to randomly
223:15 - choose an endpoint at random, then you have
weighted routing, which splits up traffic
223:19 - between different weights assigns of percentages
latency based routing, which is based off
223:23 - of routing traffic to the based on region
for the lowest possible latency for users.
223:28 - So it's not necessarily the closest geolocation
but the the lowest latency, okay, we have
223:33 - a failover routing, which uses a health check.
And you set a primary and a secondary, it's
223:37 - going to failover to the secondary if the
primary health check fails, you have geolocation
223:42 - which roads traffic based on the geolocation.
So this is based on geolocation would be like
223:49 - North America or Asia. Then you have geo proximity
routing, which can only be done in traffic
223:54 - flow allows you to set biases so you can set
basically like this map of boundaries based
224:01 - on the the different ones that you have, you
have multi value answer which is identical
224:06 - to simple, simple routing, the only difference
being that it uses a health check. In order
224:11 - to do that. We look at traffic flow, which
is a visual editor for changing routing policies,
224:16 - you conversion those record those policy records
for easy rollback, we have alias record, which
224:21 - is a debases Smart DNS record which detects
IP changes freedoms resources and adjusts
224:26 - them automatically always want to use alias
record, when you have the opportunity to do
224:30 - so you have route 53 resolver, which is a
hybrid solution. So you can connect your on
224:37 - premise and cloud so you can network between
them. And then you have health checks which
224:42 - can be created to monitor and and automatically
failover to another endpoint. And you can
224:48 - have health checks monitor other health checks
to create a chain of reactions for detecting
224:53 - issues for endpoints Hey, this is Andrew Brown
from exam Pro. And we are going to take a
225:04 - look here at AWS command line interface, also
known as ci, which control multiple services
225:10 - from the command line and automate them through
scripts. So COI lets you interact with AWS
225:17 - from anywhere by simply using a command line.
So down below here, I have a terminal, and
225:21 - I'm using the ADB COI, which starts with AWS.
So to get this installed on your computer,
225:27 - AWS has a script, a Python script that you
can use to install the COI. But once it's
225:33 - installed, you're going to now have the ability
to type AWS within your terminal followed
225:37 - by a bunch of different commands. And so the
things that you can perform from the CIA is
225:41 - you could list buckets, upload data to s3,
launch, stop, start and terminate, you see
225:45 - two instances, updates, security groups create
subnets, there's an endless amount of things
225:50 - that you can do. All right. And so I just
wanted to point out a couple of very important
225:55 - flags, flags are these things where we have
hyphen, hyphen, and then we have a name here.
225:59 - And this is going to change the behavior of
these COI commands. So we have output and
226:04 - so the outputs what's going to be returned
to us. And we have the option between having
226:07 - Jason table and plain text. I'm for profiles,
if you are switching between multiple AWS
226:14 - accounts, you can specify the profile, which
is going to reference to the credentials file
226:20 - to quickly let you perform CLA actions under
different accounts. So there you go. So now
226:30 - we're going to take a look at eight of a software
development kit known as SDK. And this allows
226:34 - you to control multiple AWS services using
popular programming languages. So to understand
226:40 - what an SDK is, let's go define that. So it
is a set of tools and libraries that you can
226:45 - use to create applications for a specific
software package. So in the case, for the
226:51 - EVAs SDK, it is a set of API libraries that
you you that let you integrate Ada services
226:57 - into your applications. Okay, so that fits
pretty well into the description of an SDK.
227:02 - And the SDK is available for the following
languages. We have c++, go Java, JavaScript,
227:07 - dotnet, no, Jess, PHP, Python, and Ruby. And
so I just have an example of a couple of things
227:14 - I wrote in the SDK. And one is a no Jess and
one is Ruby into the exact same script, it's
227:21 - for ABS recognition for detecting labels.
But just to show you how similar it is among
227:27 - different languages, so more or less the,
the syntax is going to be the same. But yeah,
227:33 - that's all you need to do. So in order to
use the line SDK, we're going to have to do
227:41 - a little bit work beforehand and enable programmatic
access for the user, where we want to be able
227:47 - to use these development tools, okay. And
so when you turn on programmatic access for
227:51 - user, you're going to then get an access key
and a secret, so then you can utilize these
227:56 - services. And so down below, you can see I
have an access key and secret generated. Now,
228:01 - once you have these, you're gonna want to
store them somewhere, and you're gonna want
228:05 - to store them in your user's home directory.
And you're going to want them within a hidden
228:11 - directory called dot AWS, and in a file called
credentials. Okay, so down below here, I have
228:16 - an example of a credentials file. And you'll
see that we have default credentials. So if
228:22 - we were to use CLR SDK, it's going to use
those ones by default if we don't specify
228:26 - any. But if we were working with multiple
AWS accounts are going to end up with multiple
228:31 - credentials. And so you can organize them
into something called profiles here. And so
228:35 - I have one here for enterprise D, and D, Space
Nine. So now that we understand programmatic
228:39 - access, let's move on to learning about CLR.
Hey, this is Andrew Brown from exam Pro. And
228:49 - we are going to do the COI and SDK follow
along here. So let's go over to I am and create
228:56 - ourselves a new user so we can generate some
database credentials. Oh, so um, now we're
229:02 - going to go ahead and create a new user. And
we're going to give them programmatic access
229:08 - so we can get a key and secret. I'm going
to name this user Spock, okay. And we're going
229:14 - to go next here. And we're going to give them
developer permissions, which is a power user
229:20 - here, okay, and you can do the same here.
So our cloud nine environment is ready here.
229:26 - Okay, and we have a terminal here, and it's
connected to any c two instance. And the first
229:31 - thing I'm going to do is I just can't stand
this light theme. So I'm gonna go to themes
229:35 - down here, go to UI themes, and we're going
to go to classic dark, okay, and that's gonna
229:39 - be a lot easier on my eyes here. And so the
first thing we want to do is we want to plug
229:44 - in our credentials here so that we can start
using the COI. So the COI is already pre installed
229:48 - on this instance here. So if I was to type
AWS, we already have access to it. But if
229:54 - we wanted to learn how to install it, let's
actually just go through the motions of that.
229:58 - Okay, so I just pulled up a couple of docks
here just to talk about the installation process
230:02 - of the COI, we already have the COI installs
of a type universe, it's already here. So
230:07 - it's going to be too hard to uninstalled it
just to install it to show you here, but I'm
230:11 - just going to kind of walk you through it
through these docks here just to get you an
230:14 - idea how you do it. So the COI requires either
Python two or Python three. And so on the
230:21 - Amazon Linux, I believe that it has both.
So if I was to type in pi and do virgin here,
230:26 - okay. Or just Python, sorry, I'm always thinking
of shorthands. This has version 3.6, point
230:32 - eight, okay. And so when you go ahead and
install it, you're going to be using Pip.
230:38 - So PIP is the way that you install things
in Python, okay, and so it could be PIP or
230:43 - PIP three, it depends, because there used
to be Python two, and southern Python three
230:48 - came out, they needed a way to distinguish
it. So they called it PIP three, but Python
230:51 - two is no longer supported. So now pin three
is just Pip. Okay, so you know, just got to
230:57 - play around based on your system, okay. But
generally, it's just Pip pip, install ATSC
231:03 - Li. And that's all there is to it. And to
get Python install, your system is going to
231:07 - vary, but generally, you know, it's just for
Amazon, Linux, it is a CentOS, or Red Hat
231:12 - kind of flavor of Linux. So it's going to
use a Yum, install Python. And just for all
231:18 - those other Unix distributions, it's mostly
going to be apt get Okay. So now that we know
231:24 - how to install the CLA, I'm just gonna type
clear here and we are going to set up our
231:28 - credentials. Alright, so we're going to go
ahead and install our credentials here, they're
231:39 - probably already installed, because cloud
nine is very good at setting you up with everything
231:42 - that you need. But we're going to go through
the motions of it anyway. And just before
231:45 - we do that, we need to install a one thing
in cloud nine here. And so I'm going to install
231:50 - via node package manager c nine, which allows
us to open files from the terminal into Cloud
231:57 - Nine here. And so the first thing I want you
to do is I want to go to your home directory,
232:01 - you do that by typing Tilda, which is for
home, and forward slash, OK. And so now I
232:06 - want you to do l LS, hyphen, LA, okay. And
it's going to list everything within this
232:11 - directory, and we were looking for a directory
called dot AWS. Now, if you don't have this
232:15 - one, you just type it MK Dir. And you do dot
AWS to create it, okay, but already exists
232:20 - for us. Because, again, cloud nine is very
good at setting things up for us. And then
232:25 - in here, we're expecting to see a credentials
file. And that should contain our credential.
232:28 - So typing c nine, the program we just installed
there, I'm just going to do credentials here,
232:33 - okay. And it's going to open it up above here.
And you can already see that it's a set of
232:38 - credentials for us, okay. And I'm just going
to flip over and just have a comparison here.
232:43 - So we have some credentials. And it is for
I don't know who, but we have them. And I'm
232:51 - going to go ahead and add a new one, I'm just
gonna make a new one down here called Spock,
232:56 - okay. All right. And basically, what I'm doing
is I'm actually creating a profile here, so
233:03 - that I can actually switch between credentials.
Okay. And I'm just going to copy and paste
233:13 - them in here. Alright, and so I'm just going
to save that there. And so now I have a second
233:21 - set of credentials within the credential file
there, and it is saved. And I'm just going
233:26 - to go down to my terminal here and do clear.
And so now what I'm going to do is I'm going
233:30 - to type in AWS s3 Ls, and I'm going to do
hyphen, hyphen profile, I'm going to now specify
233:37 - spark and that's going to use that set of
credentials there. And so now, I've done that
233:42 - using sparks credentials, and we get a list
of a bucket stuff. Okay. So now if we wanted
233:48 - to copy something down from s3, we're going
to use AWS s3, CP. And we are going to go
233:56 - into that bucket there. So it's going to be
exam Pro, enterprise D, I have this from memory.
234:05 - And we will do data dot jpg, okay. And so
what that's going to do is it's going to download
234:10 - a file but before actually run this here,
okay. I'm just going to CD dot dot and go
234:17 - back to my home directory here. Okay. And
I'm just going to copy this again here and
234:22 - paste it and so I should be able to download
it but again, I got to do a hyphen having
234:26 - profile specifies proc spark because I don't
want to use the default profile there. Okay,
234:31 - um, and uh, complain because I'm missing the
G on the end of that there, okay? And it's
234:38 - still complaining. Maybe I have to do s3 forward
slash forward slash huh? Ah, no, that's the
234:51 - command. Oh, you know why? It's because when
you use CP, you have to actually specify the
234:55 - output file here. So you need your source
and your destination. Okay, so I'm just good.
235:00 - Dr. Spock are sorry, data sorry, data dot
JPG there. Okay. And that's going to download
235:05 - that file. So, I mean, I already knew that
I had something for AWS there. So I'm just
235:11 - going to go to AWS to show you that there.
So if you want to do the same thing as I did,
235:16 - you knew, you definitely need to go set up
a bucket in a three. Okay? So if I just go
235:22 - over here, we have the exam, pro 00, enterprise
D, and we have some images there. Okay, so
235:27 - that's where I'm grabbing that image from.
And I can just move this file into my environment
235:33 - directory, so I actually can have access to
it there. Okay, so I'm just going to do MB
235:37 - data. And I'm just going to move that one
directory up here. Okay. All right. And so
235:45 - now we have data over here, okay. And so,
you know, that's how you'd go about using
235:51 - the CLA with credentials. Okay. Yeah, we just
opened that file there if we wanted to preview
235:56 - it. Okay. So now let's, uh, let's move on
to the SDK. And let's use our credentials
236:02 - to do something programmatically, okay. So
now that we know how to use the COI, and where
236:10 - to store credentials, let's go actually do
something programmatically with the SDK. And
236:16 - so I had recently contributed to database
docs, for recognition. So I figured we could
236:20 - pull some of that code and have some fun there.
Okay. So what you do is go to Google and type
236:25 - in Avis docs recognition. And we're going
to click through here to Amazon recognition,
236:31 - we're going to go to the developer developers
guide with HTML, apparently, they have a new
236:35 - look, let's give it a go. Okay, there's always
something new here. I'm not sure if I like
236:40 - it. But this is the new look to the docks.
And we're going to need to find that code
236:46 - there. So I think it is under detecting faces
here. And probably under detecting faces in
236:52 - an image, okay. And so the code that I added
was actually the Ruby and the node GS one,
236:57 - okay, so we can choose which one we want,
I'm going to do the Ruby one, because I think
237:01 - that's more fun. And that's my language of
choice. Okay. And so I'm just going to go
237:06 - ahead and copy this code here. Okay. And we're
going to go back to our cloud nine environment,
237:12 - I'm going to create a new, a new file here,
and I'm just going to call this detect faces,
237:21 - ooh, keep underscore their faces.rb. Okay.
And I'm just gonna double click in here and
237:29 - paste that code in. Alright. And what we're
going to have to do is we're going to need
237:33 - to supply our credentials here, generally,
you do want to pass them as in as environment
237:39 - variables, that's a very safe way to provide
them. So we can give that a go. But in order
237:45 - to get this working, we're going to have to
create a gem file in here. So I'm just going
237:48 - to create a new file here. Because we need
some dependencies here, we're just going to
237:53 - type in gem file, okay. And within this gem
file, we're going to have to provide the gem
238:00 - recognition. Okay, so I'm just gonna go over
here and supply that there. There is a few
238:05 - other lines here that we need to supply. So
I'm just gonna go off screen and go grab them
238:09 - for you. Okay, so I just went off screen here
and grabbed that extra code here. This is
238:14 - pretty boilerplate stuff that you have to
include in a gem file. Okay. And so what this
238:19 - is going to do, it's going to install the
AWS SDK for Ruby, but specifically just for
238:23 - recognition. So I do also have open up here,
the AWS SDK, for Ruby, and for no GS, Python,
238:31 - etc, they all have one here. And so they tells
you how you can install gems. So for dealing
238:38 - with recognition here, I'm just gonna do a
quick search here for recognition. Okay, sometimes
238:43 - it's just better to navigate on the left hand
side here. Alright, and so I'm just looking
238:48 - for a recognition. Okay, and so if we want
to learn how to use this thing, usually a
238:52 - lot of times with this, it's going to tell
you what gem you're gonna need to install.
238:55 - So this is the one we are installing. And
then we click through here through client,
238:59 - and then we can get an idea of all the kinds
of operations we can perform. Okay, so when
239:04 - I needed to figure out how to write this,
I actually went to the CLR here, and I just
239:07 - kind of read through it and pieced it together
and looked at the output to figure that out.
239:11 - Okay, so nothing too complicated there. But
anyway, we have all the stuff we need here.
239:16 - So we need to make sure we're in our environment
directory here, which is that Spock dev directory.
239:22 - So we're going to type tilde out, which goes
to our home directory environment, okay, we're
239:27 - gonna do an ls hyphen, LA. And just make sure
that we can see that file there and the gem
239:32 - file, okay, and then we can go ahead and do
a bundle install. All right, and so what that's
239:36 - going to do is it's going to now install that
dependency. so here we can see that installed
239:41 - the EVAs SDK, SDK core and also recognition.
Okay, and so now we have all our dependencies
239:47 - to run the script here. So the only thing
that we need to do here is we need to provide
239:52 - it an input. so here we can provide it a specific
bucket and a file. There is a way to provide
240:01 - a locally, we did download this file, but
I figured what we'll do is we'll actually
240:04 - provide the bucket here. So we will say, what's
the bucket called exam pro 000. And the next
240:13 - thing we need to do is define the key. So
it's probably the key here. So I'm going to
240:19 - do enterprise D. Okay, and then we're just
going to supply data there. All right. And
240:25 - we can pass these credentials via the environment
variables, we could just hard code them and
240:30 - paste them in here. But that's a bit sloppy.
So we are going to go through the full motions
240:34 - of providing them through the environment
here. And all we have to do to do that is
240:39 - we're just going to paste in, like so. Okay.
And we're just going to copy that, that's
240:46 - the first one. And then we're going to do
the password here. Oops. Okay. And hopefully,
240:54 - this is going to work the first time, and
then we'll have to do bundle exec detect faces,
241:00 - okay. And then this is how these are going
to get passed into there. And assuming that
241:04 - my key and bucket are correct, then hopefully,
we will get some output back. Okay. All right,
241:10 - it's just saying it couldn't detect faces
here, I just have to hit up here, I think
241:15 - I just need to put the word Ruby in front
of here. Okay, so my bad. Alright, and we
241:25 - is it working. So we don't have the correct
permissions here. So we are a power user.
241:30 - So maybe we just don't have enough permission.
So I'm just going to go off screen here and
241:33 - see what permissions we need to be able to
do this. So just playing around a little bit
241:39 - here, and also reading the documentation for
the Ruby SDK, I figured out what the problem
241:44 - was. And it's just that we don't need this
forward slash here. So we just take that out
241:49 - there, okay, and just run what we ran last
there, okay. And then we're gonna get some
241:54 - output back. And then it just shows us that
it detected a face. So we have the coordinates
241:59 - of a face on and if we used some additional
tool there, we could actually draw overtop
242:06 - of the image, a bounding box to show where
the face is detected. There's some interesting
242:10 - information. So it detected that the person
in the image was male, and that they were
242:14 - happy. Okay. So, you know, if you think that
that is happy, then that's what recognition
242:22 - thinks, okay. And it also detected the face
between ages 32 and 48. To be fair, data is
242:28 - an Android, so he has a very unusual skin
color. So you know, it's very hard to do to
242:34 - determine that age, but I would say that this
is the acceptable age range of the actor at
242:40 - the time of so it totally makes sense. Okay.
So yeah, and there you go. So that is the
242:46 - pragmatic way of doing it. Now, you don't
ever really want to ever store your credentials
242:52 - with on your server, okay? Because you can
always use Iam roles, attach them to EC two
242:58 - instances, and then that will safely provide
credentials onto your easy two instances,
243:02 - to have those privileges. But it's important
to know how to use the SDK. And whenever you're
243:08 - in development, working on your local machine,
or maybe you're in cloud nine environment,
243:11 - you are going to have to supply those credentials.
Okay. So there you go. So now that we are
243:19 - done with our eight, or eight USC Li and SDK,
follow along here. So we're on to the AWS
243:30 - COI in SDK ci ci, so let's jump into it. So
ci stands for command line interface SDK stands
243:35 - for software development kit. The COI lets
you enter interact with AWS from anywhere
243:41 - by simply using a command line. The SDK is
a set of API libraries that let you integrate
243:47 - data services into your applications. promatic
access must be enabled per user via the Iam
243:52 - console to UCLA or SDK a to s config command
is used to set up your ad credentials for
243:59 - this Eli, the CLA is installed via a Python
script credentials get stored in a plain text
244:06 - file, whenever possible use roles instead
of at this credentials. I do have to put that
244:10 - in there. And the SDK is available for the
following programming languages c++, go Java,
244:15 - JavaScript, dotnet, no GS, PHP, Python and
Ruby. Okay, so for the solution architect
244:22 - associate, they're probably not going to ask
you questions about the SDK, but for the developer,
244:26 - there definitely are. So just keep that in.
Hey, this is Andrew Brown from exam Pro. And
244:36 - we are looking at key management service,
which is used for creating and managing encryption
244:41 - keys for a variety of database services or
within your applications. And the way I like
244:46 - to think of it is that whenever you see a
checkbox in AWS to encrypt something, it's
244:51 - very likely using kms. So kms makes it easy
for you to create control and rotate encryption
244:58 - keys used to encrypt your data on AWS and
kms is a multi tenant hardware security module,
245:05 - which we're going to talk about in the next
slide here. And the main takeaway I want you
245:10 - to remember about kms is that whenever you
are using at a service, and you have the option
245:15 - to check box on encryption, so here we have
an example of EBS. You're going to checkbox
245:20 - on and then choose a master key. And that's
all you have to do. It's going to vary per
245:25 - service. But that's pretty much the routine.
And kms can be used alongside with cloud trail
245:31 - to audit access history. So you have to investigate
who used what key, that's how you're going
245:36 - to do it. And ATMs integrates with a lot of
different AWS services. So here I've highlighted
245:40 - the ones which are most important to remember
for the associate exam. So you got EC two
245:45 - Sq S, s3, dynamodb, elastic, cash, RDS, and
more. Okay. So kms is a multi tenant HSM.
245:54 - But what does that mean? So HSM, which stands
for hardware security module is a hardware
245:58 - that is specialized for storing your encryption
keys. It's designed to be tamper proof, and
246:03 - it stores those keys in memory. So they're
never written to disk. So imagine the power
246:08 - went out, those keys are gone. And that is
actually a security feature. And so here is
246:13 - an example of a piece of HSM. And these are
really, really, really expensive. And so this
246:20 - is where kms comes into play, because it is
multi tenant, meaning that there are multiple
246:24 - customers who are utilizing the same piece
of hardware. So you're sharing the costs with
246:29 - a bunch of different items, customers, and
those customers are isolated virtually from
246:35 - each other. So there is software that protects
you from other people's data. But if you had
246:41 - one customer who utilize the entire piece
of hardware, which we would call dedicated,
246:45 - that would be also considered a single tenant
because there's only one person using that,
246:51 - that server, and it AWS actually has a single
tenant HSM, and that is called Cloud HSM.
246:58 - And this is going to give you a lot more control.
And the reason why people would use Cloud
247:04 - HSM over kms is that cloud HSM is FIPS 142,
level three, whereas kms is only FIPS 142
247:13 - level two, but the takeaway from that is just
understand that cloud HSM is, you know more
247:19 - for enterprises that need to meet those regulations.
But kms is a really great service to utilize.
247:27 - So to really understand kms, we need to understand
what a customer master key is, because this
247:36 - is the primary resource that kms is managing.
And to start there. Let's talk about what
247:41 - encryption is. So encryption is the process
of encoding a message or information in such
247:45 - a way that only authorized parties can access
it and those who are not authorized cannot.
247:50 - Okay, pretty basic. And so that leads us to
what are cryptographic keys or data keys.
247:57 - So a data key is just a string of data that
is used to lock or unlock cryptographic functions.
248:02 - So a cryptographic function could be authentication,
authorization, or encryption. And so that
248:09 - leads us on to what is a master key. So a
master key is stored in a security hardware.
248:14 - So an HSM and master keys are used to encrypt
all other keys on the system. Those other
248:20 - keys are being data keys. And so why would
we want to use a key to encrypt another key,
248:27 - which is called envelope encryption? Well,
the reason why and here's a, here's a diagram
248:32 - of an envelope encryption is how do you know
that the keys, the data keys that you use
248:38 - that unlock the data to your database, are
secure. And that's where these master keys
248:44 - come into play. So the idea is that they create
security around those keys. So to learn a
248:52 - little bit more about customer master keys,
a customer master key is the primary resource
248:57 - that AWS kms is managing. And a customer master
key abbreviated as CMK is a logical representation
249:06 - of a master key. So you're not directly accessing
the master key. It's, it's a logical representation.
249:14 - But with that logical representation, we get
to attach a lot of metadata that's going to
249:19 - help us understand things about our master
key. So the key ID when it was created the
249:24 - creation date, we can give it a description
and say what state the key is in. And that
249:30 - CMK is going to also contain key material
used to encrypt and decrypt data. And so kms
249:37 - supports both symmetric and asymmetric cm
Ks. So if you've never heard of symmetric
249:43 - and asymmetric I'll give you a couple examples.
So some symmetric symmetric key is generally
249:50 - a 256 bit key that is used for encryption
and decryption. So you have a single key that
249:55 - you're using. And an example of this on AWS
would be when you encrypt an s3 It uses something
250:00 - called a Aes 256 to be six suggests that is
using 256 bit encryption as is the protocol
250:08 - for encryption. And so that is that method.
And the other method is asymmetric key. And
250:16 - so this would be where you have an RSA key
pair that is used for encryption and decryption
250:20 - or signing verification, but not both. And
the idea here is that you have two keys. So
250:26 - a great example this is with EC two key pairs,
you have a public key and a private key. Now
250:31 - kms isn't using when you're, when you're downloading
EC two key pairs, I don't think that they're
250:37 - using kms, or at least that they are is probably
managed by AWS, and it's transparent to you.
250:43 - But the idea of having these two methods of
keys is based on the use case. But from a
250:49 - security perspective, if you have to have
two keys one key to match to another, that
250:53 - is technically more secure. Whereas if you
have one key, if that one key is lost, then
250:58 - you know, is is less secure. Okay, so that's
customer master keys. Okay, let's do a quick
251:11 - review of some CLR commands we can use with
kms. And these are actually very common. And
251:17 - if you're studying for the developer associate,
you should absolutely commit these to memory,
251:20 - especially for your day to day kind of stuff.
But on the exam, you might see these appear.
251:27 - And so it's just good to know them. So you
can eliminate them from options that just
251:31 - do not exist. So the first one here is the
Create key command. And as the name implies,
251:36 - it creates a customer manage key, very straightforward.
Then you have your encrypt. So this is going
251:42 - to encrypt plain text into ciphertext, then
you have decrypt. So that's going to decrypt
251:48 - ciphertext that was encrypted by kms. And
then you have re encrypt. And so re encrypt
251:52 - can be used in three scenarios for manual
rotations. The CMK is when you're changing
251:57 - the CMK that protects the ciphertext, or you're
changing the encryption context of a ciphertext.
252:02 - So re encrypt. And the last one is enable
key rotation. And the idea is that you know,
252:08 - once a year, if you want to rotate out those
keys, you can turn this on, and it will just
252:14 - happen automatically. The only thing that
you have to notice that this only works for
252:17 - symmetric customer master keys. So and you
cannot perform this operation outside of CMK
252:24 - in a different in a different AWS account,
so it's within the existing account. So yeah,
252:30 - there you go. So we're at the end of the kms
section, so on to the kms. Key Management
252:41 - Service. kms creates and manages encryption
keys for variety of database services or for
252:46 - your apps. kms can be used with cloud trail
to audit a keys access history. kms has the
252:51 - ability to automatically rotate out your keys
every year with no need to re encrypt. Customer
252:56 - master keys are the primary resources in kms.
kms is a multi tenant HSM multi tenant means
253:02 - you're sharing the hardware with multiple
customers. hardware security modules HSM is
253:07 - a specialized hardware for storing your keys
and is tamper proof kms is up to FIPS 142
253:12 - level two compliant if you're looking at what's
another one called Cloud HSM, that's level
253:20 - three if you need level three kms stores master
keys not data keys. Master keys master keys
253:28 - are used to encrypt data keys and which is
called envelope encryption. kms supports two
253:32 - types of keys symmetric and asymmetric. So
symmetric is a single key using 256 bit encryption.
253:38 - So I always like to say think of s3 buckets
a as to be six. asymmetric uses two keys I
253:44 - always think of thinking about like key pair
public and private, important kms kms API
253:49 - key API's to remember because you might see
them as exam questions kms, create key kms
253:55 - encrypt, decrypt, re encrypt enable key rotation.
So there we go. That is the end of kms cheat
254:03 - sheet. If this was the AWS database, security
certification, this is like a seven page cheat
254:13 - sheet. So this is very light. But you definitely
need to know kms it's very important as a
254:17 - developer or in sysop. Hey, this is Andrew
Brown from exam Pro. And we are looking at
254:26 - Amazon cognito, which is a decentralized way
of managing authentication. So think sign
254:31 - up sign in integration for your apps, social
identity providers, like connecting with Facebook
254:36 - or Google. So Amazon cognito actually does
multiple different things. And we are going
254:42 - to look at three things in specific. We're
going to look at cognito user pools, which
254:46 - is a user directory to authenticate against
identity providers. We're going to look at
254:51 - cognito identity pools, which provides temporary
credentials for your users to access database
254:55 - services. And we're going to look at cognito
sync which syncs users data and preferences
254:59 - across all devices. So let's get to it. So
to fully understand Amazon cognito, we have
255:08 - to understand the concepts of web identity
Federation and identity providers. So let's
255:13 - go through these definitions. So for web identity
Federation, it's to exchange the identity
255:18 - and security information between an identity
provider and an application. So now looking
255:24 - at identity provider, it's a trusted provider
for your user identity that lets you authenticate
255:29 - to access other services. So an identity provider
could be Facebook, Amazon, Google, Twitter,
255:36 - GitHub, LinkedIn, you commonly see this on
websites where it allows you to log in with
255:41 - a Twitter or GitHub account, that is an identity
provider. So that would be Twitter or GitHub.
255:46 - And they're generally powered by different
protocols. So whenever you're doing this with
255:51 - social social accounts, it's going to be with
OAuth. And so that can be powered by open
255:56 - ID Connect, that's pretty much the standard
now, if there are other identity providers,
256:01 - so if you needed a single sign on solution,
SAML is the most common one. Alright. So the
256:11 - first thing we're looking at is cognito. User
pools, which is the most common use case for
256:15 - cognito. And that is just a directory of your
users, which is decentralized here. And it's
256:21 - going to handle actions such as signup sign
in account recovery. So that would be like
256:25 - resetting a password, account confirmation,
that would be like confirming your email after
256:31 - sign up. And it has the ability to connect
to identity providers. So it does have its
256:35 - own like email and password form that it can
take. But it can also leverage. Maybe if you
256:41 - want to have Facebook Connect, or Amazon Connect,
and etc, you can do that as well, the way
256:46 - it persists a connection after it's authenticated,
that generates a j WT. So that's how you're
256:52 - going to persist that connection. So let's
look at more of the options so that we can
256:55 - really bake in the utility here of user pools.
So here left hand side, we have a bunch of
257:01 - different settings. And for attributes, we
can determine what should be our primary attribute
257:06 - should be our username when they sign up,
or should it be email and phone phone number.
257:12 - And if it is, you know, can they sign up or
sign in if the email address hasn't been verified,
257:18 - where the conditions around that we can set
the restrictions on the password the length,
257:22 - if it requires special characters, we can
see what kind of attributes are required to
257:27 - collect on signup, if we need their birthday,
or email or etc. It has the capabilities of
257:31 - turning on MFA. So if you want multi factor
authentication, very easy way to integrate
257:35 - that if you want to have user campaigns, so
if you're used to like sending out campaigns
257:41 - via MailChimp, you can easily integrate cognito
with pinpoint which is a user campaigns, right.
257:47 - And you also can override a lot of functionality
using lambda. So anytime like a sign up or
257:53 - sign in or recovery passwords triggered, there
is a hook so that you can then trigger lambda
257:58 - to do something with that. So that's just
some of the things that you do with cognito
258:01 - user pools. But the most important thing to
remember is just it's a way of decentralizing
258:05 - authentication that's for for user pools.
All right. So now it's time to look at cognito
258:14 - identity pools, identity pools provide temporary
AIIMS credentials to access services such
258:19 - as Dynamo DB or s3, identity pools can be
thought of as the actual mechanism authorizing
258:24 - access to the AWS resources. So you know,
the idea is you have an identity pool, you're
258:29 - going to say, who's allowed to generate those
AWS credentials, and then use the SDK to generate
258:36 - those credentials. And then that application
can then access those database services. So
258:40 - just to really hit that home here, I do have
screenshots to give you an idea what that
258:46 - is. So first, we're going to choose our providers,
our provider can be authenticated. So we can
258:50 - choose cognito, or even a variety of other
ones, or you can have an unauthenticated.
258:56 - So that is also an option for you. And then
after you create that identity pool, they
259:01 - have an easy way for you to use the SDK. So
you could just drop down your platform and
259:05 - you have the code and you're ready to go to
go get those credentials. If you're thinking
259:09 - did I actually put in my real, real example
or identity pool ID there? It's not, it's
259:16 - not I actually go in and replace all these.
So if you're ever wondering and watching these
259:19 - videos, and you're seeing these things I always
replay. We're going to just touch on one more,
259:28 - which is cognito. Sync. And so sync lets you
sync user data and preferences across all
259:34 - devices with one line of code cognito uses
push notifications to push those updates and
259:38 - synchronize data. And under the hood, it's
using simple notification service to push
259:43 - this data to devices. And the the the data
which is user data and preferences is key
259:50 - value data. It's actually stored with the
identity pool. So that's what you're pushing
259:54 - back and forth. But the only thing you need
to know is what it does. And what it does
259:58 - is it syncs user data and preferences across
All devices will have one line of code. Hey,
260:06 - this is Angie brown from exam Pro, and we're
going to do the cognito. Follow along and
260:11 - set up a login screen. So I'm gonna go to
cognito cognito. There we go. And we're going
260:18 - to be presented two options. User pools, identity
pools, identity pools is not what we want.
260:23 - We want user pools so that people can log
in authenticate identity pools is when you
260:26 - want to give access to existing resources
on AWS. So we'll go to user pools here, I'm
260:32 - going to create a new user pool, I'm going
to call study saying, since that's kind of
260:35 - the project we've been working with, in our,
in this developer associate, I'm going to
260:41 - go to review defaults. All these defaults
are fine, you can see you can set MFA, and
260:45 - all this stuff, you can change any of this
if you want on the create this pool, and the
260:52 - pool has been created, I'm going to need a
app client, I'm going to hit Add app client,
260:56 - we're gonna name this study sync. We're going
to leave all of this defaulted here, I'm going
261:04 - to go ahead and create this app client. Now
the next thing to do is configure this app
261:09 - client. So the app client settings, we're
going to enable user pools, I'm going to put
261:12 - an example URL here for our callback. This
is what would happen after somebody logged
261:18 - in successfully. And this is where they would
go if they logged out. We want authorized
261:22 - code grants and implicit grants, Mize will
take all of these scopes, if possible, because
261:28 - why not. And we will go ahead and save these
here. In order to use the host UI, this is
261:34 - like a UI that eight of us gives you by default,
of course you can make your own, which is
261:38 - a lot of work. But to use this host of UI,
which is what we'll be using here, we need
261:42 - to set a domain name, I'm gonna call it steady
sync, we'll check for availability, it's available,
261:47 - you might have to change it based on your
case, we'll hit save. And then to view our
261:51 - hosted domain or our hosted login screen,
we're going to go back to App client settings.
261:57 - At the bottom here, hit launch UI here, I'm
just going to do a hard refresh here. Because
262:02 - it's giving us some trouble. So let me just
go back to the domain here. Yeah, it should
262:10 - be here, it looks like we also customize it,
which is kind of nice. But this should work.
262:15 - Now, let me just see here, maybe just needed
some time. There you go. So I was just way,
262:21 - way too fast. And then we'll go ahead, and
we'll just sign up. And the goal here is that
262:26 - if we sign up successfully, we should be able
to, um, we should be able to be directly redirected
262:34 - to the that that URL that we provided. So
I'm gonna need a temporary email, you use
262:39 - 10 minute mail, minute, minute of mail here.
We go here. And oh, well just changed the
262:48 - whole UI, I guess they have a new one. Oh,
it's beta. Look at that I'm early user, if
262:54 - you never use this platform, it's for getting
temporary emails very quickly. And we'll go
262:58 - back here, I will paste this in here. And
then I'll make the password testing 123 exclamation
263:05 - mark sign up. And now we have a verification
code. If I go back here, there it is. There's
263:14 - our code will copy it confirms it. And it
redirected us. So that means that we successfully
263:20 - logged in, if we go over to our users, we
should be able to see this user now if I refresh,
263:27 - or there, there I am, there's Andrew Brown.
So you know, that's all there is to setting
263:33 - it up. Clearly, there's a lot more work involved.
But for the developer associate, that's all
263:37 - you need to be comfortable with, I just want
to make sure that you got some hands on with
263:42 - cognito. Generally, if you use a sample phi,
it does a lot of the work for you for setting
263:47 - it up and integrating with your application.
Integrating for cognito for your web apps
263:54 - without amplify is extremely painful. But
it's worth it because it's so inexpensive.
263:59 - The only thing that is unfortunate about us
are cognito user pools is there's a limitation
264:05 - in terms of identity providers. So we have
Facebook, Google Amazon, but we don't have
264:10 - LinkedIn. And apparently you can do Twitter,
I think with open ID Connect. But for me,
264:16 - like LinkedIn is a deal breaker. It's not
a devices fault, either of us falls like some
264:20 - kind of security standard. And for whatever
reason, LinkedIn does not conform to that.
264:25 - And so that's why it's not in the list here.
There is a way to get LinkedIn to work, but
264:29 - it's a lot of effort. And so that's why we
see a lot people using author zero because
264:33 - it just does everything but that one's really
expensive. And there's a lot of things you
264:37 - can do in here. You know, there's a lot of
options and I recommend that you poke around
264:43 - but we're done for this case and I want you
to go ahead and delete this pool. Delete this
264:47 - pool, we have to first delete the domain name
because we are borrowing that from AWS. And
264:53 - once that's deleted, we can go to general
settings, hit delete, type in delete, and
264:58 - there we go. Our pool is gone. So that's it.
So we're onto the Amazon cognito cheat sheet.
265:09 - So let's jump into it. So cognito is a decentralized
managed authentication system. So when you
265:14 - need to easily add authentication to your
mobile or desktop apps, think cognito. So
265:19 - let's talk about user pools. So user pool
is the user directory allows users to authenticate
265:23 - using OAuth two ipds, such as Facebook, Google
Amazon to connect to your web applications.
265:30 - And cognito user pool isn't in itself an IPD.
All right, so it can be on that list as well.
265:36 - User pools use JW T's to persist authentication.
Identity pools provide temporary database
265:42 - credentials to access services, such as s3
or dynamodb, cognito. Sync can sync user data
265:48 - preferences across devices with one line of
code powered by SNS web identity Federation,
265:52 - they're not going to ask you these questions.
But you need to know what these are exchange
265:56 - identity and security information between
identity provider and an application. identity
266:00 - provider is a trusted provider for your user
to authenticate or sorry to identify that
266:06 - user. So you can use them to dedicate to access
other services. Then you have Oh, IDC is a
266:12 - type of identity provider which uses OAuth
and you have SAML, which is a type of identity
266:16 - provider which is used for single sign on
so there you go. We're done with cognito.
266:24 - Hey, this is Andrew Brown from exam Pro. And
we are looking at simple notification service
266:28 - also known as SNS, which lets you subscribe
and send notifications via text message email,
266:33 - web hooks, lambdas Sq s and mobile notification.
Alright, so to fully understand SNS, we need
266:39 - to understand the concept of pub sub. And
so pub sub is a publish subscribe pattern
266:44 - commonly implemented in messaging systems.
So in a pub sub system, the sender of messages,
266:50 - also known as the publisher here, doesn't
send the message directly to the receiver.
266:55 - Instead, they're gonna send the messages to
an Event Bus. And the event bumps categorizes
266:59 - the messages into groups. And then the receiver
of messages known as the subscriber here subscribes
267:04 - to these groups. And so whenever a new message
appears within their subscription, the messages
267:09 - are neatly delivered to them. So it's not
unlike registering for a magazine. All right.
267:15 - So, you know, down below, we have that kind
of representation. So we have those publishers,
267:19 - and they're publishing to the Event Bus which
have groups in them, and then that's going
267:22 - to send it off to those subscribers, okay,
so it's pushing it all along the way here,
267:26 - okay, so publishers have no knowledge of who
their subscribers are. Subscribers Do not
267:31 - pull for messages, they're gonna get pushed
to them. messages are instead automatically
267:35 - immediately pushed to subscribers and messages
and events are interchangeable terms in pub
267:39 - sub. So if you see me saying messages and
events, it's the same darn thing. So we're
267:45 - now looking at SNS here. So SNS is a highly
available, durable, secure, fully managed
267:50 - pub sub messaging service that enables you
to decouple microservices distributed systems
267:55 - and serverless applications. So whenever we
talking about decoupling, we're talking about
267:59 - application integration, which is like a family
of Ada services that connect one service to
268:05 - another. Another service is also Sq s. And
SNS is also application integration. So down
268:12 - below, we can see our pub sub systems. So
we have our publishers on the left side and
268:16 - our subscribers on the right side. And our
event bus is SNS Okay, so for the publisher,
268:22 - we have a few options here. It's basically
anything that can programmatically use the
268:26 - EVAs API. So the SDK and COI uses the Avis
API underneath. And so that's going to be
268:35 - the way publishers are going to publish their
messages or events onto an SNS topic. There's
268:42 - also other services on AWS that can trigger
or publish to SNS topics cloud watch, definitely
268:51 - can, because you'd be using those for billing
alarms. And then on the right hand side, you
268:54 - have your subscribers and we have a bunch
of different outputs, which we're going to
268:57 - go through, but here you can see we have lambda
Sq s email, and HTTPS protocol. So publishers
269:03 - push events to an SNS topic. So that's how
they get into the topic. And then subscribers
269:07 - subscribe to the SNS topic to have events
pushed to them. Okay. And then down below,
269:13 - you can see I have a very boring description
of SNS topic, which is it's a logical access
269:18 - point and communication channel. So that makes
a nap. That makes sense. So let's move on.
269:26 - So we're gonna take a deeper look here at
SNS, topics and topics allow you to group
269:32 - multiple subscriptions together, a topic is
able to deliver to multiple protocols at once.
269:38 - So it could be sending out email, text message
HTTPS, all the sorts of protocols we saw earlier.
269:43 - I'm publishers don't care about the subscribers
protocol, okay? Because it's sending a message
269:48 - event, it's giving you the topic and saying,
you figure it out. This is the message I want
269:52 - to send out, and it knows what subscribers
it has. And so the topic when it delivers
269:57 - that messages, it will automatically format
it for us. The message according to the subscribers
270:02 - chosen protocol, okay, and the last thing
I want you to know is that you can encrypt
270:05 - your topics via kms key management service.
And you know, so it's just as easy as turning
270:11 - it on and picking your key. So now we're taking
a look at subscriptions. And subscriptions
270:19 - are something you create on a topic, okay.
And so here I have a subscription, that is
270:25 - an email subscription. And the endpoint is
obviously going to be an email. So I provided
270:29 - my email there. If you want to say hello,
give, send me an email. And it's just as simple
270:34 - as clicking that button and filling in those
options. Now you have to choose your protocol.
270:39 - And here we have our full list here on the
right hand side. So we'll just go through
270:43 - it. So we have HTTP, HTTPS, and you're going
to want to be using this for web hooks. So
270:47 - the idea is that this is usually going to
be an API endpoint to your web applications
270:51 - that's going to listen for incoming messages
from SNS, then you can send out emails now,
270:57 - there's another service called ACS, which
specializes in sending out emails. And so
271:02 - SNS is really good for internal email notifications,
because you don't necessarily have your custom
271:07 - domain name. And also, the emails have to
be plain text only. And there's some other
271:12 - limitations around that. So they're really,
really good for internal notifications, maybe
271:16 - like billing alarms, or maybe someone signed
up on your platform you want to know about
271:20 - it, then they also have it in email, JSON.
So let's just gonna send you JSON via email,
271:25 - then you have Sq s, so you can send an SMS
message to Sq s. So that's an option you have
271:32 - there, you can also have SNS trigger lambda
functions. So that's a very useful feature
271:38 - as well. And you can also send text messages
that will be using the SNS protocol. And the
271:44 - last one here is platform application endpoints.
And that's for mobile push. So like a bunch
271:48 - of different devices, laptops, and phones
have notification systems in them. And so
271:55 - this will integrate with that. And we're just
gonna actually talk about that a bit more
271:58 - here. So I wanted to talk a bit more about
this platform application endpoint. And this
272:08 - is for doing mobile push. Okay, so we have
a bunch of different mobile devices, and even
272:14 - laptops that use notification systems in them.
And so here you can see a big list. We have
272:19 - a DM, which is Amazon device messaging, we
have Apple, the do Firebase, which is Google
272:24 - and then we have two for Microsoft. So we
have Microsoft push and Windows push, okay.
272:30 - And so you can with this protocol, push out
to that stuff. And the advantage here you're
272:33 - gonna when you push notification messages
to these mobile endpoints, it can appear in
272:39 - the mobile app just like message alerts, badges,
updates, or even sound alert. So that's pretty
272:43 - cool. Okay, so I just want you to be aware.
Alright, so on to the SMS cheat sheet. So
272:52 - simple notification service, also known as
SNS, is a fully managed pub sub messaging
272:55 - service. SNS is for application integration.
It allows decoupled services and apps to communicate
273:02 - with each other. We have a topic which is
a logical access point and communication channel,
273:07 - a topic is able to deliver to multiple protocols.
You can encrypt topics via kms. And then you
273:12 - have your publishers, and they use the EVAs
API via the CLR or the SDK to push messages
273:18 - to a topic. Many Ava's services integrate
with SNS and act as publishers Okay, so think
273:24 - cloud watch and other things. Then you have
subscriptions, so you can subscribe, which
273:30 - consists subscribe to topics. When a topic
receives a message, it automatically immediately
273:35 - pushes messages to subscribers. All messages
published to SNS are stored redundantly across
273:40 - multi az, which isn't something we talked
in the core content, but it's good to know.
273:45 - And then we have the following protocols we
can use so we have HTTP HTTPS. This is great
273:49 - for web hooks into your web application. We
have emails good for internal email notification.
273:54 - Remember, it's only plain text if you need
rich text. And custom domains are going to
273:59 - be using sts for that. Then you have email
JSON, very similar to email just sending Jason
274:03 - along the way. You can also send your your
SMS messages into an ESA s Sq sq. You can
274:11 - trigger lambdas you can send text messages.
And then the last one is you have platform
274:16 - application endpoints, which is mobile push
okay and that's going to be for systems like
274:21 - Apple, Google Microsoft Purdue alright. Hey,
this is Andrew Brown from exam pro and we
274:31 - are looking at simple queue service also known
as Sq s which is a fully managed queuing service
274:36 - that enables you to decouple and scale micro
services distributed systems and serverless
274:42 - applications. So, to fully understand Sq s,
we need to understand what a queueing system
274:48 - is. And so a queueing system is just a type
of messaging system, which provides asynchronous
274:54 - communication and decouples. Processes via
messages could also be known as events from
274:58 - a sender and receiver but in the Case for
a streaming system also known as a producer
275:02 - and consumer. So, looking at a queueing system,
when you have messages coming in, they're
275:08 - usually being deleted on the way out. So as
soon as they're consumed or deleted, it's
275:12 - for simple communication. It's not really
for real time. And just to interact with the
275:17 - queue and the messages there, both the sender
and receiver have to pull to see what to do.
275:22 - So it's not reactive. Okay, we got some examples
of queueing systems below, we have sidekick,
275:27 - sq, S, rabbit, rabbit and Q, which is debatable
because it could be considered a streaming
275:32 - service. And so now let's look at the streaming
side to see how it compares against a queueing
275:36 - system. So a streaming system can react to
events from multiple consumers. So like, if
275:45 - you have multiple people that want to do something
with that event, they can all do something
275:49 - with it, because it doesn't get immediately
deleted, it lives in the Event Stream for
275:53 - a long period of time. And the advantage of
having a message hang around in that Event
275:58 - Stream allows you to apply complex operations.
So that's the huge difference is that one
276:04 - is reactive and one is not one allows you
to do multiple things with the messages and
276:09 - retains it in the queue. One deletes it and
doesn't really doesn't really think too hard
276:14 - about what it's doing. Okay, so there's your
comparative between queuing and streaming.
276:18 - And we're going to continue on with Sq s here,
which is a queueing system. So the number
276:23 - one thing I want you to think of when you're
thinking of Sq S is application integration.
276:28 - It's for connecting isolette applications
together, acting as a bridge of communication
276:34 - and Sq s happens to use messages and queues
for that you can see Sq s appears in the Ava's
276:39 - console under application integration. So
these are all services that do application
276:43 - integration Sq S is one of them. And as we
said it uses a queue. So a queue is a temporary
276:49 - repository for messages that are waiting to
be processed, right. So just think of going
276:53 - to the bank and everyone is waiting that line,
that is the queue. And the way you interact
276:59 - with that queue is through the Avis SDK. So
you have to write code that was going to publish
277:03 - messages to the queue. And then when you want
to read them, you're going to have to use
277:07 - the database SDK to pull messages. And so
SQL is pull based, you have to pull things.
277:15 - It's not pushed based, okay. So to make this
crystal clear, I have an SQL use case here.
277:24 - And so we have a mobile app, and we have a
web app, and they want to talk to each other.
277:29 - And so using the Avis SDK, the mobile app
sends a message to the queue. And now the
277:35 - the web app, what it has to do is it has to
use the Avis SDK, and pull the queue whenever
277:40 - it wants. So it's up to the this app to code
in how frequently we'll check, it's gonna
277:45 - see if there's anything in the queue. And
if there is a message, it's going to pull
277:49 - it down, do something with it and report back
to the queue that it's consumed it meaning
277:54 - to tell the queue to go ahead and delete that
message from the queue. All right, now this
278:00 - app on the mobile left hand side to know whether
it's been consumed, it's going to have to,
278:04 - on its own schedule, periodically check to
pull to see if that message is still in the
278:09 - queue, if it no longer is, that's how it knows.
So that is the process of using Sq s between
278:14 - two applications. So let's look at some SQL
limits starting with message size. So the
278:24 - message size can be between one byte to 256
kilobytes. If you want to go beyond that message
278:30 - size, you can use the Amazon SQL extended
client library only for Java, it's not for
278:35 - anything else to extend that necessarily up
to two gigabytes in size. And so the way that
278:41 - would work is that the message would be stored
in s3 and the library would reference that
278:45 - s3 object, right? So you're not actually pushing
two gigabytes to Sq s, it's just loosely linking
278:51 - to something in an s3 bucket. Message retention.
So message retention is how long SQL will
278:58 - hold on a message before dropping it from
the queue. And so the message retention by
279:05 - default is four days, and you have a message
retention retention that can be adjusted from
279:09 - a minimum of 60 seconds to a maximum of 14
days. SQL is a queueing system. So let's talk
279:21 - about the two different types of queues. We
have standard queue which allows for a nearly
279:26 - unlimited number of transactions per second
when your transaction is just like messages.
279:30 - And it guarantees that a message will be delivered
at least once. However, the trade off here
279:35 - is that more than one copy of a message could
be potentially delivered. And that would cause
279:38 - things to happen out of order. So if ordering
really matters to you. Just consider there's
279:44 - that caveat here with standard queues, however,
you do get nearly unlimited transactions.
279:47 - So that's a trade off. It does try to provide
its best effort to ensure messages stay generally
279:55 - in the order that they were delivered. But
again, there's no guarantee. Now it If you
280:00 - need a guarantee of the the ordering of messages,
that's where we're going to use feefo, also
280:06 - known as first in first out, well, that's
what it stands for, right. And the idea here
280:11 - is that, you know, a message comes into the
queue and leaves the queue. The trade off
280:15 - here is the number of transactions you can
do per second. So we don't have nearly unlimited
280:21 - per second where we have a cap up to 300.
So there you go. So how do we prevent another
280:33 - app from reading a message while another one
is busy with that message. And the idea behind
280:37 - this is we want to avoid someone doing the
same amount of work that's already being done
280:41 - by somebody else. And that's where visibility
timeout comes into play. So visibility timeout
280:45 - is the period of time that meant that messages
are invisible VSU Sq sq. So when a reader
280:50 - picks up that message, we set a visibility
timeout, which can be between zero to 12 hours.
280:57 - By default, it's 30 seconds, and so no one
else can touch that message. And so what's
281:02 - going to happen is that whoever picked up
that message, they're going to work on it.
281:07 - And they're going to report back to the queue
that, you know, we finished working with it,
281:11 - it's going to get deleted from the queue.
Okay, but what happens, if they don't complete
281:16 - it within the within the visibility timeout
frame, what's going to happen is that message
281:19 - is now going to become visible, and anyone
can pick up that job, okay. And so there is
281:24 - one consideration you have to think of, and
that's when you build out your web apps, that
281:29 - you you bake in the time, so that if if the
job is going to be like if it's if 30 seconds
281:38 - have expired, then you should probably kill
that job, because otherwise you might end
281:42 - up this issue where you have the same messaging
being delivered twice. And that could be an
281:46 - issue. Okay, so just to consideration for
visibility. So in ask us, we have two different
281:55 - ways of doing polling, we have short versus
long. Polling is the method in which we retrieve
282:00 - messages from the queue. And by default, sq
s uses short polling, and short polling returns
282:05 - messages immediately, even if the message
queue being pulled is empty. So short polling
282:11 - can be a bit wasteful, because if there's
nothing to pull, then you're just calling
282:16 - you're just making calls for no particular
reason. But there could be a use case where
282:21 - you need a message right away. So short polling
is the use case you want. But the majority
282:24 - of use cases, the majority of use cases, you
should be using long polling, which is bizarre,
282:29 - that's not by default, but that's what it
is. So long polling waits until a message
282:33 - arrives in the queue, or the long pole timeout
expires. Okay. And long polling makes it inexpensive
282:40 - to retrieve messages from the queue as soon
as messages are available, using long polling
282:45 - will reduce the cost because you can reduce
the number of empty receives, right. So if
282:49 - there's nothing to pull, then you're wasting
your time, right. If you want to enable long
282:55 - polling it, you have to do it within the SDK.
And so what you're doing is you're setting
282:59 - the receive message requests with a wait time.
So by doing that, that's how you set long
283:06 - polling. Let's take a look at our simple queue
service cheat sheet that's going to help you
283:15 - pass your exam. So first, we have Sq S is
a queuing service using messages with a queue
283:20 - so think sidekick or rabbit mq, if that helps
if you know the services, sq S is used for
283:25 - application integration. It lets you decouple
services and apps so that they can talk to
283:31 - each other. Okay, to read Sq s, you need to
pull the queue using the ABS SDK Sq S is not
283:38 - push based. Okay, it's not reactive. SQL supports
both standard and first in first out FIFO
283:45 - queues. Standard queues allow for unlimited
messages per second does not guarantee the
283:50 - order of delivery always delivers at least
once and you must protect against duplicate
283:54 - messages being processed FIFO first in first
out maintains the order messages with a limit
284:00 - of 300. So that's the trade off there. There
are two kinds of polling short by default
284:06 - and long. Short polling returns messages immediately
even if the message queue is being pulled
284:11 - is empty. Long polling waits until messages
arrive in the queue or the long pole time
284:16 - expires. in the majority of cases long polling
is preferred over short polling majority okay.
284:22 - Visibility timeout is the period of the time
that messages are invisible to the Sq sq.
284:28 - messages will be deleted from the queue after
a job has been processed. Before the visibility
284:32 - timeout expires. If the visibility timeout
expires in a job will become visible to the
284:36 - queue again, the default visibility timeout
is 30 seconds. Timeout can be between zero
284:42 - seconds to a maximum of 12 hours. I highlighted
that zero seconds because that is a trick
284:48 - question. Sometimes on the exams. People don't
realize you can do it for zero seconds. sq
284:53 - s can retain messages from 60 seconds to 14
days by default. It is four days so 14 days
285:00 - is two weeks, that's an easy way to remember
it. Message sizes can be between one byte
285:05 - to two and 56 kilobytes. And using the extended
client library for Java can be extended to
285:10 - two gigabytes. So there you go, we're done
with SQL. Hey, this is Andrew Brown from exam
285:19 - Pro. And we are looking at Amazon kinesis,
which is a scalable and durable real time
285:23 - data streaming service to ingest and analyze
data in real time from multiple sources. So
285:28 - again, Amazon kinesis is AWS is fully managed
solution for collecting, processing and analyzing
285:33 - street streaming data in the cloud. So when
you need real time, think kinesis. So some
285:38 - examples where kinesis would be of use stock
prices, game data, social media data, geospatial
285:44 - data, clickstream data, and kinesis has four
types of streams, we have kinesis data streams,
285:51 - kinesis, firehose delivery streams, kinesis,
data analytics, and kinesis video analytics,
285:57 - and we're gonna go through all four of them.
So we're gonna first take a look at kinesis
286:06 - data streams. And the way it works is you
have producers on the left hand side, which
286:11 - are going to produce data, which is going
to send it to the kinesis data stream, and
286:17 - that data stream is going to then ingest that
data, and it has shards, so it's going to
286:22 - take that data and distribute it amongst its
shards. And then it has consumers. And so
286:27 - consumers with data streams, you have to manually
configure those yourself using some code.
286:32 - But the idea is you have these YouTube instances
that are specialized to then consume that
286:37 - data and then send it to something in particular.
So we have a consumer that is specialized
286:42 - to sending data to redshift than dynamodb
than s3, and then EMR, okay, so whatever you
286:48 - want the consumer to send it to, it can send
it wherever it wants. But the great thing
286:54 - about data streams is that when data enters
into the stream, it persists for quite a while.
286:59 - So it will be there for 24 hours, by default,
you could extend it up to 160 68 hours. So
287:06 - if you need to do more with that data, and
you want to run it through multiple consumers,
287:10 - or you want to do something else with it,
you can definitely do that with it. The way
287:15 - you pay for kinesis data streams, it's like
spinning up an EC two instance, except you're
287:18 - spinning up shards, okay. And that's what's
going to be the cost there. So as long as
287:24 - the chart is running, you pay X amount of
costs for X amount of shards. And that is
287:29 - kinesis data stream. So on to kinesis firehose
delivery stream, similar to data streams,
287:39 - but it's a lot simpler. So the way it works
is that it also has producers and those producers
287:44 - send data into kinesis firehose. The difference
here is that as soon as data is ingested,
287:51 - so like a consumer consumes that data, it
immediately disappears from the queue. Okay,
287:57 - so data is not being persisted. The other
trade off here is that you can only choose
288:02 - one consumer. So you have a few options, you
can choose s3, redshift, Elasticsearch, or
288:07 - Splunk. Generally, people are going to be
outputting to s3. So there's a lot more simplicity
288:12 - here. But there's also limitations around
it. The nice thing though, is you don't have
288:15 - to write any code to consume data. But that's
the trade off is you don't have any flexibility
288:21 - on how you want to consume the data, it's
very limited. firehose can do some manipulations
288:26 - to the data that is flowing through it, I
can transform the data. So if you have something
288:33 - where you want it from JSON, you want to convert
it to parkette. There are limited options
288:37 - for this. But the idea is that you can put
it into the right data format, so that if
288:43 - it gets inserted into s3, so maybe Athena
would be consuming that, that it's now in
288:47 - parkette file, which is optimized for Athena,
it can also compress the file. So just simply
288:52 - zip them, right. There's different compression
methods, and it can also secure them. So there's
288:57 - that advantage. Um, the big advantage is firehose
is very inexpensive because you only pay for
289:02 - what you consume. So only data that's ingested
is what you what you pay for, you can think
289:07 - of it like, I don't know, even lambda or fargate.
So the idea is you're not paying for those
289:13 - running shards, okay? It's just simpler to
use. And so if you don't need data retention,
289:18 - it's a very good choice. Okay, on to kinesis
video streams, and as the name implies, it
289:27 - is for ingesting video data. So you have producers,
and that's going to be sending either video
289:32 - or audio encoded data. And that could be from
security cameras, web cameras, or maybe even
289:37 - a mobile phone. And that data is going to
go into kinesis video streams, it's going
289:40 - to secure and retain that encoded data so
that you can consume it from services that
289:46 - are used for analyzing video and audio data.
So you got Sage maker recognition, or maybe
289:51 - you need to use TensorFlow or you have a custom
video processing or you have something that
289:54 - has like HL based video playback. So that's
all there is to it. It's just so You can analyze
290:01 - and process a video streams, applying like
ml or video processing services. Now we're
290:09 - gonna take a look at kinesis data analytics.
And the way it works is that it takes an input
290:16 - stream, and then it has an output stream.
And these can either be firehose or data streams.
290:21 - And the idea is you're going to be passing
information data analytics. And what this
290:25 - service lets you do is it lets you run custom
SQL queries so that you can analyze your data
290:33 - in real time. So if you have to do real time
reporting, this is the service you're going
290:38 - to want to use. The only downside is that
you have to use two streams. So it can get
290:44 - a little bit expensive. But for data analytics,
it's it's really great. So that's all there
290:55 - is to it. It's time to look at kinesis cheat
sheet. So Amazon kinesis is the ADA solution
291:00 - for collecting, processing and analyzing streaming
data in the cloud. When you need real time,
291:05 - think kinesis. There are four types of streams
of the first being kinesis data streams, and
291:11 - that's a you're paying per shard that's running.
So think of an easy to instance, you're always
291:15 - paying for the time it's running. So kinesis
data streams is just like that data can persist
291:20 - within that stream data is ordered, and every
consumer keeps its own position, consumers
291:25 - have to be manually added to that to be coded
to consume, which gives you a lot of custom
291:30 - flexibility. Data persists for 24 hours by
default, up to 168 hours. Now looking at kinesis
291:37 - firehose, you only pay for the data that is
ingested, okay, so think of like, I don't
291:42 - know, lambdas, or fargate. The idea is that
you're not paying for a server that's running
291:46 - all the time, just data that's ingested, data
immediately disappears. Once it's processed
291:51 - consumer, you only have the choice from a
predefined set of services, so either get
291:56 - s3, redshift, Elasticsearch, or Splunk. And
they're not custom. So you're stuck with what
292:01 - you got kinesis data analytics allows you
to perform queries in real time. So it needs
292:06 - kinesis data streams or farfalle firehose
as the input and the output, so you have to
292:11 - have two additional streams to use a service
which makes it a little bit of expensive.
292:16 - And you have kinesis video analytics, which
is for securely, ingesting and storing video
292:21 - and audio uncoated data to consumers such
as Sage maker, recognition or other services
292:27 - to apply machine learning and video processing.
to actually send data to the streams, you
292:34 - have to either use kpl, which is the kinesis
Producer library, which is like a Java library
292:38 - to write to a stream. Or you can write data
to a stream using the ABS SDK kpl is more
292:44 - efficient, but you have to choose what you
need to do in your situation. So there is
292:48 - the kinesis cheat sheet. Hey, this is Andrew
Brown from exam Pro. And we are looking at
292:57 - SYSTEMS MANAGER parameter store, which is
secure hierarchal storage for configuration,
293:02 - data management and secrets management. So
with parameter store, you can store data such
293:09 - as passwords, data, base strings, license
code, and as parameter values. And store configuration
293:14 - data and secure strings in hierarchies. And
track versions of that doesn't make sense
293:18 - it will as we work through this here. And
you can encrypt these parameters using kms.
293:23 - So you can optionally apply encryption, though
that doesn't necessarily mean things are encrypted
293:29 - in parameter store. So to really understand
parameter store, let's go ahead and look at
293:33 - what it takes to create a parameter. So the
first thing is the name. And this is the way
293:39 - you group parameters together based on a naming
convention. So by using the forward slashes,
293:43 - you're creating hierarchies. And this allows
you to fetch parameters at different levels.
293:47 - So if I created a bunch of parameters under
the Ford slash, prod, then when I use the
293:53 - API to do example, application support slash
product, give me all those parameters. So,
293:58 - so interesting way to organize your parameters
into groups. And then you get to choose your
294:04 - tier. And we'll talk about that a little bit
more shortly here. And then you choose the
294:09 - type. So you could have a string, that's just
a string, you could have a string that is
294:13 - a string list, which is a comma separated
string, you can encrypt your string using
294:20 - kms. And then you just provide the value.
So talking about those tears. There are two
294:26 - tiers we have standard and advanced. So generally,
you're using the standard one, and this is
294:31 - scope based on regions. So if you never exceed
10,000 parameters, this parameter store is
294:37 - going to be free. But once you over go over
10,000, you're now using advanced parameters.
294:43 - Or if you need to use a parameter that has
a value higher than four kilobytes, you're
294:48 - gonna have to use advanced parameter. And
if you want to apply parameter policies, you're
294:53 - going to have to use an advanced parameter.
Now a parameter can be applied per or sorry,
294:59 - the advanced here can be applied per parameter.
So you can mix and match these two here. So
295:05 - that is something that's interesting No, but
one thing you do need to know about these
295:08 - advanced parameters is that you can convert
a standard parameter to an advanced parameter
295:14 - at any time, but you can't revert an advanced
parameter to a standard parameter. So it's
295:17 - a one way process. And the reason for that
is because if you were to revert an advanced
295:22 - parameter, you would end up losing data because
you have an advanced parameter that has eight
295:27 - kilobytes, and it just can't go back to four
kilobytes, it would end up truncating the
295:31 - data. So that is the reason for that. Let's
talk about parameter policies, which is a
295:40 - feature of only advanced parameters. So the
idea here is it's going to help force you
295:46 - to update or delete your passwords, it's going
to do this by using asynchronous periodic
295:51 - scans. So after you create these policies,
you don't have to do anything parameter store
295:54 - will take care of the rest. And you can apply
multiple policies to a single parameter. So
295:59 - now that we have a bit of that, let's look
at what policies we have available to us,
296:03 - there's only three at this moment. So the
first one is expiration. So the idea with
296:07 - this policy is you say I want this, this parameter
to expire after this date and time and then
296:13 - it will just auto delete. The next one is
expiration notice, and this one will tell
296:18 - you x days before or hours or minutes before
an expiration is going to happen. So if for
296:25 - whatever reason you need to take action. With
that stored data, this is gonna give you a
296:31 - heads up. The last one is no change notification.
So let's say you're supposed to have a parameter
296:36 - that's supposed to be modified manually by
a developer. So they're supposed to update
296:41 - it themselves. After X amount of days or minutes
or hours, this parameter will tell you, hey,
296:46 - nothing has changed. So maybe we should go
ahead and investigate. So that is parameter
296:52 - policies. So to understand how the hierarchy
works with parameter store, I want to show
297:01 - you using the COI. So the first thing we want
to do is we want to create some parameters.
297:07 - So using the put parameter command, we can
just supply the name. And that's going to
297:13 - be how we define our hierarchy here, I'm going
to provide some values, we're going to store
297:17 - them as strings. And so when you run each
of these commands, it's actually going to
297:22 - tell you what version, it is, if you keep
on doing a put on the same thing, that version
297:28 - count is going to go up. And this allows you
to access older versions, because everything
297:32 - in parameter store is automatically versioned.
So we saved we put three parameters here on
297:38 - Vulcan, so how would we actually get all the
parameters in one go. And that's where we
297:44 - use get parameters by path. so here we can
just specify planets, Ford slash Vulcan, and
297:49 - all the parameters underneath will be returned
to us. So here they are. So you can see we
297:54 - have all of them there. So that's all it takes
to get your parameters. And that's how you
297:59 - generally use this within your application.
So we are using the CLR here, so you'd have
298:05 - to translate this over to the SDK. But this
is how you would get parameters in your application.
298:13 - Hey, this is Andrew Brown from exam Pro. And
we are looking at secrets manager, which is
298:20 - used to protect secrets needed to access your
applications and services. So easily rotate
298:24 - managed and retrieved database credentials,
API keys and other secrets through their lifecycle.
298:31 - So for secrets manager, you're going to generally
want to use it to automatically store and
298:35 - rotate database credentials. They say they
do API keys and other things like that. But
298:40 - really, this is where secrets manager shines.
So you know, the database is available to
298:45 - us as RDS or redshift document dB. Then we
have other databases, which we'll look at
298:50 - closely here in a second. And then you have
key value, which they say is for API's. So
298:55 - what you do is you go ahead and select the
secret type that you want to do for RDS, redshift
299:01 - and document dB. It's very straightforward
for other database and other types of secrets
299:06 - that are a little bit different. So let's
look at those in greater detail here. So selecting
299:11 - for credentials for other database, you can
see you can select a specific type of relational
299:16 - engine. But here you're providing the server
address, the database name and the port. So
299:21 - for the other three managed ones, you wouldn't
do that you just provide the username, password
299:25 - and select the resource within your AWS account.
And then for the other types of secrets, this
299:31 - is just a key value if you go over to plain
text. That's not that doesn't mean you can
299:36 - encrypt a plain text file, it just is another
representation of that key and value. So you
299:41 - can just work with a JSON object. But yeah,
those are all the types there. Just a few
299:47 - other things to highlight about secrets manager
and that is when you go ahead and create any
299:52 - credential in encryption is enforced. So with
parameter store, it doesn't necessarily have
299:58 - to be encrypted but with secrets manager,
it has be encrypted at rest. And you can just
300:02 - use the default encryption key or use a different
CMK. If you want to go make one, the pricing
300:08 - is pretty simple. So it's point 40 cents USD
per secret per month. So some people don't
300:15 - like secrets manager because it costs that
and you can pretty much use parameter store
300:19 - for free. But, you know, you have to decide
what makes sense for you. And it's a half
300:25 - a cent per 10,000 API calls there. And then
one thing to note is that if you want to monitor
300:31 - the credentials access, in case you need to
audit them, or just investigate, if you have
300:36 - a cloud trail created, then it will monitor
the secrets for you. So you can investigate
300:41 - in the future, probably a good idea to turn
on cloud trail. So the huge value with secrets
300:51 - manager is automatic rotation. So you can
set up automatic rotation for any database
300:56 - credential. So any of the managed services
and even the other databases there. There's
301:00 - no automatic rotation for the key value, secret
type. So it's just the database stuff. So
301:07 - you know, once you go through the wizard,
for any of those steps are going to come to
301:11 - this automatic rotation. So we're going to
go ahead and enable it, you're going to choose
301:14 - your rotation interval, which can be up to
365 days, up to one year. So just to show
301:21 - you here, if you expand your 360 90. And you
have custom, if you go to custom, you can
301:25 - set it up to one year. And the way secrets
manager works is that it just creates a lambda
301:30 - function for you. And so some would argue
that if you wanted to use parameter store,
301:36 - just make your own lambda, you wouldn't need
to use secrets manager and that is 100%. True.
301:40 - But you know, you have to know how to do that.
So you know, decide whether you want to put
301:44 - an extra effort, so you don't have to pay
for secrets manager. But yeah, it will go
301:49 - ahead and do that. And there's one option
down below here. And this allows you to select
301:53 - what password you're going to rotate out,
because you might not want to rotate out this
301:58 - path for you might want to rotate out a developer's
password that's connected to the database.
302:03 - So you know, that's another option for you
there. For the developer associate, it's good
302:13 - to know the COI commands, let's just look
at a couple for secrets manager, the first
302:17 - one being intimate secrets manager describes
secret. And this is going to describe information
302:23 - about a particular secret. The reason you'd
want to do this is so that you could find
302:27 - out what version IDs are there. Because you
might want to specify a specific version of
302:33 - a secret. And then you get some access information
such as the last time it was changed or accessed.
302:39 - So that might be one precursor step before
you actually try to get the secret. So the
302:43 - other ci command we need to know is get secret
value. And this actually gets the secret.
302:49 - So you can see you supply the secret ID and
then the version stage. And if you don't provide
302:55 - virgin sage, it would just default to innovyz
current, which is the current version. But
302:59 - if you use the prior step there, you could
use a different version stage there. And so
303:05 - using the COI command, you can see that we
have this secret string string. And that is
303:11 - that is what storing our credential information.
So in this case, it's just a key value. credential
303:19 - are secret. And so that's what we're looking
at here. to really help make sense of secrets
303:29 - manager, because it's not always clear, you
know, how do you access the secrets versus
303:33 - how does the database access it versus how
does the app access it? You know, I made an
303:38 - architectural diagram here for you. So you
know, the first thing is that you do secrets
303:41 - manager, and we'd set up rotation with it.
So every 90 days, it's going to rotate out
303:48 - that password that is stored in RDS. So the
password is actually an RDS. And then secrets
303:53 - manager can store a version of it as well.
But what it's doing is the lambda probably
303:59 - set up with a cloud watch event. And every
nine days it's going to say, okay, run this
304:03 - lambda, swap out that password. So that's
how the password gets swapped in RDS. But
304:07 - how does the application access it? Well,
you'd have your web app running on EC two,
304:12 - and you'd use the SP DK, which is the icon
there in the actual EC two instance here.
304:17 - And you would make an SDK call to secrets
manager and get the database credentials.
304:22 - And then you use your web server with that
new password and username and form a connection
304:28 - URL, which is just a it looks like an HTML
URL. But it's used to connect to Postgres
304:35 - databases. Not all relational database types
have a connection URL. So you might just have
304:39 - to provide the username and password. But
the point is, you are making connection to
304:44 - RDS. Now from the developer side, the way
they would probably use it, because they're
304:50 - in development. They just want to gain access
to the, the database that's in the cloud.
304:54 - They're going to use the COI to obtain the
database credentials and we just went through
304:58 - the COI commands so you have an idea They
would run, and then the way they connect to
305:02 - the database would be using a database manager.
So you can use table. Plus, if you didn't
305:07 - have a database manager, you could just go
ahead and use terminal and, and, and connect
305:13 - that way there. So hopefully that makes it
very clear in terms of how you can use secrets
305:19 - manager for all these use cases. Hey, this
is Angie brown from exam Pro. And we are looking
305:28 - at dynamodb, which is a key value and document
no SQL database, which can guarantee consistent
305:33 - reads and writes at any scale. So to really
understand dynamodb, we need to understand
305:39 - what is no SQL and I can tell you what it's
not It is neither a relational database and
305:45 - does not use SQL to query the data for results.
And the key thing that's different is how
305:51 - the data is stored. So it can be either key
value or documents. So looking first here,
305:56 - a key value store, this is a form of data
storage, which has a key which references
306:01 - a value and nothing more. Okay, so that is
one data structure you can have in Dynamo
306:06 - dB. The other one is documents stored. So
this is a form of data storage, which has
306:10 - a nested data structure. There. So that is
what they call the document. So hopefully
306:17 - that makes a bit of sense. So dynamodb is
a no SQL key value in document database for
306:24 - internet scale applications. And it has a
lot of great features. It's fully managed
306:28 - multi region, multi master durable database
built in security, backup and restore an in
306:33 - memory caching. So you can see why this is
AWS is flagship database that they're always
306:38 - promoting, because it has so much functionality.
at scale, it can provide eventual consistent
306:44 - reads and strongly consistent reads, which
we will talk about in the Dynamo DB section
306:48 - here. So don't worry if that doesn't make
sense to just as of yet. And you can specify
306:52 - the Read and Write capacity per second. So
whatever you need, you just say I need 100
306:58 - reads and writes per second. And then you
just pay that cost. So there they are. And
307:03 - we will again, we'll talk about this in greater
detail in the dynamodb section. And the last
307:08 - thing I want you to know is that the data
is stored at least across three different
307:13 - regions on SSD storage. So there's a really
fast drives, which makes your data extremely
307:19 - durable to failure. So there you go. Okay,
so I made one really, really big mistake here,
307:26 - which I've corrected here. And it's the fact
that all data is stored on SSD storage and
307:30 - is spread across three different easy's The
reason I thought it was regions was because
307:36 - when I read the documentation, it said geographical
locations, it didn't really say azs. And so
307:41 - I took the guess and thought it was region.
But when I thought about it, and I you know,
307:44 - I talked to some other people that know dynamodb
better than me. They pointed out that no,
307:48 - it's going to be data centers. And that makes
sense. Because if you have a feature called
307:52 - Global tables and allows you to copy to other
regions, it just doesn't make sense. So sorry
307:58 - for that mistake. But it's actually three
different Z's. It's technically three different
308:03 - data centers, but we'll just call these Z's
for the sake of simplicity. And yeah, that's
308:08 - that correction there. Let's take a look at
what a dynamodb table looks like and understand
308:17 - all the components that are involved. So the
first thing is the table itself. And tables
308:21 - contain rows and columns, but we don't call
them rows and columns dynamodb, they have
308:24 - different names. So that is the entire table
there. So we have items, and that is the name
308:29 - for rows. So there we have a row, which is
a single item. And then you have attributes,
308:35 - and that is the name for columns. I don't
point it out there. But imagine the entire
308:39 - year column as the attributes, then you have
the keys. And that's the name of these columns.
308:46 - So up here we have IMDb IDs, that's naming
the key. And then you have values and that
308:51 - is the actual data itself. So there's an example
of some data. So hopefully that makes it very
308:57 - clear how what the structure is for dynamodb.
So dynamodb, it replicates your database across
309:08 - three regions onto three separate hard drives.
And this allows for high availability, so
309:15 - you don't have data loss. But this comes comes
with a trade off because when you need to
309:19 - update your data, it's going to have to write
updates to all those copies. And it's possible
309:25 - for data to be inconsistent when reading from
a copy which has yet to be updated. So the
309:31 - way you work around this is with choosing
your read consistency with dynamodb. You can
309:36 - choose between two options. You can choose
eventual consistent reads, which is the default
309:41 - or strongly consistent reads. So let's talk
about eventual consistent reads first. So
309:46 - when copies are being updated, it is possible
for you to read and be returned an inconsistent
309:51 - copy. Okay, because because you're reading
from a database which has yet to be updated,
309:58 - but reads are super fast because you're not
waiting for data to become consistent. So
310:04 - you can read it immediately. But there's no
guarantee of consistency. Now, the time it
310:09 - takes for everything to become consistent
is, is around a second. So if you're building
310:15 - application, and you can wait up to a second
after read, that's how you would ensure that
310:20 - your data is up to date. Or maybe you have
an application where it'd be inconsistent
310:24 - isn't a deal breaker, so it doesn't really
matter. Now, if consistency is extremely important
310:29 - to you, this is where strongly consistent
reads are going to come into play. So this
310:32 - is when copies are being updated, and you
attempt to read it, but it's not going to
310:36 - return it unless all the copies are consistent.
So the trade off here is you're going to have
310:42 - a guarantee of consistency, it's always going
to be consistent. But you're going to have
310:47 - to wait longer for that, that read to come
back to you. So it's gonna be a slower read.
310:53 - And all copies will be consistent within a
second. That's a guarantee that it was gives
310:57 - you. So there you go, that's reconsidered.
Let's look at dynamodb partitions. And so
311:07 - what is that partition? It's a allocation
of storage for a table backed by a solid state
311:11 - drive and automatically replicated across
azs within an AWS region, and that is a device's
311:17 - definition. So let's take a look at my definition,
because I think that's a bit easier to digest.
311:22 - And that is a partition. partition is when
you slice your table up into smaller chunks
311:27 - of data or partition. And the purpose of this
is to speed up reads for very large tables
311:32 - by logically grouping similar data together.
So imagine you have a table and this table
311:38 - is giant. And so it would be faster if you
could partition it. And so the idea is that
311:44 - maybe this data would go to partition A, this
data would go to partition B and this data,
311:48 - we go to partition C. And if you're wondering,
well, how does it choose which tables and
311:52 - stuff like that, that's what we're going to
find out next. So looking at partitions a
311:56 - bit more here, dynamodb automatically creates
those partitions for you as your data grows.
312:01 - And you're going to start off with a single
partition. And there's two cases where dynamodb
312:06 - will create additional partitions for you,
that's when you exceed 10 gigabytes of data
312:11 - in a table, or if you exceed the RC use or
WC use for a single partition. And, you know,
312:18 - each partition has a maximum of 3000, RC use
or 1000. WCS, if you're wondering what those
312:24 - acronyms mean, it means read capacity units,
and write capacity units. So an example of
312:28 - setting up those capacity units would be down
here below. So you have a table. And here
312:34 - I'm saying the reads and then I'm setting
the rights. And the way it will work is when
312:39 - you split, when you get a new partition, it's
going to split the reads and writes across
312:43 - those tables. So if you go over that mark,
this is exactly where it would hit that threshold.
313:00 - So it would split. So when you create a table,
you have to define a primary key. And this
313:16 - key is going to determine where and how your
data will be stored in partitions. And it's
313:21 - important to note that the primary key cannot
be changed later. So designing your primary
313:25 - key, making the right choice early on is extremely
important because you only get one shot at
313:30 - this. So here is an example of the console
where you create your primary key. And you'd
313:37 - would define a partition key that's going
to determine which partition of data should
313:41 - be written to, then you have a sore key, which
is optional. This is how your data should
313:46 - be sorted on a partition, you're going to
notice that there are some date types here.
313:53 - And I have a third key that's a date. But
notice that there isn't a date type in dynamodb.
313:58 - So in that case, you use a string. So just
be aware that there is no data type. And there
314:03 - are two types of primary keys. There's one
which is called a simple primary key, that's
314:08 - where you only use a partition. And then you
have a composite primary key. That's where
314:13 - you have a partition and a sort key that makes
up your primary key. So let's talk about how
314:23 - a primary key with only a partition key chooses
which partition it should write data to and
314:27 - again, if you only have a partition key, that
makes it a simple primary key. So the first
314:32 - thing is we need some data, and we want to
get it into a particular partition. And so
314:39 - we make our our primary key and all it has
is a partition defined, and we want to choose
314:45 - a value that is unique. So IDs are really
great value because that's going to be extremely
314:51 - unique. And that is very important when designing
simple primary keys. So the way this is going
314:56 - to work is AWS has this thing called the Dynamo
dB. Internal hash function. And we don't actually
315:02 - know how this thing works, because it's a
secret. But it's an algorithm that decides
315:06 - which partition to write data to. And so it
needs your data and it needs your primary
315:12 - key to figure that out. So those two things
go in there. And then it writes to whatever
315:16 - partition it decides to write to. So you know,
that is how that works for a simple primary
315:23 - key. Now, we're going to take a look at how
a primary key with a partition and a sarki
315:31 - chooses which partition it should write data
to. And again, a partition and sort key is
315:37 - known as a composite primary key. So we're
going to need some data. And then we're gonna
315:42 - have to define our key here. And so you'll
notice that we're filling in both the partition
315:46 - key and also the sort key value. And what's
important is that the combination of the partition
315:52 - and sort key has to be unique. So in the case
of simple primary key, we wanted the partition
315:57 - key to be unique, but it has to be unique
in the scope of the two combined. And then
316:02 - we have our internal hash function, which
again, is a secret, nobody knows how it works.
316:06 - I've reached out on Twitter asked dynamodb,
they won't tell me, which is great for security.
316:12 - And so what we're going to do is take our
primary key and our data and it's going to
316:16 - get passed to that internal hash function.
This happens when you just write to dynamodb.
316:21 - You don't have to literally call it and then
it's going to decide to put it in partition
316:24 - See, but this is a little bit different, where
the composite or the simple primary key, I
316:29 - just put it in a random partition. This one
is putting it with data that is similar to
316:35 - it. So here we have an alien that is Romulan,
and it's grouping it together with other Romulans.
316:40 - There, and it's also sorting that data from
A to Z. So the data is close together so that
316:46 - it's faster to access it. So that's that is
the idea behind the composite primary key.
316:51 - And you know how it figures out what partitions
you go. Let's talk about primary key designs,
317:00 - we're going to be talking about simple keys
and the composite key. Again, simple keys
317:04 - is only a partition key. And a composite key
is made up of both a partition and sort key.
317:09 - So for simple, what's important to remember
is that no two items can have the same partition
317:14 - key. So here, let's say the ID is the partition
key, you can see that two values are the same,
317:20 - that's not going to work out. But here they
are different. So that is great. Then on the
317:24 - composite key side, we care about two items,
where they can have the same partition key,
317:31 - but the partition and sort key combined must
be unique. So here, let's say alien is the
317:37 - partition and name is the sort, they're both
the same, that's not going to work out. But
317:42 - here, you know, the partition key is the same,
but the sore key is different. And that's
317:47 - alright, so that's going to work out great.
So there's two things when designing your
317:50 - primary key that you have to keep in mind
you want them to be distinct, the key should
317:54 - be is as unique as possible. And the other
thing is you want the key so that it will
318:00 - evenly distribute data amongst partitions.
And that is based on how much things will
318:06 - group together based on your so and this is
more so even with the the composite key because
318:13 - when you can group things based on that partition
key, you want that to be as unique as possible.
318:18 - So that it's more even, you don't want 90%
of your records being one thing and then 10%
318:23 - being the other, you want it as even as possible.
So there you go. query and scan are two functions
318:33 - that are going to be using a lot in Dynamo
dB. And you really need to get familiar with
318:37 - these two functions. And so what I recommend
is opening up the dynamodb console, because
318:44 - they have under these, this items, this way
of exploring data where you can query. So
318:49 - there you can choose whether you want to query
your scan, and then based on what you choose
318:52 - scan has very few options. But with query,
you have a lot of things you can choose to
318:56 - explore based on the partition key. And just
look at what you can sort when you drop down
319:01 - here. So by playing around with this, you're
going to really understand in a practical
319:07 - way what you can do. But let's jump into learning
a little bit more about query and scan. So
319:17 - let's talk about the query function. So this
allows you to find items in a table based
319:21 - on the primary key values, and allows you
to query a table or secondary index that has
319:26 - a composite primary key. So going back to
the actual console, you can see that you have
319:33 - to or you have the choice of a prior partition
and a sort. And we have some filtration options.
319:39 - So anything you can do there is going to really
help you understand what you can do. By default,
319:44 - the reads are using eventual consistency.
So if you want the reads to be strongly consistent,
319:51 - while querying, you can pass a consistent
read equals true when you're using it in the
319:58 - SDK or the COI By default, it returns all
the attributes for the items. So that's all
320:03 - of the columns. And you can filter those out
by using projected expressions to only get
320:07 - the columns you want, definitely something
you're going to want to do. And by default,
320:11 - all the all the data will be returned to you
in ascending order, that's a to z. If you
320:16 - want it in descending order that Zed to a,
you can use scan index forward false to do
320:21 - that, but you can see in the console, you
can just click ascending or descending, it's
320:24 - a lot easier there. I just want to give you
an example of a payload. So this is an example
320:32 - where if you're configuring the CLR SDK, these
are the attributes you could pass out what
320:36 - you have here a consistent read, you're saying
we want it to be true projected expression,
320:40 - we're saying I only want ID name created at
and updated, scanning explored, we want to
320:45 - be returned in reverse, and we can limit it.
So we say I only want 20. There's a lot of
320:49 - different options. But these are the most
important ones for you to know. Now, we're
320:54 - going to take a look at the dynamodb scan
function. So this allows you to scan through
321:00 - all items and returns one or more items through
filters. And by default returns all the triggers
321:05 - for items. So if we were to try to do a scan,
within the dynamodb console, you can see all
321:11 - we have is the ability to add filters, it's
really, really simple. scans can be performed
321:18 - on tables and secondary indexes, they can
return specific attributes. And by using projected
321:23 - expressions, we can then limit it just like
query. So just like the other one, scan operations
321:29 - are sequential, you can speed up a scan through
parallel scans using segments and total segments.
321:35 - And this is important because when you do
a scan, it's going to return everything. And
321:40 - so you know, doesn't matter how many records,
they'll just return everything. And so that's
321:45 - why this functionality is so important. But
I'm going to tell you right now, you're going
321:49 - to want to avoid using scans when possible.
They're much less efficient than running a
321:55 - query, because, as I just said, returns everything.
As the table grows, the scans take longer
322:00 - to complete, which makes total sense. And
a large table can use all your provision throughput
322:05 - in a single scan. So you know, avoid scans
when possible, but they are there for you.
322:16 - So one thing we have to do when we make a
new table is choose its capacity, whether
322:21 - it's provisioned or on demand. And we're going
to first look at provision throughput capacity.
322:25 - And this is the maximum amount of capacity
your application is allowed to read or write
322:28 - per second from a table or index. So here
is an example of a table where we've chosen
322:34 - to use provisioned capacity. And down below,
we have the option to choose or to fill in
322:41 - our provision capacity and zooming in to tell
us what's going to cost us per month at that
322:45 - setting. So throughput is measured in capacity
units. So we have RC use, which stands for
322:51 - read capacity unit and WC use, which stands
for right capacity unit. So you'll see those
322:55 - abbreviations all over the place. And one
thing that we can do, and you'll notice there's
323:01 - an option for auto scaling. So if we were
to turn that on, this allows us to scale our
323:05 - capacity up and down based on utilization.
So if we need to go beyond that five or 10,
323:11 - we'll just set that and we can say the minimum
and the maximum. And the reason why you'd
323:14 - want to turn on auto scaling is to avoid throttling.
So if you were to go beyond the capacity that
323:20 - you're set, and let's say you didn't have
auto scaling turned on and you went beyond
323:24 - five reads per second, that data is going
to get throttled. And that means those requests
323:28 - are going to be dropped. So it's literally
data loss, they're not going to make into
323:31 - your database. So auto scaling gives you a
bit more wiggle room. But you know, there
323:36 - are limitations to it. So there you go. That's
provisioned capacity. So let's take a look
323:45 - at on demand capacity. And this is where you
pay per request. So you pay only for what
323:49 - you use. So over here, we've have it set to
on demand, you're going to notice that we
323:54 - cannot set the provision capacity, we cannot
set the auto scaling because this is all going
323:57 - to happen for you automatically. And on demand
is really good for cases where you have a
324:03 - new table, and you don't know how it's going
to turn out, you don't know what you should
324:06 - set your capacity to. So it's just easier
to go on demand, or you're never gonna have
324:11 - predictable traffic is going to be all over
the place. You don't want to be throttled
324:15 - and lose traffic and that auto scaling group
is not going to work because it's just too
324:20 - erratic. Or you know, you just like the idea
of paying for only what you use, because dynamodb
324:26 - can get pretty darn expensive at scale. And
the only limitations that are applied to you
324:32 - is whatever is the default on the upper limits
for a table and the upper limit is 40,000
324:36 - Arts use and 40,000 wc use so that's the worst,
the worst damage someone could do to you.
324:42 - And what I mean by that when I say damage
is that there is no hard limit imposed by
324:47 - on demand. So if you get a lot of traffic
and it requires 40,000 RC use, it's going
324:52 - to spin up to that. So you just have to be
careful that you know you don't have runaway
324:57 - traffic here because that will add Have you
end up with a very large bill. But again on
325:03 - demand is still a really good feature because
it gives you a lot more flexibility. So in
325:13 - Dynamo dB, it's important for us to know how
to calculate the reads or the rights. So let's
325:18 - start with read capacity units are read capacity
that represents one strongly consistent read
325:22 - per second or two eventually consistent reads
per second for an item up to four kilobytes
325:27 - in size. So the whole point is figuring out
what to put in this box. And so if we had
325:33 - data that was four kilobytes, or data or less,
at 10, this would equal 10, strongly consistent
325:38 - reads, or 20, eventually consistent reads,
but if it shows up in the exam, they're not
325:42 - gonna, they're gonna not ask you this stuff,
they're gonna ask you how to calculate this
325:46 - number here. And that's what we're gonna figure
out. But remember that we have consistent
325:50 - reads and strong reads. So we have to have
two different formulas for calculating this
325:56 - number here. So let's first look at how to
calculate RCS for strong, strongly consistent
326:02 - reads. And the formula is going to be we're
going to round our data up to the nearest
326:05 - four, we're going to divide by four. And then
we're going to times by the number of reads.
326:10 - So let's go through three examples. The first
being 50, reads at 40 kilobytes per item.
326:15 - So we don't need to round it up to four, because
40 is already divisible by four. And we're
326:21 - going to divide that by four giving us 10
timesing, that by 50, that's how many reads
326:25 - we have. And that's going to be the number
that goes up in here 500 RC use. The next
326:31 - one here is 10 reads at six kilobytes per
item. So six needs to get rounded up to eight,
326:36 - that eight is divided by four, which turns
into two, so we're going to take that two
326:40 - times two by 10. And that's going to be 20
rs use so that 20 is going to go up into that
326:45 - box there. The last one here we have 33 reads
at 17 kilobytes per item. And so 17 kilobytes
326:53 - need to be rounded up to 2020 is divisible
by four becomes four, and then we times four
326:59 - by the number of reads, which is 33. And that's
going to give us 132 RC use. So we're going
327:05 - to now look at how to calculate RCS for eventually
consistent reads. And remember that for each
327:11 - RCU, we get to eventually consistent reads
per second. So the formula is going to vary
327:15 - here, but going to be pretty darn similar.
So the first thing is we're going to round
327:19 - the data up to the nearest four, we're going
to divide by four, we're going to times by
327:24 - the number of reads that we're going to divide
by the final number by two and then we have
327:28 - to round up to the nearest whole number. So
looking at the first example, we have 50 reads
327:33 - at 40 kilobytes per item. So 40 is already
divisible by four, so we're going to divide
327:39 - by four, which gives us 10, we're going to
times by 50, which gives us 500, then divide
327:44 - by two, and that's going to give us 250 our
CPUs. The next example, we have 11 reads at
327:52 - nine kilobytes per item, so we're going to
around up a nine to 12, which is the nearest
327:57 - four, which gives us and then we're gonna
divide that by four, which gives us three,
328:01 - three times 11. That's the amount of reads,
so that's 33, we're gonna divide by two, which
328:08 - gives us 16.5. And then we'll have to round
that up to the whole number. So that's 17.
328:12 - So we're at 17 RC use. And then one more example
here. So let's say we have 14 reads at 24
328:20 - kilobytes per item. So 24 is divisible by
four, so that gives us five, five divided
328:25 - by four, or sorry, 24 divided by four is five.
And then five times five times 14 is 70. And
328:33 - then divide that by two uses 35 RC use. So
I know math isn't fun, but it is something
328:40 - you definitely need to learn. So just drill
through the stuff until you get it. And there
328:44 - you go. Now let's look at how to calculate,
right, so this is for right capacity units.
328:54 - And a right capacity unit represents one item
per second for an item up to one kilobyte.
328:59 - So if we have a right capacity of 10, and
we have one kilobyte or data of last, that's
329:05 - gonna equal 10 rights, so I wonder if you
notice, but this form is gonna be a lot easier
329:09 - than the reads. So how to calculate WC us,
we're going to round our data up to the nearest
329:15 - one kilobyte, we're going to times by the
number of rights. And that's it. So our first
329:20 - example here we have 50 rights at 40 kilobytes
per item. So that's 50 times 40, which is
329:25 - 2000 wc use. Our next example here is 11 rights
at one kilobyte per item. So that's one times
329:31 - 11 equals 11. So super easy. The last one
a little bit tricky here, but we have 18 writes
329:37 - at 500 bytes. So we round up the 500 bytes
to one kilobyte times that by 18 we have 18
329:44 - WSU so super, super easy. Just remember that
writes pretty much it's just times by whatever.
329:51 - Now we're gonna take a look at global tables
and global tables provides a fully managed
329:58 - solution for deployment. You To multi region
multi master database without having to build
330:03 - and maintain your own replication solution.
So this is a very powerful tool that dynamodb
330:08 - has when you want to go global. And in order
to use global tables, there are three things
330:14 - you must meet. So you need to use kms CMK.
So you need to have a custom master custom
330:20 - master key with kms. You need to enable streams
and stream type, it has to be new and old
330:25 - image, I believe. But something has to be
set there. So that would show up here on the
330:30 - right hand side. So you can see once you have
three checkboxes, you are good to go. And
330:36 - then you're able to create global tables.
So all you got to do is add the region that
330:40 - you want, and choose it. And that's it. So
global tables are very easy to utilize. It's
330:46 - just more of that activation process. But
just remember what global tables are for they're
330:50 - for deploying a multi region multi master
database without having to build and maintain
330:54 - your own replication solution. Dynamo DB has
support for transaction. So what is a transaction,
331:05 - this represents a change that will occur to
the database, if any dependent conditions
331:09 - failed, and then a transaction will roll back
as if the database changes never occurred.
331:14 - I think at one point dynamodb didn't have
transactions. And that was one of the reasons
331:19 - why people were like, well, that's why we
use RDS databases, or sorry, relational databases,
331:25 - because they're acid compliant. But looks
like dynamodb has bolted on that functionality.
331:30 - So that is great. And if you're wondering
what acid stands for, it is for a thomassie.
331:35 - Thomas CD, I can't say it consistency, isolation
and durability. So what I want to do is give
331:41 - you an example of a transaction so you can
conceptualize it. And these transactions work
331:46 - really well like especially when you're thinking
about money, where you have to have multiple
331:50 - steps before something goes through before
you release money. So the way it works is
331:54 - that if any of these steps fail, then the
transaction will immediately fail and rollback
331:59 - the changes. So the first thing is we create
a new payee. And then we go ahead and we verify
332:05 - that the email is correct format before sending
out the money. But it turns out that it's
332:09 - not. And so what happens is it stops and rolls
back. So none of these actions actually occurred.
332:15 - So that is in a nutshell transactions, but
let's look at it in more detail. So now we're
332:21 - going to look at how dynamodb transactions
work since we just covered conceptually what
332:25 - transactions are. So dynamodb offers the ability
to perform transactions at no additional cost
332:32 - using two functions they have which is transact
write items and transact get items. You'll
332:38 - notice one says right and one says get. And
that's when you want to group together a bunch
332:43 - of right actions or get actions. So those
are your limitations there. The transactions
332:49 - allow for all or nothing changes to multiple
items both within and across tables. Notice
332:55 - that it says across so you can do this with
multiple tables. dynamodb will perform two
333:01 - underlying reads or writes for every item
in the transaction, one to prepare the transaction,
333:06 - one to commit the transaction, so that is
going to consume your RC use or WC use. So
333:12 - there is a little bit of extra cost there.
But it's negligible, you shouldn't even think
333:15 - about it. These two underlying read and write
operations are visible in your Amazon cloudwatch
333:20 - metrics. So if you're wondering, you want
to keep track of that stuff, that's where
333:22 - you can check it. You can use condition check
with dynamodb transactions to do pre conditional
333:27 - check. If that doesn't make sense, this is
another explanation. So it checks that an
333:32 - item exists or to check the condition of a
specific specific attributes of the item.
333:37 - So it's just a way of doing a check before
you run that transaction. So that is the specifics
333:43 - of a dynamodb transit. So TTL, which stands
for Time To Live allows you to have items
333:54 - in your dynamodb expire after a given amount
of time. And so when I say expire, I mean
333:59 - they get deleted. This is great if you want
to keep your databases small and manageable.
334:05 - Or let's say you're working with temporary,
but continuous data examples could be session
334:09 - data, event logs, or just based on your usage
patterns. So the way you're going to enable
334:15 - Time To Live is in your dynamodb table, you're
going to click on TTL. And then you have to
334:22 - enable it and you need to provide it a attribute.
So here I'm providing expires that which is
334:28 - a string that has to be in a date time format.
And this is what's going to be used to determine
334:34 - when a record should be deleted doubt if you've
been paying close attention to the dynamodb
334:39 - section, you know that there is no date time.
data structure and everything are strings,
334:45 - because dynamodb just doesn't have a date
time datatype so it's important that you use
334:51 - a you format the string in epoch format, okay,
and so if you're not familiar with that, this
334:58 - is just a I think was Is ISO 8061. And what
you'd have to do is convert this into a POC,
335:05 - which looks like this. It's a bunch of numbers.
So there, you'd have to programmatically do
335:09 - this, or you'd have to use it online calculator.
But the advantage of TTL also is that it'll
335:15 - save you money, because the smaller your database
is, the less likely you'll have partitions
335:18 - and the more money you will save. So there
you go. dynamodb has a functionality called
335:28 - streams. And when you enable streams on a
table, dynamodb is going to capture every
335:32 - modification to a data to data items so that
you can react to those changes. So if an insert,
335:39 - update or delete occurs, that change will
be captured and then sent to a lambda function.
335:44 - So changes are sent in batches at a time to
your custom lambda changes are sent to your
335:49 - custom lambda in near real time, each stream
record appears exactly once in the stream
335:55 - for each item that is modify the stream records,
records appear in the same sequence as the
336:01 - actual modification. So here's an example.
Let's say I update an item. So I'm updating
336:06 - chief O'Brien, this could be insert, and it
gets inserted in the database. But now we're
336:11 - going to react to that insert and send that
data to dynamodb stream. And that dynamodb
336:18 - stream is configured to send it to a lambda.
And when that goes that lambda, we can do
336:23 - anything we want with that data. So we could
have it so that it sends out an email, or
336:27 - you get sent to kinesis, firehose, or whatever
we want to program. So it's just a way of
336:33 - reacting to inserts, updates, and deletes.
So there are a couple of errors, I want to
336:42 - review with you on dynamodb that I think you
should know the first one being throttling
336:46 - exception. So the rate of requests exceeds
the allowed throughput. This exception might
336:50 - be returned if you perform any of the following
operations to rapidly so CREATE TABLE update
336:54 - table delete table, this is likely to happen,
I would say with update table because it's
337:00 - not it's not frequent, that you're creating
tables or deleting tables, but it's possible,
337:05 - you might send multiple actions to an update
table. The other error I want you to know
337:10 - and this one is extremely common. It's provision
throughput exceeded exception. So you exceeded
337:15 - your maximum allowed provision throughput
for it for a table or for one or more global
337:20 - secondary indexes. This error occurs when
you've exceeded your, your throughput. So
337:28 - that's your capacity units, your reads or
your rights. And so you're very likely to
337:33 - see this error. When an error does occur on
dynamodb, you're gonna likely be accessing
337:39 - dynamodb via the AWS SDK. So writing programmatically
your code, and when it when an error fails,
337:46 - the SDK has a built in so it will automatically
retry when something failed, it'll try it
337:52 - again, as well, it will implement exponential
back off. Have you ever heard of this before,
337:57 - the idea is that I've encountered an error,
I'm going to wait 50 milliseconds before I
338:02 - try again. And if that one fails, I'm going
to try 100 milliseconds. And if that one fails,
338:07 - I'm going to keep on doubling that time. And
here, it's going to update it up to a minute
338:12 - before it stops. So this is just a strategy
for trying to make sure changes make their
338:17 - way through. So if you're using SDK that ensures
that you're not going to lose data, because
338:23 - it's going to try and try again. And so I
want to point out that these two exceptions
338:28 - are important because they're very likely
shopping your exam. Very, very likely provision,
338:34 - throughput exceeded exception. And notice
that it says provisioned throughput. So remember,
338:39 - there's a there's two capacity modes, we have
throughput, provision, throughput, and on
338:45 - demand. So this error provision throughput
would never happen for on demand for on demand,
338:48 - this error could never occur. On maybe it
would occur if you exceeded the 40,000 RCU,
338:56 - or w are the right capacity units or the read
capacity units. I don't know I've never exceeded
339:04 - it. So I couldn't tell you what error shows
up. Maybe it's called on demand exception,
339:08 - but this error is extremely common. So you
have to understand. So in dynamodb, there
339:17 - is the concept of indexes. And indexes are
extremely common with databases. So what is
339:22 - an index? A database index is a copy of a
selected columns of data in a database, which
339:28 - is used to quickly sort. So it's just when
you need to quickly look up information, you're
339:34 - literally cloning a version of your database.
And dynamodb has two types of indexes. We
339:41 - have LSI, which stands for local secondary
index. And these can only be created with
339:47 - the initial table. And then you have GSI eyes
which are global secondary index, and we're
339:52 - going to get into both of these in extreme
detail. But the takeaway I want you to remember
339:57 - from this is that you generally want to use
Global over local, and this is what's recommended
340:02 - in database documentations. And another deciding
factor could be strong consistency. So a local
340:08 - secondary index can provide strong consistency,
where a global secondary index cannot provide
340:15 - strong consistency. So now that we have a
little bit of idea what indexes are, and let's
340:21 - jump into lsis, and GSI. So let's first take
a look at local secondary indexes. So it's
340:33 - considered local, and that every partition
of an LSI is scoped to a base table partition
340:39 - that has the same partition key value, if
you're only what base table is, that is the
340:42 - initial table that you're creating, you're
creating an index from the total size of index
340:48 - items for any one partition key value can't
exceed 10 gigabytes. So that is definitely
340:55 - a hard limit with local secondary indexes.
It shares the same provision throughput setting
341:00 - for READ WRITE activity with the table that
is indexing. So that's the base table. And
341:05 - that makes sense because it's a local table,
so of course, it's going to share, and then
341:09 - it has a limit of five per table. Now let's
take a look at actually how we create a local
341:16 - secondary index. So the LSI is can only be
created with the initial table. So here you
341:22 - can see I'm making a table and I'm adding
index index at the time of creation. You cannot
341:26 - add, modify, or delete LSI lies outside of
this initial table step. So you have to really
341:34 - get a right here. And now and if you need
one, you literally have to make a new table.
341:39 - So the only way you can make a new also, you'd
have to make a new table and move your all
341:43 - your data over. So you need to have both a
partition and a sort key. And there are some
341:49 - conditions around those partitions or keys,
the partition key must be the same as the
341:54 - base table. And this makes sense because it's
a local, it's a local partition. So it's working
342:01 - off that base table. The second sort key should
be different from the base table. Now you
342:07 - could make it the same, but then it defeats
the whole purpose of having a secondary index,
342:11 - which is supposed to be optimized to sort
in a different manner. So you'd want to choose
342:17 - a different sort key in this case. So you
know, hopefully, that makes local secondary
342:22 - indexes more clear. So we'll move on to the
next one, which is global secondary index.
342:27 - So global secondary indexes are considered
global, because their queries on the index
342:35 - can span all of the data in the base table.
And across all partitions. These indexes have
342:40 - no size restrictions where we saw with LSI,
there was a 10 gigabyte limit for all items
342:45 - in that index. They can provision their own
throughput settings, and they consume capacity,
342:51 - but it's just not from the base table. So
that's a good thing. And there's a limited
342:55 - to 20 per table, I think you might be able
to use a service limit increase to increase
343:01 - that I'm not sure. But even if that's not
the case, it's not going to show up an exam.
343:06 - So do not worry. If you want to create a global
secondary index, you can create it while you're
343:11 - creating the table. Or you can make one afterwards
and you can modify them and delete them at
343:15 - any time. So it's extremely versatile to create
these things. And the partition key can be
343:22 - different from the base table. And you should
make it different generally, because that
343:28 - would make sense. But I guess if you had not
major local secondary index and you wanted
343:33 - a partition key with a different store key,
then I guess you'd have to make a GSI. But
343:38 - just notice that you can set the partition
key to whatever you want. And the sort key
343:43 - is optional. So you don't actually have to
have it, you could just have a partition key.
343:47 - So that'd be a simple key. But yeah, that's
global secondary indexes. So now let's just
343:52 - pull them up side by side and make sure that
we understand the differences clearly. Okay,
344:02 - so let's just reiterate through LSI versus
GSI. So we really know the difference between
344:06 - these two and which one is better in each
specific aspect. So starting at the top here,
344:12 - when we're talking about key schemas, local
secondary indexes only have composite. And
344:17 - remember, composite is both a partition and
sort key. And then global secondary indexes
344:22 - support both simple and composite. Then for
key attributes for elseis. The partition key
344:27 - must be the same as the base table because
remember, it's local, it has to use the same
344:31 - partition key for GSI is the partition and
sort key can be any attribute you like. Then
344:37 - for size restrictions, the LSI has to be 10
gigabytes or less for all index items. And
344:44 - then for GSI is it's unlimited. For online
index operations. The only time you can create
344:50 - indexes is on table creation. But for GSI
is you can add, modify, delete indexes at
344:56 - any time, and you can make these indexes at
the time of creation as well. If you want
345:00 - to, for queries and partitions, the LSI is
query over eight or a single partition as
345:06 - specified by the partition key value in the
query for GSI, is it queries over entire table
345:13 - or across all partitions? For re consistency,
we have strongly or eventual consistency.
345:19 - So you can see LSI win over GSI. In this one
case, where a GSI is only eventual consistency,
345:27 - for provision, throughput consumption, this
is shared capacity with the base table. So
345:33 - you're more likely to get throttled here,
both for GSI is it has its own capacity, so
345:37 - it's not gonna affect the base table. And
then the last one here, which we didn't talk
345:42 - about, but we'll talk about now is projected
attributes. So when you create that table,
345:46 - you say, what, what attributes. So those are
the columns that are allowed to be in that
345:51 - index. So for local secondary index, you can
request attributes that are not projected
345:56 - in into the index, whereas GSI, you can only
request attributes that are projected in the
346:02 - index. So over those two points, you can see
why gsis are generally more recommended than
346:08 - lsis. But you know, it all comes down to your
use case, so you'll have to decide for yourself.
346:13 - So here we're gonna take a look at dynamodb
accelerator, also known as DAX, and it is
346:22 - a fully managed in memory cache for Dynamo
DB that runs in a cluster. And its response
346:28 - times can be in single digit millisecond.
So DAX can reduce response times to microseconds.
346:35 - And that's really important for certain types
of workloads, which we'll talk about a little
346:39 - bit more in the next slide. So here's an illustration
of generally what DAX is. So let's just talk
346:46 - through the points of how this thing works.
So you have a DAX cluster, which consists
346:50 - of one or more nodes, so I haven't called
cash here, each node runs its own instance
346:55 - of the DAX caching software, one of the nodes
serves as the primary node for the cluster.
347:01 - Additional nodes if present serve as read
replicas, your app can access DAX by specifying
347:06 - the endpoint for the DAX cluster. So there
we can see the app and it's accessing via
347:12 - the endpoint. The DAX client software works
with the cluster endpoint to perform intelligent
347:16 - load balancing and routing. So just takes
care of stuff, you just use the endpoint and
347:20 - figures everything out. And incoming requests
are evenly distributed across all the nodes
347:24 - in the cluster. So now that we kind of have
a little bit of an overview of DAX, let's
347:29 - look at the use cases of when and when not
to use DAX. So let's take a look at DAX use
347:35 - cases. And to best understand this, let's
say what DAX is good for and what it's not
347:40 - good for. So apps that require the fastest
possible response time for reads. That's real
347:45 - time bidding, social gaming and trading applications.
This is where you're gonna want use DAX apps
347:49 - that read a small number of items more frequently
than others. apps that are read intensive,
347:54 - but are also cost sensitive. Please take note
of read intensive, that's what DAX is usually
348:01 - for apps that require repeated reads against
a large data set. Notice that we said reads
348:06 - again. And now on to the non ideal side apps
that requires strongly consistent reads. Because
348:13 - DAX is not strongly consistent is eventually
consistent apps that do not require microsecond
348:18 - response time for reads or do not need to
offload repeated reactivity from underlying
348:22 - tables, apps that are write intensive or that
do not perform much activity, no services
348:28 - on a right. That is not what DAX is intended
for. It's for read apps that already are using
348:34 - a different caching solution with dynamodb.
And are using their own client side logic
348:38 - for working with that caching solution. So
if you run into cases on the right, what would
348:44 - you do for caching in Dynamo dB, and that's
where elastic cache would come into place,
348:49 - because you can put elastic cache in front
of Dynamo dB. So if you're dealing with write
348:54 - intensive stuff, and you can make advantage
of Redis there, and so that would be the case
348:59 - there. So we're on to the Dynamo DB cheat
sheet. And this one's more special than all
349:08 - the rest. And that's why I prefixed it with
ultimate. Because Dynamo DB is the most important
349:13 - service, you need to know to pass the ADA
certified developer associate. It's extremely
349:17 - critical to the certification. And this cheat
sheet is a very long, okay, it's the longest
349:24 - one in this course. It's seven pages long.
And it actually started out as only being
349:28 - five pages. I had published a preview on Twitter,
and Kirk, who is a senior technologist at
349:34 - AWS, specifically for dynamodb. Notice I made
some mistakes and made the offer to review
349:41 - the entire cheat sheet for accuracy. And I
send it over to him. And he turned this five
349:46 - page cheat sheet into a seven page to cheat
and I even learned a lot of great things.
349:51 - So you know, I think we all benefit from Kirk's
help here. And so I want to tell you, if you
349:56 - can do me a favor to go on Twitter if you
have Twitter. I want you to tweet out to him,
350:01 - he was certified dynamodb. And thank you thank
him for helping us with this ultimate dynamodb
350:06 - cheat sheet. He did it on his own time, he
didn't have to do it, you know, and this was
350:11 - his own effort. So we greatly appreciate it.
And you know, I really hope that it helps
350:16 - you pass the exam. So let's jump into this
ultimate dynamodb cheat sheet. Okay, so let's
350:23 - jump into the ultimate dynamodb cheat sheet.
So dynamodb is a fully managed no SQL key
350:28 - value document database. dynamodb is suited
for workloads with any amounts of data that
350:33 - require predictable read and write performance
and automatic scaling from small to large.
350:38 - and everything in between dynamodb scales
up and down to support whatever read and write
350:43 - capacity you specify per second in provisioned
capacity mode, or you can set it to on demand
350:49 - mode and there is little to no capacity planning
dynamodb can be set to support eventually
350:55 - consistent reads by default, and strongly
consistent reads on a per call basis. Eventually
351:01 - consistent reads data is returned immediately,
but data can be inconsistent copies of data
351:05 - will be generally consistent in one second.
Now talking about strongly consistent reads,
351:11 - will always read from the leader partitions
since it always has to be has to have an up
351:15 - to date copy data will never be can be inconsistent,
but latency may be higher copies of data will
351:21 - be consistent with a guarantee of one second,
Dynamo DB stores three copies of data on SSD
351:28 - drives across three ACS in a region. I think
before I had said across three regions, but
351:35 - this is my misinterpretation of the documentation.
And I don't even know if it's really three
351:40 - azs more so managed data centers by AWS. But
it's easy to understand them as AZ. So that's
351:45 - what we're going to call them dynamodb most
common data types are be binary, and number
351:51 - as string. I think there's a few other ones
there. And some of them share the word like
351:55 - start with B, so it gets a bit confusing,
but these are the three that I want you to
351:58 - know. Tables consists of items, which we call
rows, and items consist of attributes which
352:04 - we call columns. A partition is when dynamodb
slices your table up into smaller chunks of
352:10 - data. This speeds up read for very large tables.
dynamodb automatically creates partitions
352:16 - for these scenarios. So every 10 gigabytes
of data when you exceed RC use of 3000 or
352:23 - WC use of 1000 limits for a single partition.
And the last scenario when dynamodb sees a
352:30 - pattern of a hot partition, it will split
that partition in an attempt to fix the issue.
352:36 - So that is page one, we're gonna go to page
two. So we're on to page two of the ultimate
352:42 - Dynamo DB cheat sheet. So Dynamo DB will try
to evenly split the RC use and WC use across
352:48 - partitions, primary keys defined where and
how your data will be stored in partitions.
352:53 - So primary keys come in two types. We have
simple primary keys using only a partition
352:57 - key, and composite primary key using both
a partition and sort key. partition key is
353:03 - also known as hash. And sir key is also known
as range. I mentioned this before. But the
353:09 - reason they used to be called hash and range,
I don't know. But the point is they change
353:12 - them. And when you're using the COI, or, or
the SDK, they still call them hash and range,
353:18 - so it's important to know both of them there.
When creating a simple primary key, the partition
353:23 - key value must be unique. When creating a
composite primary key. The combined partition
353:29 - and sort key must be unique. When using sort
key records on the partition are logically
353:34 - grouped together in ascending order. dynamodb
Global tables provide a fully managed solution
353:39 - for deploying multi region multi master databases.
dynamodb supports transactions via the transact
353:46 - write items and transact get items API calls.
I don't know if I mentioned it there. But
353:51 - the point of these transaction calls is that
they let you go across multiple tables Oh,
353:56 - I write it right here. So transactions let
you query multiple tables at once in an all
354:00 - or nothing approach. So all API API calls
must succeed. dynamodb streams allow you to
354:07 - set up a lambda function triggered every time
data is modified in a table to react to changes.
354:13 - And I love dynamodb streams, I use it all
the time for a lot of projects. I don't feel
354:18 - like we need to know too much on the developer
about it. But there's definitely a lot we
354:22 - could write about it. And streams do not consume
RCU. So that is the nice thing about it, they're
354:27 - not going to use your read count up. So that
is page two, we're moving on to page three.
354:32 - So we're on to page three of the ultimate
dynamodb cheat sheet. So dynamodb has two
354:38 - types of indexes. We first have lsis which
is local secondary index, and then we'll talk
354:43 - about gsis. So LSI supports strongly or eventually
consistent reads. They can only be created
354:48 - with the initial table. They cannot be modified
and cannot be deleted unless you're also deleting
354:53 - the table. That's the only case where you'd
be able to delete it. They only use composite
354:58 - keys. You They have to be 10 gigabytes or
less per partition the shared capacity units
355:03 - with, they have to share the capacity units
with the base table, you must share a partition
355:08 - key with the base table. So that's very important
as well. Moving on to GSI. So global secondary
355:15 - index, they cannot provide strong consistency,
the only way to get strong consistency is
355:21 - with LSI. Okay? Very important. Remember that
point. So only eventual consistent reads can
355:27 - create, modify, or delete at any time, that's
extremely convenient. Simple and composite
355:33 - keys is what you can use here can have whatever
attributes. So the partition key can be whatever
355:39 - it wants, and the sore key can be whatever
it wants, there is no size versus restrictions
355:43 - per partition. It has its own capacity settings.
So there you go. That is, I think we're page
355:50 - three onto page four. So we're on to page
four of the ultimate dynamodb cheat sheet.
355:56 - So we'll talk about scans and we'll talk about
queries. The first thing I'm going to tell
355:59 - the scans is, your table should be designed
in such a way that your workload primary access
356:03 - patterns, do not use scans. Overall scan should
be needed sparingly in frequent reports. So
356:09 - generally, you do not want to be using scans,
scan through all items in a table and then
356:15 - return. One or one or more items through filters,
by default returns all attributes for every
356:20 - item you can use per project expression. to
limit the attributes that you want to use.
356:26 - scans are sequential, you can speed up a scan
through parallel scans using segments and
356:30 - total segments. scans can be slow, especially
with very large tables and can easily consume
356:36 - your provisioned throughput. scans are one
of the most expensive ways to access data
356:41 - in Dynamo dB. And we'll move on to queries
next. So it's about finding items based on
356:47 - the primary key values, tables must have a
composite key. And in order to be able to
356:52 - query by default queries are eventually consistent.
But if you want to use strong, strongly consistent
356:59 - reads, you can use this attribute strongly
consistent reads set it to true. And now you're
357:04 - doing strong, by default returns all attributes
for each item found by query. So just like
357:09 - scans, you can use project expression to filter
stuff out by default is sorted, ascending,
357:14 - and you can use scan index forward, false
to reverse the order and descending. I don't
357:19 - know if that option is available in scan.
But you know, just know that scan index forward
357:23 - is used to flip the order. So there you go.
So we're on to the fifth page out of seven
357:29 - for the ultimate dynamodb cheat sheet. So
dynamodb has two capacity modes provision
357:34 - and on demand, we're talking about provision
first. So you can switch between these modes
357:38 - once every 24 hours. provision, throughput
capacity is the maximum amount of capacity
357:42 - or application is allowed to read or write
per second from a table or index. So the provision
357:47 - is suited for predictable or steady state
workloads. It's very important to understand
357:51 - the concepts of RC use and WCS, especially
for provision throughput, because you definitely
357:55 - set these values here. So RC use is a read
capacity unit. WC uses right capacity unit.
358:02 - And with dyno, and with provision throughput,
you can set auto scaling. And so it's recommended
358:06 - you enable auto scaling with provision capacity
mode. In this mode, you set a floor and a
358:11 - ceiling for the capacity you wish the table
to support. dynamodb will automatically add
358:15 - or remove capacity to between these values
on behalf and throttle calls that go above
358:21 - the ceiling for too long. And if you go beyond
your provision capacity, you'll get an exception
358:25 - provision throughput exceeded exception for
the exam, you want 100% want to know this,
358:30 - it will absolutely show up on the exam. And
this is what happens when throttling occurs.
358:35 - And if you're not familiar with throttling,
it's when requests are blocked due to read
358:39 - or write frequencies higher than the set threshold.
So an example for exceeding the set provision
358:44 - capacity, we got partition splitting table
index capacity mismatch. So that is provision
358:51 - throughput, we're gonna move on to on demand.
And on to the next page we go. So we're on
358:56 - to the sixth page of the ultimate dynamodb
cheat sheet talking about on demand capacity.
359:00 - And this is pay per request. So you only pay
for what you use on demand is suited for new
359:05 - or unpredictable workloads. The throughput
is only limited by the default upper limit
359:10 - of the tables, that's 40k, RCS and 40k. WCS,
WC is that's extremely high value. And throttling
359:17 - can occur if you exceed double your previous
peak capacity. So the high watermark within
359:22 - 30 minutes had no idea of this. If you previously
peaked to a maximum of 30,000 ops per second,
359:29 - you could not peak immediately at 90,000 ops
per second, but you could at 60,000 ops per
359:34 - second. So that is definitely something I
did not know. And I'm really glad Kirk put
359:38 - that in there because I had something way
more simpler before. Since there is no hard
359:42 - limit on on demand. It could be very expensive
based on emerging scenarios. So just be careful
359:47 - with on demand there. But you definitely have
the flex flexibility where you don't have
359:50 - to think about setting your capacity. So that's
pretty nice. Now let's talk about calculating
359:55 - reads and writes. This is definitely more
important for provisioned throughput, not
360:01 - for on on demand capacity, but we'll go through
it now. So for calculating reads for RC use,
360:06 - a read capacity unit represents one strongly
consistent reads per second or two eventually
360:11 - consistent reads per second for an item up
to four kilobytes in size. And how you're
360:17 - going to calculate RCS for strong is the following
round up data to the nearest four divided
360:23 - by four times by numbers of numbers of reads.
And then we'll move on to how to calculate
360:28 - for RCS for eventual, so round data up to
nears four divided by four times by the number
360:35 - of reads and divide final number by two. And
I think you got to round it up. And then round
360:40 - up the nearest whole number. If you really
can't remember that stuff, here are the examples.
360:45 - And I'm hoping you're printing out this cheat
sheet on the day of your exam, so that you
360:49 - can look through these and make sure you know
these for sure. And so that's Page Six, and
360:54 - we're on to the last page, page seven. So
we're on to the last page of the ultimate
360:59 - dynamodb cheat sheet. So let's finish strong
here, we're going to do some calculating of
361:03 - rights. So our read capacity unit represents
one rate per second for an item up to one
361:07 - kilobyte, how to calculate rights, what we're
going to do is rounded up to the nearest one
361:13 - times by the number of writes. And we'll talk
about that i will i have the example there,
361:17 - we'll just show you there at the end. Oh,
so we'll talk about dynamodb accelerator,
361:21 - also known as DAX is a fully managed in memory
write through cache for dynamodb that runs
361:27 - in a cluster. So reads are eventually consistent,
incoming requests are evenly distributed across
361:33 - all of the nodes in the cluster, DAX can reduce
read response times to microseconds, and let's
361:39 - say well, where it's ideal and where it's
not. And this is definitely debatable, but
361:43 - I got this from the docs. So you know, you
can't argue with me, but I know some people
361:47 - might consider otherwise. And if it's for
the exam, they generally fall whatever on
361:52 - the docks until they've been changed. So I
DAX is ideal for the fastest response science
361:57 - possible apps that have apps that read a small
number of items more frequently, apps that
362:02 - are read intensive. So that's the one I'm
highlighting there. And then DAX is not ideal
362:07 - for apps that require strongly consistent
reads apps that do not require microsecond
362:12 - reads response times. apps that are write
intensive, or that do not perform much read
362:19 - activity. And if you don't need DAX, consider
using elastic cache. That's not a hard rule.
362:25 - But that's a good rule for the exam. If you
if you gotta throw up, like a threw up a toss
362:30 - up between DAX, and elastic cash, and it doesn't
need micro microseconds and you know, consider
362:36 - using elastic cash there, or if it's more
write intensive. And just to show you there,
362:41 - these are the examples, you definitely want
to print out this cheat sheet and have it
362:44 - on exam day. You know, I hope this this ultimate
dynamodb cheat really makes the difference
362:49 - for your exam. So it looks like I lied, and
there's actually eight pages to this Dynamo
362:55 - DB cheat sheet I almost forgot to include
Dynamo DB API commands which you use vcli
363:02 - which is really important because these could
show up on the exam so let's go through them.
363:08 - The first being get item this returns a set
of attributes for the items with the given
363:12 - primary key if no matching item that it does
not return any data and there will be no item
363:17 - element in the response then you have put
item creates a new item replaces an old item
363:22 - with a new item. If an item has the same prime
primary key as the new item already exists
363:28 - in the specified table, the new item completely
replaces the existing item, then you have
363:33 - update item edit an existing items attributes
or adds a new item to the table if it does
363:38 - not already exist. Then you have batch get
item this returns the attributes of one or
363:43 - more items from one or more tables you identify
requested items by by primary key a single
363:50 - operation can retrieve up to 16 megabytes
of data which can contain as many as 100 items,
363:57 - then you have a batch right item puts or deletes
multiple items in one or more tables. And
364:02 - right up to 16 megabytes of data which can
be compromised, which can compromise as many
364:09 - as 25 put or delete requests individual items
to be written can be as large as 400 kilobytes
364:17 - then you have CREATE TABLE just as the name
implies, it adds a new table to your account
364:22 - table names must be unique within each region.
So you could have the same database name or
364:28 - search or same table name but in two different
regions. Then you have update table so modifies
364:33 - the provision of throughput settings, global
secondary indexes or dynamodb stream settings
364:37 - for a given table, delete table and this is
very obvious it just deletes a table with
364:43 - all of its items. Then you have transact get
items a synchronous operation that atomically
364:49 - retrieves a multiple items from one or more
tables but not from indexes in a single account
364:53 - and a region can contain up to 25 objects,
the aggregate size of the items in the transaction
364:59 - cannot exceed Four megabytes. Then we have
a transact write items a synchronous write
365:05 - operation that groups up to 25 action requests.
These actions can target items in different
365:10 - tables, but not in different AWS accounts
or regions. And no two action can target the
365:16 - same items. And we're really running out of
space here. But we have query finds item based
365:21 - on primary key values, you can create table
or secondary index that has a composite primary
365:25 - key. And last is scan returns one or more
items and more items and item attributes by
365:33 - accessing every item in a table or secondary
index. So there you go, that's the real end
365:38 - to the dynamodb cheat sheet, super long section,
but definitely worth it and super critical
365:43 - to passing the developer associate exam. Hey,
this is Andrew Brown from exam Pro, welcome
365:52 - to the dynamodb. Follow along. And what we're
going to do is we're going to create a table
365:56 - loaded up with data, write some records, delete
some records, get some records in batch. And
366:03 - just really understand how Dynamo DB works.
What I need you to do is get Dynamo DB open
366:09 - up here in a tab. So I just type in Dynamo
dB, click that there and you will make it
366:13 - to the dynamodb page, make sure you're in
US East one eight of us loves to put you in
366:18 - Ohio or somewhere else. And just to be consistent.
Let's always do US East one for these fall
366:24 - alongs, you're going to need a cloud nine
environment set up here I showed you some
366:28 - Elastic Beanstalk follow along, I show you
this in a variety of different ones here.
366:32 - So if you're not sure how to do it, go check
those out or give it a go and try to spin
366:36 - up yourself an environment. And I have the
Dynamo DB documentation open up here. So we
366:42 - can poke through this as we're working through
these commands. And we're also going to need
366:47 - a couple files from the the GitHub repo here,
the free eight of us developer associate.
366:52 - So I have a file here that helps transform
data and then the actual data we plan on importing.
366:58 - And we're going to be working with starship
data from Star Trek. So I have a list of starships.
367:03 - And we can see that this is the data we have.
And that's what we're going to be importing.
367:07 - So let's make our way over to dynamodb. Let's
take a look at what it takes to create a table.
367:11 - We're not going to create our table through
here we're going to use the COI. But let's
367:15 - just talk through what's on the page here.
So the first thing you do is you'd name your
367:19 - table. So I'd call mine starships. And then
you set your primary key, we have the option
367:23 - of setting a partition key and a third key.
AWS used to call this a hash and a range.
367:29 - And this will show up in the code because
there's so named that that way. So looking
367:34 - at our data, what we'd have to do is decide
we'll be making good partition and sort key
367:39 - and in this case, the good partition key is
going to be shipped class because it's a grouping
367:43 - of things you can see you have Crossfield,
Crossfield, Crossfield. And then, as long
367:49 - as you as long as you have a unique value
with both the sort and partition, it's okay,
367:55 - as for the sort, we're using a registry number
that identifies the ship, and those are all
368:00 - unique. So these, this will definitely be
a unique value. Generally, you want your store
368:05 - key to be a date, but it all depends on your
data. And in this case, we don't have a date,
368:07 - value. So it's going to be shipped class as
our primary or our partition, sorry, and our
368:08 - registry as our sort key. So what we do here
is we type in the name, I would call it ship
368:09 - class. For some reason dynamodb likes to name
things, camel case, or at least all the documentation
368:10 - out there. So let's just follow suit there.
But you could name the lowercase if you wanted
368:11 - to. And this would be registry. And over here
we have some data type string binary number
368:12 - dynamodb does not have a date time format,
we would normally use string in that case,
368:13 - and there might be a few other ones outside
of here. But these are the these are the only
368:14 - ones you need to know S stands for string
B is for binary and is for number. And these
368:15 - are both going to be string values. You have
some default values here, no secondary indexes
368:16 - provisioned capacity five, five reads and
five writes, it has auto scaling turned on
368:17 - and encryption is at rest as default encryption
type. So we just checkbox that off, we can
368:18 - see those values here five and five provisioned,
auto scaling is turned on. And this is defaulted.
368:19 - If you want to recreate a local secondary
index, the only time you can create it is
368:20 - at this particular time. So you'd have to
go ahead here and add those. And for local
368:21 - tables, you always want to have the same,
you always want to have the same partition
368:22 - key. And then you'd have a different sorting,
you have to specify them both. We don't have
368:23 - another value here, but I put a name and now
I can checkbox on secondary, local secondary
368:24 - index. We're not going to create any secondary
indexes. They're not that important to know
368:25 - other than like how they work, but we don't
really need to go through the motions of actually
368:26 - using them. But this is what we're going to
do but we're going to use the COI to do this.
368:27 - So make your way back to cloud nine. I'm going
to create a new folder here called MK or we'll
368:28 - type in MK dir to make that folder and type
in dynamodb Dynamo dB, we'll call it playground.
368:29 - Any files we're working with will place them
in there and already named that wrong. So
368:30 - I can go ahead here and just rename that file.
Okay. And I'm just going to go ahead and make
368:31 - a new file in here. And I'm just going to
call it like scratchpad. Because this is where
368:32 - we're going to write out all our CI commands,
and then paste them in there, just so that
368:33 - it's a bit easier to work with. I'm just gonna
clear that there. Yeah, and so let's get to
368:34 - it. So what we're going to do is create our
table using the ccli. So what we do is type
368:35 - in AWS dynamodb, I think we listed out it'll
tell us a bunch of information, something
368:36 - we've typed in help maybe. And so this is
a great way of getting a lot of information
368:37 - about dynamodb. But we're just gonna use the
docs for this. So let's look up create table.
368:38 - And in here, you can see the values that we
can specify, I'm not sure which ones are required,
368:39 - but I generally know what we need to enter,
the first thing is going to be attribute definitions.
368:40 - And so the attribute definitions, if we just
go to that section, an array of attributes
368:41 - that describe the key schema for the table
indexes. So what it's asking for is to specify
368:42 - the attributes for that we're going to use
in the actual key schema, so the partition
368:43 - key and the store key, and we decided that
we are going to use the ship class and the
368:44 - registry. So if you just look at the syntax
here, this is the format that we have to type
368:45 - it in. So we'll make our way back, I'm just
going to copy this, save myself a lot of trouble.
368:46 - So I'm really terrible at writing these things
out. I'm going to type in create table, and
368:47 - then put a backslash that lets us do multi
line. And we'll do attribute definitions.
368:48 - And to do two spaces there, and we'll go back
here, I'm just going to copy this format.
368:49 - And so that is our first one. And it's going
to be I said ship class. And it's going to
368:50 - be s for type four type string. I'm not sure
if it tells us the types around here, I'm
368:51 - not seeing it, but I but we see there's s
and NB. So back here remember we have string
368:52 - binary number, that's what the represented
in this in the CLR in the API s and NB. So
368:53 - we want them to build the string. But that's
our first one. So we'll go back here. And
368:54 - we will need a second one. And we said that
would be registry. And that's going to be
368:55 - a string type as well. The next thing we're
going to need is the key schema. And this
368:56 - specifies the attributes that make up the
primary key for the table. And below, you
368:57 - can see that we need an attribute name and
a key type. And they might have an example
368:58 - here. Yes, they do. So here it is. And we
will make our way back to here and we will
368:59 - type in key schema. And I'll just paste that
in there. And it's going to be the same thing.
369:00 - I know it's a bit redundant, but that's just
how it is. And the ship class is going to
369:01 - be the hash. And the registry is going to
be the range. Remember I said earlier that
369:02 - dynamodb used to call them hash and range
and they still appear in the code. This is
369:03 - what I'm talking about. You can see here it
says hash is the partition key range is the
369:04 - sort key on the exam for the developer associate,
you might actually see a bit of CSI stuff
369:05 - here. And then you might need to understand
what is what. So just be aware of that. And
369:06 - the next thing we might need to specify is
the provision throughput, this might have
369:07 - a default, but we'll take a look here. I'm
gonna scroll up here. So I can find the name
369:08 - here. provision throughput represents the
provision throughput for the specified table.
369:09 - This is probably a required field, I'm not
seeing any of this is required. But I almost
369:10 - feel that it is. And we're going to just set
that default for that to five and five. So
369:11 - I'm just going to copy that there, make our
way back over here. And we'll just put two
369:12 - spaces there for backslash. And then we're
going to set the Read and Write capacity,
369:13 - we're just going to set it as five and five.
So we'll do five, and five. And one more thing
369:14 - I want to do is set the region always, always
always when you're using to see Li set the
369:15 - region if you can, just because 80 of us might
default it to Ohio and then you're just gonna
369:16 - be scratching your head looking for stuff.
So I think this is our crate commands, we
369:17 - type create Dynamo DB table. Nice documentation
for ourselves. And let's see if this works.
369:18 - We'll just copy that. Paste that on in, hit
Enter. And it did not work. Let's see here.
369:19 - Let me just double check here. backslash backslash
backslash backslash looks all good to me.
369:20 - Maybe I didn't copy it and write it. We'll
just try this one more time, I'm gonna type
369:21 - clear here, paste that in, hit enter. Okay,
just give me a second to figure this out.
369:22 - Oh, my goodness, it is the most obvious thing,
I didn't specify the table name. If we don't
369:23 - specify the table name, this thing is not
going to get created. So I guess we just skipped
369:24 - right on over that, I'm just going to go down
here. And we just provided a string. So I'm
369:25 - just going to copy this here. We'll go back
up here. I call this starships. And we'll
369:26 - have a backslash there, I'm gonna save that.
And we will copy this, paste that in, hit
369:27 - Enter. And we're getting output that is good.
This so this means that it definitely has
369:28 - created and look down below where it says
table status. It says creating object crates
369:29 - is pretty darn quick. If we make our way over
to dynamodb, I'm just going to go back to
369:30 - the service here. And go to tables on the
left hand side. And it's active. So it's already
369:31 - been created super, super fast. Here we can
see our ship class, our registry. But we don't
369:32 - have any data in here. So until we have data,
it's gonna be a little bit hard to do anything.
369:33 - So I think that's what we'll do next. But
but just before we do, I just want to show
369:34 - you the describe command. So let's say that
it was taking a while for our dynamodb table
369:35 - to create. I don't know why, but let's just
say it was really slow. And we wanted to check
369:36 - on the status of it. So we wanted to see that
it was active, we could do is type in and
369:37 - I will do it up here. So we have reference
to it. Describe table. We do alias dynamodb
369:38 - describe table table name, starships. I don't
know if there's really any other output than
369:39 - that a lot of COI commands have this describe.
So when you use the COI long enough, you just
369:40 - start guessing as to what it is, I didn't
have to look it up to know what it was. We'll
369:41 - go here, we'll see if there's any other options.
No, it's just table name. So we just have
369:42 - table name here. And I'm going to go back
to the cloud nine here. And if I just take
369:43 - that and paste that in there, we're getting
pretty much the same output as before. And
369:44 - you can see now that it's active. And a lot
of times I like to change the output here.
369:45 - And I might do table, this one would probably
be good as a table. So I think it's a little
369:46 - bit easier to see table status over here,
it's a little bit easier to read that way,
369:47 - we could also do text. For this one, it probably
be a bit messy. So that's not very readable.
369:48 - So for this case, I've used table. By default,
it's JSON. So there you go. So what we'll
369:49 - do next is we'll move on to getting our data
into our table. And we're gonna have to prepare
369:50 - it because right now it's as a CSV file over
to Jason, but we'll do that next. So what
369:51 - we're going to do is we're going to make a
new folder in our dynamodb playground, because
369:52 - we're going to generate out a bunch of batch
files, because we're going to need to batch
369:53 - import our data. So when you go here in the
left hand side and make a new folder and run
369:54 - called that fetches. And we are going to need
this the state of here. So both the CSV file
369:55 - and the CSV jason.rb. Probably the easiest
way to get it in there is just to click the
369:56 - raw button here, Copy that. And we will make
a new file in here, which we'll call CSV,
369:57 - to jason.rb. If you're wondering why this
file is written in Ruby, it's because that's
369:58 - my favorite language to use. So why not. And
you don't need to know Ruby too much, you
369:59 - just I'll walk you through this file here
in a moment, just so you understand what's
370:00 - going on here. We'll go back. And the next
thing we'll need to do is grab that CSV file
370:01 - here. So I'm gonna go there, get the raw data,
copy all that. And we will right click New
370:02 - File, Starfleet dot CSV. And we'll double
click that. Ok. Close tab here. It's because
370:03 - I did this project earlier. So it had a cached
version, it was a bit confused. And so um,
370:04 - I pasted it in there and just make sure that
name is correct. So let's take a look. Let's
370:05 - take a look at this file here. Let's talk
about why we have to convert this to JSON
370:06 - and why it has to be in terms of batches.
And the easiest way to understand is actually
370:07 - to first look at the command that we're going
to be using which is the batch right item.
370:08 - So the batch rate item operation puts or deletes
multiple items in one or more tables. A single
370:09 - call to a batch rate item can write up to
16 megabytes of data, which can be comprised
370:10 - of 25 Put or delete requests. So we have some
limitations here. And individual items written
370:11 - can be as large as 400 kilobytes. So the thing
is, is that we have records here, we go back,
370:12 - because we have nice visual here and GitHub.
But you can see that we have a little boy
370:13 - 310 Records. So that's more than 25. So we
have to break this up into batches of 25.
370:14 - And we also have to provide it in the JSON
format that it's expecting, I'm not aware
370:15 - of being able to import CSV, I'm gonna check
here, nope, there's no way to import CSV,
370:16 - and the format that it's expecting. It looks
like this, here's this, here's this request
370:17 - items, JSON, it expects you to put the table
name here, and then have this structure request,
370:18 - put item and then the values. So that's what
we need to do, we need to transform our data
370:19 - into that format. And that's why I wrote this
little script here to do this. So we'll quickly
370:20 - just walk through it. So what it does is it
pulls in the CSV file, and it reads the CSV
370:21 - file using the CSV, a CSV file, Ruby library.
And this, we may or may not need. When I originally
370:22 - created the CSV file, it was an Excel and
it was encoding the file non UTF, eight, but
370:23 - this other format, but we just copy and pasted
from here, so maybe we don't need a slide
370:24 - anymore. So we'll see. This says include the
headers, so it will detect these headers here,
370:25 - name registry, etc. And then it will map it
into a JSON file here. So when you lie, we
370:26 - can then specify that then like this, the
idea is, what we want to do is iterate through
370:27 - this file, so it's going to go line by line,
it's gonna start at 25. And Amelie's, it starts
370:28 - at 25, it's gonna reset it back to one, and
it's going to push on a new batch of files.
370:29 - So it's gonna start with the starships file
that's named the file. And then it's going
370:30 - to then format the information into this put
request. And it's going to do that it's going
370:31 - to create a bunch of batches, that's going
to iterate through those batches of data and
370:32 - create a bunch of batch files for us. So hopefully,
that makes sense. But it's better to see it
370:33 - in action than to talk about it. So what I
want you to do is go down below here to your
370:34 - batch type clear. And we want to make our
way into this folder here. So type in dB Dynamo
370:35 - dB, I'm hitting tab to autocomplete it that
saves a lot of time, hit Enter. And we want
370:36 - to run this Ruby file. So type in Ruby, CSV
to json.rb. And if everything is named correctly
370:37 - in here, start fleet CSV, it looks correct
To me, this should work. And there we go.
370:38 - There's our batch file. So we just transformed
our data, let's take a look at one of them.
370:39 - And that's the format we're expecting, right?
If we look back here, that looks like the
370:40 - same format. To me, this one's a bit more
compact. But that's how we transfer my data.
370:41 - And each of these contain 25 Records. So now
to import this, we're gonna go back here,
370:42 - and we need to write our import command. So
that import command is going to look like
370:43 - eight of us dynamodb will say this, we're
using the batch, right item. So Dynamo dB,
370:44 - batch right? Item. And we'll make our way
over here and see what we need to specify,
370:45 - they should have an example here. Examples.
And so we just specify the file. So it's as
370:46 - simple as that. So I'm just gonna copy that
there. But we could just copy this whole line
370:47 - here, save us any trouble if we made any spelling
mistakes. And this file is in batches. So
370:48 - we'll type in batches. batch hyphens, 000.
I'm just going to paste this a bunch of times.
370:49 - I'll go through here. And we'll just change
this 1-234-567-8910 11 and 12. And what we'll
370:50 - do is we'll just copy all these and paste
them in. There they go. And then number 12,
370:51 - we might just have to hit enter there. And
so it says unprocessed items. So if there
370:52 - was anything in there, that means that it
didn't process those. This could happen if
370:53 - we had maybe too many items, or if we were
being throttled or not throttled, but we hit
370:54 - our read capacity or sorry, our write capacity.
We didn't execute these perfectly. So if we
370:55 - go back to dynamodb here and give us a refresh,
we can now see our records. So there they
370:56 - all are, look at that. How great is that?
Um, so now that we know how to batch right,
370:57 - I guess the next thing is let's look at how
we can actually get this data programmatically
370:58 - through cltc Li. So to get items, what we'll
do is go back to the CI here, just click Back,
370:59 - click up here. And we should have get. So
there's a basket, a basket item, which we'll
371:00 - look at in a moment, but we'll look at get
item first. So get item, you specify the table
371:01 - name, you specify the key, we have some additional
things here, where you can keep it really,
371:02 - really simple. And we're just going to do
a simple get, look at the examples here. And
371:03 - yeah, that's as simple as it gets. So we're
gonna copy this over here, we're gonna make
371:04 - our way back to cloud nine, just type here,
get item. And so we have Dynamo dB, get item
371:05 - table name, our table name is starships. We'll
just copy that here. That is a such a short
371:06 - line, I'm just gonna have it one line here.
And then we need a key file. So I'm just going
371:07 - to make a new file here. So we'll take we'll
go here and make a new file and call it key
371:08 - JSON. And the format is pretty darn simple.
So you just specify the actual key. So this
371:09 - is the, the schema key that we need to provide
here to get the record. So I'll go here, and
371:10 - it's actual values. So we have it is registry
is one. And the other one is ship class. And
371:11 - what we'll do is we'll look through our data
here and just find the record. I'm going to
371:12 - choose AB let's see here, if there's any ships
that I like, how about the Okinawa, that sounds
371:13 - cool. So we'll grab the Oakland, ah, I will
try to get this record here. So we'll grab
371:14 - it as Excelsior class here, we'll go back.
And we will paste that in there. And then
371:15 - we need the registry number, which we'll paste
in here as well. So now we have our key, and
371:16 - we'll go back to our scratchpad here. And
this Yep, that should work. So I'm just going
371:17 - to copy that, paste that in there. And there's
our record. So it returns all the fields by
371:18 - default, I think you could limit the fields
that you want returned, probably with projection
371:19 - expression, just gonna take a look at that
quickly here, a string that identifies one
371:20 - or more attributes retrieved from the table,
These can include scalar, etc. And if nothing
371:21 - is specified, then it will turn everything.
So just remember, projection expression, that's
371:22 - what it's used for. And if we want to look
at this in a different format, let's look
371:23 - at a table. Sometimes it looks nicer when
it's under the format. No, that looks terrible.
371:24 - We'll look at as text, that's a little bit
better. So you know, just consider, you can
371:25 - play around with those values there. But that
is for the guide on the next we'll look at
371:26 - how to get things back. So we'll go back to
our documentation here and click back to dynamodb.
371:27 - And let's look at how we get multiple items.
And so we have basket item request items.
371:28 - So it's not key, it's something else here.
And if we go to example, you can see it's
371:29 - as simple as this. So I'm gonna just copy
this here, and this command, we're gonna make
371:30 - our way back here. And we're going to do batch
get item, I'm gonna paste that in there, we're
371:31 - gonna make it a single line that makes a little
bit nicer. And we're going to create this
371:32 - request items file that it's suggesting here.
We'll make a new file there, paste that in.
371:33 - And what we're going to do, I think it's very
similar to this, except you can do multiples
371:34 - of it. So we'll go over here and take a look.
So here, you provide the table name, so we
371:35 - don't have to provide the table here. And
then you provide the keys. So it's pretty
371:36 - darn similar. We'll look at the actual, yeah,
so you don't provide the table name. So we'll
371:37 - look at the actual example here, because this
might be a little bit more pared down here.
371:38 - No, it's the same. And you even provide the
projection expression here if you want to
371:39 - filter out the fields that you want, but I
want all the fields on greedy like that. So
371:40 - I'm just going to paste this in below here.
Oops, actually, we'll go here, paste it in
371:41 - there. And we're just going to grab this one
to save us some time because we'll just get
371:42 - the same record. And I'm going to paste that
in as one of the keys. And I really don't
371:43 - like for spaces, but I'm not going to play
around with this. So I'm just going to type
371:44 - that in here. I'm just hitting tab to do that.
I really hate for spaces, this is too much.
371:45 - It's excessive, and I only want two records
here. So I'll take out this one here. We don't
371:46 - need to project any expressions, we just want
everything and we will get a second ship.
371:47 - So I will go back here to the data. We will
look for a another cool ship. Something that
371:48 - sounds cool USS reliance sounds pretty darn
cool. And so I'll grab the class, which is
371:49 - the Miranda. So we'll do class down here.
And I'll have to replace these of course.
371:50 - And we also need the registry number. paste
that in there. And So Oh, and we also need
371:51 - the name of the ship, which is, or the table,
sorry, which is called starships. And so that
371:52 - should do it. So let's make our way back to
our scratchpad here. And copy. Whoops, that's
371:53 - not it, I'm going to copy this down below.
I will paste that in there. And that should
371:54 - give us the record. So it gave us back two
records with all their information. Let's
371:55 - take a look what this looks like as a table.
Terrible can't even read it. So let's look
371:56 - at this as a as text. Yeah, that's a bit better.
So again, you know, just play around with
371:57 - that stuff. So that is batched get item. So
now the next thing I'm going to do is show
371:58 - you how to like write items to the database.
But we don't have any new records. So I'm
371:59 - just going to go ahead and delete one and
then we're going to re add that back to the
372:00 - database. So let's take a look at delete.
So I think it's called delete item. And we'll
372:01 - make our way over to the CLR. Here, we'll
scroll the top go to Dynamo dB. And we will
372:02 - look for delete there is delete item. And
we have delete item table name key, we'll
372:03 - just go look at the example here. Yep, and
it's as simple as that. So it looks just like
372:04 - get item but it's delete item. So we'll go
here. And I'm just going to go to the top
372:05 - here and look for starships. Scroll that back
down their table name. And we can just use
372:06 - the same key that we already have, which is
just going to delete this record here. And
372:07 - what we'll do before we do that, let's just
make sure the data exists. So this is NCC,
372:08 - 139 Excelsior. So if we go here, I wonder
if we can quickly find it here. I'm just going
372:09 - to copy the name, how you just do Command
F here and see if it shows up. Not seen it.
372:10 - I'm just gonna cheat here I want to show you
querying later, but I'm gonna just show it
372:11 - to you now. And we'll just go grab that name
here. So we'll type in the ship name Excelsior
372:12 - or ship placer. I'm gonna hit start search.
And also we will go get the registry number.
372:13 - paste that in there, hit start. And so there's
the record. So this is the record, we want
372:14 - to go delete. We'll go back here. And we will
go to our scratchpad, grab this here. And
372:15 - that should delete that record there. We'll
hit enter. We didn't get any output back.
372:16 - But that doesn't mean it didn't work. That
means it did work. And we will go ahead and
372:17 - hit start. And now that record is gone. So
that record is 100% gone. And we want to bring
372:18 - it on back. So that's what we're going to
do next. So in order to bring back that item,
372:19 - we're going to need to do a put. And so we'll
make our way over back to the documentation
372:20 - here, go to the top, search for put there
it is. And we have a put item, we specify
372:21 - the table, then we need to specify the actual
item. And we have a bunch of other options.
372:22 - But we'll go down to examples here. And you
can see that we have it was dynamodb, put
372:23 - item table name and the actual item itself.
And then we also have returned consumed capacity,
372:24 - I don't know if we need that, let's just take
a look and read about it determines the level
372:25 - of detail about provision throughput consumption
that is returned. So this is just going to
372:26 - give us additional information about actual
consumed capacity, I think that's a good idea,
372:27 - we could check out what kind of capacity is
being used up. So we'll go back to the top
372:28 - here, go to examples. But this is obviously
optional. And we're going to copy this command
372:29 - here. And we'll paste this here. And then
I'm just gonna make this a single line to
372:30 - make our lives a little bit easier. So it's
a little bit long, but it's not that bad.
372:31 - And we will make a new file called item. And
we need to grab the name of the table again
372:32 - starships. And we need to fill out that item
file. So we will open that up. And we will
372:33 - take a look at what that would look like.
So that's an example of the file. So let's
372:34 - grab this example here. And what we'll do
is we need to go get this record again. So
372:35 - we deleted What was the name of the file and
we'll go back to the key here is Excelsior
372:36 - type open ally zoom. It's this record here.
So I'm just going to grab this data here.
372:37 - Grab the whole row, and we will go back to
the item file here. Paste it on down here.
372:38 - And then we just need to name all these will
grab the names from here. Just gonna do a
372:39 - refresh. So we have shipped class. We have
registry, oops, then we have description.
372:40 - I really do it in any particular order. And
then we have named the order doesn't matter,
372:41 - as long as you get all the information. And
so this is the name of the ship. This is the
372:42 - registry number. This is the ship class. And
then this is the description. We will save
372:43 - that. And so that should be what we need to
get this to work. So I'm going to copy this
372:44 - file here. We're going to paste it in on there.
And we have an error hyphen item expecting
372:45 - a comma delimiter. So maybe we made a minor
mistake in our item file here. Yeah, we don't
372:46 - have a comma on the end here. And we'll go
back, hit up, hit Enter. And there you go.
372:47 - And it told us how many capacity units consumed,
which was one right capacity unit. So I mean,
372:48 - that can be good if you need to do something
programmatic and make sure you're not over
372:49 - consuming. Let's go see if that record now
exists in the table. So we'll just do a query
372:50 - here. We'll go back to here. Actually, we
could just use a get item to do that. That'd
372:51 - be probably smarter. So I'm just going to
copy this here. paste that in. Yep, so the
372:52 - record is back. So now that we know how to
put an item, let's talk about how we would
372:53 - actually go ahead and update an item next.
So let's see how we can update an item in
372:54 - dynamodb. So we're gonna type in update item.
Of course, I think you could just add it from
372:55 - here, right, but that's not we're gonna do
we're gonna use the COI, of course. And we'll
372:56 - make our way over to the we'll fix this file
first here, refresh. Back into there. We'll
372:57 - go back to the COI here, it goes to the top
Dynamo dB, and will look for put item. And
372:58 - so put item here we'll scroll down, we have
put item table name, you provide the item
372:59 - and we have a bunch of other values. I'm certain
This one is not a simple one, if we go to
373:00 - the example here. Now it's not that bad. It's
just Oh, sorry, we're not, we're not creating
373:01 - an item, we're going to update an item I'm
sorry, we've already done put item, we want
373:02 - to update an item. And so we have update item
table name key. And then we have a bunch of
373:03 - other stuff. Let's go look at the example.
And here you can see it's a lot more complicated.
373:04 - I don't know why it's such a pain. But this
is what it is. So I'm going to grab that on
373:05 - here. And we will paste our example here.
And this is big, rename this to starships.
373:06 - We'll leave this as key I will update the
existing file we have and we'll talk about
373:07 - these values here. So the first one is update
expression. So let's take a look at here update
373:08 - expression, an expression that defines one
or more attributes to be updated, the action
373:09 - can be performed on them and the values for
them. So that is what we're actually going
373:10 - to update. And if we look down below, it should
give us examples on how we can write these
373:11 - expressions. I'm not a lot of information
here. But I know what we need to do here.
373:12 - So we'll go here. And we can see that we have
this set value here. The next one is the expression
373:13 - expression attributes names. One or more substitution
tokens for attribute names and expression
373:14 - The following are some use cases. And so it
could be one of those reserved words or placeholder
373:15 - or special characters. So this is just like
a remapping of, of characters. This doesn't
373:16 - make sense, don't worry, we'll once we work
through this example. One or more values that
373:17 - can be substituted in an expression. Okay,
so you know, I don't feel like it's the best
373:18 - explanation But well, it makes sense when
we go through it. So what we'll do is we're
373:19 - going to need a expressions attributes name
as well extracted expressions, attributes
373:20 - valued a file, we already have this file here.
So I'm just gonna copy these names here. Make
373:21 - your way back to cloud nine. And I'm going
to make this new file in our playground. So
373:22 - that's one and then we'll go back here and
we will grab the other one. That's too. And
373:23 - so for names, what it suggests is this. And
then for this one here, we have the values.
373:24 - So the way this works is it allows you to
remap values. So We just go back to this example
373:25 - here. Notice that in the names, or sorry,
the values is the values that we want to update.
373:26 - So here, we're saying, this number should
not be 2015. This string should now be louder
373:27 - than ever. And then we have this colon y and
colon t. And this colon colon t is just like
373:28 - a representation of that actual character
than in the update expression. What it's doing,
373:29 - it's reassigning it saying, so make colon
y into pound y. And then if we look at our
373:30 - expression names, pound y equals zero, pound
80 equals album attribute. It seems like a
373:31 - lot of roundabout, it does feel that way.
But it's allow you to get flexibility. So
373:32 - you don't run into issues like with reserved
words or special characters. And we could
373:33 - pare this down. But let's actually try to
do this in full with the example that they
373:34 - have here. So make your way back here. The
first thing I'm going to do is go back to
373:35 - our scratchpad, I'm going to change the set
value, we only need, let's only change the
373:36 - description and make our lives a little bit
easier here, call this colon D. And we'll
373:37 - save that there. And the next thing we'll
do is go back to our expressions, and I'm
373:38 - going to call this hash, hash D, that's gonna
be a description, whoops, can't see what I'm
373:39 - doing description. And then we will just take
out the second one here, we'll make our way
373:40 - over to values, I'm gonna change this to a
colon D. And this is going to be a string,
373:41 - I'm going to get rid of the second value,
what we're going to do is just shorten to
373:42 - the, the the item. So we'll go back to item.
And so this is the string here. And I'm just
373:43 - going to change it here. So it says ship commanded
by Admiral James laden on which Benjamin Sisko
373:44 - served, I'm just gonna take off the end here.
Or maybe I'll just serve as so that means
373:45 - the same thing as first officer. Okay, and
what we'll do is we'll go back to our scratchpad,
373:46 - and we will copy all the stuff. And it also
has this return value, since let's take a
373:47 - look at that before we move on. Use return
values if you want to get the item attributes
373:48 - as they appear before or after they are updated.
So none is if returns values if not specified,
373:49 - all old. So return all the attributes of the
item, return only the update attributes, return
373:50 - all the new attributes of the items, etc.
And this kind of stuff is kind of important
373:51 - when we get into dynamodb streams, because
you get you can do the old or the new. In
373:52 - this case, I guess we're doing all and all
seems fine to me. This is an example. That's
373:53 - all we're doing here. And what we'll do is
we'll just copy this here, fingers crossed,
373:54 - this works first time. And we've got data
back and it says x Oh. So that looks great
373:55 - to me. And what we'll do is we'll just make
sure that it has been changed, we'll use our
373:56 - get item up here. And it has been updated.
So there you go, that is update item. So now
373:57 - that we've learned how to get put, delete,
let's look at the scan and query options next.
373:58 - So let's first look at scan because scan is
a lot, a lot easier to understand. There's
373:59 - a lot less going on there. So if we go over
here, we can see we have a query and scan
374:00 - option. Now if you created a table without
a source key, you'd only be able to scan.
374:01 - But we'll go to scan here and we'll look at
what options we have. So the way scan works
374:02 - is that it returns all the records. And then
you can apply filters after it's returned
374:03 - those records and then filter out what you
want. So if you have a very large table 1000s
374:04 - upon 1000s of records, that's not very efficient.
So generally, you always want to use query
374:05 - when you can. But if you don't have a need
to have a sore key, then you could have a
374:06 - table that just has scan. But anyway, if we
scan, we hit start, it returns everything.
374:07 - And if we want to filter stuff out, we could
choose anything we want here, I could say
374:08 - ship class will we'll say Luna we'll hit start
search. And we have some other options here
374:09 - like be begin with contains with all these
kind of options here that we can use to filter.
374:10 - But just understand that even though we filter
this out, it's already returned all 100 Records
374:11 - and then it filters them out and returns it.
So we're using up a lot of capacity when we're
374:12 - doing that. But let's look at how we can do
this via the COI. So we'll go back to the
374:13 - top here. And we will look for a scan. I feel
like scan would have a lot of options. Oh
374:14 - yeah, there's tons of options, tons of options.
We can segment stuff, we can do page size,
374:15 - all sorts of things. But let's go down and
look at an example here and this is an example
374:16 - that we are given. It looks a lot similar
to the Item one here. And we're just going
374:17 - to do something simple. So we'll do starships.
And I'm thinking, I'm thinking for this, what
374:18 - we'll do is we will filter out the description
for something. So I'm going to go back here
374:19 - to the front here. Let's say we want to get
this record or something that or something
374:20 - that starts with science. Maybe something
more common would be nice, maybe destroyed.
374:21 - So we'll look for all the records that start
with destroyed with the description. So I'll
374:22 - go back here. And what I'm going to do is
I'm going to think about this for a second.
374:23 - For the scan. I don't need to project anything,
these are records that we get back remember
374:24 - projection is if we just want to cherry pick
what we want. For the Filter Expression, we're
374:25 - just going to do a begins with. So I'm going
to do begins with and for this, it's going
374:26 - to be description. I'm just gonna do colon
D here. And then down below, we need to specify
374:27 - our names. Actually, we don't need any names
because we're not remapping anything. Remember,
374:28 - we did that before we did this remapping that's
just extremely verbose. We don't need to do
374:29 - that. I'm just going to do curlies. Here.
I'm going to do quotations colon de Colon
374:30 - curlies, and we're going to make it a string,
and then I just want it to return destroyed.
374:31 - So I just want you to notice that we didn't
have to extract this out as a JSON file, we
374:32 - can write them in line like this. And it makes
things a lot easier for us. But you're probably
374:33 - wondering what this begins with thing is,
so let's go take a look back at scan. And
374:34 - specifically look at these filter expressions.
So if we just hit enter there, if we click
374:35 - on to this tab here, I think it'll give us
more information about it. So down below,
374:36 - it shows you you can do equals these symbols.
And then we have begins with this maps, all
374:37 - with this kind of stuff here. But so begins
with in order to use it, we have to write
374:38 - that and then a then substitute whatever the
thing is. And that's what we've done, we've
374:39 - said, We want the column description replaced
with colon D, and colon D, is going to be
374:40 - destroyed. So anything that begins with destroyed
so let's give this a go and see if it works.
374:41 - We'll enter expecting a property and close
with double quotation. So I quickly made a
374:42 - mistake here, it's a little bit hard to see.
It's because I'm using doubles on the outside,
374:43 - I'm gonna use singles on the outside, we're
just having a conflict of quotations. And
374:44 - there we go, we got a lot of records back,
this is a little bit of a mess to look at,
374:45 - I'm gonna go and try output. I don't know
why I'm gonna try Table No, no good. You think
374:46 - they'd make that actually look like a table,
we'll go to text that's a little bit easier
374:47 - to see. And we can see we're getting all the
records back that are destroyed. So that is
374:48 - for scan. Now let's take a look at how to
use query. Okay, so we'll take a look here
374:49 - now at query. So if we make our way over to
the, our actual table here where we can visualize
374:50 - it, we'll go over to query and with query,
you have to specify the partition. I think
374:51 - a third key is optional. Let's take a look.
So if I just wanted Luna, I'm just going to
374:52 - click off the filter here, that's going to
give us that and then if we want to narrow
374:53 - it down further, I can grab the registry here.
And we'll hit equals, the difference here
374:54 - is that it's only returning these records,
it's not returning all 1000 or 100 Records.
374:55 - And then you can apply filters after the fact.
So I could even filter this further and just
374:56 - say, a description. And if the string begins
with the, it's obviously going to do that.
374:57 - And then you have projected columns here,
well, let's start, um, can only be used when
374:58 - creating with an index name, okay? I don't
know. But anyway, the point is, is that that
374:59 - you need to know is that you have a partition
key. And you can optionally add that or so
375:00 - you have to add that and then you have an
optional third key. And these are your options.
375:01 - And then you can again, filter further and
you can even specify these if you want again,
375:02 - that'd be a bit silly to do. But let's learn
how to do it query using the CLR. So we'll
375:03 - go down here, type the word query, and we
will make our way over to the documentation
375:04 - and type in query. And query probably has
a lot of options. Holy smokes, look at all
375:05 - those options. We're not going to go through
all through all of them. And go down to the
375:06 - examples and this is the example that it gives
us so we have Dynamo DB query table name.
375:07 - projection expression. And then we have one
called key condition expression, this one
375:08 - is different. And also this expression attributes
value we had that before. But if we go here
375:09 - and just take a look at this, what this does
the condition that specifies the key values
375:10 - for items being retrieved by reaction. So
what this is saying is, you're pretty much
375:11 - just providing these values here. So this
part right here, see this. That's what we're
375:12 - doing with that attribute there. So we'll
just go grab our example here. save ourselves
375:13 - some trouble. Make your way back here. And
we're going to specify starships. And in this
375:14 - case, we need to, we're not going to project
any values by I'm just gonna cut that out,
375:15 - because I should show you that at least once
how to do that. And for this, we're going
375:16 - to do ship class. And it's just gonna equal
coal and C. And I'm just going to do this
375:17 - in line because I just want to make this super
easy for us. We'll do curlies or single quotations,
375:18 - we learned our lesson Last time, we should
make these symbols on the outside. And we'll
375:19 - do doubles. Colon C. Colon curlies, doubles,
S for string colon, and let's filter out for
375:20 - galaxy. I know that's the type of starship
and Star Trek. So if we do this, we copy that
375:21 - there, enter, we get some value. So there
you go. That's all the the galaxy ones, we
375:22 - can go up output and do text here make our
lives a bit easier, it's a bit easier to read.
375:23 - Let's say we just wanted to get the registry
number. paste that in there. And I'm just
375:24 - going to change this to output text. Oops,
did not like that need a backslash on the
375:25 - end there. Holy smokes. Output its output.
Oh, it doesn't like the projection expression
375:26 - that I put in there. I'm pretty certain that
we can do that. Good example here, this, this
375:27 - one has one in it. Oh, you know what, because
I named it all caps. It has to be the name
375:28 - of what the actual field is. You can't just
put all caps. By mistake. try this again.
375:29 - Mmm hmm. Maybe I should go a little bit slower
here. So I'm just going to sometimes when
375:30 - you have these issues with this, a smart thing
to do is just make it a single line was backslashes
375:31 - can be tricky sometimes. There we go. So I
don't know I had some kind of error in here
375:32 - syntax. Maybe I didn't have a backslash there.
I didn't have a trailing space after it. It
375:33 - was messing up. But that's always a trick
that you can use to fix your queries. And
375:34 - I'm just going to hit enter there and just
clean this up a little bit. Not that not that
375:35 - we need to do anything else here. But let's
just make sure that it works this way. I try
375:36 - this one more time. There you go. So yeah,
that is the query. Maybe we just take a look
375:37 - here and look if there's any other important
fields that we glazed over. Scan index forward
375:38 - is a great way of flipping things the other
way. So if you did scan index for it, we'll
375:39 - just play around with some of the settings,
why not? So look at the order that it's in
375:40 - right now. So the D is first. I put that in
there and I say false. I think it's all caps
375:41 - for this. Enter. False. I should really just
read it, but I'm pretty sure now I have to
375:42 - read it. That's pretty sure that it's just
like false or true. I need an example. I can't
375:43 - tell. Oh, it's sorry. It's just scan for it
index or no scan and forward index, you don't
375:44 - actually provided a value. So if I paste that
in there, I think that's all we have to do.
375:45 - Yep, so there it's in the opposite order.
And so if you want it to be the default there,
375:46 - you could do it the other way. Right. Let's
look at this any other values of interest
375:47 - here? We looked at projection there. No, I'd
say that's pretty good. So now we are all
375:48 - done here. For this, I think we'll just take
a look at the transact COI stuff. Next here,
375:49 - we're not going to do them because they're
a little bit more work to set up. But it's
375:50 - good to know how they work. Let's just take
a quick look here at transactional transactional
375:51 - API calls for dynamodb, we have the get in
the right, we're not going to set these up,
375:52 - it's a lot of work to do. So it's just good
to conceptually know what you can do here
375:53 - and just to see how it's specified, so transact
get items, synchronous operations that automatically
375:54 - retrieves multiple items from one or more
tables. That's the key thing, one or more
375:55 - tables. And the idea is that if any of these
things happen, then the entire transaction
375:56 - will roll back. This is really good when you
have sensitive information that that could
375:57 - be committed that relies on multiple things
same with gets and writes. But if we go to
375:58 - examples down below here, we can see that
its database, dynamodb, transact get items,
375:59 - and then you provide the items that you want.
So in here, it looks like it's actually the
376:00 - same table. And it's saying that both we want
both of these records. And if there's a failure
376:01 - on one or the other, when grabbing this information,
it's going to roll back. So it has to get
376:02 - all the information or none of that. Let's
go and take a look at the rights. This is
376:03 - very similar This is when you have a right
condition. And there is a lot of stuff going
376:04 - on here, you probably want to read through
all this is probably a good idea, a condition
376:05 - and one or more expression is not met and
ongoing operation of the process updating
376:06 - the same time. This one's a very important
one where you have to update at the same time
376:07 - there is insufficient provision capacity,
an item size becomes too large, their grid
376:08 - size of items, exceeds four megabytes, there
is a user error, we're going to go up here
376:09 - at the top look at the example. And again,
it's very similar. And you could specify different
376:10 - tables here. So this is really good for cross
table rights. Um, but you can see there's
376:11 - a lot of information here. So just be aware
of transact, write items and get items and
376:12 - just consider that everything must be successful
in order for them for it to do the guts of
376:13 - the rights. And it's cross table. So we are
almost done. Well, we only need to learn one
376:14 - more COI command and this is to clean up the
entire project. And that is the Delete table
376:15 - command. We're going to go ahead and delete
our table. Now we could just go in here, and
376:16 - hit Delete table. But that's not fun. Let's
use the COI. So we're gonna go delete table.
376:17 - And we'll look at the example. I think it's
just as simple as that. Yeah, it's that simple.
376:18 - If you delete through here would ask you to
like maybe you might want to back it up. We
376:19 - do not want to back this table up, we want
it gone. And I'm gonna type it in starships
376:20 - here. And we'll hit enter there. And now you
can see that is deleting if we want to get
376:21 - an update, we can go over here and see that
it is deleting might already be gone. Hope
376:22 - it's already gone. And if it was taking time
to delete, we could use the describe command
376:23 - to give it a look here. And we can see that
nothing is being returned because there is
376:24 - no longer a table. So there you go, that is
the Dynamo DB run through. Um, if you have
376:25 - more time on your own, you might want to try
to set up a dynamodb stream. And those are
376:26 - kind of interesting to see I'm just gonna
type in starships again here. You don't you
376:27 - don't have to do any of this, I'm just going
through it just so I can show you dynamodb
376:28 - streams here, ship class registry. So dynamodb
streams, once this table creates, we'll give
376:29 - it a second. If you want to set those up,
it's under under triggers, you create a trigger,
376:30 - you'd create a existing function. And the
idea here is, whenever new records come into
376:31 - the database, it will call that that lambda
function. And then from that lambda function,
376:32 - you can then send it to wherever you want.
Generally, you'd send it to kinesis firehose,
376:33 - or some kinesis service. But that's just the
way to react to data in here. But yeah, this
376:34 - is outside the scope of the developer associate.
It's kind of a stretch goal. If you want a
376:35 - little personal project to do, but our table
is deleted, I'm just gonna go delete it here.
376:36 - And you can see that it says create a backup
before deleting, we're not going to do that.
376:37 - Um, but yeah, that's dynamodb there. So there
you go. Hey, this is Andrew Brown from exam
376:38 - Pro. And we are looking at elastic Compute
Cloud EC two, which is a cloud computing service.
376:39 - So choose your storage memory, network throughput,
and then launch and SSH into your server within
376:40 - minutes. Alright, so we're on to the introduction
to EC two. And so EC T was a highly configurable
376:41 - server. It's a resizeable compute capacity.
It takes minutes to launch new instances,
376:42 - and anything and everything I need is used
as easy to instance underneath. So whether
376:43 - it's RDS or our ECS ORS Simple System Manager.
I highly, highly believe that at AWS, they're
376:44 - all using EC two. Okay. And so we said they're
highly configurable. So what are some of the
376:45 - options we have here? Well, we get to choose
an Amazon machine image, which is going to
376:46 - have our LS. So whether you want redhead,
Ubuntu windows Amazon links or Susi, then
376:47 - you choose your instance type. And so this
is going to tell you like how much memory
376:48 - you want versus CPU. And here, you can see
that you can have very large instances. So
376:49 - here is one server that costs $5 a month,
and here you have one that's $1,000 a month.
376:50 - And this one has 36 CPUs and 16 gigabytes
of memory with 10 gigabyte performance, okay,
376:51 - then you add your storage, so you could add
EBS, or Fs and we have different volume types
376:52 - we can attach. And then you can configure
your your instance. So you can secure it and
376:53 - get your key pairs, you can have user data,
Iam roles and placement groups, which we're
376:54 - all going to talk about starting now. Alright,
so we're gonna look at instance types and
376:55 - what their usage would be. So generally, when
you launch an EC two instance, it's almost
376:56 - always going to be in the T two, or the T
three family. And yes, we have all these little
376:57 - acronyms which represent different types of
instance types. So we have these more broad
376:58 - categories. And then we have subcategories,
or families of instances that are specialized.
376:59 - Okay, so starting with general purpose, it's
a balance of compute and compute memory and
377:00 - networking resources. They're very good for
web servers and code repository. So you're
377:01 - going to be very familiar with this level
here, then you have compute optimized instances.
377:02 - So these are ideal for compute bound applications
that benefit from high performance processors.
377:03 - And as the name suggests, this is compute
it's going to have more computing power. Okay,
377:04 - so scientific modeling, dedicated gaming servers,
and ad server engines. And notice they all
377:05 - start with C. So that makes it a little bit
easier to remember, then you have memory optimized,
377:06 - and as the name implies, it's going to have
more memory on the server. So fast performance
377:07 - for workloads that process large data sets
in memory. So use cases in memory caches in
377:08 - memory databases, real time big data analytics,
then you have accelerated optimized instances,
377:09 - these are utilizing hardware accelerators,
or co processors, they're going to be good
377:10 - for machine learning, computational finance,
seismic analysis, be quick recognition, really
377:11 - cool. Future tech uses a lot of accelerated
optimized instances. And then you have storage
377:12 - optimized. So this is for high sequential
reads and write access to very large data
377:13 - sets on local storage. your use cases might
be a no SQL database in memory or transactional
377:14 - databases or data warehousing. So how is it
important? How important is it to know all
377:15 - these families, it's not so important to associate
track at the professional track, you will
377:16 - need to know themselves, all you need to know
are these general categories and what and
377:17 - just kind of remember, which, which fits into
where and just their general purposes. Alright.
377:18 - So in each family of EC two instance types,
so here we have the T two, we're going to
377:19 - have different sizes, and so we can see small,
medium, large x large, I just wanted to point
377:20 - out that generally, the way the sizing works
is you're gonna always get double of whatever
377:21 - the previous one was. Generally, I say generally,
because it does vary. But the price is almost
377:22 - always double. Okay, so from small to medium,
you can see the ram has doubled, the CPU has
377:23 - doubled for medium large, isn't exactly doubled.
But for a year, the CPU has doubled. Okay,
377:24 - but the price definitely definitely has doubled,
almost nearly so it's almost always twice
377:25 - inside. So general rule is if you're wondering
when you should upgrade, if you need to have
377:26 - something, then you're better off just going
to the next version. So we're gonna look at
377:27 - the concept called incidence profile. And
this is how your EC two instances get permissions,
377:28 - okay? So instead of embedding your AWS credentials,
your access key and secret in your code, so
377:29 - your instance has permissions to access certain
services, you can attach a role to an instance
377:30 - via an instance profile. Okay? So the concept
here is you have to do instance, and you have
377:31 - an instance profile, and that's just the container
for a role, and then you have the role that
377:32 - actually has the permissions. Alright. And
so I do need to point out that whenever you
377:33 - have the chance to not embed ABS credentials,
you should never embed them. Okay, that's
377:34 - like a hard rule with AWS. And anytime you
see an exam question on that, definitely Always
377:35 - remember that the way you set an instance
profile tuneecu instance, if you're using
377:36 - the wizard, you're going to see the IM role
here. And so you're going to choose you're
377:37 - going to create one and then attach it. But
there's one thing that people don't see is
377:38 - they don't see that instance profile because
it's kind of like this invisible step. So
377:39 - if you're using In the console, it's actually
going to create it for you. If you're doing
377:40 - this programmatically through cloud formation,
you'd actually have to create an instance
377:41 - profile. So sometimes people don't realize
that this thing exists. Okay. We're gonna
377:42 - take a look here at placement groups, the
placement groups let you choose the logical
377:43 - placement of your instances to optimize for
communication performance, or durability.
377:44 - And placement groups are absolutely free.
And they're optional, you do not have to launch
377:45 - your EC two instance in within a placement
group. But you do get some benefits based
377:46 - on your use case. So let's first look at cluster.
So cluster packs instances close together
377:47 - inside an AZ and they're good for low latency
network performance for tightly coupled node
377:48 - to node communication. So when you want servers
to be really close together, so communication
377:49 - super fast, and they're well suited for high
performance computing HPC applications, but
377:50 - clusters cannot be multi az, alright, then
you have partitions. And so partitions spread
377:51 - instances across logical partitions. Each
partition does not share the underlying hardware.
377:52 - So they're actually running on individual
racks here for each partition. They're well
377:53 - suited for large distributed and replicated
workloads, such as Hadoop, Cassandra, and
377:54 - Kafka, because these technologies use partitions.
And now we have physical partitions. So that
377:55 - makes total sense there, then you have spread.
And so spread is when each instance is placed
377:56 - on a different rack. And so when you have
critical instances that should be kept separate
377:57 - from each other. And this is the case where
you use this. And you can spread a max of
377:58 - seven instances and spreads can be multi az,
okay, whereas clusters are not allowed to
377:59 - go multi AZ. So there you go. So user data
is a script, which will automatically run
378:00 - when launching easy to instance. And this
is really useful when you want to install
378:01 - packages or apply updates or anything you'd
like before the launch of a an instance. And
378:02 - so when you're going through the easy to wizard,
there's this advanced details, step where
378:03 - you can provide your bash script here to do
whatever you'd like. So here I have it installing
378:04 - Apache, and then it starts that server, if
you were logging into any CPU instance, and
378:05 - you didn't really know whether user data script
was performed on that instance, on launch,
378:06 - you could actually use the this URL at 169
24 116 24. If you were to curl that within
378:07 - that easy to instance, with user data, it
would actually return whatever script was
378:08 - run. So that's just good to know. But yeah,
user data scripts are very useful. And I think
378:09 - you will be using one. So metadata is additional
information about your EC two instance, which
378:10 - you can get at runtime. Okay. So if you were
to SSH into your EC two instance, and run
378:11 - this curl command with latest metadata on
the end, you're going to get all this information
378:12 - here. And so the idea is that you could get
information such as the current public IP
378:13 - address, or the app ID that was used to launch
the students, or maybe the instance type.
378:14 - And so the idea here is that by being able
to do this programmatically, you could use
378:15 - a bash script, you could do something with
user data metadata to perform all sorts of
378:16 - advanced Eva staging operations. So yeah,
better data is quite useful and great for
378:17 - debugging. So yeah. So it's time to look at
the easy to cheat sheet here. So let's jump
378:18 - into it. So elastic Compute Cloud, easy to
use is a cloud computing service. So you configure
378:19 - your EC to by choosing your elastic storage,
memory and network throughput, and other options
378:20 - as well. Then you launch an SSH into your
server within minutes. ec two comes in a variety
378:21 - of instance types specialized for different
roles. So we have general purpose, that's
378:22 - for balance of compute memory and network
resources, you have compute optimized, as
378:23 - the name implies, you can get more computing
power here. So I deal for compute bound applications
378:24 - that benefit from high performance processors,
then you have memory optimized. So that's
378:25 - fast performance for workloads that process
large data sets in memory, then you have accelerated
378:26 - optimized that's hardware accelerators or
co processors, then you have storage optimized
378:27 - that's high sequential read and write access
to very large datasets on local storage. Then
378:28 - you have the concept of instant sizes. And
so instance sizes generally double in price
378:29 - and key attributes. So if you're ever wondering
when it's time to upgrade, just think when
378:30 - you're need double of what you need that time
to upgrade. Then you have placement groups,
378:31 - and they let you choose the logical placement
of your instances to optimize communication
378:32 - performance, durability, and placement groups
are free, it's not so important to remember
378:33 - the types are because I don't think we'll
come up with a solution architect associate.
378:34 - And then we have user data. So a script that
will be automatically run when launching EC
378:35 - two instance, for metadata. Metadata is about
the current instance. So you could access
378:36 - this metadata via a local endpoint when SSH
into an easy to instance. So you have this
378:37 - curl command here with metadata and meta data
could be the instance type, current IP address,
378:38 - etc, etc. And then the last thing is instance
profile. This is a container for an IM role
378:39 - that you can use to pass roll information
to an easy to instance, when the instance
378:40 - starts. Alright, so there you go, that's easy
to do. Hey, this is Andrew Brown from exam
378:41 - Pro. And we are looking at Virtual Private
Cloud known as VPC. And this service allows
378:42 - you to provision logically isolated sections
of your database cloud where you can launch
378:43 - eight of his resources in a virtual network
that you define. So here we are looking at
378:44 - an architectural diagram of a VPC with multiple
networking resources or components within
378:45 - it. And I just want to emphasize how important
it is to learn VPC and all components inside
378:46 - and out because it's for every single aidable
certification with the exception of the cloud
378:47 - practitioner, so we definitely need to master
all these things. So the easiest way to remember
378:48 - what a VPC is for is think of it as your own
personal data center, it gives you complete
378:49 - control over your virtual networking environment.
Alright, so the idea is that we have internet,
378:50 - it flows into an internet gateway, it goes
to a router, the router goes to a route table,
378:51 - the route table passes through knakal. And
the knakal sends the traffic to the public
378:52 - and private subnets. And your resources could
be contained within a security group all within
378:53 - a VPC. So there's a lot of moving parts. And
these are not even all the components. And
378:54 - there's definitely a bunch of different configurations
we can look at. So looking at the core components,
378:55 - these are the ones that we're going to learn
in depth, and there are a few more than these,
378:56 - but these are the most important ones. So
we're gonna learn what an internet gateway
378:57 - is. We're gonna learn what a virtual private
gateway is route tables, knackles, security
378:58 - groups, public and private subnets, Nat gateway
and instances customer gateway VPC endpoints
378:59 - and VPC peering. So, this section is very
overwhelming. But you know, once you get it
379:00 - down, it's it's pretty easy going forward.
So we just need to master all these things
379:01 - and commit them to memory. So now that we
kind of have an idea, what's the purpose of
379:02 - VPC, let's look at some of its key features,
limitations and some other little things we
379:03 - want to talk about. So here on the right hand
side, this is the form to create a VPC, it's
379:04 - literally four fields. It's that simple. You
name it, you give it an address, you can also
379:05 - give it an additional ipv6 address. You can't
be or it's either this and this. And you can
379:06 - set its tendencies to default or dedicated,
dedicated, meaning that it's running on dedicated
379:07 - hardware. If you're an enterprise, you might
care about that. This is what the ipv6 cider
379:08 - block would look like because you don't enter
it in Amazon generates one for you. So v PCs
379:09 - are region specific. They do not span regions,
you can create up to five v PCs per region.
379:10 - Every region comes with a default VPC, you
can have 200 subnets per v PC, that's a lot
379:11 - of subnets. You can create, as we said here,
an ipv4 cider block, you actually have to
379:12 - create one it's a requirement. And in addition
to you can provide an ipv6 cider block. It's
379:13 - good to know that when you create a VPC, it
doesn't cost you anything. That goes the same
379:14 - for route tables, knackles, internet gateway
security groups subnets and VPC peering. However,
379:15 - there are resources within the VPC that are
going to cost you money such as Nat gateways,
379:16 - VPC endpoints, VPN gateways, customer gateways,
but most of the time, you'll be working with
379:17 - the ones that don't cost any money so that
there shouldn't be too much of a concern of
379:18 - getting over billed. One thing I do want to
point out is that when you do create a VPC,
379:19 - it doesn't have DNS host names turned on by
default. If you're wondering what that option
379:20 - is for what it does is when you launch easy
two instances, and so here down below, I have
379:21 - an easy to instance, and it will get a public
IP, but it will only get a public DNS, which
379:22 - looks like a domain name like an address.
And that's literally what it is. But if this
379:23 - isn't turned on that easy to instance, won't
get one. So if you're wondering, why isn't
379:24 - that there, it's probably because your host
names are disabled and they are disabled by
379:25 - default. You just got to turn that off. So
we were saying earlier that you get a default
379:26 - VPC for every single region. And the idea
behind that is so that you can immediately
379:27 - launch EC two instances without having to
really think about all the networking stuff
379:28 - you have to set up. But for a VA certification,
we do need to know what is going on. And it's
379:29 - not just a default VPC It comes with other
things and with specific configurations and
379:30 - we definitely need to know that for the exams.
So the first thing is it creates a VPC of
379:31 - cider block size 16. We're going to also get
default subnets with it. So for every single
379:32 - AZ in that region, we're going to get a subnet
per AZ and they're going to be a cider block
379:33 - size 20. It's going to create an internet
gateway and connect it to your default VPC.
379:34 - So that means that our students is going to
reach the internet, it's going to come with
379:35 - a default security group and associated with
your default VPC. So if you launch an EC two
379:36 - instance, it will automatically or default
to the security group unless you override
379:37 - it. It will also come with by by default,
a knakal. And associated with your VPC, it
379:38 - will also default DHCP options. One thing
that it's implied is that you It comes with
379:39 - a main route table, okay, so when you create
a VPC, it automatically comes to the main
379:40 - route table. So I would assume that that comes
by default as well. So there are all the default.
379:41 - So I just wanted to touch on this 0.0 dot
zero forward slash zero here, which is also
379:42 - known as default. And what it is, is it represents
all possible IP addresses. Okay. And so you
379:43 - know, when you're doing a device networking,
you're going to be using this to get the GW
379:44 - to have a route like routing traffic to the
GW to the internet. When you're using a security
379:45 - group, when you set up your inbound rules,
you're going to set 0.0 dot 0.0 to allow any
379:46 - traffic from the internet to access your public
resources. So anytime you see this, just think
379:47 - of it as giving access from anywhere or the
internet. Okay. We're looking at VPC peering,
379:48 - which allows you to connect one VPC to another
over direct network route using private IP
379:49 - addresses. So the idea is we have VPC, a VPC
Zb. And we want to treat it so like the they
379:50 - behave like they're on the same network. And
that's what VPC peering connection allows
379:51 - us to do. So it's very simple to create a
peering connection, we just give it a name,
379:52 - we say V, what we want is the requester. So
that could be VP ca and then we want as the
379:53 - acceptor which could be VP CB, and we can
say whether it's in my account, or another
379:54 - account, or this region or another region.
So you can see that allows pvcs from same
379:55 - or different regions to talk to each other.
There is some limitations around the configuration.
379:56 - So you know, when you're peering, you're using
star configuration, so you'd have one central
379:57 - VPC and then you might have four around it.
And so for each one, you're going to have
379:58 - to have a peering connection. There's no transitive
peering. So what does that mean? Well, the
379:59 - idea is like, let's say VPC c wants to talk
to VPC, B, the traffic's not going to flow
380:00 - through a, you actually would have to create
another direct connection from C to B. So
380:01 - it's only to the nearest neighbor, where that
communication is going to happen. And you
380:02 - can't have overlapping cider blocks. So if
these had the same cider block, this was 172
380:03 - 31. This was 172 31, we're gonna have a conflict
and we're not gonna be able to talk to each
380:04 - other. So that is the VPC peering in a nutshell.
Alright, so we're taking a look here at route
380:05 - tables. The route tables are used to determine
where network traffic is directed, okay. And
380:06 - so each subnet in your V PC must be associated
with a route table. And a subnet can only
380:07 - be associated with one route table at a time,
but you can associate multiple subnets subnets
380:08 - with the same route table. Alright, so now
down below, I have just like the most common
380:09 - example of where you're using route tables.
And that's just allowing your easy two instances
380:10 - to gain access to the internet. So you'd have
a public subnet where that easy to instance
380:11 - resides and that's going to be associated
with a route table that Route Route table
380:12 - is going to have us routes in here. And here
you can see we have a route which has the
380:13 - internet gateway attached that allows access
to the internet. Okay, so there you go. That's
380:14 - all there is to route two. We're taking a
look at Internet gateway internet gateway
380:15 - allows your VPC access to the internet and
I N GW does two things. It provides a target
380:16 - in your VPC roundtables for internet routable
traffic. And it can also perform network address
380:17 - translation Nat which we'll get into in another
section for instances that have been assigned
380:18 - a public ipv4 address. Okay, so down below
here, I have a representation of how I GW
380:19 - works. So the idea is that we have internet
over here and to access the internet, we need
380:20 - an internet gateway, but to route traffic
from our EC to instances or anything. They're
380:21 - gonna have to pass through a route table to
get to our router. And so we need to create
380:22 - a new route in our route table for the GW
so I GW hyphen, Id identifies that resource,
380:23 - and then we're going to give it 0.0 point
zero point Zero as the destination. Alright,
380:24 - so that's all there is to it. So we talked
about how we could use Nat gateways or Nat
380:25 - instances to gain access to the internet for
our EC two instances that live in a private
380:26 - subnet. But let's say you wanted to SSH into
that easy to essence, well, it's in a private
380:27 - subnet, so it doesn't have a public IP address.
So what you need is you need an intermediate
380:28 - EC two instance that you're going to SSH into.
And then you're going to jump from that box
380:29 - to this one, okay? And that's why bastions
are also known as jump boxes. And this institute
380:30 - instance for the bastion is hardened. So it
should be very, very secure, because this
380:31 - is going to be your point of entry into your
private EC two instances. And some people
380:32 - might always ask, Well, if a NAT instance,
like Nat gateways, we can't obviously turn
380:33 - into bastions, but in that instance, is just
any situation it's Couldn't you have a double
380:34 - as a bastion, and the possibility of it is
possible. But generally the way you configure
380:35 - NATS and also, from a security perspective,
you'd never ever want to do that, you'd always
380:36 - want to have a different EC two instance,
as your Bastion. Now, there is a service called
380:37 - SYSTEMS MANAGER, session manager and it replaces
the need for bashes so that you don't have
380:38 - to launch your own EC two instances. So generally,
that's recommended in AWS. But you know, bastions
380:39 - are still being commonly used throughout a
lot of companies because it needs to meet
380:40 - whatever their requirements are, and they're
just comfortable with themselves. There you
380:41 - go. So we're gonna take a look at Direct Connect,
and Direct Connect is in aid of a solution
380:42 - for establishing dedicated network connections
from on premise locations to AWS, it's extremely
380:43 - fast. And so depending on what configuration
you get, if it's in the lower bandwidth, we're
380:44 - looking between 1550 megabytes to 500 megabytes,
or the higher bandwidth is one gigabytes to
380:45 - 10 gigabytes. So the transfer rate to your
on premise environment, the network to AWS,
380:46 - is it considerably fast. And this is can be
really important if you are an enterprise
380:47 - and you want to keep the same level of performance
that you're used to. So yeah, the takeaway
380:48 - here with Direct Connect is that it helps
reduce network costs increase bandwidth throughput,
380:49 - provides a more consistent network experience
than a typical internet internet based connection.
380:50 - Okay, so that's all. Hey, this is Andrew Brown
from exam Pro. And we are looking at auto
380:51 - scaling groups. So auto scaling groups lets
you set scaling rules, which will automatically
380:52 - launch additional EC two instances or shutdown
instances to meet the current demand. So here's
380:53 - our introduction to auto scaling groups. So
auto scaling groups, abbreviated as G contains
380:54 - a collection of EC two instances that are
treated as a group for the purpose of automatic
380:55 - scaling and management. And automatic scaling
can occur via capacity settings, health check
380:56 - replacements, or scaling policies, which is
going to be a huge topic. So the simplest
380:57 - way to use auto scaling groups is just to
work with the capacity settings with nothing
380:58 - else set. And so we have desired capacity,
Min, and Max. Okay, so let's talk through
380:59 - these three settings. So for min is how many
easy two instances should at least be running,
381:00 - okay, Max is the number of easy two instances
allowed to be running and desired capacity
381:01 - is how many easy two instances you ideally
want to run. So when min is set to one, and
381:02 - let's say you had a new auto scaling group,
and you lost it, and there was nothing running,
381:03 - it would always it would spin up one. And
if that server died, for whatever reason,
381:04 - because when it was unhealthy or just crashed,
for whatever reason, it's always going to
381:05 - spin up at least one. And then you have that
upper cap, where it can never go beyond two,
381:06 - because auto scaling groups could trigger
more instances. And this is like a safety
381:07 - net to make sure that you know, you just don't
have lots and lots of servers running. And
381:08 - desired capacity is what you ideally want
to run. So as you will try to get it to be
381:09 - that value. But there's no guarantee that
it will always be that value. So that's capacity.
381:10 - So another way that auto scaling can occur
with an auto scaling group is through health
381:11 - checks. And down here, we actually have two
types, we have EC two and lb. So we're gonna
381:12 - look at EC two first. So the idea here is
that when this is set, it's going to check
381:13 - the EC two instance to see if it's healthy.
And that's dependent on these two checks that's
381:14 - always performed on DC two instances. And
so if any of them fail, it's going to be considered
381:15 - unhealthy. And the auto scaling group is going
to kill that EC two instance. And if you have
381:16 - your minimum capacity set to one, it's going
to then spin up a new EC two instance. So
381:17 - that's the that's the To type now let's go
look at the lb type. So for the lb type, the
381:18 - health check is performed based on an E lb
health check. And the lb can perform a health
381:19 - check by pinging an ENT, like an endpoint
on that server could be HTTP or HTTPS, and
381:20 - it expects a response. And you can say I want
a 200. Back at this specific endpoint, or
381:21 - so here. That's actually what we do. So if
you have a web app, you might make a HTML
381:22 - page called health check. And it should return
200. And if it is, then it's considered healthy.
381:23 - If that fails, then the auto scaling group
will kill that EC two instance. And again,
381:24 - if your minimum is set to one is going to
spin up AI, healthy new EC two instance. final
381:25 - and most important way scaling gets triggered
within an auto scaling group is scaling policies.
381:26 - And there's three different types of scaling
policies. And we'll start with target tracking
381:27 - scaling policy. And what that does is it maintains
a specific metric and a target value. What
381:28 - does that mean? Well, down here, we can choose
a metric type. And so we'd say average CPU
381:29 - utilization. And if it were to exceed our
target value, and we'd set our target value
381:30 - to 75%. Here, then we could tell it to add
another server, okay, whenever we're adding
381:31 - stuff, that means we're scaling out whenever
we are removing instances, we're moving servers,
381:32 - then we're scaling in okay. The second type
of scaling policy is simple scaling policy.
381:33 - And this scales when alarm is breached. So
we create whatever alarm we want. And we would
381:34 - choose it here. And we can tell it to scale
out by adding instances, or scale in by removing
381:35 - instances. Now, this scaling policy is no
longer recommended, because it's a legacy
381:36 - policy. And now we have a new policy that
is similar but more robust. To replace this
381:37 - one, you could still use it. But you know,
it's not recommended. And it's still in the
381:38 - console. But let's look at the one that replaces
it called scaling policies with steps. So
381:39 - same concept you scale based on when alarm
is breach, but it can escalate based on the
381:40 - alarms value, which changes over time. So
before where you just had a single value here,
381:41 - we could say, well, if we have this, this
alarm, and the value is between one and two,
381:42 - then add one instance. And then when it goes
between two and three, then add another instance,
381:43 - or when it exceeds three to beyond, then add
another instance. So you can, it helps you
381:44 - grow based on that alarm, that alarm as it
changes, okay. So earlier, I was showing you
381:45 - that you can do health checks based on l B's.
But I wanted to show you actually how you
381:46 - would associate that lb to an auto scaling
group. And so we have classic load balancers.
381:47 - And then we have application load balancer
and network load balancer. So there's a bit
381:48 - of variation based on the load bouncer how
you would connect it, but it's pretty straightforward.
381:49 - So we're in the auto scaling group settings,
we have these two fields, classic load balancers
381:50 - and target groups. And for classic load balancers,
we just select the load balancer, and now
381:51 - it's associated. So it's as simple as that
it's very straightforward. But with the new
381:52 - ways, there's a target group that's in between
the auto scaling group and the load balancer.
381:53 - So you're associating the target group. And
so that's all there is to it. So that's how
381:54 - you associate. So to give you the big picture
on what happens when you get a burst of traffic,
381:55 - and auto scaling occurs, I just wanted to
walk through this architectural diagram with
381:56 - you. So let's say we have a web server and
we have one easy two instance running, okay,
381:57 - and all of a sudden, we get a burst of traffic,
and that traffic comes into revenue three,
381:58 - revenue three points to our application load
balancer application load balancer has a listener
381:59 - that sends the traffic to the target group.
And we have this these students and switches
382:00 - associated with that target group. And we
have so much traffic that it causes are your
382:01 - CPU utilization to go over 75%. And once it
goes over 75%, because we had a target scaling
382:02 - policy attached, that said anything above
75%, spin up a new EC two instance. That's
382:03 - what the auto scaling group does. And so the
way it does is it uses a launch configuration,
382:04 - which is attached to the auto scaling group,
and it launches a new EC two instance. So
382:05 - that's just to give you the, like, full visibility
on the entire pipeline of how that actually
382:06 - works. So when you have an auto scaling group,
and it launches in institutions, how does
382:07 - it know what configuration to use to launch
a new new ECU instance and that is what a
382:08 - launch configuration is. So when you have
an auto scaling group, you actually set what
382:09 - launch configuration you want to use. And
a launch configuration looks a lot like when
382:10 - you launch a new ECU instance. So you go through
and you'd set all of these options. But instead
382:11 - of launching an instance at the end, it's
actually just saving the configuration hence,
382:12 - it's called a launch configuration. A couple
of limiting Around lost configurations that
382:13 - you need to know is that a launch configuration
cannot be edited once it's been created. So
382:14 - if you need to update or replace that launch
configuration, you need to either make a new
382:15 - one, or they have this convenient button to
clone the existing configuration and make
382:16 - some tweaks to it. There is something also
known as a launch template. And they are launched
382:17 - figurations, just but with versioning. And
so it's AWS new version of lock configuration.
382:18 - And, you know, generally when there's something
new, I might recommend that you use it, but
382:19 - it seems so far that most of the communities
still uses launch configuration. So the benefit
382:20 - of versioning isn't a huge, doesn't have a
lot of value there. So, you know, I don't
382:21 - I'm not pushing you to use launch templates,
but I just want you to know the difference
382:22 - because it is a bit confusing because you
look at it, it looks like pretty much the
382:23 - same thing. And it just has versioning in
here and we can review the auto scaling group
382:24 - cheat sheet. So an S g is a collection of
up to two instances group for scaling and
382:25 - management scaling out is when you add servers
scaling is when you remove servers scaling
382:26 - up is when you increase the size of an instance
so like you'd update the launch configuration
382:27 - with a larger size. The size of an ASC is
based on the min maximum desired capacity.
382:28 - Target scaling policy scales based on when
a target value of a metric is breached. So
382:29 - example average CPU utilization exceeds 75%
simple scaling policy triggers a scaling when
382:30 - an alarm is breach. Scaling policy with steps
is the new version simple scaling policy allows
382:31 - you to create steps based on escalation alarm
values. desired capacity is how many situ
382:32 - instances you want to ideally run an ESG will
always launch instances to meet the minimum
382:33 - capacity. health checks determine the current
state of an instance in nasg. health checks
382:34 - can be run against either an EOB or an EC
two instance, when an auto scaling when an
382:35 - auto scaling group launches a new instance,
it will use a launch configuration which holds
382:36 - the configuration values of that new instance.
For example, maybe the AMI instance type of
382:37 - roll launch configurations cannot be edited
and must be cloned or a new one created. Launch
382:38 - configurations must be manually updated in
by editing the auto scaling group settings.
382:39 - So there you go. And that's everything with
auto scaling. We're looking at VPC endpoints,
382:40 - and they're used to privately connect your
V PC to other Ada services and VPC endpoint
382:41 - services. So I have a use case here to make
it crystal clear. So imagine you have an EC
382:42 - two instance, and you want to get something
from your s3 bucket. So what you normally
382:43 - do is use the ABS SDK and you would make that
call, and it would go out of your internet
382:44 - gateway to the internet back into the ABS
network to get that file or or object out
382:45 - of s3. So wouldn't it be more convenient if
we could just keep the traffic within the
382:46 - AWS network and that is the purpose of a VPC
endpoint. It helps you keep traffic within
382:47 - the network. And the idea is now because it
does not leave a network, we do not require
382:48 - a public IP address to communicate with these
services, I eliminates the need for internet
382:49 - gateway. So let's say we didn't need this
internet gateway, the only reason we were
382:50 - using it too was to get test three, we can
now eliminate that and keep everything private.
382:51 - So you know, there you go. There are two types
of VPC endpoints, inner interface endpoints
382:52 - and gateway endpoints. And we're going to
get into that. So we're going to look at the
382:53 - first type of VPC endpoint and that is interface
endpoints. And they're called interface endpoints
382:54 - because they actually provision an elastic
network interface, an actual network interface
382:55 - card with a private IP address, and they serve
as an entry point for traffic going to a supported
382:56 - service. And if you read a bit more about
interface endpoints, they are powered by at
382:57 - this private link. There's not much to say
here, this is what it is. So Access Services
382:58 - host on a bus easily securely by keeping your
network traffic within a bus network. This
382:59 - is always confused me this branding of Eva's
private link. But you know, you might as well
383:00 - just think of interface endpoints and use
private link to be in the same thing. Again,
383:01 - it does cost something because it is speeding
up and he and I. And so you know, it's it's
383:02 - point 01 cents per hour. And so over a month's
time, if you had it on for the entire time,
383:03 - it's going to cost you around $7.50. And the
interface endpoint supports a variety of data
383:04 - services, not everything. But here's a good
list of them for you. The second type of VCP
383:05 - endpoint is a gateway endpoint, a gateway
endpoint. It has a target for a specific route
383:06 - in your row table used for traffic destined
for a supported database service. And this
383:07 - endpoint is 100% free because you're just
adding something to your route table, and
383:08 - you're going to be utilizing it mostly for
Amazon s3 and dynamodb. So you saw that first
383:09 - use case where I showed you that we were getting
used to doing stock s3 that was using a gateway
383:10 - endpoint. So there you go. Here we are at
the VPC endpoint cheat sheet and this is going
383:11 - to be a quick one, so let's get to it. VPC
endpoints help keep traffic between awa services
383:12 - within the AWS network. There are two kinds
of VPC endpoints interface endpoints and gateway
383:13 - endpoints. interface endpoints cost money,
whereas gateway endpoints are free interface
383:14 - endpoints uses a elastic network interface
in the UI with a private IP address part.
383:15 - And this was all powered by private link.
gateway endpoints is a target for a specific
383:16 - route in your route table. And interface endpoints
support many at the services whereas gateway
383:17 - endpoints only support dynamodb and s3. Hey,
it's Andrew Brown from exam Pro. And we are
383:18 - looking at elastic load balancers also abbreviated
to lb, which distributes incoming application
383:19 - traffic across multiple targets such as easy
to instances containers, IP addresses, or
383:20 - lambda functions. So let's learn a little
bit What a load balancer is. a load balancer
383:21 - can be physical hardware, or virtual software
that accepts incoming traffic and then distributes
383:22 - that traffic to multiple targets. They can
balance the load via different rules. These
383:23 - rules vary based on the type of load balancers.
So for elastic load balancer, we actually
383:24 - have three load balancers to choose from.
And we're going to go into depth for each
383:25 - one, we'll just list them out here. So we
have application load balancer, network, load
383:26 - balancer, and classic load balancer. Understand
the flow of traffic for lbs, we need to understand
383:27 - the three components involved. And we have
listeners rules and target groups. And these
383:28 - things are going to vary based on our load
balancers, which we're going to find out very
383:29 - shortly here. Let's quickly just summarize
what these things are. And then see them in
383:30 - context with some visualization. So the first
one are listeners, and they listen for incoming
383:31 - traffic, and they evaluate it against a specific
port, whether that's Port 80, or 443, then
383:32 - you have rules and rules can decide what to
do with traffic. And so that's pretty straightforward.
383:33 - Then you have target groups and target groups
is a way of collecting all these two instances
383:34 - you want to route traffic to in logical groups.
So let's go take a look first at application
383:35 - load bouncer and network load balancer. So
here on the right hand side, I have traffic
383:36 - coming into repartee three, that points to
our load balancer. And once it goes our load
383:37 - bouncer goes to the listener, it's good check
what port it's running on. So if it's on port
383:38 - 80, I have a simple rule here, which is going
to redirect it to Port 443. So it's gonna
383:39 - go this listener, and this listener has a
rule attached to it, and it's going to forward
383:40 - it to target one. And that target one contains
all these YouTube instances. Okay. And down
383:41 - below here, we can just see where the listeners
are. So I have listener at 443. And this is
383:42 - where application load balancer, you can see
I also can attach a SSL certificate here.
383:43 - But if you look over at rules, and these rules
are not going to appear for network load bouncer,
383:44 - but they are going to appear for a lb. And
so I have some more complex rules. If you're
383:45 - using lb, it simply just forwards it to a
target, you don't get more rich options, which
383:46 - will show you those richer options in a future
slide. But let's talk about classic load balancer.
383:47 - So a classic load balancer is, is much simpler.
And so you have traffic coming in it goes
383:48 - to CLB. You have your listeners, they listen
on those ports, and you have registered targets.
383:49 - So there isn't target groups, you just have
loosey goosey two instances that are associated
383:50 - with the classic load balancer. Let's take
a deeper look at all three load balancers
383:51 - starting with application load balancer. So
application load balancer, also known as a
383:52 - lb is designed to balance HTTP and HTTPS traffic.
It operates at layer seven of the OSI model,
383:53 - which makes a lot of sense because layer seven
is application lb has a feature called request
383:54 - routing, which allows you to add routing rules
to your listeners based on the HTTP protocol.
383:55 - So we saw previously, when we were looking
at rules, it was only for lb that is this
383:56 - is that request routing rules. You can attach
a web application firewall to a lb. And that
383:57 - makes sense because they're both application
specific. And if you want to think of the
383:58 - use case for application load balancer, well,
it's great for web application. So now let's
383:59 - take a look at network load balancer which
is designed to balance TCP and UDP traffic,
384:00 - it operates at the layer four of the OSI model,
which is the transport layer. And it can handle
384:01 - millions of requests per second while still
maintaining extremely low latency. It can
384:02 - perform cross zone load balancing, which we'll
talk about later on. It's great for you know,
384:03 - multiplayer video games are when network performance
is the most critical thing to your application.
384:04 - Let's take a look at classic load balancers.
So it was AWS is first load balancer. So it
384:05 - is a legacy load balancer. It can balance
HTTP or TCP traffic, but not at the same time,
384:06 - it can use layer seven specific features such
as sticky sessions, it can also use a strict
384:07 - layer for for bouncing, purely TCP application.
So that's what I'm talking about where it
384:08 - can do one or the other. It can perform cross
zone load balancing, which we will talk about
384:09 - later on. And I put this one in here, because
it is kind of an exam question. I don't know
384:10 - if it still appears, but it will respond with
a 504 error in case of timeout if the underlying
384:11 - application is not responding. And an application
could be not responding spawning would be
384:12 - example as the web server or maybe the database
itself. So classic load balancer is not recommended
384:13 - for use anymore, but it's still around, you
can utilize it. But you know, it's recommended
384:14 - to use nlb or lb, when possible. So let's
look at the concept of sticky sessions. So
384:15 - sticky Sessions is an advanced load balancing
method that allows you to bind a user session
384:16 - to a specific EC two instance. And this is
useful when you have specific information
384:17 - that's only stored locally on a single instance.
And so you need to keep on sending that person
384:18 - to the same instance. So over here, I have
the diagram that shows how this works. So
384:19 - on step one, we wrote traffic to the first
EC two instance, and it sets a cookie. And
384:20 - so the next time that person comes through,
we check to see if that cookie exists. And
384:21 - we're going to send it to that same EC two
instance. Now, this feature only works for
384:22 - classic load bouncer and application load
bouncer, it's not available for nlb. And if
384:23 - you need to set it for application load bouncer,
it has to be set on the target group and not
384:24 - individually, you see two instances. So here's
a scenario you might have to worry about.
384:25 - So let's say you have a user that's requesting
something from your web application, and you
384:26 - need to know what their IP address is. So
you know, the request goes through and then
384:27 - on the EC two instance, you look for it, but
it turns out that it's not actually their
384:28 - IP address. It's the IP address of the load
balancer. So how do we actually see the user's
384:29 - IP address? Well, that's through the x forwarded
for header, which is a standardized header
384:30 - when dealing with load balancers. So the x
forwarded for header is a command method for
384:31 - identifying the originating IP address of
a connecting, or client connecting to a web
384:32 - server through HTTP proxy or a load balancer.
So you would just forward make sure that in
384:33 - your web application that you're using that
header, and then you just have to read it
384:34 - within your web application to get that user's
IP address. So we're taking a look at health
384:35 - checks for elastic load balancer. And the
purpose behind health checks is to help you
384:36 - route traffic away from unhealthy instances,
to healthy instances. And how do we determine
384:37 - if a instance is unhealthy waltz through all
these options, which for a lb lb is set on
384:38 - the target group or for closet load balancer
is directly set on the load balancer itself.
384:39 - So the idea is we are going to ping the server
at a specific URL at a with a specific protocol
384:40 - and get an expected specific response back.
And if that happens more than once over a
384:41 - specific interval that we specify, then we're
going to mark it as unhealthy and the load
384:42 - balancer is not going to send any more traffic
to it, it's going to set it as out of service.
384:43 - Okay. So that's how it works. One thing that
you really need to know is that e lb does
384:44 - not terminate unhealthy instances, it's just
going to redirect traffic to healthy instances.
384:45 - So that's all you need to know. So here we're
taking a look at cross zone load balancing,
384:46 - which is a feature that's only available for
classic and network load balancer. And we're
384:47 - going to look at it when it's enabled, and
then when it's disabled and see what the difference
384:48 - is. So when it's enabled requests are distributed
evenly across the instances in all the enabled
384:49 - availability zones. So here we have a bunch
of UC two instances in two different Z's and
384:50 - you can see the traffic is even across all
of them. Okay? Now, when it's disabled requests
384:51 - are distributed evenly across instances. It's
in only its availability zone. So here, we
384:52 - can see in az a, it's evenly distributed within
this AZ and then the same thing over here.
384:53 - And then down below if you want to know how
to enable cross zone load balancing, it's
384:54 - under the description tab and you'd edit the
attributes. And then you just check box on
384:55 - cross zone load balancing. Now we're looking
at an application load balancer specific feature
384:56 - called request routing, which allows you to
apply rules to incoming requests, and then
384:57 - for to redirect that traffic. And we can check
on a few different conditions here. So we
384:58 - have six in total. So we have the header host
header source IP path is to be header, ASP
384:59 - header method or query string. And then you
can see we have some, then options, we can
385:00 - forward redirect returned to fixed response
or authenticate. So let's just look at a use
385:01 - case down here where we actually have 1234,
or five different examples. And so one thing
385:02 - you could do is you could use this to route
traffic based on subdomain. So if you want
385:03 - an app to subdomain app to go to Target, prod,
and QA to go to the target QA, you can do
385:04 - that you can either do it also on the path.
So you could have forward slash prod and for
385:05 - slash QA and now it route to the respective
target groups, you could do it as a query
385:06 - string, you could use it by looking at HTTP
header, or you could say all the get methods
385:07 - go to prod on a why you'd want to do this,
but you could and then all the post methods
385:08 - would go to QA. So that is request request
routing in a nutshell. We made it to the end
385:09 - of the elastic load balancer section and onto
the cheat sheet. So there are three elastic
385:10 - load balancers, network application and classic
load balancer and elastic load bouncer must
385:11 - have at least two availability zones for it
to work on elastic load balancers cannot go
385:12 - cross region you must create one per region.
lbs have listeners rules and target groups
385:13 - to route traffic and OBS have listeners and
target groups to route traffic. And CL B's.
385:14 - Us listeners and ECU instances are directly
registered as targets to the CLB. For application
385:15 - load balancer, it uses HTTP s or eight or
without the S traffic. And then as the name
385:16 - implies, it's good for web applications. Network
load balancer is for TCP, UDP, and is good
385:17 - for high network throughput. So think maybe
like multiplayer video games, classic load
385:18 - balancer is legacy. And it's recommended to
use a lb, or nlb when you can, then you have
385:19 - the x forwarded for. And the idea here is
to get the original IP of the incoming traffic
385:20 - passing through the lb, you can attach web
application firewall to a lb. Because you
385:21 - know web application firewall has application
the name and nlb in CLB do not, you can attach
385:22 - an Amazon certification manager SSL certificate.
So that's an ACM to any of the L B's. To get
385:23 - SSL. For a lb you have advanced request routing
rules, where you can route based on subdomain,
385:24 - header path and other SP information. And
then you have sticky sessions which can be
385:25 - enabled for CLB or lb. And the idea is that
it helps the session remember, what would
385:26 - you say to instance, based on cookie? Hey,
it's Andrew Brown from exam Pro, we are looking
385:27 - at security groups. And they help protect
our EC two instances by acting as a virtual
385:28 - firewall controlling the inbound and outbound
traffic, as I just said, security groups acts
385:29 - as a virtual firewall at the instance level.
So you would have an easy to instance and
385:30 - you would attach to it security groups. And
so here is an easy to instance. And we've
385:31 - attached the security group to it. So what
does it look like on the inside for security
385:32 - groups, each security group contains a set
of rules that filter traffic coming into.
385:33 - So that's inbound, and out of outbound to
that easy to instance. So here we have two
385:34 - tabs, inbound and outbound. And we can set
these are rules, right. And we can set these
385:35 - rules with a particular protocol and a port
range. And also who's allowed to have access.
385:36 - So in this case, I want to be able to SSH
into this YSU instance, which uses the TCP
385:37 - protocol. And the standard port for SSH is
22. And I'm going to allow only my IP. So
385:38 - anytime you see forward slash 32 that always
means my IP. All right. So that's all you
385:39 - have to do to add inbound and outbound rules.
There are no deny rules. So all traffic is
385:40 - blocked by default unless a rule specifically
allows it. And multiple instances across multiple
385:41 - subnets can belong to a security group. So
here I have three different EC two instances,
385:42 - and they're all in different subnets. And
security groups do not care about subnets.
385:43 - You just assign easy to instance, to a security
group and you know, just in this case, and
385:44 - they're all in the same one, and now they
can all talk to each other. Okay.
385:45 - Here, I have three security group scenarios.
And they all pretty much do the same thing.
385:46 - But the configuration is different to give
you a good idea of variation on how you can
385:47 - achieve things. And so the idea is we have
a web application running on a situ instance.
385:48 - And it is connecting to an RDS database to
get its information running in a private subnet.
385:49 - Okay. And so in the first case, what we're
doing is we have an inbound rule on the SU
385:50 - database saying allowing anything from 5432,
which is the Postgres port number, for this
385:51 - specific IP address. And so it allows us to
see one instance to connect to that RDS database.
385:52 - And so the takeaway here is you can specify
the source to be an IP range, or a specific
385:53 - IP. And so this is very specific, it's forward
slash 32. And that's a nice way of saying
385:54 - exactly one IP address. Now in the second
scenario, it looks very similar. And the only
385:55 - difference is, instead of providing an IP
address as the source, we can provide another
385:56 - security group. So now anything within the
security group is allowed to gain access for
385:57 - inbound traffic on 5432. Okay, now, in our
last use case, down below, we have inbound
385:58 - traffic on port 80, and inbound traffic on
port 22, for the SG public group, and then
385:59 - we have the EC two instance and the RDS database
within its own security group. So the idea
386:00 - is that that EC two instance is allowed to
talk to that RDS database, and that EC two
386:01 - instance is not exposing the RDS database
to it well, wouldn't because it's in a private
386:02 - subnets. It doesn't have a public IP address.
But the point is, is that this is to instance,
386:03 - now is able to get traffic from the internet.
And it's also able to accept someone from
386:04 - like for any SSH access, okay. And so the
big takeaway here is that you can see that
386:05 - an instance can belong to multiple security
groups and rules are permissive. So when we
386:06 - have two security groups, and this one has
allows, and this is going to take precedence
386:07 - over su stack, which doesn't have anything,
you know, because it's denied by default,
386:08 - everything, but anything that allows is going
to override that. Okay, so you can nest multiple
386:09 - security groups onto one ACTA Ensign. So just
keep that stuff in mind. There are a few security
386:10 - group limits I want you to know about. And
so we'll look at the first you can have up
386:11 - to 10,000 security groups in a single region,
and it's defaulted to 2500. If you want to
386:12 - go beyond that 2500, you need to make a service
limit increase request to a VA support, you
386:13 - can have 60 inbound rules and 60 outbound
rules per security group. And you can have
386:14 - 16 security groups per EMI. And that's defaulted
to five. Now, if you think about like, how
386:15 - many security rules can you have on an instance?
Well, it's depending on how many you guys
386:16 - are actually attached to that security group.
So if you have to realize that it's attached
386:17 - to a security group, then by default, you'll
have 10. Or if you have the upper limit here,
386:18 - 16, you'll be able to have 32 security groups
on a single instance. Okay, so those are the
386:19 - limits, you know, I thought were worth telling.
So we're going to take a look at our security
386:20 - groups cheat sheet. So we're ready for exam
time. So security groups act as a firewall
386:21 - at the instance level, unless allowed specifically,
all inbound traffic is blocked by default,
386:22 - all outbound traffic from the instance is
allowed by default, you can specify for the
386:23 - source to be either an IP range, a single
IP address or another security group. security
386:24 - groups are stateful. If traffic is allowed
inbound, it is also allowed outbound. Okay,
386:25 - so that's what stateful means. Any changes
to your security group will take effect immediately.
386:26 - ec two instances can belong to multiple security
groups, security groups can contain multiple
386:27 - ECU instances, you cannot block a specific
IP address security groups. For this, you
386:28 - need to use knackles. Right? So again, it's
allowed by default, sorry, everything's denying
386:29 - you're only allowing things okay. You can
have up to 10,000 security groups per region
386:30 - default is 2500. You can have 60 inbound and
60 outbound rules per security group. And
386:31 - you can have 16 security groups associated
to that you and I default is five and I can
386:32 - see that I added an extra zero there so don't
worry when you print out your security scripts
386:33 - to cheat it will be all correct. Okay. Hey,
this is Angie brown from exam Pro, and we
386:34 - are looking at network access control lists
also known as knackles. It is an optional
386:35 - layer of security that acts as a firewall
for controlling traffic in and out of subnets.
386:36 - So knackles act as a virtual firewall at the
subnet level and when you create a VPC, you
386:37 - automatically get a knakal by default, just
like security groups knackles have both inbound
386:38 - and outbound rules. The difference here is
that you're going to have the ability to allow
386:39 - or deny traffic in either way. Okay, so for
security groups, you can only allow whereas
386:40 - knackles, you have deny. Now, when you create
these rules here, it's pretty much the same
386:41 - as security groups with the exception that
we have this thing called rule number And
386:42 - rule number is going to determine the order
of evaluation for these rules, and the way
386:43 - it evaluates is going to be from the lowest
to the highest, the highest rule number, it
386:44 - could be 32,766. And at best recommends that
when you come up with these rule numbers,
386:45 - you use increments of 10 or 100. So you have
some flexibility to create rules in between
386:46 - if you need B, again, subnets are at the subnet
level. So in order for them to apply, you
386:47 - need to associate subnets to knackles. And
subnets can only belong to a single knakal.
386:48 - Okay, so yeah, where you have security groups,
you can have a instances that belong to multiple
386:49 - ones for knackles. It's just a singular case,
okay. All right. So we're just gonna look
386:50 - at a use case for knackles. Here, it's going
to be really around this deny ability. So
386:51 - let's say there is a malicious actor trying
to gain access to our instances, and we know
386:52 - the IP address, well, we can add that as a
rule to our knakal and deny that IP address.
386:53 - And let's say we know that we never need to
SSH into these instances. And we just want
386:54 - an additional guarantee in case someone misconfigured
a security group that SSH access is denied.
386:55 - So we'll just deny on port 22. And now we
have those two cases covered. So there you
386:56 - go. Alright, so we're on to the knakal cheat
sheet. So network access control list is commonly
386:57 - known as nachal. v PCs are automatically given
a default knakal, which allows all outbound
386:58 - inbound traffic each subnet within a VPC must
be associated with a knakal. subnets can only
386:59 - be associated with one knakal. at a time,
associate a sub net with a new knakal with
387:00 - will remove the previous Association. If a
knakal is not explicitly associated with a
387:01 - subnet, the subnet will automatically be associated
with the default knakal knackles have both
387:02 - inbound and outbound rules just like a security
group. The rules can either be allow or deny
387:03 - traffic, unlike a security group, which can
only you can only apply allow rules knackles
387:04 - are stateless. This is This means that knackles
do not care about the rules that you set.
387:05 - So if you want to deny outbound, that doesn't
necessarily mean it's gonna deny inbound automatically,
387:06 - you have to set it individually for both sides.
And that's why it's considered stateless.
387:07 - When you create a knakal it will deny all
traffic by default knackles contain a numbered
387:08 - list of rules that get evaluated in order
from lowest to highest. If you needed to block
387:09 - a single IP address, you could you could be
a knakal. Whereas security security you cannot
387:10 - because it only has deny rules and it'd be
very difficult to block a single IP with a
387:11 - security group. So there you go. That's your
knakal teaching. Hey, this is Angie brown
387:12 - from exam pro and we are starting to VPC follow
along and this is a very long section because
387:13 - we need to learn about all the kind of networking
components that we can create. So we're going
387:14 - to learn how to create our own VPC, subnets,
route tables, internet gateways, security
387:15 - groups, Nat gateways knackles, we're going
to touch it all. Okay, so it's very core to
387:16 - learning about AWS, and it's just a great
to get it out of the way. So let's jump into
387:17 - it. So let's start off by creating our own
VPC. So on the left hand side, I want you
387:18 - to click on your VPC. And right away, you're
going to see that we already have a default
387:19 - VPC within this region of North Virginia.
Okay, your region might be different from
387:20 - mine. It doesn't actually does kind of matter
what region you use, because different regions
387:21 - have different amounts of available azs. So
I'm going to really strongly suggest that
387:22 - you switch to North Virginia to make this
section a little bit smoother for you. But
387:23 - just notice that the default VPC uses an ipv4
cider, cider block range of 172 31 0.0 forward
387:24 - slash 16. Okay, and so if I was to change
regions, no matter what region will go to
387:25 - us, West, Oregon, we're going to find that
we already have a default VPC on here as well
387:26 - and it's going to have the same a cider block
range, okay. So just be aware that at best
387:27 - does give you a default VPC so that you can
start launching resources immediately without
387:28 - having to worry about all this networking,
and there's no full power with using the default
387:29 - VPC it's totally acceptable to do so. But
we definitely need to know how to do this
387:30 - ourselves. So we're going to create our own
VPC. Okay, and so, I'm a big fan of Star Trek,
387:31 - and so I'm going to name it after the planet
of Bayshore. Which is a very well known planet
387:32 - in the Star Trek universe. And I'm going to
have to provide my own cider block, it cannot
387:33 - be one that already exists. So I can't use
that 172 range that ETS was using. So I'm
387:34 - gonna do 10.0 dot 0.0, forward slash 16. And
there is a bit of a rhyme and rhythm to choosing
387:35 - these, this one is a very commonly chosen
one. And so I mean, you might be looking this
387:36 - going, Okay, well, what is this whole thing
with the IP address slash afford 16. And we
387:37 - will definitely explain that in a separate
video here. But just to give you a quick rundown,
387:38 - you are choosing your IP address that you
want to have here. And this is the actual
387:39 - range. And this is saying how many IP addresses
you want to allocate. Okay. So yeah, we'll
387:40 - cover that more later on. And so now we have
the option to set ipv6 cider, or a cider block
387:41 - here. And so just to keep it simple, I'm going
to turn it off. But you know, obviously, ipv6
387:42 - is supported on AWS. And it is the future
of, you know, our IP protocol. So it's definitely
387:43 - something you might want to turn on. Okay,
and just be prepared for the future there
387:44 - that we have this Tennessee option, and this
is going to give us a dedicated hosts. For
387:45 - our VPC, this is an expensive, expensive option.
So we're going to leave it to default and
387:46 - go proceed and create our VPC. And so there
it has been created. And it was very fast,
387:47 - it was just instantaneous there. So we're
going to click through to that link there.
387:48 - And now we can see we have our VPC named Bayshore.
And I want you to notice that we have our
387:49 - IP V for cider range, there is no ipv6 set.
And by default, it's going to give us a route
387:50 - table and a knakal. Okay, and so we are going
to overwrite the row table because we're going
387:51 - to want to learn how to do that by ourselves.
knackles is not so important. So we might
387:52 - just gloss over that. But um, yeah, so there
you are. Now, there's just one more thing
387:53 - we have to do. Because if you look down below
here, we don't have DNS resolution, or DNS,
387:54 - or sorry, DNS hostnames is disabled by default.
And so if we launch an EC two instance, it's
387:55 - not going to get a, a DNS, DNS hostname, that's
just like a URL. So you can access that ecsu
387:56 - instance, we definitely want to turn that
on. So I'm going to drop this down to actions
387:57 - and we're going to set a host names here to
enabled okay. And so now we will get that
387:58 - and that will not cause us pain later down
the road. So now that we've created our VPC,
387:59 - we want to actually make sure the internet
can reach it. And so we're going to next learn
388:00 - about internet gateways. So we have our VPC,
but it has no way to reach the internet. And
388:01 - so we're going to need an internet gateway.
Okay, so on the left hand side, I want you
388:02 - to go to internet gateway. And we are going
to go ahead and create a new one, okay. And
388:03 - I'm just going to call it for internet gateway,
vaes yours, and people do it, who do it who
388:04 - doesn't hurt. And so our internet gateway
has been created. And so we'll just click
388:05 - through to that one. And so you're gonna see
that it's in a detached state. So internet
388:06 - gateways can only be attached to a very specific
VP v PC, it's a one to one relationship. So
388:07 - for every v PC, you're going to have an internet
gateway. And so you can see it's attach and
388:08 - there is no VPC ID. So I'm going to drop this
down and attach the VPC and then select Bayshore
388:09 - there and attach it. And there you go. Now
it's attached, and we can see the ID is associated.
388:10 - So we have an internet gateway. But that still
doesn't mean that things within our network
388:11 - can reach the internet, because we have to
add a route to our route table. Okay, so just
388:12 - closing this tab here, you can see that there
already is a route table associated with our
388:13 - VPC because it did create us a default route
table. So I'm just going to click through
388:14 - to that one here to show you, okay, and you
can see that it's our main route table, because
388:15 - it's set to main, but I want you to learn
how to create route tables. So we're gonna
388:16 - make one from scratch here. Okay. So we'll
just hit Create route table here. And we're
388:17 - just going to name it our main route table
or RG, our internet road table, I don't know
388:18 - doesn't matter. Okay, we will just say RT,
to shorten that there and we will drop down
388:19 - and choose Bayshore. And then we will go ahead
and create that route table. Okay, and so
388:20 - we'll just hit close. And we will click off
here so we can see all of our route tables.
388:21 - And so here we have our, our, our main one
here for Bayshore, and then this is the one
388:22 - we created. Okay, so if we click into this
route table here, you can see by default,
388:23 - it has the full scope of our local network
here. And so I want to show you how to change
388:24 - this one to our main. So we're just going
to click on this one here and switch it over
388:25 - to main, so set as main route table. So the
main road table is whenever you know, just
388:26 - what is going to be used by default. All right,
and so we'll just go ahead and delete the
388:27 - default one here now because we know why.
We need it. Alright, and we will go select
388:28 - our new one here and edit our routes. And
we're going to add one for the internet gateway
388:29 - here. So I'm going to just drop down here
or sorry, I'm just going to write 0.0 dot
388:30 - 0.0, forward slash, zero, which means let's
take, take anything from anywhere there. And
388:31 - then we're going to drop down select internet
gateway, select Bayshore, and hit save routes.
388:32 - Okay, and we'll hit close. And so now we,
we have a internet gateway. And we have a
388:33 - way for our subnets to reach the internet.
So there you go. So now that we have a route
388:34 - to the internet, it's time to create some
subnets. So we have some way of actually launching
388:35 - our EC two instances, somewhere. Okay, so
on the left hand side, I want you to go to
388:36 - subnets. And right away, you're going to start
to see some subnets. Here, these are the default
388:37 - ones created with you with your default VPC.
And you can see that there's exactly six of
388:38 - them. So there's exactly one for every availability
zone within each region. So the North Virginia
388:39 - has six azs. So you're going to have six,
public subnets. Okay, the reason we know these
388:40 - are public subnets. If we were to click on
one here and check the auto assign, it is
388:41 - set to Yes. So if a if this is set to Yes,
that means any easy to instance, launch in
388:42 - the subnet is going to get a public IP address.
Hence, it's going to be considered a public
388:43 - subnet. Okay. So if we were to switch over
to Canada Central, because I just want to
388:44 - make a point here, that if you are in a another
region, it's going to have a different amount
388:45 - of availability zones, Canada only has two,
which is a bit sad, we would love to have
388:46 - a third one there, you're going to see that
we have exactly one subnet for every availability
388:47 - zone. So we're going to switch back to North
Virginia here. And we are going to proceed
388:48 - to create our own subnets. So we're going
to want to create at least three subnets if
388:49 - we can. So because the reason why is a lot
of companies, especially enterprise companies
388:50 - have to run it in at least three availability
zones for high availability. Because if you
388:51 - know one goes out and you only have another
one, but what happens if two goes out. So
388:52 - there's that rule of you know, always have
at least, you know, two additional Okay, so
388:53 - we're going to create three public subnets
in one pool, one private subnet, we're not
388:54 - going to create three private subnets, just
because I don't want to be making subnets
388:55 - here all day. But we'll just get to it here.
So we're going to create our first subnet,
388:56 - I'm going to name this Bayshore public, okay,
all right, and we're going to select our VPC.
388:57 - And we're going to just choose the US East
one a, and we're going to give it a cider
388:58 - block of 10.0 dot 0.0 forward slash 24. Now,
notice, this cider range is a smaller than
388:59 - the one up here, I know the number is larger,
but from the perspective of how many IP addresses
389:00 - that allocates, there's actually a few here.
So you are taking a slice of the pie from
389:01 - the larger range here. So just be aware, you
can set this as 16, it's always going to be
389:02 - less, less than in by less, I mean, a higher
number than 16. Okay, so we'll go ahead and
389:03 - create our first public subnet here. And we'll
just hit close. And this is not by default
389:04 - public, because by default, the auto assign
is going to be set to No. So we're just going
389:05 - to go up here and modify this and set it so
that it does auto assign ipv4 and now is is
389:06 - considered a public subnet. So we're going
to go ahead and do that for our B and C here.
389:07 - So it's going to be the same thing Bayshore
public, be. Okay, choose that. We'll do B,
389:08 - we'll do 10.0 dot 1.0 24. Okay. And we're
gonna go create that close. And we're going
389:09 - to that auto assign that there. All right.
And the next thing we're going to do is create
389:10 - our next sub down here so Bayshore, how boring
I Bayshore public. See, and we will do that
389:11 - and we'll go to see here and it's going to
be 10.0 dot 2.0. forward slash 24. Okay, we'll
389:12 - create that one. Okay, let close and we will
make sure did I set that one? Yes, I did.
389:13 - Did I set that one not as of yet. And so we
will modify that there. Okay. And we will
389:14 - create a another subnet here and this is going
to be a beige, your private a, okay. And we
389:15 - are going to set that to eight here. And we're
going to set this to 10.0 dot 3.0 24. Okay,
389:16 - so this is going to be our private subnet.
All right. So we've created all of our subnets.
389:17 - So the next thing we need to do is associate
them with a route table. Actually, we don't
389:18 - have to because by default, it's going to
use the main Alright, so they're already automatically
389:19 - associated there. But for our private one,
we're not going to be wanting to really use
389:20 - the the the the main route table there, we
probably would want to create our own route
389:21 - table for our privates. That's there. So I'm
just gonna create a new one here and we can
389:22 - just call it private RT. Okay, I'm going to
drop that down, choose Bayshore here. And
389:23 - we're going to hit close, okay? And the idea
is that the, you know, we don't need this
389:24 - subnet to reach the internet, so it doesn't
really make sense to be there. And then we
389:25 - could set other things later on. Okay, so
what I want you to do is just change the association
389:26 - here. So we're gonna just edit the route table
Association. And we're just going to change
389:27 - that to be our private one. Okay. And so now
our route tables are set up. So we will move
389:28 - on to the next step. So our subnets are ready.
And now we are able to launch some EC two
389:29 - instances. So we can play around and learn
some of these other networking components.
389:30 - So what I want you to do is go to the top
here and type in EC two. And we're going to
389:31 - go to the EC two console. And we're going
to go to instances on the left hand side.
389:32 - And we're going to launch ourselves a couple
of instances. So we're going to launch our
389:33 - first instance, which is going to be for our
public subnet here. So we're going to choose
389:34 - t to micro, we're going to go next, and we
are going to choose the Bayshore VPC that
389:35 - we created. We're going to launch this in
the public subnet here public a, okay, and
389:36 - we're going to need a new Im role. So I'm
just going to right click here and create
389:37 - a new Im role, because we're going to want
to give it access to both SSM for sessions
389:38 - manager and also, so we have access to s3.
Okay, so just choosing EC two there. I'm going
389:39 - to type in SSM. Okay, SSM, there it is at
the top, then we'll type in s3, we're gonna
389:40 - give it full access, we're going to go next,
we're going to go to next and we're going
389:41 - to just type in my beige or EC two. Okay.
And we're going to hit Create role. Okay,
389:42 - so now we have the role that we need for our
EC two instance, we're just going to refresh
389:43 - that here, and then drop down and choose my
beige or EC two. Okay, and we are going to
389:44 - want to provide it a script here to run. So
I already have a script pre prepared that
389:45 - I will provide to you. And this is the public
user data.sh. All this is going to do. And
389:46 - if you want to just take a peek here at what
it does, I guess they don't have it already
389:47 - open here. But we will just quickly open this
up here. all it's going to do is it's gonna
389:48 - install an Apache server. And we're just going
to have a static website page here served
389:49 - up, okay, and so we're going to go ahead and
go to storage, nothing needs to be changed
389:50 - here, we're going to add, we don't need to
add any tags, we're gonna go to security group,
389:51 - and we're going to create a new security group,
I'm going to call it m, my, my base, your
389:52 - EC two SD, okay. And we're going to make sure
that we have access to HTTP, because this
389:53 - is a website, we're going to have to have
Port 80, open, we're going to restrict it
389:54 - down to just us. And we could also do that
for SSH. So we might as well do that there
389:55 - as well. Okay, we're gonna go ahead and review
and launch this EC two instance, and already
389:56 - have a key pair that is created, you'll just
have to go ahead and create one if you don't
389:57 - have one there. And we'll just go ahead and
launch that instance there. Okay, great. So
389:58 - now, we have this EC two instance here, which
is going to be for our public segment. Okay.
389:59 - And we will go ahead and launch another instance.
So we'll go to Amazon Lex to here, choose
390:00 - t to micro. And then this time, we're going
to choose our private subnet. Okay, I do want
390:01 - to point out that when you have this auto
assign here, see how it's by by default disabled,
390:02 - because it's inheriting whatever the parents
love that has, whereas when we set it, the
390:03 - first one, you might have not noticed, but
it was set to enable, okay. And we are going
390:04 - to also give it the same role there, my beige
or EC two. And then this time around, we're
390:05 - going to give it the other scripts here. So
I have a private script here, I'm just going
390:06 - to open it up and show it to you. Okay, and
so what this script does, is a while it doesn't
390:07 - actually need to install Apache, so we'll
just remove that, I guess it's just old. But
390:08 - anyway, what it's going to do is it's going
to reset the password on the EC to user to
390:09 - chi win. Okay, that's a character from Star
Trek Deep Space Nine. And we're also going
390:10 - to enable password authentication. So we can
SSH into this using a password. And so that's
390:11 - all the script does here. Okay, and so we
are going to go ahead and choose that file
390:12 - there and choose that and we will move on
to storage storage is totally fine. We're
390:13 - not going to add tags, secure groups, we're
gonna actually create a new security group
390:14 - here. It's not necessarily unnecessary, but
I'm going to do it anyway. So I'm gonna save
390:15 - my private, private EC to SD, maybe put Bayshore
in there. So we just keep these all grouped
390:16 - together note, therefore, it's only going
to need SSH, we're not going to have any access
390:17 - to the internet there. So like, there's no
website or anything running on here. And so
390:18 - we'll go ahead and review and launch. And
then we're going to go watch that instance
390:19 - and choose our key pair. Okay, great. So now
we're just going to wait for these two instances
390:20 - to spin up here. And then we will play around
with security groups and knackles. So I just
390:21 - had a quick coconut water, and now I'm back
here and our instances are running, they don't
390:22 - usually take that long to get started here.
And so we probably should have named these
390:23 - to make it a little bit easier. So we need
to determine which is our public and private.
390:24 - And you can see right away, this one has a
public public DNS hostname, and also it has
390:25 - its ip ip address. Okay, so this is how we
know this is the public one, so I'm just gonna
390:26 - say, Bayshore public. Okay. And this one here
is definitely the private one. All right,
390:27 - so we will say a beige, your private. Okay.
So, um, yeah, just to iterate over here, if
390:28 - we were to look, here, you can see we have
the DNS and the public IP address. And then
390:29 - for the private, there's nothing set. Okay.
So let's go see if our website is working
390:30 - here. So I'm just going to copy the public
IP address, or we can take the DNS one, it
390:31 - doesn't matter. And we will paste this in
a new tab. And here we have our working website.
390:32 - So our public IP address is definitely working.
Now, if we were to check our private one,
390:33 - there is nothing there. So there's nothing
for us to copy, we can even copy this private
390:34 - one and paste it in here. So there's no way
of accessing that website is that is running
390:35 - on the private one there. And it doesn't really
make a whole lot of sense to run your, your
390:36 - website, in the private subnet there. So you
know, just to make a very clear example of
390:37 - that, now that we have these two instances,
I guess, it's a good opportunity to learn
390:38 - about security groups. Okay, so we had created
a security group. And the reason why we were
390:39 - able to access this instance, publicly was
that in our security group, we had an inbound
390:40 - rule on port 80. So Port 80, is what websites
run on. And when we're accessing through the
390:41 - web browser, there's and we are allowing my
IP here. So that's why I was allowed to access
390:42 - it. So I just want to illustrate to you what
happens if I change my IP. So at the top here,
390:43 - I have a VPN, it's a, it's a service, you
can you can buy a lot people use it so that
390:44 - they can watch Netflix in other regions. I
use it for this purpose not to watch Netflix
390:45 - somewhere else. So don't get that in your
mind there. But I'm just going to turn it
390:46 - on. And I'm going to change my IP. So I get
I think that this is Brazil. And so I'm going
390:47 - to have an IP from Brazil here shortly once
it connects. And so now if I were to go and
390:48 - access this here, it shouldn't work. Okay,
so I'm just going to close that tab here.
390:49 - And it should just hang. Okay, so it's hanging
because I'm not using that IP. So that's how
390:50 - security groups work. Okay, and so I'm just
going to turn that off. And I think I should
390:51 - have the same one. And it should resolve instantly
there. So great. So just showing you how the
390:52 - security groups work for inbound rules, okay,
for outbound rules, that's traffic going out
390:53 - to the internet, it's almost always open like
this 0.0 dot 0.0, right, because you'd want
390:54 - to be able to download stuff, etc. So that
is pretty normal business. Okay. So now that
390:55 - now that we can see that, maybe we would like
to show off how knackles work compared to
390:56 - security groups to security groups. As you
can see, if we were just to open this one
390:57 - up here, okay. security groups, by default,
only can allow things so everything is denied.
390:58 - And then you're always opening things up.
So you're adding allow rules only you can't
390:59 - add an explicit deny rule. So we're knackles
are a very useful is that you can use it to
391:00 - block a very specific IP addresses, okay,
or IP ranges, if you will. And you cannot
391:01 - do that for a security group. Because how
would you go about doing that? So if I wanted
391:02 - to block access just to my IP address, I guess
I could only allow every other IP address
391:03 - in the world except for mine. But you can
see how that would do undue burden. So let's
391:04 - see if we can set our knakal to just block
our IP address here. Okay. So security groups
391:05 - are associated with the actual EC two instances
or so the question is, is that how do we figure
391:06 - out the knackles and knackles are associated
with the subnets. Okay, so in order to block
391:07 - our IP address for this easy to instance,
we have to determine what subnet it runs in
391:08 - and so it runs in our Bayshore public Right.
And so now we got to find the knakal. that's
391:09 - associated with it. So going up here to subnets,
I'm going to go to public a, and I'm going
391:10 - to see what knackles are associated with it.
And so it is this knakal here, and we have
391:11 - some rules that we can change. So let's actually
try just blocking my IP address here. And
391:12 - we will go just grab it from here. Okay. All
right. And just to note, if you look here,
391:13 - see has this Ford slash 32. That is mean,
that's a cider block range of exactly one
391:14 - IP address. That's how you specify a single
IP address with forward slash 32. But I'm
391:15 - going to go here and just edit the knakal
here, and we are going to this is not the
391:16 - best way to do it. So I'm just going to open
it here. Okay. And because it didn't get some
391:17 - edit options there, I don't know why. And
so we'll just go up to inbound rules here,
391:18 - I'm going to add a new rule. And it goes from
lowest to highest for these rules. So I'm
391:19 - just going to add a new rule here. And I'm
going to put in rule 10. Okay, and I'm going
391:20 - to block it here on the cider range. And I'm
going to do it for Port 80. Okay, so this
391:21 - and we're going to have an explicit deny,
okay, so this should, this should not allow
391:22 - me to access that EC two instance any any
longer. Okay, so we're going to go back to
391:23 - our instances here, we're going to grab that
IP address there and paste it in there and
391:24 - see if I still have access, and I do not okay,
so that knakal is now blocking it. So that's
391:25 - how you block individual IP addresses there.
And I'm just going to go back and now edit
391:26 - the rule here. And so we're just going to
remove this rule, and hit save. And then we're
391:27 - going to go back here and hit refresh. Okay.
And I should now have access on I do. So there
391:28 - you go. So that is security groups and knakal.
So I guess the next thing we can move on to
391:29 - is how do we actually get access to the private
subnet, okay, and the the the ways around
391:30 - that we have our our private EC two instance.
And we don't have an IP address, so there's
391:31 - no direct way to gain access to it. So we
can't just easily SSH into it. So this is
391:32 - where we're going to need a bastion. Okay,
and so we're gonna go ahead and go set one
391:33 - up here. So what I want you to do is I want
you to launch a new instance, here, I'm just
391:34 - going to open a new tab, just in case I want
this old tab here. And I'm just going to hit
391:35 - on launch instance here. Okay, and so I'm
going to go to the marketplace here, I'm gonna
391:36 - just type in Bastion. And so we have some
options here, there is this free one Bastion
391:37 - host, SSH, but I'm going to be using guacamole
and there is an associated cost here with
391:38 - it, they do have a trial version, so you can
get away without paying anything for it. So
391:39 - I'm just going to proceed and select guacamole.
And anytime you're using something from the
391:40 - marketplace, they generally will have the
instructions in here. So if you do view additional
391:41 - details here, we're going to get some extra
information. And then we will just scroll
391:42 - down here to usage information such as usage
instructions. And we're going to see there
391:43 - is more information, I'm just going to open
up this tab here because I've done this a
391:44 - few times. So I remember where all this stuff
is. Okay, and we're just going to hit continue
391:45 - here. Okay, and we're going to start setting
up this instance. So we're going to need a
391:46 - small, so this one doesn't allow you to go
into macros, okay, so there is an associated
391:47 - cost there, we're going to configure this
instance, we're going to want it in the same
391:48 - VPC as our private, okay, when we have to
launch this in a public subnet, so just make
391:49 - sure that you select the public one here,
okay. And we're going to need to create a
391:50 - new Im role. And this is part of guacamole
these instructions here because you need to
391:51 - give it some access so that it can auto discover
instances, okay, and so down here, they have
391:52 - instructions here and they're just going to
tell you to make an IM role, we could launch
391:53 - a cloudformation template to make this but
I would rather just make it by hand here.
391:54 - So we're going to grab this policy here, okay.
And we are going to make a new tab and make
391:55 - our way over to I am okay. And once we're
in I am here, we're gonna have to make this
391:56 - policy so I'm going to make this policy. Okay,
unless I already have it, see if it's already
391:57 - in here, new. Okay, good. And I'm gonna go
to JSON, paste that in there, review the policy,
391:58 - I'm going to name it they have a suggestion
here what to name it, Glock AWS, that seems
391:59 - fine to me. Okay, and here you can see it's
gonna give us permissions to cloud watch an
392:00 - sts so we'll go ahead and create that policy.
It says it already exists. So I'm I already
392:01 - have it. So just go ahead and create that
policy. And I'm just going to skip the step
392:02 - for myself. Okay, and we're just going to
cancel there. So I'm just going to type walk.
392:03 - I don't know why it's not showing up. says
it already exists. Type that in again. So
392:04 - yeah, there it is. So I already have that
policy. Okay, so I couldn't hit that last
392:05 - step. But you'll Be able to get through that
no problem. And then once you have it, you're
392:06 - gonna have to create a new role. So we're
going to create a role here and it's going
392:07 - to be for EC two, we're going to go next.
And we're going to want I believe EC to full
392:08 - access is that the right Oh, read only access,
okay. So we're going to want to give this
392:09 - easy to read only access. And we're also going
to want to give it that new GWAC role. So
392:10 - I'm going to type in type AWS here. Oh, let's
give me a hard time here. And we'll just copy
392:11 - and paste the whole name in here. There it
is. And so those are the two, two policies
392:12 - you need to have attached. And then we're
just going to name this something here. So
392:13 - I'm gonna just call it my GWAC. Bastion, okay,
roll here. I'm going to create that role.
392:14 - Okay, and so that role has now been created,
we're going to go back here, refresh the IM
392:15 - roles, and we're going to see if it exists.
And there it is my GWAC Bastion role. And
392:16 - I spell bash and wrong there. But I don't
think that really matters. And then we will
392:17 - go to storage. There's nothing to do here,
we'll skip tags, we'll go to security groups.
392:18 - And here you can see it comes with some default
configurations. So we're going to leave those
392:19 - alone. And then we're going to launch this
EC two instance. Okay. So now we're launching
392:20 - that it's taking a bit of time here, but this
is going to launch. And as soon as this is
392:21 - done, we're going to come back here and actually
start using this Bastion to get into our private
392:22 - instance. So our bashing here is now already
in provisioned. So let's go ahead and just
392:23 - type in Bastion, so we don't lose that later
on, we can go grab either the DNS or public
392:24 - IP, I'll just grab the DNS one here. And we're
going to get this connection, not private
392:25 - warning, that's fine, because we're definitely
not using SSL here. So just hit advanced,
392:26 - and then just click to proceed here. Okay,
and then it's might ask you to allow, we're
392:27 - going to definitely say allow for that, because
that's more of the advanced functionality,
392:28 - guacamole there, which we might touch in.
At the end of this here, we're going to need
392:29 - the username and password. So it has a default,
so we have block admin here, okay. And then
392:30 - the password is going to be the name of the
instance ID. All right, and this is all in
392:31 - the instructions here. I'm just speaking you
through it. And then we're going to hit login
392:32 - here. And so now it has auto discovered the
instances which are in the VPC that is launched.
392:33 - And so here, we have Bayshore, private. So
let's go ahead and try to connect to it. Okay.
392:34 - So as soon as I click, it's going to make
this shell here. And so we'll go attempt and
392:35 - login now. So our user is easy to user. And
I believe our password is KI wi n Chi win.
392:36 - And we are in our instance, so there you go.
That's how we gain access to our private instance
392:37 - here. Just before we start doing some other
things within this private easy too, I just
392:38 - want to touch on some of the functionality
of Bastion here, or sorry, guacamole, and
392:39 - so why you might actually want to use the
bastion. So it does, it is a hardened instance,
392:40 - it does allow you to authenticate via multiple
methods. So you can enable multi factor authentication
392:41 - to use this. It also has the ability to do
screen recordings, so you can really be sure
392:42 - what people are up to, okay, and then it just
has built in audit logs, and etc, etc. So
392:43 - there's definitely some good reasons to use
a bastion. But we can also use a sessions
392:44 - manager, which does a lot of this for us with
the exception of screen recording within the,
392:45 - within AWS. But anyway, so now that we're
in our instance, let's go play around here
392:46 - and see what we can do. So now that we are
in this private EC two instance, I just want
392:47 - to show you that it doesn't have any internet
access. So if I was to ping something like
392:48 - Google, right, okay, and I'm trying to get
information here to see how it's hanging,
392:49 - and we're not getting a ping back. That's
because there is no route to the internet.
392:50 - And so the way we're going to get a route
to the internet is by creating a NAT instance
392:51 - or a NAT gateway. Generally, you want to use
a NAT gateway, there are cases of use Nat
392:52 - instances. So if you were trying to save money,
you can definitely save money by having to
392:53 - manage a NAT instance by herself. But we're
gonna learn how to do Nat gateway because
392:54 - that's the way to this wants you to go. And
so back in our console, here we are in EC
392:55 - two instances where we're going to have to
switch over to a VPC. Okay, because that's
392:56 - where the NAT gateway is. So on the left hand
side, we can scroll down and we are looking
392:57 - under VPC, we have Nat gateways. And so we're
going to launch ourselves a NAT gateway, that
392:58 - gateways do cost money. So they're not terribly
expensive, but you know, we, at the end of
392:59 - this will tear it down. Okay. And so, the
idea is that we need to launch this Nat gateway
393:00 - in a public VPC or, sorry, public subnet,
and so we're gonna have to look here, I'm
393:01 - gonna watch it in the Bayshore, public eight,
doesn't matter which one just has to be one
393:02 - of the Public ones. And we can also create
an elastic IP here. I don't know if it actually
393:03 - is required assigned a pipe network, I don't
know if it really matters. But we'll try to
393:04 - go ahead and create this here without any
IP. No, it's required. So we'll just hit Create
393:05 - elastic IP there. And that's just a static
IP address. So it's never changing. Okay,
393:06 - and so now that we have that is associated
with our Nat gateway, we'll go ahead and create
393:07 - that. And it looks like it's been created.
So once your Nat gateway is created, the next
393:08 - thing we have to do is edit your route table.
So there actually is a way for that VPC to
393:09 - our sorry, that private instance to access
the internet. Okay, so let's go ahead and
393:10 - edit that road table. And so we created a
private road table specifically for our private
393:11 - EC two. And so here, we're going to edit the
routes, okay. And we're going to add a route
393:12 - for that private or to that Nat gateway. Okay.
So um, we're just going to type in 0.0 dot
393:13 - 0.0, forward slash zero. And we are then just
going to go ahead, yep. And then we're going
393:14 - to go ahead and choose our Nat gateway. And
we're going to select that there, and we're
393:15 - going to save that route. Okay, so now our
Nat gateway is configured. And so there should
393:16 - be a way for our instance to get to the internet.
So let's go back and do a ping. And back over
393:17 - here, in our private EC two instance, we're
just going to go ahead and ping Google here.
393:18 - Okay, and we're going to see if we get some
pings back, and we do so there you go. That's
393:19 - all we had to do to access the internet. All
right. So why would our private EC two instance
393:20 - need to reach the internet? So we have one
one inbound traffic, but we definitely want
393:21 - outbound because we would probably want to
update packages on our EC two instance. So
393:22 - we did a sudo Yum, update. Okay, we wouldn't
be able to do this without a outbound connection.
393:23 - All right. So it's a way of like getting access
to the internet only for the things that we
393:24 - need for outbound connections, okay. All right.
So we had a fun time playing around with our
393:25 - private EC two instance there. And so we're
pretty much wrapped up here for stuff. I mean,
393:26 - there's other things here, but you know, at
the associate level, it's, there's not much
393:27 - reason to get into all these other things
here. But I do want to show you one more thing
393:28 - for VP C's, which are VPC flow logs, okay,
and so I want you to go over to your VPC here,
393:29 - okay, and then I just want you to go up, and
I want you to create a flow log, so flow logs
393:30 - will track all the, the traffic that is going
through through your VPC. Okay, and so it's
393:31 - just nice to know how to create that. So we
can have it to accept reject, or all I'm going
393:32 - to set it to all and it can either be delivered
to cloudwatch logs or s3 Cloud watch is a
393:33 - very good destination for that. In order to
deliver that, we're going to need a destination
393:34 - log group, um, I don't have one. So in order
to send this to a log group, we're going to
393:35 - have to go to cloud watch, okay. We'll just
open this up in a new tab here. Okay. And
393:36 - then, once we're here in cloud watch, we're
going to create ourselves a new cloud watch
393:37 - log, alright. And we're just gonna say actions
create log group, and we'll just call this
393:38 - Bayes your VPC flow logs or VPC logs or flow
logs, okay. And we will hit Create there.
393:39 - And now if we go back to here and hit refresh,
we may have that destination now available
393:40 - to us. There it is. Okay, we might need an
IM role associated with this, to have permissions
393:41 - to publish to cloud watch logs. So we're definitely
going to need permissions for that. Okay.
393:42 - And I'll just pop back here with those credentials
here in two seconds. So I just wanted to collect
393:43 - a little bit of flow log data so I could show
it off to you to see what it looks like. And
393:44 - so you know, under our VPC, we can see that
we have flow logs enabled. So now we're done
393:45 - the VPC section, let's clean up whatever we
created here, so we're not incurring any cost.
393:46 - So we're gonna make our way over to EC two
instances, and you can easily filter out the
393:47 - instances which are in that. That VPC here
by going to VPC ID here, and then just selecting
393:48 - the VPC. So these are the three instances
that are running and I'm just going to terminate
393:49 - them all. Because you know, we don't want
to spend up our free credits or incur cost
393:50 - because of that bash in there. So we'll just
hit terminate there and those things are going
393:51 - to now shut down. We also have that VPC endpoint
still running. Just double check to make sure
393:52 - your Nat gateway isn't still there. So under
the back in the VPC section here, just make
393:53 - sure that you had so and there we have our
gateway endpoint there for s3. So we'll just
393:54 - go ahead and delete that. I don't believe
it cost us any money, but it doesn't hurt
393:55 - to get that out. Later, hey, this is Andrew
Brown from exam Pro. And we are looking at
393:56 - identity access management Iam, which manages
access of AWS users and resources. So now
393:57 - it's time to look at I am core components.
And we have these installed identities. And
393:58 - those are going to be users groups and roles.
Let's go through those. So a user is an end
393:59 - user who can log into the console or interact
with database resources programmatically,
394:00 - then you have groups, and that is when you
take a bunch of users, and you put them into
394:01 - a logical grouping. So they have shared permissions.
That could be administrators, developers,
394:02 - auditors, whatever you want to call that,
then you have roles and roles, have policies
394:03 - associated with them. That's what holds the
permissions. And then you can take a role
394:04 - and assign it to users or groups. And then
down below, you have policies. And this is
394:05 - a JSON document, which defines the rules in
which permissions are allowed. And so those
394:06 - are the core components. But we'll get more
in detail to all these things. Next, so now
394:07 - that we know the core components are and let's
talk about how we can mix and match them.
394:08 - Starting at the top here, we have a bunch
of users in a user group. And if we want to
394:09 - on mass, apply permissions, all we have to
do is create a role with the policies attached
394:10 - to that role. And then once we attach that
role to that group, all these users have that
394:11 - same permission great for administrators,
auditors, or developers. And this is generally
394:12 - the way you want to use Iam when assigning
roles to users, you can also assign a role
394:13 - directly to a user. And then there's also
a way of assigning a policy, which is called
394:14 - inline policy directly to a user. Okay, so
why would you do this, or maybe you have exactly
394:15 - one action you want to attach to this user,
and you want to do it for a temporary amount
394:16 - of time, you don't want to create a manage
role, because it's never, it's never going
394:17 - to be reused for anybody else. There are use
cases for that. But generally, you always
394:18 - want to stick with the top level here, a rule
can have multiple policies attached to it,
394:19 - okay. And also a role can be attached to certain
AWS resources. All right. Now, there are cases
394:20 - where resources actually have inline policies
directly attached to them. But there are cases
394:21 - where you have roles attached to or somehow
associated to resources, all right. But generally,
394:22 - this is the mix and match of it. If you were
taking the ADA security certification, then
394:23 - this stuff in detail really matters. But for
the associate and the pro level, you just
394:24 - need to conceptually know what you can and
cannot do. All right. So in I am you have
394:25 - different types of policies, the first being
managed policies, these are ones that are
394:26 - created by AWS out of convenience for you
for the most common permissions you may need.
394:27 - So over here, we'd have Amazon easy to full
access, you can tell that it's a managed policy,
394:28 - because it says it's managed by AWS, and an
even further indicator is this orange box,
394:29 - okay, then you have custom customer managed
policies, these are policies created by you,
394:30 - the customer, there are edible, whereas in
the managed policies, they are read only.
394:31 - They're marked as customer managed, you don't
have that orange box. And then last are inline
394:32 - policies. So inline policies, you don't manage
them because they're like they're one and
394:33 - done. They're intended to be attached directly
to a user or directly to a, a resource. And
394:34 - they're, and they're not managed, so you can't
apply them to more than one identity or resource.
394:35 - Okay, so those are your three types of policy.
So it's now time to actually look at a policy
394:36 - here. And so we're just going to walk through
all the sections so we can fully understand
394:37 - how these things are created. And the first
thing is the version and the version is the
394:38 - policy language version. If this changes,
then that means all the rules here could change.
394:39 - So this doesn't change very often, you can
see the last time was 2012. So it's gonna
394:40 - be years until they change it. If they did
make changes that probably would be minor,
394:41 - okay, then you have the statement. And so
the statement is just a container for the
394:42 - other policy elements. So you can have a single
one here. So here I have an array, so we have
394:43 - multiples. But if you didn't want to have
multiples, you just get rid of the the square
394:44 - brackets there, you could have a single policy
element there. Now going into the actual policy
394:45 - element, the first thing we have is Sid and
this is optional. It's just a way of labeling
394:46 - your statements. So Cid probably stands for
statement identifier, you know, again, it's
394:47 - optional. Then you have the effect, the effect
can be either allow or deny and that's going
394:48 - to set the conditions or the the access for
the rest of the policy. The next thing is
394:49 - we have the action so actions can be individualize.
Right. So here we have I am, we have an individual
394:50 - one, or we can use asterik to select everything
under s3. And these are the actual actions
394:51 - the policy will allow or deny. And so you
can see we have a deny policy. And we're denying
394:52 - access all to s3 for a very specific user
here, which gets us into the principal. And
394:53 - the principal is kind of a conditional field
as well. And what you can do is you can specify
394:54 - an account a user role or federated user,
to which you would like to allow or deny access.
394:55 - So here, we're really saying, hey, Barkley,
you're not allowed to use s3, okay, then you
394:56 - have the resource, that's the actual thing.
That is we're allowing or denying access to
394:57 - so in this case, it's a very specific s3 bucket.
And the last thing is condition. And so condition
394:58 - is going to vary based on the based on the
resource, but here we have one, and it does
394:59 - something, but I'm just showing you that there
is a condition in here. So there you go, that
395:00 - is the makeup of a policy, if you can master
master these things, it's going to make your
395:01 - life a whole lot easier. But you know, just
learn what you need to learn. So you can also
395:02 - set up password policies for your users. So
you can set like the minimum password length
395:03 - or the rules to what makes up a good password,
you can also rotate out passwords, so that
395:04 - is an option you have as well. So it will
expire after x days. And then a user then
395:05 - must reset that password. So just be aware
that you have the ability to password. Let's
395:06 - take a look at access keys. Because this is
one of the ways you can interact with AWS
395:07 - programmatically either through the ad vcli,
or the SDK. So when you create a user, and
395:08 - you say it's allowed to have programmatic
access, it's going to then create an access
395:09 - key for you, which is an ID and a secret access
key. One thing to note is that users can only
395:10 - have up to two access keys within their accounts
down below, you can see that we have one,
395:11 - as soon as we add a second one, that gray
button for create access key will vanish.
395:12 - And if we want more, we would either have
to we'd have to remove keys, okay. But you
395:13 - know, just be aware that that's what access
keys are. And you can make them inactive and
395:14 - you're only allowed to have let's quickly
talk about MFA. So MFA can be turned on per
395:15 - user. But there is a caveat to it, where the
user has to be the one that turns it on. Because
395:16 - when you turn it on, you have to connect it
to a device and your administrator is not
395:17 - going to have the device. So it's on the user
to do so there is no option for the administrator
395:18 - to go in and say, Hey, you have to use MFA.
So it cannot be enforced directly from an
395:19 - administrator or root account. But what the
minister can do if if if they want is they
395:20 - can restrict access to resources only to people
that are using MFA. So you can't make the
395:21 - user account itself have MFA. But you can
definitely restrict access to API calls and
395:22 - things like that. So temporary security credentials
are just like programmatic access keys, except
395:23 - they aren't temporary. And they're used in
the following scenarios, we were dealing with
395:24 - identity Federation, delegation, cross account
access, and Im roles. And so we're going to
395:25 - look in more detail at identity Federation
and cross account access in the upcoming slides.
395:26 - But let's talk about how they are different
for programmatic access keys. The first thing
395:27 - is that they last for minutes to an hour.
So they're very short term credentials. And
395:28 - they're not stored with the user. But they're
generated dynamically and provided the user
395:29 - when requested. So we provide like access
keys, they're strongly linked to a specific
395:30 - user. But in this case, we just give them
these credentials when they need them. And
395:31 - these temporary credentials are the basis
for roles and identity Federation. So you've
395:32 - actually been using temporary security credentials
this entire time, but you probably just don't
395:33 - know it. So anytime you use a role, an IM
role, they actually generate out an STS, which
395:34 - is the token that is used for temporary credentials,
but you just don't notice it, because eight
395:35 - of us does it for you. But let's dig deep
more into this stuff. So we said that the
395:36 - use case for these temporary security credentials
would be identity Federation. But let's talk
395:37 - about what that actually is. So identity Federation,
is the means of linking a person's electronic
395:38 - identity and attributes stored across multiple
distinct identity management systems. So that
395:39 - definition is a little bit confusing. So my
take on it is that identity Federation allows
395:40 - users to exist on a different platform. So
an example this would be your users or on
395:41 - Facebook, but They can gain access as if they
are a user in AWS, the idea is that their
395:42 - identities are hosted somewhere else, whether
it's Facebook, Google, Active Directory, or
395:43 - whatever. So with Iam, it supports two types
of identity Federation. So we have enterprise
395:44 - identity Federation and web identity Federation.
So the protocols you can use for enterprise
395:45 - identity Federation would be SAML, which is
compatible with Active Directory, a very popular
395:46 - Microsoft Windows identity system, or custom
Federation, brokers. This allows you to connect
395:47 - to other identity systems. And generally with
enterprise entity Federation, you're doing
395:48 - things with single sign on, but for the scope
of the developer associate, we don't need
395:49 - to really know about that we do need to know
about is web identity Federation. And you've
395:50 - used this before, if you've ever clicked a
button where it's like connect with Facebook
395:51 - or connect with LinkedIn, to quickly sign
into a service, you are using a web identity
395:52 - Federation system. So Amazon has won Facebook
has won Google has won, LinkedIn, etc, etc.
395:53 - But generally, the protocols that they adhere
to is the open ID Connect, oh, ICD 2.0. And
395:54 - that's actually built off of OAuth two. So
you might have heard of OAuth. So Oh, ice,
395:55 - oh, IDC is built on top of OAuth. So. But
yeah, so now we have an idea of identity Federation.
395:56 - Let's dig a little bit deeper here. So we've
been talking about these temporary security
395:57 - credentials, but actually, how do we get a
hold of them. And that's where this service
395:58 - comes into play this security token service
STS. So it's a web service that enables you
395:59 - to request temporary, limited privileged credentials
for either Im users, or for federated users.
396:00 - And we just talked about identity Federation.
So users that are outside of AWS. And so this
396:01 - service is a global service. If you were to
go into the AWS console and type in STS, nothing
396:02 - will come up, because you can't access it
through the console, you can only access it
396:03 - programmatically. And there's actually an
endpoint for it. So there's one that's SDS,
396:04 - Amazon, Ada calm. So that's where all the
requests get generated. And an sts returns.
396:05 - So like, once you use the API to get that
STS, it's going to return an access key ID
396:06 - secret access key a sessions token, an expiration,
you're going to notice, the first two should
396:07 - be very noticeable to you. Because it's the
same thing when you do programmatic access
396:08 - with a user. It's the same stuff. And in fact,
you can take this, the access key ID and secret
396:09 - access key and put it in your database credentials
file, and make a profile. Of course, these
396:10 - will be temporary. So it's not great for long
term storage. But the point is, is that they're,
396:11 - they're exactly the same, it's just that they're
temporary. And if you want to actually get
396:12 - this token, you have to use the API actions
for STS, which you could use either the SDK
396:13 - or the COI to use. And so this is a list of
them, the top three are the ones that are
396:14 - most likely in use. So we have assume role
that's used any time I am role gives you an
396:15 - STS, remember, we talked about AWS, every
time you use I am, it gives you an sts B don't
396:16 - see it, it uses a sumrall. Same with cross
account roles, it uses assume role, then you
396:17 - have SAML, that's for the enterprise identity
Federation. And then you have assume role
396:18 - with web identity, which is the one we really
need to know for the developer associate.
396:19 - And that's authenticating to Facebook, Google,
etc, etc. So let's look at that in more detail
396:20 - and see how that works. So now let's look
at actually how to get an sts from using with
396:21 - assume role with web identity. So this action
here is going to return a set of temporary
396:22 - credentials for the usage of an authorized
in a mobile or web application. So you have
396:23 - a developer, and the first thing you're going
to do is you need to authenticate with whatever
396:24 - the web identity is. So we're gonna send an
OAuth call to Facebook, as an example here.
396:25 - And they're gonna send us back a JavaScript
web token, a JW T. And then once we have that,
396:26 - what we can do is use something like the CLR.
You can also use the AWS SDK, but we'll use
396:27 - a COI and we're going to call the assume role
with web identity passing along that JW T.
396:28 - And then the STS service is going to determine
whether it's going to give us a token or not.
396:29 - And so there it passes along the temporary
credential, so we're going to get that information.
396:30 - So now that we have those temporary credentials,
We can use the COI or other means to gain
396:31 - access to a variety of AWS services within
our AWS account, and just whatever we decide
396:32 - that they're allowed to gain access to. So
that is the process for getting sts for at
396:33 - least the assume role with web identity. What's
really important to note is the order that
396:34 - this happens, and because this definitely
shows up on the exam, they'll ask you like,
396:35 - they'll give you an example of the order of
When this happens, and just know that you
396:36 - always authenticate with the web ID first,
and then you get the token second, from STS.
396:37 - So just remember that it's always the web
identity first, and you'll definitely score
396:38 - points on the exam. So I was saying earlier
that cross account roles are another thing
396:39 - that is used with sts, the security token
service. So let's talk about why would we
396:40 - want to make cross account roles. So the whole
purpose of them is to allow you to grant access
396:41 - for a user from another Eva's account to resources
within your own account. And the advantage
396:42 - here is that you don't have to create your
own, like a user for them in your own account.
396:43 - So here's an example where we have account
a so I have an account and that one, and account
396:44 - B wants to grant me access to specific resources,
what they're going to do is create a role
396:45 - for me, which is a cross account role. And
this is going to grant me access. So how do
396:46 - how does this role actually do that? Well,
there's a policy that's attached to it. And
396:47 - to that role, and it's granting me access
to assume role. And so we said that sts the
396:48 - security token service is what issues those
temporary credentials. And we saw there was
396:49 - a bunch of actions, one being web identity,
assume role, the other one being assumed role.
396:50 - So assume role is what is granting us access
across count. And this happens seamlessly,
396:51 - so you don't have to do anything. But yeah,
that is cross account roles. Hey, this is
396:52 - Angie brown from exam Pro, and we are going
to do the I am follow along. So let's make
396:53 - our way over to the IM console. So just go
up to services here and type in Im, and we
396:54 - will get to learning this a right right away.
So here I am on the IM dashboard. And we have
396:55 - a couple things that a device wants us to
do. It wants us to set MFA on our root account.
396:56 - It also wants us to apply an IM password policy,
so that our passwords stay very secure. So
396:57 - let's take what they're saying in consideration
and go through this. Now I am logged in as
396:58 - the root user. So we can go ahead and set
MFA. So what I want you to do is drop this
396:59 - down as your root user and we'll go manage
MFA. And we will get to this here. So this
397:00 - is just a general disclaimer here to help
you get started here. I don't ever want to
397:01 - see this again. So I'm just going to hide
it. And we're going to go to MFA here and
397:02 - we're going to activate MFA. So for MFA, we
have a few options available. We have a virtual
397:03 - MFA, this is what you're probably most likely
going to use where you can use a mobile device
397:04 - or computer, then you can use a you two f
security key. So this is like a UB key. And
397:05 - I actually have an OB key, but we're not going
to use it for this, but it's a physical device,
397:06 - which holds the credentials, okay, so you
can take this key around with you. And then
397:07 - there are other hardware mechanisms. Okay,
so but we're going to stick with virtual MFA
397:08 - here. Okay, so we'll hit Continue. And what
it's going to do is it's going to you need
397:09 - to install a compatible app on your mobile
phone. So if we take a look here, I bet you
397:10 - authenticator is one of them. Okay. So if
you just scroll down here, we have a few different
397:11 - kinds. I'm just looking for the virtual ones.
Yeah. So for Android or iPhone, we have Google
397:12 - Authenticator or authy, two factor authentication.
So you're going to have to go install authenticator
397:13 - on your phone. And then when you are ready
to do so you're going to have to show this
397:14 - QR code. So I'm just going to click that and
show this to you here. And then you need to
397:15 - pull out your phone. I know you can't see
me doing this, but I'm doing it right now.
397:16 - Okay. And I'm not too worried that you're
seeing this because I'm going to change this
397:17 - two factor authentication out here. So if
you decide that you want to also add this
397:18 - to your phone, you're not going to get too
far. Okay, so I'm just trying to get my authenticator
397:19 - app out here. And I'm going to hit plus and
the thing and I can scan the barcode, okay.
397:20 - And so I'm just going to put my camera over
it here. Okay, great. And so is is save the
397:21 - secret. All right, so it's been added to Google
Authenticator. Now, now that I have it in
397:22 - my application, I need to enter in to two
consecutive MFA codes. Okay, so this is a
397:23 - little confusing, it took me a while to figure
this out the first time I was using AWS, the
397:24 - idea is that you need to set the first one.
So the first one I see is 089265. Okay, and
397:25 - so I'm just going to wait for the next one
to expire, okay, so there's a little circle
397:26 - that's going around. And I'm just waiting
for that to complete to put in a second one.
397:27 - It just takes a little bit of time here. Still
going here. Great. So I have new numbers.
397:28 - So the numbers are 369626. Okay, so it's not
the same number, but it's two consecutive
397:29 - numbers, and we will hit assign MFA. And now
MFA has been set on my phone. So now when
397:30 - I go and log in, it's going to ask me to provide
additional code. Okay, and so now my root
397:31 - account is protected. So we're gonna go back
to our dashboard, and we're going to move
397:32 - on to password policies. Okay. So let's take
the recommendation down here and manage our
397:33 - password policy, okay. And we are going to
set a password policy. So password policy
397:34 - allows us to enforce some rules that we want
to have under users. And so to make passwords
397:35 - a lot stronger, so we can say it should require
at least one upper letter, one lowercase letter,
397:36 - or at least one number, a non non alphanumeric
character, enable the password expiration.
397:37 - So after 90 days, they're going to have to
change the password, you can have password
397:38 - expiration requires the administrator reset,
so you can't just reset it, the admin will
397:39 - do it for you allow users to change their
own password is something you could set as
397:40 - well. And then you could say prevent password
reuse. So for the next five passwords, you
397:41 - can't reuse the same one. Okay? So and I would
probably put this a big high numbers, so that
397:42 - a very high chance they won't use the same
one. Okay, so, um, yeah, there we go. We'll
397:43 - just hit Save Changes. And now we have a password
policy in place. Okay. And so that's, that's
397:44 - how that will be. So to make it easier for
users to log into the Iam console, you can
397:45 - provide a customized sign in link here. And
so here, it has the account ID, or I think
397:46 - that's the account ID but we want something
nicer here. So we can change it to whatever
397:47 - you want. So I can call it Deep Space Nine.
Okay. And so now what we have is if I spelt
397:48 - that, right, I think so yeah. So now that
we have a more convenient link that we can
397:49 - use to login with, okay, so I'm just going
to copy that for later, because we're going
397:50 - to use it to login. I mean, obviously, you
can name it, whatever you want. And I believe
397:51 - that these are just like, I'm like picking
like your Yahoo or your your Gmail email,
397:52 - you have to be unique. Okay, so you're not
gonna be at least Deep Space Nine, as long
397:53 - as I have to use I believe. But yeah, okay,
so maybe we'll move on to actually creating
397:54 - a user here. So here I am under the Users
tab, and I am and we already have an existing
397:55 - user that I created for myself, when I first
set up this account, we're going to create
397:56 - a new user so we can learn this process. So
we can fill the name here, Harry Kim, which
397:57 - is the character from Star Trek Voyager, you
can create multiple users in one go here,
397:58 - but I'm just gonna make one. Okay, I'm going
to give him programmatic access and also access
397:59 - to the console, so you can log in. And I'm
gonna have an auto generated password here,
398:00 - so I don't have to worry about it. And you
can see that it will require them to reset
398:01 - their password when they first sign in. So
going on to permissions, we need to usually
398:02 - put our users within a group, we don't have
to, but it's highly recommended. And here
398:03 - I have one called admin for admin, which has
add administrator access, I'm going to create
398:04 - a new group here, and I'm going to call it
developers okay. And I'm going to give them
398:05 - power access, okay, so it's not full access,
but it gives them a quite a bit of control
398:06 - within the system. Okay. And I'm just going
to create that group there. And so now I have
398:07 - a new group. And I'm going to add Harry to
that group there. And we will proceed to the
398:08 - next step here. So we have tags, ignore that
we're going to review, we're going to create
398:09 - Harry Kim the user. Okay. And so what it's
done here is it's also created a secret access
398:10 - key and a password. Okay, so if he wants that
programmatic access, he can use these and
398:11 - we can send the an email with the, with this
information along to him. Okay. And, yeah,
398:12 - we'll just close that there. Okay, and then
we'll just poke around here and Harry Kim
398:13 - for a little bit. So just before we jump into
Harry Kim here, you can see that he has never
398:14 - used his access key. He, the last time his
password was used was today, which was set
398:15 - today, and there is no activity and he does
not have MFA. So if we go into Harry Kim,
398:16 - we can look around here and we can see that
he has policies applied to him from a group
398:17 - and you can also individually attach permissions
to him. So we have the ability to give them
398:18 - permissions via group or we can Copy permissions
from existing user or we can attach policies
398:19 - directly directly to them. So if we wanted
to give them s3, full access, we could do
398:20 - so here. Okay. And then we can just apply
those permissions. So just to wrap up this
398:21 - section, we're just going to cover a rules
and policies here. So first, we'll go into
398:22 - policies. And here we have a big list of policies
here that are managed by AWS, they say they're
398:23 - managed over here, and you can definitely
tell because they're camelcase. And they also
398:24 - have this nice little orange box, okay. And
so these are policies which you cannot edit,
398:25 - they're read only, but they're a quick and
fast way for you to start giving access to
398:26 - your users. So if we were just to take a look
at one of them, like the EC 214, or maybe
398:27 - just read only access, we can click into them.
And we can see over onto the I am cheat sheets,
398:28 - let's jump into it. So identity access management
is used to manage access to users and resources
398:29 - I am is a universal system, so it's applied
to all regions at the same time, I am is also
398:30 - a free service. A root account is the account
initially created when AWS is set up and so
398:31 - it has full administrator access. New Iam
accounts have no permissions by default until
398:32 - granted, new users get assigned an access
key ID and secret when first created when
398:33 - you give them programmatic access. Access
keys are are only used for the COI and SDK,
398:34 - they cannot access the console. Access keys
are only shown once when created, if lost,
398:35 - they must be deleted and recreated again,
always set up MFA for your root accounts.
398:36 - Users must enable MFA on their own administrators
cannot turn it on for each user, I am allows
398:37 - you, you to Set password policies to set minimum
password requirements or rotate passwords.
398:38 - Then you have Iam identities, such as users,
groups and roles and we'll talk about them
398:39 - now. So we have users, those are the end users
who log into the console or interact with
398:40 - AWS resources programmatically, then you have
groups. So that is a logical group of users
398:41 - that share all the same permission levels
of that group. So think administrators, developers,
398:42 - auditors, then you have roles, which associates
permissions to a role, and then that role
398:43 - is then assigned to users or groups. Then
you have policy. So that is a Jason document
398:44 - which grants permissions for specific users
groups, roles to access services, policies
398:45 - are generally always attached to Im identities.
You have some variety of policies, you have
398:46 - managed policies, which are created by AWS,
that cannot be edited, then you have customer
398:47 - managed policies, those are policies created
by you that are editable and you have inline
398:48 - policies which are directly attached to the
user. So there you go, that is I am. Hey,
398:49 - this is Angie brown from exam Pro. And we
are looking at CloudFront, which is a CDN
398:50 - a content distribution network, it creates
cache copies of your website at various edge
398:51 - locations around the world. So to understand
what CloudFront is, we need to understand
398:52 - what a content delivery network is. So a CDN
is a distributed network of servers, which
398:53 - deliver web pages and content to users based
on their geographical location, the origin
398:54 - of the web page and a content delivery server.
So over here, I have a graphical representation
398:55 - of a CDN, specifically for CloudFront. And
so the idea is that you have your content
398:56 - hosted somewhere. So here the origin is s3.
And the idea is that the server CloudFront
398:57 - is going to distribute a copy of your website
on multiple edge locations, which are just
398:58 - servers around the world that are nearby to
the user. So when a user from Toronto tries
398:59 - to access our content, it's not going to go
to the s3 bucket, it's going to go to CloudFront.
399:00 - And CloudFront is going to then route it to
the nearest edge location so that this user
399:01 - has the lowest latency. And that's the concept
behind clustering. So it's time to look at
399:02 - the core components for CloudFront. And we'll
start with origin, which is where the original
399:03 - files are located. And generally, this is
going to be an s3 bucket. Because the most
399:04 - common use case for CloudFront is static website
hosting. However, you can also specify origin
399:05 - to be an easy to instance, on elastic load
balancer or route 53. The next thing is the
399:06 - actual distribution itself. So distribution
is a collection of edge locations, which define
399:07 - how cash content should behave. So this definition,
here is the thing that actually says, Hey,
399:08 - I'm going to pull from origin. And I want
this to update the cache, whatever whatever
399:09 - frequency or use HTTPS, or that should be
encrypted. So that is the settings for the
399:10 - distribution. And then there's the edge locations.
And an edge location is just a server. And
399:11 - it is a server that is nearby to the actual
user that stores that cache content. So those
399:12 - are the three components to pop. So we need
to look at the distribution component of CloudFront
399:13 - in a bit more detail, because there's a lot
of things that we can set in here. And I'm
399:14 - not even showing you them all, but let's just
go through it. So we have an idea, the kinds
399:15 - of things we can do with it. So again, a distribution
is a collection of edge locations. And the
399:16 - first thing you're going to do is you're going
to specify the origin. And again, that's going
399:17 - to be s three, EC two lb, or refer D three.
And when you said your distribution, what's
399:18 - really going to determine the cost and also
how much it's going to replicate across is
399:19 - the price class. So here, you can see, if
you choose all edge locations, it's gonna
399:20 - be the best performance because your website
is going to be accessible from anywhere in
399:21 - the world. But you know, if you're operating
just in North America and the EU, you can
399:22 - limit the amount of servers it replicates
to. There are two types of distributions,
399:23 - we have web, which is for websites, and rtmp,
which is for streaming media, okay, um, you
399:24 - can actually serve up streaming video under
web as well. But rtmp is a very specific protocol.
399:25 - So it is its own thing. When you set up behaviors,
there's a lot of options we have. So we could
399:26 - redirect all the traffic to be HTTPS, we could
restrict specific hv methods. So if we don't
399:27 - want to have puts, we can say we not include
those. Or we can restrict the viewer access,
399:28 - which we'll look into a little bit more detail
here, we can set the TTL, which is time to
399:29 - expiry, or Time To Live sorry, which says
like after, we could say every two minutes,
399:30 - the content should expire and then refresh
it right, depending on how, how stale we want
399:31 - our content to be. There is a thing called
invalidations in CloudFront, which allow you
399:32 - to manually set so you don't have to wait
for the TTL. to expire, you could just say
399:33 - I want to expire these files, this is very
useful when you are pushing changes to your
399:34 - s3 bucket because you're gonna have to go
manually create that invalidation. So those
399:35 - changes will immediately appear. You can also
serve back at error pages. So if you need
399:36 - a custom 404, you can do that through CloudFront.
And then you can set restrictions. So if you
399:37 - for whatever reason, aren't operating in specific
countries, and you don't want those countries
399:38 - to consume a lot of traffic, which might cost
you money, you can just restrict them saying
399:39 - I'm blocking these countries, or or you could
do the other way and say I only whitelist
399:40 - these countries, these are the only countries
that are allowed to view things from CloudFront.
399:41 - So there's one interesting feature I do want
to highlight on CloudFront, which is lambda
399:42 - edge and lambda edge are lambda functions
that override the behavior of requests and
399:43 - responses that are flowing to and from CloudFront.
And so we have four that are available to
399:44 - us, we have the viewer requests, the origin
requests, the origin response, and the viewer
399:45 - response, okay, and so on our on our CloudFront
distribution under probably behavior, we can
399:46 - associate lambda functions. And that allows
us to intercept and do things with this, what
399:47 - would you possibly use lambda edge for a very
common use case would let's say you have protected
399:48 - content, and you want to authenticate it against
something like cognito. So only users that
399:49 - are within your cognito authentication system
are allowed to access that content, that's
399:50 - just something we do on exam pro for the video
content here. So you know, that is one method
399:51 - for protecting stuff. But there's a lot of
creative solutions here with you can use lambda
399:52 - edge, you could use it to serve up a to be
testing websites, so you could have it so
399:53 - when the viewer request comes in, you have
a roll of the die, and it will change what
399:54 - it serves back. So it could be it could set
up a or set up B and that's something we also
399:55 - do in the exam pro marketing website. So there's
a lot of opportunities here with lambda edge.
399:56 - I don't know if it'll show up in the exam,
I'm sure eventually will. And it's just really
399:57 - interesting. So I thought it was worth talking.
So now we're talking about CloudFront protection.
399:58 - So CloudFront might be serving up your static
website, but you might have protected content,
399:59 - such as video content, like on exam Pro, or
other content that you don't want to be easily
400:00 - accessible. And so when you're setting up
your CloudFront distribution, you have this
400:01 - option to restrict viewer access. And so that
means that in order to view content, you're
400:02 - going to have to use signed URLs or signed
cookies. Now, when you do check this on, it
400:03 - actually will create you an origin identity
access and oh AI. And what that is it's a
400:04 - virtual user identity that it will be used
to give CloudFront distributions permission
400:05 - to fetch private objects. And so those private
objects generally mean from an s3 bucket that's
400:06 - private, right? And as soon as that set up,
and that's automatically set up for you. Now
400:07 - you can go ahead and use signed URLs and signed
cookies. So one of these things well, the
400:08 - idea behind it is a sign URL is just a URL
that CloudFront Provide you that gives you
400:09 - temporary access to those private cached objects.
Now, you might have heard of pre signed URLs.
400:10 - And that is an s3 feature. And it's similar
nature. But it's very easy to get these two
400:11 - mixed up because sign URLs and pre signed
URLs sound very similar. But just know that
400:12 - pre signed URLs are for s3 and sign URLs are
for CloudFront, then you have signed cookies,
400:13 - okay. And so it's similar to sign URLs, the
only difference is that you're you passing
400:14 - along a cookie with your request to allow
users to access multiple files, so you don't
400:15 - have to, every single time generate a signed
cookie, you set it once, as long as that cookie
400:16 - is valid and pass along, you can access as
many files as you want. This is extremely
400:17 - useful for video streaming, and we use it
on exam Pro, we could not do video streaming,
400:18 - protected with sign URLs, because all the
video streams are delivered in parts, right,
400:19 - so a cookie has to get set. So that that is
your options for protecting cloud. It's time
400:20 - to get some hands on experience with CloudFront
here and create our first distribution. But
400:21 - before we do that, we need something to serve
up to the CDN. Okay, um, so we had an s3 section
400:22 - earlier, where I uploaded a bunch of images
from Star Trek The Next Generation. And so
400:23 - for you, you can do the same or you just need
to make a bucket and have some images within
400:24 - that bucket, so that we have something to
serve up, okay. So once you have your bucket
400:25 - of images prepared, we're going to go make
our way to the CloudFront console here. And
400:26 - so just type in CloudFront, and then click
there. And you'll get to the same place as
400:27 - me here. And we can go ahead and create our
first distribution. So we're gonna be presented
400:28 - with two options, we have web an rtmp. Now
rtmp is for the Adobe Flash Media Server protocol.
400:29 - So since nobody really uses flash anymore,
we can just kind of ignore this distribution
400:30 - option. And we're going to go with web, okay.
And then we're going to have a bunch of options,
400:31 - but don't get overwhelmed, because it's not
too tricky. So the first thing we want to
400:32 - do is set our origin. So where is this distribution
going to get its files that wants to serve
400:33 - up, it's going to be from s3. So we're going
to click into here, we're going to get a drop
400:34 - down and we're going to choose our s3 bucket,
then we have path, we'll leave that alone,
400:35 - we have origin ID, we'll leave that alone.
And then we have restrict bucket access. So
400:36 - this is a cool option. So the thing is, is
that let's say you only want people to access
400:37 - your, your bucket resources through CloudFront.
Because right now, if we go to s3 console,
400:38 - I think we made was data public, right? And
if we were to look at this URL, okay, this
400:39 - is publicly accessible. But let's say we wanted
to force all traffic through CloudFront. Because
400:40 - we don't, we want to be confident they can
track things. So we get some rich analytics
400:41 - there. And we just don't want people directly
accessing this ugly URL. Well, that's where
400:42 - this option comes in, restrict bucket access,
okay, and it will, it will create an origin
400:43 - identity access for us, but we're gonna leave
it to No, I just want you to know about that.
400:44 - And then down to the actual behavior settings,
we have the ability to redirect HTTP to HTTPS.
400:45 - That seems like a very sane setting, we can
allow these to be methods, we're only going
400:46 - to be ever getting things we're never going
to be put or posting things. And then we'll
400:47 - scroll down, scroll down, we can set our TTL,
the defaults are very good. And then down
400:48 - here, we have restrict viewer access. So if
we wanted to restrict the viewer access to
400:49 - require signed URLs of site cookies to protect
access to our content, we'd press yes here.
400:50 - But again, we just want this to be publicly
available. So we're going to set it to No,
400:51 - okay. And then down below, we have distribution
settings. And this is going to really affect
400:52 - our price. The cost we're going to pay here,
as it says price class, okay. And so we can
400:53 - either distribute all copies of our files
to every single edge location, or we can just
400:54 - say US, Canada, Europe, or just US, Canada,
yeah, Europe, Asia, Middle East Africa, or
400:55 - just the the main three. So I want to be cost
saving here. So I'm really going to cost us
400:56 - a lot anyway, but I think that if we set it
to the lowest class here that it will take
400:57 - less time for the distribution to replicate
here in this tutorial go a lot faster. Okay,
400:58 - then we have the ability to set an alternate
domain name, this is important if we are using
400:59 - a CloudFront certificate and we want a custom
domain name, which we would do in a another
401:00 - follow along but not in this one here. Okay,
and if this was a website, we would set the
401:01 - default route here to index dot HTML. Okay,
so that's pretty much all we need to know
401:02 - here. And we'll go ahead and create our distribution,
okay, and so our distribution is going to
401:03 - be in progress. And we're going to wait for
it to distribute those files to all those
401:04 - edge locations. Okay, and so this will just
take a little bit of time here. He usually
401:05 - takes I don't know like three to five minutes
so we'll we'll resume the video when this
401:06 - is done creating. So creating that distribution
took a lot longer than I was hoping for, it
401:07 - was more like 15 minutes, but I think the
initial one always takes a very long time.
401:08 - And then then after, whenever you update things,
it still takes a bit of time, but it's not
401:09 - 15 minutes more like five minutes. Okay. But
anyway, um, so our distribution is created.
401:10 - Here we have an ID, we have a domain name,
and we're just going to click in to this distribution,
401:11 - and we're gonna see all the options we have
here. So we have general origins, behaviors,
401:12 - error pages, restrictions, and validations.
And tags. Okay, so when we were creating the
401:13 - distribution, we configured both general origins
and behaviors all in one go. Okay, and so
401:14 - if we wanted to override the behaviors from
before, we just clicked edit here, we're not
401:15 - going to change anything here. But I just
want to show you that we have these options
401:16 - previous. And just to see that they are broken
up between these three tabs here. So if I
401:17 - go to Edit, there's some information here
and some information there. Okay. So now that
401:18 - we have our distribution working, we have
this domain name here. And if we had, if we
401:19 - had used our own SSL, from the Amazon certification
manager, we could add a customer domain, but
401:20 - we didn't. So we just have the domain that
is provided with us. And this is how we're
401:21 - going to actually access our our cache file.
So what I'm going to do is copy that there.
401:22 - I'm just going to place it here in a text
editor here. And the idea here is we want
401:23 - to then, from the enterprise D pull one of
the images here. So if we have data, we'll
401:24 - just take the front of it there, okay. And
we are going to just assemble a new URL. So
401:25 - we're going to try data first here, and data
should work without issue. Okay. And so now
401:26 - we are serving this up from CloudFront. So
that is how it works now, but data is set
401:27 - to public access. Okay, so that isn't much
of a trick there. But for all these other
401:28 - ones, I just want to make sure that he has
public access here and it is set here yep,
401:29 - to public access. But let's look at someone
that actually doesn't have public access,
401:30 - such as Keiko, she does not have public access.
So the question is, will CloudFront make files
401:31 - that do not have public access set in here
publicly accessible, that's what we're gonna
401:32 - find out. Okay. So we're just going to then
assemble on another URL here, but this time
401:33 - with Keiko, okay, and we're gonna see if we
can access her All right. Okay, oops, I copied
401:34 - the wrong link. Just copy that one more time.
Okay, and there you go. So Keiko is not available.
401:35 - And this is because she is not publicly accessible.
Okay. So just because you create a CloudFront
401:36 - distribution doesn't necessarily mean that
these files will be accessible. So if we were
401:37 - to go to Keiko now and then set her to public,
would she be accessible now through CloudFront?
401:38 - Okay, so now she is all right. So so just
keep that in mind that when you create a CloudFront
401:39 - distribution, you're going to get these URLs.
And unless you explicitly set the objects
401:40 - in here to be publicly accessible, they're
not going to be publicly accessible. Okay.
401:41 - But yeah, that's all there is to it. So we
created our CloudFront. So we need to touch
401:42 - on one more thing here with CloudFront. And
that is invalidation. So, up here we have
401:43 - this Keiko image, which is being served up
by CloudFront. But let's say we want to replace
401:44 - it. Okay, so in order to replace images on
CloudFront, it's not as simple as just replacing
401:45 - an s3. So here we have Keiko, right, and this
is the current image. And so let's say we
401:46 - wanted to replace that. And so I have another
version of Keiko here, I'm just going to upload
401:47 - it here. And that's going to replace the existing
one. Okay. And so I'm just going to make sure
401:48 - that the new one is here. So I'm just going
to right click or sorry, gonna hit open here,
401:49 - make sure it's set to public. And then I'm
just going to click the link here. And it
401:50 - still now it's the new one, right, so here
we have the new one. And if we were to go
401:51 - to the CloudFront distribution and refresh,
it's still the old image, okay, because in
401:52 - order for these new changes to propagate,
you have to invalidate the old the old cache,
401:53 - okay, and that's where invalidation is come
into play. So to invalidate the old cache,
401:54 - we can go in here to create invalidations.
And we can put a wildcard to expire everything
401:55 - or we could just expire. Keiko. So, for Keiko,
she's at Ford slash enterprise D. So we would
401:56 - just paste that in there. And we have now
created an invalidation. And this is going
401:57 - to take five, five minutes. I'm not going
to wait around to show you this because I
401:58 - know it's going to work. But I just want you
to know that if you update something in order,
401:59 - in order for it to work, you have to create
a validation. So it's time to look at the
402:00 - CloudFront cheat sheet. And let's get to it.
So CloudFront is a CDN a content distribution
402:01 - network. It makes websites load fast by serving
cache content that is nearby CloudFront distributes
402:02 - cached copies at edge locations. Edge locations
aren't just read only you can actually write
402:03 - to them so you can do puts to them. We didn't
really cover that in the core content, but
402:04 - it's good to know. CloudFront has a feature
called TTL, which is time to live. And that
402:05 - defines how long until a cache expires. Okay,
so if you set it to expire every hour every
402:06 - day, that's how fresh or I guess you'd say
how stale your content is going to be. When
402:07 - you invalidate your cache, you're forcing
it to immediately expire. So just understand
402:08 - that invalidations means you're you're refreshing
your cache, okay? refreshing, the cast does
402:09 - cost money because of the transfer cost to
update edge locations, right. So if you have
402:10 - a file, and it's and it's expired, it then
has to then send that file to 1020, whatever
402:11 - amount of servers it is, and there's always
that outbound transfer cost, okay? origin
402:12 - is the address of where the original copies
of your files reside. And again, that can
402:13 - be a three EC two, lb raffa, d three, then
you have distribution, which defines a collection
402:14 - of edge locations and behavior on how it should
handle your cash content. We have two types
402:15 - of distributions, we have the web distribution,
also known as web, which is for static website
402:16 - content. And then you have rtmp, which is
for streaming media, again, that is a very
402:17 - specific protocol, you can serve up video
streaming via the web distribution, then we
402:18 - have origin identity access, which is used
to access private s3 buckets. If we want to
402:19 - access cash content that is protected, we
need to use sign URLs or signed cookies, again,
402:20 - don't get signed roles confused with pre signed
URLs, which is an s3 feature. But it's pretty
402:21 - much the same in terms of giving you access
to something, then you have lambda edge, which
402:22 - allows you to pass each request through a
lambda to change the behavior of the response
402:23 - or the request. Okay, so there you go. That
is cloud front in a nutshell. Hey, this is
402:24 - Andrew Brown from exam Pro. And we are looking
at Cloud trail, which is used for logging
402:25 - API calls between AWS services. And the way
I like to think about this service, it's when
402:26 - you need to know who to blame. Okay, so as
I said earlier, cloud trail is used to monitor
402:27 - API calls and actions made on an AWS account.
And whenever you see these keywords, governance,
402:28 - compliance, operational auditing, or risk
auditing, it's a good indicator, they're probably
402:29 - talking about AWS cloud trail. Now, I have
a record over here to give you an example
402:30 - of the kinds of things that cloud trail tracks
to help you know how you can blame someone
402:31 - when something's gone wrong. And so we have
the where, when, who and what so the where,
402:32 - so we have the account, Id what, like, which
account did it happen in, and the IP address
402:33 - of the person who created that request, the
lens, so the time it actually happened, the
402:34 - who, so we have the user agent, which is,
you know, you could say I could tell you the
402:35 - operating system, the language, the method
of making this API call the user itself. so
402:36 - here we can see Worf made this call and, and
what so to what service, and you know, it'll
402:37 - say what region and what service. So this
service, it's using, I am here, I in the action,
402:38 - so it's creating a user. So there you go,
that is cloud trail in a nutshell. So within
402:39 - your AWS account, you actually already have
cloud trail logging things by default, and
402:40 - it will collect into the last 90 days under
the event history here. And we get a nice
402:41 - little interface here. And we can filter out
these events. Now if you need logging be on
402:42 - 90 days. And that is a very common use case,
which you definitely want to create your own
402:43 - trail, you'd have to create a custom trail.
The only downside when you create a custom
402:44 - trail is that it doesn't have a gooey like
here, such as event history. So there is some
402:45 - manual labor involved to visualize that information.
And a very common method is to use Amazon
402:46 - Athena. So if you see cloud trail, Amazon,
Athena being mentioned in unison, there's
402:47 - that reason for that, okay. So there's a bunch
of trail options I want to highlight and you
402:48 - need to know these, they're very important
for cloud trail. So the first thing you need
402:49 - to know is that a trail can be set to log
in all regions. So we have the ability here
402:50 - to say yes, and now we know region is missed.
If you are using an organization, you'll have
402:51 - multiple accounts and you want to have coverage
across all those. So in a single trail, you
402:52 - can check box on applied to my entire organization.
You can encrypt your cloud trail logs, what
402:53 - you definitely want to do using server side
encryption via key management service, which
402:54 - abbreviate is SSE kms. And you want to enable
log file validation because this is going
402:55 - to tell whether someone's actually tampered
with your logs so it's not going to prevent
402:56 - someone from being able to tamper from your
logs. But it's going to at least let you know
402:57 - how much you can trust your logs. So I do
want to emphasize that cloud trail can deliver
402:58 - its events to cloudwatch. So there's an option
After you create the trail where you can configure,
402:59 - and then it will send your events to cloudwatch
logs. All right? I know cloud trail and cloud
403:00 - watch are confusing, because they seem like
they have overlapping of responsibilities.
403:01 - And there are a lot of Ada services that are
like that. But you know, just know that you
403:02 - can send cloud trail events to cloudwatch
logs, not the other way around. And there
403:03 - is that ability to. There are different types
of events in cloud trail, we have measurement
403:04 - events, and data events. And generally, you're
always looking at management events, because
403:05 - that's what's turned on by default. And there's
a lot of those events. So I can't really list
403:06 - them all out for you here. But I can give
you a general idea what those events are.
403:07 - So here are four categories. So it could be
configuring security. So you have attach rule
403:08 - policy, you'd be registering devices, it would
be configuring rules for routing data, it'd
403:09 - be setting up logging. Okay. So 90% of events
in cloud trail are management events. And
403:10 - then you have data events. And data events
are actually only for two services currently.
403:11 - So if you were creating your trail, you'd
see tabs, and I assume as one, they have other
403:12 - services that can leverage data events, we'll
see more tabs here. But really, it's just
403:13 - s3 and lamda. And they're turned off by default,
for good reason. Because these events are
403:14 - high volume, they occur very frequently, okay.
And so this is tracking more in detail s3,
403:15 - events, such as get object, delete object
put object, if it's a lamda, it'd be every
403:16 - time it gets invoked. So those are just higher
there. And so those are turned off by default.
403:17 - Okay. So now it's time to take a quick tour
of cloud trail and create our very own trail,
403:18 - which is something you definitely want to
do in your account. But before we jump into
403:19 - doing that, let's go over to event history
and see what we have here. So AWS, by default,
403:20 - will track events in the last 90 days. And
this is a great safeguard if you have yet
403:21 - to create your own trail. And so we have some
event history here. And if we were just to
403:22 - expand any of them doesn't matter which one
and click a view event, we get to, we get
403:23 - to see what the raw data looks like here for
a specific event. And we do have this nice
403:24 - interface where we can search via time ranges
and some additional information. But if you
403:25 - need data beyond 90 days, you're going to
have to create a trail. And also just to analyze
403:26 - this because we're not going to have this
interface, we're gonna have to use Athena
403:27 - to really make sense of any cloud trail information.
But now that we have learned that we do have
403:28 - event history available to us, let's move
on to creating our own trail. Let's go ahead
403:29 - and create our first trail. And I'm just going
to name my trail here exam pro trail, I do
403:30 - want you to notice that you can apply a trail
to all regions, and you definitely want to
403:31 - do that. Then we have management events where
we can decide whether we want to have read
403:32 - only or write only events, we're going to
want all of them, then you have data events.
403:33 - Now these can get expensive, because s3 and
lambda, the events that they're tracking are
403:34 - high frequency events. So you can imagine
how often someone might access something from
403:35 - an s3 bucket, such as a get or put. So they
definitely do not include these. And you have
403:36 - to check them on here to have the inclusion
of them. So if you do want to track data events,
403:37 - we would just say for all our s3 buckets,
or specify them and lambdas are also high
403:38 - frequency because we would track the invocations
of lambdas. And you could be in the 1000s
403:39 - upon millions there. So these are sanely not
included by default. Now down below, we need
403:40 - to choose our source location, we're going
to let it create a new s3 bucket. For us,
403:41 - that seems like a good choice. We're going
to drop down advanced here, because it had
403:42 - some really good tidbits here. So we can turn
on encryption, which is definitely something
403:43 - we want to do with kms. And so I apparently
have a key already here. So I'm just gonna
403:44 - add that I don't know if that's the default
key. I don't know if you get a default key
403:45 - with cloud trail. Usually you'd have one in
there. But I'm just going to select that one
403:46 - there. Then we have enable log file validation.
So we definitely want to have this to Yes,
403:47 - it's going to check whether someone's ever
tampered with our logs, and whether we should
403:48 - not be able to trust her logs. And then we
could send a notification about log file delivery.
403:49 - This is kind of annoying, so I don't want
to do that. And then we should be able to
403:50 - create our trail as soon as we name our bucket
here. So we will go ahead and just name it
403:51 - we'll say exam pro trails, assuming I don't
have one in another account. Okay, and so
403:52 - it doesn't like that one. That's fine. So
I'm just going to create a new kms key here.
403:53 - Okay. Keys do cost a buck purse, if you want
to skip the step you can totally do. So I'm
403:54 - just going to create one for this here called
exam pro trails. Okay. Great. And so now it
403:55 - has created that trail. And we'll just use
the site here. And then maybe we'll take a
403:56 - peek here in that s3 bucket when we do have
some data. Alright, I do want to point out
403:57 - one more thing is that you couldn't set the
the cloud watch event to track across all
403:58 - organizations, I didn't see that option there,
it's probably because I'm in a sub account.
403:59 - So if I was in my, if you have an alias organization,
right, and this was the root account, I bet
404:00 - I could probably turn it on to work across
all accounts. So we didn't have that option
404:01 - there. But just be aware that it is there.
And you can turn a trail to be across all
404:02 - organizations. So I just had to switch into
my route organization account, because I definitely
404:03 - wanted to show you that this option does exist
here. So when you create a trail, we have
404:04 - applied all regions, but we also can apply
to all organizations, which means all the
404:05 - accounts within an organization, okay. So
you know, just be aware of that. So now that
404:06 - our trail is created, I just want you to click
into and be aware that there's an additional
404:07 - feature that wasn't available to us when we
were creating the trail. And that is the ability
404:08 - to send our cloud trail events to cloud watch
logs. So if you wanted to go ahead and do
404:09 - that, you could configure that and create
an IM role and send it to a log, or cloud
404:10 - watch log group. There are additional fees
apply here. And it's not that important to
404:11 - go through the motions of this. But just be
aware that that is a capability that you can
404:12 - do with cloud trail. So I said earlier that
this will collect beyond 90 days, but you're
404:13 - not going to have that nice interface that
you have an event history here. So how would
404:14 - you go about analyzing that log, and I said,
you could use Amazon, Athena. So luckily,
404:15 - they have this link here, that's going to
save you a bunch of setup to do that. So if
404:16 - you were to click this here, and choose the
s3 bucket, which is this one here, it's going
404:17 - to create that table for you. And Athena,
we used to have to do this manually, it was
404:18 - quite the pain. So it's very nice that they
they've added this one link here. And I can
404:19 - just hit create table. And so what that's
going to do, it's going to create that table
404:20 - in Athena for us. And we can jump over to
Athena, okay. And yeah, it should be created
404:21 - here. Just give it a little refresh here,
I guess we'll just click Get Started. I'm
404:22 - not sure why it's not showing up here. We're
getting the splash screen. But we'll go in
404:23 - here and our table is there. So we got this
little goofy tutorial, I don't want to go
404:24 - through it. But that table has now been created.
And we have a bunch of stuff here. There is
404:25 - a way of running a sample query, I think he
could go here and it was preview table. And
404:26 - that will create us a query. And then we it
will just run the query. And so we can start
404:27 - getting data. So the cool advantage here is
that if we want to query our data, just like
404:28 - using SQL, you can do so here and Athena,
I'm not doing this on a day to day basis.
404:29 - So I can't say I'm the best at it. But you
know, if we gave this a try here and tried
404:30 - to query something, maybe based on event type,
I wonder if we could just like group by event
404:31 - type here. So that is definitely a option.
So we say distinct. Okay, and I want to be
404:32 - distinct on maybe event type here. Okay. doesn't
like that little bit, just take that out there.
404:33 - Great. So there we go. So that was just like
a way so I can see all the unique event types,
404:34 - I just take the limit off there, the query
will take longer. And so we do have that one
404:35 - there. But anyway, the point is, is that you
have this way of using SQL to query your logs.
404:36 - Obviously, we don't have much in our logs,
but it's just important for you to know that
404:37 - you can do that. And there's that one button,
press enter to create that table and then
404:38 - start querying your logs. So we're on to the
cloud trail cheat sheet and let's get to it.
404:39 - So Cloud trail logs calls between eight of
us services. When you see the keywords such
404:40 - as governance, compliance, audit, operational
auditing and risk auditing, it's a high chance
404:41 - they're talking about cloud trail when you
need to know who to blame. Think cloud trail
404:42 - cloud trail by default logs events data for
the past 90 days via event history. To track
404:43 - beyond 90 days you need to create a trail
to ensure logs have not been tampered with,
404:44 - you need to turn on log file validation option.
Cloud trail logs can be encrypted using kms
404:45 - cloud trail can be set to log across all Eva's
accounts in an organization and all regions
404:46 - in an account. Cloud trail logs can be streamed
cloudwatch logs, trails are outputted to s3
404:47 - buckets that you specify cloud trail logs
come in two kinds. We have a management events
404:48 - and data events, management events, log management
operations. So you know, attach rule policy,
404:49 - data events, log data operations for resources.
And there's only really two candidates hear
404:50 - s3 and lambda. So think get object, delete
object put put object did events are disabled
404:51 - by default, when creating a trail, trail log
trail logs in s3, and can be analyzed using
404:52 - Athena, I'm gonna have to reword that one.
But yeah, that is your teaching. Hey, this
404:53 - is Andrew Brown. And we are looking at Eva's
cloud formation, which is a templating language
404:54 - that defines AWS resources to be provisioned,
or automating the creation of resources via
404:55 - code. And all these concepts are called infrastructure
as code which we will cover again in just
404:56 - a moment here. So to understand cloud formation,
we need to understand infrastructure as code
404:57 - because that is what cloud formation is. So
let's reiterate over what infrastructure is
404:58 - code is. So it's the process of managing and
provision computer data centers. So in our
404:59 - case, it's AWS, through machine readable definition
files. And so in this case, it's cloudformation,
405:00 - template YAML, or JSON files, rather than
the physical hardware configuration or interactive
405:01 - configuration tools. So the idea is to stop
doing things manually, right. So if you launch
405:02 - resources, AWS, you're used to configuring
in the console all those resources, but through
405:03 - a scripting language, we can automate that
process. So now let's think about what is
405:04 - the use case for cloud formation. And so here,
I have an example, where let's pretend that
405:05 - we have our own minecraft server business,
and people sign up on our website and pay
405:06 - a monthly subscription, and we will run that
server for them. So the first thing they're
405:07 - gonna do is they're gonna tell us where they
want the server to run. So they have low latency
405:08 - and what size of servers so the larger the
server, the more performant the server will
405:09 - be. And so they give us those two inputs.
And then we somehow send that to a lambda
405:10 - function, and that lambda function triggers
to launch a new cloudformation stack using
405:11 - our cloud formation template, which defines,
you know, how to launch that server, that
405:12 - easy to instance, running Minecraft and a
security group and what region and what size.
405:13 - And when it's finished creating, we can monitor
maybe using cloud watch events that it's done,
405:14 - and using the outputs from that cloudformation
stack, send the IP address of the new micro
405:15 - server to the user, so they can log in and
start using their servers. So that's way of
405:16 - automating our infrastructure. So we're going
to look at what a cloudformation template
405:17 - looks like. And this is actually one we're
gonna use later on to show you how to launch
405:18 - a very simple Apache server. But confirmation
comes in two variations. It comes in JSON,
405:19 - and YAML. So why is there two different formats?
Well, JSON just came first. And YAML is is
405:20 - an intent based language, which is just more
concise. So it's literally the same thing,
405:21 - except it's in that base. So we don't have
to do all these curlies. And so you end up
405:22 - with something that is in length, half the
size. Most people prefer to write YAML files.
405:23 - But there are edge cases where you might want
to use JSON. But just be aware of these two
405:24 - different formats. And it doesn't matter which
one you use, just use what works best for
405:25 - you. Now we're looking at the anatomy of a
cloud formation template. And so these are
405:26 - made up of a bunch of different sections.
And here are all the sections listed out here.
405:27 - And we'll work our way from top to bottom.
And so the first one is metadata. So that
405:28 - allows you to provide additional information
about the template, I don't have one of the
405:29 - example here and I rarely ever use metadata.
But you know, it's just about additional information,
405:30 - then you have the description. So that is
just describing what you want this template
405:31 - to do. And you can write whatever you want
here. And so I described this template to
405:32 - launch a new instance running Apache and it's
hard coded work for us East one, then you
405:33 - have parameters and parameters is something
you can use a lot, which is you defining what
405:34 - inputs are allowed to be passed within this
template at runtime. So one thing we want
405:35 - to ask the user is what size of instance type
Do you want to use. It's defaulted to micro,
405:36 - but they can choose between micro and nano.
Okay, so we can have as many parameters as
405:37 - we want, which we'll use throughout our template
to reference, then you have mappings, which
405:38 - is like a lookup table, it maps keys to values
so you can change your values and something
405:39 - else. A good example of this would be, let's
say you have a region, and for each region,
405:40 - the image ID string is different. So you'd
have the region keys mapped to different image
405:41 - IDs based on the region. So that's a very
common use for mappings. Then you'd have conditions
405:42 - these are like your if else statements within
your template. Don't have an examples here,
405:43 - but that's all you need to know. Transform
is very difficult to explain if you don't
405:44 - know what macros are, but the idea it's like
applying a mod to The actual template, and
405:45 - it will actually change what you're allowed
to use in the template. So if I define a transform
405:46 - template, the rules here could be wildly different,
different based on what kind of extra functionality
405:47 - that transform adds, we see that with Sam,
the serverless application model is a transform.
405:48 - So if you ever take a look at that, you'll
have a better understanding of what I'm talking
405:49 - about there. Then you have resources, which
is the main show to the whole template, these
405:50 - are the actual resources you are defining
that will be provisioned. So think any kind
405:51 - of resource I enroll easy to instance, lambda
RDS, anything, right. And then you have outputs
405:52 - is, it's just what you want to see as the
end results. So like, when I create the server,
405:53 - it's we don't know the IP address is until
it spins it up. And so I'm saying down here,
405:54 - get me the public IP address. And then in
the console, we can see that IP address, so
405:55 - that we don't have to, like look at the easy
to console pull it out. The other advantage
405:56 - of outputs is that you can pass information
on to other cloud formation templates, or
405:57 - created like a chain of effects because we
have these outputs. But the number one thing
405:58 - you need to remember is what makes a valid
template. And there's only one thing that
405:59 - is required, and that is specifying at least
one resource. All these other fields are optional,
406:00 - but resource is mandatory, and you have to
have at least one resource. So if you're looking
406:01 - for a cloudformation templates to learn, by
example, Eva's quickstarts is a great place
406:02 - to do it, because they have a variety of different
categories, where we have templates that are
406:03 - pre built by AWS partners and the APN. And
they actually usually show the architectural
406:04 - diagram, but the idea is you launch the template,
you don't even have to run it, you can just
406:05 - press a button here and then actually see
the raw template. And that's going to help
406:06 - you understand how to connect all this stuff
together. Because if you go through the IRS
406:07 - documentation, you're going to have to spend
a lot of time figuring that out where this
406:08 - might speed that up if this is your interest.
So I just wanted to point that out for you,
406:09 - it's not really important for the exam, it's
not going to come up as an exam question.
406:10 - It's just a learning resource that I want
you to. Let's talk about stack updates. So
406:11 - the idea is that you have a cloudformation
template and you deploy it. And now you need
406:12 - to make some kind of change. And so you might
think, Well, alright, I have to delete the
406:13 - entire cloudformation template, and then re
upload recreate the entire stack. But that's
406:14 - not the case, because of the cloud formation,
you can just modify your existing cloudformation
406:15 - template, push that stack to update and then
cloudformation is going to intelligently change,
406:16 - delete, reconfigure your resources series
to doing the least amount of work to make
406:17 - those changes, and you're not doing the most
destructive path possible. There are two ways
406:18 - to perform stack updates in cloud formation.
First, we have direct update, and this is
406:19 - very straightforward. The idea is you're going
to directly upload your template to cloudformation,
406:20 - you could use the COI for this. And then it's
going to just immediately deploy. So cloud
406:21 - formation is just going to go ahead and apply
that directly to your existing stack. And
406:22 - this is super fast to do. The other method
is using change sets. So the starting process
406:23 - is the same, you're going to upload your template
to cloud formation. But what's going to happen
406:24 - is that a change set is going to be generated
all that is it's just a way of showing you
406:25 - the difference between what the current state
of the stack is and what the what changes
406:26 - will be made. And the idea is that it gives
you an opportunity to audit or review what
406:27 - gets changed. And so in order for those changes
to take effect, a developer has to manually
406:28 - confirm saying yes, I'm happy with these changes,
go ahead and do it. Okay. So you know, that's
406:29 - the two methods for stack updates. So we were
saying that stack updates are intelligent
406:30 - and cloudformation figures out what should
be performed, whether it's just configured,
406:31 - reconfigure it or recreate that resource.
So let's talk about the circumstances or the
406:32 - actions that cloudformation could take during
an update on a resource. And so the first
406:33 - one is update with no interruption. So imagine
you have any CPU instance, and you just need
406:34 - to have something changed on it like a security
group or something. So the idea is that this
406:35 - update will be performed without affecting
the operation of the actual service. So the
406:36 - availability, the service will still remain,
the physical ID will not change. So for EC
406:37 - two you actually have like an ID for it. And
maybe here when they say physical ID that
406:38 - could also mean like the Amazon resource name
will not change. So you know, it's just this
406:39 - is just a configuration has taken effect.
The next case is where we have updates with
406:40 - some interruption. So there could be cases
where we don't need to destroy the server,
406:41 - but we need to maybe disappear shared it with
a load balancer and then re associate it or
406:42 - same thing with an auto scaling group. But
because that happens, there is a chance for
406:43 - the service to experience downtime on availability,
but the physical ID is going to remain the
406:44 - same, then the third cases where replacement
has to occur. So there's no way around it,
406:45 - the only way is to create a new new instance,
or delete the old and make a new one. A good
406:46 - example is is launch configurations, launch
configurations cannot be modified, they can
406:47 - only be created and cloned. And so in this
case, you're getting a new resource. And that
406:48 - new resource is going to have a new physical
ID. So those are the three cases. So let's
406:49 - talk about preventing stack updates, because
it's possible in certain circumstances, you
406:50 - don't want there for an instance to be replaced,
because let's say you had an RDS database,
406:51 - and it would the action that would be taken
that would replace the database, you would
406:52 - that would result in data loss. So you say
no, I don't want that updated. Or it could
406:53 - be that you have a critical application, and
there's certain ecsu instances that cannot
406:54 - be interrupted. So what you can do is create
a stack policy. And specifically say that,
406:55 - you know, you're not allowed to do an update
replace on this dynamodb table, and then all
406:56 - other actions are allowed. So that's a great
way to just make sure that critical resources
406:57 - are not affected by these stack updates. So
let's take a look here at cloudformation nested
406:58 - stacks, and nested stacks allow you to reference
cloudformation templates inside of another
406:59 - cloudformation template. And the advantage
that we're going to get here is we're gonna
407:00 - be able to create modular templates, so we're
gonna get reusability. And we're going to
407:01 - be able to assemble larger templates. So that's
going to reduce complexity of managing all
407:02 - these templates. And so just to get like a
hierarchy kind of view, you have at the top
407:03 - here, this route, stack, and then underneath,
you can nest stacks within two stacks. And
407:04 - you can see we can go down multiple levels.
So it's not just one level down. But to understand,
407:05 - like, who can access what, generally, we have
nested stacks can access anything from their
407:06 - immediate children. But the root stack is
accessible by everybody. So no matter where
407:07 - the stack is, anything that's defined up there
is accessible everywhere. And just to show
407:08 - you like how you would use a nested stack.
So here I have one versus my stack, and then
407:09 - what we're gonna do is define that type a
cloudformation, stack, and then we're just
407:10 - going to have to provide access to that template.
And you're gonna want to store that template
407:11 - in s3 and serve it up from there. So there
you go. So let's take a look at drift detection
407:12 - in cloud formation. And to understand this,
we need to first understand what is drift.
407:13 - So drift is when your stacks actual configuration
differs, so has drifted by what cloudformation
407:14 - expects it to be. So why does drift happen?
Well, when developers start making ad hoc
407:15 - changes to the sack, a good example is they
go ahead and delete something. So maybe you
407:16 - provisioned a bunch of stuff in cloud formation.
And it created an easy to instance. So you
407:17 - didn't need any more. So the developer deleted
it. But when you go back to the cloud formation
407:18 - console, it still says it's there, but you
know that it's been deleted, and that can
407:19 - cause complications, or it just doesn't give
you good visibility in terms of, you know,
407:20 - what resources you have and what state they
are in. And so what the developer should have
407:21 - done as they should have went and update that
cloudformation template and let the cloud
407:22 - formation delete that resource. So cloud formation
has this feature called detect drift. And
407:23 - it does is what we've been talking about here,
which is it determines whether something's
407:24 - been deleted or it has been modified. And
so all you got to do in your cloudformation
407:25 - stack, there is this drop down, turn on detect
drift, and then you can view the results.
407:26 - Now, I do want to mention about nested stacks
and drift drift detection. So when you are
407:27 - detecting drift on a stack confirmation does
not detect drift on any nested stacks that
407:28 - you belong to and said you can initiate a
drift detection operation directly on those
407:29 - nested stacks. So you have to turn it on for
all the individual stacks, it's not going
407:30 - to trickle down to every single thing in your
hierarchy there. So let's just take a quick
407:31 - peek here what drift detection kinda looks
like so the idea is you would turn it on,
407:32 - and it will tell you whether your stack has
drifted. And you can see when it last checked
407:33 - to see if it's drifted. These are the possible
statuses that your resources could be in.
407:34 - So it could be that it's been deleted, it's
been modified. It's in sync, meaning everything
407:35 - is good, and not checks lose cases where it
just cloudformation hasn't checked it. In
407:36 - fact, if you first launch your or you turn
on drift detection, you have to wait till
407:37 - that check has happened so all of them will
say not checked. And just to take a look at
407:38 - what that looks like. Here I have a bunch
of resources. And on the right hand side,
407:39 - you can see there's some that have been deleted.
There's ones that are in sync, and there are
407:40 - ones that are modified. So there you go. Let's
just talk about rollbacks here with cloud
407:41 - formation. So when you create, update or destroy
a stack, you could encounter an error. And
407:42 - if you encountered and, like an example of
an error could be like your cloudformation
407:43 - template has a syntax error, or your stack
is trying to delete a resource which no longer
407:44 - exists. And so now it has to roll back to
get it back into the previous state. That's
407:45 - the whole point of rollbacks. So roll backs
are turned on by default. You can ignore rollbacks
407:46 - by using this command flag. So you say ignore
rollback, if you are triggering this vcli.
407:47 - I don't know if you can do it via the console,
I don't think so. And rollbacks can fail.
407:48 - So sometimes you'll have to investigate and
change resources are configurations to get
407:49 - it back to the state that you want. Or you
might have to use PDF support to resolve that
407:50 - issue on a failed rollback. Now, these are
the states that you will see it so when a
407:51 - rollback is in progress, you'll see rollback
in progress or rollback succeeds, you'll see
407:52 - update, rollback complete, and when a rollback
fails, you'll see update rollback failed.
407:53 - So you know, there you go. So let's take a
look at pseudo parameters in cloud formation.
407:54 - So parameters that are predefined by cloud
formation, and you do not declare them in
407:55 - your templates use the same way as you would
a parameter. So use it use ref to function
407:56 - to access them. So here is an example of us
using a predefined parameter. The one here
407:57 - is called AWS, a colon colon region. And so
you know, what are these and these are what
407:58 - they are. So we have aid was partition, it
was region it was stack ID, stack name, URL
407:59 - suffix. So not to go through all these because
it's not that important. But let's go through
408:00 - region, because I think that is a very prominent
one, you'll end up using a lot. And so the
408:01 - idea is that, let's say you need to get the
current region that this cloudformation template
408:02 - is running in. And so what you can do is,
as you were seeing there, we do at this colon
408:03 - colon region, and if this was running US East
one, that's what it would return. So that
408:04 - is pseudo parameters. So let's take a look
at resource attributes for cloud formation.
408:05 - These are additional behaviors, you can apply
to resources in your cloudformation templates,
408:06 - to change the relationships. And you know,
just how things happen when you have stack
408:07 - updates or deleting operations. So the first
one we want to look at is creation policy.
408:08 - And what this is going to do, it's going to
prevent the status from reaching a crate complete
408:09 - until cloudformation receives a specified
number of success signals, or the timeout
408:10 - period is exceeded. So on the left hand or
right hand side, we can see we expect either
408:11 - three successes, or we have a timeout of 15
minutes, you know, and that's just to make
408:12 - sure that everything has been created has
successfully created. So it's just an additional
408:13 - check there that you can put it in there,
then you have deletion policy. And so this
408:14 - is going to happen when you are deleting something.
So let's say you are you have resource like
408:15 - an RDS database, and you want to make sure
that anytime it's deleted takes a snapshot.
408:16 - So that's what you could do. I think in most
cases, you're going to want to retain that
408:17 - database, you generally do not want to delete
your database, but it's going to depend on
408:18 - the situation. So you have delete, retain
and snapshot. The next one is update policy.
408:19 - And this is only for SG Alaska cash domain,
and lambda aliases, and it's just whether
408:20 - the handle is going to get replaced. And so
you got to believe you have to say yes, or
408:21 - like true or false. And that's all there is
to it, then you have update replace policy.
408:22 - So this is when you're doing a stack update.
And it's the question of what's going to happen
408:23 - when that stack update occurs, are you are
you gonna delete the resource, retain it or
408:24 - take a snapshot. So it's kind of similar deletion
policy, but it's when resources are being
408:25 - replaced. The last case here is depends on.
And so this is when you have a resource that's
408:26 - dependent on another resource, so you want
that resource created first. So in our scenario
408:27 - here, we have an RDS database, and easy to
instance. And we're saying before you make
408:28 - these two instance, go make the RDS database
first. And so yeah, those are the resource
408:29 - attributes we can. So let's take a look here
at intrinsic functions for cloud formation.
408:30 - And what these do is allow you to assign values
to properties that are not available until
408:31 - runtime and the two most popular ones are
all the way to the bottom there. And that
408:32 - is reference and get attribute and these are
so important that we're going to cover them
408:33 - shortly here in it in other slides, but let's
just talk about them quickly. Here. So reference
408:34 - is going to return the value of a specified
parameter or resource. And get attribute will
408:35 - return the value of an attribute from a resource
in the template. That doesn't make sense right
408:36 - now, don't worry, we're going to cover it
here shortly. So let's go to the top of the
408:37 - list and just see what kind of stuff that
we can do with intrinsic functions. The first
408:38 - one is basics before. So this returns the
base 64 representation of the input string.
408:39 - There's just some cases where you need them
to be base 64. I can't think of anything off
408:40 - the top of my head, I've definitely had to
use this before. But yeah, there's cases for
408:41 - that, then you have cider. So cider, it returns
an array of cider address blocks. When you're
408:42 - working with VPC resources, it needs to be
in the cider format. So that's when you'll
408:43 - be using that function, then you have conditioned
functions. And so here we have n equals, if
408:44 - not, and or so this is going to allow us to
have a bit more logic within our cloudformation
408:45 - template. So if you want to add that kind
of stuff, that's what you got there. Then
408:46 - you have find in map, find a map is used with
the mapping section. So whenever you're doing
408:47 - a mappings, you're going to definitely be
using find find in map and, as the name implies,
408:48 - is trying to find a corresponding value to
a key. So that's what that is for, then you
408:49 - have transform transform is super interesting.
And it's used with Sam. So the serverless
408:50 - application model, which we definitely cover
in another section, actually right after cloudformation.
408:51 - That's what we look into next year. And what
it does is it performs a macro on part of
408:52 - the stack on cloudformation. So what essentially
does is changes the logic of how you can actually
408:53 - write cloud formation templates, giving you
access or extending the ability of cloud formation
408:54 - to do things that it couldn't do before, then
you have get azs, this is going to return
408:55 - a list of availability zones for a specified
region, then you have import value. This is
408:56 - really important when you're working with
nested stacks. So this returns the value of
408:57 - an output exported by another stack. So it's
kind of a way to have stacks talk to each
408:58 - other, then you have join. So if you have
an array, and you want to turn it into a string,
408:59 - where they're delimited, by comma, you'd use
join, then you have select. So let's say you
409:00 - have an array and you want to select an object
of that list by providing an index you do
409:01 - select, then you have split split is the opposite
of join. So you have a string that might be
409:02 - delimited by commas, and you want to turn
that back into array and use that and you
409:03 - have substitute this is where you can substitute
a variable in an input string with another
409:04 - value. So generally, you know, replacing a
part of a string. And those are intrinsic
409:05 - functions. Alright, so let's take a look at
reference in closer detail here. So reference
409:06 - short for ref returns different things for
different resources. And you'll need to look
409:07 - up each resource in the database docs to figure
out what it's returning, whether it's an Arn,
409:08 - or resource name or physical ID. And so here's
an example. And we have a parameter for address
409:09 - VPC. And then we are accessing it down below.
And so this example is actually for parameter.
409:10 - It's not for resource. So for this one, it's
very straightforward for parameters, but resources
409:11 - is a totally different thing. But I want you
to know that if there's something you can't
409:12 - get with a reference, then it's good chance
that you can get it with get attribute. So
409:13 - let's go take a look at get attribute now.
So here we're taking a look at get attribute.
409:14 - And this allows you to access many different
variables on a resource. And you know, sometimes
409:15 - there's lots sometimes there's very few. But
you'll again have to check the database doc
409:16 - to see what's available per resource. So here
in our example, we have a security group at
409:17 - the top. And then down below, you can see
we are referencing that resource with get
409:18 - attribute and then we're getting the group
ID. But yeah, that is get attribute but let's
409:19 - hop over to the docks and take a look. Alright,
so here I am on the agents resource and property
409:20 - types reference. And this is going to help
us see what what we can return with reference
409:21 - or get attribute. So let's take a look at
EC two. That's always a good one to compare
409:22 - here. So I'll open up a new tab and maybe
we'll look at dynamodb. So to figure out what
409:23 - they return, and when there's a lot of options
for easy to so we'll go easy to instance.
409:24 - And for dynamodb we'll go down in the DB table.
But there's a lot of stuff here which tells
409:25 - you how to work with EC two instance, but
we just care about reference. And the fastest
409:26 - way to find is go to get a trip, because references
right above it. And we will go over here and
409:27 - do the same thing. Oops, we'll type in get
a trip. And so there it is. So here for easy
409:28 - to win, you pass the logical ID of the resource
intrinsic function. So this returns the ID.
409:29 - So that's what it returns. Okay. And then
for Dynamo dB, if you go over here, it's going
409:30 - to return the resource name. So there's a
bit of difference there but like, it's confusing
409:31 - because sometimes with some resources, the
ref will actually return the Arn, but in the
409:32 - case where dynamodb doesn't return the Arn
you got to use Get a trip to get it. And then
409:33 - with easy to instance, these are the get the
trips that you have with it. So that's pretty
409:34 - much what you have access to. And you know,
just be aware of that and just realize that
409:35 - there isn't consistency across the board here.
And you'll have to do a bit of digging to
409:36 - figure it out every single time. So let's
take a look at weight conditions. So weight
409:37 - conditions, as the name implies, waits for
a condition. And there are two use cases where
409:38 - you're going to be using weight conditions.
The first is to coordinate stack resource
409:39 - creation with configuration actions that are
external to the stack creation. So what do
409:40 - I mean by that? Well, it just like matters
of whether you're dependent on something outside
409:41 - of your stack. So maybe you have to make sure
that this domain exists. And it's not part
409:42 - of your cloudformation template. Or maybe
you have to go hit an external API endpoint
409:43 - to make sure that something is working. So
it's just something that is external, and
409:44 - nothing about as you're spinning up within
your stack. And the second case is to track
409:45 - the status of a configuration process. So
maybe, you know, you have a resource, and
409:46 - you have to have that resource in a particular
state. And so you're going to pull and continuously
409:47 - check to see whether it's in that state before
proceeding forward. So those are your two
409:48 - use cases. And so here on the right hand side,
we actually have a example of a wait condition
409:49 - and this is for an auto scaling group. This
is actually pulled from the Ito's docs. And
409:50 - it's kind of funny, because eight of us does
not recommend it actually, I think that's
409:51 - an expression. But wait conditions are, are
similar to creation policies. But Avis recommends
409:52 - using creation policies for ESG and EC two,
but here they're using ESG. But there could
409:53 - be a use case for using you for ESG. So we
can't say that you can't use it without auto
409:54 - scaling group. But if you look on the right
hand side, you see you have a weight handle.
409:55 - And then you have something that it depends
on this depends on the web server group. And
409:56 - then you have a timeout, and you reference
the web server capacity. So there's a lot
409:57 - of different options here, you'd have to read
up the docs on it. But the takeaway is that
409:58 - a creation policy waits on the dependent resource
and awake condition waits on the wait condition,
409:59 - and generally, for something external from
your stack. So hopefully, that makes sense.
410:00 - This is Andrew Brown from exam Pro. And welcome
to the cloud formation follow along where
410:01 - we're going to learn how to write a cloud
formation template. And go ahead and deploy
410:02 - an easy to instance all using the power of
infrastructure as code. Now you're going to
410:03 - need a cloud nine environment to do this.
You could do this locally on your computer,
410:04 - but it's a lot easier to do everything through
here. And if you want to how to get started
410:05 - cloud nine, we show it in multiple fall Long's.
But in this particular case, we were showing
410:06 - how to set it up in Elastic Beanstalk follow
along. So I would recommend doing that first
410:07 - before doing this follow along here, or just
figure out how to get a cloud nine environment
410:08 - set up. But once you're in here, and you have
a cloud nine environment, what I'm going to
410:09 - do is I'm going to make a new directory to
store this cloud formation file and then a
410:10 - called MK dir cfn project to create that there,
then we're going to need a new file in there.
410:11 - So I'm just going to touch a new file, I'm
going to call it template dot yamo. It's important
410:12 - that you name it, y m l instead of y ml, it's
just more consistent here. If you look up
410:13 - based on what is the best practices between
the two, it just the whole community agrees
410:14 - to just do the full form here. So let's make
our lives easier. And just always do dot YAML.
410:15 - Now that we have that file there, we can go
ahead and open that up. So I'm just going
410:16 - to open that up. And of course, you can navigate
here and click on it as well. And the first
410:17 - thing we need to specify is the template format
version. So that is AWS template format version,
410:18 - version here. And that's going to be 2010.
Oh 909. And that's this line pretty much says
410:19 - this is this is the version of cloudformation
we're going to be used and everything else,
410:20 - like anything that's in this template is affected
by this version. This version obviously hasn't
410:21 - changed a long time, since it's nearly 10
years ago. Maybe at some point, they will
410:22 - change it. The next thing we'll want to do
is add a description. And we are going to
410:23 - use this little two characters plus to do
multi line and YAML. And I'm just going to
410:24 - say this as infrastructure. For study sync,
we're not actually deploying the study sync
410:25 - application, just because it's a little bit
too much work. But you know, I figured we
410:26 - will just name it that anyway, I change this
to two spaces, you can have four spaces, but
410:27 - generally two spaces a lot easier to work
with. So that's what I'm going to be doing
410:28 - through this YAML is in database and you have
to be very careful with YAML files. Because
410:29 - if you are not using spaces, soft tabs and
use hard tabs, you might receive errors. So
410:30 - we have our format version, we have our description
and the first thing we need to do is specify
410:31 - an actual resource, I'm gonna type resources.
And then we'll name our resource web server.
410:32 - And then it's going to be a type of AWS EC
two instance. Now I could wrap this in single
410:33 - quotations or double quotations, but I'm usually
pretty good about just turning this into a
410:34 - string. So I like to leave a minute naked,
so to speak. In order to launch an EC two
410:35 - instance, we're gonna have to do a couple
things. But before we do that, I just want
410:36 - to show you how you would know what to fill
in here. If you had to read the docs. So I
410:37 - just typed this here, into the docs. And in
cloudformation, we have all these resources
410:38 - here. So if I really wanted to note, you see
two, I would go down to DC two, I would read
410:39 - it and I'd say, Okay, well, I want to launch
a DC two instance. And this would be the closest
410:40 - matching one. And that's how I would know
how to add things to cloud formation template.
410:41 - And here, they give you a full example in
JSON and then in YAML, obviously, YAML Dudamel
410:42 - is a lot less verbose. And that's what people
prefer to use. When eight of us was first
410:43 - starting out. With cloud formation, it only
had JSON. So YAML came later, and it is the
410:44 - preferred way of doing things. Or you can
see all the formats but the question is, do
410:45 - we need to include all these these attributes
or properties? The answer is no. But you have
410:46 - to figure that out by looking at the actual
properties here, it will use a required no
410:47 - required yes or required conditional. So the
easiest way to find out what's required, we'll
410:48 - just do conditional, I don't think there's
any yeses for EC two. And it says we need
410:49 - an image ID. And we're going to need a security
group ID, which is a conditional. Because
410:50 - I would think that if you didn't provide it,
it would just automatically include the default.
410:51 - I think that's what it is. And that's what
we'll probably do, we just won't include one
410:52 - at this point. So I think really, we just
need to include an an instance ID and instance
410:53 - type. Actually, I don't think it says we have
to include an instance type. I think that
410:54 - one is defaulted to something. It is defaulted
to m one small, and we're going to want to
410:55 - T two micro because we want to save money.
So let's make our way back over to here. And
410:56 - now that we've learned a little bit, what
we're going to do is type in properties. And
410:57 - that's going to be all the properties for
this resource that we're making. And so we
410:58 - saw instance type, I want this to be a T to
micro again, I could wrap this in single or
410:59 - double quotations, I just like to leave them
naked. And the next thing we're going to need
411:00 - is the image ID. Now the image ID is going
to be the AMI ID and ami IDs vary based on
411:01 - region. So in order to get this, we're going
to have to make our way over to the EC two
411:02 - console. So type in EC two. And what we'll
do is we'll pretend that we're launching a
411:03 - new instance, we're not going to launch an
instance, but we'll just pretend as if we
411:04 - are so I'll say launch instance here. And
we will get the EC two for North Virginia,
411:05 - make sure you're in US East one, there's gonna
be very problematic if you're not consistent
411:06 - about this. And this is what we want to have
two versions x86 and 64 bit ARM we want the
411:07 - X 86 version. That's what we're used to using
arm is very cool, but not for me today. So
411:08 - I'm going to copy that. And we are going to
just paste that in. And that's all we need
411:09 - to launch an EC two instance with cloud formation.
So in order to do this, we should probably
411:10 - put this in an s3 bucket, I'm just going to
go quickly to the cloud formation wizard to
411:11 - show you the steps that we're going to have
to do and why we're going to put that in s3
411:12 - in a second. So we'll go here, we have a lot
of cloudformation templates, I never created
411:13 - these, these are automatically created for
me when we were doing other tutorials here,
411:14 - or I was running example applications, but
go up to the top here and create a new stack
411:15 - and we'll say new resources standard. And
the idea is we're gonna provide a template
411:16 - and we can either provide one from s3, or
we can upload our own template, this template
411:17 - is so darn small, we could upload it. But
I think it's good to get in the practice of
411:18 - putting it onto s3, because after a certain
size, or like when a file gets a certain size
411:19 - and length, you have to store it in s3. And
that's where you're going to want it to be
411:20 - because a lot of services expect to find your
cloudformation templates there. So what we'll
411:21 - do is we will use the CLA to create a new
bucket and store it there. So I'm going to
411:22 - type in AWS s3 API create bucket. We're gonna
type bucket here, and I'm going to call it
411:23 - study sink. Andrew, I think I already used
that song called end Andrew B. And we're gonna
411:24 - say region, US East one. Now be sure to customize
this based on your name or some other value
411:25 - because these bucket names are unique across
all AWS. So now that I have that there, we'll
411:26 - hit Enter. And if this is successful, it should
give us location back which it did that and
411:27 - that's all great. And now that we have our
bucket, what we can do is go ahead and copy
411:28 - this template file to s3. So I'm gonna type
in AWS s3 and ocers s3 api in an s3. I don't
411:29 - know why it's like that, but that's just how
it is. We'll do environment cfn project template
411:30 - YAML. And then we'll specify the s3 path here.
So it's going to be steady sync Andrew B.
411:31 - And then we will do template dot YAML. Oops,
oops, oops, oops, look at that. Why ml? I'm
411:32 - already messing things up here. I'll just
do that twice there. And I don't care about
411:33 - that other one will that will let it hang
around. If I can remember the Delete command,
411:34 - and I cannot, but we'll try it anyway. Maybe
I can delete that one there. So that it? Yes,
411:35 - it is, there we go, my memory is doing well.
So we now have copied that to cloudformation.
411:36 - And so if we go back to here, we can provide
the URL. Now notice this says HTTP s, colon
411:37 - colon. And that is a different URL there.
So if we go to s3 here, and we go and see
411:38 - your bucket, we have this one here. If we
look at this item, it's going to show us this
411:39 - is the URL. So that's what the URL is, we'll
make our way back to cloud formation here,
411:40 - I'm going to hit I'm going to paste that in
there. We'll go ahead and hit click off, we
411:41 - can go to the viewer of the designer to check
that out. But I don't really care about it.
411:42 - So we'll just go next, we have no parameters,
I'm going to name this study sync, we'll hit
411:43 - next. We will leave all of these alone, these
all seem good to me. Yep, these are all great.
411:44 - And we'll go ahead and hit next, we'll go
all the way down the bottom, and we will hit
411:45 - Create stack. And it's going to have create
and progress. And so now we just have to wait,
411:46 - we'll have to hit refresh here a bunch of
times. This is going to take however long
411:47 - it takes to spin up an EC two instance, if
this fails, it will say rollback in progress.
411:48 - If we have a syntax error, it's totally possible,
we could have a very minute error that we
411:49 - missed here because we were typing this all
manually. And we'll just wait for this resource
411:50 - to complete. So I think it must, the template
must be valid, because it looks like it's
411:51 - been enough and easy to instance. So it's
on its way. So we just have to wait for this
411:52 - to create here. So I'll see you here in a
little bit. Okay, so our EC two instance is
411:53 - now running here, two out of two checks have
passed, there's not much we can do with this
411:54 - instance, because it doesn't have a web server
installed. It doesn't have a security group
411:55 - that exposes Port 80. It's using the default.
So there's a lot of work we need to do here,
411:56 - let's make our way back to cloud formation.
And just take a look at the events. so here
411:57 - we can see the previous events, events analysis
create complete, a lot of times when you're
411:58 - waiting around here, you have to hit the refresh
button to see these changes, sometimes they'll
411:59 - give you a little blue pill telling you there
are available events. So you know, just don't
412:00 - wait around and wait for things to happen.
Click, click and see what's going on. So let's
412:01 - get a security group in here because that's
the next thing that we're going to need. So
412:02 - what we'll do is we'll make our way back to
cloud nine. And we're going to want to add
412:03 - a security group. So if we wanted a security
group, it should be under EC two, because
412:04 - it is a C two service and I typed in security
group here. There it is. And so this is going
412:05 - to tell us all the information we need to
know on setting up a security group. So let's
412:06 - go back here. And we're going to create a
new resource. And I'm going to call this one
412:07 - security group. And then it's going to be
type AWS EC two security group. And the properties
412:08 - are going to be we're going to need a VPC
ID, which if we need that, we're going to
412:09 - have to provide our own variable for that.
Right. We could just copy and paste ours in
412:10 - as we did with the image ID here. But I think
it's time for us to start making this less
412:11 - brittle. We can give it a group description,
which isn't a bad idea. So let's go ahead
412:12 - and do that. I'm going to say open port 80.
We want to set an Ingress rule that means
412:13 - inbound traffic. So you have Ingress in was
aggress. I know Ingress is inbound so and
412:14 - what we'll do is hit enter their IP protocol,
protobuf call, oops. And I'm going to type
412:15 - in TCP. And then we'll just align with the
IP there from Port 80. And then we have to
412:16 - Port 80. And then we have cider IP. Colon
will in this case, I'll do double quotations
412:17 - just because it has like weird characters.
I was thinking I have to do that here. It
412:18 - could be naked, I don't know. Um, forward
slash zero. And that says from anywhere on
412:19 - the internet, open up Port 80, which is the
common port for just plain old HTTP. I think
412:20 - that looks right. I'm just checking for spelling
mistakes. So yeah, we're gonna have to deal
412:21 - with this. Also, I think while we're here,
we might as well go ahead and turn this into
412:22 - a A variable, so I'm going to call that image
ID. And we're going to go and use some parameters.
412:23 - And the parameters is a great way for us to
pass in variables at creation of a stack,
412:24 - or an update of a stack as well. So we define
one called image ID, we will default that
412:25 - to the value we just had there, which I no
longer have. So I'm just gonna have to go
412:26 - to lunch easy to instance, here quickly, and
then grab it again. So that's a great way
412:27 - to default an option, when we know we want
something to be something. Alright, it's good
412:28 - to get these a description, because I'm going
to prompts us it's going to tell us what they
412:29 - are. So I'll go ahead and do that. I'm gonna
say, am I to use eg. I'll just tell them what
412:30 - format it has to be. And this is going to
be type string. Then the next one we need
412:31 - here is a VPC ID. And we will give this a
description and we'll just say the VPC used
412:32 - by the SG. And successful mistake there, we
will call this type string. And we're not
412:33 - going to default this one, we'll pass this
one in. And we're going to have to figure
412:34 - out what our VPC ID here in a moment, I'm
just double checking this to make sure everything
412:35 - is a Okay, looks good to me. But if we really
want to know whether our template is good,
412:36 - we can use a tool called cfn lint, which is
a node module. So I'm going to do, I'm gonna
412:37 - just go ahead and install that right now.
And this is going to check to see if our application
412:38 - is good hyphen g means it's going to install
it globally. And then what we'll do is I'm
412:39 - just going to CD into our directory, which
we called cfn. project here. And I'm just
412:40 - going to do cfn, lint, validate and then provide
the template name there. And if it's good,
412:41 - it will say it's good. If it's not, it'll
error out. And look, it's already complained.
412:42 - So clearly, I've done something wrong. I'm
not sure what the problem is, I think it's
412:43 - because I'm has an invalid type security group
resource security group hasn't developed type.
412:44 - So I've typed I've typed this wrong. It's
supposed to have two colons here. I'll go
412:45 - ahead and save that. And that will validate
it. I think we're getting closer at a sec
412:46 - to security group. VPC ID is not a valid property.
Okay, um, Oh, you know what? lowercase here?
412:47 - got to be really careful with these characters.
Image ID, it's the same problem here. I capitalize
412:48 - the D on the end of it. If I can find it in
here, image ID, do you see it? As if you're
412:49 - gonna tell me right. Okay, we'll save that.
And I'll just hit up here. And now the templates
412:50 - valid. So I mean, doesn't tell you exactly
what the problem is. I mean, you have to hunt
412:51 - it down there a bit and make sense of it.
And you'll will have to double check the names
412:52 - of these, these properties. But you know,
that's all there is to it. So what we'll do
412:53 - is we will go back here. And now that that's
done, I guess what we'll do is we will upload
412:54 - this to cloud or back to s3. But let's automate
that process, because it's going to be tiresome
412:55 - to always type in the commands over and over
again. So I'm gonna touch a new file and call
412:56 - this update.sh. So we're gonna make a bash
command here, we're gonna chmod it, which
412:57 - changes it so that it can be executable. So
if we didn't do that, and we tried to execute
412:58 - it, it's not actually a binary won't work,
or whatever you want to call, it just will
412:59 - not execute. Now that we have that file, I'm
gonna go ahead and open that up, and we are
413:00 - going to supply some information, we're going
to tell it where to look for bash, first of
413:01 - all, which is always a great idea, it should
already know what to do here. But this is
413:02 - just out of habit, I think you should always
do that. And then what we're going to do is
413:03 - we're gonna just type in our, our CI, or ADA
COI commands. And that's as simple as it is.
413:04 - So type in template type gamle, we'll do s3
colon slash slash, study sync. And I got three
413:05 - slashes in there. I'm sorry about that. It's
a bit hard to see what I'm doing. So I will
413:06 - type in Andrew B, Ford slash template dot
YAML. And that looks pretty good to me. We'll
413:07 - go back down here. And now we'll just do dot
forward slash update.sh. You could do like
413:08 - I think, sh or whatever or bash but I was
just like these the period. That's where you
413:09 - execute bash, bash files on Linux, we'll hit
Enter. And that should work. Yep, it gave
413:10 - us output it uploaded at some. So looks like
everything's great. So what we can do is we
413:11 - can go back to cloud formation and update
the stack. So I'm going to go ahead and do
413:12 - update, we're going to replace the current
template and we're going to provide that s3
413:13 - URL again, I'm gonna make my way over to CloudFront
or s3, because I cannot remember that URL.
413:14 - I'm gonna paste that on in there. We'll hit
next and It's now going to ask us for that
413:15 - VPC ID. So what we need to do is, go grab
it. So I think we have our easy to open here.
413:16 - But we will, an easier way to do this would
be to just go back to stacks. Here, click
413:17 - into this one, and we've checked the resources,
we can click into the actual instance we're
413:18 - using and get the exact VPC that it's using.
Because it could be if you have multiple v
413:19 - PCs, it could be in a different one, it should
be the default one, because we didn't specify
413:20 - anything. So we're going to just find it,
and there it is, I'm just gonna hit that little
413:21 - Copy button, we'll make our way back to cloudformation.
Here, paste that in, fingers crossed, and
413:22 - I think this will work. Nothing new here that
we need to choose, we're gonna hit Next, move
413:23 - all the way down the bottom, hit update stack.
And there it goes. And so we're just gonna
413:24 - have to wait a little while here hopefully
does not roll back on us. But since we lifted
413:25 - fine, the only thing that could mess up is
we've got those parameters wrong. So Oh, I
413:26 - think it's already done. I think it was really,
really fast, because it didn't have to create
413:27 - a new ECU instance, it's just adding a security
group. So let's go to our resources. And now
413:28 - we can see we have a security group and a
web server, it's going to close that tab there.
413:29 - We'll leave that open. We'll close that. And
we already have it open, we'll close that.
413:30 - So what I'm going to do is check out the security
group, I just want to see if it actually set
413:31 - the Port 80 to be open. If we go here to inbound
Port 80 is open. That is what we want. Now
413:32 - the question is if we check our web server
is a security group associated with it? And
413:33 - the answer is, the answer is no, it's on the
default, because we actually never associated
413:34 - it in our cloud nine template. So in here,
yeah, we have this we have that we didn't
413:35 - put a property here for the security group.
So that's what we're going to need to do next.
413:36 - Because we want to open up that Port 80. When
we install our Apache web server to view a
413:37 - web application, so we're going to do is we
are going to go ahead and add that property.
413:38 - And I'm just stalling for time, as I look
for the line that I have to type in there
413:39 - it is. So what we need to do is add security
group security. Group IDs wouldn't be so nice
413:40 - if this auto auto completed. For me, it's
like if it actually auto completed all the
413:41 - documentation, that'd be sweet. And what we're
going to use is get att, we're going to type
413:42 - in the security group here, security group,
dot group ID. And so this, get a tribs is
413:43 - a way of getting a special properties on a
resource here, and you're limited to what
413:44 - you can grab. But here we can grab the group
ID and that's what we need for this case.
413:45 - So I think that's all we need to do here right
now. So let's go ahead and actually update
413:46 - this our, our, our stack, and instead of going
through here and click on that and do all
413:47 - that work. Let's update our script here so
that we can just automate this further. So
413:48 - I'm gonna go here and type in AWS cloudformation.
And it's going to then be update stack, we
413:49 - are going to provide our region which is a
great idea, we don't want that to default
413:50 - to anything funny, put a forward slash there,
we'll provide the stack name, which should
413:51 - be called study sync. We will provide the
template URL which will be this link here.
413:52 - We are going to need to provide those parameters.
Oops, parameters. And we are going to supply
413:53 - the parameter key is parameter key and then
parameter value very verbose. They just don't
413:54 - want to make anything easy for us v PC ID,
we want it to be lowercase. And then we'll
413:55 - do parameter it's going to be value. And that's
going to have to be what our VPC ideas. So
413:56 - we'll go back here and we will grab it. There
it is. So we'll grab it from there. And we
413:57 - will paste that in there. That all looks good
to me. I'm just going to double check more
413:58 - thoroughly here. It's very easy to make a
mistake when you're typing all this stuff,
413:59 - but I'm pretty sure that's good. And so now
that our script is updated, we can just do
414:00 - update.sh Fingers crossed hopefully works
and we have an error oh geez. So the template
414:01 - URL or template body must be specified so
we have it right there but we forgot that
414:02 - Ford slash this Ford slash just allows us
are backslash backslash leans to the left
414:03 - Ford slash leans to the right. But this allows
us to make this multi line we can make this
414:04 - all a single line but that's really messy.
So that should fix that issue. We'll hit up
414:05 - here. Oh boy VPC, Id must have values. I guess
I named it this. Yeah, I did. And I guess
414:06 - I could lowercase this because we should really
be Consistent here, then I'll have to go down
414:07 - here and do that as well. Just to make our
lives a bit easier, again, that's a little
414:08 - mistake on my part. And actually, before we
update this, we should really be linting.
414:09 - This. So we'll do a cfn, lint validate. template
dot YAML, everything's good. I'm gonna just
414:10 - change this to lowercase here. Everything
looks good, we'll hit up. We're still having
414:11 - a hard time security group IDs, security group
IDs. So where did the linter didn't pick that
414:12 - up? invalid template property resource properties,
security group IDs. So I'm just going to search
414:13 - that their security group IDs, Oh, you know
what, it's got to be indented. My bad. And
414:14 - it's surprising the linter didn't pick that
up. So maybe there's another tool out there
414:15 - that's a little bit better at checking that
stuff. And there we go, we know it's created
414:16 - because it gave us back a stack ID, we go
back to our cloudformation template and give
414:17 - it a refresh, we can see, we can see that
it is updating. If we want to see that status,
414:18 - actually, in cloud nine, we can do it programmatically
by typing AWS. cloudformation, describes stacks,
414:19 - stack hyphen, name, and study sync. And I
would probably I'll put this as a table because
414:20 - it's a lot information doesn't exist, I typed
the wrong. And so there it is, there's all
414:21 - our information, could also get this back
as JSON. If we just took that off there. It's
414:22 - showing us the status of it. So it's saying
update in progress. So it's kind of like a
414:23 - cool way of taking that up. So update complete,
so it's almost done. And we can also monitor
414:24 - it through here. Got to choose what you want
to do. So let's delete and progress, I guess,
414:25 - is deleting the web server. And if it's deleting
the web server, that means we have to wait
414:26 - a little bit here. So I will see you back
here when this is 100%. Done. Okay, and so
414:27 - after a little weight here, it everything's
updated, now deleted the web server. And when
414:28 - I was building out this fall long, and I got
to the stage, it didn't actually delete it.
414:29 - So clearly, I have made a change. And maybe
it's because I was fiddling with the names
414:30 - of the parameters, there are properties that
it forced a delete. So that's the thing with
414:31 - cloud formation, it doesn't always delete
resources. But in this case, it did decide
414:32 - to wait a little bit longer here. If we go
over to EC two, we can see now that it's running.
414:33 - And let's say we wanted to actually go check
out whether the security group is the correct
414:34 - one. And it is, let's say we wanted to do
this through cloud nine, or I should say the
414:35 - CLR here. And what we could do is get a list
of the cloudformation resources. So I'll do
414:36 - ABS cloudformation, list stack resources,
stack name, study sync. And here, what we're
414:37 - getting is a list of resources. And in there,
we should have the security group. And that's
414:38 - going to give us that SG here, which is what
we're going to need to check the next thing
414:39 - and so we'll use AWS EC to describe instances.
instance IDs. And we will provide Actually,
414:40 - we don't need the SG we need the the instance
because once you have the instance, has a
414:41 - security group on it. That's what we're typing
this out for. And I'm going to just output
414:42 - This is a table kind of notice and convention
here. Describe stacks describe instances.
414:43 - That's why it's great to work with a COI for
a while, you just start to figure things out
414:44 - as you go. But you got to type in right if
you want to work. So we'll go back here, hit
414:45 - Enter. And there it is. And so we just looked
through this, there should be one called security
414:46 - groups. So we have the instance itself. Script
security group, where are you? There it is,
414:47 - and so we can see that it's attached. So there
you go. So we definitely know that it's correctly
414:48 - attached. But if we just want to double check
ourselves and go here and check the inbound
414:49 - rule, and we see that Port 80 is open. So
now we have a web app with Port 80 open, which
414:50 - is great. Now we just need to install our
web server. And the way we can do that is
414:51 - through user data. Now, if you're launching
an EC two instance, you're just going through
414:52 - the steps here like I am, you get to this
part advanced details and you provide the
414:53 - scripting to the user data and that would
initially set up whatever you want. That is
414:54 - one way to get that set up. And that's what
we're going to do using cloud formation. So
414:55 - we'll go back to our template here. I'm going
to go to our template dot YAML. And I'm just
414:56 - going to scroll on down here so I can see
what I'm doing. And we're going to add another
414:57 - property to the web server and we're going
to call this one user data. And then we're
414:58 - going to add a function called base 64. Because
this has to be base 64. That's just how it
414:59 - is and then we will use a sub on that because
we need to start Institute, well, we don't
415:00 - really need to substitute in the values. But
in case we do, we should just do that out
415:01 - of habit. And this is the developer associate.
So you don't need to know all these things
415:02 - in great detail. It's more so at the assess
option and the DevOps. So that's why I'm kind
415:03 - of glazing over these things. Because I'd
rather you get more practical experience as
415:04 - to whether to actually know exactly how they
work. And then we will do EC to user because
415:05 - it always starts you as the root user, and
you got to force it to switch to EC to user,
415:06 - then we'll do sudo Yum, install httpd that
is Apache. You think they just call it a potty
415:07 - but they don't. And then we'll do start which
will start the service. And then we'll do
415:08 - sudo service HTTP enable said if that service
is stopped, for whatever reason, it will when
415:09 - the if the server's restarted, it will continue
running. I'm going to double check this to
415:10 - make sure this is right. User bin Mr. Bash,
looks good to me. I got that right. Young
415:11 - y update. Good. es su ECC user, great sudo
yum install. That looks good to me. So everything
415:12 - is great. So that's all we need to do there.
And while we're here, we might as well add
415:13 - some outputs. outputs are going to make it
easy for us to find value. So we don't have
415:14 - to click around everywhere and make an output
called public IP, because I want to get the,
415:15 - the IP of the web server, if you don't mind.
And I believe that is typed correctly, I'm
415:16 - just gonna double check here. Yeah, it looks
good to me. So what we'll do now, I'm just
415:17 - gonna scroll all the way up here. And what
I'm going to do is I'm just going to cfn,
415:18 - or we're going to lint that I'm just gonna
hit up till I find that linter says everything
415:19 - is good. But we found out earlier, that's
not always the case. And then we'll do our
415:20 - update.sh. Fingers crossed. Yep, that's great.
So now, that has been uploaded to s3, and
415:21 - it's doing a stack update, we're gonna make
our way back here to cloud formation, I'm
415:22 - going to do refresh. And we can see this update
is in progress. So we're gonna have to wait
415:23 - on that. And what we're looking for is that
output of the actual IP address, I don't know
415:24 - if it's going to replace the server. in this,
in this case, it probably won't. If it did,
415:25 - that'd be great. But he says update in progress.
So it's not doing it's not doing a delete
415:26 - in progress for the web server. But anyway,
we'll we'll check back here in a moment. So
415:27 - let's see you soon. So our update is done
here. And notice that it didn't do a delete
415:28 - in progress, it just updated it. So that's
interesting. If we go to output here, now
415:29 - we actually have that IP address, that's gonna
make our lives a lot easier, because now we
415:30 - can just place that in there, but it's not
working. So that's a bit of a shame. But I
415:31 - actually know why it's not working. It's because
it didn't replace the the instance it didn't
415:32 - delete and recreate it, it just updated it
in place. And the reason why I know that is
415:33 - the reason that is the core reason why this
is not working is that when you use user data,
415:34 - this script only runs the absolutely first
time an instance has been launched. And so
415:35 - the only way for this to ever run again would
be if you destroyed it and recreate that instance
415:36 - entirely. If you restarted it, it wouldn't
run it again. There are ways of adding things
415:37 - here so that it will trigger the trigger restart,
I guess, in some cases, or like replace the
415:38 - actual instance. But it didn't. So what we're
gonna have to do here in this case is just
415:39 - delete our entire stack. And that's a great
chance to see how deletions work. So we'll,
415:40 - what we'll do is go ahead and delete it, hit
Delete stack. And what it's going to do, might
415:41 - have to go back a step here, it says delete
in progress, it's better to see it on this
415:42 - level, we're gonna see delete in progress,
and deletes can actually fail and then roll
415:43 - back. But I don't think this one is going
to. So what we'll do is we'll just wait a
415:44 - little while until this is done. And then
we'll recreate our stack. And actually, what
415:45 - we'll do while we're waiting here is we'll
go to our update script. And we'll change
415:46 - this from update to create stack. And we're
not going to run this until this one is done.
415:47 - Because you can't have a stack with the same
name. We could just name our new stack a different
415:48 - name, but I want to keep this consistent.
Oh, it looks like it's done. So that was pretty
415:49 - darn quick. So that means I don't have to
go away. And we can just move on to the next
415:50 - step here. So we change this with create stack,
and we will just do an update. And that is
415:51 - going to create our new stack here. We'll
do a refresh. There it goes. And we'll go
415:52 - to events. We'll just hit refresh here. And
we'll have to wait a little bit because it's
415:53 - launching ecgs. And so I'll see you that.
Okay, and so we're back. I don't know if we're
415:54 - gonna have the same IP address here. See this
one's 54 227 8467 Nope, it's different. But
415:55 - we'll give that a go. And we'll see now if
it actually works. And look at that we got
415:56 - the test page up. So we fully automated something
using cloud formation. So that is pretty much
415:57 - the scope of what I have. wanted to show you
here. And all we need to do now is just delete
415:58 - what we've created. So what we'll do is we'll
do through the CLR, because we already did
415:59 - it through here. So might as well learn how
to do see ally. I don't have to take a guess.
416:00 - But I'm pretty sure it's a realist cloud formation,
cloud formation, delete, stack, and then stack
416:01 - name. Study sank, I tell you, the more use
the COI, you just start guessing and you're
416:02 - pretty much right. So I didn't get any output
there. I'm going to go double check here.
416:03 - And I'm going to refresh, see what's going
on, since the Delete is in progress. And there
416:04 - we go. So just make sure that this deletes
because sometimes sometimes deletes fail,
416:05 - and they roll back and you still have those
resources around there. So if you don't want
416:06 - to be paying for something you don't need,
just double check that there. But yeah, that's
416:07 - cloud formation for you. So we're all done
here. So we are on to the cloudformation cheat
416:08 - sheet. This is a long one. It's like three
pages long. super important for the SIS ops
416:09 - and developer associate. So we need to know
this inside and out. So let's get into it.
416:10 - When being asked to automate the provisioning
of resources, think cloud formation when infrastructure
416:11 - as code is is mentioned, think cloud formation,
cloud formation can be written in either JSON
416:12 - or YAML. When cloud formation encourages,
encounters an error it will roll back with
416:13 - rollback and progress, cloud formation templates,
larger than 51,000 51,200 bytes, or 0.5. megabytes
416:14 - or too large to upload directly, and must
be imported into cloudformation via an s3
416:15 - bucket. nested stacks help you break up cloud
formation templates into smaller reusable
416:16 - templates that can be composed into larger
templates, at least one resource under Resources
416:17 - must be defined for a cloudformation template
to be valid. I'm going to repeat that because
416:18 - it's so darn important in a second here. And
let's talk about the the template sections.
416:19 - And you definitely need to know these for
the exam, because it'll give you the questions
416:20 - that you have to pick out which section does
what. So we have metadata that that is extra
416:21 - information about your template, the description
that tells you what your template does, you
416:22 - have parameters, this is how you get user
input into the template. Transform applies
416:23 - macros, this is like applying a mod which
can change the the anatomy to be custom. And
416:24 - a good example that is Sam, the serverless
application model. That's what stands for,
416:25 - then you have outputs. These are values you
can use to import into other stacks, you have
416:26 - mappings, these map keys to values just like
a lookup table resources to find the resources
416:27 - you want to provision. And here, I'm going
to repeat it again, at least one resource
416:28 - is required conditions are whether resources
are created or properties are assigned. And
416:29 - those are all the properties. So again, make
sure you know are the sections, make sure
416:30 - you know them inside and out because it'll
earn you a few points on the exam. onto the
416:31 - second page stack updates can be performed
two different ways we have direct updates.
416:32 - And so the way those work, you directly update
the stack, you submit changes, and it was
416:33 - cloudformation really deploys on this is what
you're going to be used to doing when you're
416:34 - using cloudformation. The other way is executing
change sets. So you can preview the change
416:35 - set to cloud formation. Or sorry, you can
preview the changes to cloud formation will
416:36 - make to your stack. And that is what we call
a change set. It's just telling you what's
416:37 - going to change, then decide whether you want
to apply those changes. So it's just like
416:38 - a review process. And then stack updates will
change the state of resources based on circumstances.
416:39 - So we have a few we need to know them. So
update with no interruption, so updates the
416:40 - resources without disrupting operation and
without changing the resources physical ID,
416:41 - then you have updates with some interruptions,
updates resources with resource with some
416:42 - interruption retains the physical ID replacement
recreates the resource during an update also
416:43 - generates a new physical ID, you can use a
stack policy to prevent stack updates on resources
416:44 - to prevent data loss or interruption to serve
services. We have drift detection, this is
416:45 - a feature that lets cloudformation tell you
when your expected configuration has changed
416:46 - due due to manual override an example of this,
let's say you have a cloud formation template
416:47 - or stack that creates a security group and
a bunch of other resources. Adult developer
416:48 - comes in and deletes that as G. So cloudformation
would think that it's still there, even though
416:49 - it's not in drift detection. If you turn that
feature on it's going to tell you that something
416:50 - that it's it's no longer the case. And then
on to the last page and this is a long one,
416:51 - we're gonna talk about rollback so occurs
when a cloud formation encounters an error
416:52 - when you create an update or destroy a stack.
When a rollback is in progress, you'll see
416:53 - rollback and progress. When a rollback succeeds,
you'll see update rollback complete rollback
416:54 - fails you'll see update rollback failed then
you have sudo parameters are defined parameters.
416:55 - So here we have a ref a double region which
would return US East one. So these are like
416:56 - eight of his predefined parameters. Then we
have resource attributes. So resource attributes
416:57 - we have a lot of different policies under
here we have creation policy. prevents its
416:58 - status from reaching create complete until
Ava's confirmation receives a specified number
416:59 - of success signals or the timeout period is
exceeded. deletion policy reserve or in some
417:00 - cases backup a resource when it stack is deleted,
retain or snapshot update policy how to handle
417:01 - an update for an ASP elastic cache domain
or lambda alias. An update replace policy
417:02 - to retain or in some cases backup the existing
physical instance for resource when it's replaced
417:03 - during a stack update operation. So we have
delete, retain, or snapshot. And then we have
417:04 - depends on the resource is created only after
the creation of resource specified in the
417:05 - depends on attribute. And there's some cases
where you use depends on there's there's other
417:06 - cases where you use a weight condition. So
we have intrinsic functions allow you to assign
417:07 - properties that are not available during runtime,
most important to to know would be the ref
417:08 - one returns the values of a specified parameter
or resource, get a trip returns the value
417:09 - of an attribute from a resource in the template.
And then just some CLR commands you should
417:10 - know you know, the Create stack command because
sometimes they'll have a question and they'll
417:11 - actually show you some COI commands. And it's
good to know what's what, then you have the
417:12 - update stack one. And the last thing I want
to just talk about a service application model
417:13 - is an extension of the cloudformation is cloudformation.
To let you define service application, it
417:14 - doesn't get its own cheat sheet because there's
just not enough information on it. There's
417:15 - a lot more stuff on cloud formation. But like
your if this is for the associates, this is
417:16 - good enough. If you're going for the developer,
the DevOps Pro, this is like a six page cheat
417:17 - sheet. So there you go. You know, hopefully
this helps you on exam day. Hey, this is Andrew
417:18 - Brown from exam Pro, we're looking at Cloud
development kit, also known as cDk, which
417:19 - is a way to write infrastructure as code using
an imperative paradigm with your favorite
417:20 - language. So let's get into it. Alright, so
to understand cDk, I want to talk about transpilers
417:21 - here for a moment. So a transpiler turns one
source code into another, and so cDk transpiled
417:22 - into cloudformation templates. So you know,
just a simple diagram, we have a cDk on the
417:23 - left, and that turns into cloud formation
templates under the hood. And so this is the
417:24 - difference between an imperative infrastructure
and a declarative infrastructure. So let's
417:25 - talk about these two differences. So imperative
is when you have something that's implicit,
417:26 - you know, what resources will be created in
the end state. And this allows for more flexibility,
417:27 - less, you have less certainty because you
don't know exactly what's going to be created.
417:28 - You don't, you don't have fully controller
visibility on it. But you generally know what
417:29 - is going to happen. But you get to write less
code. And so an example of something being
417:30 - imperative is saying, you know, I want an
EC two instance. But you go and fill in all
417:31 - the other details, I just want to tell you
that I want to have one, I don't want to have
417:32 - to worry about everything else. And so that
is what cDk is it's imperative that when we
417:33 - were looking at decorative on the right hand
side here, it's explicit, we know what resources
417:34 - will be created in the end state, there's
less flexibility, we're very, very certain
417:35 - of every single little thing that's going
to happen at but we have to write a lot more
417:36 - code. And so comparative example to imperative
is I want an easy to instance. And I have
417:37 - to tell you exactly every detail of it. And
that is what cloudformation is it's declarative
417:38 - by nature. So I said earlier, you could use
your favorite language using cDk. And so let's
417:39 - talk about some of the language support it
has. So cDk was first available only using
417:40 - TypeScript. And then they eventually started
releasing for other languages. So we have
417:41 - node, TypeScript, which again, is just node,
Python, Java, and ASP dotnet. So that's all
417:42 - we have. So far, if you're wondering exactly
what versions, that is what it supports. I'm
417:43 - still waiting for a Ruby version. And hopefully,
you know, when you're watching this video,
417:44 - a Ruby version becomes available. But generally,
I think whatever languages is supported by
417:45 - AWS generally is what we'll see. So I would
not be surprised if they do PHP, one, and
417:46 - also a Ruby one here. But I don't think you'll
get one in PowerShell. Just I want to make
417:47 - a note about how up to date. cDk is with cloud
formation. So the cDk API, they may have yet
417:48 - to implement specific API's for eatables resources
that are available on cloud formation. It's
417:49 - just because it takes time for them to write
this stuff. And they have a lot of languages
417:50 - to support. But it's my best guess that TypeScript
would be the one that supports the most AWS
417:51 - resources. And then the other ones would follow
behind. I would think Python would be next
417:52 - and then Java and then probably ASP. NET would
be last. But just consider that in mind. So
417:53 - that is one of the things you have to think
about with cDk which is if you need full control
417:54 - of what cloud front offers, you might have
to just use cloudformation templates. So you
417:55 - have to explore there and see what you can
do. Hey to Sandra brown from exam Pro, and
417:56 - we are looking at serverless application model,
also known as Sam. And this is an extension
417:57 - of cloudformation that lets you define serverless
applications. And I always like to think of
417:58 - it as just as a cloudformation. Macro reuse
transforms. So let's get to it. So Sam is
417:59 - both a AWS COI tool and also a cloudformation
macro, which makes it effortless to define
418:00 - and deploy serverless applications. And you
might be looking at that word macro and thinking,
418:01 - what does it mean? So this is the textbook
definition, which is a replacement output
418:02 - sequence according to a defined procedure,
the mapping process that instantiates a macro
418:03 - use into a specific sequence is known as a
macro expansion. Now, that doesn't make a
418:04 - whole lot of sense, at least to me. So I've
reworked that definition. And so I would say
418:05 - a macro allows you to change the rules on
how code works, allowing you to embed a language
418:06 - within a language macro serve to make code
more human readable, or allow you to write
418:07 - less code, Korean language with another language
is called a DSL a domain specific language.
418:08 - And by using macros, you're creating a DSL.
And so cloudformation allows you to specify
418:09 - macros through the transform attribute. And
this is how Sam is use. So what you do is
418:10 - you would use transform and then specify AWS
serverless. And then you have access to now
418:11 - these new resource types. That's what this
this macros, Sam is injecting into it. So
418:12 - you can define this function as API and simple
table along with a bunch of other properties.
418:13 - So to really understand the value of Sam,
it's great to put it against cloudformation
418:14 - in a one on one comparison. So what I've done
is set up a an example here. So we have API
418:15 - gateway, calling a lambda and that lambda
gets data from a database such as RDS. So
418:16 - on the left hand side, this is what it would
look like if you wrote it in pure cloud formation.
418:17 - So that's without Sam, and we're 100 lines
in and on the right hand side, we have it
418:18 - with Sam. And so this is 5050 lines in so
you have at least a 30% reduction in code
418:19 - when writing in SAM. And this is a more verbose
example. I think in most cases, you could
418:20 - see 72 to 80% reduction in code just for when
you're writing the serverless components.
418:21 - So you can see that Sam is going to save you
a lot of time. So let's talk about the SAM
418:22 - COI because I said at the beginning of this
Sam section that Sam is both a sea ally and
418:23 - a cloudformation macro. So the SAM COI makes
it easy to run packages, deploy serverless
418:24 - applications or lambdas. And these are its
COI commands, I don't think you need to learn
418:25 - them all. But it's good to go through them
and get some hands on with this kind of stuff.
418:26 - So starting at the top, we have Sam build,
and that prepares the lambdas sourced in the
418:27 - code to be deployed by patching for upload.
So it doesn't upload it, it just packages
418:28 - it into an artifact. The next one is Sam deploys.
So that uploads the lamda package code as
418:29 - an artifact and then deploy. If you're wondering
what an artifact is, it's just a fancy word
418:30 - for a zip file. And then the next one here
is Sam a net. So if you have yet to start
418:31 - a project, you could run this, it's going
to give you a bunch of default folders and
418:32 - files. And it's going to be set up for for
a serverless project, I would think that this
418:33 - would be setup for the serverless application
repository. So whatever default files you
418:34 - need there, but it would set it up for you,
then you have generate event. I think this
418:35 - is for testing, I've never used it. But if
you get different payloads for your event
418:36 - sources, then you have invoke, which runs
a single lambda locally, then you have start
418:37 - API that runs your serverless application
locally for quick development, then you have
418:38 - start lambda, which is very similar to invoke.
But looking at the two I would probably say
418:39 - you'd more likely want to run start lambda
than invoke. So that is another one there,
418:40 - then you'd have logs which fetches, I would
think from Cloud trail would fetch the logs
418:41 - for that lambda function. So you can see it
locally without having to log into the console.
418:42 - And the last three really have to do with
the serverless application repository patching
418:43 - a service application. So as packages for
patching a service application, so creates
418:44 - a zip and uploads it. It seems like it kind
of does build and deploy in one go. But again,
418:45 - for service application, then you have published
so that's going to publish it to the the actual
418:46 - service application repository. Then you have
validate, which just checks I think syntax
418:47 - errors for your Sam templates. So there you
go. That's the big rundown there. Hey, this
418:48 - is a Andrew Brown from exam Pro. And we're
going to be looking at ci CD, which is the
418:49 - automated methodologies that prepare, test
deliver or deploy code onto a production server.
418:50 - Hey, this is Andrew Brown from exam Pro. And
we are looking at the CI CD models. And before
418:51 - we jump into those models, I want to just
talk about a couple terminologies if you're
418:52 - not familiar with them, which is production
and staging production, short for prod, is
418:53 - the live server where real pain users are
using the platform. And staging is a private
418:54 - server, where developers do a final Manual
Test as a customer, which we call QA. And
418:55 - that is short for quality assurance. And they
do this before deploying the code to production.
418:56 - So it's like that last check before there.
So if you see staging and production and wondering
418:57 - what those mean, those are those terms. So
this is our pipeline from code to deploy.
418:58 - And I'm going to show you three different
models. And if you look online about ci CD,
418:59 - you might see some additional steps in here.
So there is a bit of flexibility in terms
419:00 - of like how these are defined. But this is
what I'm gonna show you and it pretty much
419:01 - is what it is. So the first one we're going
to look at is continuous integration, which
419:02 - is short for ci. And that's automatically,
that's automatically reviews the developers
419:03 - code. The next one is continuous delivery
that's automatically preparing developers
419:04 - code for release to production. And the last
one is continuous deployment, which is automatically
419:05 - deploying code as soon as developers push
code. And if all tests pass, you're going
419:06 - to notice that both of these have the same
initialism, which is a bit confusing, but
419:07 - that's just how it is. So if someone says
CD to you, you've got to get clarification
419:08 - because it could mean delivery, or deployment.
So let's move into these individually and
419:09 - learn more about continuous integration is
first on our list here. And so this is the
419:10 - practice of automating the integration of
code changes from multiple contributors into
419:11 - a single software project. So our pipeline
is going to be code, build, integrate, and
419:12 - test. And this, the reason we do this is it
encourages small changes more frequently.
419:13 - And each commit triggers a build during which
tests are run that help to identify if anything
419:14 - was broken by the changes. So it's somebody
checking over the developers code as they're
419:15 - coding, which is going to definitely speed
up productivity with your team. So here is
419:16 - an example down below. So we have a developer
and they push a new feature to GitHub in a
419:17 - new branch. And what they would normally do
on GitHub as trigger a web hook to something
419:18 - like circle ci, and then circle ci would go
create a new build server, that build server
419:19 - would pull in other developers code, run the
test suite, and then it would report back
419:20 - the results. And the results would give us
test coverage saying how much test code has
419:21 - been written to written to cover the code.
And the other part is, did any tests fail.
419:22 - And so that is generally what most people
are familiar outside of AWS. But if we're
419:23 - using AWS, we can just replace these things.
So we could replace GitHub with code commit,
419:24 - we could replace that what web hook with lambda,
we could replace circle ci with code build,
419:25 - and the results would go into s3, and it'd
be called an artifact, which is a fancy word
419:26 - for a zip, and code pipeline. So you know,
that's continuous integration. So let's move
419:27 - on to the next one. So now we're going to
look at continuous delivery. And this is the
419:28 - practice of automating the preparation of
code to be released to production or staging
419:29 - branches, you're preparing the code base for
deployment, deployment is still a manual process.
419:30 - So in our pipeline, now we have code build,
integrate, test and release with a strong
419:31 - focus on the last one release. So here, I
have an example. And you're going to notice
419:32 - that half of it, we've already seen in the
previous example. But in this case, we're
419:33 - now using all AWS services. So we have code
commit the lambda function, code build, and
419:34 - stuff like that. And so that was the continuous
integration part. So now let's look at the
419:35 - continuous delivery. So we have that s3 artifact,
which tells us that the test coverage is good.
419:36 - So in some projects, you know, code can't
be accepted unless you write a certain amount
419:37 - of test code. So whatever that threshold is,
is defined per project could be 30%, could
419:38 - be 70%. And then all the tests pass, so that's
great. And so we push that to a lambda, and
419:39 - that lambda is going to check and say, okay,
all the all the tests and everything is good.
419:40 - So go ahead and make a pull request, it creates
a pull request, and code commit. And now that
419:41 - is left for the developers to review. So the
developers are going to check over that code.
419:42 - And if they're happy with it, there are three
of them are going to all vote on it. And if
419:43 - there's a consensus that one person is going
to decide to release that code, and so they
419:44 - say approved, and then that that code gets
some merged into master. And so master is
419:45 - generally the production server, or it could
be a production branch. But whichever master
419:46 - production say, usually the same thing. And
so that doesn't mean it's deployed, but it's
419:47 - ready to be deployed. So we're on our last
model here, which is continuous deployment.
419:48 - And this is the same as continuous delivery,
but it automatically deploys changes to production.
419:49 - And so there's our pipeline, it's blue all
the way across. And here is our technical
419:50 - architecture. And looks very similar to the
last one. But we have a little bit on the
419:51 - end here. So we know continuous integration,
we know continuous delivery. And now we're
419:52 - looking at continuous deployment. So the last
thing we saw in continuous delivery was the
419:53 - feature branch being merged into master. So
all the code was ready to be deployed. And
419:54 - so this is where clews deployment comes in.
So something would check to see if something
419:55 - changed in the source, it would be monitoring
code, commit, or GitHub. And as soon as it
419:56 - was merged into it, it would then trigger
code pipeline to start doing things. And in
419:57 - code pipeline, you would define something
such as code deploy. So the source would then
419:58 - get checked out, maybe go to code, build and
run the test one final time. And that those
419:59 - pass that pass on to code deploy, and then
code deploy would start that process of deployment.
420:00 - And so that is the whole pipeline there. Hey,
this is Andrew Brown from exam Pro, and we
420:01 - are at the end of the C ICD section. So let's
do a quick review with the C ICD cheat sheet.
420:02 - So C ICD is automated methodology that prepares
tests delivers or deploys code onto servers,
420:03 - and generally beat production servers, which
can be abbreviated to prod and environment,
420:04 - which is intended to be used by paying users.
It's the it's your, your live server. Then
420:05 - you have your staging server, your staging
environment, which is intended to simulate
420:06 - a production environment for last stage debugging.
That's why they call it a staging server,
420:07 - then you have continuous integration represented
a CI and this is automating the review of
420:08 - developers code making sure their code is
in good shape before we allow it to be turned
420:09 - into pull requests, or just to speed up our
development cycle. So run tests with a build
420:10 - server. So in this case, we use code build,
and that's how that would work, then you have
420:11 - continuous delivery. So this is automating
the preparation of the developers code for
420:12 - release. So it's not being deployed. It's
just one step before that. So an example here
420:13 - is very similar to last one, except you'd
run the test suite with a build server at
420:14 - the test pass, we, it would automatically
create a pull request or merge branch into
420:15 - staging because sometimes staging is a precursor
to production, you know, so it's saying we're
420:16 - ready to deploy this code, but you need to
check it over before before doing so thank
420:17 - goodness deployment takes that a step further,
also abbreviated to CD. And the idea here
420:18 - is it's automatically deploying developers
code with which, which is ready for release.
420:19 - So it's all the steps prior, so it's going
to run test suite built with a build server.
420:20 - And here, if the tests pass, then it's just
going to immediately merge it into production
420:21 - and deploy it. So it just does everything
automated and ends. And the last thing I just
420:22 - want to touch on canoes deployment here is
that it can refer to the entire pipeline.
420:23 - So when you do continuous deployment on AWS,
you should be thinking of code, pipeline,
420:24 - code, commit, code, build, and code deploy
all combined. So there you go. Hey, this is
420:25 - Andrew Brown from exam Pro. And we are looking
at code commit, which is a fully managed source
420:26 - control service that hosts secure Git based
repositories. And I like to think of it as
420:27 - the GitHub for AWS. So to understand code
commit, we need to know what a version control
420:28 - system is. And that is a system that records
changes to a file or set of files over time,
420:29 - so that you can recall specific versions later,
a very famous story about a codebase not having
420:30 - a version control system, I think, is either
Doom or Wolfenstein. I can't remember which
420:31 - one. But the idea was that they had a bunch
of people working on computers to program
420:32 - that game back in the early 90s. And since
there was no control version control system,
420:33 - or they weren't using one, they had to move
code around on, on floppies. And, you know,
420:34 - it was very difficult to manage all that code.
And also, you know, if you had one computer
420:35 - that had that source code, if anything happened,
that computer, all your code was gone. So
420:36 - this is what version control systems alleviate.
And so in 1990, we had CVS which stands for
420:37 - I think, control version system, not a very
creative name, but very clear as to what it
420:38 - does. Then you had some version, this is where
I started out using sbn. And then in 2005,
420:39 - we had a renaissance for version control systems
with Mercurial and also get which you might
420:40 - be familiar with, and Git is actually the
most popular version control system and there's
420:41 - good reason for it. It's a distributed version
control system. And its goals, which it does
420:42 - definitely deliver on is speed, data integrity
and support for distributed nonlinear workflows.
420:43 - So because of, you know all those features
of git, that's why that is the primary one.
420:44 - And so any version control system that is
generally what it is using. So what is code
420:45 - commit, then so code commit is a service,
which lets you store your Git repositories
420:46 - in the cloud. So developers can push and pull
code from the cloud repository, and it has
420:47 - tools to resolve conflicts. And so you just
go ahead and create your repository. And then
420:48 - you can download your code from there, upload
and etc. And if you've ever used something
420:49 - such as GitHub, Bitbucket or Git lab, this
is the same thing. But this is just like AWS,
420:50 - a solution for hosting repositories, but with
some special features. So why would you want
420:51 - to use code commit? So let's talk about the
key features. And so the first one, which
420:52 - I think is a really strong point, is that
is in scope with a lot of compliance programs.
420:53 - So one compliance program is HIPAA. And so
this might be an advantage it has over other
420:54 - competitors, or it might be more cost effective
to get this compliancy. Whereas like, maybe
420:55 - with GitHub, you have to pay for enterprise
support, it's very expensive, we're on AWS,
420:56 - it's very inexpensive. repositories, repositories
are encrypted at rest, as well as in transit.
420:57 - So security is a definitely a key feature.
It can handle repositories with large numbers
420:58 - of files, branches, large file sizes, and
lengthy revision histories, though I've never
420:59 - felt any kind of limitations on other platforms.
So I kind of feel like they're, they all do
421:00 - this. But you know, it's great that they state
that there's no limit on the size of your
421:01 - repositories or the file types you can store.
You keep your repositories close to your other
421:02 - production resources Database Cloud. Now,
that, to me is the largest value is that code
421:03 - commit has a lot of synergies with a lot of
AWS services. And the benefit there is going
421:04 - to help increase the speed and frequency of
your development life cycles. And also, to
421:05 - be able to code in some creative automation.
And to control access to code commit, you're
421:06 - going to use Iam. And that's going to say,
hey, these users are allowed to have access
421:07 - to this repository. Hey, this is Andrew Brown
from exam Pro. And we are looking at Docker,
421:08 - which is a third party tool designed to make
it easier to create, deploy and run apps by
421:09 - using containers. to best understand containers,
we should compare them against virtual machines,
421:10 - which is what we're used to using. So imagine
we launched an EC two instance. And anytime
421:11 - you launch a new instance, it's likely going
to launch you a virtual machine. And on that
421:12 - EC two instance, there is a hypervisor installed.
And that is what is used for launching VMs.
421:13 - So you launch your virtual machine, you choose
Ubuntu, and you're gonna have to go in and
421:14 - then set up your dependencies. So if you want
to run a Django app, you're gonna have to
421:15 - install Python and additional libraries to
run that application. So you go ahead and
421:16 - install that Django application. But this
easy to install is really large. So you want
421:17 - to make best use of use of it, as you can
see, say, Okay, well, I'm gonna put my MongoDB
421:18 - database on here. So you'd have to go in the
server install packages and binaries to run
421:19 - that. And then you say, Okay, I also want
to run rabbit mq on here. So you go into the
421:20 - virtual machine install that. But the issue
with installing multiple applications on a
421:21 - virtual machine is that some of these programs,
you know, their dependencies may not work
421:22 - best in a particular OS. So maybe rabbit mq
doesn't run best on Ubuntu or maybe Django
421:23 - and Mongo share, have a similar library, but
they have different conflicting versions.
421:24 - And that makes it hard for you to have them
installed side by side. Another issue here
421:25 - is when they're all on the same virtual machine,
let's say rabbit mq ends up eating up all
421:26 - the memory, there's nothing from stopping
it from consuming all the memory and then
421:27 - and stalling the Django app installing the
Mongo DB app. Or imagine if someone broke
421:28 - into your virtual machine, they now have access
to everything. So there is an issue with all
421:29 - this shared space. So generally, people will
not launch multiple applications on a virtual
421:30 - machine, they'll launch up additionally see
two instances, and have an app per virtual
421:31 - machine. But you'll always end up with with
leftover space. And so that is an issue. It's
421:32 - because the idea is that you're always choosing
a virtual machine that is never the correct
421:33 - size for the job at hand, or just, you know,
you can't make best use of that space. Now
421:34 - let's look at containers. So relaunching these
two instance. And this has the Docker daemon
421:35 - installed, and that is what is used to launch
containers. And so when you launch a container,
421:36 - the container has a Oh s and even though like
YouTube, this one's Alpine, but even though
421:37 - you're using Using this Oh, so not everything
is actually virtualized. So all the hardware
421:38 - virtualization isn't part of that container.
But you get to package in your custom libraries
421:39 - and packages and binaries, any dependencies
that you need, that are specific to this Django
421:40 - application. Now, when you want to launch
MongoDB, you can just do so and you package
421:41 - the libraries and the OS is specific to MongoDB.
And then you do that with rabid mq, okay.
421:42 - And so you can see, it's very easy to launch
applications, and you can remove them easily
421:43 - as well. Whereas, when you install everything
on the virtual machine left, if you remove
421:44 - rabid mq, that doesn't mean the library's
back packages and binaries go with it. Or
421:45 - let's say you just want to kill MongoDB completely
and reinstall it. You can't do that because,
421:46 - or you could but it'd be a lot of labor to
do so. So containers gives us a lot of flexibility
421:47 - to easily launch and destroy new applications.
And so we think of this more as available
421:48 - space. So if this easy to instance, we're
not utilizing at all, it's easy for us to
421:49 - launch things into it. So just to reiterate,
VMs do not make best use of space apps are
421:50 - not isolate, which could cause configuration
conflicts, security problems or resource hogging,
421:51 - were with containers. This allows you to run
multiple apps, which are virtually isolated
421:52 - from each other, you launch new containers
and configure those dependencies per container.
421:53 - So let's take a look at Docker files. And
this is a file that is used to run the commands
421:54 - that would be required to assemble the final
image, your Docker image. And so here's an
421:55 - example of a Docker file. And this is for
setting up a Ruby on Rails application. And
421:56 - it has the Postgres client, so it can use
Postgres database. And it's installing No,
421:57 - Jess probably for its front end stuff. But
let's just walk through here and I understand
421:58 - how this dockerfile works. So the first command
we have here is called from, and this allows
421:59 - us to pull in a another Docker image as the
basis of our Docker file. The reason why you
422:00 - do this is because if you had to set up Ruby,
there's a lot of dependencies. And this would
422:01 - make this fall really large. So it's nice
to be able to build off another Docker image.
422:02 - And after this Docker file is turned into
Docker image, you could then build off of
422:03 - that one. So that's kind of an interesting
strategy, then you have the Run command. And
422:04 - this allows you to execute any kind of like,
command that you normally run in bash. So
422:05 - you see apt get, so we're installing packages,
and the Mk dir, that's for making a new directory.
422:06 - And it does this in layers. So every time
a run command for whatever is in the scope
422:07 - of it, it turns it into a layer. And as Docker
is Docker is building that image, it caches
422:08 - those layers. So let's say you made a change
later in this Docker file, then what it would
422:09 - do is it wouldn't rebuild from scratch, it
would just go from the last cache layer, and
422:10 - then on so that makes rebuilding these a lot
faster. The next thing is working directory.
422:11 - And this allows you to change the default
folder for future commands. So just makes
422:12 - it a lot less verbose. When writing, writing
those future commands in this file, then you
422:13 - have copy, this is going to copy files or
folders from your local computer onto this
422:14 - image, then we have the entry point. So this
is the command that is executed when the containers
422:15 - first started, you cannot override this command.
So that's what it's going to be, then you
422:16 - have the expose. And this allows you to listen
on specified network ports at runtime, we
422:17 - can see the port is 3000. That is the default
port for Ruby on Rails when running in development
422:18 - mode. So it gives you an idea that this is
a Ruby on Rails app that's using Postgres
422:19 - that is intended for development. And the
last one here is the command command command.
422:20 - And it is passing the default arguments for
the entry point. So we're not sure what's
422:21 - in the entry point.sh. But there's something
in there. Because it is clearly a bash script
422:22 - that does something and it was included into
here. But what we're doing is we're passing
422:23 - in Rails server hyphen B, so binding to Port
000. So that starts up the application. So
422:24 - hopefully that gives you a perspective of,
you know, what a Docker file is and how they
422:25 - work. So I just want to cover some Docker
commands here with you. And these are Docker
422:26 - commands that are very common, and I think
that you should know them. There's definitely
422:27 - a lot more than what's in this list, but I
thought we could just quickly go through it.
422:28 - So the first one is Docker build, and this
is the command you're going to use when you
422:29 - want to build an image from Docker. Then you
have docker ps, which lists produces a list
422:30 - of containers, you're only going to see containers
here that are actually running. So you know,
422:31 - just be aware that Docker images is the list
of images you have on your machine. So those
422:32 - are the Dockers images you built from your
Docker files or if you pulled them from repositories.
422:33 - Then you have Docker run and this is going
to run a command in a new container. Then
422:34 - you have Docker push and this is if you have
an image you want to push it to a repo. A
422:35 - repo here could be Docker hub or VM or AWS
is elastic Container Registry ECR. And then
422:36 - you have poll. So you could pull an image
from a repo. So, you know, those are the ones
422:37 - I think are important for you to get some
hands on experience with. So yeah, there you
422:38 - go. Hey, this is Andrew Brown from exam Pro.
And we are looking at code bill, which is
422:39 - a fully managed bill pipeline to create and
create temporary servers to build and test
422:40 - code projects. So code build is a fully managed
build service in the cloud, it compiles your
422:41 - source code runs unit tests, and produces
artifacts that are ready to deploy, it eliminates
422:42 - the need to provision, manage and scale your
own build servers, which by the way, is really
422:43 - hard to do. I tried it and I'll never do it.
Again. It provides pre packaged build environments
422:44 - for popular programming languages and build
tools such as Apache, Maven, Gradle, and more.
422:45 - You can also customize build environments
to use your own build tools, which is very
422:46 - useful. They're talking about using Docker,
and scales automatically to meet peak build
422:47 - requests. So now that we have a little bit
knowledge on code build, let's look at some
422:48 - use cases. And dig in a bit deeper here. So
let's look at a code build workflow. So the
422:49 - first thing is you're gonna have to trigger
code build, somehow, you could do that be
422:50 - via the service console, the COI the SDK,
or the most common use case is that it's going
422:51 - to be part of your code pipeline. And after
it pulls the source, it's going to pass that
422:52 - on to code build. And that's how it's going
to get triggered. When you set up code build,
422:53 - you have to set up a build environment, and
AWS has some pre built manage images for you.
422:54 - So they have an Amazon Linux to Ubuntu and
Windows Server. If they don't have the things
422:55 - that you need installed on them, then you
have to provide a custom image, which is a
422:56 - Docker image. And you would normally store
this in ECR elastic container repository,
422:57 - where you'd upload your image and you could
reference it for that you could reference
422:58 - from other Docker sources like Docker Hub,
but you're probably going to use ECR. The
422:59 - next thing is the source code. So you need
to get source code into that build project.
423:00 - So you'd have to use a source provider. So
that could be from code, commit GitHub, Bitbucket,
423:01 - or etc. You probably could also just pass
along the codes to see how there's like no
423:02 - source. So you could have no source and as
the, the build script triggers, it could then
423:03 - just use the internet and pull from there.
But generally, you want to provide a source
423:04 - provider. And speaking of like, how are we
going to run our build that comes down to
423:05 - that build spec yamo file, and this is generally
part of your code base. So you know, when
423:06 - you pull in that your code from the source
provider, this file would be in that your
423:07 - root directory there, and it's going to talk
about or it's going to tell you all the commands
423:08 - that need to be run. But one thing I want
to tell you that even if you have a build
423:09 - spec file with your project, you can override
it with other build commands, which is very
423:10 - important for you know, for the exam, because
they might bring it up saying like, Hey, you
423:11 - have this build spec file, but you want to
override it, how can you do it. So it's like,
423:12 - well use the COI for that. So hopefully that
gives you kind of an idea about the code build
423:13 - a workflow. So I just wanted to touch again
on the build environment. So Docker images
423:14 - are managed by code build, which is AWS, it
was managed. And so you want to check these
423:15 - images to see what comes pre installed. So
you choose that environment image, you have
423:16 - Amazon Lex to Ubuntu Windows Server, and these
are the images. So if you want to dig into
423:17 - them there in the universe documentations.
But you can see that there are two variants
423:18 - for Amazon Lex two, there's two variants for
Ubuntu. And these are images, so you could
423:19 - download them and theoretically run them in
Docker and poke around and see what's in them.
423:20 - But I'm pretty sure they listed on the docks
there. So if your needs aren't met by these
423:21 - managed images, then you'll have to make a
Docker image stored on ECR and then reference
423:22 - it that way. So let's look at a couple of
use cases for code build. The first one being
423:23 - generating outside pages from a jam stack
jam stands for JavaScript API's and markup.
423:24 - And so we have a website that was built with
Gatsby that's a jam stack application that
423:25 - needs to render out stack pages and delivered
them to s3 static website hosting. So let's
423:26 - say we have our code or Gatsby code or a code
commit. So we could trigger code build with
423:27 - the aid of this console. Code build is going
to pull from its source at source build being
423:28 - code commit, and then the build is going to
render out the static pages, and then it's
423:29 - going to output that artifact into an s3 bucket.
So those are ready for static website hosting.
423:30 - The next one is let's say we want to run test
code and report test coverage. This is probably
423:31 - the most common use case for code build. So
developer needs to ensure their code passes
423:32 - all tests before being allowed to make a pull
request. So, you know, you let's say you have
423:33 - a Ruby on Rails application, and you've pushed
some code to it to a feature branch, that's
423:34 - going to trigger a GitHub webhook. That will
trigger a lambda function that uses the SDK
423:35 - to then tell code build to start building
on the source code. And so the code build
423:36 - will pull down the Ruby on Rails project.
And it's going to run r spec, which will generate
423:37 - out test coverage, and also say, well, the
tests failed or not, it's going to take those
423:38 - reports put in a zip, and then you could pass
it on to another lambda function. And then
423:39 - that information gets passed back to GitHub
to determine whether that pull request should
423:40 - occur or not. So those are two use cases.
Let's take a look at the build spec yamo file,
423:41 - which is the most important thing you need
to know about code build, you need to know
423:42 - this inside and out. And definitely get some
hands on experience with this because it's
423:43 - super important. So the build spec provides
the build instructions, the build spec YAML
423:44 - needs to be at the root of your project folder.
And here I have an example. And this is actually
423:45 - the one we use on exam Pro. So we have a Ruby
on Rails application. And so we don't use
423:46 - it all the stages, but you generally use pretty
much everything. And so let's walk through
423:47 - it. So the first thing we need to define is
the version of the build spec. And so there's
423:48 - 0.1, and 0.2, and 0.2 is what is generally
recommended. And if you want to know the difference,
423:49 - I don't think they show up on the exam. But
it's good to know. And this affects the default
423:50 - shell in the built environment. So 0.1 runs
each build command in a separate instance.
423:51 - Whereas 0.2 runs all build commands in the
same instance, build environment. So this
423:52 - is important because it tells you, you know,
what information is going to get shared into
423:53 - to the next command. So you know, that changes
the way you're going to write that file. So
423:54 - the next thing are phases. So phases are the
commands that run during each phase of the
423:55 - build, and there is a very specific order
that they go through. The first is install.
423:56 - So this is only for installing packages in
the build environment. Then the next one is
423:57 - pre build. So these are commands that run
before building, then you have build. So these
423:58 - are commands that you run during the build,
then you have post build, these are commands
423:59 - you run after the build. So they're pretty
straightforward in terms of their definitions.
424:00 - The only thing that's not clear is actually
at what step does the source code get pulled?
424:01 - I think it is after the install step. So that
is a step that is not very clear there. But
424:02 - you know, it's not that important for the
exam. But you should know these different
424:03 - build phases. And then the last thing there
is artifacts, so we're not showing that here
424:04 - in this document. But you can configure the
artifact to build somewhere else. So if you
424:05 - wanted, there's like a default place where
that build artifact goes. But if you wanted
424:06 - to specify the s3 output, you could do so
in here. And again, artifacts are just the
424:07 - results that are zipped that go to s3. So
there you go. Alright, so we are at the end
424:08 - of the code build section. So let's move on
to the code build cheat sheet. So code build
424:09 - is a fully managed build pipeline to create
temporary servers to build and test code,
424:10 - compile source code, run unit tests and produce
artifacts that are ready to deploy provides
424:11 - pre pre packaged build environments, or you
can build your own environments as Docker
424:12 - containers, use a build spec yamo to provide
build instructions. This file is stored in
424:13 - the root of your project. And we need to know
the contents of this file. So let's go through
424:14 - it right now. So we have version 0.1 of this
file, which runs each build command in a separate
424:15 - instance. Then you have version 0.2, which
runs all build commands in a separate install
424:16 - in the same instance. And I think it's like
it's when it says instance, that means like
424:17 - instance of a shell or bash shell, I'm not
necessarily another easy to instance, that
424:18 - would be ridiculous. Then we have different
phases. So we have runs these commands and
424:19 - through different phases. So we have the install
phase. This is only for installing packages
424:20 - in the built environment. We have the pre
build stage, which is for commands that run
424:21 - before building, we have the build stage phase,
which is commands that you run during the
424:22 - build. And the last one is post build command
post bill which is for commands that run after
424:23 - the build. So there you go. That's code build
in a nutshell, and we're grading for the exam.
424:24 - Hey, this is Andrew Brown from exam Pro, and
we are Looking at code deploy, which is a
424:25 - fully managed deploy pipeline to deploy to
staging or production environments. So looking
424:26 - at code deploy, it is a fully managed deploy
service in the cloud, you can deploy easy
424:27 - to on premise lambda or elastic Container
Service, you can rapidly release new features,
424:28 - which is the idea behind code deploy, you
can update lambda function versions, so that
424:29 - as a method of deployment you can do there,
you can avoid downtime during application
424:30 - deployment. This is generally with bluegreen
deployment, because the idea is that replicates
424:31 - an entire environment and moves it over and
then kills the old one, where in place generally
424:32 - takes effect on the existing server. So we
have, we have the ability to perform in place
424:33 - in bluegreen, which we just talked about there
a moment ago. It integrates with tools, such
424:34 - as Jenkins code, pipeline, and other ci CD
tools, and integrates with existing configuration
424:35 - management tools, such as puppet chef and
Ansible. Alright, so the first thing we're
424:36 - going to look at here for core components
is creating an application, this is the first
424:37 - thing you do in code deploy on an application,
it really is just a container for the rest
424:38 - of the components that make up code deploy.
But for an application, it's as simple as
424:39 - going in and naming your application. And
then you have to go ahead and configure a
424:40 - bunch of other components. The first component
that you need to configure is deployment groups.
424:41 - And this is a set of easy to instances or
lambda functions, where your new revision
424:42 - is going to be deployed to. And then once
you have your deployment group where you know,
424:43 - you're going to do your deploys, you can then
go ahead and create a deployment. And so you
424:44 - create a deployment and you choose the code
that you want to upload. And then you can
424:45 - go ahead and configure that deployment with
a lot of rules. So whether you want it to
424:46 - roll back, or whether you want it to, like
how you want it to fail, and a bunch of other
424:47 - rules in there, which we'll look at in more
detail at the end code deploy, follow along,
424:48 - then you're gonna have your app spec yamo
file, this is extremely important to know.
424:49 - And we definitely cover it in here in great
detail. And this contains all the deployment
424:50 - actions that code deploy is going to use to
figure out how to execute or like how to install
424:51 - restart your application on the actual server.
It's just a YAML file. And then the last,
424:52 - last, here, we have the actual revision itself.
So these, this is just an embodiment of all
424:53 - the changes that will take effect on the actual
server. So that is your apps like yamo file
424:54 - application files, you'd have to install configuration
files if installed, or any kind of executable.
424:55 - So there you go, that is the core components
to code deploy. So it's very important that
424:56 - we know the difference between inplace deployments
and bluegreen deployments for code deploy.
424:57 - So we're going to start here with inplace
deployments. So when you set up your deployment
424:58 - group, or I think it's deployment group, you
get this option between in place in blue green,
424:59 - so we choose in place, and then you choose
the environment, whether you want to do EC
425:00 - two in an auto scaling group. Or you could
choose EC two instances based off their tags,
425:01 - or you can actually choose on premise instances.
So let's understand the process of how this
425:02 - works. So the app on each instance, in the
deployment group is stopped. The latest app
425:03 - provision is installed and the new version
of the application is started and validated.
425:04 - You can use a load bouncer so that each instance
is registered during his deployment and then
425:05 - restored to service after the deployment.
deployment is complete. Generally, when you're
425:06 - doing in place, you want to use a load balancer,
it's a great idea. And then last only deployments
425:07 - that use easy to on premise compute platform
can use in place deployments. And notice that
425:08 - you cannot do a lambda function here. And
we do not have ECS clusters. So just be aware
425:09 - of that. And now we'll move on to bluegreen.
Alright, so let's take a look here at bluegreen
425:10 - deployments for code deploy. So here you choose
the blue green option. And then you have choosing
425:11 - between the automatically copy aect Auto Scaling
group or manually provision instances, you're
425:12 - definitely going to want to choose the first
case. And this is what would generally show
425:13 - up on the exam if they do talk about this.
And the idea here is if you already have an
425:14 - environment, and it's using auto scaling group,
you specify it here, it's going to copy that
425:15 - one over. And a new easy to instance is going
to spin up code deploy is going to then apply
425:16 - the latest code version and install it using
the app spec yamo file. And then what's going
425:17 - to do is shut down the old environments as
well, it will shift over the traffic from
425:18 - old to new, and then destroy the old old infrastructure.
So let's go through that just again here.
425:19 - So instances are provisioned for the replacement
environment. So that could be the auto scaling
425:20 - group being cloned the latest application
revision installed on the on the replacement
425:21 - instances. So if you're using code pipeline,
it could be passing the the artifacts from
425:22 - the source to code deploy and then it will
install into the correct location. We will
425:23 - see that in the app spec yamo file and then
it will install from there. An optional wait
425:24 - time occurs for activities such as application
testing and system verification. Is this isn't
425:25 - the replacement environment are registered
with an EOB, causing traffic to be rerouted
425:26 - to them. So that's the shift of traffic, and
instances in the original vironment are deregistered
425:27 - and can be terminated or kept running for
other uses. So people don't always terminate
425:28 - their, their old environments right away.
Because sometimes you need to debug them,
425:29 - or you need to fall back to them in the case
of a disaster. So it's up to you on that case.
425:30 - So we're gonna take a look at the app spec
yamo file, which is responsible for saying
425:31 - where the code should be installed, and how
to get the new piece of code running. And
425:32 - this example here is actually from the exam
pros, Ruby on Rails application. It's an older
425:33 - version, but it makes still for a very great
example of a real world use of this aspect
425:34 - gamble. So we'll just walk through this, and
there is some variation on this file here.
425:35 - But this file is a very good example. So the
first thing is we choose our o 's, this could
425:36 - be Linux or Windows, then for files, we are
saying where the code should be downloaded
425:37 - to. So Ford slash is after that zip is because
you provide the code in the form of zip. So
425:38 - wherever that zip resides, take it and put
it in home easy to use your app because I
425:39 - want it to be an app, then we can apply permissions
to anywhere. So I just want to make sure that
425:40 - easy to user is the owner of that app directory.
And then we have our hooks. So you have an
425:41 - application stop. So this command should be
responsible for doing the stop. If you notice,
425:42 - we're providing a location to a bash file.
So the way aspect camel works is you write
425:43 - bash scripts for all them. And then you specify
them with part of the zip that is provided
425:44 - to code deploy. So the aspect YAML will be
in a zip, and then all these files. So this
425:45 - one, this one, this one, this one, this one,
they'll all be in that zip there, then you
425:46 - have a timeout. So you can set a timeout,
I think there is one by by default, but if
425:47 - you know generally how long these are going
to run, you should set them because this will
425:48 - just speed up the process. If one of these
hang, for whatever reason, then you can set
425:49 - what it should run. So I always say easy to
user, especially with Amazon is one, Amazon
425:50 - x two. And let's look at some of the other
books we have before install. So that's before
425:51 - it's downloaded the code to your server after
install things that you'd want it to happen
425:52 - afterwards. And then a command to restart
the application up. And I want to point out
425:53 - that the lifecycle event hooks are are going
to be different based on whether you're using
425:54 - ECS, ECS, and lambda. So you have to look
up the documentation to what's available to
425:55 - you. But this is for an easy to instance.
And so this is the most common use case. So
425:56 - when you do perform a deploy in code deploy,
you're going to get to be able to visually
425:57 - see all the lifecycle event hooks that are
available to you. As they perform, you'll
425:58 - see these go from pending to successful or
if they fail, they'll show you additional
425:59 - information. And you can see that we get the
durations of it. So it's really good way to
426:00 - get that overview of it. In the case of a
failure, it will look like this. So you can
426:01 - see that this script got through here, and
then it failed at this point. And then you'd
426:02 - click into that. And then once you click into
it, you will get a little bit more information.
426:03 - It's not always clear as what has gone wrong.
So here you can see that it's saying start
426:04 - dot start underscore puma.sh has failed. So
that script in particular that I wrote had
426:05 - an issue. And then inside we might get some
information actually as to what failed Exactly.
426:06 - And this one is totally not clear. So there's
a lot of cases where you have to log into
426:07 - the EC two instance, to debug code deploy.
Generally, what you want to do is stream your
426:08 - code, deploy logs to cloud watch logs, which
I don't show you in this in this section.
426:09 - But it's something you definitely want to
do if you're running code deploy in production,
426:10 - because it's such a pain to log into an easy
to instance, and debug this stuff. So but
426:11 - there you go. So I just wanted to hop over
to the documentation here quickly to show
426:12 - you what hooks are actually available to you.
And what's going to affect hook availability
426:13 - is the deployment methodology. You can notice
for end up in place deployment, we have all
426:14 - hooks available to us. And then for blue green
based on the case it's going to change. So
426:15 - it's interesting here because when you do
bluegreen deployment, you will see all these
426:16 - hooks in the code deploy you'll see all of
them but it's just trying to separate them
426:17 - out to say these are the original ones that
are going to happen on that side the Replace
426:18 - replacement ones on this side. So just be
aware of that and then these are just for
426:19 - rollback. So for rollback so you can see the
scope is quite fear there. Do you need remembering
426:20 - these for the exam? No, but it's just good
to know because it can save you time debugging.
426:21 - And if you're doing this for real in a practical
use. So to get code deploy working with your
426:22 - computing powers, you're going to need two
things, you're going to need the code deploy
426:23 - agent. I always think this is pre installed
on Amazon, Linux one and two, but it's not.
426:24 - So what this is, it's just a service. It's
like a binary that you it's actually written
426:25 - in Ruby, but you have to install Ruby on your
server, and then download the script script,
426:26 - install it. And then what that will do is
it will have the cloud code deploy agent continuously
426:27 - running. And it's going to report back to
code deploy the progress of when you run the
426:28 - the lifecycle hooks and installs all the steps.
The other component to it is you need to create
426:29 - a code deploy service role. And this is pretty
easy through the Iam console, if you just
426:30 - go to View, choose code deploy, they have
some preset ones for you. And what this does
426:31 - is it gives access to things like auto scaling
group CLB, and things like that, I say you
426:32 - may need to create it, because I think in
some cases, you might not need it. But this
426:33 - allows code deployed to create an additional
auto scaling group and like shift traffic
426:34 - between elastic load balancers. So these are
the two things you'll need to set up, which
426:35 - is not very clear, when you're using code
deploy. Hey, this is Angie brown from exam
426:36 - Pro. And we are going to be doing the code
deploy follow along. So we're going to learn
426:37 - how to set up automated deployment using code
deploy. And the first thing we're going to
426:38 - need to do is get an EC two instance, set
up with a basic web page that we're going
426:39 - to turn to Nami, which we will use for code
deploy. So let's make our way over to EC two.
426:40 - And what we're going to do here is we're going
to go to instances on the left hand side,
426:41 - we're going to launch a new instance. And
I want you to choose Amazon Linux to the top
426:42 - one there it's select, we're going to go with
the T two micro because we want to save money.
426:43 - And what I want you to do is just drop down
here and you should have this SS SS m EC to
426:44 - service role. If you don't, let's just go
through the process of creating that right
426:45 - now. But we have created an other follow along
here. But just in case you don't have it,
426:46 - I'm just going to delete this role here and
make a new one. And this allows us to gain
426:47 - access to sessions manager if we need to log
into this instance, I'm gonna go up to next
426:48 - we are going to let this load here. I'm gonna
type SSM. Next, Next, SSM EC to service role.
426:49 - That's what I like to call it, we'll go ahead
and create that there. And then what we'll
426:50 - do is go back here are refresh. And then this
role should now appear. So that's very important
426:51 - that you set that because it's going to save
us a lot of trouble. Now down below, we have
426:52 - the ability to put in a user data script.
And this is what we came here to do. So let's
426:53 - set up a basic web application. So what I'm
going to do is go over to cloud nine, and
426:54 - you should have a cloud nine environment.
And if you don't watch the Elastic Beanstalk
426:55 - follow along or some of the other ones where
I show you how to set up cloud nine, it's
426:56 - very easy to get environment set up. And what
we're going to do is we're going to make a
426:57 - new folder here on the left hand side, I'm
going to call this code deploy project. And
426:58 - I'm just going to make a new file in here
called user data.sh. And what we're going
426:59 - to do is set up a script that's going to set
up a patchy a basic web page. So the first
427:00 - thing we need to do is specify our shibang
or shibang is going to tell tell this script,
427:01 - what did you use to execute it, which will
be bash, we're going to want to switch to
427:02 - the sudo the ES VC to user when we log in,
we're going to want to install HTTP, which
427:03 - is what Apache is, then we will want to change
the ownership of the directory for the var
427:04 - WW. This just makes it a lot easier to work
with it. So we're gonna change that to the
427:05 - easy to user. The next thing we want to do
is we're going to make a basic web page. So
427:06 - we're going to place an index file in bar
www dot html and have index HTML. That's where
427:07 - a patchy looks by default. And we're going
to use a nice here doc here to do a multi
427:08 - line here in bash, which is very useful to
know. And so I'm just gonna make a very basic
427:09 - website, the ugliest website I can think of.
So we'll just get an HTML in there when we
427:10 - will get a head and we'll get a body in there.
And we'll need a title my code deploy app
427:11 - and we will do the same thing here in the
body except instead of having A title we're
427:12 - gonna make that h1. And then there's just
a couple things we need to do down below,
427:13 - we need to start up Apache, we're using system
CTL, you could use the alternative syntax,
427:14 - which is sudo, sudo, service HTTP, D start.
But I noticed that if you if you do that the
427:15 - enabled doesn't work. So this is why I'm using
the more long form format here. So that's
427:16 - for anyone that is very familiar with this,
for whatever reason, it's not working in the
427:17 - other format. And so that should be everything.
So switch DC to user install, install Apache,
427:18 - change ownership, create that initial file,
and that looks all good to me. So I'm gonna
427:19 - copy this here, go over here, as you can see,
you could input a file, I'm just gonna use
427:20 - text here, nice and plain. And assuming I
got that, all right, uh, we'll be in great
427:21 - shape. If we don't, that's okay, we'll make
our way in there using sessions manager. So
427:22 - we just need to set up a security group here.
So I'm gonna go next, next. Next, I like naming
427:23 - actually gonna go to tags, this is always
a good idea, we're just gonna put a name,
427:24 - I'm gonna say, my or my code deploy app. Because
if we do that, it will name it. And I usually
427:25 - do that afterwards, we're going to make a
new security group here, just so we can see
427:26 - what we're doing my code deploy SG, and we're
going to open up Port 80, we do not need Port
427:27 - 22. Because we will not SSH into this, we
can get into it using sessions manager will
427:28 - hit launch. Once you drop down, proceed without
a key pair, which is super, super more safe.
427:29 - And we will go to will click on that instance
there. And now all we have to do is wait for
427:30 - it to launch. So I'll see you here in a moment.
Okay, so we have a running state, both status
427:31 - checks have not passed. But that's okay. Because
generally, we can check the website. But we
427:32 - should usually wait for those two checks.
I'm just grabbing the IP address here and
427:33 - refreshing. And there's my application. Now
what's really important to check whenever
427:34 - you set up an app here is to restart the server
because we absolutely want to be sure that
427:35 - if we stop and start this again, it reboots.
So what we're going to do here is we're going
427:36 - to go stop. And we're just going to let this
shut down here, it shouldn't take too long.
427:37 - Keep on refreshing here, and then once that
stopped, we're going to start it back up.
427:38 - So that is just a pro tip for you because
there's a high chance that the service might
427:39 - not start up because something's wrong their
script. And to find that out later is such
427:40 - a pain, you have to rebate the EMI, I do not
want to do that. So I'm going to go ahead
427:41 - and start this up again. And we'll just give
that a refresh. I guess I'll see you back
427:42 - here in a moment. When it's a it's active
again. Welcome back, I didn't wait for my
427:43 - status checks again. But it is running. If
we copy that IP address, we can see that the
427:44 - application is there. So we're all in great
shape. If you did want to connect to this
427:45 - instance, you could go to sessions manager,
but in other ways, just go click Connect here
427:46 - and go to sessions manager, we don't need
to do anything here, I'm just showing you
427:47 - around. And so what you normally do is do
sudo su hyphen easy to user because it always
427:48 - logs you in as root, it'd be really nice.
If it logged you in as easy to user, you could
427:49 - select that, if you're listening AWS, please
make that change. If we go to var WW, HTML,
427:50 - HTML and do all stuff in LA, there's our file.
Alright, so just getting you a little bit
427:51 - comfortable with EC two. And we'll pop out
there. And so the next thing we need to do
427:52 - is make an ami of this image. So I'm going
to go here and create a new image. I'm going
427:53 - to call this my code deploy app 000. It's
great to version in this way. Always give
427:54 - it three zero, so you have a little bit of
room to breathe. And what we'll do is we'll
427:55 - go ahead and create that image. And so if
we go here, we'll just have to wait for that
427:56 - image to go pending. And once that is done,
then we will actually leave our current server
427:57 - running. That is totally fine. But what we
need to do next is set up a code commit repos.
427:58 - So we'll go to code commit. And the next thing
here, I don't know if we need code commit
427:59 - right now. We'll need it for later. But yeah,
you know, we'll come back to code commit later,
428:00 - I was thinking later when we added to the
code pipeline, that's something we can definitely
428:01 - do later. So what I'd rather do instead is
to get code deploy set up. And for that, we'll
428:02 - we're going to need a a deploy file, which
they call app spec dot YAML. I'm just gonna
428:03 - make a new file here called app spec dot YAML.
Let's make our way over to code deploy, we
428:04 - might get stuck through this process if we
don't actually have a way of deploying. But
428:05 - we will do the best we can here. So what I
want you to do is on the left hand side, go
428:06 - to applications, I'm going to create a new
application. And I can see that I have a test
428:07 - one here, I'm gonna go ahead and delete that
you're not going to have this. Oops. Anyway,
428:08 - we'll go back to applications here, create
a new one, I'm going to call this my code,
428:09 - deploy app. And we'll hit we'll choose EC
two, we'll create the application. And so
428:10 - now that we have an application, we have to
set up a deployment group. And we'll call
428:11 - this my deployment group, or my code deploy
deployment group, which is a bit silly, enter
428:12 - a service role. Code deploy permissions that
grants it was code deploy to access your target
428:13 - instances. So we don't have one of those.
So that's something we're gonna have to go
428:14 - get. We probably can just go edit our existing
EC to roll. So maybe that's what we'll end
428:15 - up doing. Okay, so I just pulled up the documentation
quickly here. And I believe we need to make
428:16 - this service roll. So what we'll do is we'll
make our way over to I am, so I already have
428:17 - it here on the left hand side, you might have
to type in I am, we'll go over to roles, we're
428:18 - going to create a new role. We are going to
choose probably code deploy, what do we have
428:19 - to choose here, she was a service with the
roll, choose code deploy. All right. We'll
428:20 - go back to high em here. We'll choose code
deploy. And then down below, we already have
428:21 - those presets. So we're gonna choose code
deploy, which is for auto scaling. Yeah, that's
428:22 - fine to me. We'll go next, we will see what
we get there. So it's already predefined,
428:23 - we will hit next. I love it. When they have
those predefined ones for you. It saves you
428:24 - so much time. And we're just going to name
it as suggested. So we'll go up to here. I'm
428:25 - just going to call that will create that role.
That will go back here. And let's see if it
428:26 - autocompletes enter a service role Arn. Well,
there you go, I'll use so that's what we'll
428:27 - do. So we'll go into here, I'm just going
to copy that Arn and paste it on in hopefully
428:28 - that works we have in place in blue green,
I'm going to try to keep it simple and do
428:29 - in place the configuration, select what you
want to deploy. So we'll choose EC two instance.
428:30 - And you can add up to three tags. So I guess
I have to tag my C two instances, oh boy.
428:31 - So what we'll do is we'll make our way over
DC two, we'll go to instances. There's our
428:32 - app. And what we'll do is we'll just add a
tag. So we'll go edit tags, I call this app
428:33 - CD. And we'll save that we'll go back over
here, I'll say app, CD tag, oops, I guess
428:34 - we only really need one there. So that should
do it. Um, we're gonna do all at once sounds
428:35 - great to me, we're going to turn off load
balancer, we don't need that. We're gonna
428:36 - disable rollbacks, we're gonna go ahead and
create that deployment group. So now that
428:37 - we have this deployment group, it should know
how to deploy. So if we want to go ahead and
428:38 - do create, deploy, and we scroll down here,
um, what we need to do is supply the revision.
428:39 - So it's interesting, you can't supply like
you have GitHub, you can't do code commit.
428:40 - So which is totally fine, I guess. So I guess
what we're going to have to do is we need
428:41 - to supply a change to our server revision
to our server. So what we'll have to do is
428:42 - make our way over to cloud nine, and we will
make a revision, and then what we'll do is
428:43 - place it in an s3 bucket. So let's go over
to cloud nine here, I'm gonna make a new file
428:44 - here. I'm gonna call index dot HTML. I'm going
to grab our code from over here. I'm going
428:45 - to copy this and paste this. I'm just going
to call this v2, v2. Now, I think, I think
428:46 - we're going to need to have to have an app
spec dot YAML file in this because that's
428:47 - what it uses to figure out the deploy. So
yeah, I think that's what we'll need. So just
428:48 - give me a second to pull up the documentation
for that. Okay, so I just pulled up the documentation
428:49 - for the app spec yamo file. But actually,
you know what, I think what we should do is
428:50 - we should go set up that code commit because
it's gonna make things a little bit more organized.
428:51 - I will just get this out of the way. So I'm
gonna go create here, I'm gonna say, my code
428:52 - deploy app. And what we'll do is
428:53 - We'll hit Create. And we're going to have
this clone URL. So grab the HTTPS when it
428:54 - copies to your clipboard, make your way back
to your cloud nine environment. And what I
428:55 - want you to do is get into this folder here.
So just go to Tilda, Tilda takes it to the
428:56 - home cloud deploy project. And then we'll
do git clone there. And that will put a folder
428:57 - inside of a folder a little bit messy, but
that works. And what am I to do is drag this
428:58 - index dot HTML into there. And then we'll
drag this app dot YAML into there as well.
428:59 - So that's a little bit more organized. And
then let's open up our app, app spec dot YAML.
429:00 - So the first thing we need to do is define
a version. It's gonna be version 0.0. I don't
429:01 - think there's any other version at this. At
this point, this is the only version there
429:02 - is for app spec. Then we'll do LS, and we'll
choose Linux because we're not using Windows.
429:03 - And we could set some other things like files
and permissions. And that's what that's where
429:04 - it's going to place the file. So I'm, I'm
trying to decide where this should go. I guess
429:05 - I'm going to just place it. So we'll say source,
which is the root of the the revision will
429:06 - upload. And then I'm going to supply the destination
destination. I'm going to Home Ec to user
429:07 - oops, no, no, no, we're going to put var www
dot html. So that's going to place it where
429:08 - we want it to be. And we're just going to
make sure the permissions are as what we need
429:09 - them to be. So I'm going to do object, we're
going to say var WW. Dot, we've already done
429:10 - this already. But I'm going to just force
it anyway. And it's also necessary to see
429:11 - how we can use permissions to change permissions.
You see to user, we're going to do group,
429:12 - EC to user as well. And the next thing we'll
do is we'll put in some hooks. So for hooks,
429:13 - there's the application stop, that's something
we definitely want to do. And we will just
429:14 - name this little save location. And we'll
say stop app.sh will give it a timeout, it's
429:15 - always good to provide timeout. So that doesn't
take forever. Stopping the app is super, super
429:16 - fast. So we'll give it I don't know, 10 seconds,
which is maybe we don't even need 10 seconds,
429:17 - I think it takes like six seconds. And we'll
run this as easy to user. And so that should
429:18 - be the premise of that one. We're gonna need
that that stop app there. So I'm gonna go
429:19 - here and make a new file, say, Stop app.sh.
Whoops, we will rename that because of its
429:20 - name wrong, we're gonna have a lot of trouble
on our hands. So I'm going to just double
429:21 - click that there, we're going to go to user
data. And we're going to grab this line here.
429:22 - And what we're going to do is paste that in
there. Generally, when you run commands, you
429:23 - want to give it the absolute path. So we need
to know where the absolute path of httpd is.
429:24 - So what I'm going to do is connect to the
instance here, we're going to use sessions
429:25 - manager, I hit Connect. And we're going to
here sudo sudo, Su hyphen EC to user. And
429:26 - then what we're going to do here is type in
Where is httpd. And that's going to give us
429:27 - the full path. So this I believe is the full
path. I really hope that's the case. I guess
429:28 - I could just run it. Yep, that's it. And what
we'll do is we'll go back here, and I'm just
429:29 - going to paste this in fully here. Because
I do not want to have any problems. And I
429:30 - think it's this. Here we go. So that's our
stop. And then and we don't necessarily have
429:31 - to stop it, we could just restart it, I suppose.
Maybe that's a little bit cleaner to do, I
429:32 - think that's what I'm going to do, actually.
So I'm going to rename this to restart app.
429:33 - That's going to avoid us a bunch of problems.
And so instead of doing this in the application
429:34 - stop, I'm gonna do this the application start,
which is one of the last things that it does.
429:35 - So for application stop, we're not going to
do much here. There's the before install that
429:36 - we can put in here. There's nothing really
to do there, but we're definitely going to
429:37 - need an after install. So after install that
means like after the code has been downloaded
429:38 - to the source directory. So wondering if this
is a smart idea to place it here. Um, maybe
429:39 - I'll just put in a folder called revision.
That's actually what I think I would prefer
429:40 - to do. And then what we can do here is we
will just copy this here. And we are going
429:41 - to make a we'll rename this to restart and
we'll rename this to update, update app. And
429:42 - What we'll do here is make that new file over
here. And I call that update app.sh. And we
429:43 - will open that one up there. And what I need
to do is I need to make a command that's going
429:44 - to copy that revision. So I'm just going to
double check, make sure I put a forward slash
429:45 - in there. Yeah. So the revision should be
stored here. So the first thing I would do
429:46 - is remove the old file. So I'd say remove
var, www dot html index dot HTML. And then
429:47 - I'd say copy, revision to var. W Yeah, this
one up here. Not gonna read the whole thing
429:48 - out there. So this should do it. Okay. And
so yeah, this should be our code spec. Now,
429:49 - what I'm going to go ahead here and do is
commit all this stuff to our repo. For a later
429:50 - step, actually, we'll leave it alone. For
the time being, we don't have to do that.
429:51 - But I do need to zip the contents of the stuff
that is here. So let me think about that.
429:52 - We'll go back to code deploy, I'm pretty sure
it expects it to be a zip, copy and paste
429:53 - the Amazon s3 bucket where your revision is
stored. And here you can see we have folder
429:54 - object dot zip, targ, or t g. Zed. So what
I'm going to do is I'm just going to figure
429:55 - out what these command is. And I'll be back
here in a moment, because I never remember
429:56 - that off the top my head. Okay, I'm back.
I think I know what the command is. It's zip.
429:57 - And then you supply the name. And then we'll
do this in the that actual folder there. So
429:58 - go into there, I'm going to type in zip, and
then whatever we want, call this say this
429:59 - is revision, revision 000 0.1, maybe zip.
And then we need to say what files we want.
430:00 - So we want the aspect camel, we want the index
HTML, we want the restart app, we want the
430:01 - update, sh. Maybe we should match the version.
Because it says version two, let's just call
430:02 - it version two, it'll be less confusing. So
I think we have everything update, restart.
430:03 - Yep, we have all the files we need. What we'll
do is we'll hit Enter. And that all looks
430:04 - good to me. So now we just need to get this
revision file onto s3. And we're going to
430:05 - need a new s3 buckets. So using this, Eli,
it's going to be AWS s3 API, create bucket.
430:06 - And we're going to give it a bucket name,
of course. And the name is going to be my
430:07 - code deploy. app. If that doesn't work, you
might have to choose a more unique name because
430:08 - s3 names are unique, like domain names. And
we're going to specify the region always specify
430:09 - the region makes our life a lot easier, always
use East one. And I did not use the command
430:10 - correctly. It was s3 API three. Hold on here.
Eva's s3 API. Help. Yeah, there's a create
430:11 - bucket command. And so we'll do create bucket.
Just scroll down here for a second. Just looking
430:12 - for an example. Okay, so it says colon colon
bucket. I called it bucket name. So that's
430:13 - not the command that is supposed to do there.
So I'm just going to hit up there. And we're
430:14 - going to make our way over here and just take
out the hyphen name part. And there we go,
430:15 - it created the bucket. So now what we need
to do is copy this revision to there. So I'm
430:16 - gonna do it was it was s3 cp, why it's not
s3 API, why there's different ones, I have
430:17 - no idea why, but that's just how they have
it. And so we'll specify that file which is
430:18 - revision two. And then we need to specify
the bucket, so s3 colon, colon or colon slash
430:19 - slash, then we'll put the name of the bucket
here. And we'll just maybe put in a new folder
430:20 - called revisions. And, yeah, maybe we'll call
it version two. And we'll hit up, upload there.
430:21 - And so now it's uploaded. So if we make our
way back over to here, it's now expecting
430:22 - that bucket path. And we are just going to
go back to our cloud nine environment and
430:23 - we have it right here. So I'm going to grab
it Okay, and paste that there. The reason
430:24 - type is zip That is correct. And there's that
application stop lifecycle. I don't remember
430:25 - this option being here before don't fail to
deploy to an instance, if this life cycle
430:26 - event on instance fails. That's that's a cool
option here. So we don't have to do anything
430:27 - with that you we don't need to override anything
there. We don't do anything like that. Normally,
430:28 - you have to add a A code deploy agent to the
EC two instance, for this to work. So I'm
430:29 - just wondering if I need to give permissions
for that there. Maybe we should do that before
430:30 - we proceed forward. Because I feel like that's
going to give us trouble if we don't do that.
430:31 - So what we'll do is go back to our EC two
instance, it's already running, and we have
430:32 - this SSM aect role that's here. So what I
want you to do is I want you to go to I am,
430:33 - we're going to use create ourselves a new
unique role for this app. So I'm just gonna
430:34 - go EC to next. And I'm going to, I'm going
to add some policies. So I want the SSM one
430:35 - definitely. And then I want the code deploy
agent. Yep. And we'll hit Next Next, and I'm
430:36 - going to call this my code deploy app role.
And we will create that rule. What we'll do
430:37 - is we'll make our way back to EC to console
to where that is, we'll just type that in
430:38 - again. We'll go to instances and left hand
side will look up this instance, I'm pretty
430:39 - sure we can just replace the roll through
here. Where are you attach replace I am role.
430:40 - So we will swap it out for my code deploy
role? And we'll hit Apply. And we will close
430:41 - that. We don't know if we'll have to restart
the AP. I don't think so. I don't think so.
430:42 - And the IM role is not part of the actual
ami, we attach it when it launches. So that's
430:43 - another thing we don't have to worry about.
So what we'll do is we will go back to code
430:44 - deploy. And I think we're all in good shape
here. Deployment description, add a brief
430:45 - description, deploy version two. That's what
we want to happen. Now whether that will work,
430:46 - I guess we'll find out here in a moment. And
we'll hit create, deploy. And I'm so used
430:47 - to code deploy, deploy is failing. So if this
fails, I'm totally okay with that. Now, this
430:48 - is an in place deploy. Yeah, so I was thinking
I needed the code deploy agent, because code
430:49 - deploy might have to start up another auto
scaling group or something. But I realized
430:50 - as an in place deployment, that means it's
not taking, not creating a new server, it's
430:51 - just going to take the server out of service,
and then apply the updates in place. And we
430:52 - will see we click View events here and see
how that goes. Generally, this is super fast,
430:53 - so I'm not sure why it's taking its time.
But I guess what I'll do here, I'm just gonna
430:54 - click back. Since it's in place, that should
be super fast. What I'm gonna do is I'm just
430:55 - going to wait until we have the results here.
If it fails, it fails or if the time's up,
430:56 - I guess we'll find out here shortly. Alright,
so this failed, but that's okay. I figured
430:57 - out what the problem was. I hopped onto it
with support, because I was really, really
430:58 - stuck. And I realized I didn't install the
code deploy agent. I guess I just thought
430:59 - it was pre installed on Amazon Linux two,
but I guess it's not which is a bit frustrating,
431:00 - and they don't have a run command for that
either. So I guess we'll have to manually
431:01 - install that. Not a big deal because we were
smart enough to set up SSM just in case we
431:02 - had to go into our server. So let's go ahead
and hit Connect, go to sessions manager Connect.
431:03 - And then we'll go ahead and install code deploy.
So

Cleaned transcript:

Hey, this is Andrew Brown from exam Pro, and I'm bringing you another animal certification course. And this one happens to be the hardest at the associate track, which is the developer associate. So if you're looking to gain hands on knowledge, building, securing, and deploying web applications to AWS, this is the course for you. And remember, if you really want to pass, make sure you do all the follow ons that we provide to you here in your own database account. And we definitely welcome any type of feedback you have. So go on any social media, and we will find you. And if you do pass, definitely tell me on Twitter or LinkedIn, I love to hear that people are passing, and good luck on your exam. So I'm sure you have a lot of questions about the developer associate. I'm hoping that I can answer some of these before you get started on your journey to make sure that you're making the right choice about gaining developer associate. And the first question we're going to ask is, who is this certification for. And I would say that if you're a web developer, and you're looking to add cloud computing skills, to your developer toolkit, this is the certification for you. If you if you want to know how to build web applications, while thinking about how to build them cloud first, this allows you to push a lot of your web application complexity into managed services. And then you have easier, more modular web applications. That is another goal or sorry, another thing you'll achieve getting the Dell developer associate, you're going to learn how to deploy web applications for a variety of different cloud architectures. So serverless, microservices, traditional applications, there's a variety of different ways to deploy to the cloud. And the last thing is that if you are a web developer, and you're looking to transition into a cloud engineering role, this is a certification for you. Now, what value does the developer associate hold? What's it going to do for you? And I would say first, that the this certification is the hardest eight associate certification. So you have the solutions architect and the sysops. This one is absolutely, brutally hard. And the reason why is that you have to have practical knowledge of AWS to pass this exam, it's very hands on. But that the great advantage of that is that it's going to directly help you get a job as a cloud engineer, because it's, it really is about doing the actual work. Okay. And the last thing I want to point out is that it will help you stand out on resumes. It's not likely to increase your salary unless your company really values cloud engineers. But you're definitely get more job opportunities. And eventually, this is going to be the de facto of a web developer having cloud skills. So it's important you get this now, because you're going to get ahead of everybody else. So another question people ask me is how long to study to pass as developer associate, and you just heard me say that it is the hardest associate certification. So this is a little bit longer than standard. So if you're a developer, I'm going to say it's going to take 1.5 months of study, I generally say for the sysops, or the solution architect associate one to two months, but this one is definitely going to be 1.5 months, it going definitely into two months, if you are a developer, if you are a bootcamp grad, you don't know anything about AWS, and you're doing this from scratch, you're looking at at least two months study pushing on to three months. If you are already a cloud engineer, and you're just trying to add the certification to around out your resume, you're looking at 30 hours of study. So if you really sat down and took the time, you could get this out in a week. So you know, that is the scope that I would say there. And the last thing here is, is just figure out how much how long and how many questions. So the cost of this certification, just like all the other associates is $150 USD, it's 130 minutes. So it's a you get a lot of Apple time there, there's 65 questions in this exam. And to pass, you have to have a 70, around 72%. It's not an exact number, that number can float around there. I think one more point is that this certification will be valid for three years. So it's definitely you know, $150 seems like a lot but it's gonna last you for quite a while. So, you know, hopefully that answers your, your questions, and we'll move on to the exam guide. Alright, so we're taking a look here at the exam guide breakdown. The first thing I want you to know, is the course code, which is dva cs 01. This might be the future and you're booking your exam and you might be presented with two versions of the exam. This happens when a new version of the exam is out. And there's some overlap between the old one and the new one. So the solution architect associate is sa c 02. That's the new one and the old one is C 01. There has there, they have yet to release a co2 version of the developer, I can tell you right now that there isn't a huge change between CL one and co2, at least at this, Susan artech associate, all they've really done is rebalanced, the domains percentages, and revise the questions. But the bulk of the content for course, material is the same. So if this is the future, and you're looking for co2, or you're going to be totally fine using this course content here, we'll move on the next plot part, which is the passing grade is 720 points out of 1000. I don't know how those points are distributed, it's not that important to know, all you really need to know is that you need to get 72% to pass. And that's not necessarily exactly 72%. AWS can adjust that value based on how many people are passing and failing. So you can go on an exam get 73% and still fail. So you do have to consider that there are 65 questions in this exam, that means you can afford to get 18 questions wrong. So you have a great margin of error here for this particular exam. And the duration is 130 minutes. So that means you get two minutes per question. For this exam, for the developer associate, it's very unlikely that you'll run out of time. Well, you'll end up with surplus time at the pro levels, you always run out like you're always running against the clock. So you have to know your your pacing per question for developer associate, that's not the case. And if you have surplus time, you're definitely going to want to go back and review all your questions and utilize 100% of that time. Now what we'll talk about is just the type of questions you'll encounter. So you'll see multiple choice, this is where you choose one out of four. And then you have multiple responses. And this is where you choose two or more out of five. So those are the two typical formats you're going to see on the exam here. And now just getting to the actual breakdown of the exam, it's broken down into domains. And it also has sub domains. And we'll look at that in greater detail when we actually pull up the exam guide. But the first domain is deployment worth 22% deployment is extremely important to know for the developer associate, and you're going to end up with around 14 to 15 questions that you'll be presented with. The next domain is security. And so security is becoming super important in all the certifications. So we're seeing that really across the board everywhere. And here. It's super important with 26%. And you're gonna see between 16 to 17 questions, then we have development with AWS services. So this is at 30%. So you're gonna see between 19 to 20 questions, then you have refactoring, which is worth 10%. So this is between six to seven questions. And the last domain is monitoring and troubleshooting at 12%. So we have questions between seven to eight that you'll see on the exam. So you can see that development with Ada services is the highest percentage. And that makes sense, because it's the developer associate, you should learn how to develop with a diverse services. So in the exam guide recommends white papers that you should read. And so if you're not familiar with white papers, what these are, are PDFs that are published by AWS, which are 100%, free for you to download. And they're kind of like Ava's documentation, but they have a sales perspective to help you adopt AWS. And when they create the exams, they actually base a lot of information from the contents of these white papers. So it's important for you to read some of these white papers, when you're studying for professionals, it becomes absolutely essential to know these white papers inside and out at the associate level, not so much. But if there are white papers that you should absolutely read, it's these ones in red, you absolutely absolutely need to read those white papers, the ones in black are things I suggest for you to read. And the the ones in gray are the ones that I would say, it would not matter whatsoever if you read them. So that is my recommendation for white papers. And if you're looking for them, they're free to download, you just got to go to aws.amazon.com forward slash white papers. And I believe that they also have the hyperlinks in the actual exam Guide, which you can download from AWS as well. But you know, now that we've gone through that, let's actually open up the the white paper. Alright, so what I've done here is I've pulled up a dress.amazon.com, certification certified developer associate. So we can look at the exam guide, maybe the sample questions and a little bit more about this portal here. Just because there is really great information here for you. Before you go and take an exam, or even study, just make sure if any changes are happening. You go to here, a recertification go coming soon. I check this all the time, I have to watch it like a hawk. And it tells you when things are changing. So here you can see that the solution architect associate exam has changed. They have a bunch of information here. You can also take Beta exams, I think at half price off, which is here beta exams are offered 50% discount for standard exam pricing. I don't ever bother to sit beta exams. I don't know, I guess you get certified if your beta. I'm not really sure about that. But it's definitely not something I actually ever do. I always just wait for the official exams. But the reason I want you to check this is just to see, you know, are there any changes coming to the exam that you are interested in? And that might affect? You know, when you want to take that exam? Would you rather want to wait for the new one, or take the old one, or even in the case here, the big data one has been split into two certifications. So is it valuable to still get the big data one if you if there's a ellickson database? Will this big data make you look dated in terms of your certification, so just peek around there and consider that. But coming back to this page, you can see that we have recommended knowledge and experience. And then on the right hand side here, we can download the exam guide and download the sample questions. So here I have the exam guide open, I'm just going to zoom in a little bit. And the first thing I want to check is the actual course code, make sure that's the one that you want to study. It's interesting here because it says CEO one and then over here, it says dva 001. I don't know if that's a mistake or not. But whatever. Then down below, we have the recommended Eva's knowledge. And you'll notice this is exactly the same thing from the list here. So they're just copied and pasted it there. And then for exam preparation, they recommend a was training. So we have developing on AWS, an instructor led live or virtual three day course, I believe that's like 1000 $2,000. for registration. I don't know anybody that uses this. And really, it's for enterprise companies that have a lot of money. So there's a lot of like government programs where if you send someone for training, and the training is at $2,000, the government will reimburse like 75% of it. And that's why you have these really expensive training packages, which make no sense, would you think they'd be charged $100. It's just kind of that scheme there. I haven't heard good things about these whatsoever. But if you work for a very large company, and they're willing to pay for it, maybe you want to take it, then you have the ADA, a digital training on the training platform, these are actually Okay, so I would definitely consider checking them out as supplemental content to this course here. Then you have the white paper recommendations. And as you can see, they are all hyperlinked, I could open up this exam guide month over month, and this list will slightly change because they're always revising or updating these white papers. It'll be the same white paper, but the date will change here, maybe the title will change. And some some of the content will be revised. But again, for the developer associate, reading white papers is not a big deal. And you saw the recommendations I made in my list. And I like how they've actually now listed as documentation here, they never did before. But when you're going through my exam, all I'm doing is I mean, I've taken the exam, and I have a lot of practical experience. But I'm going through the documentation and highly, highly condensing it. If you were to study, just read it, it was documentation alone, you can definitely do it. But you know, you're looking at three, four times longer study, and you might be over studying on stuff that you should not be doing. But you definitely want to check out the database documentation could become extremely important at the professional level at the associate level, it can be a bit of a time sink. Then down to the actual exam content here you can see they're talking about those multiple choice multiple responses which we talked about there. Then there's unscored content, this happens sometimes, so sometimes you'll end up having additional questions. So like that will never ever be scored. And the reason why is that a device is always testing out new questions. So if you take an exam, and you get a question that is like so far out there, and you're really stressed out about you feel like you know, you didn't study well enough, just consider maybe it's just a test question. And you're not supposed to know the answer. There's always like two or three in there. So you know, don't get stressed out about that. Then we have the exam results, they talk about this point system, and there it is, it says 700 720. And then we'll go down below and we'll actually look at the domains and sub domain. So we see them again, deployment, security development, database services, refactoring, monitoring, and troubleshooting. And then here are our sub domains. So under deployment, we have deploy written code and eight of us using existing ci CD pipelines, processes and patterns. So that's why in my course, we're going to be talking about code pipeline code build, code deploy, and code commit, then deploying applications using Elastic Beanstalk. This is something we heavily heavily heavily covered in this in this course here and again. Have you really good follow along the 100% need to do is really good. Prepare the application deployment package to be deployed to AWS. This is kind of talking about Elastic Beanstalk in terms of preparing them. This could be preparing containers for deployment. or this could be just preparing artifacts that need to be deployed via code deploy. Then you have deploy serverless applications. Here, they're talking about Sam the serverless application model, or using cloud formation templates. Moving on to security. So make authenticated calls to AWS services. I guess that's just using the COI or API or SDK. I guess it also could be Eva's cognito. That could be sts tokens, implementing encryption using Ada services, that is absolutely 100% going to be kms. And we're probably looking at both in transit and encryption at rest. So also ACM Amazon certification manager, implement application authentication, authorization, authorization, so that is specifically going to be cognito. I set it up here, but this is definitely cognito. Knowing about web identity Federation, maybe I am accounts that we have development with Ada services, write code for service applications, so step functions lambdas, X ray API gateway, translate functional requirements into application design. I guess that's giving you like, functional, it's giving you scenarios and then you kind of like pick out what technologies you should use. That's not too complicated. Implement application design into application code. So they give you a design and you have to translate that into code. I don't know what kind of questions that would be on the exam. And I've written a lot of exam questions. So that one's a little bit vague for me write code that interacts with the AWS services, so API SDK COI, you definitely will see on the exam, like they'll show you COI commands. So in this course, here, I try to expose you to as many ci commands as possible. And even when we could use the console, I'm making us use a cry just so you get those COI commands etched in your brain and also get some experience with SDK, then you have refactoring. So optimize applications to best use Eva services and features. So that's just making the best choice based on the scenario, migrate existing application code to run on AWS. I guess it's like just deploying code. It's weird because it says migration. So I'm not really sure what they mean by that. And I've written I've written practice exam questions for all this. So I mean, I definitely know. But I, sometimes I lie every design, and we're talking about monitoring and troubleshooting write code that can be monitored. So Cloud watch X ray, there are new features in cloud watch for like cloud watch insights, is a cloud watch synthetics, there's like a new one for serverless applications, maybe cloud trail, perform root cause analysis on faults found in testing or production code. So this would be like knowing when a code deploy fails, and then it shows you errors, like how do you read it and understand what's going on what happens when cloud formation rolls back, and you have to investigate and go fix that, you know, logging things out into Cloud watch. So like, if you're using a lambda and something goes wrong, you know, to open up cloud watch monitoring route through the logs. So things like that. So that's the general breakdown of this exam guide. And hopefully, it gives you a bit of perspective of what is in front of us. So let's get to it. So just before we jump into the certification content, I want to remind you about the AWS certified challenge, and this was officially started by Free Code Camp. And the idea here is it allows you to get support from other people that are also on the same journey as you so you don't have to do it alone. And all you have to do is if you have a Twitter account, and you could do this on LinkedIn, as well. Tweet a photo of yourself thumbs up, announce that you begun the ADA certified challenge. Tweet your daily progress of what you learned, encourage other people that are taking the challenge as well. And when you earn that certification, print out and pose with it. And as an added bonus of there's also an official discord group that you can join. I think we're almost 1000 members there. I sit in it all day long. And I'm pretty good about answering questions in real time. And there's a lot of support there and other resources being shared there. So definitely if you want to maximize your ability to learn and get that support, join the discord. And also, you know, join the ido certified challenge on Twitter or LinkedIn. Hey, this is Angie brown from exam Pro. And we are looking at Elastic Beanstalk, which you can use to quickly deploy and manage web applications on AWS without worrying about the underlying infrastructure. And this is a platform as a service which we'll talk About here shortly. So to understand Elastic Beanstalk, we're going to need to know what a platform as a services or a pass, and that that is a platform allowing customers to develop, run and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app. So that's exactly what Elastic Beanstalk does. You upload your code, and everything just runs. If you want to give a similar service to Elastic Beanstalk. I always say think of Elastic Beanstalk as the Roku for AWS. And the way we abbreviate Elastic Beanstalk is with EBS. So yeah, there you go. So the idea is you choose a platform, you upload your code, and it runs with little knowledge of the underlying infrastructure on Elastic Beanstalk. If you read the documentations, on AWS, they'll say it's not recommended for production applications. But what does he mean by that, because I know a lot of startups and sizable companies that run their production workloads on Elastic Beanstalk. So they're talking to enterprise and large companies, you have to understand that eight of us is a very large company. And so their clients can be like mega corporations, or governments. And they'll think, Okay, I'm gonna run my my infrastructure on beanstalk. But it's just not the use case. So just take that with a grain of salt that you can run production applications, but AWS is just warning large companies not to rely on it. So Elastic Beanstalk is powered by cloudformation template templates. So when you spin up an Elastic Beanstalk environment, that's what it's doing. It's just a very fancy cloudformation template with a really fancy UI. So if you go over to the cloudformation console, you can actually see what it's provisioned. And you can go try to read through that cloudformation template. It's very complicated. But it's just interesting to see what it's doing there. And so lots of beanstalk can set up things such as elastic load balancer, auto scaling groups, RDS database, and it comes with pre configured easy to instances. And this is the big list of it. So you can see you can do Docker, multi container Docker, you have go Java, Ruby, etc. And generally, these pre configured platforms come with the common technology, you need to run certain frameworks. So if you're using Ruby, it's gonna be able to run rails, etc. Then you have monitoring. So it has cloud watch, and SNS integrated into its dashboard, which is really nice. It has in place and bluegreen deployment methodology. So you don't have to go out and build a complex code pipeline, which you could spend weeks doing so it already has it for you there. It can rotate out your passwords for RDS so keeps things very secure, and it can run dockerized environments. So if you are, if you are employing microservices, you can definitely use Elastic Beanstalk as your gateway to elastic container service, which is what it uses under the hood to run Docker containers. So let's just take a quick look here at the supported languages for Elastic Beanstalk. So we got go Node JS, Java, Python, Ruby, PHP, dotnet, and Docker. And I, you know, I said this previously, but I'm gonna say it again, these pre configured, platforms generally have the tools that you need, or the lot of the things you need to run the frameworks as well. So if you're going to be Ruby, you should be comfortable about running rails. If you're running the Python platform, you can run Django if it's PHP lorelle, Tomcat spring no Jess Express Yes. So just keep that in mind. So when you first create a Elastic Beanstalk application, you have to choose an environment and you are choosing between web versus worker. And so if you're you need to build a web application, it can be choosing a web environment. But if you need to run background jobs, you can choose a work environment, a lot of the cases when you're building web apps, you're going to make two environments, you can actually make both of these one web and one worker, and they're going to be interconnected together. But let's just talk about the components that are involved here. So we have a bit of an idea of how they're different. So on the left hand side, we have our web environment. And this comes into variants, which we'll talk about in another slide here. But the idea is that you have these EC two instances, maybe it's one, maybe it's multiples, and they're running an auto scaling group. And it also creates an elastic load balancer for you, which is optional. If you want to save money, you just don't have one there. And that goes out to the internet. So it's a very simple setup here on the left hand side. But then on the right hand side, we have our our worker environment, and this is again for background jobs. So you'd have your EC two instances, they would be in an auto scaling group. And you'd also create an Sq sq. So if you didn't have a queue, we create one for you, and it would also install the Sq s daemon on all Those easy two instances so that it can seamlessly communicate with the Sq sq. But it also has this other setup here, which cloudwatch, it will watch the amount of instances you have. So that if you're under capacity, it will spin up more instances and adjust the auto scaling group there. So that's really nice. So there you go. That's your web and worker environments. So in the prior slide there, I said there were two types of web web environments are two variants. So let's look at them. The first one we have already seen is a load balanced environment. And so the idea with this one is that you have easy two instances running an auto scaling group. But that auto scaling group is set to scale. So if you get a lot of traffic coming in, it's going to spin up more instances. And then when the traffic declines, it's going to remove instances. So that means that there could be a variable cost based on traffic, you have an elastic load balancer there. And that's where traffic is coming into the lb. So the other case is you can set up a single instance environment. And so this one is extremely cost effective, because you're only running a single server, but you still are using an auto scaling group, because auto scaling groups are great for keeping, not just for scaling us to add servers, but just to keep you in a single server running. So the desired capacity is always set to one, there is no elastic load balancer. And that's just to save on costs. But with no EOB, that means that there's going to be a public IP address that is used. So if you have roughly two, three, it's going to point to that IP address where in the load balance environment, it's going to be pointing to that load balancer. So there you go. So I was saying earlier, that Elastic Beanstalk comes with deployment options built in. And this is definitely going to save you a lot of time. So you don't have to set up your own code pipeline. So let's talk about some of the deployment policies that are available with Elastic Beanstalk. And so what we have is all at once rolling, rolling with additional batch and immutable. And we're going to be walking through every single one of these. But based on what deployment policy choose, they're only available for specific web environments. And so you can see for rolling and rolling with additional batch, they're not available in single instance environments. And the reason why is that is that you need a load balancer in order to do that, because the load balancer is going to attach and detach instances, in batches from EOB, what you're going to notice is that there is no mention of in place, or blue green on this list. And the reason why is basically this entire list is in place. But we're gonna explain in place in blue, green, the context of it. So that makes a lot more sense. Because even myself, when I first started learning this, I was like, Okay, we have this list of words in place in blue, green. So hopefully I can clear that up for you. So let's take a look first at all at once deployment. So the first thing we're going to do is we're going to deploy the new app version to all the instances at the same time, then we're going to take all the instances out of service while the deployment is processing, and then the servers are going to become available again. So this is the fastest, but also the most dangerous deployment method, it's fast, because it's doing everything at once. It's dangerous, because you're taking those instances out of service, meaning that your services are going to become an unavailable. And also you're you're applying updates to all your instances that exact same time. So if you have a major failure here, do you might have to rollback your changes. But if that rollback fails, you'll have all these instances that are in a broken state. And you'll have to deploy the original version again. And it can get kind of messy. So that is all at once deployment. So now we're going to look at a rolling deploy, it looks really complicated, but we just need a big graphic in order to explain this. So the first thing is we're going to deploy the new app version to a batch of instances at a time. So whereas all at once was all of them, this one is just like we'll do two at a time, right. And so if we have four servers, we're gonna do two. So we take in batches, those instances out of service. And then once that, once those are good, we're gonna reattach them with the update instances, we're going to move on to the next batch, and so forth and so forth until we've gone through all the servers. So you can see that that mitigates some of the problems with all at once deploy, it's going to be definitely a bit slower. But you know, if we need, we might need to perform additional rolling updates in order to rollback the changes. So rollback could be still pretty darn complicated. There. Let's take a look at rolling with additional batch. So the idea here is that when you start to deploy, you're going to spin up new servers. So if you're doing it in batches of two or whatever size, instead of taking a batch out of service, we're just going to add new servers. And then we're going to apply our app version there. And once those are good, we're going to then terminate our old instances or, or another batch. And the idea here is that by doing this, we're never going to reduce our capacity. And this is important for applications where a reduction in capacity could cause availability issues for users, because we saw with rolling, we will have reduced capacity for a short period of time, in this case, we'll never have a reduced capacity. But you know, we still have the same issues where if you are, if you want to do a rollback, you're going to have to perform an additional rolling update. So roll backs is still quite painful in this case and slow. So let's take a look at immutable deploys. And this one, it really is reliant on the auto scaling group. So over here, what you can see is that we already have an elastic load balancer that points needs to do instance, that's inside of an auto scaling group. But what we're going to do is we're going to make a new auto scaling group with a single EC two instance in it, or whatever, however many servers we need to replace. And then the next thing we're going to do is we're going to deploy the updated version of our app on the new AC two instances in that new auto scaling group. And then what we're going to do is we're going to point that elastic load balancer to the new ASC. And then we're going to delete the old SGX, which is going to terminate all the old instances. So the reason why you'd want to do this is that this is the safest way to deploy critical applications. And when you want to roll back, if we just go over here, the idea is that you don't have to destroy this auto scaling group immediately. You can wait until this new production and auto scaling group is running smoothly, you could wait days or weeks however long you want. And then you could destroy this old auto scaling group. Or if you had to roll back, you could instantly move back to that auto scaling group because all the infrastructure exists. So, rollbacks are really easy. It's super safe, you know, but there's not a lot of downsides to it. I mean, this is the one I would choose to do. So we're going to take a look at deployment methodologies for Elastic Beanstalk. And you're going to notice that down below, I have blue green, so we never covered blue, green as of yet. So these have all been immutable all the way to all at once I've been deployment policies, these are built in deployment methods into Elastic Beanstalk. But we have blue, green. And we'll explain the difference between in in place and blue green in the next slide here. But let's just compare these methodologies and understand what the trade offs are because this is definitely important on the exam. So the first one is all at once. And it has the fastest deploy time because it updates all the servers at the exact same time. So if we have four servers, it takes them out of service applies the updates and puts them back into service. But when they're out of service, we're going to experience a downtime. And downtime could be a bad thing. If your users notice. And that could impact their experience, or if they're doing serious or series are critical transactions, that could be a problem. So you have to decide whether that is a trade off you want to take but most people do not want to use all at once. And also if you encounter an error, let's say all your deploy fails, and you have four servers, now you have to roll them back manually. And so that's kind of a pain. But also imagine if you encountered an error during rollback, so the rollback fails, and now you have four servers all stuck in a broken state, that could be extremely detrimental to your business, losing hours upon hours of time, so you have to weigh those trade offs. Now, next, we have is rolling. And so rolling, mitigates this downtime problem where we have instances that are out of service. So what it does is, so if you have four servers, like the previous case, it's going to update these in batches. So it's going to take the first two and take them out of service, right, and then put them back into service. And then it's gonna move on the next one. But the trade off, there is still a downside, which is we're going to have a reduced capacity. So if you have to have always four servers to run your critical workload or to just handle the current usage, this is not going to be ideal for you. So what you're going to want to do is you're going to want to use rolling with additional batch. So rolling with additional batch is very similar to rolling. So it's going to work in batches, but instead of taking a batch out of service, it's going to add a new batch. And once this one is good and running, it just kills an old one. And so this way, you always have at least the minimum amount of servers you need running to meet your capacity needs. But you know, rolling back for these methodologies, so can be difficult, it's still a manual process. And you can just imagine having to roll each section back can be painful. Imagine that you're rolling this back, and then this part, this rollback fails. And now this one's stuck into the state. So you have a weird number of servers messed up here, this can be extremely difficult. So the last one here is immutable, immutable, the idea is you just replicate your entire environment. So like all at once, you know, if you had four servers, it would take them out of service with immutable, it would just create four new servers. And once they're all good, then you could move over to those four new servers. And if you had a rollback, you just point back to the old ones, because they still exist, because they haven't yet been deleted until you decide to do so. So immutable gives you the best flexibility in terms of the rollback process. It can be more expensive, depending on how long he goes servers around the provisioning time takes a long time, the actual switch takes very little time, because it's very fast to rollback or switch to the new version. But to provision those servers take a while because you're replicating everything at once. And before you can actually start routing traffic to it, because with rollbacks, you could start with rolling, you can start routing traffic to new instances, gradually remotely. Whereas mutable, you have to wait till all the servers are ready. But I you know, I think immutable is extremely, extremely safe, and a good a good deployment methodology to use. And then last, we have blue, green, and blue green is very, very similar to immutable, where it will replicate the all the servers. So if you have four servers, it's going to make four new servers. But also, it could spin up other infrastructure, like elastic load balancers and stuff like that. But that these are super similar. And that's why we have another slide to talk about them to understand when something's in place versus blue, green. But just before we move on here for Elastic Beanstalk, the bluegreen methodology uses a DNS way of doing bluegreen. So that means that when you want to move over to these new servers, it refers to three points to a new load balancer. So there's a new load balancer with new instances on it. And so the change is happening at the DNS level. And the reason why this can be a negative is that DNS, DNS changes have to propagate to DNS servers around the world. And the effect that could happen is that there could you could have this new production server ready, but people are still being pointed the old one or it's going nowhere. And so even though the server's not down, there could, some users could experience unavailability, just because you know, they're being pointed to the wrong thing. So that is a consideration you have to think about, though, for Elastic Beanstalk. It's not generally that bad, but it does happen. So we have to consider that as a negative. But now that we know deployment methodologies, let's move on to the comparison between in place and bluegreen. So let's take a look at inplace versus bluegreen. Deployment here. And these terms are confusing because they're not definitive in definition. And the context can change the scope of what they mean. So it's important to learn these not just for Elastic Beanstalk, but for DevOps in general. So we're gonna spend a little bit time here making sure you really know this stuff inside and out. So Elastic Beanstalk, by default, performs in place updates. And that's all the deployment policies we've been looking at, they've all been considered in place. But let's change the context to see how that affects the scope, which will change what is considered in place. So the first we're going to look at, which we're already familiar with is with the Elastic Beanstalk environment. So when that is our our scope, that means that all the policies we saw prior all at once rolling rolling with additional batch and immutable are considered in place deployment methodologies. So let's say we change the scope to outside of Elastic Beanstalk. And now it's just servers. So we have these servers. But what's really important is that these servers are never replaced. So they always have to be the same servers, the existing servers. So that's our scope. And now the only deployment methodologies that are available to us is all at once are rolling, which are considered in place because they never replaced a server, they'll take a server out of service and make changes and put it back into service. Now, that doesn't mean that rolling with additional batch immutable would be considered bluegreen. They just wouldn't be considered in place for the scope of that scenario. Let's set up another one where the scope is now we have a server that can never be interrupted. So Doesn't mean like, that means we can't replace the server with a new server. But that also means we can't take it out of service. So it should never lose traffic, it should always be traffic should always be pointed to it. And so to solve this, we use zero downtime deploys. This is where bluegreen occurs on the actual server itself, like in a virtual sense where you, you have your code base, and you deploy the second version of the code base on the server, and you facilitate the change the switch within the server virtually. And you can't do this on Elastic Beanstalk. I used to do this for years with Capistrano, Ruby on Rails and unicorn. And this allows for deploys that happened within minutes. So I like in 30 seconds or one minute, I'd have the latest version updated, it was amazing. But when we have to consider all these cloud components, you know, this, this kind of agility is has been lost. But we get other kind of trade off to it. So you know, that's the stuff that we're looking at. But when we're talking about the exams itself, and they're talking about in place, they're going to be talking about generally in the context of Elastic Beanstalk environment. So I just want you to know, the different the differences of this, but for the exam, this is the one you're going to focus on, for in place, okay. Okay, so this is the slide where everything is going to fall into place. And you're going to really understand the difference between in place versus bluegreen deployment. So this is this is going to be in the context of Elastic Beanstalk. And we're first going to look at an immutable deployment methodology. So we know the immutable that what happens is, it replicates the auto scaling group with another easy to instance, and then it facilitates the the transition to the new production servers by switching over to this auto scaling group and then destroying the old one. Now, this is funny, because in our last slide, we just saw that there was a blue green methodology called EOB, blue green. And this is exactly what it did. So why isn't this considered blue, green, and it's called in place. And that has to do with the the boundaries of the environment. And so the environment is defined here as being the Elastic Beanstalk container. And so because the mechanism of our deploys inside the boundaries of this environment, it's considered in place because it's inside of the environment. So now looking at a bluegreen deploy, for Elastic Beanstalk. This can only occur at the DNS at the DNS level. So it's roughly three that's facilitating it. But notice that it's outside of the environments, it's outside of it, it's going from a blue environment to a green environment. And so that is why it's called Blue Green deploy. If it's all within the same environment, it can't be considered blue green. So, you know, talking about doing a DNS, like having the DNS level facilitate the the switch of servers, or having the load balancer, I was saying the load balancer was a lot better. Because with DNS, we could have an interruption in service, not because the servers aren't ready, but like the DNS servers have to propagate the changes. And so there could be some unavailability for servers. So why would you still use this, then if this one is better, this has to do with where some of your external resources lie. So in the context of Elastic Beanstalk, it really has to do with your database. So if you are using inplace, deployment, a lot of them are very destructive. So your your database would be inside of the environment, it would be running on an EC two instance, generally, and, you know, it would get terminated with the environment. And so you'd lose your data. So that would not be good whatsoever. So your database would have to sit outside of your environment will like on RDS. And so you know, the better place to do, it would generally be with a bluegreen deploy with your database outside of it. So that is generally the reason why we use blue green. That doesn't mean you can't use RDS with in place. But generally it's more for bluegreen deploy. So hopefully that clears up a lot of stuff. And you can understand and visualize those boundaries to see when something's in place and when something is not considered in place. So if we want to change the way our Elastic Beanstalk environment works programmatically, then the way we're going to do it is through configuration files. These configuration files sit in a hidden folder called Eb extensions at the root of your project. And they're going to have an extension that says dot config on them. So there's going to be a variety of different ones. But that's what Osip Bienstock expects. And what we can change to these configuration files is the option settings for our initial environment. We can do things very specific to Linux and Windows. And then we can also set up custom resources. So if we need other services to integrate, that's what we can do there. But the motivation of having this file here is that let's say we want to hand this project to somebody else, they can just provision all the things they need, with the exact configuration they need. So this is something you'll definitely run into. If you're working on Elastic Beanstalk, you'll have to do a little bit of configuring. So I said that, you could configure some options settings for the Elastic Beanstalk environment. And that's called the environment manifest, which is in a file called the E and V dot YAML. And you're going to store that at the root of your project. So this file is important because when you first create your Elastic Beanstalk environment, it's going to look for the style and set up a bunch of defaults. So this is the way that you're really sharing that configuration with other people or just, you know, saving that configuration for yourself. So what I've done here is I pulled out a little example. And we're just going to look at some of these attributes here. So the first one that we're looking at is the environment name. So that's whatever you want the name to be. And we'll talk about what that pluses in a moment on the end there, then there you have your solution stack. So that could be Ruby, Python, Java, whatever. But that's going to be what ami is chosen. Then you have environments, links, and this associates to other environments. So we saw that you could have a web environment, and a worker environment. And so this is a way to connect them together. And then you have some default configurations for specific AWS services. So here we are setting a load balancer to be cross zone. And there is a lot more options here. But we don't need to go through all of them, you just have to generally know that you can do this stuff. And let's just talk about that plus there on the end. So that plus is used to add the environment name to the end there. So to give these more unique names. So that's all that thing is for. So there you go. So now we're going to take a look at Linux server configuration, there is one for Windows server configuration. But I think we only need to really learn one here, because you can pretty much apply this to both. And that will be good enough for our studies. So the first thing you could configure is being able to download packages. So here I have Ruby, or we have mem cache, I think the package manager will generally be young, because that's what AWS uses for both Amazon is one Amazon likes to if you can use some kind of other OSS that might change to something else. Then you can set Linux Unix groups, something I don't do, very often, but something you can configure, then you can also configure users and assign them to Linux groups. You can also create files or download files from the internet using a URL, that URL is for public facing file. So I don't think there's a way to download private files. But for the content, you just specify content and provide what you want. There, I'm providing a yamo file, then you have commands, these are commands that you want to run before your application has been installed. So Elastic Beanstalk polls, your code base, but this is happens before that code is actually in the environment, then you have services. So maybe you've installed nginx, and you want to ensure that it is running, when it starts up it continuously run. So for whatever reason it shuts down, it will try to make it start up again. And then the last thing our container commands. And so these are commands that are are specific to your application. So after your application or your source code has been downloaded to the environment, these are what you want to run it. It says container commands, which make you think that these are for Docker containers, that's not the case. So just be aware of that. But you know, for for Windows, you're going to have similar ones who have container commands, commands, probably something for packages and files. So it's more or less similar, just as long as you conceptually understand the things you can set at the server level. So Elastic Beanstalk has a COI which you can install. And that's going to give you more of a Heroku like experience and going to know some of these commands are very similar to Heroku commands. So to get the COI, you need to go to GitHub and just had an install it. So it's just as simple as cloning the repo. I believe this is using Python. And so you just execute that command. If you're on a Mac. This should just work for other systems. You're gonna have to read a little bit more on the GitHub page itself, which you can see is that the universe Elastic Beanstalk COI set up there, but let's just run through some of the commands that are available to us starting with Eb a net so this configures your project directory in Elastic Beanstalk COI This is the first thing you're going to want to run because it's going to set up a bunch of defaults on your computer. And if you don't want to make a project, you just delete that project afterwards. But you still want to run this. When you want to create an environment you're going to be doing Eb create. When you want to check the status of the environment, you're going to do Eb status. When you want to check the health, you can get health on the particular instances and the overall health of the environment. If you want to get that in real time, or near real time, you can do a refreshable update every 10 seconds. If you need to see a bunch of events that are being output outputted by the Elastic Beanstalk environment, you can run that if you want to see a logs from the actual instances yourself, you can run Eb logs, if you want to open up the the application in your browser, there's Eb open, but it's really not that hard to do, you could just go to your browser, when you're ready to deploy your current version of code. You do Eb deploy, if you want to see what kind of configuration setup you have for that environment, you know, seeing whether it's running Ruby or other options, you can check that as well. And if you want to terminate that instance, because you want to save money, you can do Eb terminate. So, you know, go ahead and download the CLR and give it give it a go. So we're gonna look at how you can use your own custom image in Elastic Beanstalk. And this is where you can provide your own ami instead of the standard Elastic Beanstalk ami of your choice. The reason why you'd want to do this is because it could improve provisioning time. So if all if you have a lot of software like packages, you need to install to run your software, it takes a long time to pull all those if you bake it into an ami, it's just gonna speed things up because it's already there. So let's go through the process of how you'd actually get a custom image. Because it's not hard, but there's a lot of steps to it. So the first thing you want to do is go to AWS docs, there's a page called like supported platforms, and you get a list of all the type of standard Elastic Beanstalk kaomise. And what you're doing is you're trying to get the platform information there. So that you can then use the COI to use this command called describe platform version. And so by using describe platform version, and you can see over there, it has the platform name in it, it's going to get us back an image ID and that image ID is the the AMI ID so that we can then find the AMI and then extend that ami to do what we want with it. So using that ami Id go to the EC two marketplace, in probably into the community section and paste in that ami, so we would find the AMI we're looking for. And then we would launch a new EC two server. Once that easy two server is launched, we could then log into it either using SSL or sessions manager, I would suggest sessions manager to do so you would need the correct Im role attached to it. So just be aware of that. But once you get into that machine, you configure it however you want. So you'd go in and you install those packages manually. And then you would go ahead and bake that ami and so you'd have your new ami. So once you have the new ami, you could go in your configuration like your files, or you could even do it through the console and set the new ami ID. And then when you create new environments, that's what it's going to use. So that is the whole process to setting up a custom image. So we need to talk about configuring your RDS database with Elastic Beanstalk because you actually have two options, you can add a database inside or outside your Elastic Beanstalk environment. And you might not even be aware that you're doing it if you're setting it up. So it's important to know the difference. So let's talk about inside Elastic Beanstalk environments. So when you go to create an environment through the console, you'll have the option to create a RDS database. And if you are doing that, that means it's going to be within the Elastic Beanstalk environment. Now, the thing is if you do this, that means whenever this environment is terminated for any reason, it will take out the database with it. So that means that generally this setup is for development environments. That doesn't mean you can't use it for production. Because as long as you're using in place deployment mechanisms, so like let's say use immutable and stuff that's gonna replace the EC two servers, it's never going to remove the RDS database but if you for whatever reason, delete that entire environment, your database is gone with it. Then on the other side you have outside the Elastic Beanstalk environment and the way you know you're doing this as you're creating your database first and RDS. And then you configure it with your EC two instances that are in your inside your Elastic Beanstalk environment. Now, when the Elastic Beanstalk environment is terminated, the database is going to remain because it wasn't created part of the EB environment. And so these are generally suited for production environments. And with this setup, you know you generally are using bluegreen deployment. You don't have to but you totally can. I just want you to know that distinction. Vote Inside and outside the environment. Hey, this is Angie brown from exam Pro. And we made it to the end of the Elastic Beanstalk overview and that means it's time for the cheat sheet. So let's review. Elastic Beanstalk handles the deployment from capacity provisioning, load balancing, auto scaling to application health monitoring. So it really sets up a lot of infrastructure for you. It's a good time to use Eb when you want to run a web app. But you don't want to have to think about the underlying infrastructure. And we just saw this big list of infrastructure above. It costs nothing to use Eb only the resources that provision so if it spins up RDS, an E lb CT, you're gonna be paying for those but EDI itself costs nothing recommended for tests or development apps not recommended for production use. Remember, when AWS says this, when they say not for production use, they're talking about super large enterprises who think that they can use Elastic Beanstalk for the production environments. If your small to medium business Elastic Beanstalk is a okay. You can choose from the following pre configured platforms we have Java dotnet, PHP, no GS Python, Ruby go and Docker. You can run containers on Eb either in single container or multi container mode. These containers are running on ECS. Instead of EC two, you can launch either a web environment or a worker environment. Web environments come in two types we have single instance at or load balance for single instance environments, it launches a single EC two instance and assigns it an elastic IP address to that EC two instance, for a low bounce environment, it's going to launch easy to instances behind nlb managed by an auto scaling group I didn't mention in the last one. But for the single instance environment, it isn't an auto scaling group as well, it's just set to one set to design capacity of one. But I don't that's important for the exam, it's not a big deal. Then you have your work environments, this creates an SQL queue installs the SQL daemon on all these instances has an auto scaling scaling policy, which will add or remove instances based on the queue size. Then we have Eb has the following deployment policies. So we have all at once. So this takes all the servers out of service applies changes, put servers back in service, this is super fast and has has downtime. So that is one condition you have to think about. Then you have rolling update servers in batches reduce capacity based on batch size, rolling with additional batch adds new servers and batches to replace the old never reduces capacity, then you have immutable creates the same amount of servers and switches all at once to the new servers removing old servers, you really really need to know these deployment policies inside and out. So make sure you know the difference. And if you don't know, go back to the lecture content. Look at those diagrams and make sure it clicks. And then we're on to the last page here. So rollbacks rollback deployment policies require an EOB. So rollback that it should be called rolling. So I meant to write rolling update this here, but for the video, it's just gonna say rollback. So rolling deployments rolling or rolling with additional additional additional batch policies requires an E lb so it cannot be used with single instance web environments. Just consider that in place deployment is when deployment occurs within the environment, all deployment policies are in place. Blue Green is when deployment swaps environments outside an environment. When you have external resources such as RDS which cannot be stored or destroyed. It's suited for bluegreen deployment. Eb extensions is a folder which contains all configuration files. With Eb you can provide a custom image which can improve provisioning times. If you let Elastic Beanstalk create the RDS instance that means when delete your environment, it will delete the database. The setup is intended for development and test environments. Really do consider that. And the last thing here is Docker at most JSON is similar to ECS task definition file and defines multi container configuration. So yeah, if you looked at a task definition, you'll understand it. We don't have to go through the the the guts of that here. But this is generally what you need to know for the exam, but really know those deployment models, okay. Hey, this is Andrew Brown from exam Pro. And we are going to start with the Elastic Beanstalk follow along we're going to look at how to deploy Elastic Beanstalk a variety different ways. So we know it inside and out. I want to point out first, before we get started here, make sure you are in the correct region. And we always do everything in US East one, because that's where the most abundance of Ada services are available. And it just makes things a lot easier. So just go up here and make sure you're in US East one. And be very careful because 80 of us likes to switch out that region on you sometimes. So if you feel like things aren't going the way that should be going just double check your region. So now that we have that out of the way, let's go ahead and make our way over to cloud nine because we're going to need a developer, a developer environment to run and test our application and then go ahead and take that over and deploy the Elastic Beanstalk. So I'm gonna go ahead here to cloud nine. I don't have Any region or environments created here, so we'll go ahead and create an environment. I'm going to name this Dev, n, e and V was, which is developer environment here saying, not to use root account, I'm definitely not logged in as the root account. So I'm not sure why I'm getting that message. But we'll go ahead here, hit next. And what we're gonna do is we're gonna make sure this is a TT micro, that's part of the free tier eligibility, we'll scroll down here we have the choice between Amazon Linux and Ubuntu. Amazon Linux one is supposed to be unsupported at some point, because they want us Amazon Linux to. So if you're watching this in the future, maybe Amazon Linux two will be here, you'll have to use a bun two. But if Amazon Lex one is here, absolutely use it because it is amazing. We're gonna leave the default cost saving settings here to 30 minutes. So if we're not using if we don't have any activity, or we don't have the browser open here, it will shut down the server save us money. It looks like it wants to create an IM role, we'll let it go ahead and do that, we'll go ahead and hit next. And down below, it has some best practices for us. And just shows us a confirmation of what we're creating. This is all great. So just hit Create environment. And we'll just have to wait here a little bit. And I'll see you here in a moment. Alright, so our cloud nine environment here is ready. And just before I get started, I like to use the dark theme. So I'm gonna just switch it down here to the classic dark theme. And I also like to use vim, I would recommend just using the default, but vim is what I use, that rebounds all the keys for Super efficiency. So you know, it's just because I've been doing it for years. But anyway, now that we have our cloud nine environment, let's actually get an application going here. And since this is a very developer focused, I think we should try to use the terminal as much as we can to get as much experience as possible. The first thing I want you to do is I want you to type in NPM, i c nine hyphen G, so C nine, which is short for cloud nine. This is a Node JS utility that makes it easy to open files directly from the terminal here. So you know, we have this README file. But also just noticed that C versus environment, this actually maps to this dev at ebn directory, if I hover over there, you can see it'll autocomplete to that. So I don't know why CLOUD NINE does that. But that's how they name it. But anyway, I just want to show you how c nine works. So we have a readme in here. And if I just wanted to open it up, it actually is I think open right there. But if I just typed in LS, and then I typed in C nine README, then it would open up that README file. So that is going to give us a little bit of help along the way. So now that we have seen I installed, let's go set up the actual application itself. And so the first thing we're going to do is we're going to type in MK dir, which makes a new directory, and we want to make that in our environment. So I'm gonna use Tilda to make sure I'm always at home, I'm going to type environment study, sync is the name of the application we are creating today. And you can see up here that it created a folder. Okay, I will go ahead and we'll just create some additional files. So I'm just going to CD into that folder to save myself some trouble. And the first thing we need to do is initialize an empty our node projects, we'll do NPM and knit hyphen y. Okay, and what that did is created a package dot JSON for us here, which we will adjust momentarily. But we want to run a web app. So we're gonna need some kind of web framework. So we're gonna go ahead and use Express. Okay, so we'll go ahead and type that in. What's, what's that going to do, it's going to add it as a dependency there. So now we can use Express. The next thing we want to do is we're going to need some initial files to work with here. So I'm going to type in touch, we're going to type in main.js, we'll probably need index dot HTML, actually not instead of main, we'll call it index. I think actually, I normally call it index. Then we will have index dot HTML app.gs and style dot CSS. And so that created all the initial files that we need to work with. And so now we just need to populate those files. So the first thing I want to do is I want to have a way of actually running our application here node. So we're going to add a new script up here called start. Okay, and we'll just type in Node main.js actually going to call that index there. And the next thing we're going to do is we're to start populating this file. So if you make your way over to the GitHub, and you go to exam Pro, co the free database developer associate, there's a folder here called study sync 000. And these are the files that we're going to copy on over. So the first is the index.js. So we'll go there and just hit raw. And we'll copy the contents here. And we'll double click on that and we will just paste it. Then we will click back here and we'll go grab the styling. Okay, we'll hit raw It's not that important for you to know how to program. But I mean, you know, we need to get as comfortable as we can here. So we're not going to really need to learn all this stuff that we're doing. Just copy paste it through. And we just need I think, the app GS file, yep, the JavaScript file here. And we will grab all that data. And so those are the three files we need. And just give you a very quick tour of what's going on here. We have this index dot HTML file, oh, I guess we didn't populate that. Okay. Give me two seconds here. Sometimes you think you do something and you don't. So anyway, the index HTML file loads this style dot CSS file, which is located there. What we're doing is we're using a CDN to polen mithral, which is a JavaScript front end framework, we are going to use App JS to load our JavaScript. Going over to our JavaScript here, we're using the mythical framework. So it's very simple, we have this app here. And the idea is it's we're going to have a question, and we have multiple choices, and we can submit the answer somewhere. And then we just have some plain stallion CSS. So now that we have that all going, the next thing we need to do is actually preview this application, because before we can deploy it, and package it, we need to make sure that it is working here. So I'm just going to go ahead here and close these tabs. And there's just going to be a couple things that we need to do next. Okay, so what we need to do is we need to get our application running to make sure that it's all in working order before we go ahead and package it. And so we can preview in cloud nine, but cloud nine, by default doesn't open up its ports to the internet. So we have to go ahead and do that. This would be no different than you setting up a web app on an EC two instance, you'd still have to open up ports. And so generally, the ports that clubnight allows out to the internet is 80 8080 8081, and 8082. So what we'll do, I just want to show you how you normally do this. So you go here, and you go to easy two instances, you go to instances on the left hand side, and we find the one that's running, that's our cloud nine environment. And over here, we're going to find security groups. And if we expand that, and we check inbound rules, we go here at it, and we just add 8080, right. And then we restricted to our IP, this is a development environment, oops, I gotta hit plus there. And but we're not going to do it this way. Because I want you to get as much programmatic experience because this is a developer associate. So we're gonna figure out how to do it completely from the terminal. So we're gonna do the exact same there, I know that this is much faster, but like, trust me, this is gonna help you in the long run for studying here. So let's get to it. So what we're going to do here, I'm just as I play here, so my screen is nice and clean, is we need to figure out what the MAC addresses for the CC two instance. And then we use that MAC address to get the security group IDs. And then from that we use the COI to update and create our own inbound rule. So whenever you want to get information about an easy to instance, that is where the metadata service comes into play. And it's very easy to access on a server, whether you're here in your SSH into an EC two instance, or you're here in cloud nine, you just type in curl, hyphen s, HTTP, colon, forward slash forward slash, and then it's 169 254 169 254. Latest and metadata. So here, it's showing you that there's a way you can get a lot of data in here, this one is going to be for, you should know this IP address, it should be etched into your brain, because it's definitely a standard here when working with EC two. And as a developer, you need to know it. But we need to find the MAC address. And here we have this and it says Mac. So we'll go ahead here and type in Mac. And now we have the MAC address. And the next thing is we're gonna use this MAC MAC address to find out all the security group IDs for the network interfaces that use that Mac. So what we'll do is we're going to hit up and we're just going to back out here a bit. And we'll do network. And we'll just piecemeal it because if you make the whole link, sometimes it's a big pain in the butt. And it's hard to hunt down the problems. So I'm going to just keep on doing this bit by bit. Oh just shows the MAC address that's even more convenient. And we'll just hit Enter. And then we want our security group IDs. And there that's so we only have one security group if there was multiples we like attaches easy to instance, we probably see more, but we only have this single one here. So now that we have this security group ID what we'll do is we will use The COI, the COI in vcli is is already installed on this cloud nine environment because it's Amazon Lex one already comes pre installed. And CLOUD NINE also loads your credentials from your user account. So we don't have to play with the credentials file here. If you're doing this on your local computer, you absolutely would have to set that up, we'll type in AWS EC two. And we'll type in authorize security groups. ingress and there is a new intimacy ally and it has autocomplete. So like you could hit tab and it would complete that stuff there for you. But I don't believe I have the latest one installed here. So I have to do things manually. And so we'll place in that security group will say what port we want to open up. So Port 8080, we want we'll have to specify the protocol, it's going to be a TCP. And then we need to supply the cider. So that's the IP address that we're going to want to be accessible. So before we hit enter here, we actually need to go get our IP address, because that's what we're going to put in here. So what we'll do is use one of AWS services, which is called check IP. So it's a very useful service. Let's go use that now. So I just open up a new tab here, and I'm just going to type in. I'm going to type in check IP, Amazon AWS COMM And this will tell me what my IP address for my local computer is. There's other websites like what's my IP, but let's use the AWS service, because they took the time to make it for us. And we'll go back over here. And we will hit enter and we want forward slash 32. The forward slash 32 is very, very important. Because that says only a single only a single IP address, a cider cider block is a range of IP addresses, something we definitely cover in this course. And you definitely need to know what networking like cider blocks are in the associate. But for the time being if you don't know what it is just understand that you need to put your IP address in there and type in forward slash. And what we'll do is we'll go hit Enter. And actually, before we do that, no, no, we'll just hit enter, that's fine. So it didn't show us anything. So I mean, I believe that successfully created it. So we'll go over here and just take a look here to see if it actually made it. And there it is. But let's say that we didn't want to make our way over here. And we want to do this pragmatically, let's go confirm the security group, through the COI. So what we're going to do is have an a to s EC to describe security groups. We're going to put in that group ID, it takes a bunch of them so but we only need one here. And we'll I'll put it this as text allows us to generally default Jason, but that's just really hard to read in this case. And then we're going to use filters. So we'll type in filters name equals IP, hyphen, permission.to, hyphen, port, values, equal 8080. So what I'm saying here is describe all the security groups to me, and filter it out. So we own or only select this security group, display this text, and then filter it out so that we only see inbound rules that have Port 8080. We'll hit enter here. And we have a invalid command there. So I'm just going to double check here. I might have typed something wrong. permissions does not look spelt correctly to me. Yeah, it's gonna be P er permissions. And I'm still having a bit of trouble here. IP hyphen permissions to port. Oh, you know what it's a singular permissions is not with an ass, I think. There we go. So it's a little bit hard to read. But the idea here is that it's going to say this is our security group that returned it saying that Port 8080 has been setting that's the IP address. Now, if it hadn't been set, and we ran this command, it would just show nothing. So the fact that something shows up here means that that these that that inbound rule was created. But of course, in practicality, you probably just use the console. So now that we have, we'll just type clear here to clear this stuff up. So now that we've opened that, that port, the next thing is actually getting the application running. But before we can even do that, we need to know what the actual IP address of this cloud nine environment is. And I'm pretty sure if we use our EC two instance here, we can go here, and we can go and check it and that is its public IP address. I think that'd be the same thing. But let's again, do it programmatically. So we'll type in curl, hyphen s HTTP, colon forward slash forward slash 160 9.2 54.1 69.2 54. Latest metadata. I'm gonna hit enter, just so I'm not having too much trouble here. You can even see right here. It's public ipv4. So we'll type in public, ipv4 and it says 385 910. I'm gonna go back here. Yep. So it's the same one there. So that's what we're going to use to access the web application. And let's go ahead and actually start this application up. So to start it up, we just have to make sure we're in that study sync directory here. And we want to start it up on port 8080. So if you're wondering, like, why are we typing Port 8080 there, the way this application works, if you open up the index dot j, s, uses process AV port, and it's going to pass that port number to express. So it knows to start up on that port number. And so what we'll do here is we'll just go and type in NPM start. And we'll see if this starts up. And we have a little error here. And that's totally okay. It's it's failed to parse package JSON data, you know what happened, we forgot to comment, something I do all the time. So remember, we wrote this line in here, we have to just make sure there's a comma on the end of it, or it's not valid JSON. And I'm just gonna go back down here, hit Ctrl C, which kills that there. And we'll hit up again, we'll see if we're in better shape. So now it says that it's launched on port 8080. So what we can do now is get that IP address that we have earlier. So it is somewhere in here. Um, can't seem to see it. So I'm just going to go here and copy this here and make a new tab in Terminal and just paste it in. Of course, we could just go up to instance. But why do that when we can try to do the proper way. And so what we'll need to do is do this and say, Port 8080, we're not going to open it up here. But I'm just typing it out so that I'm having less trouble here. I will just copy that. And we will see if this works. And there's our application. So subrogation doesn't really do much, you can select something and submit but it doesn't really submit to anywhere, maybe we will start hooking this up and do more with it. I call this the study sync application supposed to help you study I guess, um, but you know, it's just a superficial application. So now that we have our application, running, and we can preview it from Cloud Nine, and we have some COI experience, the next thing is to get a git repository set up. So that's what we'll do next. Okay, so let's get going here. And I'm just going to go ahead and close this bash command will go back here, and we'll just stop the server, I'm doing that by typing Ctrl C on my keyboard, you can see that it says Ctrl C, this little icon represents control. And we're going to need a dot Git ignore file because there's files that we just do not want to include. So up here, we have no modules, this is how node works. They just put all the libraries in line. And we do not want that in our Git repo. That's just too much stuff. And so with git, there's a file called dot Git ignore. And what we'll do is we'll just make sure we create it here in our study stink directory. So just make sure your environment study sync, if you're not sure where you are just type in environment, studies sync as such. And then we will just go ahead and touch a dot Git ignore file. And that should now exist, I just don't see it here on the left hand side, it might be hiding it or we might have to hit refresh. Well, I definitely know that it's there. So because if we do an ls l hyphen, LA, there, we can see that hidden file, there's probably a way to turn it on there. I'm just not sure at the current moment. But that's fine, because this is all about learning how to use the terminal and see Eliza developers. So if we want to open up that dot Git ignore, we're going to use our handy dandy c nine command, type, type Git ignore. And if you don't want to type everything, just hit tab on your keyboard that saves a lot of time when your autocompleting stuff works for most things we're going to do is type in node modules. what's what's that going to do, it's going to ignore this file completely just folder completely because we don't need to include that with Git. Now that we have that set up, you may want to set up your Git config global you username and email. We're not gonna worry about it right now, but we'll probably be prompted for it and it'll just be annoying message every time we see it. So now that we have a git ignore, let's set up our Git. Let's actually set up this Git repo. So we'll type in get a net and it's initialized. An empty repo is create a new folder called dot get here. So we just do an ls, LS hyphen LA, you can see I have a dot Git directory now. And we're not going to really get into the details about Git here. But you know, just showing you what's going on. The next thing we want to do is want to add all the stuff that we worked on so far to our actual repos. So if we type in git status, it'll show that we have these untracked files, meaning that they, they aren't going to be committed. So let's add them to be committed. So we have to get add, we'll do git add period, that will just add them all. So we'll hit enter, we'll type in git status. And so now you see one from untracked two changes to be committed. And so now we just need to write our git commit message hyphen, M, initial commit. And they are now committed. And there's that thing I was talking about the git, git config username and email, generally, you want to set these with your name an email, since I've just doing this for practice here, I'm not going to do that. And you'll probably keep on popping up. But we've created a git repository, but it only lives on this cloud nine environment. And we really want to make sure that this is hosted somewhere in the cloud, you could use GitHub. But for the this course we are, or this project, we are going to be using code commit so that we get some hands on experience with that. I think a lot of developers already have experience with GitHub. But yeah, we'll get to that. Next here. So I said, the next thing we want to do is get this, this local repo, that's this, this folder here, well, we can't see it on the left hand side, because it's hidden. We want to get this all the contents of this entire folder and the stock get file into a repo and we're going to use code commit. Now when you use the, the Elastic Beanstalk COI, you're setting up a project for the new time, it'll the first time it will set up a code commit project for you. And so I figured that's the way we should go ahead and do it. So we'll just go up here and make a new tab. Because the the CLR is not pre installed. So the EVA CLR is pre installed on this cloud nine instance, but not the Elastic Beanstalk one. So what we'll do here is we're just going to go ahead and type in an Elastic Beanstalk COI GitHub. And because that's going to have the instructions for us to do this install here, and so I'm just going to scroll down here. And based on your environment, you might have to install additional things, you can see there's a bunch of things. But since we're working cloud nine, there's not going to be anything that's too difficult here. And all we have to do to install this is to run the git clone command. So let's go ahead and give that a go. So that we make our way back to our cloud environments, I'm just gonna type in clear so we can see what we're doing. And I'm just going to go back one directory, because this is going to clone that repo like download this folder, and I just don't want to have it in my study sync here. So I'm just going to go back and directory, which is a CD dot dot, and we're gonna type in git clone. And this is already complaining. Too many arguments. Oh, you know what, because when we copied it, it already typed git clone for us, it was trying to save me some trouble. So I'm gonna go back in there, copy, I wrote it in manually, I'm silly. And we'll hit Enter. And that's going to clone it. So that's just going to download it to our local computer. And now to run it, if we go over here. It should be probably this command. Yep, that's the command. So we'll go back here, and we'll just hit Enter. And what it's going to do is it's going to install a bunch of stuff. This is probably not this cloud nine environments, probably not using this version of Python. If we just go over here, new tab, we're not gonna we're not going to mess with this one. But if I just type in, I think it's like, just type in Python hyphen version here. Oops, oops, oops, I didn't want that. hyphen, hyphen version, maybe. Okay, so it says version 3.6 10. And this one wants version 3.72. So, you know, that's just the state of Amazon Linux one right now. And we're just going to have to wait, this is going to take a few minutes, as it says here, I'm just gonna go over here, as I have, every time I stopped the video, I have to, I have to hit a command key and messes up terminal. But we're just going to wait for this to install. And this again, takes several minutes. So please be patient. You know, it might be three, four minutes, and I'll see you back here in a moment. Okay, so just waiting here a little bit here. Coming back to our first tab, we can see that it has completed so it just took a little bit time to install Python. And there's one more thing that we need to do and it's just to add this loss of Bienstock to our path so if we were to type in Eb, it shouldn't be able to find it because it just doesn't know where that binary is stored. So we just need to take its recommendation recommendation Here we are using bash until reason bash says bash up here. We just need to echo that command here. I'll just hit that. It also seems to suggest this. I don't remember having to do this last time. And I think we can go ahead and do that. I think it's safe to do. So. I hope that doesn't mess up this, this follow along here, but I'm pretty sure that won't. And so now if we type in Eb, we have Elastic Beanstalk pop up. So that's really great. What do I need to do is just delete this folder, we don't need it anymore. It's just creating clutter. So just type in LS hyphen, LA, make sure you are at this environment. directory, if you don't know just type in CD tilde forward slash environment. And we'll type in RM hyphen, RF, e to us, autocomplete it, hit Enter. And we saw that advantage there. So it's just a bit of housecleaning, because we don't need that sticking around. So now that we have the Elastic Beanstalk environment installed, or the COI, let's see what we can do with it, which will be actually setting up an application, I'm just enclose this other tab here. And I'll see you here in a moment. So now that we have the CLR installed, we're ready to initialize a new Elastic Beanstalk project. Now I want to point out that we are currently in Tilda environment, that's our home directory, it's very important that we run this command in the study sync directory, because it needs to find this dot Git directory in order for it to upload our code to code commit. So just type in TT, or CD tilde a Ford slash environment, study sync, and do LS hyphen LA, make sure you see that docket directory there. Before we get going here, I'm just gonna open up a couple tabs in AWS. And we're gonna go we're gonna go to one that's actually at the COI here. So we're going to make her way or sorry, to Elastic Beanstalk. And then for this one, we're going to make sure that it's on code commit. Just so we can see what's happening in the background here. So what I want you to do is I want you to type in Eb will give you a full list of commands. We're not, we'll probably won't end up using all these commands. But these are the most general ones. And they tell you to use Eb net Eb create an EB open for Eb open, we don't actually have the ability to use this command. This makes it so the application opens up in the URL in the browser, which is very convenient. If you're if you're running Elastic Beanstalk or your local computer, not in cloud nine, that's a great command to use. But here we won't be able to use it. Let's go ahead and do Eb Annette. The first thing it's going to ask us is the region we definitely want to default that to US East one always US East one because it makes things easier. It's going to ask us to select the application it's going to be study sync and knows that because it's picking it up from the dot Git folder there. It's gonna ask us if we're using no Jess, we absolutely are. So yes. Do we want to use code code commit, we'll say yes. And then enter the repository name, we're going to call this study think. And then it's going to ask us if or what we want our our default branch to be we want it to be Master, we'll just type in Master, you just hit enter there either. And what it's going to do, it's going to go ahead here and in code commit, it will show us that it created that repo so here it is. So it is now here in code commit Elastic Beanstalk. We're not going to see anything here yet. This is just an old one here. But if I do a refresh, I go back here. Well, you probably won't see this. But I had I had a older terminated instances here. So you might not see anything here as of yet. And then down below, it's gonna ask us if we want to set up SSH access, I'm gonna say no, we do not need that. And so now we've initialized our project. Notice that it's created a new folder called Eb Elastic Beanstalk. The period means that it's a hidden folder, we're going to go and open up this config dot yamo file. And so these are some of the options we chose when setting up the EB Annette. So now that we've initialized our project, we have our code on code commit, we need to configure this application so we'll move on to are the the Elastic Beanstalk environment. So we'll move on to that. Okay, so the next thing we need to do is configure this environment. So you generally have to do this for all Elastic Beanstalk projects based on what environment you're using, and configurations show up in the.eb extensions directory so we'll have to create one ourselves. I'm just gonna type clear here I'm going to make sure that I'm in the right place. So study sync. And I'm just going to type in MK dir which is going to make a new folder.eb extensions. Okay, and so now I have that new folder there. I'm just gonna double check it Eb extensions. That is totally cool. We're going to need to create a couple files in here. So I'm going to do touch, or sorry, I'm just going to CD into this Eb extensions folder, they'll save me some time, I touch a file called 001 n var dot config. And I'm going to do another one called node command. I'm just going to go back a directory. So if we open up this folder here, we now have n bar. And this is where we're going to set some default environment variables. By default, you don't have to specify an environment here, it would, it would just go to the Elastic Beanstalk environment, but we are going to be explicit here. And this is what we want to type. I always always type the word environment wrong, E and VIRO, m e NT, when you call it on the end there. And so I'm just gonna set Port 8081. And we're gonna say node envy, production. And it's four spaces in debt, that's totally fine. I'm just gonna leave it alone. And so that is one file configured. Then we will go over to the next one here. I'm just gonna double check this one here, option settings. Yep, that's all good. And so the next thing we need to do is tell Elastic Beanstalk actually how to start up our application because it has no idea. So we'll do AWS Elastic Beanstalk container. And then no, Jess. So we're going to give it some no Jess specific configurations here. First, we're going to provide no command that's going to tell what command it's going to run, when it first starts up, we want it to NPM start. And then we need to specify the node version, this is going to be 10 point 18.1. I'd actually have to put the parentheses there and use double quotes, I think you use single quotes, but I'm just gonna stick to what I wrote earlier, because I don't want anything to go wrong in my follow along here. And if you do node hyphen version, you can see that choosing 10 point 19.0. And it is better to use the latest version. But that doesn't necessarily mean that those like Elastic Beanstalk can run that I definitely know that 10 point 19 is not out for Elastic Beanstalk, and 10. Point 18.1 is available. So we'll stick with this one. But yeah, that's the configuration there. So we'll go ahead and we'll just commit it. So get add all get commit hyphen m configuration for Elastic Beanstalk. It's very important that we do commit this and then we push it because of these halls aren't there and we tried to create an environment, it's just going to air out because it always looks at what's in the repo here. So if I go into here, code commit Eb extensions, we can see we have those files there. So that is the configuration phase over here. I'm going to close a couple things here. Just clean up here a bit. And what we'll do next is we'll actually create a new Elastic Beanstalk environment. Alright, so we are ready to create our Elastic Beanstalk environment. And to do that we need to use the EB create command. But we're not just going to write Eb create, we're going to write Eb create, hyphen, hyphen, single, this hyphen, hyphen, single This is a command flag. What it's going to do is tell this to create a environment an EB environment that is it running in single instance mode. If you don't provide this flag, it's going to spin up an elastic load balancer elastic load balancers cost money. Technically, if you're using the free tier, if it's if you made this account, and you're still in your first year, you got one lb free running. But you know, we just want to avoid these kind of problems. So let's just not use it. And there's we're not going to really be doing anything with elastic load balancer anyway, through this walkthrough. So do Eb single, it's going to prompt us with some options. I'm going to name this study, sync, prod, even though we're in our environment, our developer environment of my iOS account, if we go back up here, where it says, example, Dev, I'm going to pretend this is a production application. And I'm just hit enter there, we just wanted it to be the same. We don't want you to spot instances, that's a great way to save money. And then it says it's insufficient Iam privileges, unable to determine if this rule exists. Assuming that exists. It's gonna go ahead and spin that up. I don't know this is gonna cause us a problem, but we're gonna have to wait here. And this is gonna it's gonna take a little bit of time to get going here. So I'm just going to wait for this to get started. I'm gonna open up another terminal here. And I'm just gonna go to study skincare. And I'm just gonna type in AV status. And this actually shows us the current status of the application. Right now the health is gray and the status is launching if we go over here You can see I was trying to launch some stuff earlier here, those are terminated instances. This is the new one here pending. And you can see it's a nice dark gray. And this mirrors exactly what we're looking at over here. So this is going to take about five to 10 minutes to launch. And I will see you back here momentarily. Okay, so after waiting a little while here, I go back here, and it says that it successfully launched the EC two instance. So it looks like to me that it's all in good working order. We go over here and we type in Eb status. You can see that says, ready and Yellow Yellow is not a great status to have. So if we come back over to the Elastic Beanstalk environment, I can't tell if it's finished yet, but I'm just gonna go back here to study sync, click into the yellow here. And it's giving us a warning, it's saying unable to assume the role. It was asked Bienstock service role, so it's supposed to create that for us. But for whatever reason, it just did not. When I first wrote this fall long, it definitely created that for me. So I'm not sure why it's not creating it. But if we go up to this link here, and open it up here, the application clearly is working. So, um, yeah, the old commands not that great, but maybe it'll go away on its own, it's gonna hit refresh. So I'm just gonna wait a little bit here and see if it actually does go away or not. Okay, so our yellow eventually became a red. And it really has to do with this Avis Elastic Beanstalk service role. This is confusing, because when you run Eb create, it's supposed to create this for you, it's supposed to actually create two different Im roles. As I go over here, I don't see them in here at all. Now, you could go ahead and manually create them. I've tried to do this, and I haven't had much success as well. But there is another way for us to create these roles without us having to do a lot of manual labor. And, again, you might not have to do this, those those roles are Im roles may exist. But in my case, I'm just having a hard time today with Elastic Beanstalk. So to get them created, what I'm going to do is I'm just going to start another Elastic Beanstalk project. So I'll go here, create a new app, I'll just call test, it doesn't matter. Because if we launch one from here, it will absolutely create us a those roles. So I'm saying here, go to web server environments. Okay, leave it as test, I'm gonna choose a Ruby, I'm going to go down here and launch a sample application at launch rails. I'm just gonna write test in here, check for availability, that's all good. hit Create environment. And so what that should do is it should trigger to the console. That should go create those Im roles. So if I refresh here, now you see they exist as Elastic Beanstalk aect role as Elastic Beanstalk service role. So I have no idea why those aren't appearing, but now they do. But the trick is, I just need to delete this environment now. So I can't stop this as it's running. So we'll have to wait till it goes through the motions of it off of this test environment. And then once it's done here, we'll just go ahead and terminate it. Okay, so the environment spun up. And now we're just gonna have to go ahead and delete it. I know this is really silly. But I mean, that's the only way I can get these roles to be created. But, you know, the definitely, definitely should be able to make a manual in this definitely should automatically happen. I'm going to go back, I just clicked to all applications here, then we'll go into here on the right hand side, we'll see if we can go ahead and delete it here. So I'll just put test in here. And so what that will do is it should automatically start deleting this environment. So if I go into here, it's terminated yet. So that's that. But this environment is just no good. So what we'll do is, I mean, we can terminate it, I suppose. I guess what we'll do is we'll just terminate this environment as well. So I'm just gonna go here and type this in here. And I'm just gonna wait for this one to delete. And then we'll try Eb create again, and hopefully, we won't have any issues this time around. Okay, so I did a refresh there and it's terminated. And what I'm going to do is I'm gonna go back to cloud nine. And I'm going to try this again. So I'm gonna do I'm gonna go back to our first tab here. I'm going to do is just hit up. It's gonna ask me for the name is going to study, sink, fraud. Hit Enter again. We don't want spot here. It's saying that it was possible inside ECG roll it can't find it. That's okay. When I first did this, I had that error and it wasn't an issue. But let's just really make sure that it actually is there. As long as it's there, that's all that matters. Okay, so the roll is there. So we shouldn't have any problems this time around. But we'll just wait here and see what the result is. I'm just gonna go over to Elastic Beanstalk. Give it a refresh, click on it here. And we'll just wait a few minutes and see how it goes. Okay, great. So this Elastic Beanstalk has gone green. So I was creating that temporary application, even though as silly as it is, fix the issue here. Hopefully, you don't have to do that. And those roles just exist for you. So if we go over to cloud nine, here, I'm just going to do a clear here. If you do Eb status, that's going to show you the status here, so green and ready, which is the same thing that is over here. If we wanted to go view this web application, it shows the C name in here. So if I copy this out, we can see we have a link there. And if we scroll up here, we can also see that it assigned us an elastic IP address. So that's another way of accessing the web application here is that IP address. If we typed in Eb logs, this would show us what happened actually, on the EC two instance, if anything was logged out, I showed you this prior, but I'm going to show you quickly here again. And so you can see that the application started up here, I'm not sure what's going on down below here. I don't think that matters. But this is what we really do want to see, we hit q to exit that out there. If we type in Eb events, that is going to show us the event history that has happened. So if you go over here and events, that's the same information here. Really great way to debug stuff. I want to point out that the deployment, the deployment model we're using is all at once right now, we haven't actually done a deploy yet we've I mean, we technically have deploy, but we haven't deployed to an existing environments. So I'm just showing you that it's using all at once. And so the next thing we're going to do is switch this over to immutable and see what the difference is there. But we're not going to do it in here, we could just click immutable hit apply, and then do a deploy. But I want to do everything for the console. So that's what we're going to do next here. So let's get to that. So to switch this over to mutable deploys, again, I said that we could modify this file, but we want to do it programmatically. And we're gonna do that through the configuration files. So what I want you to do is going to Eb dot extensions here. Stop letting me type that happens to you just close bash, and open a new window here, I already have one open. And I'm just going to go into Eb extensions, I'm going to make a new file, and I'm going to call this 000 deploy config, I just wanted to be ahead of those other ones. I don't think that order really matters, but it's just what I want to do. Oops, it's not make we want to do touch. So we'll touch a new file there. And what we're gonna do here is we're gonna set option settings. And we're gonna do eight of us Elastic Beanstalk here. And we're going to command and what we're gonna do is set the deployment policy to be immutable. And then what we're also going to do is set health check, success, threshold, threshold, to warning. And then we're going to set ignore health check to true, and we're gonna set timeout to 600 here. So I'm just gonna read this over very quickly here, make sure to make mistakes here, Elastic Beanstalk? That's correct. immutable, or deployment policies immutable, that looks right to me. Health Check status threshold that looks good to me. Ignore health check. So we're gonna say over here, what we're doing. We're just we're actually checked boxing that off, okay. And we're pretty much setting the same settings that are here, except this is going to be warning. And that should make the deploy really fast. And also, while we're at it, let's just make a superficial change. So when we do deploy, we can actually see if the effects have taken effect here. So I'm just gonna go to our actual application here, I'm gonna just change this to study sink. version one, if we were to actually check out this application before we deploy it here. Notice that it says hello world here. So we're changing that to study sync of version one. We're gonna go back to bash here, we're going to go back and directory, it's gonna type clear. Make sure you're in the environments, study sinkin here, and we're going to add all the changes. So we may modify that file, we added a new configuration file, so I'm going to add them both commit I found em immutable deploys, I'm going to push those changes, I'm going to go ahead and deploy that. So so we'll just type in Eb deploy. And right away, it should start switching over to a mutable deploys, because it's going to pull those configuration files, look at them, and then it's going to decide on what to do. So I think we just wait here a moment, it's actually going to tell us, and we can see that I actually have an error here. So I'm going to go ahead and abort that. So I'm just type Ed abort. But we can see contains invalid key option settings. So I probably just made a mistake here. Oops. Yep. We'll try this again, AB deploy, oh, well, we have to commit those changes. Push, do AV deploy again. So what we're looking for, is to see if it'll actually say that it's doing an immutable deploy. And there it is, it says immutable deployment policy enabled. So we're gonna have to wait for that deploy to happen, I'm just gonna open a new tab here, because I'm going to stop the video here. And immutable deploys are a lot slower than that all once. But the advantage here is that it won't take our server out of service, it's going to create a new server, and then when that new server is good, it's going to switch over to it. So our users will never have an interruption in service. So I'll see you back here shortly. All right, great. So our mutable deploy has completed. It's actually been quite a while since last time I was here, because I actually recording this in the next day. But I can tell you that that immutable deploy didn't take too long. So it definitely takes longer than the than the all at once deploy, all the ones is extremely fast, where these immutable deploys, have to go through health checks, and then go through multiple checks before it determines that the new service is good and moves over. So you know, that was immutable deploy. But what I want you to do is go back to cloud nine. And we are just going to undo those changes there, because the next thing we're going to learn how to do is bluegreen deployment. And I don't want these immutable deploys, slowing down or development here. So just to get rid of this immutable deploy stuff, all we're going to do is remove that file there. So when you type in RM, and we're going to do tilde to here environments, study sync, Eb extensions, 000 config deploy. And then we're just going to add those changes, I'm going to make sure that I'm in that study sync directory there because it looks like I was in the wrong place there. And we will do git add all get commit hyphen m, revert back to immutable deploys. Okay, git push. And just before we do anything else, here, I just want to go back to the environment here and just show you under your configuration that it should have switched to immutable deploys. So here you can see it's immutable and the health checks are disabled. But anyway, now that we have that set up, all I'm going to do here is now that I've made these changes, I did a push, I'm just going to do a cap deploy or not cap deploy, Eb deploy, I'm thinking of Capistrano, which is for Ruby on Rails. It's not what we're doing here. And, and what we'll do is we'll just revert this back to all at once. And this isn't going to take too long. So I'm just gonna go back here to my dashboard, and we can see this is updating. And I'll see you here when this is done. And then we'll move on to bluegreen deployments which should be super exciting. Alright, so after a short wait there are, are mutable deploys your back to all at once deploys, and we can just double check here under the configuration. If we go down here, we should see now it's now it's all at once. So let's move on to bluegreen deploy. So bluegreen deploy is when you you switch environments. So right now we have this environment here which we can consider our blue environment. And the idea is we're going to spin up identical environment called our green environment, which will have our latest changes. And once that environment is in good shape, what we'll do is we will swap the URL of the environments. Now this option isn't available to us right now. Because there's nothing for it to swap to. But once we have that other environment, that's how we will make that switch over. So in order to do so we're gonna go back to cloud nine. And what we need to do is clone this environment, make a copy of it, and we could go in here, and I think we could go right into here, and then click Actions and clone the environment. But let's do it through the CLR. Because again, this is the developer associate, this is the best way to learn. So we're gonna type in Eb two, or Eb clone. And then it's going to prompt us for the name, I think clone is okay, for our case here. So I'll hit enter, we'll keep the C name the same. And what's going to do is, start up a new environment there. So we'll go back to study sync. And I'm just gonna give it a refresh. and here we can see that this environment is spinning up. So again, we're gonna have to wait a little bit here, and I will see you back momentarily. Alright, so after a short wait there, because it's using alt once deployment, we have our production clone environment up, I know, it doesn't look like it's running here. But if we just go back here a second, and I do a refresh here, we can see now it's green. So I don't always trust the universe console, always refresh and look around. Because sometimes things are ready, and you're just waiting around for nothing. So if we take a look here at this clone environment, Following this, this see name here, or the DNS, what do they call it, I'll just say URL, this URL here, we can see that it's running. But we want to make sure that we have a new version here. So this is version one. So let's just make a superficial change client version two, and then see how we can deploy to this new environment and then facilitate the switch. So what I'm going to do is go back to cloud nine, and I'm going to make my way over to the app. And it's just making some complaints here. So I'm gonna close these tabs. And what we want to do is we want to go ahead and open this app js file and just change this to version two. All right, and so now that I've changed the version two, I'm going to commit this to the repo. So I'm gonna go get add, well, I'll do git status, we should always do that, we can see the file one to add git add all get commit change to version two. We will do a git push. git status. Alright, great. So our version two is there. So how would we go about deploying to the green environment because we have these two environments. So if we ran Eb deploy, I think by default, it's going to deploy to the original environment. But if we want to specify the environment we want to deploy to, which is the green environment, we just have to provide its name. So that's called study, sync, prod, clone, I'm pretty sure that's the name of it. Yep, study, sync, prod, clone. And so this should now deploy the latest changes to this environment. So go ahead and press that there. And it's going to start up again. We'll just give it a second here. We'll flip back, we will go back to study sync. I'm just gonna give it a refresh here, because something should be changing. Probably the prod. Yep. So it's updating. So we will let that deploy there. We'll give that a little bit of time. And I'll see you back here, when that's done. And we'll just double check, it's version two. And if that's all good, we can do the slop. Alright, so pushing our changes, or version two changes to the clone is done, we go to this page and refresh, you're going to notice that there hasn't been any change, but actually has worked on this is just an issue with Chrome. Because if you open up another browser, and refresh in Firefox, it says version two for the same URL. So this is a chrome caching issue. I spent hours upon hours trying to solve this problem and not realizing it's Chrome. So just be aware that anytime you're doing deployments to anything, you're checking stuff, always just rule out your browser. And sometimes it's not even AWS. So you know, if we want to get to see the latest version here, I'm going to open up Inspector, make sure you're on network and have disabled cache, do a refresh there. And now it says version two, this will not work unless you have this open this checkbox and then you do the refresh. All right. Um, but yeah, now that we have figured out how to deploy our second version with bluegreen deployment, well, we can go ahead and do is swap over the environment URL. And so we said that what we can do is go to here to actions and to go to swap environment URL. And we could go here and choose our other version and swap here. However, since this is for the developer, associate, I really do want you to get as much experience with the CLR. I'm gonna just keep on saying that. And what we're going to do is go ahead and use cloud nine and use the actual ebci to do that. So the command here is Eb, swap. And then we're going to say, if we hit enter, now, it would prompt us to ask, you know, the source and the destination. But I just want to be very explicit here. And I'm going to just type in the source. So I want to swap the prod, with the clone. So we'll say study, sync, prod clone. And this will do exactly the same thing as swapping the URL URL out here, okay. I'm just gonna hit enter, unrecognized argument here, cologne. Let me just make sure that I spelt that name correctly, I'm just gonna go over here and check. Study sync, prod clone. Study sync product clone. Oh, sorry, you know what, I have to provide a flag here it is, hyphen, hyphen, destination name. And that will hit Enter. And this is going to trigger that swap action there. And so we're just going to have to wait a little bit there. It says it's completed the swap. Wow, that was really fast. So we'll go back here. And if we are to click on prod Now, what I want you to notice is this is the clone URL here, right. And we are now in the clone environment. So this used to say clone, but now it says prod it's taking the original seed name from the other environment. If we go to the first environment here, this one now has clone. So that's the swap that occurred. So that's how we know that it worked. Now that that swap has occurred, what we want to do is just get rid of our old environment, because we know our new environment is running. With no problems here. If we just go to prod like this, it's running version two, so we're all good. And so what we need to do is go in here and then just delete, terminate this environment. But let's do it from the COI. So we'll go back here to cloud nine. And what we're going to do is type in Eb, terminate, study, sync, prod. And then it's going to just ask us to confirm it. Hit enter. And that's going to terminate. Now, we're pretty much done with bluegreen deployment here. And with that out of the way, we can actually move on to learning how to deploy a single Docker container next. So what we're going to need to do is rip down everything because we do not need even this clone anymore. So I'm going to also terminate this one, but I'm going to do through the console here. And we need to type its environment name. So I'm just going to copy it here. paste that in. Make sure that's right. And what we're gonna do is we're going to wait for these to shut down here. And when these are both terminated, we'll move on to the next step, which is deploying a single container Docker environment to Elastic Beanstalk. Alright, so we are back here, it went through the whole process here and built everything and it's saying that it's running on port 8080, what we're going to need to do is open a new tab here, because we're going to need to get the IP address of this Docker or this cloud nine environment. So we've got a terminal here, I'm going to type in curl, HTTP, colon forward slash forward slash 160 9.2 54 160 9.2 54. Latest metadata, we'll hit enter, make sure this works. And then we'll type forward slash public IP v four. And that's the IP address. So what we can do with this, just copy this here, and go Port 8080. And we will see if this works. Oh, let's just click it to open. There it is. So this is running in a single Docker container. The reason you'd want to Docker eyes your environment is because it allows you to ship your your configuration with your code base. So you saw before we are restricted to version 10, point 18, whatever have no but now we are only restricted to whatever we provide with it. So a lot more flexibility around that. In order to prepare this for deployment, we are going to need this node modules anymore. Because we are using a Docker container and this is just going to do nothing. So we'll go ahead and remove that. So what I want you to do is just close this tab here. I'm going to do Ctrl C to stop the Docker container. I'm going to go CD Tilda. Oh, we're already in All right place, but this is where we need to be. And we're just going to do RM, Eb extension 002 and just remove that file. So now that's been removed, we need to make an adjustment to this file here, I'll just hit keep open, this needs to be Port 8080, because that's what we're setting in our application. And let's go ahead and commit our changes here. So configure give me for Docker. So now that our pushes have been changed, what we can do is go ahead and do an EB create. I've been I've been single, so we don't launch a load balancer. We are going to name this one differences so we can identify it, we're gonna say study sync, Docker, we'll hit enter, we'll hit No, we don't want spot instances. And we will make our way over to the cloud nine, or CLOUD NINE over here, do a refresh. We'll give it a second here to start up there goes. We can see that using the Docker platform, we'll do a refresh. And we will just wait until this is done and see. So after a little while there, our environment is now running here. Let's take a look to see if it's working. And there you go. We're running on Docker, it was that easy. The thing with Elastic Beanstalk is that it did all the work for us. We just had the Docker file in here. And when we uploaded it, it did all the work it built the image for us. But normally, what you'd have to do is build the image yourself, and then push it to an actual Docker repository that could be Docker Hub, or in the case of AWS, you can use elastic Container Registry or ECR. And that's what we're going to do because that's a more complex setup and the more common stuff that people will be using, because most people outgrow this Docker file, the simple setup here. In order to do that, what we're going to need to do is create a new file called a Docker run itos dot JSON file, and we're going to have to build an image and push it to ECR. But before we do that, let's just make a revision to our actual code base here. And we will go to App dot j s, and we'll call this version three. And the next thing we will do is we'll go ahead and build our Docker image. So I'm going to type in a Docker build, hyphen t study sync period. So this is going to build a Docker image. And it's going to name it study sync. So we'll just wait here a little bit. And there it is, it's done. That was fast. The next thing we need to do is we need to authenticate to ECR. And this is a very long command, so we'll get to it. It's AWS ECR get login, password, pipe, Docker, login hyphen, hyphen, username, AWS hyphen, hyphen, password, st in, we're going to have to provide our account ID here. I don't know what my account ID is for this account, we need to poke around, we should be able to find it somewhere. It's generally under my account settings, I just don't want to show all of my billing information here. So another easier way, we'll just go make our way over to I am, I feel like that's always a place where we see our account number. We should see it anywhere. We'll just go into even the user here. Here's one, our account number is everywhere. I just need part of it there. So I'm just going to paste it over here and then extract it out. And then what we need to do is provide it as such. And then we need to type in this URL. So we need d k r.ec r. And then we need the region we're operating in. So US East one dot Amazon AWS comm if you're wondering how I got this whole link is in the Airbus documentation for ECR. So what this is going to do is log us in and generate out a token so we can authenticate. So it says there's an unknown flag name user, so I'm just going to double check that there. It's actually supposed to be user name. So we'll go ahead here and type this in. And here it says it's created that credentials helper file, so there you go. So notice that it's created a file here called Doc, or a hidden file called Docker config dot Jason. And that's what storing the token which is going to help us to authenticate. So let's take a look at the actual images that are here. And we can see that we have our images built here. And we're going to get how to get this image ID next. And what we need to do is tag this Docker image. So I'm going to put in that image ID, we need our account ID again here. And it's actually the same link here. So it would probably be easier if I just copy that out like that. And then we need to specify the name. So now that it has been tagged, what we can do is do Docker push. And I believe it is the same URL here. So let's copy this. And it says here that the repository does not exist with this ID. So maybe what we should do is make our way over to ECR. And just maybe we need to make the repo beforehand. I always forget this. So I guess we'll find out. I thought we just create it for us. Nope, I guess not. So we'll just type in study sync here, I'll hit Create repository, then we'll make our way back to cloud nine, just hit up. And there we go. It's uploading our Docker images, kind of like GitHub is incredibly small, Docker image. So it's not taking too long, which is really nice. One advantage of using Node JS over other languages and frameworks. So I'll just wait here a little bit, and I'll see you back in a moment. So our Docker image is now built and pushed to our ECR repo here. So if we go in here, we can see that we have it. And the next thing is to prepare our actual, this next environment here, and instead of working with this one, because it's gonna be a lot of work here, we're just gonna make a new folder. So once you go, cd cd.or, just go actually here to CD Tilda Ford slash environment, we're gonna make a new directory, we're gonna call it study sync external. And what I want you to do is make a new file in here. So we'll just CD into this. We're going to call it Docker run dot ABS dot JSON. If you've ever seen a task definition file, it's extremely similar. And in this developer associate course, we definitely cover how to deploy with ECS, and fargate. So this will become extremely familiar to you shortly. But what we need to do is open up that file, I made it as a directory, that was an accident, I should have made that a file, so I'm just going to remove that. And instead of doing MK dir, I'm going to type in touch. Okay, and then we can just open up that file there. And what we're going to do is write some JSON. So the first thing we need to do is define the Docker version for EBS here, so Docker run version, it's going to double check, make sure that is correct. Yep, that is right. And we're going to specify version one. version one is for single, single containers, when you do multi container, you do version two, then we need to specify the image. And that's going to be the URL we were seeing there earlier. I feel like we could grab that from ECR. Yep, it's pretty much the same thing here. I just want this part of it. I don't know if I need to put latest in there. Until we'll put that in there, we have to specify the ports. So we'll go ahead here and do that. So it knows what to map to. And then that we will do this little bit of cleanup here. I'm just gonna double check to make sure everything is right here. Sometimes it's easy to miss these commas. It looks all correct to me, so we're in good shape. The next thing we need to do is initialize a Git repo. Here's we're gonna do Git init. And we're gonna copy over a couple files. So we want to bring over our Git ignore Eb extensions file, and our n var config file, I think so we will go ahead and do that. I'm just trying to think the easiest way to do this, probably just make the files again, so I'm gonna just type touch dot Git ignore. And then we will do we'll make a new directory called Eb extensions. And then we will touch Eb extensions. 001 dot n var config. And I think that's the only two files we need to move over. So we will go to our old one here. And it has some Elastic Beanstalk stuff in here, and that will just take all of it, that's totally fine. And we'll go to our new one here and paste that in. And we said, we need to set this as well. So we'll go to our old one, Copy that, paste that, paste that there. And now that we have those files in there, I want you to do is go ahead and do a git status. So we have three files, that's great git add git commit hyphen, M. Docker run. And we need this, we need a Git repo because it's going to create a new one when we run Eb net here in a moment. So I'll just do git status, make sure that all worked fine. Great, and we will do Eb in it. So we're going to choose US East one. So number one, we are going to create a new application. So press two, we are going to stick with the name that we are given here. So hit enter. We are definitely using Docker. So we'll hit yes, we want to use code commit. Sure. So we'll hit y we need to select a repo, we are making a new repo. So press to enter the repos name is going to be called study sync external. Make sure you type it right. We are going to want it to be master branch. So hit enter. We don't need to SSH in. That's okay. And so there we go. So now that that is created, I'm just going to double check and make sure that is the case. So we'll make our way over to code commit. And here we have our external repo. So it's all in good shape. So now that that's all set up, we should be able to create a new environment. So we'll do Eb create, hyphen, hyphen, single, we will name it pretty much the same, I'm just going to take off the dead part on it doesn't matter too much as long as you can remember what you set it to. We'll say no for spot instances. And what we're gonna do here is just wait a little bit here, it's nice to see the message, just make sure that it's creating what we want to create. And this is a Docker image. So hopefully this works. First try, I'm going to make my way back to Docker here, we'll go over here. And while that's going, we can go ahead and terminate this one, we don't need this one anymore. And just to point out, like, look at this, this doesn't contain any of our code. So where is our code, our code is part of the actual Docker container. That's why we don't see here, because when we built it, it copied it and put it into the actual container. Whereas in this setup, the Docker file is here. And so we can work with our source code and have it all in one place. So you just have to decide you know what workflow works best for you. And you know, if you can get away with just having a Docker file like that, that's definitely better. And this is creating, I'm just going to go back here and do a refresh. I don't see this new environment yet. should be called external write. Study, think external. Oh, here it is. Yeah, because it's a completely new application. That's totally fine. So yeah, I'll see you back here in a moment once this environment is done creating. Alright, so our deploy is done, but we have helped degraded and it looks like we have an error here. It turns out, Elastic Beanstalk can't authenticate to ECR because we didn't give it permissions to do so. Whereas in cloud nine, we had pulled that all the credentials and stored it in here so that we could read from ECR. So what we need to do is update the incidence profile of the actual eccm instance that runs here. So what we'll need to do is make our way over to Iam. So just type in Im here. We'll open this in a new tab. On the left hand side, we'll go to roles. We'll go to EC to roll here for Elastic Beanstalk. We're going to attach permissions. We're gonna type in Amazon EC two Read Only container will attach that policy. So this should allow us to gain access to ECR. And then what we'll do is go back to cloud nine. And we'll simply do Eb deploy. And so what that will do is it will just deploy again. But now it will also update the Iam role, and it should have permissions this time around. So we'll give it a second here to get started. We'll make our way back over to here, we'll do a refresh. And we can see this is in progress. And I'll see you back here momentarily. And our deploy is done. So I'm just going to close these additional tabs here, I'm just going to open up the new tab here. And we are now seeing version three, if you don't remember could be chrome caching it but there you go. So we went through a lot of different variations here with Elastic Beanstalk. And you know, that is a lot of stuff, but is necessary to go all through these things. Let's just go ahead and clean up what we have. So I'm going to go back all the way to applications. And what we can do is go ahead and just delete these applications, this should terminate all the environments. So hit Delete. And we're going to also delete this application here. We'll say delete, click into this, this should be a terminating. So our cloud nine environment was not a big issue, it's going to shut down after 30 minutes, when we're not in use for our elastic containers, we're going to go to code commits. I don't think this is really an issue having these around. So I'm I don't have much motivation to delete them. And we might be using them for the ECS and fargate tutorial. So we're gonna leave these alone, or we're gonna leave our code commits, or, or Yeah, co committed ECR alone, so we'll leave this alone as well. But yeah, that's it for the Elastic Beanstalk walkthrough. Hey, this is Andrew Brown from exam Pro. And we are looking at elastic container service, which is a fully managed container orchestration service. It's highly secure, reliable and scalable way to run containers. So let's take a look at the components of ECS. And so over here, we have a technical architectural diagram. And we'll just talk about all the components involved. So the first is the cluster itself. So ECS cluster is, is just a grouping of EC two instances, they call them EC two containers, which is a bit confusing, because inside these instances have containers within them, so they are both running Docker installed on them. So you can launch EC two or Docker containers. And so another thing that's really important is task definition files, we don't have a representation of any of that in our architectural diagram here. But that's just a JSON file that defines the configuration of up to 10 containers that you're going to want to run, then you have a task, and that uses a task definition to launch containers. And a task is a container, which runs for only the duration of the workload. So the idea is, let's say you have a background job you want to run as soon as it's done, the task stops or deletes itself. So it's really good for one off jobs, then you have a service. And this is exactly like a task, except that it's long running. It's intended for web application. So Ruby on Rails, Django, express j s, where you don't intend these things to shut down. And the last thing we want to talk about here is the container agent. It's not represented in the diagram here. But this is a binary that's installed on the EC two instances. And this just monitors, as it is monitors the tasks as well as it stops and starts. Let's talk about the options, we have to choose one configuring a cluster. So the first thing we're going to do is go ahead and create that cluster. And you're gonna have to choose between fargate or ECS clusters and whether you want it to have some networking components involved. And then once you've done that, you have to go through and choose a bunch of options. So you have to choose whether you want to be spot or on demand. So with ECS, you can save money with spot because if you're running background tasks, maybe they it's not a big deal for those to get interrupted. Then you don't want to have to, you're going to want to choose the EC two instance type, and then you'll have to choose the number of instances. You'll have to choose the EBS storage volume and then you can choose whether you want use Amazon Lex to her Amazon Linux one which both have Docker installed on them. So there are some of those options right there for you as you can see, then you'll have to choose your VPC or create a new VPC. Then you You need to decide in an IM role, then you have the option to turn on cloudwatch container insights, this is going to give you richer metrics about the operations of your containers. And then you can choose a key pair, which is unusual because you don't necessarily need to log into your instances. And 80 of us generally does not recommend you to SSH into those containers. But you totally can. So that's all the options for ECS. And we'll see this again, when we go through the follow. Let's take a look at that task definition file that we talked about that is used to launch our tasks or services. So what you do is you'd go hit the Create new task definition, and this would actually have a wizard to help you get set up. But if you had to write this by hand, this is what the actual file would look like. And in this file, you can define multiple containers within a task, which is actually what we're doing here on the right hand side. And the Docker images can be provided either by ECR, which is elastic container repository, which we'll talk about the next slide, or an official Docker repository, such as Docker Hub. So here, you can see that we are specifying an image and that image is WordPress. And then another important thing is that you must have at least one essential container. So this container fails or stops, and all the other containers will be stopped. So this is just to make sure that you have at least one dependent resource there. And if you aren't sure how this all works, it's okay because eight of us has that wizard. So when you click that create new task definition button at the top, they have all the fields that you fill out to create this, but if you wanted to create by hand, you could totally do so. So I want to take a quick look here at tasks Container Registry, which stands for ECR. And this is a fully managed Docker container registry that makes it easy for developers to store manage and deploy Docker container images. So just to give a representation here, you have a Docker image, and then you can push that to ECR, which is like it's like a repo for Docker, Docker images. And then once you have it there, you're gonna be able to access it from elastic Container Service, fargate, Kubernetes, or even on premise. So it's just an alternative way of storing a Docker image as opposed to Docker hub or somewhere else or hosting your own on it's highly secure. Hey, this is Andrew Brown from exam Pro, and welcome to the ECS. Follow along. In order to do this follow along, you have to do the Elastic Beanstalk one first, because in that fall long, we build out a web application, and we turn into a Docker image and then we host it on ECR. And we are going to need this ECR Docker image in order to complete this follow along so please go to that one first. And do that follow along and you definitely should. And once you have that done, come back here and we will proceed forward. So before we can create our ECS cluster, we're going to need to create an IM role for our EC two instances. Each of us has documentation all this the Amazon ECS instance roll, these instructions aren't very clear, but I know what to do. So let's follow along. So the first thing is we're going to want to name it this UCS incident roll. Well, you could name it anything you want. But let's just be consistent here as because this is what everybody else names, it will make our way over to I am on the left hand side, you want to go to roles, we're going to create a new role. We're going to leave that here, choose EC to go next. And then what I want you to do is type in EC to container, or we're looking Container Service role for EC to this one here, I'm going to double check it just make sure it is what I'm expecting it to be, I usually can tell what this stuff is by looking at the services. Yep, this is the one we will go next, we will hit Next, we will name that role, we will create that role. So now we have this role, and we're ready to go create our cluster. So going back to our first tab here, I want you to make it over to ECS. So we'll click on that left hand side, we will choose cluster we will create a cluster and we'll be presented with multiple options. It's defaulted to fargate, which we will be doing in the fargate Follow along but right now we're doing ECS and the way you know if it's ECS is that it's not powered by fargate. And you do not create an empty cluster. So we are going to use Linux. So we'll hit next here. And if we check box this to here, this would make it fargate. But that's not what we're doing. I'm gonna call this my ECS cluster, we'll leave it as on demand spot is a really great way to save money but I don't I just don't want anything to go wrong in this fall long. So we'll just leave it as on demand. I want you to go look for T to micro here. Because that's part of the free tier there I found it. We're gonna only have one instance we want to keep this very inexpensive. Amazon links to seems fine to me. We do not need to set a key pair. We do not need to set any of the VPC settings here and we need to make sure our ECS instance role is there. It automatically select Did it and then we can go ahead and hit Create. What's that going to do? It's going to create an ECS cluster, then it's going to make sure we have that Im policy. And now we're just waiting for this cloudformation stack to complete. So this won't take too long. It's still pending right here. We're just waiting for the auto scaling group and the internet gateway. I think what I'll do is I'll just pause the video here. Oh, no, it looks like it's almost done. Maybe we'll just give it a second here. Oh, okay. It's proceeding forward. Now, we're just waiting for the security group and the auto scaling group. Okay, great. Now, we're just waiting for these last two here, the Virtual Private gateway in the auto scaling group. Alright, now we're just waiting for the aect route. You can see that it sets up a lot of stuff to make this cluster. And that looks like it's done. It's still spinning, though. I think this is pending here, the the route table subnet Association. So we're just waiting for the route tables to set up and there we go. So let's go ahead and view your cluster. And what we need to do is want to create a service. But before we can do that, we're going to need to create a task definition to actually launch. So go to task definitions create a new task definition, we have the option between far gaming c two, we obviously want to see two, but that's for ECS. or hit next, I'm gonna name this study sync, we need to choose a task role. Optional, Im role that the task can use to make API request to authorize data services, create a one here, we might need one here for using ECR. I'm not sure I guess we'll find out as we go. We'll go down here and we'll have to specify some memory. In order to do this, we need to know how much memory comes with a T T two micro that is 500 megabytes. So that's the maximum we can do. And with a T two micro you get one v PC, so I'm going to place that in there. Here you can set the CPU units, which I have no idea what to do for that I don't know if one Vc vcpus wanted you for probably is but I definitely know there is only one v v CPU for virtual CPU for t twos. T two micros. So now that we're there, we'll go ahead and add our container. I'm gonna name this study sink container. And then we need to specify our image repos. So we'll go over to ECR, I'm going to copy that. Paste that there. Um, I think we want the latest. I think we can place the latest in here. And we'll set it to 256 megabytes, it's a no GS app, it doesn't require a lot of memory, we want to map the port. So 80 is the is the port that goes to the internet. That's what we want. And 8080 is what our container port is. That's what we start up our web application on. And we will see that if like you'll see that in the Elastic Beanstalk follow along, but that's what we do, we're not going to set up the health check. We do need to set up some environment variables, we don't need to set an entry point or Command, these are set in that build image. If we wanted to override them, we can place them in here. Same thing with working directory, we're going to set up the port here, which will be 8080. And then the node environment which will be production. Then we'll go down here. There's some there's a quite a few options here. Not super important. We'll keep on going. And yeah, that should be everything that we need. We'll hit Add. I still don't know if we're gonna run into an issue because I know that ECR requires authentication. And I don't know if it will pull it without that task role, but I guess we'll find out. We have a lot of interesting options here for service integration. So app mesh, proxy configuration, log router with fire lens, I don't even know what forlenza sounds like a new service to me. We can add volumes not necessary. I do want to point out one thing I can't maybe I can click it here. I just didn't show to you. But when you set environment variables you can do value from and this allows you to provide the Arn of a systems parameter stork key that allows you to pull in secret parameters. So if you had secrets you want to put in there, you could definitely do that. I'm just going to now it might show up on the exam. We'll go ahead and hit Create unable to create this task definition. It doesn't like something I've done. I'm just looking through it here. looks okay to me. The only thing I can think of is because it doesn't have a TAS role. But it says optional Im role. Alright, so I tried going ahead and creating this task definition. And I got this here. And it's exactly what I thought it was, we just don't have permissions here. And so I did a bit of googling, and I believe we are missing this ECS task execution role. So here they have the instructions. So we will go ahead and go ahead and create that. So luckily, we still have I am open here, I'm going to create a new role. I think we need to choose ecso, select trust entity choose elastic Container Service. What's interesting is that this would actually have been created for us if we had launched Amazon ECS console first run experience. So the first time you do it automatically creates it, but like, I don't have anything to launch. I've heard das definition. It's like a catch 22. One of those things that I wish at best would improve upon I'm sure if I can play it on Twitter, they will definitely do that. But anyway, we'll just go ahead and manually do it. Because it's all about figuring stuff on your own here. And they might actually already have a pre made one here. But we'll just go down here. Select your use case, choose elastic Container Service task. Oh, there it is. Okay, we'll hit next. And it should automatically add the policy, I guess not attached permissions policy. Sometimes when you select those pre made ones, that automatically fills it in for you. But I guess in this case, that's the one we need. We'll hit next, we'll hit next. Let's name it what it suggests us to name it. Whoops, there we go. Now we have a roll, we'll scroll up, hit refresh. And there it is. And then we'll go down here, you're given permission to ECS to create and use the ECS. So here, it's suggesting that it would have created it on our behalf, but I never did. Alright. That's fine. We'll sit great. Anyway. Hi, all right, I'm back here again, sorry, for all the hard cuts, I'm just having a really hard time with ECS. I made this fall along in my other iOS account, it works perfectly the first time, and I'm just getting beat up at every single corner. And trust me, I go on Twitter. And I literally complain to AWS about these things, just because they are really painful. But I just want you to know that even myself as being an ageless expert. I even have hard time getting through these follow along. So you know, just stick with it if you have any issues. But anyway, I was able to create this task definition, I have to create a revision here to show you what was going on there. So I'm just gonna hit Create revision. And it has all the same filled in information. And the only problem I had, we definitely probably had to create this task role, we should have made it anyway. But we needed to click on here. And I had latest on the end of here, which I had I thought prior, but I guess it doesn't allow you to do it. So just remove latest on the end of an end of here. And then you'll be able to create this task definition. And also in this box here. I wasn't carefully reading it. But it was actually saying that it would have created this for us anyway. So if we left this blank, and we removed that latest there and created it, it would we would have had that role. But anyway, we made we made it through there. So just go ahead and hit Create. And you should be exactly where I am with the task definition. So now that we have our task definition, I'm definitely gonna close these things here. Because we have a lot of tabs open. I'm just going to keep on going here, all the way here, we are ready to launch this, this task. So we'll go over to clusters, oops, I clicked Eks. Definitely not doing Kubernetes. Today, we'll go to clusters here, we'll click on my ECS cluster. And we have services and tasks. So services continuously run tasks. And as soon as the task is done. So this is a web application, we definitely want to make it a service. So we'll hit Create here, we want to be easy to because that's free CS, we're not making a fargate type. We'll leave that name alone, that's totally fine. We'll name the service, I'm just going to call my service very unoriginal. We're gonna leave it as a replica. We want one task running here. We'll leave it as rolling update, that seems fine to me. Easy balance spreads seem fine to me, I don't play around with these too much. We'll go ahead and hit next. And it's gonna ask us what low bounce we want to use. We don't want when we want to save money, obviously, it's recommended to use a load balancer and and have things in an auto scaling group. But you know, we just want to be able to launch a service. That's all that we really need to learn here. And we'll scroll down we have a lot of options you don't need we don't need to read any of this will hit next that we have service auto scaling is without an auto scaling group. We do not need one so we'll hit next and then we will create the service. So this should actually be very fast. So when you log launch an easy to instance, it takes forever, right? Because you have to wait for it to create one. But there's already one running, all it has to do is put that task in there. So we just have to wait a little bit on this will take a little bit of time here the first time around. But I'm pretty sure that when we launch tasks after this initial setup here, it's really, really fast. So we'll just wait here a little bit, and I'll see you back in a moment. And that was actually really fast. I did not even have to stop the video, but I did so. So that was the started, we can go check out that my service button, click the big blue button down below. And here we can see our task is running. And it's running, study sync colon one. So version one. Yep, that version is fine. And we'll click into task because we only actually have one version. And in here, what we can do is we can see the container instance, if we click into that, it's going to give us information such as the public IP address, and etc. I think if we click into the task here, and we drop down here, I know we just went backwards, but actually shows us Port 80, and 8080. We just have this convenient link to get to it, there's tons of ways to get this link. And we can see our application is running. So version three was the one we had last and Elastic Beanstalk. So that's all it takes to run an ECS task. Now there's obviously a lot of options in ECS. Not important for this fall long. But definitely, you know, read through all the lecture content, if you are doing the DevOps, you definitely no need to know all those options. So way longer follow along with ECS. For that, but for the developer associate, all we need to know is how to launch that service. Now this cluster costs money, because it is an easy to instance, that is constantly running. So I'm gonna go ahead and delete it. So we'll just type in delete me. And it takes a little bit of time here. And once that's deleted, we'll be in good shape. I'm not going to wait around for this video to see this delete, it's deleting I know it will happen. But yeah, once this is done, that means we can move on to the fargate follow on which is very similar except there is no easy to instance running. So that's super exciting, because it's a serverless container. So we'll see you then, hey, this is Andrew Brown from exam Pro. And we are looking at fargate, which is serverless containers. So don't worry about the servers, run the containers and pay based on the duration and consumption. So fargate is sometimes branded ECS fargate, or just fargate by itself, but it is under the ECS console. But the difference is that when you want to launch fargate, you go ahead and you create an ECS cluster, but you actually make an empty cluster, so there's no provision to EC two, and then you launch the tasks as fargate. And you could also launch services there as well. So you no longer have to provision configure and scale clusters of EC two instances to run those containers, you're charged for at least one minute, and then after that it's per second. You pay based on the duration and consumption, which we'll look at a bit more. But to really understand the difference, I just want to give you a visual comparison. So this is ECS, which we saw prior. And you can see that there are you see two containers. And then for fargate. It's extremely similar except there's no containers. So you know, hopefully that makes that clear. So now we're going to look at how to configure a fargate task. And the first thing you're going to do is using the task definition wizard in the fargate console, you're going to have to choose how much memory and CPU will utilize entirely for all your tasks. And then what you're going to do is you're going to add containers and then allocate memory in CPU for each based on the requirements based on the allocation that you've defined above. So here I have a Ruby and a Python container. And you can see that I've split the memory, half and half for both. When you run the task, you can choose what VPC and subnet it'll run in a lot of people think that there are no VPC, subnets with fargate because it's a because it's serverless containers, but it's not true. So you do have flexibility flexibility around that. And you can apply security groups to a task. This is a great way to you know, have a secure security around your tasks. And this actually puts you up as an exam question knowing that you can apply that the reason why there's some, like this is such an important question is because, you know, would you apply the task to the EC two container that's running the actual server or would you apply it to the tasks and this is gonna apply for both ECS and fargate. It's always at the task level. You can also apply an IM role to the task those you can say for every individual task delegated different policies. And just to reiterate, here, you can apply a security group and an IM role for both ECS and fargate. So that's for both tasks and services. And again, that might show up as an example. So I want to do a quick comparison between fargate and lambda, because they're both serverless compute. And so they seem like they solve the same problem. But there are a few key differences. And so we're just going to quickly walk through those. The first is they both have cold starts. And it can be argued that fargate cold starts are slightly shorter, I can't really remember as to why, but in the documentation, I think it says that they're a big different factor is duration. So with a lambda, the maximum time you can run it is 15 minutes, where the duration with the fargate task can run as long as you want, because you can just make it a service. And it runs indefinitely. In terms of memory, lambda has an upper upper limit of three gigabytes, whereas fargate has an upper limit of 30 gigabytes. So if you need a lot of memory, go over with fargate. For containers, you provide your own containers with fargate. So you definitely have a lot more flexibility in terms of configuration there. For lambda, setting up containers, is extremely limited. Use the standardized containers, and then you build stuff on top of that. So if you really need something highly configurable, you're going to need to go with fargate. For integration, lambda just has seamless integration with a lot of serverless services. And it just makes it really easy to chain stuff together. Even recently, like lambda has output destinations, and it's just keeps on getting more and more easier to integrate with stuff fargate, you know, you can orchestrate things together. So like with step functions, you can tie things together, but it's just not as seamless as lambda. So and you know, you do have to do a lot of configuration to get your cluster set up and the fargate tasks set up. But definitely less less than doing an ECS cluster without you know, without fargate. And the last thing is pricing. So with lambda you're paying per 100 milliseconds, where with fargate, you're paying at least one minute and then every additional second is how the pricing works there. Obviously, the amount of memory you use, and the CPUs also comes into factor there. But just the takeaway that I want you to know is that lambda is 100 milliseconds, right, and fargate is one minute and every additional second. So hopefully, that makes a little bit of sense to you there. And you can kind of have an idea of like the use cases for each. Hey, this is Andrew Brown from exam Pro, and we are looking at the fargate. Follow along if you have yet to do the ECS and Elastic Beanstalk follow along, you have to do those ones first. And the reason why is that this is all dependent on a Docker image that we created in Elastic Beanstalk follow along. And it's good to do this ECS one first so that you get a difference between ECS and fargate. So, you know, go ahead and do that. And once you have this Docker image, you can proceed forward here. So for this one, what we're going to do is make our way over to ECS. Because that's where fargate is sometimes atrios calls it ECS fargate, sometimes it's just called fargate. All right, but anyway, fargate is ECS, we'll go over to clusters, we're going to create a new cluster, and we're going to use networking only because eight of us is going to manage the EC two instances for us. So we don't have the same kind of options as we would here with the other ones. If you were to go here and choose an empty cluster that is basically a fargate cluster, I'm going to go back here and choose networking only, which is the default. And we're going to name this cluster, I'm gonna call my fargate cluster fargate cluster, we are not going to create a VPC, we're gonna use the default one here, we're gonna hit create, and it creates it right away. Because it's serverless. It's super fast. There's there's no server running as of yet. So that's why it's super fast, we need to create a task definition, we created one before for study sync for ECS. And this time, we're going to make one for fargate. And we're going to name it study sync. And I'm going to put an F on the end. That's just my convention for distinguishing those tasks here. We're going to choose the ECS task execution role. If we had not had this here, I feel like it would have would make it for us. We have the network mo which is AWS VPC, I was on Ada support. And I was not aware of this until I talk to the sport engineer, I guess is the default. And the way it works is that whatever port you set is what is what mapping you're going to get and explain that in a moment. When we get to adding our container. We're going to choose Test memory size and we're gonna choose our V CPU, I noticed that like, I can't go up here. So there are some restrictions as to what you can do. A quarter of a virtual CPU is well enough for this example, no GS does not require a lot of memory, or memory or CPU power, at least our use case doesn't. We'll go ahead and add a container, we're going to need the ECR image here, I'm going to hit copy. And look here, it says that we can put colon tags. So in the ECS, follow along, I had colon latest from really sure that we can do. But for whatever reason wasn't working for me, maybe I had a spelling mistake, maybe there was something off there. But it'll definitely seems like I can go in here and use this one where has colon latest, but I just don't want to have any problems. So I'm going to leave that out there, we're going to name this container, I'm going to call it my fargate container. Okay, not not really original there, we'll just select 128, it's less than we used in the ECS. One, but it's totally fine. And notice, we don't have a host mapping. So this is what I was talking about this AWS VPC thing. So our host port is going to be 8080, we don't have a choice in the matter. That's all we get. We're going to scroll down, we're going to make our way over to the environment variables, I'm going to set one for port, and we'll be at 80. And then we'll set one on node E and V. production. Now, I guess if we want to run this on port 80, I could change this to 80. But the thing is, like, if you've ever ran, if you ever set up a server, anytime you run Port 80, on Amazon, Linux two, or whatever throws errors, you got to make sure it's sudo, it's a big pain, so we're just not going to do it. But for a production environment, you'd want some 80. Or we'd have to, we have to find another way around this like doing a proxy a container, which we'll talk about later. In this follow along here. We don't need to fill out anything else in here, there's a lot of options here not even worth discussing. And we're gonna hit Add. And you can see it's only utilizing this amount of the actual container, I'm going to scroll on down here all the way to the bottom, we're gonna hit create an ad creates super, super fast, we're gonna view that task definition. And we don't really need to do anything here. But we'll make our way over to the clusters. And we have my fargate cluster, and it should say one service running, it's probably just starting starting. It's getting going. So we'll click into it here. We should see a service Oh, no, we won't, because we just created a task definition, we actually haven't launched the service yet. So we'll hit create, we will choose the f1. We'll choose fargate. It's one we don't have anything else, we have my fargate cluster, a name is my fargate. Service, I ran into an error, where I named this my service, which was what we named the ECS one and then it errored out on me, which was weird because it didn't exist anymore. So you may have to fiddle with the name here. So I'm just changing it just so I avoid that issue. We only want one task to be running, we're gonna leave it as rolling deploys, and we'll go next. And then we have to choose the cluster VPC and subnet, which when you think of like serverless containers, you think you would have to choose anything, we're gonna go ahead here and choose VPC, we're gonna choose the first subnet, we're going to scroll all the way down, we're not going to use a load balancer, we just want to save money and not have any complications. But for a production environment, you definitely would want to set a load balancer all the way down the bottom, we're gonna hit next, we're not gonna use auto scaling group. But for production, I would absolutely do that. There's no additional cost there. So it's easy. And then we'll scroll all the way down to the bottom, we get a nice review. And we'll hit Create. And this is super fast, way faster than ECS. Well, actually, I shouldn't say that ECS is faster, because it doesn't have to start up a new CTO instance. So this looks like it's fast. But when you go here, you're gonna notice that it's provisioning. So it's actually not running as of yet. So UCS is way way faster for getting your tasks running. So that is one trade off, you have to consider. Now, while this is getting started, we can see we have this public IP. It's, it's in pending or provisioning. So if we go here, we're not going to get anything. And also it's not going to work because it's running on port 8080. So we're going to need to expose that port there is a security group attached to this cluster or this task or service or something. security groups are around tasks, so we should click into the task. Yeah, there it is. So we'll click into this. And we will go into inbound rules, we will edit a rule and we are going to add a new rule 8080 for everybody in the internet. Let's save and then we'll go back and we will check. Check on our service here to see if if it's warmed up. And so it's active. We'll click into it. We'll go to tasks. We'll click into the task, we will copy the public IP address dress. we'll paste it in here. And we'll do AD AD. And does it work? Nothing as of yet. Oh, it still says pending here. Okay, we don't have any errors. Let me go back here. pending. I'm just trying to make sure that it's not stopping and starting, if there was a configuration issue, it would stop and start. But it also would tell us the error somewhere in here might be in here or in the logs. But we're not seeing issues yet. So I think we just have to wait a little while it's still warming up. So what I'm going to do is just stop the video here. And when it is running, I'm going to come back or if there's an error, we'll talk about it. Okay, and we're back here is moved to activating state and says it's running, I was looking up to see what would happen if it was in a pending state, it could say that the Docker daemon is unresponsive, the Docker image is large, the ECS container agent lost connectivity, these this greater agent takes a long time to stop the existing task. But we're actually not having any problems, it was just me thinking that there was more of a problem there than there actually was. Because if we go here and copy this IP address, and then do AD AD, it works. So what if we wanted this not to be at ages Port 80, which would be nothing on here on the end, what we would need to do is either we would need to start like in our very variable, set it so that it's Port 80. And, but then we might have to set sudo, it's a big pain to get Port 80 running for web apps. And so I just didn't want to go through that hassle. Another thing you could do is you could have a, you can run multiple containers, and one container could be nginx. And nginx could be used as a proxy, where you change Port 8080 to Port 80. So that's something that you could use an nginx container for. And that'd be a great strategy, if you really have a bunch of containers, and then you want to then have more flexibility on it. They're mapping. But I just want to show you how the like that it is hard coded to Port 80 there. So we go back to that task definition, I'm going to go into here, study, think f f one, we'll go to the JSON. And you can see here it says AD AD. All right, and we didn't set that this is what it is hard coded into here. But we're done with this service, what we'll do is we'll go ahead and delete it. And we will type and delete me, I we don't have to do this because we never set up any service discovery with it. That would be for like app mesh, which I'm not really familiar with. Maybe that would show up in the interest advanced networking certification. And we will hit delete, and that is fargate. Now we can go ahead and delete this cluster, it's not going to do anything because it's not raining easy to instance, costs money, but let's just delete it because it's a good habit to do so. And there you go. So that should be deleted now. It's still showing up. Deleting Okay, sometimes you got to do it twice. Anyway, um, that's the fargate follow along and we're all done. Okay, so we're on to the ECS and fargate cheat sheet, I group these ones together because fargate is under the ECS console. And there's not a lot you need to know about these two. So let's get to it. So elastic Container Service is a fully managed container orchestration service highly secure, reliable, scalable, and way to run containers. The components of VCs is as follows cluster multiple, you see two instances that will host the Docker containers, a task definition file, a JSON file that defines the Configure configuration up to 10 containers that you want to run task. Launch containers defined in task definition tasks do not, do not remain running once workloads complete service ensure tasks remain running. So I think of a web app last to you 100% need to know container agent binary on each EC two instance, which monitors starts stops tasks. And we'll talk about ECR for a second here, I guess it's part of this cheat sheet as well. It is a fully managed Docker container registry that makes it easy for developers to store manage and deploy Docker container images. So think of it like the Git for Docker containers. And then we'll move on to fargate, which is the last thing here. So fargate is a sort of serverless containers. You don't worry about servers, run containers and pay based on duration and consumption. When you create a fargate cluster. It's just like making ECS cluster except it's empty. There's no servers, so it's really easy to set up. fargate has cold start. So if this is an issue, then you're want to consider using ECS which does not have cold start. The duration is as long as you want. So you can run this forever. This is important to note versus lambdas, which have a hard limit in terms of how long they can run. Memory can go up to 30 gigabytes, which is pretty great. And the price pay at least one minute and every additional second from then on. I didn't mention vcpus. I don't think it really matters. But there's a limit to how many v CPUs you can set on it. But it's definitely a lot more computing than lambda will give you. But yeah, there you go, that is ECS fargate, I guess ecrs. Well, hey, this is Andrew Brown from exam Pro. And we are looking at ABS X ray, which helps developers analyze and debug applications utilizing micro service architecture. And also another way to think of it as x ray is a distributed tracing system or a performance monitoring system. So to understand X ray, we're gonna have to understand what micro services are. So micro services is an architectural or organizational approach to software development, where software is composed of small, independent services that can communicate over well defined API's. So these are services owned by small self contained teams, microservice architecture makes apps easier to scale and faster develop enabling innovation accelerating time to market for new features. And to really put this in perspective, if you're using AWS, and you're using a host of different services, you might already be using micro service architecture. So the idea is that you have all these isolette services. So as you have your source storage, you have instead of using large GC tools to handle all the functionality of your application, you break it up into containers, or serverless functions. And then you have your databases, notifications, queuing and streaming. So in the combination of all these services being used, utilized together is microservice architecture. But the question is, is how do we keep track of or debug the communication between all these services, because if you're using a lot of them, it can get a bit confusing. And that's kind of what X ray solves. So what exactly is X ray? Well, it is a distributed tracing system. So let's talk about what that means. So distributed tracing, also known as distributed request tracing, is a method used to profile and monitor apps, especially those built using a microservice architecture. Distributed tracing helps pinpoint where failures occur, and what causes poor performance. Notice, at the end, they are where failures occur, and what causes poor performance. So those are, that's the major thing that X ray is doing. Now, performance monitoring, which is its own thing is also kind of the scope of X ray. So traditionally, we used to have application performance monitors, I mean, we still have them, but they refer traditional architecture where you had a single EC two. And all of your business logic or all the tasks that your application was handled was in one specific application. But let's talk about what performance monitoring is because X ray basically falls under that category. So monitoring and management of performance and availability of software apps APM. The A's for application strives to detect and diagnose complex application performance problems to maintain an expected level of service. So you could say that x ray is both of these. But you know, I would lean towards saying it's a distributed tracing system. So to try to help you understand X ray, let's just talk about some third party services that are kind of similar, where it's cloud monitoring or application performance monitoring, I do feel that these services are starting to tack on, like distributed tracing to their services. So some of these might not really match X ray, but I'm sure they will catch up in time. The number one service I can think of is data dog, which does a lot of stuff, it does APN, log monitoring, and other sorts like that, then you have New Relic was which was traditionally just application monitoring for large applications. But now we can do considerable more signal FX, which is supposed to be like data dog, but is more in real time. And then you have lamego, which is an I don't know if I'm saying it right. But that's that's all I'm saying lamego. And it is focused on serverless monitoring. So it looks a lot like x ray. But it has a huge emphasis on just serverless services. So you know, hopefully, if you go look up those and take a peek around there, it'll give you a better perspective on how X ray fits in the market of these monitoring services. So x ray is a distributed tracing system. And it collects data about requests dates that your application serves. It can view filter collected data to identify issues and avenues for optimization. For any trace request to your application. You can see detailed information not only about the request and response, but also about the calls that your app makes to downstream ated with resources, microservices, databases, HTTP web API's, and I'm certain that there are more things in there but just to visualize that, here, we can see that we have a trace and we have dynamodb. And it goes and it shows you all the steps within the table that it's taking. So you know you can get some very detailed information out of this. But let's move on and start breaking up. What are the components to X ray? Now we're going to take a look at the anatomy of X ray. So this basically, this is the overall landscape of how it works. So the first thing is we have the X ray console. And this is where we are going to be able to visualize monitor, or a drill down into our tracing data. That data is going to be coming from the X ray API. Now you think that you would just directly send your request to this API, but that's not the case, you send them to the X ray daemon. And this is used as a buffer, because you're going to be sending a lot of requests, the data is coming from the X ray SDK. And so here we have Ruby, Python and node, there's a lot more than just the ones there. We'll talk about that later. And then we have the SDK and see ally. And this could as well be sending the segment data to X ray Damon, or it can be directly interacting interacting with the X ray API. Let's talk about the X ray SDK, because this is where all our segment data is coming from. And that is what we're visualizing in the X ray console. So the X ray SDK provides a way to intercept incoming HTTP requests. So that means request information around that kind of stuff, then it has something called client handlers. And this is just really the, the SDKs that are specific per language. So when it says SDK client, it we're talking just about, like the Ruby client, or the Python Client, etc. And we're able to set up instrumenting, I will talk about that next slide, because instrument is kind of a vague term, that'll make more sense, soon enough. And then it also has an HTTP client. So we can use this to instrument calls to internal external HP web services. And also just to deliver our our information actually, to the X ray daemon. So that's another component of it. And I want to point out that the SDK does support instrumenting calls for the for SQL databases. And for other features, the one that's worth highlighting SQL is SQL databases, because it really lets you drill down to see what's happening with your database calls for that. But anyway, let's move on to what instrumenting is. So I mentioned the term instrumenting, or instrument in the last slide, I figured that was worth exploring more, because even myself, I wasn't sure what the term meant. So instrumenting is the ability to monitor or measure the level of a product's performance to diagnose errors and to write trace information. So to me, it just sounds like you're logging information. That's exactly what it is. So good. To get an idea of what it looks like to instrument something in X ray, is, we have this piece of code here. And this is for a Node JS, express JS application. And what we're doing is we're including the X ray SDK, and then what we do is we open up a segment segments and sub segments are the pieces of information that we want to capture, or to send X ray and that is instrumenting. So we have a part where we open and then we have a part that we close. And everything that is in between there is going to get captured, like the duration of it or additional information. And that's what's gonna get passed along. Generally, in the code, you don't have to necessarily set the segments the segments get captured. So using your code, you're setting up sub segments. But yeah, just to give you a visualization. So just remember that when you hear instrument, just think of it like logging information and sending it to X ray. Let's take a closer look here at the X ray daemon. And I said earlier, when we're looking at the anatomy that we do not send our segment data directly to the kms API, what we do is we send it to the daemon and then the daemon buffers it and then sends it off to the kms API. So let's look at that in more detail. So instead of sending your traces directly to X ray, the SDK sends Jason segment documents. So that's what the segments are made of JSON files to the daemon processes and listening on UDP traffic. So here's our SDKs and other clients. And here it is sending that JSON document to the X ray daemon, right. And then the X ray daemon buffers segments into a queue and uploads them to X ray in batches. So the idea is that it's creating this buffer. This makes sense, because if you are sending instrument, if you're sending logging or instrument data, you're gonna have a lot of requests. And so you don't want the API to be flooded there. So this is acts as a buffer. And then there it is sending the batch information to X ray API so that it's not so burdensome. You can install the daemon on Windows, Linux and Mac and it's already included on Elastic Beanstalk and lambda platforms. So I think when you're using it like when you're using the serverless, like, you're setting up a serverless application and you turn on X ray, there's already a daemon running there. I don't know where it is, but I know it's working. It looks like you've also just set one up locally for development. And x ray uses trace data from the universe resources that power your cloud applications to generate a detailed service graph. So let's just say all this segment data turns into that graph. So, you know, hopefully, that makes clear the utility of the X ray data. Now we're going to cover all the X ray concepts. So the first thing that we really need to understand are segments because that is the core of X ray. That's the data that X ray receives. And x ray can group segments and have common requests, which would be called traces. Okay. And then X ray processes those traces to generate a graph, and that provides a visual representation for your app. So that's the general idea. But there are a lot of components to it. So let's go through the list really quickly here. So we have segments, we have sub segments, we have service graph, we have traces, we have sampling, we have the tracing header, we have filter expressions, we have groups, we have annotation metadata, and then we have errors, faults and exceptions, mostly just exceptions. So we're going to walk through every single one of these components, and then we'll really understand how X ray works. So let's first look at the service graph. And this is something that's visible when you click into a trace. And so the service graph shows the client your front end services and back end services. And then might not be very clear looking at this, which is the client front end and back end. So I'm going to help divide this up so that we can make sense of that. So the first one is the client. It's just the little person in terms of representation. So nothing super exciting there. But then we still have a bunch of these other services are running. So then we have your front end services. And we can divide that here. And what we see running here is like computing and application integration. So that could be SNS, lambdas, ECS, you see two Sq s. So anything like that. And then the last thing is backend services. This is generally your databases. So here, all these calls are related to dynamodb. So you know, hopefully that makes that all clear there. And the service graph, the whole purpose of having this is help you perform, like identify bottlenecks, latency spikes, and other issues. So the idea is that you can click into any of these and drill down and really figure out how to optimize your. So let's take a look at segments, which is the most important thing to know about x ray. And so if you have a bunch of compute resources that are running your application logic there, you're going to want to send data about that to X ray. And we call that sending work. So we're sending our work off to specific segments. So here is an actual segment that we could drill down into. And there's a lot of different information here. So we have things like the host information. So that'd be hostname, alias, or IP address, I call this this lambda function, if you can see the origins as as lambda, I call it in the console. I didn't call through API gateway, so it doesn't have any of that data here. So that's why we're not seeing it. Then you'd have request information. So the method client address path user agent, again, I didn't use API gateway. So we're not seeing that information there. But if if I had done so we'd see a lot more information, then you'd have the response. So that's the status and content here we have a 200. So that means everything's good. The work done. So the start and end times, we have the duration and sub segments. And then the last thing would be issues that occurred so errors, faults and exceptions. And we have a dedicated tab there for that. So that's what it looks like to drill down into a segment. So let's take a look at sub segments, which allows us to get more granular timing information or details about downstream calls that your app makes. So these are basically operations within your app that you want to capture. So a sub segment can contain calls to either services external HTTP API's, or SQL, SQL calls within a database, and also things within dynamodb. And that's actually an example we're going to look at is dynamodb. So here, what we have is we have an Elastic Beanstalk environment and there's a put request going to it. And then underneath in that application, it's making calls to Dynamo dB. So we can see get item, update item, etc. So those are the sub segments. Now, you can actually define your own arbitrary sub segments to instrument specific functions or lines of code. And that's how I'm gonna show you the next step. But a lot of these things, you know, if you're using specific native services, they might already log these sub segments for you. But you know, if you need something more specific, you can write your own. So here, we have a trace. And within this trace, we have the segment and then there's a sub segment, can you tell where the custom sub segment is? It's right there. So that's the one I defined. And I want to show you how you do that in code. So here is the actual code example. And the first thing you're going to want to do is include the SDK. So there's the X ray SDK core, I'm writing this, obviously, no Gs. And it's very similar to when we defined a segment earlier on. But in this case, we first have to get the current segment. And then we're going to add a new sub segment. So I call this mining. And then you have some code that runs and then you have is you close the sub segment, so everything in between the certain end will be captured. And that's going to go to that segment there. I'm just doing console log. So it's 0.0 milliseconds, nothing super exciting there. You generally don't define, like you don't create segments in code. I mean, the earlier example, we saw it, because that was expressed yes application. But generally, the only thing you're defining in code for instrumentation is sub segments. But you know, that's just this is just an example to show you how to set up sub sub. Let's just take a look here at traces. So here I've drilled into a trace. And the way you can think of a trace is just, it's a way of grouping all your segments together. And they, they're all started from a single request, so that that first request is triggered. And then the idea is we need to keep track of its path through all these segments and services. So that's why we have a trace ID. And the idea is when that trace ID is generated for the first service that it interacts with, it's going to generate that ID and that's going to get passed along through all this stuff. And that's, that's what helps build out this, that like graph down below that tells us all that information. So we're going to talk about trace IDs a bit more. But we're gonna first talk more about sampling, because not every trace that happens here is actually recorded. So that confuses me a lot. When I first started using X ray, where I was triggering stuff, I was expecting to see every single thing that I triggered, recorded, and that's not the case. So I was saying the last slide, we were looking at traces that not every single request is actually passed on or captured. And the reason why is when you're using the X ray SDK, it uses a sampling algorithm to determine which request will actually get traced. So by default, the SDK is going to record the record the first request for each second, and 5% of every additional requests. So why would we have this why won't be the purpose of sampling. And the reason is to avoid incurring service charges. So when you're first getting started, you want the default sampling rate to be low. Just because you're not maybe not definitely X ray, and you don't realize how many traces or requests you're sending. And you might not be able to make use of that. So you can actually see the options that are set up for this particular sampling algorithm. And you can actually modify the sampling rule and add it add additional rule. So if you were over here, you can go hit Create sampling rule and set a bunch of options. So you can set it to match up on the service name, the service type, and a bunch of other things. And down below, you can see it says limit to one requests per second, and then five seconds per fixed rate. So the whole purpose of sampling is just to help you save money, because it's gonna help you reduce the amount of traces for high volume and unimportant requests, you're just going to find that there could be like a component within your application, where, let's say does polling, so it's constantly pulling like frequently, but you don't need to capture the information because it doesn't provide you any valuable information. Now we're gonna take a look at the trace header itself. So all requests are traced up to a configurable minimum, and after reaching that minimum, a percentage of the requests are traced to avoid unnecessary costs. Okay, the sampling decision and the trace ID are added to the HTTP requests in in tracing headers named x, Amazon trace ID. So we were talking about the trace Id get set, but this is just what it looks like. So the first X ray integrated service that the request hits is when the tracing header gets out. So here, this is what it looks like. It has a root, and it has a sampled number there. And then the next thing is, if you have a tracer header, where it's originating from an actual instrument and application, it could actually have this parent thing here as well, you don't really need to remember this stuff for the exam, I'm just showing it to you. But just understand that the trace editor determines whether this will show up in the in the in the trace or not, it's going to be based off that sample number that is assigned to it. So even with sampling, a complex application still generates a lot of data. So sampling isn't going to remove enough so that it's very easy for you to make sense of stuff. So that's where filter expressions come in. And this helps you narrow down to specific paths, or users. So here's an example. And this is we're looking at all traces. So there's a list of traces. And in that Filter Expression, you can put in a thing in there. And that's going to filter out what traces you want to look at. If you're wondering like what the actual syntax or filter expressions is, you just click that question mark there. And there's a lot of information under there. So these are a lot of attributes that you can use. Here, you can see that we were filtering out based off the request URL, but there's a lot of stuff here. And also that you can group these results together. So they have a bunch of predefined groups, for you to just make sense of stuff a lot easier. You can see in the graphic there by default, it is for URL. So let's talk about groups we just saw a group by but you can also assign a filter expression to a group that you create. And the way it works is you're going to name that group. And it could be just a name, or you can use an Arn. And the advantage of making your groups is that it's going to generate on service graph, create summaries and cloudwatch metrics. And this is gonna save you a lot of time when investigating very common scenarios. So the idea is right beside where the Filter Expression is, you can drop down and hit Create group. And from there, you're going to name that group and again, you can put an Arn in there. And then you just provide your filter expression. Notice that you can only create up to 25 groups by default. But I guess you can use a service limit to increase beyond that. If you're going beyond 25, you must have a very complex application. But I do want to warn you about these groups, because let's and this is just the the nature of it. So when you create a group and you set that Filter Expression, and let's say down the road, you know, like a week later, you want to adjust that Filter Expression doesn't, it's not going to retroactively apply that to all the previous data, it's only going to apply this regular expression for or this filter expression for future data. So that's an issue where the data is doesn't exactly represent what you expect it to look like, because you have this old expression and new expression. So what you can do is if it really matters to you that the expression only represents data that is exactly based on that Filter Expression, you're just gonna have to delete the current group and make a new one. So the X ray, you can add annotations and metadata to your segment documents. So that's the JSON files that get sent to X ray. That is generally information you're doing there. And you can aggregate this at the trace level. And it can be added to both the segments and a sub segments. It's not just limited to top level segments. And so let's talk about annotations. So this is just a key value pair that that is indexed for use with filter expressions. And you can have up to 50 annotations per trace. Now for metadata, you have a key value pair that are not indexed, and the values can be of any type, including objects and lists, so they sound really similar, they both key pair values, the only difference is that annotation is indexed. And metadata is not. So just to better understand the use case, you want to use annotations to record data that you want to use in group traces in the console or when you're calling the get trace summary. So it's really for group traces. And then you want use metadata to record data when you want to store in the trace, but you don't need to use it for searching traces. So for annotations, it's help, it's for you to help find data. And for metadata. It's just to have additional data there when you need it. And you can view the annotation and metadata in the segment sub segment details X ray console. So if you just click into a segment, you'll see annotations and met data there. So now let's just point out errors, faults and exceptions. So when an exception occurs, while your app is serving instrument requests, the X ray SDK records exception details and the stack Trace if available, these are the type of errors you could encounter. So we have error fault and throttle errors are client errors. So they're gonna be 400 fault is server errors, it's gonna be 500. Throttle is throttling errors, it's just too many requests. And if you're looking for this data, it's going to show up under the exceptions tab, tab for the segment details. And it also shows up in the actual segment graph there. So we're going to trace and you see all the segments and their their duration. Sometimes it will have like a little exclamation or it'll color it differently. So you know, there's an error at some part during the path of the trace. Now let's take a look at what services integrate in with X ray. And the most important we've highlighted in yellow because this is the Give me the most common use cases. So we have lambda API gateway app mesh cloud trail cloud, watch, Ava's config Elastic Beanstalk. I'm not exactly sure how that works, but Elastic Beanstalk seems to want integrate with everything nowadays. Then you have EBS you have SNS Sq s EC to elastic Container Service fargate. I wouldn't be surprised if it also supports elastic Kubernetes service. But yeah, that is the run of it. But the ones that are highlighted are the ones you most likely need to know. And probably ECS as well there, even though I didn't highlight. For the exam, we're going to need to know what languages does X ray support and supports go Java node, Python, Ruby, ASP. net and PHP. So it's all the culprits here, the only thing it doesn't support is PowerShell, which there's no reason to support that. But yeah, this is generally what AWS would support for most services for languages. So it has the whole gambit here. Hey, this is Andrew Brown from exam Pro, and we've made it to the X ray cheat sheet. And this one is a little bit long, but x ray is very important to know. So we're going to be very thorough here in our review. So let's get to it. So x ray helps developers analyze and debug applications that utilize a micro service architecture. Remember that it's really, really keen for micro service architectures to be using lambdas. Or, or you're using containers. X ray is really good for that. X ray is a distributed tracing system. It is a method used to prove to profile and monitor apps, especially those built using a micro service architecture, to pinpoint where failures occur and what causes poor performance. The X ray daemon is a software application that listens for traffic on UDP, UDP port 2000, gathers raw segment data and relays it to the database X ray API data is generally not sent directly to the X ray API and passes through the X ray daemon, which uploads in bulk and that's just to help create a buffer between the x, the X ray API, and the actual data being sent. Will the damond show up on the actual exam? Probably not. But we should really know all the components of X ray segments provide the resources name details about the request and details about the work done. Then you have sub segments, which provides more granular timing information and details about downstream calls of your app made to fulfill the original request, then you have your service graph, that graph that is that flow chart visualization for average response of microservices individually pinpoint failure, then you have traces. This collects all segments generated by similar requests so that you can track the path of request through multiple services. Then you have sampling, which is an algorithm that decides which requests should be traced by default X ray SDK records the first request each second, and 5% of additional requests. So if you have a question on the exam, and they're asking, like, why don't you see this information, just think about sampling, tracing headers, the name x, Amazon trace ID and identifies a trace with which pass along downstream services. What's important to remember here is x Amazon trace ID, you might see a question where they show multiple, like, like to pick the right one. So remembering that is key. Filter expressions allow you to narrow down specific paths or users groups allows allow you to save filter expressions, so you can quickly filter traces. And then on to the second page, we're almost done here. annotation metadata allows you to capture additional additional information and key value pairs. So for annotations, they are index, for use with filter expressions with a limit of 50. Metadata or not index use metadata to record data you want to store in the trace, but don't need to use for searching traces. So if you need to search for traces, and need that metadata you're going to be used or that additional information, you're going to be using annotations. errors are 400 faults or 500. Throttle is four to nine too many requests. You probably want to just know the last one there. X ray supports the following languages. I don't feel like they'd asked you a question but they used to, but I think the exam is getting a little bit harder. So they're not they're not just asking you to like Choose the language that is not applicable anymore. But X ray supports basically all possible language, the data Lewis Howes on there, as long as they're not telling you, you're putting Perl on the list, it should be pretty easy to figure that out. X ray supports a ton of service integrations with the following lambda API gateway app mesh cloud trail cloud watch, Eva's config, Eb lb, MSS Sq s, easy to ECS and fargate. So there you go, that is x ray. And we are done here. Hey, this is Andrew Brown from exam Pro. And we are looking at Amazon certificate manager also known as ACM, which is used to provision manage and deploy public and private SSL certificates for use within your AWS services. So let's look at ACM a little bit more in detail. It handles the complexity of creating and managing public SSL certificates for your AWS based websites and applications. It handles two types of certificates we have public, those are ones that are provided by AWS, and they are free. And then you have private. So these are certificates that you import, and they cost $400 per month, you generally just want to use a public certificate, if you ever use Let's Encrypt, those are all public. So if you're comfortable with that you're comfortable with these ones, just make sure that when you're creating that certificate, you do not make the wrong choice. I myself have chosen incorrectly, and but luckily I reached out to to support and before they charged me they fixed that issue. But that is a tricky one to get your money back on if you make that mistake. So ACM can handle multiple subdomains and wildcard domains. So you can see them entering exam Pro is the naked domain. And then I have a wildcard one. That is the setup, I always recommend that you use because otherwise, you'd have to create a bunch of domains, or sorry certificates later. And that's kind of a pain. And ACM is attached to very specific Eva services or resources. So you can attach it to elastic load balancer CloudFront API gateway, and you can apparently use it with Elastic Beanstalk. But I'm imagining that is through the ELD. So those are the three services you need to know that ACM attaches to so remember that, there you go. So we're gonna look at a couple of ACM examples. And the point of this is to understand SSL termination. So the first one here we have is we're using ACM, and we're attaching that certificate to our load balancer, which is application load bouncer. And if you see that line, the idea is that this red line represents the traffic that is encrypted. And so once it hits the alrb, the certificate is going to decrypt that, that traffic, and then everything between the alrb to the CPU instance, is now unencrypted. And that's totally fine, because it's within your network. So it's still secure. But you know, for someone to take advantage of that, they'd have to break into your AWS account, and they'd have to be able to intercept that traffic. So it's a very low risk. But but at the ACM can only really be attached to lots of little bits, or CloudFront, or API gateway. So it's not easy to protect this traffic here. But the advantage of attaching your certificates at this level here is that you can add as many easy to instances as you want, you don't have to configure each one of them to be able to handle a certificate. So that makes it a lot easier to manage certificates. Now, the other case is terming SSL end to end. And this is where the traffic from the start to the finish is encrypted. So even within your network, it's going to be encrypted. And the way you do that. I don't know how to do with ACM, I don't even think you can deal with ACM, ACM, because I've only noted being able to attach to resources over here. But you, I guess you could use Let's Encrypt. And so you'd have to set that up on every single server, and then rotate them out. And that's kind of a bit of a hassle to maintain. But a lot of people are used to doing that. And this is if you need end to end encryption, this is going to be dependent on your compliancy. So if you're a large corporation, maybe you have like a rule that says you have to encrypt end to end. But I have for 99% of other use cases, this is more ideal terminating SSL at the load balancer. So there you go. Hey, this is Angie brown from exam Pro, and we are looking at remedy three, which is a highly available and scalable domain name service. So whenever you think about revenue three, the easiest way to remember what it does is think of GoDaddy or Namecheap, which are both DNS providers. But the difference is that revenue three has more synergies with AWS services, so you have a lot more rich functionality that you could do on on AWS than you could with one of these other DNS providers. So what can you do with revenue three, you can register and manage domains, you can create various record sets on a domain, you can implement complex traffic flows such as bluegreen, deploys, or failing You can continuously monitor records via health checks and resolve epcs outside of AWS. So here I have a use case. And this is actually how we use it at exam Pro is that we have our domain name, you can purchase it, or you can have revenue three managed the name servers, which allow you to then set your record sets within route 53. And so here we have a bunch of different record sets for sub domains. And we want those sub domains to point to different resources on AWS. So for our app, our app runs behind elastic load balancer, if we need to work on an ami image, we could launch a single EC two instance and point that subdomain there for our API, if it was powered by API gateway, we use that subdomain for that for our static website hosting, we would probably want to point to CloudFront. So the WW dot points to CloudFront distribution. And for fun and for learning, we might run a minecraft server on a very specific IP, probably would be elastic IP because we wouldn't want it to change. And that could be Minecraft exam pro Co. So there's a basic example. But we're going to jump into all the different complex rules that we can do in revenue three here. So in the previous use case, we saw a bunch of sub domains, which were pointing to AWS resources, well, how do we create that link so that a revenue three will point to those resources, and that is by creating record sets. So here, I just have the form for record sets. So you can see the kind of the types of records that you can create. But it's very simple, you just fill in your sub domain, or even leave the naked domain, and then you choose the type. And in the case for a this is allows you to point this sub domain to a specific IP address, you just fill it in, that's all there is to it. Okay, now, I do need to make note of this alias option here, which is a special option created by AWS. So here in the next slide here, we've set alias to true. And what it allows us to do is directly select specific AWS resources. So we could select CloudFront, Elastic Beanstalk, EOB, s3 VPC API gateway. And why would you want to do this over making a traditional type record? Well, the idea here is that this alias has the ability to detect changes of IP addresses. So it continuously keeps pointing that endpoint to the correct resource. Okay. So that's normally when if, if and whenever you can use alias always use alias because it just makes it easier to manage the connections between resources via roughly three record sets. And the limitations are listed here as follows. The major advantage of Route 53 is it's seven types of routing policies. And we're going to go through every single one here. So we understand the use case, for all seven, before we get into that a really good way to visualize how to work with these different routing policies is through traffic flow. And so traffic flow is a visual editor that lets you create sophisticated routing configurations within route 53. Another advantage of traffic flow is that we conversion, these policy routes, so if you created of complex routing policy and you wanted to change it tomorrow, you could save it as version one, version two, and roll, roll this one out or roll back to that. And just to play around traffic flow, it does cost $2 per policy record. So this whole thing is one policy record. But they don't charge you until you create it. So if you do want to play around with it, just just create a new traffic flow, and name it and it will get you'll get to this visual editor. And it's not until you save this. So you can play around with this to get an idea of like all the different routing rules and how you can come up with creative solutions. But now that we've covered traffic flow, and we know that there are seven routing rules, let's go deep and look at what we can do. So we're gonna look at our first routing policy, which is the simple routing policy. And it's also the default routing policies. So when you create a record set, and here I have one called random, and we're on the a type here, down below, you're gonna see that routing policy box that's always by default set to simple Okay, so what can we do with simple The idea is that you have one record, which is here, and you can provide either a single IP address or multiple IP addresses. And if it's just a single, that just means that random is going to go to that first IP address every single time. But if you have multiples, it's going to pick one at or at random. So it's good way to make like a if you wanted some kind of random thing made for a to b testing, you could do this and that is as simple as it is. So there you go. So now we're looking at weighted routing policies. And so what a weighting routing policy lets you do is allows you to split up traffic based on different weights assigned. Okay, so down below, we have app.example.co. And we would create two record sets in roughly three, and they'd be the exact same thing, we both say after example car, but we'd set them both to waited, and we give them two different weights. So for this one, we would name it stable. So we've named that one stable, give it 85%. And then we make a new record set with the exact same sub domain, and set this one to 15%. And call experiment, okay. And the idea is that when ever traffic, any traffic hits app.example.co, it's going to look at the two way two values, a 5% is going to go to the stable one. And for the 15%, it's going to go to the experimental one. And a good use case for that is to test a small amount of traffic to minimize impact when you're testing out new experimental features. So that's a very good use case for a weighted routing. So now we're going to take a look at latency based routing. Okay, so layer based routing allows you to direct traffic based on the lowest network latency possible for your end user based on a region. Okay, so the idea is, let's say people want to hit app dot exam pro.co. And they're coming from Toronto. Alright, so coming from Toronto. And the idea is that we have, we've created two records, which have latency with this sub domain, and one is set to us West. So that's on the west coast. And then we have one central Canada, I believe that's located in Montreal. And so the idea is that it's going to look here and say, Okay, which one produces the least amount of latency, it doesn't necessarily mean that it has to be the closest one geographically, just whoever has the lowest return in milliseconds is the one that it's going to route traffic to. And so in this case, it's 12 milliseconds. And logically, things that are closer by should be faster, and so that, so it's going to route it to this a lb, as opposed to that one. So that's, that's how latency based routing works. So now we're looking at another routing policy, this one is for failover. So failover allows you to create an Active Passive setup in situations where you want a primary site in one location, and a secondary data recovery site and another one. Okay, another thing to note is that revenue three automatically monitors via health checks from your primary site to determine if that that endpoint is healthy. If it determines that it's in a failed state, then all the traffic will be automatically redirected to that secondary location. So here we have down below an example. So we have apt out exam pro CO, and we have a primary location and a secondary one. Alright. And so the idea is that roughly three, it's going to check. And if it determines that this one is unhealthy based on a health check, it's going to then reroute the traffic to our secondary location. So you'd have to create you know, two routing policies with the exact same. The exact same domain, you just said which one is the primary and which one is the secondary, and it's that simple. So here, we are looking at the geolocation routing policy. And it allows you to direct traffic based on the geolocation, geographical location of where the request is originating from. So down below, we have a request from the US hitting app dot exam pro.co. And we have a a record set for a geolocation that's set for North America. So since the US is in North America, it's going to go to this record set. Alright. And that's as simple as that. So we're going to look at geo proximity routing policy, which is probably the most complex routing policy is a bit confusing, because it sounds a lot like geolocation, but it's not. And we'll see shortly here, you cannot create this using record sets, you have to use traffic flow, because it is a lot more complicated, and you need to visually see what you're doing. And so it's gonna be crystal clear, we're just going to go through here and look at what it does. So the idea is that you are choosing a region. So you can choose one of the existing Eva's regions, or you can give your own set of coordinates. And the idea is that you're giving it a bias around this location, and then it's going to draw boundaries. So the idea is that if we created a geo proximity routing for these regions, this is what it would look like. But if we were to give this 120 5% more bias, you're going to see that here it was a bit smaller, now it's a bit larger, but if we minus it, it's going to reduce it. So this is the idea behind a geo proximity where you have these boundaries, okay. Now, just to look at in more detail here, the idea is that you can set as many regions or points as Do you want here and so here, I just have two as an example. So I have China chosen over here. And this looks like we have Dublin chose. So just an idea to show you a simple example. Here's a really complicated one here, I chose every single region just so you have an idea of splits. So the idea is you can choose as little or as many as you want. And then you can also give it custom coordinates. So here I chose Hawaii. So I looked at the Hawaii coordinates, plugged it in, and then I turned the bias down to 80%. So that it would have exactly around here, and I could have honed it in more. So it just gives you a really clear picture of how geo proximity works. And it really is boundary bays, and you have to use traffic flow for that. So the last routing policy we're gonna look at is multivalue. And multivalue is exactly like simple routing policy. The only difference is that it uses a health check. Okay, so the idea is that if it picks one by random, it's going to check if it's healthy. And if it's not, it's just going to pick another one by random. So that is the only difference between multivalue and simple. So there you go. Another really powerful feature of Route 53 is the ability to do health checks. Okay, so the idea is that you can go create a health check, and I can say for app.exam.pro.co, it will check on a regular basis to see whether it is healthy or not. And that's a good way to see at the DNS level if something's wrong with your instance, or if you want to failover. So let's get into the details of here. So we can check health every 30 seconds by default, and we can it can be reduced down to 10 seconds, okay, eye health checking, initiate a failover. If status is returned, unhealthy, a cloudwatch alarm can be created to alert you of status unhealthy, a health check can monitor other health checks to create a chain of reactions, you can have up to 50 in a single AWS account. And the pricing is pretty affordable. So it's 50 cents. So that's two quarters for per endpoint on AWS. And there are some additional features, which is $1 per feature. Okay. So if you're using route 53, you might wonder well, how do I route traffic to my on premise environment. And that's where revenue three resolver comes into play, formerly known as dot two. resolver is a regional service that lets you connect route DNS queries between your VBC and your network. So it is a tool for hybrid environments on premises and cloud. And we have some options here, if we just want to do inbound and outbound inbound only or outbound only. So that's all you really need to know about it. And that's how you do hybrid networks. So now we're taking a look at revenue three cheat sheet, and just we're going to summarize everything that we have learned about revenue three, so revenue three as a DNS provider to register and manage domains create record sets, think GoDaddy or namecheap. Okay, there's seven different types of routing policies, starting with simple routing policy, which allows you to input a single or multiple IP addresses to randomly choose an endpoint at random, then you have weighted routing, which splits up traffic between different weights assigns of percentages latency based routing, which is based off of routing traffic to the based on region for the lowest possible latency for users. So it's not necessarily the closest geolocation but the the lowest latency, okay, we have a failover routing, which uses a health check. And you set a primary and a secondary, it's going to failover to the secondary if the primary health check fails, you have geolocation which roads traffic based on the geolocation. So this is based on geolocation would be like North America or Asia. Then you have geo proximity routing, which can only be done in traffic flow allows you to set biases so you can set basically like this map of boundaries based on the the different ones that you have, you have multi value answer which is identical to simple, simple routing, the only difference being that it uses a health check. In order to do that. We look at traffic flow, which is a visual editor for changing routing policies, you conversion those record those policy records for easy rollback, we have alias record, which is a debases Smart DNS record which detects IP changes freedoms resources and adjusts them automatically always want to use alias record, when you have the opportunity to do so you have route 53 resolver, which is a hybrid solution. So you can connect your on premise and cloud so you can network between them. And then you have health checks which can be created to monitor and and automatically failover to another endpoint. And you can have health checks monitor other health checks to create a chain of reactions for detecting issues for endpoints Hey, this is Andrew Brown from exam Pro. And we are going to take a look here at AWS command line interface, also known as ci, which control multiple services from the command line and automate them through scripts. So COI lets you interact with AWS from anywhere by simply using a command line. So down below here, I have a terminal, and I'm using the ADB COI, which starts with AWS. So to get this installed on your computer, AWS has a script, a Python script that you can use to install the COI. But once it's installed, you're going to now have the ability to type AWS within your terminal followed by a bunch of different commands. And so the things that you can perform from the CIA is you could list buckets, upload data to s3, launch, stop, start and terminate, you see two instances, updates, security groups create subnets, there's an endless amount of things that you can do. All right. And so I just wanted to point out a couple of very important flags, flags are these things where we have hyphen, hyphen, and then we have a name here. And this is going to change the behavior of these COI commands. So we have output and so the outputs what's going to be returned to us. And we have the option between having Jason table and plain text. I'm for profiles, if you are switching between multiple AWS accounts, you can specify the profile, which is going to reference to the credentials file to quickly let you perform CLA actions under different accounts. So there you go. So now we're going to take a look at eight of a software development kit known as SDK. And this allows you to control multiple AWS services using popular programming languages. So to understand what an SDK is, let's go define that. So it is a set of tools and libraries that you can use to create applications for a specific software package. So in the case, for the EVAs SDK, it is a set of API libraries that you you that let you integrate Ada services into your applications. Okay, so that fits pretty well into the description of an SDK. And the SDK is available for the following languages. We have c++, go Java, JavaScript, dotnet, no, Jess, PHP, Python, and Ruby. And so I just have an example of a couple of things I wrote in the SDK. And one is a no Jess and one is Ruby into the exact same script, it's for ABS recognition for detecting labels. But just to show you how similar it is among different languages, so more or less the, the syntax is going to be the same. But yeah, that's all you need to do. So in order to use the line SDK, we're going to have to do a little bit work beforehand and enable programmatic access for the user, where we want to be able to use these development tools, okay. And so when you turn on programmatic access for user, you're going to then get an access key and a secret, so then you can utilize these services. And so down below, you can see I have an access key and secret generated. Now, once you have these, you're gonna want to store them somewhere, and you're gonna want to store them in your user's home directory. And you're going to want them within a hidden directory called dot AWS, and in a file called credentials. Okay, so down below here, I have an example of a credentials file. And you'll see that we have default credentials. So if we were to use CLR SDK, it's going to use those ones by default if we don't specify any. But if we were working with multiple AWS accounts are going to end up with multiple credentials. And so you can organize them into something called profiles here. And so I have one here for enterprise D, and D, Space Nine. So now that we understand programmatic access, let's move on to learning about CLR. Hey, this is Andrew Brown from exam Pro. And we are going to do the COI and SDK follow along here. So let's go over to I am and create ourselves a new user so we can generate some database credentials. Oh, so um, now we're going to go ahead and create a new user. And we're going to give them programmatic access so we can get a key and secret. I'm going to name this user Spock, okay. And we're going to go next here. And we're going to give them developer permissions, which is a power user here, okay, and you can do the same here. So our cloud nine environment is ready here. Okay, and we have a terminal here, and it's connected to any c two instance. And the first thing I'm going to do is I just can't stand this light theme. So I'm gonna go to themes down here, go to UI themes, and we're going to go to classic dark, okay, and that's gonna be a lot easier on my eyes here. And so the first thing we want to do is we want to plug in our credentials here so that we can start using the COI. So the COI is already pre installed on this instance here. So if I was to type AWS, we already have access to it. But if we wanted to learn how to install it, let's actually just go through the motions of that. Okay, so I just pulled up a couple of docks here just to talk about the installation process of the COI, we already have the COI installs of a type universe, it's already here. So it's going to be too hard to uninstalled it just to install it to show you here, but I'm just going to kind of walk you through it through these docks here just to get you an idea how you do it. So the COI requires either Python two or Python three. And so on the Amazon Linux, I believe that it has both. So if I was to type in pi and do virgin here, okay. Or just Python, sorry, I'm always thinking of shorthands. This has version 3.6, point eight, okay. And so when you go ahead and install it, you're going to be using Pip. So PIP is the way that you install things in Python, okay, and so it could be PIP or PIP three, it depends, because there used to be Python two, and southern Python three came out, they needed a way to distinguish it. So they called it PIP three, but Python two is no longer supported. So now pin three is just Pip. Okay, so you know, just got to play around based on your system, okay. But generally, it's just Pip pip, install ATSC Li. And that's all there is to it. And to get Python install, your system is going to vary, but generally, you know, it's just for Amazon, Linux, it is a CentOS, or Red Hat kind of flavor of Linux. So it's going to use a Yum, install Python. And just for all those other Unix distributions, it's mostly going to be apt get Okay. So now that we know how to install the CLA, I'm just gonna type clear here and we are going to set up our credentials. Alright, so we're going to go ahead and install our credentials here, they're probably already installed, because cloud nine is very good at setting you up with everything that you need. But we're going to go through the motions of it anyway. And just before we do that, we need to install a one thing in cloud nine here. And so I'm going to install via node package manager c nine, which allows us to open files from the terminal into Cloud Nine here. And so the first thing I want you to do is I want to go to your home directory, you do that by typing Tilda, which is for home, and forward slash, OK. And so now I want you to do l LS, hyphen, LA, okay. And it's going to list everything within this directory, and we were looking for a directory called dot AWS. Now, if you don't have this one, you just type it MK Dir. And you do dot AWS to create it, okay, but already exists for us. Because, again, cloud nine is very good at setting things up for us. And then in here, we're expecting to see a credentials file. And that should contain our credential. So typing c nine, the program we just installed there, I'm just going to do credentials here, okay. And it's going to open it up above here. And you can already see that it's a set of credentials for us, okay. And I'm just going to flip over and just have a comparison here. So we have some credentials. And it is for I don't know who, but we have them. And I'm going to go ahead and add a new one, I'm just gonna make a new one down here called Spock, okay. All right. And basically, what I'm doing is I'm actually creating a profile here, so that I can actually switch between credentials. Okay. And I'm just going to copy and paste them in here. Alright, and so I'm just going to save that there. And so now I have a second set of credentials within the credential file there, and it is saved. And I'm just going to go down to my terminal here and do clear. And so now what I'm going to do is I'm going to type in AWS s3 Ls, and I'm going to do hyphen, hyphen profile, I'm going to now specify spark and that's going to use that set of credentials there. And so now, I've done that using sparks credentials, and we get a list of a bucket stuff. Okay. So now if we wanted to copy something down from s3, we're going to use AWS s3, CP. And we are going to go into that bucket there. So it's going to be exam Pro, enterprise D, I have this from memory. And we will do data dot jpg, okay. And so what that's going to do is it's going to download a file but before actually run this here, okay. I'm just going to CD dot dot and go back to my home directory here. Okay. And I'm just going to copy this again here and paste it and so I should be able to download it but again, I got to do a hyphen having profile specifies proc spark because I don't want to use the default profile there. Okay, um, and uh, complain because I'm missing the G on the end of that there, okay? And it's still complaining. Maybe I have to do s3 forward slash forward slash huh? Ah, no, that's the command. Oh, you know why? It's because when you use CP, you have to actually specify the output file here. So you need your source and your destination. Okay, so I'm just good. Dr. Spock are sorry, data sorry, data dot JPG there. Okay. And that's going to download that file. So, I mean, I already knew that I had something for AWS there. So I'm just going to go to AWS to show you that there. So if you want to do the same thing as I did, you knew, you definitely need to go set up a bucket in a three. Okay? So if I just go over here, we have the exam, pro 00, enterprise D, and we have some images there. Okay, so that's where I'm grabbing that image from. And I can just move this file into my environment directory, so I actually can have access to it there. Okay, so I'm just going to do MB data. And I'm just going to move that one directory up here. Okay. All right. And so now we have data over here, okay. And so, you know, that's how you'd go about using the CLA with credentials. Okay. Yeah, we just opened that file there if we wanted to preview it. Okay. So now let's, uh, let's move on to the SDK. And let's use our credentials to do something programmatically, okay. So now that we know how to use the COI, and where to store credentials, let's go actually do something programmatically with the SDK. And so I had recently contributed to database docs, for recognition. So I figured we could pull some of that code and have some fun there. Okay. So what you do is go to Google and type in Avis docs recognition. And we're going to click through here to Amazon recognition, we're going to go to the developer developers guide with HTML, apparently, they have a new look, let's give it a go. Okay, there's always something new here. I'm not sure if I like it. But this is the new look to the docks. And we're going to need to find that code there. So I think it is under detecting faces here. And probably under detecting faces in an image, okay. And so the code that I added was actually the Ruby and the node GS one, okay, so we can choose which one we want, I'm going to do the Ruby one, because I think that's more fun. And that's my language of choice. Okay. And so I'm just going to go ahead and copy this code here. Okay. And we're going to go back to our cloud nine environment, I'm going to create a new, a new file here, and I'm just going to call this detect faces, ooh, keep underscore their faces.rb. Okay. And I'm just gonna double click in here and paste that code in. Alright. And what we're going to have to do is we're going to need to supply our credentials here, generally, you do want to pass them as in as environment variables, that's a very safe way to provide them. So we can give that a go. But in order to get this working, we're going to have to create a gem file in here. So I'm just going to create a new file here. Because we need some dependencies here, we're just going to type in gem file, okay. And within this gem file, we're going to have to provide the gem recognition. Okay, so I'm just gonna go over here and supply that there. There is a few other lines here that we need to supply. So I'm just gonna go off screen and go grab them for you. Okay, so I just went off screen here and grabbed that extra code here. This is pretty boilerplate stuff that you have to include in a gem file. Okay. And so what this is going to do, it's going to install the AWS SDK for Ruby, but specifically just for recognition. So I do also have open up here, the AWS SDK, for Ruby, and for no GS, Python, etc, they all have one here. And so they tells you how you can install gems. So for dealing with recognition here, I'm just gonna do a quick search here for recognition. Okay, sometimes it's just better to navigate on the left hand side here. Alright, and so I'm just looking for a recognition. Okay, and so if we want to learn how to use this thing, usually a lot of times with this, it's going to tell you what gem you're gonna need to install. So this is the one we are installing. And then we click through here through client, and then we can get an idea of all the kinds of operations we can perform. Okay, so when I needed to figure out how to write this, I actually went to the CLR here, and I just kind of read through it and pieced it together and looked at the output to figure that out. Okay, so nothing too complicated there. But anyway, we have all the stuff we need here. So we need to make sure we're in our environment directory here, which is that Spock dev directory. So we're going to type tilde out, which goes to our home directory environment, okay, we're gonna do an ls hyphen, LA. And just make sure that we can see that file there and the gem file, okay, and then we can go ahead and do a bundle install. All right, and so what that's going to do is it's going to now install that dependency. so here we can see that installed the EVAs SDK, SDK core and also recognition. Okay, and so now we have all our dependencies to run the script here. So the only thing that we need to do here is we need to provide it an input. so here we can provide it a specific bucket and a file. There is a way to provide a locally, we did download this file, but I figured what we'll do is we'll actually provide the bucket here. So we will say, what's the bucket called exam pro 000. And the next thing we need to do is define the key. So it's probably the key here. So I'm going to do enterprise D. Okay, and then we're just going to supply data there. All right. And we can pass these credentials via the environment variables, we could just hard code them and paste them in here. But that's a bit sloppy. So we are going to go through the full motions of providing them through the environment here. And all we have to do to do that is we're just going to paste in, like so. Okay. And we're just going to copy that, that's the first one. And then we're going to do the password here. Oops. Okay. And hopefully, this is going to work the first time, and then we'll have to do bundle exec detect faces, okay. And then this is how these are going to get passed into there. And assuming that my key and bucket are correct, then hopefully, we will get some output back. Okay. All right, it's just saying it couldn't detect faces here, I just have to hit up here, I think I just need to put the word Ruby in front of here. Okay, so my bad. Alright, and we is it working. So we don't have the correct permissions here. So we are a power user. So maybe we just don't have enough permission. So I'm just going to go off screen here and see what permissions we need to be able to do this. So just playing around a little bit here, and also reading the documentation for the Ruby SDK, I figured out what the problem was. And it's just that we don't need this forward slash here. So we just take that out there, okay, and just run what we ran last there, okay. And then we're gonna get some output back. And then it just shows us that it detected a face. So we have the coordinates of a face on and if we used some additional tool there, we could actually draw overtop of the image, a bounding box to show where the face is detected. There's some interesting information. So it detected that the person in the image was male, and that they were happy. Okay. So, you know, if you think that that is happy, then that's what recognition thinks, okay. And it also detected the face between ages 32 and 48. To be fair, data is an Android, so he has a very unusual skin color. So you know, it's very hard to do to determine that age, but I would say that this is the acceptable age range of the actor at the time of so it totally makes sense. Okay. So yeah, and there you go. So that is the pragmatic way of doing it. Now, you don't ever really want to ever store your credentials with on your server, okay? Because you can always use Iam roles, attach them to EC two instances, and then that will safely provide credentials onto your easy two instances, to have those privileges. But it's important to know how to use the SDK. And whenever you're in development, working on your local machine, or maybe you're in cloud nine environment, you are going to have to supply those credentials. Okay. So there you go. So now that we are done with our eight, or eight USC Li and SDK, follow along here. So we're on to the AWS COI in SDK ci ci, so let's jump into it. So ci stands for command line interface SDK stands for software development kit. The COI lets you enter interact with AWS from anywhere by simply using a command line. The SDK is a set of API libraries that let you integrate data services into your applications. promatic access must be enabled per user via the Iam console to UCLA or SDK a to s config command is used to set up your ad credentials for this Eli, the CLA is installed via a Python script credentials get stored in a plain text file, whenever possible use roles instead of at this credentials. I do have to put that in there. And the SDK is available for the following programming languages c++, go Java, JavaScript, dotnet, no GS, PHP, Python and Ruby. Okay, so for the solution architect associate, they're probably not going to ask you questions about the SDK, but for the developer, there definitely are. So just keep that in. Hey, this is Andrew Brown from exam Pro. And we are looking at key management service, which is used for creating and managing encryption keys for a variety of database services or within your applications. And the way I like to think of it is that whenever you see a checkbox in AWS to encrypt something, it's very likely using kms. So kms makes it easy for you to create control and rotate encryption keys used to encrypt your data on AWS and kms is a multi tenant hardware security module, which we're going to talk about in the next slide here. And the main takeaway I want you to remember about kms is that whenever you are using at a service, and you have the option to check box on encryption, so here we have an example of EBS. You're going to checkbox on and then choose a master key. And that's all you have to do. It's going to vary per service. But that's pretty much the routine. And kms can be used alongside with cloud trail to audit access history. So you have to investigate who used what key, that's how you're going to do it. And ATMs integrates with a lot of different AWS services. So here I've highlighted the ones which are most important to remember for the associate exam. So you got EC two Sq S, s3, dynamodb, elastic, cash, RDS, and more. Okay. So kms is a multi tenant HSM. But what does that mean? So HSM, which stands for hardware security module is a hardware that is specialized for storing your encryption keys. It's designed to be tamper proof, and it stores those keys in memory. So they're never written to disk. So imagine the power went out, those keys are gone. And that is actually a security feature. And so here is an example of a piece of HSM. And these are really, really, really expensive. And so this is where kms comes into play, because it is multi tenant, meaning that there are multiple customers who are utilizing the same piece of hardware. So you're sharing the costs with a bunch of different items, customers, and those customers are isolated virtually from each other. So there is software that protects you from other people's data. But if you had one customer who utilize the entire piece of hardware, which we would call dedicated, that would be also considered a single tenant because there's only one person using that, that server, and it AWS actually has a single tenant HSM, and that is called Cloud HSM. And this is going to give you a lot more control. And the reason why people would use Cloud HSM over kms is that cloud HSM is FIPS 142, level three, whereas kms is only FIPS 142 level two, but the takeaway from that is just understand that cloud HSM is, you know more for enterprises that need to meet those regulations. But kms is a really great service to utilize. So to really understand kms, we need to understand what a customer master key is, because this is the primary resource that kms is managing. And to start there. Let's talk about what encryption is. So encryption is the process of encoding a message or information in such a way that only authorized parties can access it and those who are not authorized cannot. Okay, pretty basic. And so that leads us to what are cryptographic keys or data keys. So a data key is just a string of data that is used to lock or unlock cryptographic functions. So a cryptographic function could be authentication, authorization, or encryption. And so that leads us on to what is a master key. So a master key is stored in a security hardware. So an HSM and master keys are used to encrypt all other keys on the system. Those other keys are being data keys. And so why would we want to use a key to encrypt another key, which is called envelope encryption? Well, the reason why and here's a, here's a diagram of an envelope encryption is how do you know that the keys, the data keys that you use that unlock the data to your database, are secure. And that's where these master keys come into play. So the idea is that they create security around those keys. So to learn a little bit more about customer master keys, a customer master key is the primary resource that AWS kms is managing. And a customer master key abbreviated as CMK is a logical representation of a master key. So you're not directly accessing the master key. It's, it's a logical representation. But with that logical representation, we get to attach a lot of metadata that's going to help us understand things about our master key. So the key ID when it was created the creation date, we can give it a description and say what state the key is in. And that CMK is going to also contain key material used to encrypt and decrypt data. And so kms supports both symmetric and asymmetric cm Ks. So if you've never heard of symmetric and asymmetric I'll give you a couple examples. So some symmetric symmetric key is generally a 256 bit key that is used for encryption and decryption. So you have a single key that you're using. And an example of this on AWS would be when you encrypt an s3 It uses something called a Aes 256 to be six suggests that is using 256 bit encryption as is the protocol for encryption. And so that is that method. And the other method is asymmetric key. And so this would be where you have an RSA key pair that is used for encryption and decryption or signing verification, but not both. And the idea here is that you have two keys. So a great example this is with EC two key pairs, you have a public key and a private key. Now kms isn't using when you're, when you're downloading EC two key pairs, I don't think that they're using kms, or at least that they are is probably managed by AWS, and it's transparent to you. But the idea of having these two methods of keys is based on the use case. But from a security perspective, if you have to have two keys one key to match to another, that is technically more secure. Whereas if you have one key, if that one key is lost, then you know, is is less secure. Okay, so that's customer master keys. Okay, let's do a quick review of some CLR commands we can use with kms. And these are actually very common. And if you're studying for the developer associate, you should absolutely commit these to memory, especially for your day to day kind of stuff. But on the exam, you might see these appear. And so it's just good to know them. So you can eliminate them from options that just do not exist. So the first one here is the Create key command. And as the name implies, it creates a customer manage key, very straightforward. Then you have your encrypt. So this is going to encrypt plain text into ciphertext, then you have decrypt. So that's going to decrypt ciphertext that was encrypted by kms. And then you have re encrypt. And so re encrypt can be used in three scenarios for manual rotations. The CMK is when you're changing the CMK that protects the ciphertext, or you're changing the encryption context of a ciphertext. So re encrypt. And the last one is enable key rotation. And the idea is that you know, once a year, if you want to rotate out those keys, you can turn this on, and it will just happen automatically. The only thing that you have to notice that this only works for symmetric customer master keys. So and you cannot perform this operation outside of CMK in a different in a different AWS account, so it's within the existing account. So yeah, there you go. So we're at the end of the kms section, so on to the kms. Key Management Service. kms creates and manages encryption keys for variety of database services or for your apps. kms can be used with cloud trail to audit a keys access history. kms has the ability to automatically rotate out your keys every year with no need to re encrypt. Customer master keys are the primary resources in kms. kms is a multi tenant HSM multi tenant means you're sharing the hardware with multiple customers. hardware security modules HSM is a specialized hardware for storing your keys and is tamper proof kms is up to FIPS 142 level two compliant if you're looking at what's another one called Cloud HSM, that's level three if you need level three kms stores master keys not data keys. Master keys master keys are used to encrypt data keys and which is called envelope encryption. kms supports two types of keys symmetric and asymmetric. So symmetric is a single key using 256 bit encryption. So I always like to say think of s3 buckets a as to be six. asymmetric uses two keys I always think of thinking about like key pair public and private, important kms kms API key API's to remember because you might see them as exam questions kms, create key kms encrypt, decrypt, re encrypt enable key rotation. So there we go. That is the end of kms cheat sheet. If this was the AWS database, security certification, this is like a seven page cheat sheet. So this is very light. But you definitely need to know kms it's very important as a developer or in sysop. Hey, this is Andrew Brown from exam Pro. And we are looking at Amazon cognito, which is a decentralized way of managing authentication. So think sign up sign in integration for your apps, social identity providers, like connecting with Facebook or Google. So Amazon cognito actually does multiple different things. And we are going to look at three things in specific. We're going to look at cognito user pools, which is a user directory to authenticate against identity providers. We're going to look at cognito identity pools, which provides temporary credentials for your users to access database services. And we're going to look at cognito sync which syncs users data and preferences across all devices. So let's get to it. So to fully understand Amazon cognito, we have to understand the concepts of web identity Federation and identity providers. So let's go through these definitions. So for web identity Federation, it's to exchange the identity and security information between an identity provider and an application. So now looking at identity provider, it's a trusted provider for your user identity that lets you authenticate to access other services. So an identity provider could be Facebook, Amazon, Google, Twitter, GitHub, LinkedIn, you commonly see this on websites where it allows you to log in with a Twitter or GitHub account, that is an identity provider. So that would be Twitter or GitHub. And they're generally powered by different protocols. So whenever you're doing this with social social accounts, it's going to be with OAuth. And so that can be powered by open ID Connect, that's pretty much the standard now, if there are other identity providers, so if you needed a single sign on solution, SAML is the most common one. Alright. So the first thing we're looking at is cognito. User pools, which is the most common use case for cognito. And that is just a directory of your users, which is decentralized here. And it's going to handle actions such as signup sign in account recovery. So that would be like resetting a password, account confirmation, that would be like confirming your email after sign up. And it has the ability to connect to identity providers. So it does have its own like email and password form that it can take. But it can also leverage. Maybe if you want to have Facebook Connect, or Amazon Connect, and etc, you can do that as well, the way it persists a connection after it's authenticated, that generates a j WT. So that's how you're going to persist that connection. So let's look at more of the options so that we can really bake in the utility here of user pools. So here left hand side, we have a bunch of different settings. And for attributes, we can determine what should be our primary attribute should be our username when they sign up, or should it be email and phone phone number. And if it is, you know, can they sign up or sign in if the email address hasn't been verified, where the conditions around that we can set the restrictions on the password the length, if it requires special characters, we can see what kind of attributes are required to collect on signup, if we need their birthday, or email or etc. It has the capabilities of turning on MFA. So if you want multi factor authentication, very easy way to integrate that if you want to have user campaigns, so if you're used to like sending out campaigns via MailChimp, you can easily integrate cognito with pinpoint which is a user campaigns, right. And you also can override a lot of functionality using lambda. So anytime like a sign up or sign in or recovery passwords triggered, there is a hook so that you can then trigger lambda to do something with that. So that's just some of the things that you do with cognito user pools. But the most important thing to remember is just it's a way of decentralizing authentication that's for for user pools. All right. So now it's time to look at cognito identity pools, identity pools provide temporary AIIMS credentials to access services such as Dynamo DB or s3, identity pools can be thought of as the actual mechanism authorizing access to the AWS resources. So you know, the idea is you have an identity pool, you're going to say, who's allowed to generate those AWS credentials, and then use the SDK to generate those credentials. And then that application can then access those database services. So just to really hit that home here, I do have screenshots to give you an idea what that is. So first, we're going to choose our providers, our provider can be authenticated. So we can choose cognito, or even a variety of other ones, or you can have an unauthenticated. So that is also an option for you. And then after you create that identity pool, they have an easy way for you to use the SDK. So you could just drop down your platform and you have the code and you're ready to go to go get those credentials. If you're thinking did I actually put in my real, real example or identity pool ID there? It's not, it's not I actually go in and replace all these. So if you're ever wondering and watching these videos, and you're seeing these things I always replay. We're going to just touch on one more, which is cognito. Sync. And so sync lets you sync user data and preferences across all devices with one line of code cognito uses push notifications to push those updates and synchronize data. And under the hood, it's using simple notification service to push this data to devices. And the the the data which is user data and preferences is key value data. It's actually stored with the identity pool. So that's what you're pushing back and forth. But the only thing you need to know is what it does. And what it does is it syncs user data and preferences across All devices will have one line of code. Hey, this is Angie brown from exam Pro, and we're going to do the cognito. Follow along and set up a login screen. So I'm gonna go to cognito cognito. There we go. And we're going to be presented two options. User pools, identity pools, identity pools is not what we want. We want user pools so that people can log in authenticate identity pools is when you want to give access to existing resources on AWS. So we'll go to user pools here, I'm going to create a new user pool, I'm going to call study saying, since that's kind of the project we've been working with, in our, in this developer associate, I'm going to go to review defaults. All these defaults are fine, you can see you can set MFA, and all this stuff, you can change any of this if you want on the create this pool, and the pool has been created, I'm going to need a app client, I'm going to hit Add app client, we're gonna name this study sync. We're going to leave all of this defaulted here, I'm going to go ahead and create this app client. Now the next thing to do is configure this app client. So the app client settings, we're going to enable user pools, I'm going to put an example URL here for our callback. This is what would happen after somebody logged in successfully. And this is where they would go if they logged out. We want authorized code grants and implicit grants, Mize will take all of these scopes, if possible, because why not. And we will go ahead and save these here. In order to use the host UI, this is like a UI that eight of us gives you by default, of course you can make your own, which is a lot of work. But to use this host of UI, which is what we'll be using here, we need to set a domain name, I'm gonna call it steady sync, we'll check for availability, it's available, you might have to change it based on your case, we'll hit save. And then to view our hosted domain or our hosted login screen, we're going to go back to App client settings. At the bottom here, hit launch UI here, I'm just going to do a hard refresh here. Because it's giving us some trouble. So let me just go back to the domain here. Yeah, it should be here, it looks like we also customize it, which is kind of nice. But this should work. Now, let me just see here, maybe just needed some time. There you go. So I was just way, way too fast. And then we'll go ahead, and we'll just sign up. And the goal here is that if we sign up successfully, we should be able to, um, we should be able to be directly redirected to the that that URL that we provided. So I'm gonna need a temporary email, you use 10 minute mail, minute, minute of mail here. We go here. And oh, well just changed the whole UI, I guess they have a new one. Oh, it's beta. Look at that I'm early user, if you never use this platform, it's for getting temporary emails very quickly. And we'll go back here, I will paste this in here. And then I'll make the password testing 123 exclamation mark sign up. And now we have a verification code. If I go back here, there it is. There's our code will copy it confirms it. And it redirected us. So that means that we successfully logged in, if we go over to our users, we should be able to see this user now if I refresh, or there, there I am, there's Andrew Brown. So you know, that's all there is to setting it up. Clearly, there's a lot more work involved. But for the developer associate, that's all you need to be comfortable with, I just want to make sure that you got some hands on with cognito. Generally, if you use a sample phi, it does a lot of the work for you for setting it up and integrating with your application. Integrating for cognito for your web apps without amplify is extremely painful. But it's worth it because it's so inexpensive. The only thing that is unfortunate about us are cognito user pools is there's a limitation in terms of identity providers. So we have Facebook, Google Amazon, but we don't have LinkedIn. And apparently you can do Twitter, I think with open ID Connect. But for me, like LinkedIn is a deal breaker. It's not a devices fault, either of us falls like some kind of security standard. And for whatever reason, LinkedIn does not conform to that. And so that's why it's not in the list here. There is a way to get LinkedIn to work, but it's a lot of effort. And so that's why we see a lot people using author zero because it just does everything but that one's really expensive. And there's a lot of things you can do in here. You know, there's a lot of options and I recommend that you poke around but we're done for this case and I want you to go ahead and delete this pool. Delete this pool, we have to first delete the domain name because we are borrowing that from AWS. And once that's deleted, we can go to general settings, hit delete, type in delete, and there we go. Our pool is gone. So that's it. So we're onto the Amazon cognito cheat sheet. So let's jump into it. So cognito is a decentralized managed authentication system. So when you need to easily add authentication to your mobile or desktop apps, think cognito. So let's talk about user pools. So user pool is the user directory allows users to authenticate using OAuth two ipds, such as Facebook, Google Amazon to connect to your web applications. And cognito user pool isn't in itself an IPD. All right, so it can be on that list as well. User pools use JW T's to persist authentication. Identity pools provide temporary database credentials to access services, such as s3 or dynamodb, cognito. Sync can sync user data preferences across devices with one line of code powered by SNS web identity Federation, they're not going to ask you these questions. But you need to know what these are exchange identity and security information between identity provider and an application. identity provider is a trusted provider for your user to authenticate or sorry to identify that user. So you can use them to dedicate to access other services. Then you have Oh, IDC is a type of identity provider which uses OAuth and you have SAML, which is a type of identity provider which is used for single sign on so there you go. We're done with cognito. Hey, this is Andrew Brown from exam Pro. And we are looking at simple notification service also known as SNS, which lets you subscribe and send notifications via text message email, web hooks, lambdas Sq s and mobile notification. Alright, so to fully understand SNS, we need to understand the concept of pub sub. And so pub sub is a publish subscribe pattern commonly implemented in messaging systems. So in a pub sub system, the sender of messages, also known as the publisher here, doesn't send the message directly to the receiver. Instead, they're gonna send the messages to an Event Bus. And the event bumps categorizes the messages into groups. And then the receiver of messages known as the subscriber here subscribes to these groups. And so whenever a new message appears within their subscription, the messages are neatly delivered to them. So it's not unlike registering for a magazine. All right. So, you know, down below, we have that kind of representation. So we have those publishers, and they're publishing to the Event Bus which have groups in them, and then that's going to send it off to those subscribers, okay, so it's pushing it all along the way here, okay, so publishers have no knowledge of who their subscribers are. Subscribers Do not pull for messages, they're gonna get pushed to them. messages are instead automatically immediately pushed to subscribers and messages and events are interchangeable terms in pub sub. So if you see me saying messages and events, it's the same darn thing. So we're now looking at SNS here. So SNS is a highly available, durable, secure, fully managed pub sub messaging service that enables you to decouple microservices distributed systems and serverless applications. So whenever we talking about decoupling, we're talking about application integration, which is like a family of Ada services that connect one service to another. Another service is also Sq s. And SNS is also application integration. So down below, we can see our pub sub systems. So we have our publishers on the left side and our subscribers on the right side. And our event bus is SNS Okay, so for the publisher, we have a few options here. It's basically anything that can programmatically use the EVAs API. So the SDK and COI uses the Avis API underneath. And so that's going to be the way publishers are going to publish their messages or events onto an SNS topic. There's also other services on AWS that can trigger or publish to SNS topics cloud watch, definitely can, because you'd be using those for billing alarms. And then on the right hand side, you have your subscribers and we have a bunch of different outputs, which we're going to go through, but here you can see we have lambda Sq s email, and HTTPS protocol. So publishers push events to an SNS topic. So that's how they get into the topic. And then subscribers subscribe to the SNS topic to have events pushed to them. Okay. And then down below, you can see I have a very boring description of SNS topic, which is it's a logical access point and communication channel. So that makes a nap. That makes sense. So let's move on. So we're gonna take a deeper look here at SNS, topics and topics allow you to group multiple subscriptions together, a topic is able to deliver to multiple protocols at once. So it could be sending out email, text message HTTPS, all the sorts of protocols we saw earlier. I'm publishers don't care about the subscribers protocol, okay? Because it's sending a message event, it's giving you the topic and saying, you figure it out. This is the message I want to send out, and it knows what subscribers it has. And so the topic when it delivers that messages, it will automatically format it for us. The message according to the subscribers chosen protocol, okay, and the last thing I want you to know is that you can encrypt your topics via kms key management service. And you know, so it's just as easy as turning it on and picking your key. So now we're taking a look at subscriptions. And subscriptions are something you create on a topic, okay. And so here I have a subscription, that is an email subscription. And the endpoint is obviously going to be an email. So I provided my email there. If you want to say hello, give, send me an email. And it's just as simple as clicking that button and filling in those options. Now you have to choose your protocol. And here we have our full list here on the right hand side. So we'll just go through it. So we have HTTP, HTTPS, and you're going to want to be using this for web hooks. So the idea is that this is usually going to be an API endpoint to your web applications that's going to listen for incoming messages from SNS, then you can send out emails now, there's another service called ACS, which specializes in sending out emails. And so SNS is really good for internal email notifications, because you don't necessarily have your custom domain name. And also, the emails have to be plain text only. And there's some other limitations around that. So they're really, really good for internal notifications, maybe like billing alarms, or maybe someone signed up on your platform you want to know about it, then they also have it in email, JSON. So let's just gonna send you JSON via email, then you have Sq s, so you can send an SMS message to Sq s. So that's an option you have there, you can also have SNS trigger lambda functions. So that's a very useful feature as well. And you can also send text messages that will be using the SNS protocol. And the last one here is platform application endpoints. And that's for mobile push. So like a bunch of different devices, laptops, and phones have notification systems in them. And so this will integrate with that. And we're just gonna actually talk about that a bit more here. So I wanted to talk a bit more about this platform application endpoint. And this is for doing mobile push. Okay, so we have a bunch of different mobile devices, and even laptops that use notification systems in them. And so here you can see a big list. We have a DM, which is Amazon device messaging, we have Apple, the do Firebase, which is Google and then we have two for Microsoft. So we have Microsoft push and Windows push, okay. And so you can with this protocol, push out to that stuff. And the advantage here you're gonna when you push notification messages to these mobile endpoints, it can appear in the mobile app just like message alerts, badges, updates, or even sound alert. So that's pretty cool. Okay, so I just want you to be aware. Alright, so on to the SMS cheat sheet. So simple notification service, also known as SNS, is a fully managed pub sub messaging service. SNS is for application integration. It allows decoupled services and apps to communicate with each other. We have a topic which is a logical access point and communication channel, a topic is able to deliver to multiple protocols. You can encrypt topics via kms. And then you have your publishers, and they use the EVAs API via the CLR or the SDK to push messages to a topic. Many Ava's services integrate with SNS and act as publishers Okay, so think cloud watch and other things. Then you have subscriptions, so you can subscribe, which consists subscribe to topics. When a topic receives a message, it automatically immediately pushes messages to subscribers. All messages published to SNS are stored redundantly across multi az, which isn't something we talked in the core content, but it's good to know. And then we have the following protocols we can use so we have HTTP HTTPS. This is great for web hooks into your web application. We have emails good for internal email notification. Remember, it's only plain text if you need rich text. And custom domains are going to be using sts for that. Then you have email JSON, very similar to email just sending Jason along the way. You can also send your your SMS messages into an ESA s Sq sq. You can trigger lambdas you can send text messages. And then the last one is you have platform application endpoints, which is mobile push okay and that's going to be for systems like Apple, Google Microsoft Purdue alright. Hey, this is Andrew Brown from exam pro and we are looking at simple queue service also known as Sq s which is a fully managed queuing service that enables you to decouple and scale micro services distributed systems and serverless applications. So, to fully understand Sq s, we need to understand what a queueing system is. And so a queueing system is just a type of messaging system, which provides asynchronous communication and decouples. Processes via messages could also be known as events from a sender and receiver but in the Case for a streaming system also known as a producer and consumer. So, looking at a queueing system, when you have messages coming in, they're usually being deleted on the way out. So as soon as they're consumed or deleted, it's for simple communication. It's not really for real time. And just to interact with the queue and the messages there, both the sender and receiver have to pull to see what to do. So it's not reactive. Okay, we got some examples of queueing systems below, we have sidekick, sq, S, rabbit, rabbit and Q, which is debatable because it could be considered a streaming service. And so now let's look at the streaming side to see how it compares against a queueing system. So a streaming system can react to events from multiple consumers. So like, if you have multiple people that want to do something with that event, they can all do something with it, because it doesn't get immediately deleted, it lives in the Event Stream for a long period of time. And the advantage of having a message hang around in that Event Stream allows you to apply complex operations. So that's the huge difference is that one is reactive and one is not one allows you to do multiple things with the messages and retains it in the queue. One deletes it and doesn't really doesn't really think too hard about what it's doing. Okay, so there's your comparative between queuing and streaming. And we're going to continue on with Sq s here, which is a queueing system. So the number one thing I want you to think of when you're thinking of Sq S is application integration. It's for connecting isolette applications together, acting as a bridge of communication and Sq s happens to use messages and queues for that you can see Sq s appears in the Ava's console under application integration. So these are all services that do application integration Sq S is one of them. And as we said it uses a queue. So a queue is a temporary repository for messages that are waiting to be processed, right. So just think of going to the bank and everyone is waiting that line, that is the queue. And the way you interact with that queue is through the Avis SDK. So you have to write code that was going to publish messages to the queue. And then when you want to read them, you're going to have to use the database SDK to pull messages. And so SQL is pull based, you have to pull things. It's not pushed based, okay. So to make this crystal clear, I have an SQL use case here. And so we have a mobile app, and we have a web app, and they want to talk to each other. And so using the Avis SDK, the mobile app sends a message to the queue. And now the the web app, what it has to do is it has to use the Avis SDK, and pull the queue whenever it wants. So it's up to the this app to code in how frequently we'll check, it's gonna see if there's anything in the queue. And if there is a message, it's going to pull it down, do something with it and report back to the queue that it's consumed it meaning to tell the queue to go ahead and delete that message from the queue. All right, now this app on the mobile left hand side to know whether it's been consumed, it's going to have to, on its own schedule, periodically check to pull to see if that message is still in the queue, if it no longer is, that's how it knows. So that is the process of using Sq s between two applications. So let's look at some SQL limits starting with message size. So the message size can be between one byte to 256 kilobytes. If you want to go beyond that message size, you can use the Amazon SQL extended client library only for Java, it's not for anything else to extend that necessarily up to two gigabytes in size. And so the way that would work is that the message would be stored in s3 and the library would reference that s3 object, right? So you're not actually pushing two gigabytes to Sq s, it's just loosely linking to something in an s3 bucket. Message retention. So message retention is how long SQL will hold on a message before dropping it from the queue. And so the message retention by default is four days, and you have a message retention retention that can be adjusted from a minimum of 60 seconds to a maximum of 14 days. SQL is a queueing system. So let's talk about the two different types of queues. We have standard queue which allows for a nearly unlimited number of transactions per second when your transaction is just like messages. And it guarantees that a message will be delivered at least once. However, the trade off here is that more than one copy of a message could be potentially delivered. And that would cause things to happen out of order. So if ordering really matters to you. Just consider there's that caveat here with standard queues, however, you do get nearly unlimited transactions. So that's a trade off. It does try to provide its best effort to ensure messages stay generally in the order that they were delivered. But again, there's no guarantee. Now it If you need a guarantee of the the ordering of messages, that's where we're going to use feefo, also known as first in first out, well, that's what it stands for, right. And the idea here is that, you know, a message comes into the queue and leaves the queue. The trade off here is the number of transactions you can do per second. So we don't have nearly unlimited per second where we have a cap up to 300. So there you go. So how do we prevent another app from reading a message while another one is busy with that message. And the idea behind this is we want to avoid someone doing the same amount of work that's already being done by somebody else. And that's where visibility timeout comes into play. So visibility timeout is the period of time that meant that messages are invisible VSU Sq sq. So when a reader picks up that message, we set a visibility timeout, which can be between zero to 12 hours. By default, it's 30 seconds, and so no one else can touch that message. And so what's going to happen is that whoever picked up that message, they're going to work on it. And they're going to report back to the queue that, you know, we finished working with it, it's going to get deleted from the queue. Okay, but what happens, if they don't complete it within the within the visibility timeout frame, what's going to happen is that message is now going to become visible, and anyone can pick up that job, okay. And so there is one consideration you have to think of, and that's when you build out your web apps, that you you bake in the time, so that if if the job is going to be like if it's if 30 seconds have expired, then you should probably kill that job, because otherwise you might end up this issue where you have the same messaging being delivered twice. And that could be an issue. Okay, so just to consideration for visibility. So in ask us, we have two different ways of doing polling, we have short versus long. Polling is the method in which we retrieve messages from the queue. And by default, sq s uses short polling, and short polling returns messages immediately, even if the message queue being pulled is empty. So short polling can be a bit wasteful, because if there's nothing to pull, then you're just calling you're just making calls for no particular reason. But there could be a use case where you need a message right away. So short polling is the use case you want. But the majority of use cases, the majority of use cases, you should be using long polling, which is bizarre, that's not by default, but that's what it is. So long polling waits until a message arrives in the queue, or the long pole timeout expires. Okay. And long polling makes it inexpensive to retrieve messages from the queue as soon as messages are available, using long polling will reduce the cost because you can reduce the number of empty receives, right. So if there's nothing to pull, then you're wasting your time, right. If you want to enable long polling it, you have to do it within the SDK. And so what you're doing is you're setting the receive message requests with a wait time. So by doing that, that's how you set long polling. Let's take a look at our simple queue service cheat sheet that's going to help you pass your exam. So first, we have Sq S is a queuing service using messages with a queue so think sidekick or rabbit mq, if that helps if you know the services, sq S is used for application integration. It lets you decouple services and apps so that they can talk to each other. Okay, to read Sq s, you need to pull the queue using the ABS SDK Sq S is not push based. Okay, it's not reactive. SQL supports both standard and first in first out FIFO queues. Standard queues allow for unlimited messages per second does not guarantee the order of delivery always delivers at least once and you must protect against duplicate messages being processed FIFO first in first out maintains the order messages with a limit of 300. So that's the trade off there. There are two kinds of polling short by default and long. Short polling returns messages immediately even if the message queue is being pulled is empty. Long polling waits until messages arrive in the queue or the long pole time expires. in the majority of cases long polling is preferred over short polling majority okay. Visibility timeout is the period of the time that messages are invisible to the Sq sq. messages will be deleted from the queue after a job has been processed. Before the visibility timeout expires. If the visibility timeout expires in a job will become visible to the queue again, the default visibility timeout is 30 seconds. Timeout can be between zero seconds to a maximum of 12 hours. I highlighted that zero seconds because that is a trick question. Sometimes on the exams. People don't realize you can do it for zero seconds. sq s can retain messages from 60 seconds to 14 days by default. It is four days so 14 days is two weeks, that's an easy way to remember it. Message sizes can be between one byte to two and 56 kilobytes. And using the extended client library for Java can be extended to two gigabytes. So there you go, we're done with SQL. Hey, this is Andrew Brown from exam Pro. And we are looking at Amazon kinesis, which is a scalable and durable real time data streaming service to ingest and analyze data in real time from multiple sources. So again, Amazon kinesis is AWS is fully managed solution for collecting, processing and analyzing street streaming data in the cloud. So when you need real time, think kinesis. So some examples where kinesis would be of use stock prices, game data, social media data, geospatial data, clickstream data, and kinesis has four types of streams, we have kinesis data streams, kinesis, firehose delivery streams, kinesis, data analytics, and kinesis video analytics, and we're gonna go through all four of them. So we're gonna first take a look at kinesis data streams. And the way it works is you have producers on the left hand side, which are going to produce data, which is going to send it to the kinesis data stream, and that data stream is going to then ingest that data, and it has shards, so it's going to take that data and distribute it amongst its shards. And then it has consumers. And so consumers with data streams, you have to manually configure those yourself using some code. But the idea is you have these YouTube instances that are specialized to then consume that data and then send it to something in particular. So we have a consumer that is specialized to sending data to redshift than dynamodb than s3, and then EMR, okay, so whatever you want the consumer to send it to, it can send it wherever it wants. But the great thing about data streams is that when data enters into the stream, it persists for quite a while. So it will be there for 24 hours, by default, you could extend it up to 160 68 hours. So if you need to do more with that data, and you want to run it through multiple consumers, or you want to do something else with it, you can definitely do that with it. The way you pay for kinesis data streams, it's like spinning up an EC two instance, except you're spinning up shards, okay. And that's what's going to be the cost there. So as long as the chart is running, you pay X amount of costs for X amount of shards. And that is kinesis data stream. So on to kinesis firehose delivery stream, similar to data streams, but it's a lot simpler. So the way it works is that it also has producers and those producers send data into kinesis firehose. The difference here is that as soon as data is ingested, so like a consumer consumes that data, it immediately disappears from the queue. Okay, so data is not being persisted. The other trade off here is that you can only choose one consumer. So you have a few options, you can choose s3, redshift, Elasticsearch, or Splunk. Generally, people are going to be outputting to s3. So there's a lot more simplicity here. But there's also limitations around it. The nice thing though, is you don't have to write any code to consume data. But that's the trade off is you don't have any flexibility on how you want to consume the data, it's very limited. firehose can do some manipulations to the data that is flowing through it, I can transform the data. So if you have something where you want it from JSON, you want to convert it to parkette. There are limited options for this. But the idea is that you can put it into the right data format, so that if it gets inserted into s3, so maybe Athena would be consuming that, that it's now in parkette file, which is optimized for Athena, it can also compress the file. So just simply zip them, right. There's different compression methods, and it can also secure them. So there's that advantage. Um, the big advantage is firehose is very inexpensive because you only pay for what you consume. So only data that's ingested is what you what you pay for, you can think of it like, I don't know, even lambda or fargate. So the idea is you're not paying for those running shards, okay? It's just simpler to use. And so if you don't need data retention, it's a very good choice. Okay, on to kinesis video streams, and as the name implies, it is for ingesting video data. So you have producers, and that's going to be sending either video or audio encoded data. And that could be from security cameras, web cameras, or maybe even a mobile phone. And that data is going to go into kinesis video streams, it's going to secure and retain that encoded data so that you can consume it from services that are used for analyzing video and audio data. So you got Sage maker recognition, or maybe you need to use TensorFlow or you have a custom video processing or you have something that has like HL based video playback. So that's all there is to it. It's just so You can analyze and process a video streams, applying like ml or video processing services. Now we're gonna take a look at kinesis data analytics. And the way it works is that it takes an input stream, and then it has an output stream. And these can either be firehose or data streams. And the idea is you're going to be passing information data analytics. And what this service lets you do is it lets you run custom SQL queries so that you can analyze your data in real time. So if you have to do real time reporting, this is the service you're going to want to use. The only downside is that you have to use two streams. So it can get a little bit expensive. But for data analytics, it's it's really great. So that's all there is to it. It's time to look at kinesis cheat sheet. So Amazon kinesis is the ADA solution for collecting, processing and analyzing streaming data in the cloud. When you need real time, think kinesis. There are four types of streams of the first being kinesis data streams, and that's a you're paying per shard that's running. So think of an easy to instance, you're always paying for the time it's running. So kinesis data streams is just like that data can persist within that stream data is ordered, and every consumer keeps its own position, consumers have to be manually added to that to be coded to consume, which gives you a lot of custom flexibility. Data persists for 24 hours by default, up to 168 hours. Now looking at kinesis firehose, you only pay for the data that is ingested, okay, so think of like, I don't know, lambdas, or fargate. The idea is that you're not paying for a server that's running all the time, just data that's ingested, data immediately disappears. Once it's processed consumer, you only have the choice from a predefined set of services, so either get s3, redshift, Elasticsearch, or Splunk. And they're not custom. So you're stuck with what you got kinesis data analytics allows you to perform queries in real time. So it needs kinesis data streams or farfalle firehose as the input and the output, so you have to have two additional streams to use a service which makes it a little bit of expensive. And you have kinesis video analytics, which is for securely, ingesting and storing video and audio uncoated data to consumers such as Sage maker, recognition or other services to apply machine learning and video processing. to actually send data to the streams, you have to either use kpl, which is the kinesis Producer library, which is like a Java library to write to a stream. Or you can write data to a stream using the ABS SDK kpl is more efficient, but you have to choose what you need to do in your situation. So there is the kinesis cheat sheet. Hey, this is Andrew Brown from exam Pro. And we are looking at SYSTEMS MANAGER parameter store, which is secure hierarchal storage for configuration, data management and secrets management. So with parameter store, you can store data such as passwords, data, base strings, license code, and as parameter values. And store configuration data and secure strings in hierarchies. And track versions of that doesn't make sense it will as we work through this here. And you can encrypt these parameters using kms. So you can optionally apply encryption, though that doesn't necessarily mean things are encrypted in parameter store. So to really understand parameter store, let's go ahead and look at what it takes to create a parameter. So the first thing is the name. And this is the way you group parameters together based on a naming convention. So by using the forward slashes, you're creating hierarchies. And this allows you to fetch parameters at different levels. So if I created a bunch of parameters under the Ford slash, prod, then when I use the API to do example, application support slash product, give me all those parameters. So, so interesting way to organize your parameters into groups. And then you get to choose your tier. And we'll talk about that a little bit more shortly here. And then you choose the type. So you could have a string, that's just a string, you could have a string that is a string list, which is a comma separated string, you can encrypt your string using kms. And then you just provide the value. So talking about those tears. There are two tiers we have standard and advanced. So generally, you're using the standard one, and this is scope based on regions. So if you never exceed 10,000 parameters, this parameter store is going to be free. But once you over go over 10,000, you're now using advanced parameters. Or if you need to use a parameter that has a value higher than four kilobytes, you're gonna have to use advanced parameter. And if you want to apply parameter policies, you're going to have to use an advanced parameter. Now a parameter can be applied per or sorry, the advanced here can be applied per parameter. So you can mix and match these two here. So that is something that's interesting No, but one thing you do need to know about these advanced parameters is that you can convert a standard parameter to an advanced parameter at any time, but you can't revert an advanced parameter to a standard parameter. So it's a one way process. And the reason for that is because if you were to revert an advanced parameter, you would end up losing data because you have an advanced parameter that has eight kilobytes, and it just can't go back to four kilobytes, it would end up truncating the data. So that is the reason for that. Let's talk about parameter policies, which is a feature of only advanced parameters. So the idea here is it's going to help force you to update or delete your passwords, it's going to do this by using asynchronous periodic scans. So after you create these policies, you don't have to do anything parameter store will take care of the rest. And you can apply multiple policies to a single parameter. So now that we have a bit of that, let's look at what policies we have available to us, there's only three at this moment. So the first one is expiration. So the idea with this policy is you say I want this, this parameter to expire after this date and time and then it will just auto delete. The next one is expiration notice, and this one will tell you x days before or hours or minutes before an expiration is going to happen. So if for whatever reason you need to take action. With that stored data, this is gonna give you a heads up. The last one is no change notification. So let's say you're supposed to have a parameter that's supposed to be modified manually by a developer. So they're supposed to update it themselves. After X amount of days or minutes or hours, this parameter will tell you, hey, nothing has changed. So maybe we should go ahead and investigate. So that is parameter policies. So to understand how the hierarchy works with parameter store, I want to show you using the COI. So the first thing we want to do is we want to create some parameters. So using the put parameter command, we can just supply the name. And that's going to be how we define our hierarchy here, I'm going to provide some values, we're going to store them as strings. And so when you run each of these commands, it's actually going to tell you what version, it is, if you keep on doing a put on the same thing, that version count is going to go up. And this allows you to access older versions, because everything in parameter store is automatically versioned. So we saved we put three parameters here on Vulcan, so how would we actually get all the parameters in one go. And that's where we use get parameters by path. so here we can just specify planets, Ford slash Vulcan, and all the parameters underneath will be returned to us. So here they are. So you can see we have all of them there. So that's all it takes to get your parameters. And that's how you generally use this within your application. So we are using the CLR here, so you'd have to translate this over to the SDK. But this is how you would get parameters in your application. Hey, this is Andrew Brown from exam Pro. And we are looking at secrets manager, which is used to protect secrets needed to access your applications and services. So easily rotate managed and retrieved database credentials, API keys and other secrets through their lifecycle. So for secrets manager, you're going to generally want to use it to automatically store and rotate database credentials. They say they do API keys and other things like that. But really, this is where secrets manager shines. So you know, the database is available to us as RDS or redshift document dB. Then we have other databases, which we'll look at closely here in a second. And then you have key value, which they say is for API's. So what you do is you go ahead and select the secret type that you want to do for RDS, redshift and document dB. It's very straightforward for other database and other types of secrets that are a little bit different. So let's look at those in greater detail here. So selecting for credentials for other database, you can see you can select a specific type of relational engine. But here you're providing the server address, the database name and the port. So for the other three managed ones, you wouldn't do that you just provide the username, password and select the resource within your AWS account. And then for the other types of secrets, this is just a key value if you go over to plain text. That's not that doesn't mean you can encrypt a plain text file, it just is another representation of that key and value. So you can just work with a JSON object. But yeah, those are all the types there. Just a few other things to highlight about secrets manager and that is when you go ahead and create any credential in encryption is enforced. So with parameter store, it doesn't necessarily have to be encrypted but with secrets manager, it has be encrypted at rest. And you can just use the default encryption key or use a different CMK. If you want to go make one, the pricing is pretty simple. So it's point 40 cents USD per secret per month. So some people don't like secrets manager because it costs that and you can pretty much use parameter store for free. But, you know, you have to decide what makes sense for you. And it's a half a cent per 10,000 API calls there. And then one thing to note is that if you want to monitor the credentials access, in case you need to audit them, or just investigate, if you have a cloud trail created, then it will monitor the secrets for you. So you can investigate in the future, probably a good idea to turn on cloud trail. So the huge value with secrets manager is automatic rotation. So you can set up automatic rotation for any database credential. So any of the managed services and even the other databases there. There's no automatic rotation for the key value, secret type. So it's just the database stuff. So you know, once you go through the wizard, for any of those steps are going to come to this automatic rotation. So we're going to go ahead and enable it, you're going to choose your rotation interval, which can be up to 365 days, up to one year. So just to show you here, if you expand your 360 90. And you have custom, if you go to custom, you can set it up to one year. And the way secrets manager works is that it just creates a lambda function for you. And so some would argue that if you wanted to use parameter store, just make your own lambda, you wouldn't need to use secrets manager and that is 100%. True. But you know, you have to know how to do that. So you know, decide whether you want to put an extra effort, so you don't have to pay for secrets manager. But yeah, it will go ahead and do that. And there's one option down below here. And this allows you to select what password you're going to rotate out, because you might not want to rotate out this path for you might want to rotate out a developer's password that's connected to the database. So you know, that's another option for you there. For the developer associate, it's good to know the COI commands, let's just look at a couple for secrets manager, the first one being intimate secrets manager describes secret. And this is going to describe information about a particular secret. The reason you'd want to do this is so that you could find out what version IDs are there. Because you might want to specify a specific version of a secret. And then you get some access information such as the last time it was changed or accessed. So that might be one precursor step before you actually try to get the secret. So the other ci command we need to know is get secret value. And this actually gets the secret. So you can see you supply the secret ID and then the version stage. And if you don't provide virgin sage, it would just default to innovyz current, which is the current version. But if you use the prior step there, you could use a different version stage there. And so using the COI command, you can see that we have this secret string string. And that is that is what storing our credential information. So in this case, it's just a key value. credential are secret. And so that's what we're looking at here. to really help make sense of secrets manager, because it's not always clear, you know, how do you access the secrets versus how does the database access it versus how does the app access it? You know, I made an architectural diagram here for you. So you know, the first thing is that you do secrets manager, and we'd set up rotation with it. So every 90 days, it's going to rotate out that password that is stored in RDS. So the password is actually an RDS. And then secrets manager can store a version of it as well. But what it's doing is the lambda probably set up with a cloud watch event. And every nine days it's going to say, okay, run this lambda, swap out that password. So that's how the password gets swapped in RDS. But how does the application access it? Well, you'd have your web app running on EC two, and you'd use the SP DK, which is the icon there in the actual EC two instance here. And you would make an SDK call to secrets manager and get the database credentials. And then you use your web server with that new password and username and form a connection URL, which is just a it looks like an HTML URL. But it's used to connect to Postgres databases. Not all relational database types have a connection URL. So you might just have to provide the username and password. But the point is, you are making connection to RDS. Now from the developer side, the way they would probably use it, because they're in development. They just want to gain access to the, the database that's in the cloud. They're going to use the COI to obtain the database credentials and we just went through the COI commands so you have an idea They would run, and then the way they connect to the database would be using a database manager. So you can use table. Plus, if you didn't have a database manager, you could just go ahead and use terminal and, and, and connect that way there. So hopefully that makes it very clear in terms of how you can use secrets manager for all these use cases. Hey, this is Angie brown from exam Pro. And we are looking at dynamodb, which is a key value and document no SQL database, which can guarantee consistent reads and writes at any scale. So to really understand dynamodb, we need to understand what is no SQL and I can tell you what it's not It is neither a relational database and does not use SQL to query the data for results. And the key thing that's different is how the data is stored. So it can be either key value or documents. So looking first here, a key value store, this is a form of data storage, which has a key which references a value and nothing more. Okay, so that is one data structure you can have in Dynamo dB. The other one is documents stored. So this is a form of data storage, which has a nested data structure. There. So that is what they call the document. So hopefully that makes a bit of sense. So dynamodb is a no SQL key value in document database for internet scale applications. And it has a lot of great features. It's fully managed multi region, multi master durable database built in security, backup and restore an in memory caching. So you can see why this is AWS is flagship database that they're always promoting, because it has so much functionality. at scale, it can provide eventual consistent reads and strongly consistent reads, which we will talk about in the Dynamo DB section here. So don't worry if that doesn't make sense to just as of yet. And you can specify the Read and Write capacity per second. So whatever you need, you just say I need 100 reads and writes per second. And then you just pay that cost. So there they are. And we will again, we'll talk about this in greater detail in the dynamodb section. And the last thing I want you to know is that the data is stored at least across three different regions on SSD storage. So there's a really fast drives, which makes your data extremely durable to failure. So there you go. Okay, so I made one really, really big mistake here, which I've corrected here. And it's the fact that all data is stored on SSD storage and is spread across three different easy's The reason I thought it was regions was because when I read the documentation, it said geographical locations, it didn't really say azs. And so I took the guess and thought it was region. But when I thought about it, and I you know, I talked to some other people that know dynamodb better than me. They pointed out that no, it's going to be data centers. And that makes sense. Because if you have a feature called Global tables and allows you to copy to other regions, it just doesn't make sense. So sorry for that mistake. But it's actually three different Z's. It's technically three different data centers, but we'll just call these Z's for the sake of simplicity. And yeah, that's that correction there. Let's take a look at what a dynamodb table looks like and understand all the components that are involved. So the first thing is the table itself. And tables contain rows and columns, but we don't call them rows and columns dynamodb, they have different names. So that is the entire table there. So we have items, and that is the name for rows. So there we have a row, which is a single item. And then you have attributes, and that is the name for columns. I don't point it out there. But imagine the entire year column as the attributes, then you have the keys. And that's the name of these columns. So up here we have IMDb IDs, that's naming the key. And then you have values and that is the actual data itself. So there's an example of some data. So hopefully that makes it very clear how what the structure is for dynamodb. So dynamodb, it replicates your database across three regions onto three separate hard drives. And this allows for high availability, so you don't have data loss. But this comes comes with a trade off because when you need to update your data, it's going to have to write updates to all those copies. And it's possible for data to be inconsistent when reading from a copy which has yet to be updated. So the way you work around this is with choosing your read consistency with dynamodb. You can choose between two options. You can choose eventual consistent reads, which is the default or strongly consistent reads. So let's talk about eventual consistent reads first. So when copies are being updated, it is possible for you to read and be returned an inconsistent copy. Okay, because because you're reading from a database which has yet to be updated, but reads are super fast because you're not waiting for data to become consistent. So you can read it immediately. But there's no guarantee of consistency. Now, the time it takes for everything to become consistent is, is around a second. So if you're building application, and you can wait up to a second after read, that's how you would ensure that your data is up to date. Or maybe you have an application where it'd be inconsistent isn't a deal breaker, so it doesn't really matter. Now, if consistency is extremely important to you, this is where strongly consistent reads are going to come into play. So this is when copies are being updated, and you attempt to read it, but it's not going to return it unless all the copies are consistent. So the trade off here is you're going to have a guarantee of consistency, it's always going to be consistent. But you're going to have to wait longer for that, that read to come back to you. So it's gonna be a slower read. And all copies will be consistent within a second. That's a guarantee that it was gives you. So there you go, that's reconsidered. Let's look at dynamodb partitions. And so what is that partition? It's a allocation of storage for a table backed by a solid state drive and automatically replicated across azs within an AWS region, and that is a device's definition. So let's take a look at my definition, because I think that's a bit easier to digest. And that is a partition. partition is when you slice your table up into smaller chunks of data or partition. And the purpose of this is to speed up reads for very large tables by logically grouping similar data together. So imagine you have a table and this table is giant. And so it would be faster if you could partition it. And so the idea is that maybe this data would go to partition A, this data would go to partition B and this data, we go to partition C. And if you're wondering, well, how does it choose which tables and stuff like that, that's what we're going to find out next. So looking at partitions a bit more here, dynamodb automatically creates those partitions for you as your data grows. And you're going to start off with a single partition. And there's two cases where dynamodb will create additional partitions for you, that's when you exceed 10 gigabytes of data in a table, or if you exceed the RC use or WC use for a single partition. And, you know, each partition has a maximum of 3000, RC use or 1000. WCS, if you're wondering what those acronyms mean, it means read capacity units, and write capacity units. So an example of setting up those capacity units would be down here below. So you have a table. And here I'm saying the reads and then I'm setting the rights. And the way it will work is when you split, when you get a new partition, it's going to split the reads and writes across those tables. So if you go over that mark, this is exactly where it would hit that threshold. So it would split. So when you create a table, you have to define a primary key. And this key is going to determine where and how your data will be stored in partitions. And it's important to note that the primary key cannot be changed later. So designing your primary key, making the right choice early on is extremely important because you only get one shot at this. So here is an example of the console where you create your primary key. And you'd would define a partition key that's going to determine which partition of data should be written to, then you have a sore key, which is optional. This is how your data should be sorted on a partition, you're going to notice that there are some date types here. And I have a third key that's a date. But notice that there isn't a date type in dynamodb. So in that case, you use a string. So just be aware that there is no data type. And there are two types of primary keys. There's one which is called a simple primary key, that's where you only use a partition. And then you have a composite primary key. That's where you have a partition and a sort key that makes up your primary key. So let's talk about how a primary key with only a partition key chooses which partition it should write data to and again, if you only have a partition key, that makes it a simple primary key. So the first thing is we need some data, and we want to get it into a particular partition. And so we make our our primary key and all it has is a partition defined, and we want to choose a value that is unique. So IDs are really great value because that's going to be extremely unique. And that is very important when designing simple primary keys. So the way this is going to work is AWS has this thing called the Dynamo dB. Internal hash function. And we don't actually know how this thing works, because it's a secret. But it's an algorithm that decides which partition to write data to. And so it needs your data and it needs your primary key to figure that out. So those two things go in there. And then it writes to whatever partition it decides to write to. So you know, that is how that works for a simple primary key. Now, we're going to take a look at how a primary key with a partition and a sarki chooses which partition it should write data to. And again, a partition and sort key is known as a composite primary key. So we're going to need some data. And then we're gonna have to define our key here. And so you'll notice that we're filling in both the partition key and also the sort key value. And what's important is that the combination of the partition and sort key has to be unique. So in the case of simple primary key, we wanted the partition key to be unique, but it has to be unique in the scope of the two combined. And then we have our internal hash function, which again, is a secret, nobody knows how it works. I've reached out on Twitter asked dynamodb, they won't tell me, which is great for security. And so what we're going to do is take our primary key and our data and it's going to get passed to that internal hash function. This happens when you just write to dynamodb. You don't have to literally call it and then it's going to decide to put it in partition See, but this is a little bit different, where the composite or the simple primary key, I just put it in a random partition. This one is putting it with data that is similar to it. So here we have an alien that is Romulan, and it's grouping it together with other Romulans. There, and it's also sorting that data from A to Z. So the data is close together so that it's faster to access it. So that's that is the idea behind the composite primary key. And you know how it figures out what partitions you go. Let's talk about primary key designs, we're going to be talking about simple keys and the composite key. Again, simple keys is only a partition key. And a composite key is made up of both a partition and sort key. So for simple, what's important to remember is that no two items can have the same partition key. So here, let's say the ID is the partition key, you can see that two values are the same, that's not going to work out. But here they are different. So that is great. Then on the composite key side, we care about two items, where they can have the same partition key, but the partition and sort key combined must be unique. So here, let's say alien is the partition and name is the sort, they're both the same, that's not going to work out. But here, you know, the partition key is the same, but the sore key is different. And that's alright, so that's going to work out great. So there's two things when designing your primary key that you have to keep in mind you want them to be distinct, the key should be is as unique as possible. And the other thing is you want the key so that it will evenly distribute data amongst partitions. And that is based on how much things will group together based on your so and this is more so even with the the composite key because when you can group things based on that partition key, you want that to be as unique as possible. So that it's more even, you don't want 90% of your records being one thing and then 10% being the other, you want it as even as possible. So there you go. query and scan are two functions that are going to be using a lot in Dynamo dB. And you really need to get familiar with these two functions. And so what I recommend is opening up the dynamodb console, because they have under these, this items, this way of exploring data where you can query. So there you can choose whether you want to query your scan, and then based on what you choose scan has very few options. But with query, you have a lot of things you can choose to explore based on the partition key. And just look at what you can sort when you drop down here. So by playing around with this, you're going to really understand in a practical way what you can do. But let's jump into learning a little bit more about query and scan. So let's talk about the query function. So this allows you to find items in a table based on the primary key values, and allows you to query a table or secondary index that has a composite primary key. So going back to the actual console, you can see that you have to or you have the choice of a prior partition and a sort. And we have some filtration options. So anything you can do there is going to really help you understand what you can do. By default, the reads are using eventual consistency. So if you want the reads to be strongly consistent, while querying, you can pass a consistent read equals true when you're using it in the SDK or the COI By default, it returns all the attributes for the items. So that's all of the columns. And you can filter those out by using projected expressions to only get the columns you want, definitely something you're going to want to do. And by default, all the all the data will be returned to you in ascending order, that's a to z. If you want it in descending order that Zed to a, you can use scan index forward false to do that, but you can see in the console, you can just click ascending or descending, it's a lot easier there. I just want to give you an example of a payload. So this is an example where if you're configuring the CLR SDK, these are the attributes you could pass out what you have here a consistent read, you're saying we want it to be true projected expression, we're saying I only want ID name created at and updated, scanning explored, we want to be returned in reverse, and we can limit it. So we say I only want 20. There's a lot of different options. But these are the most important ones for you to know. Now, we're going to take a look at the dynamodb scan function. So this allows you to scan through all items and returns one or more items through filters. And by default returns all the triggers for items. So if we were to try to do a scan, within the dynamodb console, you can see all we have is the ability to add filters, it's really, really simple. scans can be performed on tables and secondary indexes, they can return specific attributes. And by using projected expressions, we can then limit it just like query. So just like the other one, scan operations are sequential, you can speed up a scan through parallel scans using segments and total segments. And this is important because when you do a scan, it's going to return everything. And so you know, doesn't matter how many records, they'll just return everything. And so that's why this functionality is so important. But I'm going to tell you right now, you're going to want to avoid using scans when possible. They're much less efficient than running a query, because, as I just said, returns everything. As the table grows, the scans take longer to complete, which makes total sense. And a large table can use all your provision throughput in a single scan. So you know, avoid scans when possible, but they are there for you. So one thing we have to do when we make a new table is choose its capacity, whether it's provisioned or on demand. And we're going to first look at provision throughput capacity. And this is the maximum amount of capacity your application is allowed to read or write per second from a table or index. So here is an example of a table where we've chosen to use provisioned capacity. And down below, we have the option to choose or to fill in our provision capacity and zooming in to tell us what's going to cost us per month at that setting. So throughput is measured in capacity units. So we have RC use, which stands for read capacity unit and WC use, which stands for right capacity unit. So you'll see those abbreviations all over the place. And one thing that we can do, and you'll notice there's an option for auto scaling. So if we were to turn that on, this allows us to scale our capacity up and down based on utilization. So if we need to go beyond that five or 10, we'll just set that and we can say the minimum and the maximum. And the reason why you'd want to turn on auto scaling is to avoid throttling. So if you were to go beyond the capacity that you're set, and let's say you didn't have auto scaling turned on and you went beyond five reads per second, that data is going to get throttled. And that means those requests are going to be dropped. So it's literally data loss, they're not going to make into your database. So auto scaling gives you a bit more wiggle room. But you know, there are limitations to it. So there you go. That's provisioned capacity. So let's take a look at on demand capacity. And this is where you pay per request. So you pay only for what you use. So over here, we've have it set to on demand, you're going to notice that we cannot set the provision capacity, we cannot set the auto scaling because this is all going to happen for you automatically. And on demand is really good for cases where you have a new table, and you don't know how it's going to turn out, you don't know what you should set your capacity to. So it's just easier to go on demand, or you're never gonna have predictable traffic is going to be all over the place. You don't want to be throttled and lose traffic and that auto scaling group is not going to work because it's just too erratic. Or you know, you just like the idea of paying for only what you use, because dynamodb can get pretty darn expensive at scale. And the only limitations that are applied to you is whatever is the default on the upper limits for a table and the upper limit is 40,000 Arts use and 40,000 wc use so that's the worst, the worst damage someone could do to you. And what I mean by that when I say damage is that there is no hard limit imposed by on demand. So if you get a lot of traffic and it requires 40,000 RC use, it's going to spin up to that. So you just have to be careful that you know you don't have runaway traffic here because that will add Have you end up with a very large bill. But again on demand is still a really good feature because it gives you a lot more flexibility. So in Dynamo dB, it's important for us to know how to calculate the reads or the rights. So let's start with read capacity units are read capacity that represents one strongly consistent read per second or two eventually consistent reads per second for an item up to four kilobytes in size. So the whole point is figuring out what to put in this box. And so if we had data that was four kilobytes, or data or less, at 10, this would equal 10, strongly consistent reads, or 20, eventually consistent reads, but if it shows up in the exam, they're not gonna, they're gonna not ask you this stuff, they're gonna ask you how to calculate this number here. And that's what we're gonna figure out. But remember that we have consistent reads and strong reads. So we have to have two different formulas for calculating this number here. So let's first look at how to calculate RCS for strong, strongly consistent reads. And the formula is going to be we're going to round our data up to the nearest four, we're going to divide by four. And then we're going to times by the number of reads. So let's go through three examples. The first being 50, reads at 40 kilobytes per item. So we don't need to round it up to four, because 40 is already divisible by four. And we're going to divide that by four giving us 10 timesing, that by 50, that's how many reads we have. And that's going to be the number that goes up in here 500 RC use. The next one here is 10 reads at six kilobytes per item. So six needs to get rounded up to eight, that eight is divided by four, which turns into two, so we're going to take that two times two by 10. And that's going to be 20 rs use so that 20 is going to go up into that box there. The last one here we have 33 reads at 17 kilobytes per item. And so 17 kilobytes need to be rounded up to 2020 is divisible by four becomes four, and then we times four by the number of reads, which is 33. And that's going to give us 132 RC use. So we're going to now look at how to calculate RCS for eventually consistent reads. And remember that for each RCU, we get to eventually consistent reads per second. So the formula is going to vary here, but going to be pretty darn similar. So the first thing is we're going to round the data up to the nearest four, we're going to divide by four, we're going to times by the number of reads that we're going to divide by the final number by two and then we have to round up to the nearest whole number. So looking at the first example, we have 50 reads at 40 kilobytes per item. So 40 is already divisible by four, so we're going to divide by four, which gives us 10, we're going to times by 50, which gives us 500, then divide by two, and that's going to give us 250 our CPUs. The next example, we have 11 reads at nine kilobytes per item, so we're going to around up a nine to 12, which is the nearest four, which gives us and then we're gonna divide that by four, which gives us three, three times 11. That's the amount of reads, so that's 33, we're gonna divide by two, which gives us 16.5. And then we'll have to round that up to the whole number. So that's 17. So we're at 17 RC use. And then one more example here. So let's say we have 14 reads at 24 kilobytes per item. So 24 is divisible by four, so that gives us five, five divided by four, or sorry, 24 divided by four is five. And then five times five times 14 is 70. And then divide that by two uses 35 RC use. So I know math isn't fun, but it is something you definitely need to learn. So just drill through the stuff until you get it. And there you go. Now let's look at how to calculate, right, so this is for right capacity units. And a right capacity unit represents one item per second for an item up to one kilobyte. So if we have a right capacity of 10, and we have one kilobyte or data of last, that's gonna equal 10 rights, so I wonder if you notice, but this form is gonna be a lot easier than the reads. So how to calculate WC us, we're going to round our data up to the nearest one kilobyte, we're going to times by the number of rights. And that's it. So our first example here we have 50 rights at 40 kilobytes per item. So that's 50 times 40, which is 2000 wc use. Our next example here is 11 rights at one kilobyte per item. So that's one times 11 equals 11. So super easy. The last one a little bit tricky here, but we have 18 writes at 500 bytes. So we round up the 500 bytes to one kilobyte times that by 18 we have 18 WSU so super, super easy. Just remember that writes pretty much it's just times by whatever. Now we're gonna take a look at global tables and global tables provides a fully managed solution for deployment. You To multi region multi master database without having to build and maintain your own replication solution. So this is a very powerful tool that dynamodb has when you want to go global. And in order to use global tables, there are three things you must meet. So you need to use kms CMK. So you need to have a custom master custom master key with kms. You need to enable streams and stream type, it has to be new and old image, I believe. But something has to be set there. So that would show up here on the right hand side. So you can see once you have three checkboxes, you are good to go. And then you're able to create global tables. So all you got to do is add the region that you want, and choose it. And that's it. So global tables are very easy to utilize. It's just more of that activation process. But just remember what global tables are for they're for deploying a multi region multi master database without having to build and maintain your own replication solution. Dynamo DB has support for transaction. So what is a transaction, this represents a change that will occur to the database, if any dependent conditions failed, and then a transaction will roll back as if the database changes never occurred. I think at one point dynamodb didn't have transactions. And that was one of the reasons why people were like, well, that's why we use RDS databases, or sorry, relational databases, because they're acid compliant. But looks like dynamodb has bolted on that functionality. So that is great. And if you're wondering what acid stands for, it is for a thomassie. Thomas CD, I can't say it consistency, isolation and durability. So what I want to do is give you an example of a transaction so you can conceptualize it. And these transactions work really well like especially when you're thinking about money, where you have to have multiple steps before something goes through before you release money. So the way it works is that if any of these steps fail, then the transaction will immediately fail and rollback the changes. So the first thing is we create a new payee. And then we go ahead and we verify that the email is correct format before sending out the money. But it turns out that it's not. And so what happens is it stops and rolls back. So none of these actions actually occurred. So that is in a nutshell transactions, but let's look at it in more detail. So now we're going to look at how dynamodb transactions work since we just covered conceptually what transactions are. So dynamodb offers the ability to perform transactions at no additional cost using two functions they have which is transact write items and transact get items. You'll notice one says right and one says get. And that's when you want to group together a bunch of right actions or get actions. So those are your limitations there. The transactions allow for all or nothing changes to multiple items both within and across tables. Notice that it says across so you can do this with multiple tables. dynamodb will perform two underlying reads or writes for every item in the transaction, one to prepare the transaction, one to commit the transaction, so that is going to consume your RC use or WC use. So there is a little bit of extra cost there. But it's negligible, you shouldn't even think about it. These two underlying read and write operations are visible in your Amazon cloudwatch metrics. So if you're wondering, you want to keep track of that stuff, that's where you can check it. You can use condition check with dynamodb transactions to do pre conditional check. If that doesn't make sense, this is another explanation. So it checks that an item exists or to check the condition of a specific specific attributes of the item. So it's just a way of doing a check before you run that transaction. So that is the specifics of a dynamodb transit. So TTL, which stands for Time To Live allows you to have items in your dynamodb expire after a given amount of time. And so when I say expire, I mean they get deleted. This is great if you want to keep your databases small and manageable. Or let's say you're working with temporary, but continuous data examples could be session data, event logs, or just based on your usage patterns. So the way you're going to enable Time To Live is in your dynamodb table, you're going to click on TTL. And then you have to enable it and you need to provide it a attribute. So here I'm providing expires that which is a string that has to be in a date time format. And this is what's going to be used to determine when a record should be deleted doubt if you've been paying close attention to the dynamodb section, you know that there is no date time. data structure and everything are strings, because dynamodb just doesn't have a date time datatype so it's important that you use a you format the string in epoch format, okay, and so if you're not familiar with that, this is just a I think was Is ISO 8061. And what you'd have to do is convert this into a POC, which looks like this. It's a bunch of numbers. So there, you'd have to programmatically do this, or you'd have to use it online calculator. But the advantage of TTL also is that it'll save you money, because the smaller your database is, the less likely you'll have partitions and the more money you will save. So there you go. dynamodb has a functionality called streams. And when you enable streams on a table, dynamodb is going to capture every modification to a data to data items so that you can react to those changes. So if an insert, update or delete occurs, that change will be captured and then sent to a lambda function. So changes are sent in batches at a time to your custom lambda changes are sent to your custom lambda in near real time, each stream record appears exactly once in the stream for each item that is modify the stream records, records appear in the same sequence as the actual modification. So here's an example. Let's say I update an item. So I'm updating chief O'Brien, this could be insert, and it gets inserted in the database. But now we're going to react to that insert and send that data to dynamodb stream. And that dynamodb stream is configured to send it to a lambda. And when that goes that lambda, we can do anything we want with that data. So we could have it so that it sends out an email, or you get sent to kinesis, firehose, or whatever we want to program. So it's just a way of reacting to inserts, updates, and deletes. So there are a couple of errors, I want to review with you on dynamodb that I think you should know the first one being throttling exception. So the rate of requests exceeds the allowed throughput. This exception might be returned if you perform any of the following operations to rapidly so CREATE TABLE update table delete table, this is likely to happen, I would say with update table because it's not it's not frequent, that you're creating tables or deleting tables, but it's possible, you might send multiple actions to an update table. The other error I want you to know and this one is extremely common. It's provision throughput exceeded exception. So you exceeded your maximum allowed provision throughput for it for a table or for one or more global secondary indexes. This error occurs when you've exceeded your, your throughput. So that's your capacity units, your reads or your rights. And so you're very likely to see this error. When an error does occur on dynamodb, you're gonna likely be accessing dynamodb via the AWS SDK. So writing programmatically your code, and when it when an error fails, the SDK has a built in so it will automatically retry when something failed, it'll try it again, as well, it will implement exponential back off. Have you ever heard of this before, the idea is that I've encountered an error, I'm going to wait 50 milliseconds before I try again. And if that one fails, I'm going to try 100 milliseconds. And if that one fails, I'm going to keep on doubling that time. And here, it's going to update it up to a minute before it stops. So this is just a strategy for trying to make sure changes make their way through. So if you're using SDK that ensures that you're not going to lose data, because it's going to try and try again. And so I want to point out that these two exceptions are important because they're very likely shopping your exam. Very, very likely provision, throughput exceeded exception. And notice that it says provisioned throughput. So remember, there's a there's two capacity modes, we have throughput, provision, throughput, and on demand. So this error provision throughput would never happen for on demand for on demand, this error could never occur. On maybe it would occur if you exceeded the 40,000 RCU, or w are the right capacity units or the read capacity units. I don't know I've never exceeded it. So I couldn't tell you what error shows up. Maybe it's called on demand exception, but this error is extremely common. So you have to understand. So in dynamodb, there is the concept of indexes. And indexes are extremely common with databases. So what is an index? A database index is a copy of a selected columns of data in a database, which is used to quickly sort. So it's just when you need to quickly look up information, you're literally cloning a version of your database. And dynamodb has two types of indexes. We have LSI, which stands for local secondary index. And these can only be created with the initial table. And then you have GSI eyes which are global secondary index, and we're going to get into both of these in extreme detail. But the takeaway I want you to remember from this is that you generally want to use Global over local, and this is what's recommended in database documentations. And another deciding factor could be strong consistency. So a local secondary index can provide strong consistency, where a global secondary index cannot provide strong consistency. So now that we have a little bit of idea what indexes are, and let's jump into lsis, and GSI. So let's first take a look at local secondary indexes. So it's considered local, and that every partition of an LSI is scoped to a base table partition that has the same partition key value, if you're only what base table is, that is the initial table that you're creating, you're creating an index from the total size of index items for any one partition key value can't exceed 10 gigabytes. So that is definitely a hard limit with local secondary indexes. It shares the same provision throughput setting for READ WRITE activity with the table that is indexing. So that's the base table. And that makes sense because it's a local table, so of course, it's going to share, and then it has a limit of five per table. Now let's take a look at actually how we create a local secondary index. So the LSI is can only be created with the initial table. So here you can see I'm making a table and I'm adding index index at the time of creation. You cannot add, modify, or delete LSI lies outside of this initial table step. So you have to really get a right here. And now and if you need one, you literally have to make a new table. So the only way you can make a new also, you'd have to make a new table and move your all your data over. So you need to have both a partition and a sort key. And there are some conditions around those partitions or keys, the partition key must be the same as the base table. And this makes sense because it's a local, it's a local partition. So it's working off that base table. The second sort key should be different from the base table. Now you could make it the same, but then it defeats the whole purpose of having a secondary index, which is supposed to be optimized to sort in a different manner. So you'd want to choose a different sort key in this case. So you know, hopefully, that makes local secondary indexes more clear. So we'll move on to the next one, which is global secondary index. So global secondary indexes are considered global, because their queries on the index can span all of the data in the base table. And across all partitions. These indexes have no size restrictions where we saw with LSI, there was a 10 gigabyte limit for all items in that index. They can provision their own throughput settings, and they consume capacity, but it's just not from the base table. So that's a good thing. And there's a limited to 20 per table, I think you might be able to use a service limit increase to increase that I'm not sure. But even if that's not the case, it's not going to show up an exam. So do not worry. If you want to create a global secondary index, you can create it while you're creating the table. Or you can make one afterwards and you can modify them and delete them at any time. So it's extremely versatile to create these things. And the partition key can be different from the base table. And you should make it different generally, because that would make sense. But I guess if you had not major local secondary index and you wanted a partition key with a different store key, then I guess you'd have to make a GSI. But just notice that you can set the partition key to whatever you want. And the sort key is optional. So you don't actually have to have it, you could just have a partition key. So that'd be a simple key. But yeah, that's global secondary indexes. So now let's just pull them up side by side and make sure that we understand the differences clearly. Okay, so let's just reiterate through LSI versus GSI. So we really know the difference between these two and which one is better in each specific aspect. So starting at the top here, when we're talking about key schemas, local secondary indexes only have composite. And remember, composite is both a partition and sort key. And then global secondary indexes support both simple and composite. Then for key attributes for elseis. The partition key must be the same as the base table because remember, it's local, it has to use the same partition key for GSI is the partition and sort key can be any attribute you like. Then for size restrictions, the LSI has to be 10 gigabytes or less for all index items. And then for GSI is it's unlimited. For online index operations. The only time you can create indexes is on table creation. But for GSI is you can add, modify, delete indexes at any time, and you can make these indexes at the time of creation as well. If you want to, for queries and partitions, the LSI is query over eight or a single partition as specified by the partition key value in the query for GSI, is it queries over entire table or across all partitions? For re consistency, we have strongly or eventual consistency. So you can see LSI win over GSI. In this one case, where a GSI is only eventual consistency, for provision, throughput consumption, this is shared capacity with the base table. So you're more likely to get throttled here, both for GSI is it has its own capacity, so it's not gonna affect the base table. And then the last one here, which we didn't talk about, but we'll talk about now is projected attributes. So when you create that table, you say, what, what attributes. So those are the columns that are allowed to be in that index. So for local secondary index, you can request attributes that are not projected in into the index, whereas GSI, you can only request attributes that are projected in the index. So over those two points, you can see why gsis are generally more recommended than lsis. But you know, it all comes down to your use case, so you'll have to decide for yourself. So here we're gonna take a look at dynamodb accelerator, also known as DAX, and it is a fully managed in memory cache for Dynamo DB that runs in a cluster. And its response times can be in single digit millisecond. So DAX can reduce response times to microseconds. And that's really important for certain types of workloads, which we'll talk about a little bit more in the next slide. So here's an illustration of generally what DAX is. So let's just talk through the points of how this thing works. So you have a DAX cluster, which consists of one or more nodes, so I haven't called cash here, each node runs its own instance of the DAX caching software, one of the nodes serves as the primary node for the cluster. Additional nodes if present serve as read replicas, your app can access DAX by specifying the endpoint for the DAX cluster. So there we can see the app and it's accessing via the endpoint. The DAX client software works with the cluster endpoint to perform intelligent load balancing and routing. So just takes care of stuff, you just use the endpoint and figures everything out. And incoming requests are evenly distributed across all the nodes in the cluster. So now that we kind of have a little bit of an overview of DAX, let's look at the use cases of when and when not to use DAX. So let's take a look at DAX use cases. And to best understand this, let's say what DAX is good for and what it's not good for. So apps that require the fastest possible response time for reads. That's real time bidding, social gaming and trading applications. This is where you're gonna want use DAX apps that read a small number of items more frequently than others. apps that are read intensive, but are also cost sensitive. Please take note of read intensive, that's what DAX is usually for apps that require repeated reads against a large data set. Notice that we said reads again. And now on to the non ideal side apps that requires strongly consistent reads. Because DAX is not strongly consistent is eventually consistent apps that do not require microsecond response time for reads or do not need to offload repeated reactivity from underlying tables, apps that are write intensive or that do not perform much activity, no services on a right. That is not what DAX is intended for. It's for read apps that already are using a different caching solution with dynamodb. And are using their own client side logic for working with that caching solution. So if you run into cases on the right, what would you do for caching in Dynamo dB, and that's where elastic cache would come into place, because you can put elastic cache in front of Dynamo dB. So if you're dealing with write intensive stuff, and you can make advantage of Redis there, and so that would be the case there. So we're on to the Dynamo DB cheat sheet. And this one's more special than all the rest. And that's why I prefixed it with ultimate. Because Dynamo DB is the most important service, you need to know to pass the ADA certified developer associate. It's extremely critical to the certification. And this cheat sheet is a very long, okay, it's the longest one in this course. It's seven pages long. And it actually started out as only being five pages. I had published a preview on Twitter, and Kirk, who is a senior technologist at AWS, specifically for dynamodb. Notice I made some mistakes and made the offer to review the entire cheat sheet for accuracy. And I send it over to him. And he turned this five page cheat sheet into a seven page to cheat and I even learned a lot of great things. So you know, I think we all benefit from Kirk's help here. And so I want to tell you, if you can do me a favor to go on Twitter if you have Twitter. I want you to tweet out to him, he was certified dynamodb. And thank you thank him for helping us with this ultimate dynamodb cheat sheet. He did it on his own time, he didn't have to do it, you know, and this was his own effort. So we greatly appreciate it. And you know, I really hope that it helps you pass the exam. So let's jump into this ultimate dynamodb cheat sheet. Okay, so let's jump into the ultimate dynamodb cheat sheet. So dynamodb is a fully managed no SQL key value document database. dynamodb is suited for workloads with any amounts of data that require predictable read and write performance and automatic scaling from small to large. and everything in between dynamodb scales up and down to support whatever read and write capacity you specify per second in provisioned capacity mode, or you can set it to on demand mode and there is little to no capacity planning dynamodb can be set to support eventually consistent reads by default, and strongly consistent reads on a per call basis. Eventually consistent reads data is returned immediately, but data can be inconsistent copies of data will be generally consistent in one second. Now talking about strongly consistent reads, will always read from the leader partitions since it always has to be has to have an up to date copy data will never be can be inconsistent, but latency may be higher copies of data will be consistent with a guarantee of one second, Dynamo DB stores three copies of data on SSD drives across three ACS in a region. I think before I had said across three regions, but this is my misinterpretation of the documentation. And I don't even know if it's really three azs more so managed data centers by AWS. But it's easy to understand them as AZ. So that's what we're going to call them dynamodb most common data types are be binary, and number as string. I think there's a few other ones there. And some of them share the word like start with B, so it gets a bit confusing, but these are the three that I want you to know. Tables consists of items, which we call rows, and items consist of attributes which we call columns. A partition is when dynamodb slices your table up into smaller chunks of data. This speeds up read for very large tables. dynamodb automatically creates partitions for these scenarios. So every 10 gigabytes of data when you exceed RC use of 3000 or WC use of 1000 limits for a single partition. And the last scenario when dynamodb sees a pattern of a hot partition, it will split that partition in an attempt to fix the issue. So that is page one, we're gonna go to page two. So we're on to page two of the ultimate Dynamo DB cheat sheet. So Dynamo DB will try to evenly split the RC use and WC use across partitions, primary keys defined where and how your data will be stored in partitions. So primary keys come in two types. We have simple primary keys using only a partition key, and composite primary key using both a partition and sort key. partition key is also known as hash. And sir key is also known as range. I mentioned this before. But the reason they used to be called hash and range, I don't know. But the point is they change them. And when you're using the COI, or, or the SDK, they still call them hash and range, so it's important to know both of them there. When creating a simple primary key, the partition key value must be unique. When creating a composite primary key. The combined partition and sort key must be unique. When using sort key records on the partition are logically grouped together in ascending order. dynamodb Global tables provide a fully managed solution for deploying multi region multi master databases. dynamodb supports transactions via the transact write items and transact get items API calls. I don't know if I mentioned it there. But the point of these transaction calls is that they let you go across multiple tables Oh, I write it right here. So transactions let you query multiple tables at once in an all or nothing approach. So all API API calls must succeed. dynamodb streams allow you to set up a lambda function triggered every time data is modified in a table to react to changes. And I love dynamodb streams, I use it all the time for a lot of projects. I don't feel like we need to know too much on the developer about it. But there's definitely a lot we could write about it. And streams do not consume RCU. So that is the nice thing about it, they're not going to use your read count up. So that is page two, we're moving on to page three. So we're on to page three of the ultimate dynamodb cheat sheet. So dynamodb has two types of indexes. We first have lsis which is local secondary index, and then we'll talk about gsis. So LSI supports strongly or eventually consistent reads. They can only be created with the initial table. They cannot be modified and cannot be deleted unless you're also deleting the table. That's the only case where you'd be able to delete it. They only use composite keys. You They have to be 10 gigabytes or less per partition the shared capacity units with, they have to share the capacity units with the base table, you must share a partition key with the base table. So that's very important as well. Moving on to GSI. So global secondary index, they cannot provide strong consistency, the only way to get strong consistency is with LSI. Okay? Very important. Remember that point. So only eventual consistent reads can create, modify, or delete at any time, that's extremely convenient. Simple and composite keys is what you can use here can have whatever attributes. So the partition key can be whatever it wants, and the sore key can be whatever it wants, there is no size versus restrictions per partition. It has its own capacity settings. So there you go. That is, I think we're page three onto page four. So we're on to page four of the ultimate dynamodb cheat sheet. So we'll talk about scans and we'll talk about queries. The first thing I'm going to tell the scans is, your table should be designed in such a way that your workload primary access patterns, do not use scans. Overall scan should be needed sparingly in frequent reports. So generally, you do not want to be using scans, scan through all items in a table and then return. One or one or more items through filters, by default returns all attributes for every item you can use per project expression. to limit the attributes that you want to use. scans are sequential, you can speed up a scan through parallel scans using segments and total segments. scans can be slow, especially with very large tables and can easily consume your provisioned throughput. scans are one of the most expensive ways to access data in Dynamo dB. And we'll move on to queries next. So it's about finding items based on the primary key values, tables must have a composite key. And in order to be able to query by default queries are eventually consistent. But if you want to use strong, strongly consistent reads, you can use this attribute strongly consistent reads set it to true. And now you're doing strong, by default returns all attributes for each item found by query. So just like scans, you can use project expression to filter stuff out by default is sorted, ascending, and you can use scan index forward, false to reverse the order and descending. I don't know if that option is available in scan. But you know, just know that scan index forward is used to flip the order. So there you go. So we're on to the fifth page out of seven for the ultimate dynamodb cheat sheet. So dynamodb has two capacity modes provision and on demand, we're talking about provision first. So you can switch between these modes once every 24 hours. provision, throughput capacity is the maximum amount of capacity or application is allowed to read or write per second from a table or index. So the provision is suited for predictable or steady state workloads. It's very important to understand the concepts of RC use and WCS, especially for provision throughput, because you definitely set these values here. So RC use is a read capacity unit. WC uses right capacity unit. And with dyno, and with provision throughput, you can set auto scaling. And so it's recommended you enable auto scaling with provision capacity mode. In this mode, you set a floor and a ceiling for the capacity you wish the table to support. dynamodb will automatically add or remove capacity to between these values on behalf and throttle calls that go above the ceiling for too long. And if you go beyond your provision capacity, you'll get an exception provision throughput exceeded exception for the exam, you want 100% want to know this, it will absolutely show up on the exam. And this is what happens when throttling occurs. And if you're not familiar with throttling, it's when requests are blocked due to read or write frequencies higher than the set threshold. So an example for exceeding the set provision capacity, we got partition splitting table index capacity mismatch. So that is provision throughput, we're gonna move on to on demand. And on to the next page we go. So we're on to the sixth page of the ultimate dynamodb cheat sheet talking about on demand capacity. And this is pay per request. So you only pay for what you use on demand is suited for new or unpredictable workloads. The throughput is only limited by the default upper limit of the tables, that's 40k, RCS and 40k. WCS, WC is that's extremely high value. And throttling can occur if you exceed double your previous peak capacity. So the high watermark within 30 minutes had no idea of this. If you previously peaked to a maximum of 30,000 ops per second, you could not peak immediately at 90,000 ops per second, but you could at 60,000 ops per second. So that is definitely something I did not know. And I'm really glad Kirk put that in there because I had something way more simpler before. Since there is no hard limit on on demand. It could be very expensive based on emerging scenarios. So just be careful with on demand there. But you definitely have the flex flexibility where you don't have to think about setting your capacity. So that's pretty nice. Now let's talk about calculating reads and writes. This is definitely more important for provisioned throughput, not for on on demand capacity, but we'll go through it now. So for calculating reads for RC use, a read capacity unit represents one strongly consistent reads per second or two eventually consistent reads per second for an item up to four kilobytes in size. And how you're going to calculate RCS for strong is the following round up data to the nearest four divided by four times by numbers of numbers of reads. And then we'll move on to how to calculate for RCS for eventual, so round data up to nears four divided by four times by the number of reads and divide final number by two. And I think you got to round it up. And then round up the nearest whole number. If you really can't remember that stuff, here are the examples. And I'm hoping you're printing out this cheat sheet on the day of your exam, so that you can look through these and make sure you know these for sure. And so that's Page Six, and we're on to the last page, page seven. So we're on to the last page of the ultimate dynamodb cheat sheet. So let's finish strong here, we're going to do some calculating of rights. So our read capacity unit represents one rate per second for an item up to one kilobyte, how to calculate rights, what we're going to do is rounded up to the nearest one times by the number of writes. And we'll talk about that i will i have the example there, we'll just show you there at the end. Oh, so we'll talk about dynamodb accelerator, also known as DAX is a fully managed in memory write through cache for dynamodb that runs in a cluster. So reads are eventually consistent, incoming requests are evenly distributed across all of the nodes in the cluster, DAX can reduce read response times to microseconds, and let's say well, where it's ideal and where it's not. And this is definitely debatable, but I got this from the docs. So you know, you can't argue with me, but I know some people might consider otherwise. And if it's for the exam, they generally fall whatever on the docks until they've been changed. So I DAX is ideal for the fastest response science possible apps that have apps that read a small number of items more frequently, apps that are read intensive. So that's the one I'm highlighting there. And then DAX is not ideal for apps that require strongly consistent reads apps that do not require microsecond reads response times. apps that are write intensive, or that do not perform much read activity. And if you don't need DAX, consider using elastic cache. That's not a hard rule. But that's a good rule for the exam. If you if you gotta throw up, like a threw up a toss up between DAX, and elastic cash, and it doesn't need micro microseconds and you know, consider using elastic cash there, or if it's more write intensive. And just to show you there, these are the examples, you definitely want to print out this cheat sheet and have it on exam day. You know, I hope this this ultimate dynamodb cheat really makes the difference for your exam. So it looks like I lied, and there's actually eight pages to this Dynamo DB cheat sheet I almost forgot to include Dynamo DB API commands which you use vcli which is really important because these could show up on the exam so let's go through them. The first being get item this returns a set of attributes for the items with the given primary key if no matching item that it does not return any data and there will be no item element in the response then you have put item creates a new item replaces an old item with a new item. If an item has the same prime primary key as the new item already exists in the specified table, the new item completely replaces the existing item, then you have update item edit an existing items attributes or adds a new item to the table if it does not already exist. Then you have batch get item this returns the attributes of one or more items from one or more tables you identify requested items by by primary key a single operation can retrieve up to 16 megabytes of data which can contain as many as 100 items, then you have a batch right item puts or deletes multiple items in one or more tables. And right up to 16 megabytes of data which can be compromised, which can compromise as many as 25 put or delete requests individual items to be written can be as large as 400 kilobytes then you have CREATE TABLE just as the name implies, it adds a new table to your account table names must be unique within each region. So you could have the same database name or search or same table name but in two different regions. Then you have update table so modifies the provision of throughput settings, global secondary indexes or dynamodb stream settings for a given table, delete table and this is very obvious it just deletes a table with all of its items. Then you have transact get items a synchronous operation that atomically retrieves a multiple items from one or more tables but not from indexes in a single account and a region can contain up to 25 objects, the aggregate size of the items in the transaction cannot exceed Four megabytes. Then we have a transact write items a synchronous write operation that groups up to 25 action requests. These actions can target items in different tables, but not in different AWS accounts or regions. And no two action can target the same items. And we're really running out of space here. But we have query finds item based on primary key values, you can create table or secondary index that has a composite primary key. And last is scan returns one or more items and more items and item attributes by accessing every item in a table or secondary index. So there you go, that's the real end to the dynamodb cheat sheet, super long section, but definitely worth it and super critical to passing the developer associate exam. Hey, this is Andrew Brown from exam Pro, welcome to the dynamodb. Follow along. And what we're going to do is we're going to create a table loaded up with data, write some records, delete some records, get some records in batch. And just really understand how Dynamo DB works. What I need you to do is get Dynamo DB open up here in a tab. So I just type in Dynamo dB, click that there and you will make it to the dynamodb page, make sure you're in US East one eight of us loves to put you in Ohio or somewhere else. And just to be consistent. Let's always do US East one for these fall alongs, you're going to need a cloud nine environment set up here I showed you some Elastic Beanstalk follow along, I show you this in a variety of different ones here. So if you're not sure how to do it, go check those out or give it a go and try to spin up yourself an environment. And I have the Dynamo DB documentation open up here. So we can poke through this as we're working through these commands. And we're also going to need a couple files from the the GitHub repo here, the free eight of us developer associate. So I have a file here that helps transform data and then the actual data we plan on importing. And we're going to be working with starship data from Star Trek. So I have a list of starships. And we can see that this is the data we have. And that's what we're going to be importing. So let's make our way over to dynamodb. Let's take a look at what it takes to create a table. We're not going to create our table through here we're going to use the COI. But let's just talk through what's on the page here. So the first thing you do is you'd name your table. So I'd call mine starships. And then you set your primary key, we have the option of setting a partition key and a third key. AWS used to call this a hash and a range. And this will show up in the code because there's so named that that way. So looking at our data, what we'd have to do is decide we'll be making good partition and sort key and in this case, the good partition key is going to be shipped class because it's a grouping of things you can see you have Crossfield, Crossfield, Crossfield. And then, as long as you as long as you have a unique value with both the sort and partition, it's okay, as for the sort, we're using a registry number that identifies the ship, and those are all unique. So these, this will definitely be a unique value. Generally, you want your store key to be a date, but it all depends on your data. And in this case, we don't have a date, value. So it's going to be shipped class as our primary or our partition, sorry, and our registry as our sort key. So what we do here is we type in the name, I would call it ship class. For some reason dynamodb likes to name things, camel case, or at least all the documentation out there. So let's just follow suit there. But you could name the lowercase if you wanted to. And this would be registry. And over here we have some data type string binary number dynamodb does not have a date time format, we would normally use string in that case, and there might be a few other ones outside of here. But these are the these are the only ones you need to know S stands for string B is for binary and is for number. And these are both going to be string values. You have some default values here, no secondary indexes provisioned capacity five, five reads and five writes, it has auto scaling turned on and encryption is at rest as default encryption type. So we just checkbox that off, we can see those values here five and five provisioned, auto scaling is turned on. And this is defaulted. If you want to recreate a local secondary index, the only time you can create it is at this particular time. So you'd have to go ahead here and add those. And for local tables, you always want to have the same, you always want to have the same partition key. And then you'd have a different sorting, you have to specify them both. We don't have another value here, but I put a name and now I can checkbox on secondary, local secondary index. We're not going to create any secondary indexes. They're not that important to know other than like how they work, but we don't really need to go through the motions of actually using them. But this is what we're going to do but we're going to use the COI to do this. So make your way back to cloud nine. I'm going to create a new folder here called MK or we'll type in MK dir to make that folder and type in dynamodb Dynamo dB, we'll call it playground. Any files we're working with will place them in there and already named that wrong. So I can go ahead here and just rename that file. Okay. And I'm just going to go ahead and make a new file in here. And I'm just going to call it like scratchpad. Because this is where we're going to write out all our CI commands, and then paste them in there, just so that it's a bit easier to work with. I'm just gonna clear that there. Yeah, and so let's get to it. So what we're going to do is create our table using the ccli. So what we do is type in AWS dynamodb, I think we listed out it'll tell us a bunch of information, something we've typed in help maybe. And so this is a great way of getting a lot of information about dynamodb. But we're just gonna use the docs for this. So let's look up create table. And in here, you can see the values that we can specify, I'm not sure which ones are required, but I generally know what we need to enter, the first thing is going to be attribute definitions. And so the attribute definitions, if we just go to that section, an array of attributes that describe the key schema for the table indexes. So what it's asking for is to specify the attributes for that we're going to use in the actual key schema, so the partition key and the store key, and we decided that we are going to use the ship class and the registry. So if you just look at the syntax here, this is the format that we have to type it in. So we'll make our way back, I'm just going to copy this, save myself a lot of trouble. So I'm really terrible at writing these things out. I'm going to type in create table, and then put a backslash that lets us do multi line. And we'll do attribute definitions. And to do two spaces there, and we'll go back here, I'm just going to copy this format. And so that is our first one. And it's going to be I said ship class. And it's going to be s for type four type string. I'm not sure if it tells us the types around here, I'm not seeing it, but I but we see there's s and NB. So back here remember we have string binary number, that's what the represented in this in the CLR in the API s and NB. So we want them to build the string. But that's our first one. So we'll go back here. And we will need a second one. And we said that would be registry. And that's going to be a string type as well. The next thing we're going to need is the key schema. And this specifies the attributes that make up the primary key for the table. And below, you can see that we need an attribute name and a key type. And they might have an example here. Yes, they do. So here it is. And we will make our way back to here and we will type in key schema. And I'll just paste that in there. And it's going to be the same thing. I know it's a bit redundant, but that's just how it is. And the ship class is going to be the hash. And the registry is going to be the range. Remember I said earlier that dynamodb used to call them hash and range and they still appear in the code. This is what I'm talking about. You can see here it says hash is the partition key range is the sort key on the exam for the developer associate, you might actually see a bit of CSI stuff here. And then you might need to understand what is what. So just be aware of that. And the next thing we might need to specify is the provision throughput, this might have a default, but we'll take a look here. I'm gonna scroll up here. So I can find the name here. provision throughput represents the provision throughput for the specified table. This is probably a required field, I'm not seeing any of this is required. But I almost feel that it is. And we're going to just set that default for that to five and five. So I'm just going to copy that there, make our way back over here. And we'll just put two spaces there for backslash. And then we're going to set the Read and Write capacity, we're just going to set it as five and five. So we'll do five, and five. And one more thing I want to do is set the region always, always always when you're using to see Li set the region if you can, just because 80 of us might default it to Ohio and then you're just gonna be scratching your head looking for stuff. So I think this is our crate commands, we type create Dynamo DB table. Nice documentation for ourselves. And let's see if this works. We'll just copy that. Paste that on in, hit Enter. And it did not work. Let's see here. Let me just double check here. backslash backslash backslash backslash looks all good to me. Maybe I didn't copy it and write it. We'll just try this one more time, I'm gonna type clear here, paste that in, hit enter. Okay, just give me a second to figure this out. Oh, my goodness, it is the most obvious thing, I didn't specify the table name. If we don't specify the table name, this thing is not going to get created. So I guess we just skipped right on over that, I'm just going to go down here. And we just provided a string. So I'm just going to copy this here. We'll go back up here. I call this starships. And we'll have a backslash there, I'm gonna save that. And we will copy this, paste that in, hit Enter. And we're getting output that is good. This so this means that it definitely has created and look down below where it says table status. It says creating object crates is pretty darn quick. If we make our way over to dynamodb, I'm just going to go back to the service here. And go to tables on the left hand side. And it's active. So it's already been created super, super fast. Here we can see our ship class, our registry. But we don't have any data in here. So until we have data, it's gonna be a little bit hard to do anything. So I think that's what we'll do next. But but just before we do, I just want to show you the describe command. So let's say that it was taking a while for our dynamodb table to create. I don't know why, but let's just say it was really slow. And we wanted to check on the status of it. So we wanted to see that it was active, we could do is type in and I will do it up here. So we have reference to it. Describe table. We do alias dynamodb describe table table name, starships. I don't know if there's really any other output than that a lot of COI commands have this describe. So when you use the COI long enough, you just start guessing as to what it is, I didn't have to look it up to know what it was. We'll go here, we'll see if there's any other options. No, it's just table name. So we just have table name here. And I'm going to go back to the cloud nine here. And if I just take that and paste that in there, we're getting pretty much the same output as before. And you can see now that it's active. And a lot of times I like to change the output here. And I might do table, this one would probably be good as a table. So I think it's a little bit easier to see table status over here, it's a little bit easier to read that way, we could also do text. For this one, it probably be a bit messy. So that's not very readable. So for this case, I've used table. By default, it's JSON. So there you go. So what we'll do next is we'll move on to getting our data into our table. And we're gonna have to prepare it because right now it's as a CSV file over to Jason, but we'll do that next. So what we're going to do is we're going to make a new folder in our dynamodb playground, because we're going to generate out a bunch of batch files, because we're going to need to batch import our data. So when you go here in the left hand side and make a new folder and run called that fetches. And we are going to need this the state of here. So both the CSV file and the CSV jason.rb. Probably the easiest way to get it in there is just to click the raw button here, Copy that. And we will make a new file in here, which we'll call CSV, to jason.rb. If you're wondering why this file is written in Ruby, it's because that's my favorite language to use. So why not. And you don't need to know Ruby too much, you just I'll walk you through this file here in a moment, just so you understand what's going on here. We'll go back. And the next thing we'll need to do is grab that CSV file here. So I'm gonna go there, get the raw data, copy all that. And we will right click New File, Starfleet dot CSV. And we'll double click that. Ok. Close tab here. It's because I did this project earlier. So it had a cached version, it was a bit confused. And so um, I pasted it in there and just make sure that name is correct. So let's take a look. Let's take a look at this file here. Let's talk about why we have to convert this to JSON and why it has to be in terms of batches. And the easiest way to understand is actually to first look at the command that we're going to be using which is the batch right item. So the batch rate item operation puts or deletes multiple items in one or more tables. A single call to a batch rate item can write up to 16 megabytes of data, which can be comprised of 25 Put or delete requests. So we have some limitations here. And individual items written can be as large as 400 kilobytes. So the thing is, is that we have records here, we go back, because we have nice visual here and GitHub. But you can see that we have a little boy 310 Records. So that's more than 25. So we have to break this up into batches of 25. And we also have to provide it in the JSON format that it's expecting, I'm not aware of being able to import CSV, I'm gonna check here, nope, there's no way to import CSV, and the format that it's expecting. It looks like this, here's this, here's this request items, JSON, it expects you to put the table name here, and then have this structure request, put item and then the values. So that's what we need to do, we need to transform our data into that format. And that's why I wrote this little script here to do this. So we'll quickly just walk through it. So what it does is it pulls in the CSV file, and it reads the CSV file using the CSV, a CSV file, Ruby library. And this, we may or may not need. When I originally created the CSV file, it was an Excel and it was encoding the file non UTF, eight, but this other format, but we just copy and pasted from here, so maybe we don't need a slide anymore. So we'll see. This says include the headers, so it will detect these headers here, name registry, etc. And then it will map it into a JSON file here. So when you lie, we can then specify that then like this, the idea is, what we want to do is iterate through this file, so it's going to go line by line, it's gonna start at 25. And Amelie's, it starts at 25, it's gonna reset it back to one, and it's going to push on a new batch of files. So it's gonna start with the starships file that's named the file. And then it's going to then format the information into this put request. And it's going to do that it's going to create a bunch of batches, that's going to iterate through those batches of data and create a bunch of batch files for us. So hopefully, that makes sense. But it's better to see it in action than to talk about it. So what I want you to do is go down below here to your batch type clear. And we want to make our way into this folder here. So type in dB Dynamo dB, I'm hitting tab to autocomplete it that saves a lot of time, hit Enter. And we want to run this Ruby file. So type in Ruby, CSV to json.rb. And if everything is named correctly in here, start fleet CSV, it looks correct To me, this should work. And there we go. There's our batch file. So we just transformed our data, let's take a look at one of them. And that's the format we're expecting, right? If we look back here, that looks like the same format. To me, this one's a bit more compact. But that's how we transfer my data. And each of these contain 25 Records. So now to import this, we're gonna go back here, and we need to write our import command. So that import command is going to look like eight of us dynamodb will say this, we're using the batch, right item. So Dynamo dB, batch right? Item. And we'll make our way over here and see what we need to specify, they should have an example here. Examples. And so we just specify the file. So it's as simple as that. So I'm just gonna copy that there. But we could just copy this whole line here, save us any trouble if we made any spelling mistakes. And this file is in batches. So we'll type in batches. batch hyphens, 000. I'm just going to paste this a bunch of times. I'll go through here. And we'll just change this 12345678910 11 and 12. And what we'll do is we'll just copy all these and paste them in. There they go. And then number 12, we might just have to hit enter there. And so it says unprocessed items. So if there was anything in there, that means that it didn't process those. This could happen if we had maybe too many items, or if we were being throttled or not throttled, but we hit our read capacity or sorry, our write capacity. We didn't execute these perfectly. So if we go back to dynamodb here and give us a refresh, we can now see our records. So there they all are, look at that. How great is that? Um, so now that we know how to batch right, I guess the next thing is let's look at how we can actually get this data programmatically through cltc Li. So to get items, what we'll do is go back to the CI here, just click Back, click up here. And we should have get. So there's a basket, a basket item, which we'll look at in a moment, but we'll look at get item first. So get item, you specify the table name, you specify the key, we have some additional things here, where you can keep it really, really simple. And we're just going to do a simple get, look at the examples here. And yeah, that's as simple as it gets. So we're gonna copy this over here, we're gonna make our way back to cloud nine, just type here, get item. And so we have Dynamo dB, get item table name, our table name is starships. We'll just copy that here. That is a such a short line, I'm just gonna have it one line here. And then we need a key file. So I'm just going to make a new file here. So we'll take we'll go here and make a new file and call it key JSON. And the format is pretty darn simple. So you just specify the actual key. So this is the, the schema key that we need to provide here to get the record. So I'll go here, and it's actual values. So we have it is registry is one. And the other one is ship class. And what we'll do is we'll look through our data here and just find the record. I'm going to choose AB let's see here, if there's any ships that I like, how about the Okinawa, that sounds cool. So we'll grab the Oakland, ah, I will try to get this record here. So we'll grab it as Excelsior class here, we'll go back. And we will paste that in there. And then we need the registry number, which we'll paste in here as well. So now we have our key, and we'll go back to our scratchpad here. And this Yep, that should work. So I'm just going to copy that, paste that in there. And there's our record. So it returns all the fields by default, I think you could limit the fields that you want returned, probably with projection expression, just gonna take a look at that quickly here, a string that identifies one or more attributes retrieved from the table, These can include scalar, etc. And if nothing is specified, then it will turn everything. So just remember, projection expression, that's what it's used for. And if we want to look at this in a different format, let's look at a table. Sometimes it looks nicer when it's under the format. No, that looks terrible. We'll look at as text, that's a little bit better. So you know, just consider, you can play around with those values there. But that is for the guide on the next we'll look at how to get things back. So we'll go back to our documentation here and click back to dynamodb. And let's look at how we get multiple items. And so we have basket item request items. So it's not key, it's something else here. And if we go to example, you can see it's as simple as this. So I'm gonna just copy this here, and this command, we're gonna make our way back here. And we're going to do batch get item, I'm gonna paste that in there, we're gonna make it a single line that makes a little bit nicer. And we're going to create this request items file that it's suggesting here. We'll make a new file there, paste that in. And what we're going to do, I think it's very similar to this, except you can do multiples of it. So we'll go over here and take a look. So here, you provide the table name, so we don't have to provide the table here. And then you provide the keys. So it's pretty darn similar. We'll look at the actual, yeah, so you don't provide the table name. So we'll look at the actual example here, because this might be a little bit more pared down here. No, it's the same. And you even provide the projection expression here if you want to filter out the fields that you want, but I want all the fields on greedy like that. So I'm just going to paste this in below here. Oops, actually, we'll go here, paste it in there. And we're just going to grab this one to save us some time because we'll just get the same record. And I'm going to paste that in as one of the keys. And I really don't like for spaces, but I'm not going to play around with this. So I'm just going to type that in here. I'm just hitting tab to do that. I really hate for spaces, this is too much. It's excessive, and I only want two records here. So I'll take out this one here. We don't need to project any expressions, we just want everything and we will get a second ship. So I will go back here to the data. We will look for a another cool ship. Something that sounds cool USS reliance sounds pretty darn cool. And so I'll grab the class, which is the Miranda. So we'll do class down here. And I'll have to replace these of course. And we also need the registry number. paste that in there. And So Oh, and we also need the name of the ship, which is, or the table, sorry, which is called starships. And so that should do it. So let's make our way back to our scratchpad here. And copy. Whoops, that's not it, I'm going to copy this down below. I will paste that in there. And that should give us the record. So it gave us back two records with all their information. Let's take a look what this looks like as a table. Terrible can't even read it. So let's look at this as a as text. Yeah, that's a bit better. So again, you know, just play around with that stuff. So that is batched get item. So now the next thing I'm going to do is show you how to like write items to the database. But we don't have any new records. So I'm just going to go ahead and delete one and then we're going to re add that back to the database. So let's take a look at delete. So I think it's called delete item. And we'll make our way over to the CLR. Here, we'll scroll the top go to Dynamo dB. And we will look for delete there is delete item. And we have delete item table name key, we'll just go look at the example here. Yep, and it's as simple as that. So it looks just like get item but it's delete item. So we'll go here. And I'm just going to go to the top here and look for starships. Scroll that back down their table name. And we can just use the same key that we already have, which is just going to delete this record here. And what we'll do before we do that, let's just make sure the data exists. So this is NCC, 139 Excelsior. So if we go here, I wonder if we can quickly find it here. I'm just going to copy the name, how you just do Command F here and see if it shows up. Not seen it. I'm just gonna cheat here I want to show you querying later, but I'm gonna just show it to you now. And we'll just go grab that name here. So we'll type in the ship name Excelsior or ship placer. I'm gonna hit start search. And also we will go get the registry number. paste that in there, hit start. And so there's the record. So this is the record, we want to go delete. We'll go back here. And we will go to our scratchpad, grab this here. And that should delete that record there. We'll hit enter. We didn't get any output back. But that doesn't mean it didn't work. That means it did work. And we will go ahead and hit start. And now that record is gone. So that record is 100% gone. And we want to bring it on back. So that's what we're going to do next. So in order to bring back that item, we're going to need to do a put. And so we'll make our way over back to the documentation here, go to the top, search for put there it is. And we have a put item, we specify the table, then we need to specify the actual item. And we have a bunch of other options. But we'll go down to examples here. And you can see that we have it was dynamodb, put item table name and the actual item itself. And then we also have returned consumed capacity, I don't know if we need that, let's just take a look and read about it determines the level of detail about provision throughput consumption that is returned. So this is just going to give us additional information about actual consumed capacity, I think that's a good idea, we could check out what kind of capacity is being used up. So we'll go back to the top here, go to examples. But this is obviously optional. And we're going to copy this command here. And we'll paste this here. And then I'm just gonna make this a single line to make our lives a little bit easier. So it's a little bit long, but it's not that bad. And we will make a new file called item. And we need to grab the name of the table again starships. And we need to fill out that item file. So we will open that up. And we will take a look at what that would look like. So that's an example of the file. So let's grab this example here. And what we'll do is we need to go get this record again. So we deleted What was the name of the file and we'll go back to the key here is Excelsior type open ally zoom. It's this record here. So I'm just going to grab this data here. Grab the whole row, and we will go back to the item file here. Paste it on down here. And then we just need to name all these will grab the names from here. Just gonna do a refresh. So we have shipped class. We have registry, oops, then we have description. I really do it in any particular order. And then we have named the order doesn't matter, as long as you get all the information. And so this is the name of the ship. This is the registry number. This is the ship class. And then this is the description. We will save that. And so that should be what we need to get this to work. So I'm going to copy this file here. We're going to paste it in on there. And we have an error hyphen item expecting a comma delimiter. So maybe we made a minor mistake in our item file here. Yeah, we don't have a comma on the end here. And we'll go back, hit up, hit Enter. And there you go. And it told us how many capacity units consumed, which was one right capacity unit. So I mean, that can be good if you need to do something programmatic and make sure you're not over consuming. Let's go see if that record now exists in the table. So we'll just do a query here. We'll go back to here. Actually, we could just use a get item to do that. That'd be probably smarter. So I'm just going to copy this here. paste that in. Yep, so the record is back. So now that we know how to put an item, let's talk about how we would actually go ahead and update an item next. So let's see how we can update an item in dynamodb. So we're gonna type in update item. Of course, I think you could just add it from here, right, but that's not we're gonna do we're gonna use the COI, of course. And we'll make our way over to the we'll fix this file first here, refresh. Back into there. We'll go back to the COI here, it goes to the top Dynamo dB, and will look for put item. And so put item here we'll scroll down, we have put item table name, you provide the item and we have a bunch of other values. I'm certain This one is not a simple one, if we go to the example here. Now it's not that bad. It's just Oh, sorry, we're not, we're not creating an item, we're going to update an item I'm sorry, we've already done put item, we want to update an item. And so we have update item table name key. And then we have a bunch of other stuff. Let's go look at the example. And here you can see it's a lot more complicated. I don't know why it's such a pain. But this is what it is. So I'm going to grab that on here. And we will paste our example here. And this is big, rename this to starships. We'll leave this as key I will update the existing file we have and we'll talk about these values here. So the first one is update expression. So let's take a look at here update expression, an expression that defines one or more attributes to be updated, the action can be performed on them and the values for them. So that is what we're actually going to update. And if we look down below, it should give us examples on how we can write these expressions. I'm not a lot of information here. But I know what we need to do here. So we'll go here. And we can see that we have this set value here. The next one is the expression expression attributes names. One or more substitution tokens for attribute names and expression The following are some use cases. And so it could be one of those reserved words or placeholder or special characters. So this is just like a remapping of, of characters. This doesn't make sense, don't worry, we'll once we work through this example. One or more values that can be substituted in an expression. Okay, so you know, I don't feel like it's the best explanation But well, it makes sense when we go through it. So what we'll do is we're going to need a expressions attributes name as well extracted expressions, attributes valued a file, we already have this file here. So I'm just gonna copy these names here. Make your way back to cloud nine. And I'm going to make this new file in our playground. So that's one and then we'll go back here and we will grab the other one. That's too. And so for names, what it suggests is this. And then for this one here, we have the values. So the way this works is it allows you to remap values. So We just go back to this example here. Notice that in the names, or sorry, the values is the values that we want to update. So here, we're saying, this number should not be 2015. This string should now be louder than ever. And then we have this colon y and colon t. And this colon colon t is just like a representation of that actual character than in the update expression. What it's doing, it's reassigning it saying, so make colon y into pound y. And then if we look at our expression names, pound y equals zero, pound 80 equals album attribute. It seems like a lot of roundabout, it does feel that way. But it's allow you to get flexibility. So you don't run into issues like with reserved words or special characters. And we could pare this down. But let's actually try to do this in full with the example that they have here. So make your way back here. The first thing I'm going to do is go back to our scratchpad, I'm going to change the set value, we only need, let's only change the description and make our lives a little bit easier here, call this colon D. And we'll save that there. And the next thing we'll do is go back to our expressions, and I'm going to call this hash, hash D, that's gonna be a description, whoops, can't see what I'm doing description. And then we will just take out the second one here, we'll make our way over to values, I'm gonna change this to a colon D. And this is going to be a string, I'm going to get rid of the second value, what we're going to do is just shorten to the, the the item. So we'll go back to item. And so this is the string here. And I'm just going to change it here. So it says ship commanded by Admiral James laden on which Benjamin Sisko served, I'm just gonna take off the end here. Or maybe I'll just serve as so that means the same thing as first officer. Okay, and what we'll do is we'll go back to our scratchpad, and we will copy all the stuff. And it also has this return value, since let's take a look at that before we move on. Use return values if you want to get the item attributes as they appear before or after they are updated. So none is if returns values if not specified, all old. So return all the attributes of the item, return only the update attributes, return all the new attributes of the items, etc. And this kind of stuff is kind of important when we get into dynamodb streams, because you get you can do the old or the new. In this case, I guess we're doing all and all seems fine to me. This is an example. That's all we're doing here. And what we'll do is we'll just copy this here, fingers crossed, this works first time. And we've got data back and it says x Oh. So that looks great to me. And what we'll do is we'll just make sure that it has been changed, we'll use our get item up here. And it has been updated. So there you go, that is update item. So now that we've learned how to get put, delete, let's look at the scan and query options next. So let's first look at scan because scan is a lot, a lot easier to understand. There's a lot less going on there. So if we go over here, we can see we have a query and scan option. Now if you created a table without a source key, you'd only be able to scan. But we'll go to scan here and we'll look at what options we have. So the way scan works is that it returns all the records. And then you can apply filters after it's returned those records and then filter out what you want. So if you have a very large table 1000s upon 1000s of records, that's not very efficient. So generally, you always want to use query when you can. But if you don't have a need to have a sore key, then you could have a table that just has scan. But anyway, if we scan, we hit start, it returns everything. And if we want to filter stuff out, we could choose anything we want here, I could say ship class will we'll say Luna we'll hit start search. And we have some other options here like be begin with contains with all these kind of options here that we can use to filter. But just understand that even though we filter this out, it's already returned all 100 Records and then it filters them out and returns it. So we're using up a lot of capacity when we're doing that. But let's look at how we can do this via the COI. So we'll go back to the top here. And we will look for a scan. I feel like scan would have a lot of options. Oh yeah, there's tons of options, tons of options. We can segment stuff, we can do page size, all sorts of things. But let's go down and look at an example here and this is an example that we are given. It looks a lot similar to the Item one here. And we're just going to do something simple. So we'll do starships. And I'm thinking, I'm thinking for this, what we'll do is we will filter out the description for something. So I'm going to go back here to the front here. Let's say we want to get this record or something that or something that starts with science. Maybe something more common would be nice, maybe destroyed. So we'll look for all the records that start with destroyed with the description. So I'll go back here. And what I'm going to do is I'm going to think about this for a second. For the scan. I don't need to project anything, these are records that we get back remember projection is if we just want to cherry pick what we want. For the Filter Expression, we're just going to do a begins with. So I'm going to do begins with and for this, it's going to be description. I'm just gonna do colon D here. And then down below, we need to specify our names. Actually, we don't need any names because we're not remapping anything. Remember, we did that before we did this remapping that's just extremely verbose. We don't need to do that. I'm just going to do curlies. Here. I'm going to do quotations colon de Colon curlies, and we're going to make it a string, and then I just want it to return destroyed. So I just want you to notice that we didn't have to extract this out as a JSON file, we can write them in line like this. And it makes things a lot easier for us. But you're probably wondering what this begins with thing is, so let's go take a look back at scan. And specifically look at these filter expressions. So if we just hit enter there, if we click on to this tab here, I think it'll give us more information about it. So down below, it shows you you can do equals these symbols. And then we have begins with this maps, all with this kind of stuff here. But so begins with in order to use it, we have to write that and then a then substitute whatever the thing is. And that's what we've done, we've said, We want the column description replaced with colon D, and colon D, is going to be destroyed. So anything that begins with destroyed so let's give this a go and see if it works. We'll enter expecting a property and close with double quotation. So I quickly made a mistake here, it's a little bit hard to see. It's because I'm using doubles on the outside, I'm gonna use singles on the outside, we're just having a conflict of quotations. And there we go, we got a lot of records back, this is a little bit of a mess to look at, I'm gonna go and try output. I don't know why I'm gonna try Table No, no good. You think they'd make that actually look like a table, we'll go to text that's a little bit easier to see. And we can see we're getting all the records back that are destroyed. So that is for scan. Now let's take a look at how to use query. Okay, so we'll take a look here now at query. So if we make our way over to the, our actual table here where we can visualize it, we'll go over to query and with query, you have to specify the partition. I think a third key is optional. Let's take a look. So if I just wanted Luna, I'm just going to click off the filter here, that's going to give us that and then if we want to narrow it down further, I can grab the registry here. And we'll hit equals, the difference here is that it's only returning these records, it's not returning all 1000 or 100 Records. And then you can apply filters after the fact. So I could even filter this further and just say, a description. And if the string begins with the, it's obviously going to do that. And then you have projected columns here, well, let's start, um, can only be used when creating with an index name, okay? I don't know. But anyway, the point is, is that that you need to know is that you have a partition key. And you can optionally add that or so you have to add that and then you have an optional third key. And these are your options. And then you can again, filter further and you can even specify these if you want again, that'd be a bit silly to do. But let's learn how to do it query using the CLR. So we'll go down here, type the word query, and we will make our way over to the documentation and type in query. And query probably has a lot of options. Holy smokes, look at all those options. We're not going to go through all through all of them. And go down to the examples and this is the example that it gives us so we have Dynamo DB query table name. projection expression. And then we have one called key condition expression, this one is different. And also this expression attributes value we had that before. But if we go here and just take a look at this, what this does the condition that specifies the key values for items being retrieved by reaction. So what this is saying is, you're pretty much just providing these values here. So this part right here, see this. That's what we're doing with that attribute there. So we'll just go grab our example here. save ourselves some trouble. Make your way back here. And we're going to specify starships. And in this case, we need to, we're not going to project any values by I'm just gonna cut that out, because I should show you that at least once how to do that. And for this, we're going to do ship class. And it's just gonna equal coal and C. And I'm just going to do this in line because I just want to make this super easy for us. We'll do curlies or single quotations, we learned our lesson Last time, we should make these symbols on the outside. And we'll do doubles. Colon C. Colon curlies, doubles, S for string colon, and let's filter out for galaxy. I know that's the type of starship and Star Trek. So if we do this, we copy that there, enter, we get some value. So there you go. That's all the the galaxy ones, we can go up output and do text here make our lives a bit easier, it's a bit easier to read. Let's say we just wanted to get the registry number. paste that in there. And I'm just going to change this to output text. Oops, did not like that need a backslash on the end there. Holy smokes. Output its output. Oh, it doesn't like the projection expression that I put in there. I'm pretty certain that we can do that. Good example here, this, this one has one in it. Oh, you know what, because I named it all caps. It has to be the name of what the actual field is. You can't just put all caps. By mistake. try this again. Mmm hmm. Maybe I should go a little bit slower here. So I'm just going to sometimes when you have these issues with this, a smart thing to do is just make it a single line was backslashes can be tricky sometimes. There we go. So I don't know I had some kind of error in here syntax. Maybe I didn't have a backslash there. I didn't have a trailing space after it. It was messing up. But that's always a trick that you can use to fix your queries. And I'm just going to hit enter there and just clean this up a little bit. Not that not that we need to do anything else here. But let's just make sure that it works this way. I try this one more time. There you go. So yeah, that is the query. Maybe we just take a look here and look if there's any other important fields that we glazed over. Scan index forward is a great way of flipping things the other way. So if you did scan index for it, we'll just play around with some of the settings, why not? So look at the order that it's in right now. So the D is first. I put that in there and I say false. I think it's all caps for this. Enter. False. I should really just read it, but I'm pretty sure now I have to read it. That's pretty sure that it's just like false or true. I need an example. I can't tell. Oh, it's sorry. It's just scan for it index or no scan and forward index, you don't actually provided a value. So if I paste that in there, I think that's all we have to do. Yep, so there it's in the opposite order. And so if you want it to be the default there, you could do it the other way. Right. Let's look at this any other values of interest here? We looked at projection there. No, I'd say that's pretty good. So now we are all done here. For this, I think we'll just take a look at the transact COI stuff. Next here, we're not going to do them because they're a little bit more work to set up. But it's good to know how they work. Let's just take a quick look here at transactional transactional API calls for dynamodb, we have the get in the right, we're not going to set these up, it's a lot of work to do. So it's just good to conceptually know what you can do here and just to see how it's specified, so transact get items, synchronous operations that automatically retrieves multiple items from one or more tables. That's the key thing, one or more tables. And the idea is that if any of these things happen, then the entire transaction will roll back. This is really good when you have sensitive information that that could be committed that relies on multiple things same with gets and writes. But if we go to examples down below here, we can see that its database, dynamodb, transact get items, and then you provide the items that you want. So in here, it looks like it's actually the same table. And it's saying that both we want both of these records. And if there's a failure on one or the other, when grabbing this information, it's going to roll back. So it has to get all the information or none of that. Let's go and take a look at the rights. This is very similar This is when you have a right condition. And there is a lot of stuff going on here, you probably want to read through all this is probably a good idea, a condition and one or more expression is not met and ongoing operation of the process updating the same time. This one's a very important one where you have to update at the same time there is insufficient provision capacity, an item size becomes too large, their grid size of items, exceeds four megabytes, there is a user error, we're going to go up here at the top look at the example. And again, it's very similar. And you could specify different tables here. So this is really good for cross table rights. Um, but you can see there's a lot of information here. So just be aware of transact, write items and get items and just consider that everything must be successful in order for them for it to do the guts of the rights. And it's cross table. So we are almost done. Well, we only need to learn one more COI command and this is to clean up the entire project. And that is the Delete table command. We're going to go ahead and delete our table. Now we could just go in here, and hit Delete table. But that's not fun. Let's use the COI. So we're gonna go delete table. And we'll look at the example. I think it's just as simple as that. Yeah, it's that simple. If you delete through here would ask you to like maybe you might want to back it up. We do not want to back this table up, we want it gone. And I'm gonna type it in starships here. And we'll hit enter there. And now you can see that is deleting if we want to get an update, we can go over here and see that it is deleting might already be gone. Hope it's already gone. And if it was taking time to delete, we could use the describe command to give it a look here. And we can see that nothing is being returned because there is no longer a table. So there you go, that is the Dynamo DB run through. Um, if you have more time on your own, you might want to try to set up a dynamodb stream. And those are kind of interesting to see I'm just gonna type in starships again here. You don't you don't have to do any of this, I'm just going through it just so I can show you dynamodb streams here, ship class registry. So dynamodb streams, once this table creates, we'll give it a second. If you want to set those up, it's under under triggers, you create a trigger, you'd create a existing function. And the idea here is, whenever new records come into the database, it will call that that lambda function. And then from that lambda function, you can then send it to wherever you want. Generally, you'd send it to kinesis firehose, or some kinesis service. But that's just the way to react to data in here. But yeah, this is outside the scope of the developer associate. It's kind of a stretch goal. If you want a little personal project to do, but our table is deleted, I'm just gonna go delete it here. And you can see that it says create a backup before deleting, we're not going to do that. Um, but yeah, that's dynamodb there. So there you go. Hey, this is Andrew Brown from exam Pro. And we are looking at elastic Compute Cloud EC two, which is a cloud computing service. So choose your storage memory, network throughput, and then launch and SSH into your server within minutes. Alright, so we're on to the introduction to EC two. And so EC T was a highly configurable server. It's a resizeable compute capacity. It takes minutes to launch new instances, and anything and everything I need is used as easy to instance underneath. So whether it's RDS or our ECS ORS Simple System Manager. I highly, highly believe that at AWS, they're all using EC two. Okay. And so we said they're highly configurable. So what are some of the options we have here? Well, we get to choose an Amazon machine image, which is going to have our LS. So whether you want redhead, Ubuntu windows Amazon links or Susi, then you choose your instance type. And so this is going to tell you like how much memory you want versus CPU. And here, you can see that you can have very large instances. So here is one server that costs $5 a month, and here you have one that's $1,000 a month. And this one has 36 CPUs and 16 gigabytes of memory with 10 gigabyte performance, okay, then you add your storage, so you could add EBS, or Fs and we have different volume types we can attach. And then you can configure your your instance. So you can secure it and get your key pairs, you can have user data, Iam roles and placement groups, which we're all going to talk about starting now. Alright, so we're gonna look at instance types and what their usage would be. So generally, when you launch an EC two instance, it's almost always going to be in the T two, or the T three family. And yes, we have all these little acronyms which represent different types of instance types. So we have these more broad categories. And then we have subcategories, or families of instances that are specialized. Okay, so starting with general purpose, it's a balance of compute and compute memory and networking resources. They're very good for web servers and code repository. So you're going to be very familiar with this level here, then you have compute optimized instances. So these are ideal for compute bound applications that benefit from high performance processors. And as the name suggests, this is compute it's going to have more computing power. Okay, so scientific modeling, dedicated gaming servers, and ad server engines. And notice they all start with C. So that makes it a little bit easier to remember, then you have memory optimized, and as the name implies, it's going to have more memory on the server. So fast performance for workloads that process large data sets in memory. So use cases in memory caches in memory databases, real time big data analytics, then you have accelerated optimized instances, these are utilizing hardware accelerators, or co processors, they're going to be good for machine learning, computational finance, seismic analysis, be quick recognition, really cool. Future tech uses a lot of accelerated optimized instances. And then you have storage optimized. So this is for high sequential reads and write access to very large data sets on local storage. your use cases might be a no SQL database in memory or transactional databases or data warehousing. So how is it important? How important is it to know all these families, it's not so important to associate track at the professional track, you will need to know themselves, all you need to know are these general categories and what and just kind of remember, which, which fits into where and just their general purposes. Alright. So in each family of EC two instance types, so here we have the T two, we're going to have different sizes, and so we can see small, medium, large x large, I just wanted to point out that generally, the way the sizing works is you're gonna always get double of whatever the previous one was. Generally, I say generally, because it does vary. But the price is almost always double. Okay, so from small to medium, you can see the ram has doubled, the CPU has doubled for medium large, isn't exactly doubled. But for a year, the CPU has doubled. Okay, but the price definitely definitely has doubled, almost nearly so it's almost always twice inside. So general rule is if you're wondering when you should upgrade, if you need to have something, then you're better off just going to the next version. So we're gonna look at the concept called incidence profile. And this is how your EC two instances get permissions, okay? So instead of embedding your AWS credentials, your access key and secret in your code, so your instance has permissions to access certain services, you can attach a role to an instance via an instance profile. Okay? So the concept here is you have to do instance, and you have an instance profile, and that's just the container for a role, and then you have the role that actually has the permissions. Alright. And so I do need to point out that whenever you have the chance to not embed ABS credentials, you should never embed them. Okay, that's like a hard rule with AWS. And anytime you see an exam question on that, definitely Always remember that the way you set an instance profile tuneecu instance, if you're using the wizard, you're going to see the IM role here. And so you're going to choose you're going to create one and then attach it. But there's one thing that people don't see is they don't see that instance profile because it's kind of like this invisible step. So if you're using In the console, it's actually going to create it for you. If you're doing this programmatically through cloud formation, you'd actually have to create an instance profile. So sometimes people don't realize that this thing exists. Okay. We're gonna take a look here at placement groups, the placement groups let you choose the logical placement of your instances to optimize for communication performance, or durability. And placement groups are absolutely free. And they're optional, you do not have to launch your EC two instance in within a placement group. But you do get some benefits based on your use case. So let's first look at cluster. So cluster packs instances close together inside an AZ and they're good for low latency network performance for tightly coupled node to node communication. So when you want servers to be really close together, so communication super fast, and they're well suited for high performance computing HPC applications, but clusters cannot be multi az, alright, then you have partitions. And so partitions spread instances across logical partitions. Each partition does not share the underlying hardware. So they're actually running on individual racks here for each partition. They're well suited for large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka, because these technologies use partitions. And now we have physical partitions. So that makes total sense there, then you have spread. And so spread is when each instance is placed on a different rack. And so when you have critical instances that should be kept separate from each other. And this is the case where you use this. And you can spread a max of seven instances and spreads can be multi az, okay, whereas clusters are not allowed to go multi AZ. So there you go. So user data is a script, which will automatically run when launching easy to instance. And this is really useful when you want to install packages or apply updates or anything you'd like before the launch of a an instance. And so when you're going through the easy to wizard, there's this advanced details, step where you can provide your bash script here to do whatever you'd like. So here I have it installing Apache, and then it starts that server, if you were logging into any CPU instance, and you didn't really know whether user data script was performed on that instance, on launch, you could actually use the this URL at 169 24 116 24. If you were to curl that within that easy to instance, with user data, it would actually return whatever script was run. So that's just good to know. But yeah, user data scripts are very useful. And I think you will be using one. So metadata is additional information about your EC two instance, which you can get at runtime. Okay. So if you were to SSH into your EC two instance, and run this curl command with latest metadata on the end, you're going to get all this information here. And so the idea is that you could get information such as the current public IP address, or the app ID that was used to launch the students, or maybe the instance type. And so the idea here is that by being able to do this programmatically, you could use a bash script, you could do something with user data metadata to perform all sorts of advanced Eva staging operations. So yeah, better data is quite useful and great for debugging. So yeah. So it's time to look at the easy to cheat sheet here. So let's jump into it. So elastic Compute Cloud, easy to use is a cloud computing service. So you configure your EC to by choosing your elastic storage, memory and network throughput, and other options as well. Then you launch an SSH into your server within minutes. ec two comes in a variety of instance types specialized for different roles. So we have general purpose, that's for balance of compute memory and network resources, you have compute optimized, as the name implies, you can get more computing power here. So I deal for compute bound applications that benefit from high performance processors, then you have memory optimized. So that's fast performance for workloads that process large data sets in memory, then you have accelerated optimized that's hardware accelerators or co processors, then you have storage optimized that's high sequential read and write access to very large datasets on local storage. Then you have the concept of instant sizes. And so instance sizes generally double in price and key attributes. So if you're ever wondering when it's time to upgrade, just think when you're need double of what you need that time to upgrade. Then you have placement groups, and they let you choose the logical placement of your instances to optimize communication performance, durability, and placement groups are free, it's not so important to remember the types are because I don't think we'll come up with a solution architect associate. And then we have user data. So a script that will be automatically run when launching EC two instance, for metadata. Metadata is about the current instance. So you could access this metadata via a local endpoint when SSH into an easy to instance. So you have this curl command here with metadata and meta data could be the instance type, current IP address, etc, etc. And then the last thing is instance profile. This is a container for an IM role that you can use to pass roll information to an easy to instance, when the instance starts. Alright, so there you go, that's easy to do. Hey, this is Andrew Brown from exam Pro. And we are looking at Virtual Private Cloud known as VPC. And this service allows you to provision logically isolated sections of your database cloud where you can launch eight of his resources in a virtual network that you define. So here we are looking at an architectural diagram of a VPC with multiple networking resources or components within it. And I just want to emphasize how important it is to learn VPC and all components inside and out because it's for every single aidable certification with the exception of the cloud practitioner, so we definitely need to master all these things. So the easiest way to remember what a VPC is for is think of it as your own personal data center, it gives you complete control over your virtual networking environment. Alright, so the idea is that we have internet, it flows into an internet gateway, it goes to a router, the router goes to a route table, the route table passes through knakal. And the knakal sends the traffic to the public and private subnets. And your resources could be contained within a security group all within a VPC. So there's a lot of moving parts. And these are not even all the components. And there's definitely a bunch of different configurations we can look at. So looking at the core components, these are the ones that we're going to learn in depth, and there are a few more than these, but these are the most important ones. So we're gonna learn what an internet gateway is. We're gonna learn what a virtual private gateway is route tables, knackles, security groups, public and private subnets, Nat gateway and instances customer gateway VPC endpoints and VPC peering. So, this section is very overwhelming. But you know, once you get it down, it's it's pretty easy going forward. So we just need to master all these things and commit them to memory. So now that we kind of have an idea, what's the purpose of VPC, let's look at some of its key features, limitations and some other little things we want to talk about. So here on the right hand side, this is the form to create a VPC, it's literally four fields. It's that simple. You name it, you give it an address, you can also give it an additional ipv6 address. You can't be or it's either this and this. And you can set its tendencies to default or dedicated, dedicated, meaning that it's running on dedicated hardware. If you're an enterprise, you might care about that. This is what the ipv6 cider block would look like because you don't enter it in Amazon generates one for you. So v PCs are region specific. They do not span regions, you can create up to five v PCs per region. Every region comes with a default VPC, you can have 200 subnets per v PC, that's a lot of subnets. You can create, as we said here, an ipv4 cider block, you actually have to create one it's a requirement. And in addition to you can provide an ipv6 cider block. It's good to know that when you create a VPC, it doesn't cost you anything. That goes the same for route tables, knackles, internet gateway security groups subnets and VPC peering. However, there are resources within the VPC that are going to cost you money such as Nat gateways, VPC endpoints, VPN gateways, customer gateways, but most of the time, you'll be working with the ones that don't cost any money so that there shouldn't be too much of a concern of getting over billed. One thing I do want to point out is that when you do create a VPC, it doesn't have DNS host names turned on by default. If you're wondering what that option is for what it does is when you launch easy two instances, and so here down below, I have an easy to instance, and it will get a public IP, but it will only get a public DNS, which looks like a domain name like an address. And that's literally what it is. But if this isn't turned on that easy to instance, won't get one. So if you're wondering, why isn't that there, it's probably because your host names are disabled and they are disabled by default. You just got to turn that off. So we were saying earlier that you get a default VPC for every single region. And the idea behind that is so that you can immediately launch EC two instances without having to really think about all the networking stuff you have to set up. But for a VA certification, we do need to know what is going on. And it's not just a default VPC It comes with other things and with specific configurations and we definitely need to know that for the exams. So the first thing is it creates a VPC of cider block size 16. We're going to also get default subnets with it. So for every single AZ in that region, we're going to get a subnet per AZ and they're going to be a cider block size 20. It's going to create an internet gateway and connect it to your default VPC. So that means that our students is going to reach the internet, it's going to come with a default security group and associated with your default VPC. So if you launch an EC two instance, it will automatically or default to the security group unless you override it. It will also come with by by default, a knakal. And associated with your VPC, it will also default DHCP options. One thing that it's implied is that you It comes with a main route table, okay, so when you create a VPC, it automatically comes to the main route table. So I would assume that that comes by default as well. So there are all the default. So I just wanted to touch on this 0.0 dot zero forward slash zero here, which is also known as default. And what it is, is it represents all possible IP addresses. Okay. And so you know, when you're doing a device networking, you're going to be using this to get the GW to have a route like routing traffic to the GW to the internet. When you're using a security group, when you set up your inbound rules, you're going to set 0.0 dot 0.0 to allow any traffic from the internet to access your public resources. So anytime you see this, just think of it as giving access from anywhere or the internet. Okay. We're looking at VPC peering, which allows you to connect one VPC to another over direct network route using private IP addresses. So the idea is we have VPC, a VPC Zb. And we want to treat it so like the they behave like they're on the same network. And that's what VPC peering connection allows us to do. So it's very simple to create a peering connection, we just give it a name, we say V, what we want is the requester. So that could be VP ca and then we want as the acceptor which could be VP CB, and we can say whether it's in my account, or another account, or this region or another region. So you can see that allows pvcs from same or different regions to talk to each other. There is some limitations around the configuration. So you know, when you're peering, you're using star configuration, so you'd have one central VPC and then you might have four around it. And so for each one, you're going to have to have a peering connection. There's no transitive peering. So what does that mean? Well, the idea is like, let's say VPC c wants to talk to VPC, B, the traffic's not going to flow through a, you actually would have to create another direct connection from C to B. So it's only to the nearest neighbor, where that communication is going to happen. And you can't have overlapping cider blocks. So if these had the same cider block, this was 172 31. This was 172 31, we're gonna have a conflict and we're not gonna be able to talk to each other. So that is the VPC peering in a nutshell. Alright, so we're taking a look here at route tables. The route tables are used to determine where network traffic is directed, okay. And so each subnet in your V PC must be associated with a route table. And a subnet can only be associated with one route table at a time, but you can associate multiple subnets subnets with the same route table. Alright, so now down below, I have just like the most common example of where you're using route tables. And that's just allowing your easy two instances to gain access to the internet. So you'd have a public subnet where that easy to instance resides and that's going to be associated with a route table that Route Route table is going to have us routes in here. And here you can see we have a route which has the internet gateway attached that allows access to the internet. Okay, so there you go. That's all there is to route two. We're taking a look at Internet gateway internet gateway allows your VPC access to the internet and I N GW does two things. It provides a target in your VPC roundtables for internet routable traffic. And it can also perform network address translation Nat which we'll get into in another section for instances that have been assigned a public ipv4 address. Okay, so down below here, I have a representation of how I GW works. So the idea is that we have internet over here and to access the internet, we need an internet gateway, but to route traffic from our EC to instances or anything. They're gonna have to pass through a route table to get to our router. And so we need to create a new route in our route table for the GW so I GW hyphen, Id identifies that resource, and then we're going to give it 0.0 point zero point Zero as the destination. Alright, so that's all there is to it. So we talked about how we could use Nat gateways or Nat instances to gain access to the internet for our EC two instances that live in a private subnet. But let's say you wanted to SSH into that easy to essence, well, it's in a private subnet, so it doesn't have a public IP address. So what you need is you need an intermediate EC two instance that you're going to SSH into. And then you're going to jump from that box to this one, okay? And that's why bastions are also known as jump boxes. And this institute instance for the bastion is hardened. So it should be very, very secure, because this is going to be your point of entry into your private EC two instances. And some people might always ask, Well, if a NAT instance, like Nat gateways, we can't obviously turn into bastions, but in that instance, is just any situation it's Couldn't you have a double as a bastion, and the possibility of it is possible. But generally the way you configure NATS and also, from a security perspective, you'd never ever want to do that, you'd always want to have a different EC two instance, as your Bastion. Now, there is a service called SYSTEMS MANAGER, session manager and it replaces the need for bashes so that you don't have to launch your own EC two instances. So generally, that's recommended in AWS. But you know, bastions are still being commonly used throughout a lot of companies because it needs to meet whatever their requirements are, and they're just comfortable with themselves. There you go. So we're gonna take a look at Direct Connect, and Direct Connect is in aid of a solution for establishing dedicated network connections from on premise locations to AWS, it's extremely fast. And so depending on what configuration you get, if it's in the lower bandwidth, we're looking between 1550 megabytes to 500 megabytes, or the higher bandwidth is one gigabytes to 10 gigabytes. So the transfer rate to your on premise environment, the network to AWS, is it considerably fast. And this is can be really important if you are an enterprise and you want to keep the same level of performance that you're used to. So yeah, the takeaway here with Direct Connect is that it helps reduce network costs increase bandwidth throughput, provides a more consistent network experience than a typical internet internet based connection. Okay, so that's all. Hey, this is Andrew Brown from exam Pro. And we are looking at auto scaling groups. So auto scaling groups lets you set scaling rules, which will automatically launch additional EC two instances or shutdown instances to meet the current demand. So here's our introduction to auto scaling groups. So auto scaling groups, abbreviated as G contains a collection of EC two instances that are treated as a group for the purpose of automatic scaling and management. And automatic scaling can occur via capacity settings, health check replacements, or scaling policies, which is going to be a huge topic. So the simplest way to use auto scaling groups is just to work with the capacity settings with nothing else set. And so we have desired capacity, Min, and Max. Okay, so let's talk through these three settings. So for min is how many easy two instances should at least be running, okay, Max is the number of easy two instances allowed to be running and desired capacity is how many easy two instances you ideally want to run. So when min is set to one, and let's say you had a new auto scaling group, and you lost it, and there was nothing running, it would always it would spin up one. And if that server died, for whatever reason, because when it was unhealthy or just crashed, for whatever reason, it's always going to spin up at least one. And then you have that upper cap, where it can never go beyond two, because auto scaling groups could trigger more instances. And this is like a safety net to make sure that you know, you just don't have lots and lots of servers running. And desired capacity is what you ideally want to run. So as you will try to get it to be that value. But there's no guarantee that it will always be that value. So that's capacity. So another way that auto scaling can occur with an auto scaling group is through health checks. And down here, we actually have two types, we have EC two and lb. So we're gonna look at EC two first. So the idea here is that when this is set, it's going to check the EC two instance to see if it's healthy. And that's dependent on these two checks that's always performed on DC two instances. And so if any of them fail, it's going to be considered unhealthy. And the auto scaling group is going to kill that EC two instance. And if you have your minimum capacity set to one, it's going to then spin up a new EC two instance. So that's the that's the To type now let's go look at the lb type. So for the lb type, the health check is performed based on an E lb health check. And the lb can perform a health check by pinging an ENT, like an endpoint on that server could be HTTP or HTTPS, and it expects a response. And you can say I want a 200. Back at this specific endpoint, or so here. That's actually what we do. So if you have a web app, you might make a HTML page called health check. And it should return 200. And if it is, then it's considered healthy. If that fails, then the auto scaling group will kill that EC two instance. And again, if your minimum is set to one is going to spin up AI, healthy new EC two instance. final and most important way scaling gets triggered within an auto scaling group is scaling policies. And there's three different types of scaling policies. And we'll start with target tracking scaling policy. And what that does is it maintains a specific metric and a target value. What does that mean? Well, down here, we can choose a metric type. And so we'd say average CPU utilization. And if it were to exceed our target value, and we'd set our target value to 75%. Here, then we could tell it to add another server, okay, whenever we're adding stuff, that means we're scaling out whenever we are removing instances, we're moving servers, then we're scaling in okay. The second type of scaling policy is simple scaling policy. And this scales when alarm is breached. So we create whatever alarm we want. And we would choose it here. And we can tell it to scale out by adding instances, or scale in by removing instances. Now, this scaling policy is no longer recommended, because it's a legacy policy. And now we have a new policy that is similar but more robust. To replace this one, you could still use it. But you know, it's not recommended. And it's still in the console. But let's look at the one that replaces it called scaling policies with steps. So same concept you scale based on when alarm is breach, but it can escalate based on the alarms value, which changes over time. So before where you just had a single value here, we could say, well, if we have this, this alarm, and the value is between one and two, then add one instance. And then when it goes between two and three, then add another instance, or when it exceeds three to beyond, then add another instance. So you can, it helps you grow based on that alarm, that alarm as it changes, okay. So earlier, I was showing you that you can do health checks based on l B's. But I wanted to show you actually how you would associate that lb to an auto scaling group. And so we have classic load balancers. And then we have application load balancer and network load balancer. So there's a bit of variation based on the load bouncer how you would connect it, but it's pretty straightforward. So we're in the auto scaling group settings, we have these two fields, classic load balancers and target groups. And for classic load balancers, we just select the load balancer, and now it's associated. So it's as simple as that it's very straightforward. But with the new ways, there's a target group that's in between the auto scaling group and the load balancer. So you're associating the target group. And so that's all there is to it. So that's how you associate. So to give you the big picture on what happens when you get a burst of traffic, and auto scaling occurs, I just wanted to walk through this architectural diagram with you. So let's say we have a web server and we have one easy two instance running, okay, and all of a sudden, we get a burst of traffic, and that traffic comes into revenue three, revenue three points to our application load balancer application load balancer has a listener that sends the traffic to the target group. And we have this these students and switches associated with that target group. And we have so much traffic that it causes are your CPU utilization to go over 75%. And once it goes over 75%, because we had a target scaling policy attached, that said anything above 75%, spin up a new EC two instance. That's what the auto scaling group does. And so the way it does is it uses a launch configuration, which is attached to the auto scaling group, and it launches a new EC two instance. So that's just to give you the, like, full visibility on the entire pipeline of how that actually works. So when you have an auto scaling group, and it launches in institutions, how does it know what configuration to use to launch a new new ECU instance and that is what a launch configuration is. So when you have an auto scaling group, you actually set what launch configuration you want to use. And a launch configuration looks a lot like when you launch a new ECU instance. So you go through and you'd set all of these options. But instead of launching an instance at the end, it's actually just saving the configuration hence, it's called a launch configuration. A couple of limiting Around lost configurations that you need to know is that a launch configuration cannot be edited once it's been created. So if you need to update or replace that launch configuration, you need to either make a new one, or they have this convenient button to clone the existing configuration and make some tweaks to it. There is something also known as a launch template. And they are launched figurations, just but with versioning. And so it's AWS new version of lock configuration. And, you know, generally when there's something new, I might recommend that you use it, but it seems so far that most of the communities still uses launch configuration. So the benefit of versioning isn't a huge, doesn't have a lot of value there. So, you know, I don't I'm not pushing you to use launch templates, but I just want you to know the difference because it is a bit confusing because you look at it, it looks like pretty much the same thing. And it just has versioning in here and we can review the auto scaling group cheat sheet. So an S g is a collection of up to two instances group for scaling and management scaling out is when you add servers scaling is when you remove servers scaling up is when you increase the size of an instance so like you'd update the launch configuration with a larger size. The size of an ASC is based on the min maximum desired capacity. Target scaling policy scales based on when a target value of a metric is breached. So example average CPU utilization exceeds 75% simple scaling policy triggers a scaling when an alarm is breach. Scaling policy with steps is the new version simple scaling policy allows you to create steps based on escalation alarm values. desired capacity is how many situ instances you want to ideally run an ESG will always launch instances to meet the minimum capacity. health checks determine the current state of an instance in nasg. health checks can be run against either an EOB or an EC two instance, when an auto scaling when an auto scaling group launches a new instance, it will use a launch configuration which holds the configuration values of that new instance. For example, maybe the AMI instance type of roll launch configurations cannot be edited and must be cloned or a new one created. Launch configurations must be manually updated in by editing the auto scaling group settings. So there you go. And that's everything with auto scaling. We're looking at VPC endpoints, and they're used to privately connect your V PC to other Ada services and VPC endpoint services. So I have a use case here to make it crystal clear. So imagine you have an EC two instance, and you want to get something from your s3 bucket. So what you normally do is use the ABS SDK and you would make that call, and it would go out of your internet gateway to the internet back into the ABS network to get that file or or object out of s3. So wouldn't it be more convenient if we could just keep the traffic within the AWS network and that is the purpose of a VPC endpoint. It helps you keep traffic within the network. And the idea is now because it does not leave a network, we do not require a public IP address to communicate with these services, I eliminates the need for internet gateway. So let's say we didn't need this internet gateway, the only reason we were using it too was to get test three, we can now eliminate that and keep everything private. So you know, there you go. There are two types of VPC endpoints, inner interface endpoints and gateway endpoints. And we're going to get into that. So we're going to look at the first type of VPC endpoint and that is interface endpoints. And they're called interface endpoints because they actually provision an elastic network interface, an actual network interface card with a private IP address, and they serve as an entry point for traffic going to a supported service. And if you read a bit more about interface endpoints, they are powered by at this private link. There's not much to say here, this is what it is. So Access Services host on a bus easily securely by keeping your network traffic within a bus network. This is always confused me this branding of Eva's private link. But you know, you might as well just think of interface endpoints and use private link to be in the same thing. Again, it does cost something because it is speeding up and he and I. And so you know, it's it's point 01 cents per hour. And so over a month's time, if you had it on for the entire time, it's going to cost you around $7.50. And the interface endpoint supports a variety of data services, not everything. But here's a good list of them for you. The second type of VCP endpoint is a gateway endpoint, a gateway endpoint. It has a target for a specific route in your row table used for traffic destined for a supported database service. And this endpoint is 100% free because you're just adding something to your route table, and you're going to be utilizing it mostly for Amazon s3 and dynamodb. So you saw that first use case where I showed you that we were getting used to doing stock s3 that was using a gateway endpoint. So there you go. Here we are at the VPC endpoint cheat sheet and this is going to be a quick one, so let's get to it. VPC endpoints help keep traffic between awa services within the AWS network. There are two kinds of VPC endpoints interface endpoints and gateway endpoints. interface endpoints cost money, whereas gateway endpoints are free interface endpoints uses a elastic network interface in the UI with a private IP address part. And this was all powered by private link. gateway endpoints is a target for a specific route in your route table. And interface endpoints support many at the services whereas gateway endpoints only support dynamodb and s3. Hey, it's Andrew Brown from exam Pro. And we are looking at elastic load balancers also abbreviated to lb, which distributes incoming application traffic across multiple targets such as easy to instances containers, IP addresses, or lambda functions. So let's learn a little bit What a load balancer is. a load balancer can be physical hardware, or virtual software that accepts incoming traffic and then distributes that traffic to multiple targets. They can balance the load via different rules. These rules vary based on the type of load balancers. So for elastic load balancer, we actually have three load balancers to choose from. And we're going to go into depth for each one, we'll just list them out here. So we have application load balancer, network, load balancer, and classic load balancer. Understand the flow of traffic for lbs, we need to understand the three components involved. And we have listeners rules and target groups. And these things are going to vary based on our load balancers, which we're going to find out very shortly here. Let's quickly just summarize what these things are. And then see them in context with some visualization. So the first one are listeners, and they listen for incoming traffic, and they evaluate it against a specific port, whether that's Port 80, or 443, then you have rules and rules can decide what to do with traffic. And so that's pretty straightforward. Then you have target groups and target groups is a way of collecting all these two instances you want to route traffic to in logical groups. So let's go take a look first at application load bouncer and network load balancer. So here on the right hand side, I have traffic coming into repartee three, that points to our load balancer. And once it goes our load bouncer goes to the listener, it's good check what port it's running on. So if it's on port 80, I have a simple rule here, which is going to redirect it to Port 443. So it's gonna go this listener, and this listener has a rule attached to it, and it's going to forward it to target one. And that target one contains all these YouTube instances. Okay. And down below here, we can just see where the listeners are. So I have listener at 443. And this is where application load balancer, you can see I also can attach a SSL certificate here. But if you look over at rules, and these rules are not going to appear for network load bouncer, but they are going to appear for a lb. And so I have some more complex rules. If you're using lb, it simply just forwards it to a target, you don't get more rich options, which will show you those richer options in a future slide. But let's talk about classic load balancer. So a classic load balancer is, is much simpler. And so you have traffic coming in it goes to CLB. You have your listeners, they listen on those ports, and you have registered targets. So there isn't target groups, you just have loosey goosey two instances that are associated with the classic load balancer. Let's take a deeper look at all three load balancers starting with application load balancer. So application load balancer, also known as a lb is designed to balance HTTP and HTTPS traffic. It operates at layer seven of the OSI model, which makes a lot of sense because layer seven is application lb has a feature called request routing, which allows you to add routing rules to your listeners based on the HTTP protocol. So we saw previously, when we were looking at rules, it was only for lb that is this is that request routing rules. You can attach a web application firewall to a lb. And that makes sense because they're both application specific. And if you want to think of the use case for application load balancer, well, it's great for web application. So now let's take a look at network load balancer which is designed to balance TCP and UDP traffic, it operates at the layer four of the OSI model, which is the transport layer. And it can handle millions of requests per second while still maintaining extremely low latency. It can perform cross zone load balancing, which we'll talk about later on. It's great for you know, multiplayer video games are when network performance is the most critical thing to your application. Let's take a look at classic load balancers. So it was AWS is first load balancer. So it is a legacy load balancer. It can balance HTTP or TCP traffic, but not at the same time, it can use layer seven specific features such as sticky sessions, it can also use a strict layer for for bouncing, purely TCP application. So that's what I'm talking about where it can do one or the other. It can perform cross zone load balancing, which we will talk about later on. And I put this one in here, because it is kind of an exam question. I don't know if it still appears, but it will respond with a 504 error in case of timeout if the underlying application is not responding. And an application could be not responding spawning would be example as the web server or maybe the database itself. So classic load balancer is not recommended for use anymore, but it's still around, you can utilize it. But you know, it's recommended to use nlb or lb, when possible. So let's look at the concept of sticky sessions. So sticky Sessions is an advanced load balancing method that allows you to bind a user session to a specific EC two instance. And this is useful when you have specific information that's only stored locally on a single instance. And so you need to keep on sending that person to the same instance. So over here, I have the diagram that shows how this works. So on step one, we wrote traffic to the first EC two instance, and it sets a cookie. And so the next time that person comes through, we check to see if that cookie exists. And we're going to send it to that same EC two instance. Now, this feature only works for classic load bouncer and application load bouncer, it's not available for nlb. And if you need to set it for application load bouncer, it has to be set on the target group and not individually, you see two instances. So here's a scenario you might have to worry about. So let's say you have a user that's requesting something from your web application, and you need to know what their IP address is. So you know, the request goes through and then on the EC two instance, you look for it, but it turns out that it's not actually their IP address. It's the IP address of the load balancer. So how do we actually see the user's IP address? Well, that's through the x forwarded for header, which is a standardized header when dealing with load balancers. So the x forwarded for header is a command method for identifying the originating IP address of a connecting, or client connecting to a web server through HTTP proxy or a load balancer. So you would just forward make sure that in your web application that you're using that header, and then you just have to read it within your web application to get that user's IP address. So we're taking a look at health checks for elastic load balancer. And the purpose behind health checks is to help you route traffic away from unhealthy instances, to healthy instances. And how do we determine if a instance is unhealthy waltz through all these options, which for a lb lb is set on the target group or for closet load balancer is directly set on the load balancer itself. So the idea is we are going to ping the server at a specific URL at a with a specific protocol and get an expected specific response back. And if that happens more than once over a specific interval that we specify, then we're going to mark it as unhealthy and the load balancer is not going to send any more traffic to it, it's going to set it as out of service. Okay. So that's how it works. One thing that you really need to know is that e lb does not terminate unhealthy instances, it's just going to redirect traffic to healthy instances. So that's all you need to know. So here we're taking a look at cross zone load balancing, which is a feature that's only available for classic and network load balancer. And we're going to look at it when it's enabled, and then when it's disabled and see what the difference is. So when it's enabled requests are distributed evenly across the instances in all the enabled availability zones. So here we have a bunch of UC two instances in two different Z's and you can see the traffic is even across all of them. Okay? Now, when it's disabled requests are distributed evenly across instances. It's in only its availability zone. So here, we can see in az a, it's evenly distributed within this AZ and then the same thing over here. And then down below if you want to know how to enable cross zone load balancing, it's under the description tab and you'd edit the attributes. And then you just check box on cross zone load balancing. Now we're looking at an application load balancer specific feature called request routing, which allows you to apply rules to incoming requests, and then for to redirect that traffic. And we can check on a few different conditions here. So we have six in total. So we have the header host header source IP path is to be header, ASP header method or query string. And then you can see we have some, then options, we can forward redirect returned to fixed response or authenticate. So let's just look at a use case down here where we actually have 1234, or five different examples. And so one thing you could do is you could use this to route traffic based on subdomain. So if you want an app to subdomain app to go to Target, prod, and QA to go to the target QA, you can do that you can either do it also on the path. So you could have forward slash prod and for slash QA and now it route to the respective target groups, you could do it as a query string, you could use it by looking at HTTP header, or you could say all the get methods go to prod on a why you'd want to do this, but you could and then all the post methods would go to QA. So that is request request routing in a nutshell. We made it to the end of the elastic load balancer section and onto the cheat sheet. So there are three elastic load balancers, network application and classic load balancer and elastic load bouncer must have at least two availability zones for it to work on elastic load balancers cannot go cross region you must create one per region. lbs have listeners rules and target groups to route traffic and OBS have listeners and target groups to route traffic. And CL B's. Us listeners and ECU instances are directly registered as targets to the CLB. For application load balancer, it uses HTTP s or eight or without the S traffic. And then as the name implies, it's good for web applications. Network load balancer is for TCP, UDP, and is good for high network throughput. So think maybe like multiplayer video games, classic load balancer is legacy. And it's recommended to use a lb, or nlb when you can, then you have the x forwarded for. And the idea here is to get the original IP of the incoming traffic passing through the lb, you can attach web application firewall to a lb. Because you know web application firewall has application the name and nlb in CLB do not, you can attach an Amazon certification manager SSL certificate. So that's an ACM to any of the L B's. To get SSL. For a lb you have advanced request routing rules, where you can route based on subdomain, header path and other SP information. And then you have sticky sessions which can be enabled for CLB or lb. And the idea is that it helps the session remember, what would you say to instance, based on cookie? Hey, it's Andrew Brown from exam Pro, we are looking at security groups. And they help protect our EC two instances by acting as a virtual firewall controlling the inbound and outbound traffic, as I just said, security groups acts as a virtual firewall at the instance level. So you would have an easy to instance and you would attach to it security groups. And so here is an easy to instance. And we've attached the security group to it. So what does it look like on the inside for security groups, each security group contains a set of rules that filter traffic coming into. So that's inbound, and out of outbound to that easy to instance. So here we have two tabs, inbound and outbound. And we can set these are rules, right. And we can set these rules with a particular protocol and a port range. And also who's allowed to have access. So in this case, I want to be able to SSH into this YSU instance, which uses the TCP protocol. And the standard port for SSH is 22. And I'm going to allow only my IP. So anytime you see forward slash 32 that always means my IP. All right. So that's all you have to do to add inbound and outbound rules. There are no deny rules. So all traffic is blocked by default unless a rule specifically allows it. And multiple instances across multiple subnets can belong to a security group. So here I have three different EC two instances, and they're all in different subnets. And security groups do not care about subnets. You just assign easy to instance, to a security group and you know, just in this case, and they're all in the same one, and now they can all talk to each other. Okay. Here, I have three security group scenarios. And they all pretty much do the same thing. But the configuration is different to give you a good idea of variation on how you can achieve things. And so the idea is we have a web application running on a situ instance. And it is connecting to an RDS database to get its information running in a private subnet. Okay. And so in the first case, what we're doing is we have an inbound rule on the SU database saying allowing anything from 5432, which is the Postgres port number, for this specific IP address. And so it allows us to see one instance to connect to that RDS database. And so the takeaway here is you can specify the source to be an IP range, or a specific IP. And so this is very specific, it's forward slash 32. And that's a nice way of saying exactly one IP address. Now in the second scenario, it looks very similar. And the only difference is, instead of providing an IP address as the source, we can provide another security group. So now anything within the security group is allowed to gain access for inbound traffic on 5432. Okay, now, in our last use case, down below, we have inbound traffic on port 80, and inbound traffic on port 22, for the SG public group, and then we have the EC two instance and the RDS database within its own security group. So the idea is that that EC two instance is allowed to talk to that RDS database, and that EC two instance is not exposing the RDS database to it well, wouldn't because it's in a private subnets. It doesn't have a public IP address. But the point is, is that this is to instance, now is able to get traffic from the internet. And it's also able to accept someone from like for any SSH access, okay. And so the big takeaway here is that you can see that an instance can belong to multiple security groups and rules are permissive. So when we have two security groups, and this one has allows, and this is going to take precedence over su stack, which doesn't have anything, you know, because it's denied by default, everything, but anything that allows is going to override that. Okay, so you can nest multiple security groups onto one ACTA Ensign. So just keep that stuff in mind. There are a few security group limits I want you to know about. And so we'll look at the first you can have up to 10,000 security groups in a single region, and it's defaulted to 2500. If you want to go beyond that 2500, you need to make a service limit increase request to a VA support, you can have 60 inbound rules and 60 outbound rules per security group. And you can have 16 security groups per EMI. And that's defaulted to five. Now, if you think about like, how many security rules can you have on an instance? Well, it's depending on how many you guys are actually attached to that security group. So if you have to realize that it's attached to a security group, then by default, you'll have 10. Or if you have the upper limit here, 16, you'll be able to have 32 security groups on a single instance. Okay, so those are the limits, you know, I thought were worth telling. So we're going to take a look at our security groups cheat sheet. So we're ready for exam time. So security groups act as a firewall at the instance level, unless allowed specifically, all inbound traffic is blocked by default, all outbound traffic from the instance is allowed by default, you can specify for the source to be either an IP range, a single IP address or another security group. security groups are stateful. If traffic is allowed inbound, it is also allowed outbound. Okay, so that's what stateful means. Any changes to your security group will take effect immediately. ec two instances can belong to multiple security groups, security groups can contain multiple ECU instances, you cannot block a specific IP address security groups. For this, you need to use knackles. Right? So again, it's allowed by default, sorry, everything's denying you're only allowing things okay. You can have up to 10,000 security groups per region default is 2500. You can have 60 inbound and 60 outbound rules per security group. And you can have 16 security groups associated to that you and I default is five and I can see that I added an extra zero there so don't worry when you print out your security scripts to cheat it will be all correct. Okay. Hey, this is Angie brown from exam Pro, and we are looking at network access control lists also known as knackles. It is an optional layer of security that acts as a firewall for controlling traffic in and out of subnets. So knackles act as a virtual firewall at the subnet level and when you create a VPC, you automatically get a knakal by default, just like security groups knackles have both inbound and outbound rules. The difference here is that you're going to have the ability to allow or deny traffic in either way. Okay, so for security groups, you can only allow whereas knackles, you have deny. Now, when you create these rules here, it's pretty much the same as security groups with the exception that we have this thing called rule number And rule number is going to determine the order of evaluation for these rules, and the way it evaluates is going to be from the lowest to the highest, the highest rule number, it could be 32,766. And at best recommends that when you come up with these rule numbers, you use increments of 10 or 100. So you have some flexibility to create rules in between if you need B, again, subnets are at the subnet level. So in order for them to apply, you need to associate subnets to knackles. And subnets can only belong to a single knakal. Okay, so yeah, where you have security groups, you can have a instances that belong to multiple ones for knackles. It's just a singular case, okay. All right. So we're just gonna look at a use case for knackles. Here, it's going to be really around this deny ability. So let's say there is a malicious actor trying to gain access to our instances, and we know the IP address, well, we can add that as a rule to our knakal and deny that IP address. And let's say we know that we never need to SSH into these instances. And we just want an additional guarantee in case someone misconfigured a security group that SSH access is denied. So we'll just deny on port 22. And now we have those two cases covered. So there you go. Alright, so we're on to the knakal cheat sheet. So network access control list is commonly known as nachal. v PCs are automatically given a default knakal, which allows all outbound inbound traffic each subnet within a VPC must be associated with a knakal. subnets can only be associated with one knakal. at a time, associate a sub net with a new knakal with will remove the previous Association. If a knakal is not explicitly associated with a subnet, the subnet will automatically be associated with the default knakal knackles have both inbound and outbound rules just like a security group. The rules can either be allow or deny traffic, unlike a security group, which can only you can only apply allow rules knackles are stateless. This is This means that knackles do not care about the rules that you set. So if you want to deny outbound, that doesn't necessarily mean it's gonna deny inbound automatically, you have to set it individually for both sides. And that's why it's considered stateless. When you create a knakal it will deny all traffic by default knackles contain a numbered list of rules that get evaluated in order from lowest to highest. If you needed to block a single IP address, you could you could be a knakal. Whereas security security you cannot because it only has deny rules and it'd be very difficult to block a single IP with a security group. So there you go. That's your knakal teaching. Hey, this is Angie brown from exam pro and we are starting to VPC follow along and this is a very long section because we need to learn about all the kind of networking components that we can create. So we're going to learn how to create our own VPC, subnets, route tables, internet gateways, security groups, Nat gateways knackles, we're going to touch it all. Okay, so it's very core to learning about AWS, and it's just a great to get it out of the way. So let's jump into it. So let's start off by creating our own VPC. So on the left hand side, I want you to click on your VPC. And right away, you're going to see that we already have a default VPC within this region of North Virginia. Okay, your region might be different from mine. It doesn't actually does kind of matter what region you use, because different regions have different amounts of available azs. So I'm going to really strongly suggest that you switch to North Virginia to make this section a little bit smoother for you. But just notice that the default VPC uses an ipv4 cider, cider block range of 172 31 0.0 forward slash 16. Okay, and so if I was to change regions, no matter what region will go to us, West, Oregon, we're going to find that we already have a default VPC on here as well and it's going to have the same a cider block range, okay. So just be aware that at best does give you a default VPC so that you can start launching resources immediately without having to worry about all this networking, and there's no full power with using the default VPC it's totally acceptable to do so. But we definitely need to know how to do this ourselves. So we're going to create our own VPC. Okay, and so, I'm a big fan of Star Trek, and so I'm going to name it after the planet of Bayshore. Which is a very well known planet in the Star Trek universe. And I'm going to have to provide my own cider block, it cannot be one that already exists. So I can't use that 172 range that ETS was using. So I'm gonna do 10.0 dot 0.0, forward slash 16. And there is a bit of a rhyme and rhythm to choosing these, this one is a very commonly chosen one. And so I mean, you might be looking this going, Okay, well, what is this whole thing with the IP address slash afford 16. And we will definitely explain that in a separate video here. But just to give you a quick rundown, you are choosing your IP address that you want to have here. And this is the actual range. And this is saying how many IP addresses you want to allocate. Okay. So yeah, we'll cover that more later on. And so now we have the option to set ipv6 cider, or a cider block here. And so just to keep it simple, I'm going to turn it off. But you know, obviously, ipv6 is supported on AWS. And it is the future of, you know, our IP protocol. So it's definitely something you might want to turn on. Okay, and just be prepared for the future there that we have this Tennessee option, and this is going to give us a dedicated hosts. For our VPC, this is an expensive, expensive option. So we're going to leave it to default and go proceed and create our VPC. And so there it has been created. And it was very fast, it was just instantaneous there. So we're going to click through to that link there. And now we can see we have our VPC named Bayshore. And I want you to notice that we have our IP V for cider range, there is no ipv6 set. And by default, it's going to give us a route table and a knakal. Okay, and so we are going to overwrite the row table because we're going to want to learn how to do that by ourselves. knackles is not so important. So we might just gloss over that. But um, yeah, so there you are. Now, there's just one more thing we have to do. Because if you look down below here, we don't have DNS resolution, or DNS, or sorry, DNS hostnames is disabled by default. And so if we launch an EC two instance, it's not going to get a, a DNS, DNS hostname, that's just like a URL. So you can access that ecsu instance, we definitely want to turn that on. So I'm going to drop this down to actions and we're going to set a host names here to enabled okay. And so now we will get that and that will not cause us pain later down the road. So now that we've created our VPC, we want to actually make sure the internet can reach it. And so we're going to next learn about internet gateways. So we have our VPC, but it has no way to reach the internet. And so we're going to need an internet gateway. Okay, so on the left hand side, I want you to go to internet gateway. And we are going to go ahead and create a new one, okay. And I'm just going to call it for internet gateway, vaes yours, and people do it, who do it who doesn't hurt. And so our internet gateway has been created. And so we'll just click through to that one. And so you're gonna see that it's in a detached state. So internet gateways can only be attached to a very specific VP v PC, it's a one to one relationship. So for every v PC, you're going to have an internet gateway. And so you can see it's attach and there is no VPC ID. So I'm going to drop this down and attach the VPC and then select Bayshore there and attach it. And there you go. Now it's attached, and we can see the ID is associated. So we have an internet gateway. But that still doesn't mean that things within our network can reach the internet, because we have to add a route to our route table. Okay, so just closing this tab here, you can see that there already is a route table associated with our VPC because it did create us a default route table. So I'm just going to click through to that one here to show you, okay, and you can see that it's our main route table, because it's set to main, but I want you to learn how to create route tables. So we're gonna make one from scratch here. Okay. So we'll just hit Create route table here. And we're just going to name it our main route table or RG, our internet road table, I don't know doesn't matter. Okay, we will just say RT, to shorten that there and we will drop down and choose Bayshore. And then we will go ahead and create that route table. Okay, and so we'll just hit close. And we will click off here so we can see all of our route tables. And so here we have our, our, our main one here for Bayshore, and then this is the one we created. Okay, so if we click into this route table here, you can see by default, it has the full scope of our local network here. And so I want to show you how to change this one to our main. So we're just going to click on this one here and switch it over to main, so set as main route table. So the main road table is whenever you know, just what is going to be used by default. All right, and so we'll just go ahead and delete the default one here now because we know why. We need it. Alright, and we will go select our new one here and edit our routes. And we're going to add one for the internet gateway here. So I'm going to just drop down here or sorry, I'm just going to write 0.0 dot 0.0, forward slash, zero, which means let's take, take anything from anywhere there. And then we're going to drop down select internet gateway, select Bayshore, and hit save routes. Okay, and we'll hit close. And so now we, we have a internet gateway. And we have a way for our subnets to reach the internet. So there you go. So now that we have a route to the internet, it's time to create some subnets. So we have some way of actually launching our EC two instances, somewhere. Okay, so on the left hand side, I want you to go to subnets. And right away, you're going to start to see some subnets. Here, these are the default ones created with you with your default VPC. And you can see that there's exactly six of them. So there's exactly one for every availability zone within each region. So the North Virginia has six azs. So you're going to have six, public subnets. Okay, the reason we know these are public subnets. If we were to click on one here and check the auto assign, it is set to Yes. So if a if this is set to Yes, that means any easy to instance, launch in the subnet is going to get a public IP address. Hence, it's going to be considered a public subnet. Okay. So if we were to switch over to Canada Central, because I just want to make a point here, that if you are in a another region, it's going to have a different amount of availability zones, Canada only has two, which is a bit sad, we would love to have a third one there, you're going to see that we have exactly one subnet for every availability zone. So we're going to switch back to North Virginia here. And we are going to proceed to create our own subnets. So we're going to want to create at least three subnets if we can. So because the reason why is a lot of companies, especially enterprise companies have to run it in at least three availability zones for high availability. Because if you know one goes out and you only have another one, but what happens if two goes out. So there's that rule of you know, always have at least, you know, two additional Okay, so we're going to create three public subnets in one pool, one private subnet, we're not going to create three private subnets, just because I don't want to be making subnets here all day. But we'll just get to it here. So we're going to create our first subnet, I'm going to name this Bayshore public, okay, all right, and we're going to select our VPC. And we're going to just choose the US East one a, and we're going to give it a cider block of 10.0 dot 0.0 forward slash 24. Now, notice, this cider range is a smaller than the one up here, I know the number is larger, but from the perspective of how many IP addresses that allocates, there's actually a few here. So you are taking a slice of the pie from the larger range here. So just be aware, you can set this as 16, it's always going to be less, less than in by less, I mean, a higher number than 16. Okay, so we'll go ahead and create our first public subnet here. And we'll just hit close. And this is not by default public, because by default, the auto assign is going to be set to No. So we're just going to go up here and modify this and set it so that it does auto assign ipv4 and now is is considered a public subnet. So we're going to go ahead and do that for our B and C here. So it's going to be the same thing Bayshore public, be. Okay, choose that. We'll do B, we'll do 10.0 dot 1.0 24. Okay. And we're gonna go create that close. And we're going to that auto assign that there. All right. And the next thing we're going to do is create our next sub down here so Bayshore, how boring I Bayshore public. See, and we will do that and we'll go to see here and it's going to be 10.0 dot 2.0. forward slash 24. Okay, we'll create that one. Okay, let close and we will make sure did I set that one? Yes, I did. Did I set that one not as of yet. And so we will modify that there. Okay. And we will create a another subnet here and this is going to be a beige, your private a, okay. And we are going to set that to eight here. And we're going to set this to 10.0 dot 3.0 24. Okay, so this is going to be our private subnet. All right. So we've created all of our subnets. So the next thing we need to do is associate them with a route table. Actually, we don't have to because by default, it's going to use the main Alright, so they're already automatically associated there. But for our private one, we're not going to be wanting to really use the the the the main route table there, we probably would want to create our own route table for our privates. That's there. So I'm just gonna create a new one here and we can just call it private RT. Okay, I'm going to drop that down, choose Bayshore here. And we're going to hit close, okay? And the idea is that the, you know, we don't need this subnet to reach the internet, so it doesn't really make sense to be there. And then we could set other things later on. Okay, so what I want you to do is just change the association here. So we're gonna just edit the route table Association. And we're just going to change that to be our private one. Okay. And so now our route tables are set up. So we will move on to the next step. So our subnets are ready. And now we are able to launch some EC two instances. So we can play around and learn some of these other networking components. So what I want you to do is go to the top here and type in EC two. And we're going to go to the EC two console. And we're going to go to instances on the left hand side. And we're going to launch ourselves a couple of instances. So we're going to launch our first instance, which is going to be for our public subnet here. So we're going to choose t to micro, we're going to go next, and we are going to choose the Bayshore VPC that we created. We're going to launch this in the public subnet here public a, okay, and we're going to need a new Im role. So I'm just going to right click here and create a new Im role, because we're going to want to give it access to both SSM for sessions manager and also, so we have access to s3. Okay, so just choosing EC two there. I'm going to type in SSM. Okay, SSM, there it is at the top, then we'll type in s3, we're gonna give it full access, we're going to go next, we're going to go to next and we're going to just type in my beige or EC two. Okay. And we're going to hit Create role. Okay, so now we have the role that we need for our EC two instance, we're just going to refresh that here, and then drop down and choose my beige or EC two. Okay, and we are going to want to provide it a script here to run. So I already have a script pre prepared that I will provide to you. And this is the public user data.sh. All this is going to do. And if you want to just take a peek here at what it does, I guess they don't have it already open here. But we will just quickly open this up here. all it's going to do is it's gonna install an Apache server. And we're just going to have a static website page here served up, okay, and so we're going to go ahead and go to storage, nothing needs to be changed here, we're going to add, we don't need to add any tags, we're gonna go to security group, and we're going to create a new security group, I'm going to call it m, my, my base, your EC two SD, okay. And we're going to make sure that we have access to HTTP, because this is a website, we're going to have to have Port 80, open, we're going to restrict it down to just us. And we could also do that for SSH. So we might as well do that there as well. Okay, we're gonna go ahead and review and launch this EC two instance, and already have a key pair that is created, you'll just have to go ahead and create one if you don't have one there. And we'll just go ahead and launch that instance there. Okay, great. So now, we have this EC two instance here, which is going to be for our public segment. Okay. And we will go ahead and launch another instance. So we'll go to Amazon Lex to here, choose t to micro. And then this time, we're going to choose our private subnet. Okay, I do want to point out that when you have this auto assign here, see how it's by by default disabled, because it's inheriting whatever the parents love that has, whereas when we set it, the first one, you might have not noticed, but it was set to enable, okay. And we are going to also give it the same role there, my beige or EC two. And then this time around, we're going to give it the other scripts here. So I have a private script here, I'm just going to open it up and show it to you. Okay, and so what this script does, is a while it doesn't actually need to install Apache, so we'll just remove that, I guess it's just old. But anyway, what it's going to do is it's going to reset the password on the EC to user to chi win. Okay, that's a character from Star Trek Deep Space Nine. And we're also going to enable password authentication. So we can SSH into this using a password. And so that's all the script does here. Okay, and so we are going to go ahead and choose that file there and choose that and we will move on to storage storage is totally fine. We're not going to add tags, secure groups, we're gonna actually create a new security group here. It's not necessarily unnecessary, but I'm going to do it anyway. So I'm gonna save my private, private EC to SD, maybe put Bayshore in there. So we just keep these all grouped together note, therefore, it's only going to need SSH, we're not going to have any access to the internet there. So like, there's no website or anything running on here. And so we'll go ahead and review and launch. And then we're going to go watch that instance and choose our key pair. Okay, great. So now we're just going to wait for these two instances to spin up here. And then we will play around with security groups and knackles. So I just had a quick coconut water, and now I'm back here and our instances are running, they don't usually take that long to get started here. And so we probably should have named these to make it a little bit easier. So we need to determine which is our public and private. And you can see right away, this one has a public public DNS hostname, and also it has its ip ip address. Okay, so this is how we know this is the public one, so I'm just gonna say, Bayshore public. Okay. And this one here is definitely the private one. All right, so we will say a beige, your private. Okay. So, um, yeah, just to iterate over here, if we were to look, here, you can see we have the DNS and the public IP address. And then for the private, there's nothing set. Okay. So let's go see if our website is working here. So I'm just going to copy the public IP address, or we can take the DNS one, it doesn't matter. And we will paste this in a new tab. And here we have our working website. So our public IP address is definitely working. Now, if we were to check our private one, there is nothing there. So there's nothing for us to copy, we can even copy this private one and paste it in here. So there's no way of accessing that website is that is running on the private one there. And it doesn't really make a whole lot of sense to run your, your website, in the private subnet there. So you know, just to make a very clear example of that, now that we have these two instances, I guess, it's a good opportunity to learn about security groups. Okay, so we had created a security group. And the reason why we were able to access this instance, publicly was that in our security group, we had an inbound rule on port 80. So Port 80, is what websites run on. And when we're accessing through the web browser, there's and we are allowing my IP here. So that's why I was allowed to access it. So I just want to illustrate to you what happens if I change my IP. So at the top here, I have a VPN, it's a, it's a service, you can you can buy a lot people use it so that they can watch Netflix in other regions. I use it for this purpose not to watch Netflix somewhere else. So don't get that in your mind there. But I'm just going to turn it on. And I'm going to change my IP. So I get I think that this is Brazil. And so I'm going to have an IP from Brazil here shortly once it connects. And so now if I were to go and access this here, it shouldn't work. Okay, so I'm just going to close that tab here. And it should just hang. Okay, so it's hanging because I'm not using that IP. So that's how security groups work. Okay, and so I'm just going to turn that off. And I think I should have the same one. And it should resolve instantly there. So great. So just showing you how the security groups work for inbound rules, okay, for outbound rules, that's traffic going out to the internet, it's almost always open like this 0.0 dot 0.0, right, because you'd want to be able to download stuff, etc. So that is pretty normal business. Okay. So now that now that we can see that, maybe we would like to show off how knackles work compared to security groups to security groups. As you can see, if we were just to open this one up here, okay. security groups, by default, only can allow things so everything is denied. And then you're always opening things up. So you're adding allow rules only you can't add an explicit deny rule. So we're knackles are a very useful is that you can use it to block a very specific IP addresses, okay, or IP ranges, if you will. And you cannot do that for a security group. Because how would you go about doing that? So if I wanted to block access just to my IP address, I guess I could only allow every other IP address in the world except for mine. But you can see how that would do undue burden. So let's see if we can set our knakal to just block our IP address here. Okay. So security groups are associated with the actual EC two instances or so the question is, is that how do we figure out the knackles and knackles are associated with the subnets. Okay, so in order to block our IP address for this easy to instance, we have to determine what subnet it runs in and so it runs in our Bayshore public Right. And so now we got to find the knakal. that's associated with it. So going up here to subnets, I'm going to go to public a, and I'm going to see what knackles are associated with it. And so it is this knakal here, and we have some rules that we can change. So let's actually try just blocking my IP address here. And we will go just grab it from here. Okay. All right. And just to note, if you look here, see has this Ford slash 32. That is mean, that's a cider block range of exactly one IP address. That's how you specify a single IP address with forward slash 32. But I'm going to go here and just edit the knakal here, and we are going to this is not the best way to do it. So I'm just going to open it here. Okay. And because it didn't get some edit options there, I don't know why. And so we'll just go up to inbound rules here, I'm going to add a new rule. And it goes from lowest to highest for these rules. So I'm just going to add a new rule here. And I'm going to put in rule 10. Okay, and I'm going to block it here on the cider range. And I'm going to do it for Port 80. Okay, so this and we're going to have an explicit deny, okay, so this should, this should not allow me to access that EC two instance any any longer. Okay, so we're going to go back to our instances here, we're going to grab that IP address there and paste it in there and see if I still have access, and I do not okay, so that knakal is now blocking it. So that's how you block individual IP addresses there. And I'm just going to go back and now edit the rule here. And so we're just going to remove this rule, and hit save. And then we're going to go back here and hit refresh. Okay. And I should now have access on I do. So there you go. So that is security groups and knakal. So I guess the next thing we can move on to is how do we actually get access to the private subnet, okay, and the the the ways around that we have our our private EC two instance. And we don't have an IP address, so there's no direct way to gain access to it. So we can't just easily SSH into it. So this is where we're going to need a bastion. Okay, and so we're gonna go ahead and go set one up here. So what I want you to do is I want you to launch a new instance, here, I'm just going to open a new tab, just in case I want this old tab here. And I'm just going to hit on launch instance here. Okay, and so I'm going to go to the marketplace here, I'm gonna just type in Bastion. And so we have some options here, there is this free one Bastion host, SSH, but I'm going to be using guacamole and there is an associated cost here with it, they do have a trial version, so you can get away without paying anything for it. So I'm just going to proceed and select guacamole. And anytime you're using something from the marketplace, they generally will have the instructions in here. So if you do view additional details here, we're going to get some extra information. And then we will just scroll down here to usage information such as usage instructions. And we're going to see there is more information, I'm just going to open up this tab here because I've done this a few times. So I remember where all this stuff is. Okay, and we're just going to hit continue here. Okay, and we're going to start setting up this instance. So we're going to need a small, so this one doesn't allow you to go into macros, okay, so there is an associated cost there, we're going to configure this instance, we're going to want it in the same VPC as our private, okay, when we have to launch this in a public subnet, so just make sure that you select the public one here, okay. And we're going to need to create a new Im role. And this is part of guacamole these instructions here because you need to give it some access so that it can auto discover instances, okay, and so down here, they have instructions here and they're just going to tell you to make an IM role, we could launch a cloudformation template to make this but I would rather just make it by hand here. So we're going to grab this policy here, okay. And we are going to make a new tab and make our way over to I am okay. And once we're in I am here, we're gonna have to make this policy so I'm going to make this policy. Okay, unless I already have it, see if it's already in here, new. Okay, good. And I'm gonna go to JSON, paste that in there, review the policy, I'm going to name it they have a suggestion here what to name it, Glock AWS, that seems fine to me. Okay, and here you can see it's gonna give us permissions to cloud watch an sts so we'll go ahead and create that policy. It says it already exists. So I'm I already have it. So just go ahead and create that policy. And I'm just going to skip the step for myself. Okay, and we're just going to cancel there. So I'm just going to type walk. I don't know why it's not showing up. says it already exists. Type that in again. So yeah, there it is. So I already have that policy. Okay, so I couldn't hit that last step. But you'll Be able to get through that no problem. And then once you have it, you're gonna have to create a new role. So we're going to create a role here and it's going to be for EC two, we're going to go next. And we're going to want I believe EC to full access is that the right Oh, read only access, okay. So we're going to want to give this easy to read only access. And we're also going to want to give it that new GWAC role. So I'm going to type in type AWS here. Oh, let's give me a hard time here. And we'll just copy and paste the whole name in here. There it is. And so those are the two, two policies you need to have attached. And then we're just going to name this something here. So I'm gonna just call it my GWAC. Bastion, okay, roll here. I'm going to create that role. Okay, and so that role has now been created, we're going to go back here, refresh the IM roles, and we're going to see if it exists. And there it is my GWAC Bastion role. And I spell bash and wrong there. But I don't think that really matters. And then we will go to storage. There's nothing to do here, we'll skip tags, we'll go to security groups. And here you can see it comes with some default configurations. So we're going to leave those alone. And then we're going to launch this EC two instance. Okay. So now we're launching that it's taking a bit of time here, but this is going to launch. And as soon as this is done, we're going to come back here and actually start using this Bastion to get into our private instance. So our bashing here is now already in provisioned. So let's go ahead and just type in Bastion, so we don't lose that later on, we can go grab either the DNS or public IP, I'll just grab the DNS one here. And we're going to get this connection, not private warning, that's fine, because we're definitely not using SSL here. So just hit advanced, and then just click to proceed here. Okay, and then it's might ask you to allow, we're going to definitely say allow for that, because that's more of the advanced functionality, guacamole there, which we might touch in. At the end of this here, we're going to need the username and password. So it has a default, so we have block admin here, okay. And then the password is going to be the name of the instance ID. All right, and this is all in the instructions here. I'm just speaking you through it. And then we're going to hit login here. And so now it has auto discovered the instances which are in the VPC that is launched. And so here, we have Bayshore, private. So let's go ahead and try to connect to it. Okay. So as soon as I click, it's going to make this shell here. And so we'll go attempt and login now. So our user is easy to user. And I believe our password is KI wi n Chi win. And we are in our instance, so there you go. That's how we gain access to our private instance here. Just before we start doing some other things within this private easy too, I just want to touch on some of the functionality of Bastion here, or sorry, guacamole, and so why you might actually want to use the bastion. So it does, it is a hardened instance, it does allow you to authenticate via multiple methods. So you can enable multi factor authentication to use this. It also has the ability to do screen recordings, so you can really be sure what people are up to, okay, and then it just has built in audit logs, and etc, etc. So there's definitely some good reasons to use a bastion. But we can also use a sessions manager, which does a lot of this for us with the exception of screen recording within the, within AWS. But anyway, so now that we're in our instance, let's go play around here and see what we can do. So now that we are in this private EC two instance, I just want to show you that it doesn't have any internet access. So if I was to ping something like Google, right, okay, and I'm trying to get information here to see how it's hanging, and we're not getting a ping back. That's because there is no route to the internet. And so the way we're going to get a route to the internet is by creating a NAT instance or a NAT gateway. Generally, you want to use a NAT gateway, there are cases of use Nat instances. So if you were trying to save money, you can definitely save money by having to manage a NAT instance by herself. But we're gonna learn how to do Nat gateway because that's the way to this wants you to go. And so back in our console, here we are in EC two instances where we're going to have to switch over to a VPC. Okay, because that's where the NAT gateway is. So on the left hand side, we can scroll down and we are looking under VPC, we have Nat gateways. And so we're going to launch ourselves a NAT gateway, that gateways do cost money. So they're not terribly expensive, but you know, we, at the end of this will tear it down. Okay. And so, the idea is that we need to launch this Nat gateway in a public VPC or, sorry, public subnet, and so we're gonna have to look here, I'm gonna watch it in the Bayshore, public eight, doesn't matter which one just has to be one of the Public ones. And we can also create an elastic IP here. I don't know if it actually is required assigned a pipe network, I don't know if it really matters. But we'll try to go ahead and create this here without any IP. No, it's required. So we'll just hit Create elastic IP there. And that's just a static IP address. So it's never changing. Okay, and so now that we have that is associated with our Nat gateway, we'll go ahead and create that. And it looks like it's been created. So once your Nat gateway is created, the next thing we have to do is edit your route table. So there actually is a way for that VPC to our sorry, that private instance to access the internet. Okay, so let's go ahead and edit that road table. And so we created a private road table specifically for our private EC two. And so here, we're going to edit the routes, okay. And we're going to add a route for that private or to that Nat gateway. Okay. So um, we're just going to type in 0.0 dot 0.0, forward slash zero. And we are then just going to go ahead, yep. And then we're going to go ahead and choose our Nat gateway. And we're going to select that there, and we're going to save that route. Okay, so now our Nat gateway is configured. And so there should be a way for our instance to get to the internet. So let's go back and do a ping. And back over here, in our private EC two instance, we're just going to go ahead and ping Google here. Okay, and we're going to see if we get some pings back, and we do so there you go. That's all we had to do to access the internet. All right. So why would our private EC two instance need to reach the internet? So we have one one inbound traffic, but we definitely want outbound because we would probably want to update packages on our EC two instance. So we did a sudo Yum, update. Okay, we wouldn't be able to do this without a outbound connection. All right. So it's a way of like getting access to the internet only for the things that we need for outbound connections, okay. All right. So we had a fun time playing around with our private EC two instance there. And so we're pretty much wrapped up here for stuff. I mean, there's other things here, but you know, at the associate level, it's, there's not much reason to get into all these other things here. But I do want to show you one more thing for VP C's, which are VPC flow logs, okay, and so I want you to go over to your VPC here, okay, and then I just want you to go up, and I want you to create a flow log, so flow logs will track all the, the traffic that is going through through your VPC. Okay, and so it's just nice to know how to create that. So we can have it to accept reject, or all I'm going to set it to all and it can either be delivered to cloudwatch logs or s3 Cloud watch is a very good destination for that. In order to deliver that, we're going to need a destination log group, um, I don't have one. So in order to send this to a log group, we're going to have to go to cloud watch, okay. We'll just open this up in a new tab here. Okay. And then, once we're here in cloud watch, we're going to create ourselves a new cloud watch log, alright. And we're just gonna say actions create log group, and we'll just call this Bayes your VPC flow logs or VPC logs or flow logs, okay. And we will hit Create there. And now if we go back to here and hit refresh, we may have that destination now available to us. There it is. Okay, we might need an IM role associated with this, to have permissions to publish to cloud watch logs. So we're definitely going to need permissions for that. Okay. And I'll just pop back here with those credentials here in two seconds. So I just wanted to collect a little bit of flow log data so I could show it off to you to see what it looks like. And so you know, under our VPC, we can see that we have flow logs enabled. So now we're done the VPC section, let's clean up whatever we created here, so we're not incurring any cost. So we're gonna make our way over to EC two instances, and you can easily filter out the instances which are in that. That VPC here by going to VPC ID here, and then just selecting the VPC. So these are the three instances that are running and I'm just going to terminate them all. Because you know, we don't want to spend up our free credits or incur cost because of that bash in there. So we'll just hit terminate there and those things are going to now shut down. We also have that VPC endpoint still running. Just double check to make sure your Nat gateway isn't still there. So under the back in the VPC section here, just make sure that you had so and there we have our gateway endpoint there for s3. So we'll just go ahead and delete that. I don't believe it cost us any money, but it doesn't hurt to get that out. Later, hey, this is Andrew Brown from exam Pro. And we are looking at identity access management Iam, which manages access of AWS users and resources. So now it's time to look at I am core components. And we have these installed identities. And those are going to be users groups and roles. Let's go through those. So a user is an end user who can log into the console or interact with database resources programmatically, then you have groups, and that is when you take a bunch of users, and you put them into a logical grouping. So they have shared permissions. That could be administrators, developers, auditors, whatever you want to call that, then you have roles and roles, have policies associated with them. That's what holds the permissions. And then you can take a role and assign it to users or groups. And then down below, you have policies. And this is a JSON document, which defines the rules in which permissions are allowed. And so those are the core components. But we'll get more in detail to all these things. Next, so now that we know the core components are and let's talk about how we can mix and match them. Starting at the top here, we have a bunch of users in a user group. And if we want to on mass, apply permissions, all we have to do is create a role with the policies attached to that role. And then once we attach that role to that group, all these users have that same permission great for administrators, auditors, or developers. And this is generally the way you want to use Iam when assigning roles to users, you can also assign a role directly to a user. And then there's also a way of assigning a policy, which is called inline policy directly to a user. Okay, so why would you do this, or maybe you have exactly one action you want to attach to this user, and you want to do it for a temporary amount of time, you don't want to create a manage role, because it's never, it's never going to be reused for anybody else. There are use cases for that. But generally, you always want to stick with the top level here, a rule can have multiple policies attached to it, okay. And also a role can be attached to certain AWS resources. All right. Now, there are cases where resources actually have inline policies directly attached to them. But there are cases where you have roles attached to or somehow associated to resources, all right. But generally, this is the mix and match of it. If you were taking the ADA security certification, then this stuff in detail really matters. But for the associate and the pro level, you just need to conceptually know what you can and cannot do. All right. So in I am you have different types of policies, the first being managed policies, these are ones that are created by AWS out of convenience for you for the most common permissions you may need. So over here, we'd have Amazon easy to full access, you can tell that it's a managed policy, because it says it's managed by AWS, and an even further indicator is this orange box, okay, then you have custom customer managed policies, these are policies created by you, the customer, there are edible, whereas in the managed policies, they are read only. They're marked as customer managed, you don't have that orange box. And then last are inline policies. So inline policies, you don't manage them because they're like they're one and done. They're intended to be attached directly to a user or directly to a, a resource. And they're, and they're not managed, so you can't apply them to more than one identity or resource. Okay, so those are your three types of policy. So it's now time to actually look at a policy here. And so we're just going to walk through all the sections so we can fully understand how these things are created. And the first thing is the version and the version is the policy language version. If this changes, then that means all the rules here could change. So this doesn't change very often, you can see the last time was 2012. So it's gonna be years until they change it. If they did make changes that probably would be minor, okay, then you have the statement. And so the statement is just a container for the other policy elements. So you can have a single one here. So here I have an array, so we have multiples. But if you didn't want to have multiples, you just get rid of the the square brackets there, you could have a single policy element there. Now going into the actual policy element, the first thing we have is Sid and this is optional. It's just a way of labeling your statements. So Cid probably stands for statement identifier, you know, again, it's optional. Then you have the effect, the effect can be either allow or deny and that's going to set the conditions or the the access for the rest of the policy. The next thing is we have the action so actions can be individualize. Right. So here we have I am, we have an individual one, or we can use asterik to select everything under s3. And these are the actual actions the policy will allow or deny. And so you can see we have a deny policy. And we're denying access all to s3 for a very specific user here, which gets us into the principal. And the principal is kind of a conditional field as well. And what you can do is you can specify an account a user role or federated user, to which you would like to allow or deny access. So here, we're really saying, hey, Barkley, you're not allowed to use s3, okay, then you have the resource, that's the actual thing. That is we're allowing or denying access to so in this case, it's a very specific s3 bucket. And the last thing is condition. And so condition is going to vary based on the based on the resource, but here we have one, and it does something, but I'm just showing you that there is a condition in here. So there you go, that is the makeup of a policy, if you can master master these things, it's going to make your life a whole lot easier. But you know, just learn what you need to learn. So you can also set up password policies for your users. So you can set like the minimum password length or the rules to what makes up a good password, you can also rotate out passwords, so that is an option you have as well. So it will expire after x days. And then a user then must reset that password. So just be aware that you have the ability to password. Let's take a look at access keys. Because this is one of the ways you can interact with AWS programmatically either through the ad vcli, or the SDK. So when you create a user, and you say it's allowed to have programmatic access, it's going to then create an access key for you, which is an ID and a secret access key. One thing to note is that users can only have up to two access keys within their accounts down below, you can see that we have one, as soon as we add a second one, that gray button for create access key will vanish. And if we want more, we would either have to we'd have to remove keys, okay. But you know, just be aware that that's what access keys are. And you can make them inactive and you're only allowed to have let's quickly talk about MFA. So MFA can be turned on per user. But there is a caveat to it, where the user has to be the one that turns it on. Because when you turn it on, you have to connect it to a device and your administrator is not going to have the device. So it's on the user to do so there is no option for the administrator to go in and say, Hey, you have to use MFA. So it cannot be enforced directly from an administrator or root account. But what the minister can do if if if they want is they can restrict access to resources only to people that are using MFA. So you can't make the user account itself have MFA. But you can definitely restrict access to API calls and things like that. So temporary security credentials are just like programmatic access keys, except they aren't temporary. And they're used in the following scenarios, we were dealing with identity Federation, delegation, cross account access, and Im roles. And so we're going to look in more detail at identity Federation and cross account access in the upcoming slides. But let's talk about how they are different for programmatic access keys. The first thing is that they last for minutes to an hour. So they're very short term credentials. And they're not stored with the user. But they're generated dynamically and provided the user when requested. So we provide like access keys, they're strongly linked to a specific user. But in this case, we just give them these credentials when they need them. And these temporary credentials are the basis for roles and identity Federation. So you've actually been using temporary security credentials this entire time, but you probably just don't know it. So anytime you use a role, an IM role, they actually generate out an STS, which is the token that is used for temporary credentials, but you just don't notice it, because eight of us does it for you. But let's dig deep more into this stuff. So we said that the use case for these temporary security credentials would be identity Federation. But let's talk about what that actually is. So identity Federation, is the means of linking a person's electronic identity and attributes stored across multiple distinct identity management systems. So that definition is a little bit confusing. So my take on it is that identity Federation allows users to exist on a different platform. So an example this would be your users or on Facebook, but They can gain access as if they are a user in AWS, the idea is that their identities are hosted somewhere else, whether it's Facebook, Google, Active Directory, or whatever. So with Iam, it supports two types of identity Federation. So we have enterprise identity Federation and web identity Federation. So the protocols you can use for enterprise identity Federation would be SAML, which is compatible with Active Directory, a very popular Microsoft Windows identity system, or custom Federation, brokers. This allows you to connect to other identity systems. And generally with enterprise entity Federation, you're doing things with single sign on, but for the scope of the developer associate, we don't need to really know about that we do need to know about is web identity Federation. And you've used this before, if you've ever clicked a button where it's like connect with Facebook or connect with LinkedIn, to quickly sign into a service, you are using a web identity Federation system. So Amazon has won Facebook has won Google has won, LinkedIn, etc, etc. But generally, the protocols that they adhere to is the open ID Connect, oh, ICD 2.0. And that's actually built off of OAuth two. So you might have heard of OAuth. So Oh, ice, oh, IDC is built on top of OAuth. So. But yeah, so now we have an idea of identity Federation. Let's dig a little bit deeper here. So we've been talking about these temporary security credentials, but actually, how do we get a hold of them. And that's where this service comes into play this security token service STS. So it's a web service that enables you to request temporary, limited privileged credentials for either Im users, or for federated users. And we just talked about identity Federation. So users that are outside of AWS. And so this service is a global service. If you were to go into the AWS console and type in STS, nothing will come up, because you can't access it through the console, you can only access it programmatically. And there's actually an endpoint for it. So there's one that's SDS, Amazon, Ada calm. So that's where all the requests get generated. And an sts returns. So like, once you use the API to get that STS, it's going to return an access key ID secret access key a sessions token, an expiration, you're going to notice, the first two should be very noticeable to you. Because it's the same thing when you do programmatic access with a user. It's the same stuff. And in fact, you can take this, the access key ID and secret access key and put it in your database credentials file, and make a profile. Of course, these will be temporary. So it's not great for long term storage. But the point is, is that they're, they're exactly the same, it's just that they're temporary. And if you want to actually get this token, you have to use the API actions for STS, which you could use either the SDK or the COI to use. And so this is a list of them, the top three are the ones that are most likely in use. So we have assume role that's used any time I am role gives you an STS, remember, we talked about AWS, every time you use I am, it gives you an sts B don't see it, it uses a sumrall. Same with cross account roles, it uses assume role, then you have SAML, that's for the enterprise identity Federation. And then you have assume role with web identity, which is the one we really need to know for the developer associate. And that's authenticating to Facebook, Google, etc, etc. So let's look at that in more detail and see how that works. So now let's look at actually how to get an sts from using with assume role with web identity. So this action here is going to return a set of temporary credentials for the usage of an authorized in a mobile or web application. So you have a developer, and the first thing you're going to do is you need to authenticate with whatever the web identity is. So we're gonna send an OAuth call to Facebook, as an example here. And they're gonna send us back a JavaScript web token, a JW T. And then once we have that, what we can do is use something like the CLR. You can also use the AWS SDK, but we'll use a COI and we're going to call the assume role with web identity passing along that JW T. And then the STS service is going to determine whether it's going to give us a token or not. And so there it passes along the temporary credential, so we're going to get that information. So now that we have those temporary credentials, We can use the COI or other means to gain access to a variety of AWS services within our AWS account, and just whatever we decide that they're allowed to gain access to. So that is the process for getting sts for at least the assume role with web identity. What's really important to note is the order that this happens, and because this definitely shows up on the exam, they'll ask you like, they'll give you an example of the order of When this happens, and just know that you always authenticate with the web ID first, and then you get the token second, from STS. So just remember that it's always the web identity first, and you'll definitely score points on the exam. So I was saying earlier that cross account roles are another thing that is used with sts, the security token service. So let's talk about why would we want to make cross account roles. So the whole purpose of them is to allow you to grant access for a user from another Eva's account to resources within your own account. And the advantage here is that you don't have to create your own, like a user for them in your own account. So here's an example where we have account a so I have an account and that one, and account B wants to grant me access to specific resources, what they're going to do is create a role for me, which is a cross account role. And this is going to grant me access. So how do how does this role actually do that? Well, there's a policy that's attached to it. And to that role, and it's granting me access to assume role. And so we said that sts the security token service is what issues those temporary credentials. And we saw there was a bunch of actions, one being web identity, assume role, the other one being assumed role. So assume role is what is granting us access across count. And this happens seamlessly, so you don't have to do anything. But yeah, that is cross account roles. Hey, this is Angie brown from exam Pro, and we are going to do the I am follow along. So let's make our way over to the IM console. So just go up to services here and type in Im, and we will get to learning this a right right away. So here I am on the IM dashboard. And we have a couple things that a device wants us to do. It wants us to set MFA on our root account. It also wants us to apply an IM password policy, so that our passwords stay very secure. So let's take what they're saying in consideration and go through this. Now I am logged in as the root user. So we can go ahead and set MFA. So what I want you to do is drop this down as your root user and we'll go manage MFA. And we will get to this here. So this is just a general disclaimer here to help you get started here. I don't ever want to see this again. So I'm just going to hide it. And we're going to go to MFA here and we're going to activate MFA. So for MFA, we have a few options available. We have a virtual MFA, this is what you're probably most likely going to use where you can use a mobile device or computer, then you can use a you two f security key. So this is like a UB key. And I actually have an OB key, but we're not going to use it for this, but it's a physical device, which holds the credentials, okay, so you can take this key around with you. And then there are other hardware mechanisms. Okay, so but we're going to stick with virtual MFA here. Okay, so we'll hit Continue. And what it's going to do is it's going to you need to install a compatible app on your mobile phone. So if we take a look here, I bet you authenticator is one of them. Okay. So if you just scroll down here, we have a few different kinds. I'm just looking for the virtual ones. Yeah. So for Android or iPhone, we have Google Authenticator or authy, two factor authentication. So you're going to have to go install authenticator on your phone. And then when you are ready to do so you're going to have to show this QR code. So I'm just going to click that and show this to you here. And then you need to pull out your phone. I know you can't see me doing this, but I'm doing it right now. Okay. And I'm not too worried that you're seeing this because I'm going to change this two factor authentication out here. So if you decide that you want to also add this to your phone, you're not going to get too far. Okay, so I'm just trying to get my authenticator app out here. And I'm going to hit plus and the thing and I can scan the barcode, okay. And so I'm just going to put my camera over it here. Okay, great. And so is is save the secret. All right, so it's been added to Google Authenticator. Now, now that I have it in my application, I need to enter in to two consecutive MFA codes. Okay, so this is a little confusing, it took me a while to figure this out the first time I was using AWS, the idea is that you need to set the first one. So the first one I see is 089265. Okay, and so I'm just going to wait for the next one to expire, okay, so there's a little circle that's going around. And I'm just waiting for that to complete to put in a second one. It just takes a little bit of time here. Still going here. Great. So I have new numbers. So the numbers are 369626. Okay, so it's not the same number, but it's two consecutive numbers, and we will hit assign MFA. And now MFA has been set on my phone. So now when I go and log in, it's going to ask me to provide additional code. Okay, and so now my root account is protected. So we're gonna go back to our dashboard, and we're going to move on to password policies. Okay. So let's take the recommendation down here and manage our password policy, okay. And we are going to set a password policy. So password policy allows us to enforce some rules that we want to have under users. And so to make passwords a lot stronger, so we can say it should require at least one upper letter, one lowercase letter, or at least one number, a non non alphanumeric character, enable the password expiration. So after 90 days, they're going to have to change the password, you can have password expiration requires the administrator reset, so you can't just reset it, the admin will do it for you allow users to change their own password is something you could set as well. And then you could say prevent password reuse. So for the next five passwords, you can't reuse the same one. Okay? So and I would probably put this a big high numbers, so that a very high chance they won't use the same one. Okay, so, um, yeah, there we go. We'll just hit Save Changes. And now we have a password policy in place. Okay. And so that's, that's how that will be. So to make it easier for users to log into the Iam console, you can provide a customized sign in link here. And so here, it has the account ID, or I think that's the account ID but we want something nicer here. So we can change it to whatever you want. So I can call it Deep Space Nine. Okay. And so now what we have is if I spelt that, right, I think so yeah. So now that we have a more convenient link that we can use to login with, okay, so I'm just going to copy that for later, because we're going to use it to login. I mean, obviously, you can name it, whatever you want. And I believe that these are just like, I'm like picking like your Yahoo or your your Gmail email, you have to be unique. Okay, so you're not gonna be at least Deep Space Nine, as long as I have to use I believe. But yeah, okay, so maybe we'll move on to actually creating a user here. So here I am under the Users tab, and I am and we already have an existing user that I created for myself, when I first set up this account, we're going to create a new user so we can learn this process. So we can fill the name here, Harry Kim, which is the character from Star Trek Voyager, you can create multiple users in one go here, but I'm just gonna make one. Okay, I'm going to give him programmatic access and also access to the console, so you can log in. And I'm gonna have an auto generated password here, so I don't have to worry about it. And you can see that it will require them to reset their password when they first sign in. So going on to permissions, we need to usually put our users within a group, we don't have to, but it's highly recommended. And here I have one called admin for admin, which has add administrator access, I'm going to create a new group here, and I'm going to call it developers okay. And I'm going to give them power access, okay, so it's not full access, but it gives them a quite a bit of control within the system. Okay. And I'm just going to create that group there. And so now I have a new group. And I'm going to add Harry to that group there. And we will proceed to the next step here. So we have tags, ignore that we're going to review, we're going to create Harry Kim the user. Okay. And so what it's done here is it's also created a secret access key and a password. Okay, so if he wants that programmatic access, he can use these and we can send the an email with the, with this information along to him. Okay. And, yeah, we'll just close that there. Okay, and then we'll just poke around here and Harry Kim for a little bit. So just before we jump into Harry Kim here, you can see that he has never used his access key. He, the last time his password was used was today, which was set today, and there is no activity and he does not have MFA. So if we go into Harry Kim, we can look around here and we can see that he has policies applied to him from a group and you can also individually attach permissions to him. So we have the ability to give them permissions via group or we can Copy permissions from existing user or we can attach policies directly directly to them. So if we wanted to give them s3, full access, we could do so here. Okay. And then we can just apply those permissions. So just to wrap up this section, we're just going to cover a rules and policies here. So first, we'll go into policies. And here we have a big list of policies here that are managed by AWS, they say they're managed over here, and you can definitely tell because they're camelcase. And they also have this nice little orange box, okay. And so these are policies which you cannot edit, they're read only, but they're a quick and fast way for you to start giving access to your users. So if we were just to take a look at one of them, like the EC 214, or maybe just read only access, we can click into them. And we can see over onto the I am cheat sheets, let's jump into it. So identity access management is used to manage access to users and resources I am is a universal system, so it's applied to all regions at the same time, I am is also a free service. A root account is the account initially created when AWS is set up and so it has full administrator access. New Iam accounts have no permissions by default until granted, new users get assigned an access key ID and secret when first created when you give them programmatic access. Access keys are are only used for the COI and SDK, they cannot access the console. Access keys are only shown once when created, if lost, they must be deleted and recreated again, always set up MFA for your root accounts. Users must enable MFA on their own administrators cannot turn it on for each user, I am allows you, you to Set password policies to set minimum password requirements or rotate passwords. Then you have Iam identities, such as users, groups and roles and we'll talk about them now. So we have users, those are the end users who log into the console or interact with AWS resources programmatically, then you have groups. So that is a logical group of users that share all the same permission levels of that group. So think administrators, developers, auditors, then you have roles, which associates permissions to a role, and then that role is then assigned to users or groups. Then you have policy. So that is a Jason document which grants permissions for specific users groups, roles to access services, policies are generally always attached to Im identities. You have some variety of policies, you have managed policies, which are created by AWS, that cannot be edited, then you have customer managed policies, those are policies created by you that are editable and you have inline policies which are directly attached to the user. So there you go, that is I am. Hey, this is Angie brown from exam Pro. And we are looking at CloudFront, which is a CDN a content distribution network, it creates cache copies of your website at various edge locations around the world. So to understand what CloudFront is, we need to understand what a content delivery network is. So a CDN is a distributed network of servers, which deliver web pages and content to users based on their geographical location, the origin of the web page and a content delivery server. So over here, I have a graphical representation of a CDN, specifically for CloudFront. And so the idea is that you have your content hosted somewhere. So here the origin is s3. And the idea is that the server CloudFront is going to distribute a copy of your website on multiple edge locations, which are just servers around the world that are nearby to the user. So when a user from Toronto tries to access our content, it's not going to go to the s3 bucket, it's going to go to CloudFront. And CloudFront is going to then route it to the nearest edge location so that this user has the lowest latency. And that's the concept behind clustering. So it's time to look at the core components for CloudFront. And we'll start with origin, which is where the original files are located. And generally, this is going to be an s3 bucket. Because the most common use case for CloudFront is static website hosting. However, you can also specify origin to be an easy to instance, on elastic load balancer or route 53. The next thing is the actual distribution itself. So distribution is a collection of edge locations, which define how cash content should behave. So this definition, here is the thing that actually says, Hey, I'm going to pull from origin. And I want this to update the cache, whatever whatever frequency or use HTTPS, or that should be encrypted. So that is the settings for the distribution. And then there's the edge locations. And an edge location is just a server. And it is a server that is nearby to the actual user that stores that cache content. So those are the three components to pop. So we need to look at the distribution component of CloudFront in a bit more detail, because there's a lot of things that we can set in here. And I'm not even showing you them all, but let's just go through it. So we have an idea, the kinds of things we can do with it. So again, a distribution is a collection of edge locations. And the first thing you're going to do is you're going to specify the origin. And again, that's going to be s three, EC two lb, or refer D three. And when you said your distribution, what's really going to determine the cost and also how much it's going to replicate across is the price class. So here, you can see, if you choose all edge locations, it's gonna be the best performance because your website is going to be accessible from anywhere in the world. But you know, if you're operating just in North America and the EU, you can limit the amount of servers it replicates to. There are two types of distributions, we have web, which is for websites, and rtmp, which is for streaming media, okay, um, you can actually serve up streaming video under web as well. But rtmp is a very specific protocol. So it is its own thing. When you set up behaviors, there's a lot of options we have. So we could redirect all the traffic to be HTTPS, we could restrict specific hv methods. So if we don't want to have puts, we can say we not include those. Or we can restrict the viewer access, which we'll look into a little bit more detail here, we can set the TTL, which is time to expiry, or Time To Live sorry, which says like after, we could say every two minutes, the content should expire and then refresh it right, depending on how, how stale we want our content to be. There is a thing called invalidations in CloudFront, which allow you to manually set so you don't have to wait for the TTL. to expire, you could just say I want to expire these files, this is very useful when you are pushing changes to your s3 bucket because you're gonna have to go manually create that invalidation. So those changes will immediately appear. You can also serve back at error pages. So if you need a custom 404, you can do that through CloudFront. And then you can set restrictions. So if you for whatever reason, aren't operating in specific countries, and you don't want those countries to consume a lot of traffic, which might cost you money, you can just restrict them saying I'm blocking these countries, or or you could do the other way and say I only whitelist these countries, these are the only countries that are allowed to view things from CloudFront. So there's one interesting feature I do want to highlight on CloudFront, which is lambda edge and lambda edge are lambda functions that override the behavior of requests and responses that are flowing to and from CloudFront. And so we have four that are available to us, we have the viewer requests, the origin requests, the origin response, and the viewer response, okay, and so on our on our CloudFront distribution under probably behavior, we can associate lambda functions. And that allows us to intercept and do things with this, what would you possibly use lambda edge for a very common use case would let's say you have protected content, and you want to authenticate it against something like cognito. So only users that are within your cognito authentication system are allowed to access that content, that's just something we do on exam pro for the video content here. So you know, that is one method for protecting stuff. But there's a lot of creative solutions here with you can use lambda edge, you could use it to serve up a to be testing websites, so you could have it so when the viewer request comes in, you have a roll of the die, and it will change what it serves back. So it could be it could set up a or set up B and that's something we also do in the exam pro marketing website. So there's a lot of opportunities here with lambda edge. I don't know if it'll show up in the exam, I'm sure eventually will. And it's just really interesting. So I thought it was worth talking. So now we're talking about CloudFront protection. So CloudFront might be serving up your static website, but you might have protected content, such as video content, like on exam Pro, or other content that you don't want to be easily accessible. And so when you're setting up your CloudFront distribution, you have this option to restrict viewer access. And so that means that in order to view content, you're going to have to use signed URLs or signed cookies. Now, when you do check this on, it actually will create you an origin identity access and oh AI. And what that is it's a virtual user identity that it will be used to give CloudFront distributions permission to fetch private objects. And so those private objects generally mean from an s3 bucket that's private, right? And as soon as that set up, and that's automatically set up for you. Now you can go ahead and use signed URLs and signed cookies. So one of these things well, the idea behind it is a sign URL is just a URL that CloudFront Provide you that gives you temporary access to those private cached objects. Now, you might have heard of pre signed URLs. And that is an s3 feature. And it's similar nature. But it's very easy to get these two mixed up because sign URLs and pre signed URLs sound very similar. But just know that pre signed URLs are for s3 and sign URLs are for CloudFront, then you have signed cookies, okay. And so it's similar to sign URLs, the only difference is that you're you passing along a cookie with your request to allow users to access multiple files, so you don't have to, every single time generate a signed cookie, you set it once, as long as that cookie is valid and pass along, you can access as many files as you want. This is extremely useful for video streaming, and we use it on exam Pro, we could not do video streaming, protected with sign URLs, because all the video streams are delivered in parts, right, so a cookie has to get set. So that that is your options for protecting cloud. It's time to get some hands on experience with CloudFront here and create our first distribution. But before we do that, we need something to serve up to the CDN. Okay, um, so we had an s3 section earlier, where I uploaded a bunch of images from Star Trek The Next Generation. And so for you, you can do the same or you just need to make a bucket and have some images within that bucket, so that we have something to serve up, okay. So once you have your bucket of images prepared, we're going to go make our way to the CloudFront console here. And so just type in CloudFront, and then click there. And you'll get to the same place as me here. And we can go ahead and create our first distribution. So we're gonna be presented with two options, we have web an rtmp. Now rtmp is for the Adobe Flash Media Server protocol. So since nobody really uses flash anymore, we can just kind of ignore this distribution option. And we're going to go with web, okay. And then we're going to have a bunch of options, but don't get overwhelmed, because it's not too tricky. So the first thing we want to do is set our origin. So where is this distribution going to get its files that wants to serve up, it's going to be from s3. So we're going to click into here, we're going to get a drop down and we're going to choose our s3 bucket, then we have path, we'll leave that alone, we have origin ID, we'll leave that alone. And then we have restrict bucket access. So this is a cool option. So the thing is, is that let's say you only want people to access your, your bucket resources through CloudFront. Because right now, if we go to s3 console, I think we made was data public, right? And if we were to look at this URL, okay, this is publicly accessible. But let's say we wanted to force all traffic through CloudFront. Because we don't, we want to be confident they can track things. So we get some rich analytics there. And we just don't want people directly accessing this ugly URL. Well, that's where this option comes in, restrict bucket access, okay, and it will, it will create an origin identity access for us, but we're gonna leave it to No, I just want you to know about that. And then down to the actual behavior settings, we have the ability to redirect HTTP to HTTPS. That seems like a very sane setting, we can allow these to be methods, we're only going to be ever getting things we're never going to be put or posting things. And then we'll scroll down, scroll down, we can set our TTL, the defaults are very good. And then down here, we have restrict viewer access. So if we wanted to restrict the viewer access to require signed URLs of site cookies to protect access to our content, we'd press yes here. But again, we just want this to be publicly available. So we're going to set it to No, okay. And then down below, we have distribution settings. And this is going to really affect our price. The cost we're going to pay here, as it says price class, okay. And so we can either distribute all copies of our files to every single edge location, or we can just say US, Canada, Europe, or just US, Canada, yeah, Europe, Asia, Middle East Africa, or just the the main three. So I want to be cost saving here. So I'm really going to cost us a lot anyway, but I think that if we set it to the lowest class here that it will take less time for the distribution to replicate here in this tutorial go a lot faster. Okay, then we have the ability to set an alternate domain name, this is important if we are using a CloudFront certificate and we want a custom domain name, which we would do in a another follow along but not in this one here. Okay, and if this was a website, we would set the default route here to index dot HTML. Okay, so that's pretty much all we need to know here. And we'll go ahead and create our distribution, okay, and so our distribution is going to be in progress. And we're going to wait for it to distribute those files to all those edge locations. Okay, and so this will just take a little bit of time here. He usually takes I don't know like three to five minutes so we'll we'll resume the video when this is done creating. So creating that distribution took a lot longer than I was hoping for, it was more like 15 minutes, but I think the initial one always takes a very long time. And then then after, whenever you update things, it still takes a bit of time, but it's not 15 minutes more like five minutes. Okay. But anyway, um, so our distribution is created. Here we have an ID, we have a domain name, and we're just going to click in to this distribution, and we're gonna see all the options we have here. So we have general origins, behaviors, error pages, restrictions, and validations. And tags. Okay, so when we were creating the distribution, we configured both general origins and behaviors all in one go. Okay, and so if we wanted to override the behaviors from before, we just clicked edit here, we're not going to change anything here. But I just want to show you that we have these options previous. And just to see that they are broken up between these three tabs here. So if I go to Edit, there's some information here and some information there. Okay. So now that we have our distribution working, we have this domain name here. And if we had, if we had used our own SSL, from the Amazon certification manager, we could add a customer domain, but we didn't. So we just have the domain that is provided with us. And this is how we're going to actually access our our cache file. So what I'm going to do is copy that there. I'm just going to place it here in a text editor here. And the idea here is we want to then, from the enterprise D pull one of the images here. So if we have data, we'll just take the front of it there, okay. And we are going to just assemble a new URL. So we're going to try data first here, and data should work without issue. Okay. And so now we are serving this up from CloudFront. So that is how it works now, but data is set to public access. Okay, so that isn't much of a trick there. But for all these other ones, I just want to make sure that he has public access here and it is set here yep, to public access. But let's look at someone that actually doesn't have public access, such as Keiko, she does not have public access. So the question is, will CloudFront make files that do not have public access set in here publicly accessible, that's what we're gonna find out. Okay. So we're just going to then assemble on another URL here, but this time with Keiko, okay, and we're gonna see if we can access her All right. Okay, oops, I copied the wrong link. Just copy that one more time. Okay, and there you go. So Keiko is not available. And this is because she is not publicly accessible. Okay. So just because you create a CloudFront distribution doesn't necessarily mean that these files will be accessible. So if we were to go to Keiko now and then set her to public, would she be accessible now through CloudFront? Okay, so now she is all right. So so just keep that in mind that when you create a CloudFront distribution, you're going to get these URLs. And unless you explicitly set the objects in here to be publicly accessible, they're not going to be publicly accessible. Okay. But yeah, that's all there is to it. So we created our CloudFront. So we need to touch on one more thing here with CloudFront. And that is invalidation. So, up here we have this Keiko image, which is being served up by CloudFront. But let's say we want to replace it. Okay, so in order to replace images on CloudFront, it's not as simple as just replacing an s3. So here we have Keiko, right, and this is the current image. And so let's say we wanted to replace that. And so I have another version of Keiko here, I'm just going to upload it here. And that's going to replace the existing one. Okay. And so I'm just going to make sure that the new one is here. So I'm just going to right click or sorry, gonna hit open here, make sure it's set to public. And then I'm just going to click the link here. And it still now it's the new one, right, so here we have the new one. And if we were to go to the CloudFront distribution and refresh, it's still the old image, okay, because in order for these new changes to propagate, you have to invalidate the old the old cache, okay, and that's where invalidation is come into play. So to invalidate the old cache, we can go in here to create invalidations. And we can put a wildcard to expire everything or we could just expire. Keiko. So, for Keiko, she's at Ford slash enterprise D. So we would just paste that in there. And we have now created an invalidation. And this is going to take five, five minutes. I'm not going to wait around to show you this because I know it's going to work. But I just want you to know that if you update something in order, in order for it to work, you have to create a validation. So it's time to look at the CloudFront cheat sheet. And let's get to it. So CloudFront is a CDN a content distribution network. It makes websites load fast by serving cache content that is nearby CloudFront distributes cached copies at edge locations. Edge locations aren't just read only you can actually write to them so you can do puts to them. We didn't really cover that in the core content, but it's good to know. CloudFront has a feature called TTL, which is time to live. And that defines how long until a cache expires. Okay, so if you set it to expire every hour every day, that's how fresh or I guess you'd say how stale your content is going to be. When you invalidate your cache, you're forcing it to immediately expire. So just understand that invalidations means you're you're refreshing your cache, okay? refreshing, the cast does cost money because of the transfer cost to update edge locations, right. So if you have a file, and it's and it's expired, it then has to then send that file to 1020, whatever amount of servers it is, and there's always that outbound transfer cost, okay? origin is the address of where the original copies of your files reside. And again, that can be a three EC two, lb raffa, d three, then you have distribution, which defines a collection of edge locations and behavior on how it should handle your cash content. We have two types of distributions, we have the web distribution, also known as web, which is for static website content. And then you have rtmp, which is for streaming media, again, that is a very specific protocol, you can serve up video streaming via the web distribution, then we have origin identity access, which is used to access private s3 buckets. If we want to access cash content that is protected, we need to use sign URLs or signed cookies, again, don't get signed roles confused with pre signed URLs, which is an s3 feature. But it's pretty much the same in terms of giving you access to something, then you have lambda edge, which allows you to pass each request through a lambda to change the behavior of the response or the request. Okay, so there you go. That is cloud front in a nutshell. Hey, this is Andrew Brown from exam Pro. And we are looking at Cloud trail, which is used for logging API calls between AWS services. And the way I like to think about this service, it's when you need to know who to blame. Okay, so as I said earlier, cloud trail is used to monitor API calls and actions made on an AWS account. And whenever you see these keywords, governance, compliance, operational auditing, or risk auditing, it's a good indicator, they're probably talking about AWS cloud trail. Now, I have a record over here to give you an example of the kinds of things that cloud trail tracks to help you know how you can blame someone when something's gone wrong. And so we have the where, when, who and what so the where, so we have the account, Id what, like, which account did it happen in, and the IP address of the person who created that request, the lens, so the time it actually happened, the who, so we have the user agent, which is, you know, you could say I could tell you the operating system, the language, the method of making this API call the user itself. so here we can see Worf made this call and, and what so to what service, and you know, it'll say what region and what service. So this service, it's using, I am here, I in the action, so it's creating a user. So there you go, that is cloud trail in a nutshell. So within your AWS account, you actually already have cloud trail logging things by default, and it will collect into the last 90 days under the event history here. And we get a nice little interface here. And we can filter out these events. Now if you need logging be on 90 days. And that is a very common use case, which you definitely want to create your own trail, you'd have to create a custom trail. The only downside when you create a custom trail is that it doesn't have a gooey like here, such as event history. So there is some manual labor involved to visualize that information. And a very common method is to use Amazon Athena. So if you see cloud trail, Amazon, Athena being mentioned in unison, there's that reason for that, okay. So there's a bunch of trail options I want to highlight and you need to know these, they're very important for cloud trail. So the first thing you need to know is that a trail can be set to log in all regions. So we have the ability here to say yes, and now we know region is missed. If you are using an organization, you'll have multiple accounts and you want to have coverage across all those. So in a single trail, you can check box on applied to my entire organization. You can encrypt your cloud trail logs, what you definitely want to do using server side encryption via key management service, which abbreviate is SSE kms. And you want to enable log file validation because this is going to tell whether someone's actually tampered with your logs so it's not going to prevent someone from being able to tamper from your logs. But it's going to at least let you know how much you can trust your logs. So I do want to emphasize that cloud trail can deliver its events to cloudwatch. So there's an option After you create the trail where you can configure, and then it will send your events to cloudwatch logs. All right? I know cloud trail and cloud watch are confusing, because they seem like they have overlapping of responsibilities. And there are a lot of Ada services that are like that. But you know, just know that you can send cloud trail events to cloudwatch logs, not the other way around. And there is that ability to. There are different types of events in cloud trail, we have measurement events, and data events. And generally, you're always looking at management events, because that's what's turned on by default. And there's a lot of those events. So I can't really list them all out for you here. But I can give you a general idea what those events are. So here are four categories. So it could be configuring security. So you have attach rule policy, you'd be registering devices, it would be configuring rules for routing data, it'd be setting up logging. Okay. So 90% of events in cloud trail are management events. And then you have data events. And data events are actually only for two services currently. So if you were creating your trail, you'd see tabs, and I assume as one, they have other services that can leverage data events, we'll see more tabs here. But really, it's just s3 and lamda. And they're turned off by default, for good reason. Because these events are high volume, they occur very frequently, okay. And so this is tracking more in detail s3, events, such as get object, delete object put object, if it's a lamda, it'd be every time it gets invoked. So those are just higher there. And so those are turned off by default. Okay. So now it's time to take a quick tour of cloud trail and create our very own trail, which is something you definitely want to do in your account. But before we jump into doing that, let's go over to event history and see what we have here. So AWS, by default, will track events in the last 90 days. And this is a great safeguard if you have yet to create your own trail. And so we have some event history here. And if we were just to expand any of them doesn't matter which one and click a view event, we get to, we get to see what the raw data looks like here for a specific event. And we do have this nice interface where we can search via time ranges and some additional information. But if you need data beyond 90 days, you're going to have to create a trail. And also just to analyze this because we're not going to have this interface, we're gonna have to use Athena to really make sense of any cloud trail information. But now that we have learned that we do have event history available to us, let's move on to creating our own trail. Let's go ahead and create our first trail. And I'm just going to name my trail here exam pro trail, I do want you to notice that you can apply a trail to all regions, and you definitely want to do that. Then we have management events where we can decide whether we want to have read only or write only events, we're going to want all of them, then you have data events. Now these can get expensive, because s3 and lambda, the events that they're tracking are high frequency events. So you can imagine how often someone might access something from an s3 bucket, such as a get or put. So they definitely do not include these. And you have to check them on here to have the inclusion of them. So if you do want to track data events, we would just say for all our s3 buckets, or specify them and lambdas are also high frequency because we would track the invocations of lambdas. And you could be in the 1000s upon millions there. So these are sanely not included by default. Now down below, we need to choose our source location, we're going to let it create a new s3 bucket. For us, that seems like a good choice. We're going to drop down advanced here, because it had some really good tidbits here. So we can turn on encryption, which is definitely something we want to do with kms. And so I apparently have a key already here. So I'm just gonna add that I don't know if that's the default key. I don't know if you get a default key with cloud trail. Usually you'd have one in there. But I'm just going to select that one there. Then we have enable log file validation. So we definitely want to have this to Yes, it's going to check whether someone's ever tampered with our logs, and whether we should not be able to trust her logs. And then we could send a notification about log file delivery. This is kind of annoying, so I don't want to do that. And then we should be able to create our trail as soon as we name our bucket here. So we will go ahead and just name it we'll say exam pro trails, assuming I don't have one in another account. Okay, and so it doesn't like that one. That's fine. So I'm just going to create a new kms key here. Okay. Keys do cost a buck purse, if you want to skip the step you can totally do. So I'm just going to create one for this here called exam pro trails. Okay. Great. And so now it has created that trail. And we'll just use the site here. And then maybe we'll take a peek here in that s3 bucket when we do have some data. Alright, I do want to point out one more thing is that you couldn't set the the cloud watch event to track across all organizations, I didn't see that option there, it's probably because I'm in a sub account. So if I was in my, if you have an alias organization, right, and this was the root account, I bet I could probably turn it on to work across all accounts. So we didn't have that option there. But just be aware that it is there. And you can turn a trail to be across all organizations. So I just had to switch into my route organization account, because I definitely wanted to show you that this option does exist here. So when you create a trail, we have applied all regions, but we also can apply to all organizations, which means all the accounts within an organization, okay. So you know, just be aware of that. So now that our trail is created, I just want you to click into and be aware that there's an additional feature that wasn't available to us when we were creating the trail. And that is the ability to send our cloud trail events to cloud watch logs. So if you wanted to go ahead and do that, you could configure that and create an IM role and send it to a log, or cloud watch log group. There are additional fees apply here. And it's not that important to go through the motions of this. But just be aware that that is a capability that you can do with cloud trail. So I said earlier that this will collect beyond 90 days, but you're not going to have that nice interface that you have an event history here. So how would you go about analyzing that log, and I said, you could use Amazon, Athena. So luckily, they have this link here, that's going to save you a bunch of setup to do that. So if you were to click this here, and choose the s3 bucket, which is this one here, it's going to create that table for you. And Athena, we used to have to do this manually, it was quite the pain. So it's very nice that they they've added this one link here. And I can just hit create table. And so what that's going to do, it's going to create that table in Athena for us. And we can jump over to Athena, okay. And yeah, it should be created here. Just give it a little refresh here, I guess we'll just click Get Started. I'm not sure why it's not showing up here. We're getting the splash screen. But we'll go in here and our table is there. So we got this little goofy tutorial, I don't want to go through it. But that table has now been created. And we have a bunch of stuff here. There is a way of running a sample query, I think he could go here and it was preview table. And that will create us a query. And then we it will just run the query. And so we can start getting data. So the cool advantage here is that if we want to query our data, just like using SQL, you can do so here and Athena, I'm not doing this on a day to day basis. So I can't say I'm the best at it. But you know, if we gave this a try here and tried to query something, maybe based on event type, I wonder if we could just like group by event type here. So that is definitely a option. So we say distinct. Okay, and I want to be distinct on maybe event type here. Okay. doesn't like that little bit, just take that out there. Great. So there we go. So that was just like a way so I can see all the unique event types, I just take the limit off there, the query will take longer. And so we do have that one there. But anyway, the point is, is that you have this way of using SQL to query your logs. Obviously, we don't have much in our logs, but it's just important for you to know that you can do that. And there's that one button, press enter to create that table and then start querying your logs. So we're on to the cloud trail cheat sheet and let's get to it. So Cloud trail logs calls between eight of us services. When you see the keywords such as governance, compliance, audit, operational auditing and risk auditing, it's a high chance they're talking about cloud trail when you need to know who to blame. Think cloud trail cloud trail by default logs events data for the past 90 days via event history. To track beyond 90 days you need to create a trail to ensure logs have not been tampered with, you need to turn on log file validation option. Cloud trail logs can be encrypted using kms cloud trail can be set to log across all Eva's accounts in an organization and all regions in an account. Cloud trail logs can be streamed cloudwatch logs, trails are outputted to s3 buckets that you specify cloud trail logs come in two kinds. We have a management events and data events, management events, log management operations. So you know, attach rule policy, data events, log data operations for resources. And there's only really two candidates hear s3 and lambda. So think get object, delete object put put object did events are disabled by default, when creating a trail, trail log trail logs in s3, and can be analyzed using Athena, I'm gonna have to reword that one. But yeah, that is your teaching. Hey, this is Andrew Brown. And we are looking at Eva's cloud formation, which is a templating language that defines AWS resources to be provisioned, or automating the creation of resources via code. And all these concepts are called infrastructure as code which we will cover again in just a moment here. So to understand cloud formation, we need to understand infrastructure as code because that is what cloud formation is. So let's reiterate over what infrastructure is code is. So it's the process of managing and provision computer data centers. So in our case, it's AWS, through machine readable definition files. And so in this case, it's cloudformation, template YAML, or JSON files, rather than the physical hardware configuration or interactive configuration tools. So the idea is to stop doing things manually, right. So if you launch resources, AWS, you're used to configuring in the console all those resources, but through a scripting language, we can automate that process. So now let's think about what is the use case for cloud formation. And so here, I have an example, where let's pretend that we have our own minecraft server business, and people sign up on our website and pay a monthly subscription, and we will run that server for them. So the first thing they're gonna do is they're gonna tell us where they want the server to run. So they have low latency and what size of servers so the larger the server, the more performant the server will be. And so they give us those two inputs. And then we somehow send that to a lambda function, and that lambda function triggers to launch a new cloudformation stack using our cloud formation template, which defines, you know, how to launch that server, that easy to instance, running Minecraft and a security group and what region and what size. And when it's finished creating, we can monitor maybe using cloud watch events that it's done, and using the outputs from that cloudformation stack, send the IP address of the new micro server to the user, so they can log in and start using their servers. So that's way of automating our infrastructure. So we're going to look at what a cloudformation template looks like. And this is actually one we're gonna use later on to show you how to launch a very simple Apache server. But confirmation comes in two variations. It comes in JSON, and YAML. So why is there two different formats? Well, JSON just came first. And YAML is is an intent based language, which is just more concise. So it's literally the same thing, except it's in that base. So we don't have to do all these curlies. And so you end up with something that is in length, half the size. Most people prefer to write YAML files. But there are edge cases where you might want to use JSON. But just be aware of these two different formats. And it doesn't matter which one you use, just use what works best for you. Now we're looking at the anatomy of a cloud formation template. And so these are made up of a bunch of different sections. And here are all the sections listed out here. And we'll work our way from top to bottom. And so the first one is metadata. So that allows you to provide additional information about the template, I don't have one of the example here and I rarely ever use metadata. But you know, it's just about additional information, then you have the description. So that is just describing what you want this template to do. And you can write whatever you want here. And so I described this template to launch a new instance running Apache and it's hard coded work for us East one, then you have parameters and parameters is something you can use a lot, which is you defining what inputs are allowed to be passed within this template at runtime. So one thing we want to ask the user is what size of instance type Do you want to use. It's defaulted to micro, but they can choose between micro and nano. Okay, so we can have as many parameters as we want, which we'll use throughout our template to reference, then you have mappings, which is like a lookup table, it maps keys to values so you can change your values and something else. A good example of this would be, let's say you have a region, and for each region, the image ID string is different. So you'd have the region keys mapped to different image IDs based on the region. So that's a very common use for mappings. Then you'd have conditions these are like your if else statements within your template. Don't have an examples here, but that's all you need to know. Transform is very difficult to explain if you don't know what macros are, but the idea it's like applying a mod to The actual template, and it will actually change what you're allowed to use in the template. So if I define a transform template, the rules here could be wildly different, different based on what kind of extra functionality that transform adds, we see that with Sam, the serverless application model is a transform. So if you ever take a look at that, you'll have a better understanding of what I'm talking about there. Then you have resources, which is the main show to the whole template, these are the actual resources you are defining that will be provisioned. So think any kind of resource I enroll easy to instance, lambda RDS, anything, right. And then you have outputs is, it's just what you want to see as the end results. So like, when I create the server, it's we don't know the IP address is until it spins it up. And so I'm saying down here, get me the public IP address. And then in the console, we can see that IP address, so that we don't have to, like look at the easy to console pull it out. The other advantage of outputs is that you can pass information on to other cloud formation templates, or created like a chain of effects because we have these outputs. But the number one thing you need to remember is what makes a valid template. And there's only one thing that is required, and that is specifying at least one resource. All these other fields are optional, but resource is mandatory, and you have to have at least one resource. So if you're looking for a cloudformation templates to learn, by example, Eva's quickstarts is a great place to do it, because they have a variety of different categories, where we have templates that are pre built by AWS partners and the APN. And they actually usually show the architectural diagram, but the idea is you launch the template, you don't even have to run it, you can just press a button here and then actually see the raw template. And that's going to help you understand how to connect all this stuff together. Because if you go through the IRS documentation, you're going to have to spend a lot of time figuring that out where this might speed that up if this is your interest. So I just wanted to point that out for you, it's not really important for the exam, it's not going to come up as an exam question. It's just a learning resource that I want you to. Let's talk about stack updates. So the idea is that you have a cloudformation template and you deploy it. And now you need to make some kind of change. And so you might think, Well, alright, I have to delete the entire cloudformation template, and then re upload recreate the entire stack. But that's not the case, because of the cloud formation, you can just modify your existing cloudformation template, push that stack to update and then cloudformation is going to intelligently change, delete, reconfigure your resources series to doing the least amount of work to make those changes, and you're not doing the most destructive path possible. There are two ways to perform stack updates in cloud formation. First, we have direct update, and this is very straightforward. The idea is you're going to directly upload your template to cloudformation, you could use the COI for this. And then it's going to just immediately deploy. So cloud formation is just going to go ahead and apply that directly to your existing stack. And this is super fast to do. The other method is using change sets. So the starting process is the same, you're going to upload your template to cloud formation. But what's going to happen is that a change set is going to be generated all that is it's just a way of showing you the difference between what the current state of the stack is and what the what changes will be made. And the idea is that it gives you an opportunity to audit or review what gets changed. And so in order for those changes to take effect, a developer has to manually confirm saying yes, I'm happy with these changes, go ahead and do it. Okay. So you know, that's the two methods for stack updates. So we were saying that stack updates are intelligent and cloudformation figures out what should be performed, whether it's just configured, reconfigure it or recreate that resource. So let's talk about the circumstances or the actions that cloudformation could take during an update on a resource. And so the first one is update with no interruption. So imagine you have any CPU instance, and you just need to have something changed on it like a security group or something. So the idea is that this update will be performed without affecting the operation of the actual service. So the availability, the service will still remain, the physical ID will not change. So for EC two you actually have like an ID for it. And maybe here when they say physical ID that could also mean like the Amazon resource name will not change. So you know, it's just this is just a configuration has taken effect. The next case is where we have updates with some interruption. So there could be cases where we don't need to destroy the server, but we need to maybe disappear shared it with a load balancer and then re associate it or same thing with an auto scaling group. But because that happens, there is a chance for the service to experience downtime on availability, but the physical ID is going to remain the same, then the third cases where replacement has to occur. So there's no way around it, the only way is to create a new new instance, or delete the old and make a new one. A good example is is launch configurations, launch configurations cannot be modified, they can only be created and cloned. And so in this case, you're getting a new resource. And that new resource is going to have a new physical ID. So those are the three cases. So let's talk about preventing stack updates, because it's possible in certain circumstances, you don't want there for an instance to be replaced, because let's say you had an RDS database, and it would the action that would be taken that would replace the database, you would that would result in data loss. So you say no, I don't want that updated. Or it could be that you have a critical application, and there's certain ecsu instances that cannot be interrupted. So what you can do is create a stack policy. And specifically say that, you know, you're not allowed to do an update replace on this dynamodb table, and then all other actions are allowed. So that's a great way to just make sure that critical resources are not affected by these stack updates. So let's take a look here at cloudformation nested stacks, and nested stacks allow you to reference cloudformation templates inside of another cloudformation template. And the advantage that we're going to get here is we're gonna be able to create modular templates, so we're gonna get reusability. And we're going to be able to assemble larger templates. So that's going to reduce complexity of managing all these templates. And so just to get like a hierarchy kind of view, you have at the top here, this route, stack, and then underneath, you can nest stacks within two stacks. And you can see we can go down multiple levels. So it's not just one level down. But to understand, like, who can access what, generally, we have nested stacks can access anything from their immediate children. But the root stack is accessible by everybody. So no matter where the stack is, anything that's defined up there is accessible everywhere. And just to show you like how you would use a nested stack. So here I have one versus my stack, and then what we're gonna do is define that type a cloudformation, stack, and then we're just going to have to provide access to that template. And you're gonna want to store that template in s3 and serve it up from there. So there you go. So let's take a look at drift detection in cloud formation. And to understand this, we need to first understand what is drift. So drift is when your stacks actual configuration differs, so has drifted by what cloudformation expects it to be. So why does drift happen? Well, when developers start making ad hoc changes to the sack, a good example is they go ahead and delete something. So maybe you provisioned a bunch of stuff in cloud formation. And it created an easy to instance. So you didn't need any more. So the developer deleted it. But when you go back to the cloud formation console, it still says it's there, but you know that it's been deleted, and that can cause complications, or it just doesn't give you good visibility in terms of, you know, what resources you have and what state they are in. And so what the developer should have done as they should have went and update that cloudformation template and let the cloud formation delete that resource. So cloud formation has this feature called detect drift. And it does is what we've been talking about here, which is it determines whether something's been deleted or it has been modified. And so all you got to do in your cloudformation stack, there is this drop down, turn on detect drift, and then you can view the results. Now, I do want to mention about nested stacks and drift drift detection. So when you are detecting drift on a stack confirmation does not detect drift on any nested stacks that you belong to and said you can initiate a drift detection operation directly on those nested stacks. So you have to turn it on for all the individual stacks, it's not going to trickle down to every single thing in your hierarchy there. So let's just take a quick peek here what drift detection kinda looks like so the idea is you would turn it on, and it will tell you whether your stack has drifted. And you can see when it last checked to see if it's drifted. These are the possible statuses that your resources could be in. So it could be that it's been deleted, it's been modified. It's in sync, meaning everything is good, and not checks lose cases where it just cloudformation hasn't checked it. In fact, if you first launch your or you turn on drift detection, you have to wait till that check has happened so all of them will say not checked. And just to take a look at what that looks like. Here I have a bunch of resources. And on the right hand side, you can see there's some that have been deleted. There's ones that are in sync, and there are ones that are modified. So there you go. Let's just talk about rollbacks here with cloud formation. So when you create, update or destroy a stack, you could encounter an error. And if you encountered and, like an example of an error could be like your cloudformation template has a syntax error, or your stack is trying to delete a resource which no longer exists. And so now it has to roll back to get it back into the previous state. That's the whole point of rollbacks. So roll backs are turned on by default. You can ignore rollbacks by using this command flag. So you say ignore rollback, if you are triggering this vcli. I don't know if you can do it via the console, I don't think so. And rollbacks can fail. So sometimes you'll have to investigate and change resources are configurations to get it back to the state that you want. Or you might have to use PDF support to resolve that issue on a failed rollback. Now, these are the states that you will see it so when a rollback is in progress, you'll see rollback in progress or rollback succeeds, you'll see update, rollback complete, and when a rollback fails, you'll see update rollback failed. So you know, there you go. So let's take a look at pseudo parameters in cloud formation. So parameters that are predefined by cloud formation, and you do not declare them in your templates use the same way as you would a parameter. So use it use ref to function to access them. So here is an example of us using a predefined parameter. The one here is called AWS, a colon colon region. And so you know, what are these and these are what they are. So we have aid was partition, it was region it was stack ID, stack name, URL suffix. So not to go through all these because it's not that important. But let's go through region, because I think that is a very prominent one, you'll end up using a lot. And so the idea is that, let's say you need to get the current region that this cloudformation template is running in. And so what you can do is, as you were seeing there, we do at this colon colon region, and if this was running US East one, that's what it would return. So that is pseudo parameters. So let's take a look at resource attributes for cloud formation. These are additional behaviors, you can apply to resources in your cloudformation templates, to change the relationships. And you know, just how things happen when you have stack updates or deleting operations. So the first one we want to look at is creation policy. And what this is going to do, it's going to prevent the status from reaching a crate complete until cloudformation receives a specified number of success signals, or the timeout period is exceeded. So on the left hand or right hand side, we can see we expect either three successes, or we have a timeout of 15 minutes, you know, and that's just to make sure that everything has been created has successfully created. So it's just an additional check there that you can put it in there, then you have deletion policy. And so this is going to happen when you are deleting something. So let's say you are you have resource like an RDS database, and you want to make sure that anytime it's deleted takes a snapshot. So that's what you could do. I think in most cases, you're going to want to retain that database, you generally do not want to delete your database, but it's going to depend on the situation. So you have delete, retain and snapshot. The next one is update policy. And this is only for SG Alaska cash domain, and lambda aliases, and it's just whether the handle is going to get replaced. And so you got to believe you have to say yes, or like true or false. And that's all there is to it, then you have update replace policy. So this is when you're doing a stack update. And it's the question of what's going to happen when that stack update occurs, are you are you gonna delete the resource, retain it or take a snapshot. So it's kind of similar deletion policy, but it's when resources are being replaced. The last case here is depends on. And so this is when you have a resource that's dependent on another resource, so you want that resource created first. So in our scenario here, we have an RDS database, and easy to instance. And we're saying before you make these two instance, go make the RDS database first. And so yeah, those are the resource attributes we can. So let's take a look here at intrinsic functions for cloud formation. And what these do is allow you to assign values to properties that are not available until runtime and the two most popular ones are all the way to the bottom there. And that is reference and get attribute and these are so important that we're going to cover them shortly here in it in other slides, but let's just talk about them quickly. Here. So reference is going to return the value of a specified parameter or resource. And get attribute will return the value of an attribute from a resource in the template. That doesn't make sense right now, don't worry, we're going to cover it here shortly. So let's go to the top of the list and just see what kind of stuff that we can do with intrinsic functions. The first one is basics before. So this returns the base 64 representation of the input string. There's just some cases where you need them to be base 64. I can't think of anything off the top of my head, I've definitely had to use this before. But yeah, there's cases for that, then you have cider. So cider, it returns an array of cider address blocks. When you're working with VPC resources, it needs to be in the cider format. So that's when you'll be using that function, then you have conditioned functions. And so here we have n equals, if not, and or so this is going to allow us to have a bit more logic within our cloudformation template. So if you want to add that kind of stuff, that's what you got there. Then you have find in map, find a map is used with the mapping section. So whenever you're doing a mappings, you're going to definitely be using find find in map and, as the name implies, is trying to find a corresponding value to a key. So that's what that is for, then you have transform transform is super interesting. And it's used with Sam. So the serverless application model, which we definitely cover in another section, actually right after cloudformation. That's what we look into next year. And what it does is it performs a macro on part of the stack on cloudformation. So what essentially does is changes the logic of how you can actually write cloud formation templates, giving you access or extending the ability of cloud formation to do things that it couldn't do before, then you have get azs, this is going to return a list of availability zones for a specified region, then you have import value. This is really important when you're working with nested stacks. So this returns the value of an output exported by another stack. So it's kind of a way to have stacks talk to each other, then you have join. So if you have an array, and you want to turn it into a string, where they're delimited, by comma, you'd use join, then you have select. So let's say you have an array and you want to select an object of that list by providing an index you do select, then you have split split is the opposite of join. So you have a string that might be delimited by commas, and you want to turn that back into array and use that and you have substitute this is where you can substitute a variable in an input string with another value. So generally, you know, replacing a part of a string. And those are intrinsic functions. Alright, so let's take a look at reference in closer detail here. So reference short for ref returns different things for different resources. And you'll need to look up each resource in the database docs to figure out what it's returning, whether it's an Arn, or resource name or physical ID. And so here's an example. And we have a parameter for address VPC. And then we are accessing it down below. And so this example is actually for parameter. It's not for resource. So for this one, it's very straightforward for parameters, but resources is a totally different thing. But I want you to know that if there's something you can't get with a reference, then it's good chance that you can get it with get attribute. So let's go take a look at get attribute now. So here we're taking a look at get attribute. And this allows you to access many different variables on a resource. And you know, sometimes there's lots sometimes there's very few. But you'll again have to check the database doc to see what's available per resource. So here in our example, we have a security group at the top. And then down below, you can see we are referencing that resource with get attribute and then we're getting the group ID. But yeah, that is get attribute but let's hop over to the docks and take a look. Alright, so here I am on the agents resource and property types reference. And this is going to help us see what what we can return with reference or get attribute. So let's take a look at EC two. That's always a good one to compare here. So I'll open up a new tab and maybe we'll look at dynamodb. So to figure out what they return, and when there's a lot of options for easy to so we'll go easy to instance. And for dynamodb we'll go down in the DB table. But there's a lot of stuff here which tells you how to work with EC two instance, but we just care about reference. And the fastest way to find is go to get a trip, because references right above it. And we will go over here and do the same thing. Oops, we'll type in get a trip. And so there it is. So here for easy to win, you pass the logical ID of the resource intrinsic function. So this returns the ID. So that's what it returns. Okay. And then for Dynamo dB, if you go over here, it's going to return the resource name. So there's a bit of difference there but like, it's confusing because sometimes with some resources, the ref will actually return the Arn, but in the case where dynamodb doesn't return the Arn you got to use Get a trip to get it. And then with easy to instance, these are the get the trips that you have with it. So that's pretty much what you have access to. And you know, just be aware of that and just realize that there isn't consistency across the board here. And you'll have to do a bit of digging to figure it out every single time. So let's take a look at weight conditions. So weight conditions, as the name implies, waits for a condition. And there are two use cases where you're going to be using weight conditions. The first is to coordinate stack resource creation with configuration actions that are external to the stack creation. So what do I mean by that? Well, it just like matters of whether you're dependent on something outside of your stack. So maybe you have to make sure that this domain exists. And it's not part of your cloudformation template. Or maybe you have to go hit an external API endpoint to make sure that something is working. So it's just something that is external, and nothing about as you're spinning up within your stack. And the second case is to track the status of a configuration process. So maybe, you know, you have a resource, and you have to have that resource in a particular state. And so you're going to pull and continuously check to see whether it's in that state before proceeding forward. So those are your two use cases. And so here on the right hand side, we actually have a example of a wait condition and this is for an auto scaling group. This is actually pulled from the Ito's docs. And it's kind of funny, because eight of us does not recommend it actually, I think that's an expression. But wait conditions are, are similar to creation policies. But Avis recommends using creation policies for ESG and EC two, but here they're using ESG. But there could be a use case for using you for ESG. So we can't say that you can't use it without auto scaling group. But if you look on the right hand side, you see you have a weight handle. And then you have something that it depends on this depends on the web server group. And then you have a timeout, and you reference the web server capacity. So there's a lot of different options here, you'd have to read up the docs on it. But the takeaway is that a creation policy waits on the dependent resource and awake condition waits on the wait condition, and generally, for something external from your stack. So hopefully, that makes sense. This is Andrew Brown from exam Pro. And welcome to the cloud formation follow along where we're going to learn how to write a cloud formation template. And go ahead and deploy an easy to instance all using the power of infrastructure as code. Now you're going to need a cloud nine environment to do this. You could do this locally on your computer, but it's a lot easier to do everything through here. And if you want to how to get started cloud nine, we show it in multiple fall Long's. But in this particular case, we were showing how to set it up in Elastic Beanstalk follow along. So I would recommend doing that first before doing this follow along here, or just figure out how to get a cloud nine environment set up. But once you're in here, and you have a cloud nine environment, what I'm going to do is I'm going to make a new directory to store this cloud formation file and then a called MK dir cfn project to create that there, then we're going to need a new file in there. So I'm just going to touch a new file, I'm going to call it template dot yamo. It's important that you name it, y m l instead of y ml, it's just more consistent here. If you look up based on what is the best practices between the two, it just the whole community agrees to just do the full form here. So let's make our lives easier. And just always do dot YAML. Now that we have that file there, we can go ahead and open that up. So I'm just going to open that up. And of course, you can navigate here and click on it as well. And the first thing we need to specify is the template format version. So that is AWS template format version, version here. And that's going to be 2010. Oh 909. And that's this line pretty much says this is this is the version of cloudformation we're going to be used and everything else, like anything that's in this template is affected by this version. This version obviously hasn't changed a long time, since it's nearly 10 years ago. Maybe at some point, they will change it. The next thing we'll want to do is add a description. And we are going to use this little two characters plus to do multi line and YAML. And I'm just going to say this as infrastructure. For study sync, we're not actually deploying the study sync application, just because it's a little bit too much work. But you know, I figured we will just name it that anyway, I change this to two spaces, you can have four spaces, but generally two spaces a lot easier to work with. So that's what I'm going to be doing through this YAML is in database and you have to be very careful with YAML files. Because if you are not using spaces, soft tabs and use hard tabs, you might receive errors. So we have our format version, we have our description and the first thing we need to do is specify an actual resource, I'm gonna type resources. And then we'll name our resource web server. And then it's going to be a type of AWS EC two instance. Now I could wrap this in single quotations or double quotations, but I'm usually pretty good about just turning this into a string. So I like to leave a minute naked, so to speak. In order to launch an EC two instance, we're gonna have to do a couple things. But before we do that, I just want to show you how you would know what to fill in here. If you had to read the docs. So I just typed this here, into the docs. And in cloudformation, we have all these resources here. So if I really wanted to note, you see two, I would go down to DC two, I would read it and I'd say, Okay, well, I want to launch a DC two instance. And this would be the closest matching one. And that's how I would know how to add things to cloud formation template. And here, they give you a full example in JSON and then in YAML, obviously, YAML Dudamel is a lot less verbose. And that's what people prefer to use. When eight of us was first starting out. With cloud formation, it only had JSON. So YAML came later, and it is the preferred way of doing things. Or you can see all the formats but the question is, do we need to include all these these attributes or properties? The answer is no. But you have to figure that out by looking at the actual properties here, it will use a required no required yes or required conditional. So the easiest way to find out what's required, we'll just do conditional, I don't think there's any yeses for EC two. And it says we need an image ID. And we're going to need a security group ID, which is a conditional. Because I would think that if you didn't provide it, it would just automatically include the default. I think that's what it is. And that's what we'll probably do, we just won't include one at this point. So I think really, we just need to include an an instance ID and instance type. Actually, I don't think it says we have to include an instance type. I think that one is defaulted to something. It is defaulted to m one small, and we're going to want to T two micro because we want to save money. So let's make our way back over to here. And now that we've learned a little bit, what we're going to do is type in properties. And that's going to be all the properties for this resource that we're making. And so we saw instance type, I want this to be a T to micro again, I could wrap this in single or double quotations, I just like to leave them naked. And the next thing we're going to need is the image ID. Now the image ID is going to be the AMI ID and ami IDs vary based on region. So in order to get this, we're going to have to make our way over to the EC two console. So type in EC two. And what we'll do is we'll pretend that we're launching a new instance, we're not going to launch an instance, but we'll just pretend as if we are so I'll say launch instance here. And we will get the EC two for North Virginia, make sure you're in US East one, there's gonna be very problematic if you're not consistent about this. And this is what we want to have two versions x86 and 64 bit ARM we want the X 86 version. That's what we're used to using arm is very cool, but not for me today. So I'm going to copy that. And we are going to just paste that in. And that's all we need to launch an EC two instance with cloud formation. So in order to do this, we should probably put this in an s3 bucket, I'm just going to go quickly to the cloud formation wizard to show you the steps that we're going to have to do and why we're going to put that in s3 in a second. So we'll go here, we have a lot of cloudformation templates, I never created these, these are automatically created for me when we were doing other tutorials here, or I was running example applications, but go up to the top here and create a new stack and we'll say new resources standard. And the idea is we're gonna provide a template and we can either provide one from s3, or we can upload our own template, this template is so darn small, we could upload it. But I think it's good to get in the practice of putting it onto s3, because after a certain size, or like when a file gets a certain size and length, you have to store it in s3. And that's where you're going to want it to be because a lot of services expect to find your cloudformation templates there. So what we'll do is we will use the CLA to create a new bucket and store it there. So I'm going to type in AWS s3 API create bucket. We're gonna type bucket here, and I'm going to call it study sink. Andrew, I think I already used that song called end Andrew B. And we're gonna say region, US East one. Now be sure to customize this based on your name or some other value because these bucket names are unique across all AWS. So now that I have that there, we'll hit Enter. And if this is successful, it should give us location back which it did that and that's all great. And now that we have our bucket, what we can do is go ahead and copy this template file to s3. So I'm gonna type in AWS s3 and ocers s3 api in an s3. I don't know why it's like that, but that's just how it is. We'll do environment cfn project template YAML. And then we'll specify the s3 path here. So it's going to be steady sync Andrew B. And then we will do template dot YAML. Oops, oops, oops, oops, look at that. Why ml? I'm already messing things up here. I'll just do that twice there. And I don't care about that other one will that will let it hang around. If I can remember the Delete command, and I cannot, but we'll try it anyway. Maybe I can delete that one there. So that it? Yes, it is, there we go, my memory is doing well. So we now have copied that to cloudformation. And so if we go back to here, we can provide the URL. Now notice this says HTTP s, colon colon. And that is a different URL there. So if we go to s3 here, and we go and see your bucket, we have this one here. If we look at this item, it's going to show us this is the URL. So that's what the URL is, we'll make our way back to cloud formation here, I'm going to hit I'm going to paste that in there. We'll go ahead and hit click off, we can go to the viewer of the designer to check that out. But I don't really care about it. So we'll just go next, we have no parameters, I'm going to name this study sync, we'll hit next. We will leave all of these alone, these all seem good to me. Yep, these are all great. And we'll go ahead and hit next, we'll go all the way down the bottom, and we will hit Create stack. And it's going to have create and progress. And so now we just have to wait, we'll have to hit refresh here a bunch of times. This is going to take however long it takes to spin up an EC two instance, if this fails, it will say rollback in progress. If we have a syntax error, it's totally possible, we could have a very minute error that we missed here because we were typing this all manually. And we'll just wait for this resource to complete. So I think it must, the template must be valid, because it looks like it's been enough and easy to instance. So it's on its way. So we just have to wait for this to create here. So I'll see you here in a little bit. Okay, so our EC two instance is now running here, two out of two checks have passed, there's not much we can do with this instance, because it doesn't have a web server installed. It doesn't have a security group that exposes Port 80. It's using the default. So there's a lot of work we need to do here, let's make our way back to cloud formation. And just take a look at the events. so here we can see the previous events, events analysis create complete, a lot of times when you're waiting around here, you have to hit the refresh button to see these changes, sometimes they'll give you a little blue pill telling you there are available events. So you know, just don't wait around and wait for things to happen. Click, click and see what's going on. So let's get a security group in here because that's the next thing that we're going to need. So what we'll do is we'll make our way back to cloud nine. And we're going to want to add a security group. So if we wanted a security group, it should be under EC two, because it is a C two service and I typed in security group here. There it is. And so this is going to tell us all the information we need to know on setting up a security group. So let's go back here. And we're going to create a new resource. And I'm going to call this one security group. And then it's going to be type AWS EC two security group. And the properties are going to be we're going to need a VPC ID, which if we need that, we're going to have to provide our own variable for that. Right. We could just copy and paste ours in as we did with the image ID here. But I think it's time for us to start making this less brittle. We can give it a group description, which isn't a bad idea. So let's go ahead and do that. I'm going to say open port 80. We want to set an Ingress rule that means inbound traffic. So you have Ingress in was aggress. I know Ingress is inbound so and what we'll do is hit enter their IP protocol, protobuf call, oops. And I'm going to type in TCP. And then we'll just align with the IP there from Port 80. And then we have to Port 80. And then we have cider IP. Colon will in this case, I'll do double quotations just because it has like weird characters. I was thinking I have to do that here. It could be naked, I don't know. Um, forward slash zero. And that says from anywhere on the internet, open up Port 80, which is the common port for just plain old HTTP. I think that looks right. I'm just checking for spelling mistakes. So yeah, we're gonna have to deal with this. Also, I think while we're here, we might as well go ahead and turn this into a A variable, so I'm going to call that image ID. And we're going to go and use some parameters. And the parameters is a great way for us to pass in variables at creation of a stack, or an update of a stack as well. So we define one called image ID, we will default that to the value we just had there, which I no longer have. So I'm just gonna have to go to lunch easy to instance, here quickly, and then grab it again. So that's a great way to default an option, when we know we want something to be something. Alright, it's good to get these a description, because I'm going to prompts us it's going to tell us what they are. So I'll go ahead and do that. I'm gonna say, am I to use eg. I'll just tell them what format it has to be. And this is going to be type string. Then the next one we need here is a VPC ID. And we will give this a description and we'll just say the VPC used by the SG. And successful mistake there, we will call this type string. And we're not going to default this one, we'll pass this one in. And we're going to have to figure out what our VPC ID here in a moment, I'm just double checking this to make sure everything is a Okay, looks good to me. But if we really want to know whether our template is good, we can use a tool called cfn lint, which is a node module. So I'm going to do, I'm gonna just go ahead and install that right now. And this is going to check to see if our application is good hyphen g means it's going to install it globally. And then what we'll do is I'm just going to CD into our directory, which we called cfn. project here. And I'm just going to do cfn, lint, validate and then provide the template name there. And if it's good, it will say it's good. If it's not, it'll error out. And look, it's already complained. So clearly, I've done something wrong. I'm not sure what the problem is, I think it's because I'm has an invalid type security group resource security group hasn't developed type. So I've typed I've typed this wrong. It's supposed to have two colons here. I'll go ahead and save that. And that will validate it. I think we're getting closer at a sec to security group. VPC ID is not a valid property. Okay, um, Oh, you know what? lowercase here? got to be really careful with these characters. Image ID, it's the same problem here. I capitalize the D on the end of it. If I can find it in here, image ID, do you see it? As if you're gonna tell me right. Okay, we'll save that. And I'll just hit up here. And now the templates valid. So I mean, doesn't tell you exactly what the problem is. I mean, you have to hunt it down there a bit and make sense of it. And you'll will have to double check the names of these, these properties. But you know, that's all there is to it. So what we'll do is we will go back here. And now that that's done, I guess what we'll do is we will upload this to cloud or back to s3. But let's automate that process, because it's going to be tiresome to always type in the commands over and over again. So I'm gonna touch a new file and call this update.sh. So we're gonna make a bash command here, we're gonna chmod it, which changes it so that it can be executable. So if we didn't do that, and we tried to execute it, it's not actually a binary won't work, or whatever you want to call, it just will not execute. Now that we have that file, I'm gonna go ahead and open that up, and we are going to supply some information, we're going to tell it where to look for bash, first of all, which is always a great idea, it should already know what to do here. But this is just out of habit, I think you should always do that. And then what we're going to do is we're gonna just type in our, our CI, or ADA COI commands. And that's as simple as it is. So type in template type gamle, we'll do s3 colon slash slash, study sync. And I got three slashes in there. I'm sorry about that. It's a bit hard to see what I'm doing. So I will type in Andrew B, Ford slash template dot YAML. And that looks pretty good to me. We'll go back down here. And now we'll just do dot forward slash update.sh. You could do like I think, sh or whatever or bash but I was just like these the period. That's where you execute bash, bash files on Linux, we'll hit Enter. And that should work. Yep, it gave us output it uploaded at some. So looks like everything's great. So what we can do is we can go back to cloud formation and update the stack. So I'm going to go ahead and do update, we're going to replace the current template and we're going to provide that s3 URL again, I'm gonna make my way over to CloudFront or s3, because I cannot remember that URL. I'm gonna paste that on in there. We'll hit next and It's now going to ask us for that VPC ID. So what we need to do is, go grab it. So I think we have our easy to open here. But we will, an easier way to do this would be to just go back to stacks. Here, click into this one, and we've checked the resources, we can click into the actual instance we're using and get the exact VPC that it's using. Because it could be if you have multiple v PCs, it could be in a different one, it should be the default one, because we didn't specify anything. So we're going to just find it, and there it is, I'm just gonna hit that little Copy button, we'll make our way back to cloudformation. Here, paste that in, fingers crossed, and I think this will work. Nothing new here that we need to choose, we're gonna hit Next, move all the way down the bottom, hit update stack. And there it goes. And so we're just gonna have to wait a little while here hopefully does not roll back on us. But since we lifted fine, the only thing that could mess up is we've got those parameters wrong. So Oh, I think it's already done. I think it was really, really fast, because it didn't have to create a new ECU instance, it's just adding a security group. So let's go to our resources. And now we can see we have a security group and a web server, it's going to close that tab there. We'll leave that open. We'll close that. And we already have it open, we'll close that. So what I'm going to do is check out the security group, I just want to see if it actually set the Port 80 to be open. If we go here to inbound Port 80 is open. That is what we want. Now the question is if we check our web server is a security group associated with it? And the answer is, the answer is no, it's on the default, because we actually never associated it in our cloud nine template. So in here, yeah, we have this we have that we didn't put a property here for the security group. So that's what we're going to need to do next. Because we want to open up that Port 80. When we install our Apache web server to view a web application, so we're going to do is we are going to go ahead and add that property. And I'm just stalling for time, as I look for the line that I have to type in there it is. So what we need to do is add security group security. Group IDs wouldn't be so nice if this auto auto completed. For me, it's like if it actually auto completed all the documentation, that'd be sweet. And what we're going to use is get att, we're going to type in the security group here, security group, dot group ID. And so this, get a tribs is a way of getting a special properties on a resource here, and you're limited to what you can grab. But here we can grab the group ID and that's what we need for this case. So I think that's all we need to do here right now. So let's go ahead and actually update this our, our, our stack, and instead of going through here and click on that and do all that work. Let's update our script here so that we can just automate this further. So I'm gonna go here and type in AWS cloudformation. And it's going to then be update stack, we are going to provide our region which is a great idea, we don't want that to default to anything funny, put a forward slash there, we'll provide the stack name, which should be called study sync. We will provide the template URL which will be this link here. We are going to need to provide those parameters. Oops, parameters. And we are going to supply the parameter key is parameter key and then parameter value very verbose. They just don't want to make anything easy for us v PC ID, we want it to be lowercase. And then we'll do parameter it's going to be value. And that's going to have to be what our VPC ideas. So we'll go back here and we will grab it. There it is. So we'll grab it from there. And we will paste that in there. That all looks good to me. I'm just going to double check more thoroughly here. It's very easy to make a mistake when you're typing all this stuff, but I'm pretty sure that's good. And so now that our script is updated, we can just do update.sh Fingers crossed hopefully works and we have an error oh geez. So the template URL or template body must be specified so we have it right there but we forgot that Ford slash this Ford slash just allows us are backslash backslash leans to the left Ford slash leans to the right. But this allows us to make this multi line we can make this all a single line but that's really messy. So that should fix that issue. We'll hit up here. Oh boy VPC, Id must have values. I guess I named it this. Yeah, I did. And I guess I could lowercase this because we should really be Consistent here, then I'll have to go down here and do that as well. Just to make our lives a bit easier, again, that's a little mistake on my part. And actually, before we update this, we should really be linting. This. So we'll do a cfn, lint validate. template dot YAML, everything's good. I'm gonna just change this to lowercase here. Everything looks good, we'll hit up. We're still having a hard time security group IDs, security group IDs. So where did the linter didn't pick that up? invalid template property resource properties, security group IDs. So I'm just going to search that their security group IDs, Oh, you know what, it's got to be indented. My bad. And it's surprising the linter didn't pick that up. So maybe there's another tool out there that's a little bit better at checking that stuff. And there we go, we know it's created because it gave us back a stack ID, we go back to our cloudformation template and give it a refresh, we can see, we can see that it is updating. If we want to see that status, actually, in cloud nine, we can do it programmatically by typing AWS. cloudformation, describes stacks, stack hyphen, name, and study sync. And I would probably I'll put this as a table because it's a lot information doesn't exist, I typed the wrong. And so there it is, there's all our information, could also get this back as JSON. If we just took that off there. It's showing us the status of it. So it's saying update in progress. So it's kind of like a cool way of taking that up. So update complete, so it's almost done. And we can also monitor it through here. Got to choose what you want to do. So let's delete and progress, I guess, is deleting the web server. And if it's deleting the web server, that means we have to wait a little bit here. So I will see you back here when this is 100%. Done. Okay, and so after a little weight here, it everything's updated, now deleted the web server. And when I was building out this fall long, and I got to the stage, it didn't actually delete it. So clearly, I have made a change. And maybe it's because I was fiddling with the names of the parameters, there are properties that it forced a delete. So that's the thing with cloud formation, it doesn't always delete resources. But in this case, it did decide to wait a little bit longer here. If we go over to EC two, we can see now that it's running. And let's say we wanted to actually go check out whether the security group is the correct one. And it is, let's say we wanted to do this through cloud nine, or I should say the CLR here. And what we could do is get a list of the cloudformation resources. So I'll do ABS cloudformation, list stack resources, stack name, study sync. And here, what we're getting is a list of resources. And in there, we should have the security group. And that's going to give us that SG here, which is what we're going to need to check the next thing and so we'll use AWS EC to describe instances. instance IDs. And we will provide Actually, we don't need the SG we need the the instance because once you have the instance, has a security group on it. That's what we're typing this out for. And I'm going to just output This is a table kind of notice and convention here. Describe stacks describe instances. That's why it's great to work with a COI for a while, you just start to figure things out as you go. But you got to type in right if you want to work. So we'll go back here, hit Enter. And there it is. And so we just looked through this, there should be one called security groups. So we have the instance itself. Script security group, where are you? There it is, and so we can see that it's attached. So there you go. So we definitely know that it's correctly attached. But if we just want to double check ourselves and go here and check the inbound rule, and we see that Port 80 is open. So now we have a web app with Port 80 open, which is great. Now we just need to install our web server. And the way we can do that is through user data. Now, if you're launching an EC two instance, you're just going through the steps here like I am, you get to this part advanced details and you provide the scripting to the user data and that would initially set up whatever you want. That is one way to get that set up. And that's what we're going to do using cloud formation. So we'll go back to our template here. I'm going to go to our template dot YAML. And I'm just going to scroll on down here so I can see what I'm doing. And we're going to add another property to the web server and we're going to call this one user data. And then we're going to add a function called base 64. Because this has to be base 64. That's just how it is and then we will use a sub on that because we need to start Institute, well, we don't really need to substitute in the values. But in case we do, we should just do that out of habit. And this is the developer associate. So you don't need to know all these things in great detail. It's more so at the assess option and the DevOps. So that's why I'm kind of glazing over these things. Because I'd rather you get more practical experience as to whether to actually know exactly how they work. And then we will do EC to user because it always starts you as the root user, and you got to force it to switch to EC to user, then we'll do sudo Yum, install httpd that is Apache. You think they just call it a potty but they don't. And then we'll do start which will start the service. And then we'll do sudo service HTTP enable said if that service is stopped, for whatever reason, it will when the if the server's restarted, it will continue running. I'm going to double check this to make sure this is right. User bin Mr. Bash, looks good to me. I got that right. Young y update. Good. es su ECC user, great sudo yum install. That looks good to me. So everything is great. So that's all we need to do there. And while we're here, we might as well add some outputs. outputs are going to make it easy for us to find value. So we don't have to click around everywhere and make an output called public IP, because I want to get the, the IP of the web server, if you don't mind. And I believe that is typed correctly, I'm just gonna double check here. Yeah, it looks good to me. So what we'll do now, I'm just gonna scroll all the way up here. And what I'm going to do is I'm just going to cfn, or we're going to lint that I'm just gonna hit up till I find that linter says everything is good. But we found out earlier, that's not always the case. And then we'll do our update.sh. Fingers crossed. Yep, that's great. So now, that has been uploaded to s3, and it's doing a stack update, we're gonna make our way back here to cloud formation, I'm going to do refresh. And we can see this update is in progress. So we're gonna have to wait on that. And what we're looking for is that output of the actual IP address, I don't know if it's going to replace the server. in this, in this case, it probably won't. If it did, that'd be great. But he says update in progress. So it's not doing it's not doing a delete in progress for the web server. But anyway, we'll we'll check back here in a moment. So let's see you soon. So our update is done here. And notice that it didn't do a delete in progress, it just updated it. So that's interesting. If we go to output here, now we actually have that IP address, that's gonna make our lives a lot easier, because now we can just place that in there, but it's not working. So that's a bit of a shame. But I actually know why it's not working. It's because it didn't replace the the instance it didn't delete and recreate it, it just updated it in place. And the reason why I know that is the reason that is the core reason why this is not working is that when you use user data, this script only runs the absolutely first time an instance has been launched. And so the only way for this to ever run again would be if you destroyed it and recreate that instance entirely. If you restarted it, it wouldn't run it again. There are ways of adding things here so that it will trigger the trigger restart, I guess, in some cases, or like replace the actual instance. But it didn't. So what we're gonna have to do here in this case is just delete our entire stack. And that's a great chance to see how deletions work. So we'll, what we'll do is go ahead and delete it, hit Delete stack. And what it's going to do, might have to go back a step here, it says delete in progress, it's better to see it on this level, we're gonna see delete in progress, and deletes can actually fail and then roll back. But I don't think this one is going to. So what we'll do is we'll just wait a little while until this is done. And then we'll recreate our stack. And actually, what we'll do while we're waiting here is we'll go to our update script. And we'll change this from update to create stack. And we're not going to run this until this one is done. Because you can't have a stack with the same name. We could just name our new stack a different name, but I want to keep this consistent. Oh, it looks like it's done. So that was pretty darn quick. So that means I don't have to go away. And we can just move on to the next step here. So we change this with create stack, and we will just do an update. And that is going to create our new stack here. We'll do a refresh. There it goes. And we'll go to events. We'll just hit refresh here. And we'll have to wait a little bit because it's launching ecgs. And so I'll see you that. Okay, and so we're back. I don't know if we're gonna have the same IP address here. See this one's 54 227 8467 Nope, it's different. But we'll give that a go. And we'll see now if it actually works. And look at that we got the test page up. So we fully automated something using cloud formation. So that is pretty much the scope of what I have. wanted to show you here. And all we need to do now is just delete what we've created. So what we'll do is we'll do through the CLR, because we already did it through here. So might as well learn how to do see ally. I don't have to take a guess. But I'm pretty sure it's a realist cloud formation, cloud formation, delete, stack, and then stack name. Study sank, I tell you, the more use the COI, you just start guessing and you're pretty much right. So I didn't get any output there. I'm going to go double check here. And I'm going to refresh, see what's going on, since the Delete is in progress. And there we go. So just make sure that this deletes because sometimes sometimes deletes fail, and they roll back and you still have those resources around there. So if you don't want to be paying for something you don't need, just double check that there. But yeah, that's cloud formation for you. So we're all done here. So we are on to the cloudformation cheat sheet. This is a long one. It's like three pages long. super important for the SIS ops and developer associate. So we need to know this inside and out. So let's get into it. When being asked to automate the provisioning of resources, think cloud formation when infrastructure as code is is mentioned, think cloud formation, cloud formation can be written in either JSON or YAML. When cloud formation encourages, encounters an error it will roll back with rollback and progress, cloud formation templates, larger than 51,000 51,200 bytes, or 0.5. megabytes or too large to upload directly, and must be imported into cloudformation via an s3 bucket. nested stacks help you break up cloud formation templates into smaller reusable templates that can be composed into larger templates, at least one resource under Resources must be defined for a cloudformation template to be valid. I'm going to repeat that because it's so darn important in a second here. And let's talk about the the template sections. And you definitely need to know these for the exam, because it'll give you the questions that you have to pick out which section does what. So we have metadata that that is extra information about your template, the description that tells you what your template does, you have parameters, this is how you get user input into the template. Transform applies macros, this is like applying a mod which can change the the anatomy to be custom. And a good example that is Sam, the serverless application model. That's what stands for, then you have outputs. These are values you can use to import into other stacks, you have mappings, these map keys to values just like a lookup table resources to find the resources you want to provision. And here, I'm going to repeat it again, at least one resource is required conditions are whether resources are created or properties are assigned. And those are all the properties. So again, make sure you know are the sections, make sure you know them inside and out because it'll earn you a few points on the exam. onto the second page stack updates can be performed two different ways we have direct updates. And so the way those work, you directly update the stack, you submit changes, and it was cloudformation really deploys on this is what you're going to be used to doing when you're using cloudformation. The other way is executing change sets. So you can preview the change set to cloud formation. Or sorry, you can preview the changes to cloud formation will make to your stack. And that is what we call a change set. It's just telling you what's going to change, then decide whether you want to apply those changes. So it's just like a review process. And then stack updates will change the state of resources based on circumstances. So we have a few we need to know them. So update with no interruption, so updates the resources without disrupting operation and without changing the resources physical ID, then you have updates with some interruptions, updates resources with resource with some interruption retains the physical ID replacement recreates the resource during an update also generates a new physical ID, you can use a stack policy to prevent stack updates on resources to prevent data loss or interruption to serve services. We have drift detection, this is a feature that lets cloudformation tell you when your expected configuration has changed due due to manual override an example of this, let's say you have a cloud formation template or stack that creates a security group and a bunch of other resources. Adult developer comes in and deletes that as G. So cloudformation would think that it's still there, even though it's not in drift detection. If you turn that feature on it's going to tell you that something that it's it's no longer the case. And then on to the last page and this is a long one, we're gonna talk about rollback so occurs when a cloud formation encounters an error when you create an update or destroy a stack. When a rollback is in progress, you'll see rollback and progress. When a rollback succeeds, you'll see update rollback complete rollback fails you'll see update rollback failed then you have sudo parameters are defined parameters. So here we have a ref a double region which would return US East one. So these are like eight of his predefined parameters. Then we have resource attributes. So resource attributes we have a lot of different policies under here we have creation policy. prevents its status from reaching create complete until Ava's confirmation receives a specified number of success signals or the timeout period is exceeded. deletion policy reserve or in some cases backup a resource when it stack is deleted, retain or snapshot update policy how to handle an update for an ASP elastic cache domain or lambda alias. An update replace policy to retain or in some cases backup the existing physical instance for resource when it's replaced during a stack update operation. So we have delete, retain, or snapshot. And then we have depends on the resource is created only after the creation of resource specified in the depends on attribute. And there's some cases where you use depends on there's there's other cases where you use a weight condition. So we have intrinsic functions allow you to assign properties that are not available during runtime, most important to to know would be the ref one returns the values of a specified parameter or resource, get a trip returns the value of an attribute from a resource in the template. And then just some CLR commands you should know you know, the Create stack command because sometimes they'll have a question and they'll actually show you some COI commands. And it's good to know what's what, then you have the update stack one. And the last thing I want to just talk about a service application model is an extension of the cloudformation is cloudformation. To let you define service application, it doesn't get its own cheat sheet because there's just not enough information on it. There's a lot more stuff on cloud formation. But like your if this is for the associates, this is good enough. If you're going for the developer, the DevOps Pro, this is like a six page cheat sheet. So there you go. You know, hopefully this helps you on exam day. Hey, this is Andrew Brown from exam Pro, we're looking at Cloud development kit, also known as cDk, which is a way to write infrastructure as code using an imperative paradigm with your favorite language. So let's get into it. Alright, so to understand cDk, I want to talk about transpilers here for a moment. So a transpiler turns one source code into another, and so cDk transpiled into cloudformation templates. So you know, just a simple diagram, we have a cDk on the left, and that turns into cloud formation templates under the hood. And so this is the difference between an imperative infrastructure and a declarative infrastructure. So let's talk about these two differences. So imperative is when you have something that's implicit, you know, what resources will be created in the end state. And this allows for more flexibility, less, you have less certainty because you don't know exactly what's going to be created. You don't, you don't have fully controller visibility on it. But you generally know what is going to happen. But you get to write less code. And so an example of something being imperative is saying, you know, I want an EC two instance. But you go and fill in all the other details, I just want to tell you that I want to have one, I don't want to have to worry about everything else. And so that is what cDk is it's imperative that when we were looking at decorative on the right hand side here, it's explicit, we know what resources will be created in the end state, there's less flexibility, we're very, very certain of every single little thing that's going to happen at but we have to write a lot more code. And so comparative example to imperative is I want an easy to instance. And I have to tell you exactly every detail of it. And that is what cloudformation is it's declarative by nature. So I said earlier, you could use your favorite language using cDk. And so let's talk about some of the language support it has. So cDk was first available only using TypeScript. And then they eventually started releasing for other languages. So we have node, TypeScript, which again, is just node, Python, Java, and ASP dotnet. So that's all we have. So far, if you're wondering exactly what versions, that is what it supports. I'm still waiting for a Ruby version. And hopefully, you know, when you're watching this video, a Ruby version becomes available. But generally, I think whatever languages is supported by AWS generally is what we'll see. So I would not be surprised if they do PHP, one, and also a Ruby one here. But I don't think you'll get one in PowerShell. Just I want to make a note about how up to date. cDk is with cloud formation. So the cDk API, they may have yet to implement specific API's for eatables resources that are available on cloud formation. It's just because it takes time for them to write this stuff. And they have a lot of languages to support. But it's my best guess that TypeScript would be the one that supports the most AWS resources. And then the other ones would follow behind. I would think Python would be next and then Java and then probably ASP. NET would be last. But just consider that in mind. So that is one of the things you have to think about with cDk which is if you need full control of what cloud front offers, you might have to just use cloudformation templates. So you have to explore there and see what you can do. Hey to Sandra brown from exam Pro, and we are looking at serverless application model, also known as Sam. And this is an extension of cloudformation that lets you define serverless applications. And I always like to think of it as just as a cloudformation. Macro reuse transforms. So let's get to it. So Sam is both a AWS COI tool and also a cloudformation macro, which makes it effortless to define and deploy serverless applications. And you might be looking at that word macro and thinking, what does it mean? So this is the textbook definition, which is a replacement output sequence according to a defined procedure, the mapping process that instantiates a macro use into a specific sequence is known as a macro expansion. Now, that doesn't make a whole lot of sense, at least to me. So I've reworked that definition. And so I would say a macro allows you to change the rules on how code works, allowing you to embed a language within a language macro serve to make code more human readable, or allow you to write less code, Korean language with another language is called a DSL a domain specific language. And by using macros, you're creating a DSL. And so cloudformation allows you to specify macros through the transform attribute. And this is how Sam is use. So what you do is you would use transform and then specify AWS serverless. And then you have access to now these new resource types. That's what this this macros, Sam is injecting into it. So you can define this function as API and simple table along with a bunch of other properties. So to really understand the value of Sam, it's great to put it against cloudformation in a one on one comparison. So what I've done is set up a an example here. So we have API gateway, calling a lambda and that lambda gets data from a database such as RDS. So on the left hand side, this is what it would look like if you wrote it in pure cloud formation. So that's without Sam, and we're 100 lines in and on the right hand side, we have it with Sam. And so this is 5050 lines in so you have at least a 30% reduction in code when writing in SAM. And this is a more verbose example. I think in most cases, you could see 72 to 80% reduction in code just for when you're writing the serverless components. So you can see that Sam is going to save you a lot of time. So let's talk about the SAM COI because I said at the beginning of this Sam section that Sam is both a sea ally and a cloudformation macro. So the SAM COI makes it easy to run packages, deploy serverless applications or lambdas. And these are its COI commands, I don't think you need to learn them all. But it's good to go through them and get some hands on with this kind of stuff. So starting at the top, we have Sam build, and that prepares the lambdas sourced in the code to be deployed by patching for upload. So it doesn't upload it, it just packages it into an artifact. The next one is Sam deploys. So that uploads the lamda package code as an artifact and then deploy. If you're wondering what an artifact is, it's just a fancy word for a zip file. And then the next one here is Sam a net. So if you have yet to start a project, you could run this, it's going to give you a bunch of default folders and files. And it's going to be set up for for a serverless project, I would think that this would be setup for the serverless application repository. So whatever default files you need there, but it would set it up for you, then you have generate event. I think this is for testing, I've never used it. But if you get different payloads for your event sources, then you have invoke, which runs a single lambda locally, then you have start API that runs your serverless application locally for quick development, then you have start lambda, which is very similar to invoke. But looking at the two I would probably say you'd more likely want to run start lambda than invoke. So that is another one there, then you'd have logs which fetches, I would think from Cloud trail would fetch the logs for that lambda function. So you can see it locally without having to log into the console. And the last three really have to do with the serverless application repository patching a service application. So as packages for patching a service application, so creates a zip and uploads it. It seems like it kind of does build and deploy in one go. But again, for service application, then you have published so that's going to publish it to the the actual service application repository. Then you have validate, which just checks I think syntax errors for your Sam templates. So there you go. That's the big rundown there. Hey, this is a Andrew Brown from exam Pro. And we're going to be looking at ci CD, which is the automated methodologies that prepare, test deliver or deploy code onto a production server. Hey, this is Andrew Brown from exam Pro. And we are looking at the CI CD models. And before we jump into those models, I want to just talk about a couple terminologies if you're not familiar with them, which is production and staging production, short for prod, is the live server where real pain users are using the platform. And staging is a private server, where developers do a final Manual Test as a customer, which we call QA. And that is short for quality assurance. And they do this before deploying the code to production. So it's like that last check before there. So if you see staging and production and wondering what those mean, those are those terms. So this is our pipeline from code to deploy. And I'm going to show you three different models. And if you look online about ci CD, you might see some additional steps in here. So there is a bit of flexibility in terms of like how these are defined. But this is what I'm gonna show you and it pretty much is what it is. So the first one we're going to look at is continuous integration, which is short for ci. And that's automatically, that's automatically reviews the developers code. The next one is continuous delivery that's automatically preparing developers code for release to production. And the last one is continuous deployment, which is automatically deploying code as soon as developers push code. And if all tests pass, you're going to notice that both of these have the same initialism, which is a bit confusing, but that's just how it is. So if someone says CD to you, you've got to get clarification because it could mean delivery, or deployment. So let's move into these individually and learn more about continuous integration is first on our list here. And so this is the practice of automating the integration of code changes from multiple contributors into a single software project. So our pipeline is going to be code, build, integrate, and test. And this, the reason we do this is it encourages small changes more frequently. And each commit triggers a build during which tests are run that help to identify if anything was broken by the changes. So it's somebody checking over the developers code as they're coding, which is going to definitely speed up productivity with your team. So here is an example down below. So we have a developer and they push a new feature to GitHub in a new branch. And what they would normally do on GitHub as trigger a web hook to something like circle ci, and then circle ci would go create a new build server, that build server would pull in other developers code, run the test suite, and then it would report back the results. And the results would give us test coverage saying how much test code has been written to written to cover the code. And the other part is, did any tests fail. And so that is generally what most people are familiar outside of AWS. But if we're using AWS, we can just replace these things. So we could replace GitHub with code commit, we could replace that what web hook with lambda, we could replace circle ci with code build, and the results would go into s3, and it'd be called an artifact, which is a fancy word for a zip, and code pipeline. So you know, that's continuous integration. So let's move on to the next one. So now we're going to look at continuous delivery. And this is the practice of automating the preparation of code to be released to production or staging branches, you're preparing the code base for deployment, deployment is still a manual process. So in our pipeline, now we have code build, integrate, test and release with a strong focus on the last one release. So here, I have an example. And you're going to notice that half of it, we've already seen in the previous example. But in this case, we're now using all AWS services. So we have code commit the lambda function, code build, and stuff like that. And so that was the continuous integration part. So now let's look at the continuous delivery. So we have that s3 artifact, which tells us that the test coverage is good. So in some projects, you know, code can't be accepted unless you write a certain amount of test code. So whatever that threshold is, is defined per project could be 30%, could be 70%. And then all the tests pass, so that's great. And so we push that to a lambda, and that lambda is going to check and say, okay, all the all the tests and everything is good. So go ahead and make a pull request, it creates a pull request, and code commit. And now that is left for the developers to review. So the developers are going to check over that code. And if they're happy with it, there are three of them are going to all vote on it. And if there's a consensus that one person is going to decide to release that code, and so they say approved, and then that that code gets some merged into master. And so master is generally the production server, or it could be a production branch. But whichever master production say, usually the same thing. And so that doesn't mean it's deployed, but it's ready to be deployed. So we're on our last model here, which is continuous deployment. And this is the same as continuous delivery, but it automatically deploys changes to production. And so there's our pipeline, it's blue all the way across. And here is our technical architecture. And looks very similar to the last one. But we have a little bit on the end here. So we know continuous integration, we know continuous delivery. And now we're looking at continuous deployment. So the last thing we saw in continuous delivery was the feature branch being merged into master. So all the code was ready to be deployed. And so this is where clews deployment comes in. So something would check to see if something changed in the source, it would be monitoring code, commit, or GitHub. And as soon as it was merged into it, it would then trigger code pipeline to start doing things. And in code pipeline, you would define something such as code deploy. So the source would then get checked out, maybe go to code, build and run the test one final time. And that those pass that pass on to code deploy, and then code deploy would start that process of deployment. And so that is the whole pipeline there. Hey, this is Andrew Brown from exam Pro, and we are at the end of the C ICD section. So let's do a quick review with the C ICD cheat sheet. So C ICD is automated methodology that prepares tests delivers or deploys code onto servers, and generally beat production servers, which can be abbreviated to prod and environment, which is intended to be used by paying users. It's the it's your, your live server. Then you have your staging server, your staging environment, which is intended to simulate a production environment for last stage debugging. That's why they call it a staging server, then you have continuous integration represented a CI and this is automating the review of developers code making sure their code is in good shape before we allow it to be turned into pull requests, or just to speed up our development cycle. So run tests with a build server. So in this case, we use code build, and that's how that would work, then you have continuous delivery. So this is automating the preparation of the developers code for release. So it's not being deployed. It's just one step before that. So an example here is very similar to last one, except you'd run the test suite with a build server at the test pass, we, it would automatically create a pull request or merge branch into staging because sometimes staging is a precursor to production, you know, so it's saying we're ready to deploy this code, but you need to check it over before before doing so thank goodness deployment takes that a step further, also abbreviated to CD. And the idea here is it's automatically deploying developers code with which, which is ready for release. So it's all the steps prior, so it's going to run test suite built with a build server. And here, if the tests pass, then it's just going to immediately merge it into production and deploy it. So it just does everything automated and ends. And the last thing I just want to touch on canoes deployment here is that it can refer to the entire pipeline. So when you do continuous deployment on AWS, you should be thinking of code, pipeline, code, commit, code, build, and code deploy all combined. So there you go. Hey, this is Andrew Brown from exam Pro. And we are looking at code commit, which is a fully managed source control service that hosts secure Git based repositories. And I like to think of it as the GitHub for AWS. So to understand code commit, we need to know what a version control system is. And that is a system that records changes to a file or set of files over time, so that you can recall specific versions later, a very famous story about a codebase not having a version control system, I think, is either Doom or Wolfenstein. I can't remember which one. But the idea was that they had a bunch of people working on computers to program that game back in the early 90s. And since there was no control version control system, or they weren't using one, they had to move code around on, on floppies. And, you know, it was very difficult to manage all that code. And also, you know, if you had one computer that had that source code, if anything happened, that computer, all your code was gone. So this is what version control systems alleviate. And so in 1990, we had CVS which stands for I think, control version system, not a very creative name, but very clear as to what it does. Then you had some version, this is where I started out using sbn. And then in 2005, we had a renaissance for version control systems with Mercurial and also get which you might be familiar with, and Git is actually the most popular version control system and there's good reason for it. It's a distributed version control system. And its goals, which it does definitely deliver on is speed, data integrity and support for distributed nonlinear workflows. So because of, you know all those features of git, that's why that is the primary one. And so any version control system that is generally what it is using. So what is code commit, then so code commit is a service, which lets you store your Git repositories in the cloud. So developers can push and pull code from the cloud repository, and it has tools to resolve conflicts. And so you just go ahead and create your repository. And then you can download your code from there, upload and etc. And if you've ever used something such as GitHub, Bitbucket or Git lab, this is the same thing. But this is just like AWS, a solution for hosting repositories, but with some special features. So why would you want to use code commit? So let's talk about the key features. And so the first one, which I think is a really strong point, is that is in scope with a lot of compliance programs. So one compliance program is HIPAA. And so this might be an advantage it has over other competitors, or it might be more cost effective to get this compliancy. Whereas like, maybe with GitHub, you have to pay for enterprise support, it's very expensive, we're on AWS, it's very inexpensive. repositories, repositories are encrypted at rest, as well as in transit. So security is a definitely a key feature. It can handle repositories with large numbers of files, branches, large file sizes, and lengthy revision histories, though I've never felt any kind of limitations on other platforms. So I kind of feel like they're, they all do this. But you know, it's great that they state that there's no limit on the size of your repositories or the file types you can store. You keep your repositories close to your other production resources Database Cloud. Now, that, to me is the largest value is that code commit has a lot of synergies with a lot of AWS services. And the benefit there is going to help increase the speed and frequency of your development life cycles. And also, to be able to code in some creative automation. And to control access to code commit, you're going to use Iam. And that's going to say, hey, these users are allowed to have access to this repository. Hey, this is Andrew Brown from exam Pro. And we are looking at Docker, which is a third party tool designed to make it easier to create, deploy and run apps by using containers. to best understand containers, we should compare them against virtual machines, which is what we're used to using. So imagine we launched an EC two instance. And anytime you launch a new instance, it's likely going to launch you a virtual machine. And on that EC two instance, there is a hypervisor installed. And that is what is used for launching VMs. So you launch your virtual machine, you choose Ubuntu, and you're gonna have to go in and then set up your dependencies. So if you want to run a Django app, you're gonna have to install Python and additional libraries to run that application. So you go ahead and install that Django application. But this easy to install is really large. So you want to make best use of use of it, as you can see, say, Okay, well, I'm gonna put my MongoDB database on here. So you'd have to go in the server install packages and binaries to run that. And then you say, Okay, I also want to run rabbit mq on here. So you go into the virtual machine install that. But the issue with installing multiple applications on a virtual machine is that some of these programs, you know, their dependencies may not work best in a particular OS. So maybe rabbit mq doesn't run best on Ubuntu or maybe Django and Mongo share, have a similar library, but they have different conflicting versions. And that makes it hard for you to have them installed side by side. Another issue here is when they're all on the same virtual machine, let's say rabbit mq ends up eating up all the memory, there's nothing from stopping it from consuming all the memory and then and stalling the Django app installing the Mongo DB app. Or imagine if someone broke into your virtual machine, they now have access to everything. So there is an issue with all this shared space. So generally, people will not launch multiple applications on a virtual machine, they'll launch up additionally see two instances, and have an app per virtual machine. But you'll always end up with with leftover space. And so that is an issue. It's because the idea is that you're always choosing a virtual machine that is never the correct size for the job at hand, or just, you know, you can't make best use of that space. Now let's look at containers. So relaunching these two instance. And this has the Docker daemon installed, and that is what is used to launch containers. And so when you launch a container, the container has a Oh s and even though like YouTube, this one's Alpine, but even though you're using Using this Oh, so not everything is actually virtualized. So all the hardware virtualization isn't part of that container. But you get to package in your custom libraries and packages and binaries, any dependencies that you need, that are specific to this Django application. Now, when you want to launch MongoDB, you can just do so and you package the libraries and the OS is specific to MongoDB. And then you do that with rabid mq, okay. And so you can see, it's very easy to launch applications, and you can remove them easily as well. Whereas, when you install everything on the virtual machine left, if you remove rabid mq, that doesn't mean the library's back packages and binaries go with it. Or let's say you just want to kill MongoDB completely and reinstall it. You can't do that because, or you could but it'd be a lot of labor to do so. So containers gives us a lot of flexibility to easily launch and destroy new applications. And so we think of this more as available space. So if this easy to instance, we're not utilizing at all, it's easy for us to launch things into it. So just to reiterate, VMs do not make best use of space apps are not isolate, which could cause configuration conflicts, security problems or resource hogging, were with containers. This allows you to run multiple apps, which are virtually isolated from each other, you launch new containers and configure those dependencies per container. So let's take a look at Docker files. And this is a file that is used to run the commands that would be required to assemble the final image, your Docker image. And so here's an example of a Docker file. And this is for setting up a Ruby on Rails application. And it has the Postgres client, so it can use Postgres database. And it's installing No, Jess probably for its front end stuff. But let's just walk through here and I understand how this dockerfile works. So the first command we have here is called from, and this allows us to pull in a another Docker image as the basis of our Docker file. The reason why you do this is because if you had to set up Ruby, there's a lot of dependencies. And this would make this fall really large. So it's nice to be able to build off another Docker image. And after this Docker file is turned into Docker image, you could then build off of that one. So that's kind of an interesting strategy, then you have the Run command. And this allows you to execute any kind of like, command that you normally run in bash. So you see apt get, so we're installing packages, and the Mk dir, that's for making a new directory. And it does this in layers. So every time a run command for whatever is in the scope of it, it turns it into a layer. And as Docker is Docker is building that image, it caches those layers. So let's say you made a change later in this Docker file, then what it would do is it wouldn't rebuild from scratch, it would just go from the last cache layer, and then on so that makes rebuilding these a lot faster. The next thing is working directory. And this allows you to change the default folder for future commands. So just makes it a lot less verbose. When writing, writing those future commands in this file, then you have copy, this is going to copy files or folders from your local computer onto this image, then we have the entry point. So this is the command that is executed when the containers first started, you cannot override this command. So that's what it's going to be, then you have the expose. And this allows you to listen on specified network ports at runtime, we can see the port is 3000. That is the default port for Ruby on Rails when running in development mode. So it gives you an idea that this is a Ruby on Rails app that's using Postgres that is intended for development. And the last one here is the command command command. And it is passing the default arguments for the entry point. So we're not sure what's in the entry point.sh. But there's something in there. Because it is clearly a bash script that does something and it was included into here. But what we're doing is we're passing in Rails server hyphen B, so binding to Port 000. So that starts up the application. So hopefully that gives you a perspective of, you know, what a Docker file is and how they work. So I just want to cover some Docker commands here with you. And these are Docker commands that are very common, and I think that you should know them. There's definitely a lot more than what's in this list, but I thought we could just quickly go through it. So the first one is Docker build, and this is the command you're going to use when you want to build an image from Docker. Then you have docker ps, which lists produces a list of containers, you're only going to see containers here that are actually running. So you know, just be aware that Docker images is the list of images you have on your machine. So those are the Dockers images you built from your Docker files or if you pulled them from repositories. Then you have Docker run and this is going to run a command in a new container. Then you have Docker push and this is if you have an image you want to push it to a repo. A repo here could be Docker hub or VM or AWS is elastic Container Registry ECR. And then you have poll. So you could pull an image from a repo. So, you know, those are the ones I think are important for you to get some hands on experience with. So yeah, there you go. Hey, this is Andrew Brown from exam Pro. And we are looking at code bill, which is a fully managed bill pipeline to create and create temporary servers to build and test code projects. So code build is a fully managed build service in the cloud, it compiles your source code runs unit tests, and produces artifacts that are ready to deploy, it eliminates the need to provision, manage and scale your own build servers, which by the way, is really hard to do. I tried it and I'll never do it. Again. It provides pre packaged build environments for popular programming languages and build tools such as Apache, Maven, Gradle, and more. You can also customize build environments to use your own build tools, which is very useful. They're talking about using Docker, and scales automatically to meet peak build requests. So now that we have a little bit knowledge on code build, let's look at some use cases. And dig in a bit deeper here. So let's look at a code build workflow. So the first thing is you're gonna have to trigger code build, somehow, you could do that be via the service console, the COI the SDK, or the most common use case is that it's going to be part of your code pipeline. And after it pulls the source, it's going to pass that on to code build. And that's how it's going to get triggered. When you set up code build, you have to set up a build environment, and AWS has some pre built manage images for you. So they have an Amazon Linux to Ubuntu and Windows Server. If they don't have the things that you need installed on them, then you have to provide a custom image, which is a Docker image. And you would normally store this in ECR elastic container repository, where you'd upload your image and you could reference it for that you could reference from other Docker sources like Docker Hub, but you're probably going to use ECR. The next thing is the source code. So you need to get source code into that build project. So you'd have to use a source provider. So that could be from code, commit GitHub, Bitbucket, or etc. You probably could also just pass along the codes to see how there's like no source. So you could have no source and as the, the build script triggers, it could then just use the internet and pull from there. But generally, you want to provide a source provider. And speaking of like, how are we going to run our build that comes down to that build spec yamo file, and this is generally part of your code base. So you know, when you pull in that your code from the source provider, this file would be in that your root directory there, and it's going to talk about or it's going to tell you all the commands that need to be run. But one thing I want to tell you that even if you have a build spec file with your project, you can override it with other build commands, which is very important for you know, for the exam, because they might bring it up saying like, Hey, you have this build spec file, but you want to override it, how can you do it. So it's like, well use the COI for that. So hopefully that gives you kind of an idea about the code build a workflow. So I just wanted to touch again on the build environment. So Docker images are managed by code build, which is AWS, it was managed. And so you want to check these images to see what comes pre installed. So you choose that environment image, you have Amazon Lex to Ubuntu Windows Server, and these are the images. So if you want to dig into them there in the universe documentations. But you can see that there are two variants for Amazon Lex two, there's two variants for Ubuntu. And these are images, so you could download them and theoretically run them in Docker and poke around and see what's in them. But I'm pretty sure they listed on the docks there. So if your needs aren't met by these managed images, then you'll have to make a Docker image stored on ECR and then reference it that way. So let's look at a couple of use cases for code build. The first one being generating outside pages from a jam stack jam stands for JavaScript API's and markup. And so we have a website that was built with Gatsby that's a jam stack application that needs to render out stack pages and delivered them to s3 static website hosting. So let's say we have our code or Gatsby code or a code commit. So we could trigger code build with the aid of this console. Code build is going to pull from its source at source build being code commit, and then the build is going to render out the static pages, and then it's going to output that artifact into an s3 bucket. So those are ready for static website hosting. The next one is let's say we want to run test code and report test coverage. This is probably the most common use case for code build. So developer needs to ensure their code passes all tests before being allowed to make a pull request. So, you know, you let's say you have a Ruby on Rails application, and you've pushed some code to it to a feature branch, that's going to trigger a GitHub webhook. That will trigger a lambda function that uses the SDK to then tell code build to start building on the source code. And so the code build will pull down the Ruby on Rails project. And it's going to run r spec, which will generate out test coverage, and also say, well, the tests failed or not, it's going to take those reports put in a zip, and then you could pass it on to another lambda function. And then that information gets passed back to GitHub to determine whether that pull request should occur or not. So those are two use cases. Let's take a look at the build spec yamo file, which is the most important thing you need to know about code build, you need to know this inside and out. And definitely get some hands on experience with this because it's super important. So the build spec provides the build instructions, the build spec YAML needs to be at the root of your project folder. And here I have an example. And this is actually the one we use on exam Pro. So we have a Ruby on Rails application. And so we don't use it all the stages, but you generally use pretty much everything. And so let's walk through it. So the first thing we need to define is the version of the build spec. And so there's 0.1, and 0.2, and 0.2 is what is generally recommended. And if you want to know the difference, I don't think they show up on the exam. But it's good to know. And this affects the default shell in the built environment. So 0.1 runs each build command in a separate instance. Whereas 0.2 runs all build commands in the same instance, build environment. So this is important because it tells you, you know, what information is going to get shared into to the next command. So you know, that changes the way you're going to write that file. So the next thing are phases. So phases are the commands that run during each phase of the build, and there is a very specific order that they go through. The first is install. So this is only for installing packages in the build environment. Then the next one is pre build. So these are commands that run before building, then you have build. So these are commands that you run during the build, then you have post build, these are commands you run after the build. So they're pretty straightforward in terms of their definitions. The only thing that's not clear is actually at what step does the source code get pulled? I think it is after the install step. So that is a step that is not very clear there. But you know, it's not that important for the exam. But you should know these different build phases. And then the last thing there is artifacts, so we're not showing that here in this document. But you can configure the artifact to build somewhere else. So if you wanted, there's like a default place where that build artifact goes. But if you wanted to specify the s3 output, you could do so in here. And again, artifacts are just the results that are zipped that go to s3. So there you go. Alright, so we are at the end of the code build section. So let's move on to the code build cheat sheet. So code build is a fully managed build pipeline to create temporary servers to build and test code, compile source code, run unit tests and produce artifacts that are ready to deploy provides pre pre packaged build environments, or you can build your own environments as Docker containers, use a build spec yamo to provide build instructions. This file is stored in the root of your project. And we need to know the contents of this file. So let's go through it right now. So we have version 0.1 of this file, which runs each build command in a separate instance. Then you have version 0.2, which runs all build commands in a separate install in the same instance. And I think it's like it's when it says instance, that means like instance of a shell or bash shell, I'm not necessarily another easy to instance, that would be ridiculous. Then we have different phases. So we have runs these commands and through different phases. So we have the install phase. This is only for installing packages in the built environment. We have the pre build stage, which is for commands that run before building, we have the build stage phase, which is commands that you run during the build. And the last one is post build command post bill which is for commands that run after the build. So there you go. That's code build in a nutshell, and we're grading for the exam. Hey, this is Andrew Brown from exam Pro, and we are Looking at code deploy, which is a fully managed deploy pipeline to deploy to staging or production environments. So looking at code deploy, it is a fully managed deploy service in the cloud, you can deploy easy to on premise lambda or elastic Container Service, you can rapidly release new features, which is the idea behind code deploy, you can update lambda function versions, so that as a method of deployment you can do there, you can avoid downtime during application deployment. This is generally with bluegreen deployment, because the idea is that replicates an entire environment and moves it over and then kills the old one, where in place generally takes effect on the existing server. So we have, we have the ability to perform in place in bluegreen, which we just talked about there a moment ago. It integrates with tools, such as Jenkins code, pipeline, and other ci CD tools, and integrates with existing configuration management tools, such as puppet chef and Ansible. Alright, so the first thing we're going to look at here for core components is creating an application, this is the first thing you do in code deploy on an application, it really is just a container for the rest of the components that make up code deploy. But for an application, it's as simple as going in and naming your application. And then you have to go ahead and configure a bunch of other components. The first component that you need to configure is deployment groups. And this is a set of easy to instances or lambda functions, where your new revision is going to be deployed to. And then once you have your deployment group where you know, you're going to do your deploys, you can then go ahead and create a deployment. And so you create a deployment and you choose the code that you want to upload. And then you can go ahead and configure that deployment with a lot of rules. So whether you want it to roll back, or whether you want it to, like how you want it to fail, and a bunch of other rules in there, which we'll look at in more detail at the end code deploy, follow along, then you're gonna have your app spec yamo file, this is extremely important to know. And we definitely cover it in here in great detail. And this contains all the deployment actions that code deploy is going to use to figure out how to execute or like how to install restart your application on the actual server. It's just a YAML file. And then the last, last, here, we have the actual revision itself. So these, this is just an embodiment of all the changes that will take effect on the actual server. So that is your apps like yamo file application files, you'd have to install configuration files if installed, or any kind of executable. So there you go, that is the core components to code deploy. So it's very important that we know the difference between inplace deployments and bluegreen deployments for code deploy. So we're going to start here with inplace deployments. So when you set up your deployment group, or I think it's deployment group, you get this option between in place in blue green, so we choose in place, and then you choose the environment, whether you want to do EC two in an auto scaling group. Or you could choose EC two instances based off their tags, or you can actually choose on premise instances. So let's understand the process of how this works. So the app on each instance, in the deployment group is stopped. The latest app provision is installed and the new version of the application is started and validated. You can use a load bouncer so that each instance is registered during his deployment and then restored to service after the deployment. deployment is complete. Generally, when you're doing in place, you want to use a load balancer, it's a great idea. And then last only deployments that use easy to on premise compute platform can use in place deployments. And notice that you cannot do a lambda function here. And we do not have ECS clusters. So just be aware of that. And now we'll move on to bluegreen. Alright, so let's take a look here at bluegreen deployments for code deploy. So here you choose the blue green option. And then you have choosing between the automatically copy aect Auto Scaling group or manually provision instances, you're definitely going to want to choose the first case. And this is what would generally show up on the exam if they do talk about this. And the idea here is if you already have an environment, and it's using auto scaling group, you specify it here, it's going to copy that one over. And a new easy to instance is going to spin up code deploy is going to then apply the latest code version and install it using the app spec yamo file. And then what's going to do is shut down the old environments as well, it will shift over the traffic from old to new, and then destroy the old old infrastructure. So let's go through that just again here. So instances are provisioned for the replacement environment. So that could be the auto scaling group being cloned the latest application revision installed on the on the replacement instances. So if you're using code pipeline, it could be passing the the artifacts from the source to code deploy and then it will install into the correct location. We will see that in the app spec yamo file and then it will install from there. An optional wait time occurs for activities such as application testing and system verification. Is this isn't the replacement environment are registered with an EOB, causing traffic to be rerouted to them. So that's the shift of traffic, and instances in the original vironment are deregistered and can be terminated or kept running for other uses. So people don't always terminate their, their old environments right away. Because sometimes you need to debug them, or you need to fall back to them in the case of a disaster. So it's up to you on that case. So we're gonna take a look at the app spec yamo file, which is responsible for saying where the code should be installed, and how to get the new piece of code running. And this example here is actually from the exam pros, Ruby on Rails application. It's an older version, but it makes still for a very great example of a real world use of this aspect gamble. So we'll just walk through this, and there is some variation on this file here. But this file is a very good example. So the first thing is we choose our o 's, this could be Linux or Windows, then for files, we are saying where the code should be downloaded to. So Ford slash is after that zip is because you provide the code in the form of zip. So wherever that zip resides, take it and put it in home easy to use your app because I want it to be an app, then we can apply permissions to anywhere. So I just want to make sure that easy to user is the owner of that app directory. And then we have our hooks. So you have an application stop. So this command should be responsible for doing the stop. If you notice, we're providing a location to a bash file. So the way aspect camel works is you write bash scripts for all them. And then you specify them with part of the zip that is provided to code deploy. So the aspect YAML will be in a zip, and then all these files. So this one, this one, this one, this one, this one, they'll all be in that zip there, then you have a timeout. So you can set a timeout, I think there is one by by default, but if you know generally how long these are going to run, you should set them because this will just speed up the process. If one of these hang, for whatever reason, then you can set what it should run. So I always say easy to user, especially with Amazon is one, Amazon x two. And let's look at some of the other books we have before install. So that's before it's downloaded the code to your server after install things that you'd want it to happen afterwards. And then a command to restart the application up. And I want to point out that the lifecycle event hooks are are going to be different based on whether you're using ECS, ECS, and lambda. So you have to look up the documentation to what's available to you. But this is for an easy to instance. And so this is the most common use case. So when you do perform a deploy in code deploy, you're going to get to be able to visually see all the lifecycle event hooks that are available to you. As they perform, you'll see these go from pending to successful or if they fail, they'll show you additional information. And you can see that we get the durations of it. So it's really good way to get that overview of it. In the case of a failure, it will look like this. So you can see that this script got through here, and then it failed at this point. And then you'd click into that. And then once you click into it, you will get a little bit more information. It's not always clear as what has gone wrong. So here you can see that it's saying start dot start underscore puma.sh has failed. So that script in particular that I wrote had an issue. And then inside we might get some information actually as to what failed Exactly. And this one is totally not clear. So there's a lot of cases where you have to log into the EC two instance, to debug code deploy. Generally, what you want to do is stream your code, deploy logs to cloud watch logs, which I don't show you in this in this section. But it's something you definitely want to do if you're running code deploy in production, because it's such a pain to log into an easy to instance, and debug this stuff. So but there you go. So I just wanted to hop over to the documentation here quickly to show you what hooks are actually available to you. And what's going to affect hook availability is the deployment methodology. You can notice for end up in place deployment, we have all hooks available to us. And then for blue green based on the case it's going to change. So it's interesting here because when you do bluegreen deployment, you will see all these hooks in the code deploy you'll see all of them but it's just trying to separate them out to say these are the original ones that are going to happen on that side the Replace replacement ones on this side. So just be aware of that and then these are just for rollback. So for rollback so you can see the scope is quite fear there. Do you need remembering these for the exam? No, but it's just good to know because it can save you time debugging. And if you're doing this for real in a practical use. So to get code deploy working with your computing powers, you're going to need two things, you're going to need the code deploy agent. I always think this is pre installed on Amazon, Linux one and two, but it's not. So what this is, it's just a service. It's like a binary that you it's actually written in Ruby, but you have to install Ruby on your server, and then download the script script, install it. And then what that will do is it will have the cloud code deploy agent continuously running. And it's going to report back to code deploy the progress of when you run the the lifecycle hooks and installs all the steps. The other component to it is you need to create a code deploy service role. And this is pretty easy through the Iam console, if you just go to View, choose code deploy, they have some preset ones for you. And what this does is it gives access to things like auto scaling group CLB, and things like that, I say you may need to create it, because I think in some cases, you might not need it. But this allows code deployed to create an additional auto scaling group and like shift traffic between elastic load balancers. So these are the two things you'll need to set up, which is not very clear, when you're using code deploy. Hey, this is Angie brown from exam Pro. And we are going to be doing the code deploy follow along. So we're going to learn how to set up automated deployment using code deploy. And the first thing we're going to need to do is get an EC two instance, set up with a basic web page that we're going to turn to Nami, which we will use for code deploy. So let's make our way over to EC two. And what we're going to do here is we're going to go to instances on the left hand side, we're going to launch a new instance. And I want you to choose Amazon Linux to the top one there it's select, we're going to go with the T two micro because we want to save money. And what I want you to do is just drop down here and you should have this SS SS m EC to service role. If you don't, let's just go through the process of creating that right now. But we have created an other follow along here. But just in case you don't have it, I'm just going to delete this role here and make a new one. And this allows us to gain access to sessions manager if we need to log into this instance, I'm gonna go up to next we are going to let this load here. I'm gonna type SSM. Next, Next, SSM EC to service role. That's what I like to call it, we'll go ahead and create that there. And then what we'll do is go back here are refresh. And then this role should now appear. So that's very important that you set that because it's going to save us a lot of trouble. Now down below, we have the ability to put in a user data script. And this is what we came here to do. So let's set up a basic web application. So what I'm going to do is go over to cloud nine, and you should have a cloud nine environment. And if you don't watch the Elastic Beanstalk follow along or some of the other ones where I show you how to set up cloud nine, it's very easy to get environment set up. And what we're going to do is we're going to make a new folder here on the left hand side, I'm going to call this code deploy project. And I'm just going to make a new file in here called user data.sh. And what we're going to do is set up a script that's going to set up a patchy a basic web page. So the first thing we need to do is specify our shibang or shibang is going to tell tell this script, what did you use to execute it, which will be bash, we're going to want to switch to the sudo the ES VC to user when we log in, we're going to want to install HTTP, which is what Apache is, then we will want to change the ownership of the directory for the var WW. This just makes it a lot easier to work with it. So we're gonna change that to the easy to user. The next thing we want to do is we're going to make a basic web page. So we're going to place an index file in bar www dot html and have index HTML. That's where a patchy looks by default. And we're going to use a nice here doc here to do a multi line here in bash, which is very useful to know. And so I'm just gonna make a very basic website, the ugliest website I can think of. So we'll just get an HTML in there when we will get a head and we'll get a body in there. And we'll need a title my code deploy app and we will do the same thing here in the body except instead of having A title we're gonna make that h1. And then there's just a couple things we need to do down below, we need to start up Apache, we're using system CTL, you could use the alternative syntax, which is sudo, sudo, service HTTP, D start. But I noticed that if you if you do that the enabled doesn't work. So this is why I'm using the more long form format here. So that's for anyone that is very familiar with this, for whatever reason, it's not working in the other format. And so that should be everything. So switch DC to user install, install Apache, change ownership, create that initial file, and that looks all good to me. So I'm gonna copy this here, go over here, as you can see, you could input a file, I'm just gonna use text here, nice and plain. And assuming I got that, all right, uh, we'll be in great shape. If we don't, that's okay, we'll make our way in there using sessions manager. So we just need to set up a security group here. So I'm gonna go next, next. Next, I like naming actually gonna go to tags, this is always a good idea, we're just gonna put a name, I'm gonna say, my or my code deploy app. Because if we do that, it will name it. And I usually do that afterwards, we're going to make a new security group here, just so we can see what we're doing my code deploy SG, and we're going to open up Port 80, we do not need Port 22. Because we will not SSH into this, we can get into it using sessions manager will hit launch. Once you drop down, proceed without a key pair, which is super, super more safe. And we will go to will click on that instance there. And now all we have to do is wait for it to launch. So I'll see you here in a moment. Okay, so we have a running state, both status checks have not passed. But that's okay. Because generally, we can check the website. But we should usually wait for those two checks. I'm just grabbing the IP address here and refreshing. And there's my application. Now what's really important to check whenever you set up an app here is to restart the server because we absolutely want to be sure that if we stop and start this again, it reboots. So what we're going to do here is we're going to go stop. And we're just going to let this shut down here, it shouldn't take too long. Keep on refreshing here, and then once that stopped, we're going to start it back up. So that is just a pro tip for you because there's a high chance that the service might not start up because something's wrong their script. And to find that out later is such a pain, you have to rebate the EMI, I do not want to do that. So I'm going to go ahead and start this up again. And we'll just give that a refresh. I guess I'll see you back here in a moment. When it's a it's active again. Welcome back, I didn't wait for my status checks again. But it is running. If we copy that IP address, we can see that the application is there. So we're all in great shape. If you did want to connect to this instance, you could go to sessions manager, but in other ways, just go click Connect here and go to sessions manager, we don't need to do anything here, I'm just showing you around. And so what you normally do is do sudo su hyphen easy to user because it always logs you in as root, it'd be really nice. If it logged you in as easy to user, you could select that, if you're listening AWS, please make that change. If we go to var WW, HTML, HTML and do all stuff in LA, there's our file. Alright, so just getting you a little bit comfortable with EC two. And we'll pop out there. And so the next thing we need to do is make an ami of this image. So I'm going to go here and create a new image. I'm going to call this my code deploy app 000. It's great to version in this way. Always give it three zero, so you have a little bit of room to breathe. And what we'll do is we'll go ahead and create that image. And so if we go here, we'll just have to wait for that image to go pending. And once that is done, then we will actually leave our current server running. That is totally fine. But what we need to do next is set up a code commit repos. So we'll go to code commit. And the next thing here, I don't know if we need code commit right now. We'll need it for later. But yeah, you know, we'll come back to code commit later, I was thinking later when we added to the code pipeline, that's something we can definitely do later. So what I'd rather do instead is to get code deploy set up. And for that, we'll we're going to need a a deploy file, which they call app spec dot YAML. I'm just gonna make a new file here called app spec dot YAML. Let's make our way over to code deploy, we might get stuck through this process if we don't actually have a way of deploying. But we will do the best we can here. So what I want you to do is on the left hand side, go to applications, I'm going to create a new application. And I can see that I have a test one here, I'm gonna go ahead and delete that you're not going to have this. Oops. Anyway, we'll go back to applications here, create a new one, I'm going to call this my code, deploy app. And we'll hit we'll choose EC two, we'll create the application. And so now that we have an application, we have to set up a deployment group. And we'll call this my deployment group, or my code deploy deployment group, which is a bit silly, enter a service role. Code deploy permissions that grants it was code deploy to access your target instances. So we don't have one of those. So that's something we're gonna have to go get. We probably can just go edit our existing EC to roll. So maybe that's what we'll end up doing. Okay, so I just pulled up the documentation quickly here. And I believe we need to make this service roll. So what we'll do is we'll make our way over to I am, so I already have it here on the left hand side, you might have to type in I am, we'll go over to roles, we're going to create a new role. We are going to choose probably code deploy, what do we have to choose here, she was a service with the roll, choose code deploy. All right. We'll go back to high em here. We'll choose code deploy. And then down below, we already have those presets. So we're gonna choose code deploy, which is for auto scaling. Yeah, that's fine to me. We'll go next, we will see what we get there. So it's already predefined, we will hit next. I love it. When they have those predefined ones for you. It saves you so much time. And we're just going to name it as suggested. So we'll go up to here. I'm just going to call that will create that role. That will go back here. And let's see if it autocompletes enter a service role Arn. Well, there you go, I'll use so that's what we'll do. So we'll go into here, I'm just going to copy that Arn and paste it on in hopefully that works we have in place in blue green, I'm going to try to keep it simple and do in place the configuration, select what you want to deploy. So we'll choose EC two instance. And you can add up to three tags. So I guess I have to tag my C two instances, oh boy. So what we'll do is we'll make our way over DC two, we'll go to instances. There's our app. And what we'll do is we'll just add a tag. So we'll go edit tags, I call this app CD. And we'll save that we'll go back over here, I'll say app, CD tag, oops, I guess we only really need one there. So that should do it. Um, we're gonna do all at once sounds great to me, we're going to turn off load balancer, we don't need that. We're gonna disable rollbacks, we're gonna go ahead and create that deployment group. So now that we have this deployment group, it should know how to deploy. So if we want to go ahead and do create, deploy, and we scroll down here, um, what we need to do is supply the revision. So it's interesting, you can't supply like you have GitHub, you can't do code commit. So which is totally fine, I guess. So I guess what we're going to have to do is we need to supply a change to our server revision to our server. So what we'll have to do is make our way over to cloud nine, and we will make a revision, and then what we'll do is place it in an s3 bucket. So let's go over to cloud nine here, I'm gonna make a new file here. I'm gonna call index dot HTML. I'm going to grab our code from over here. I'm going to copy this and paste this. I'm just going to call this v2, v2. Now, I think, I think we're going to need to have to have an app spec dot YAML file in this because that's what it uses to figure out the deploy. So yeah, I think that's what we'll need. So just give me a second to pull up the documentation for that. Okay, so I just pulled up the documentation for the app spec yamo file. But actually, you know what, I think what we should do is we should go set up that code commit because it's gonna make things a little bit more organized. I will just get this out of the way. So I'm gonna go create here, I'm gonna say, my code deploy app. And what we'll do is We'll hit Create. And we're going to have this clone URL. So grab the HTTPS when it copies to your clipboard, make your way back to your cloud nine environment. And what I want you to do is get into this folder here. So just go to Tilda, Tilda takes it to the home cloud deploy project. And then we'll do git clone there. And that will put a folder inside of a folder a little bit messy, but that works. And what am I to do is drag this index dot HTML into there. And then we'll drag this app dot YAML into there as well. So that's a little bit more organized. And then let's open up our app, app spec dot YAML. So the first thing we need to do is define a version. It's gonna be version 0.0. I don't think there's any other version at this. At this point, this is the only version there is for app spec. Then we'll do LS, and we'll choose Linux because we're not using Windows. And we could set some other things like files and permissions. And that's what that's where it's going to place the file. So I'm, I'm trying to decide where this should go. I guess I'm going to just place it. So we'll say source, which is the root of the the revision will upload. And then I'm going to supply the destination destination. I'm going to Home Ec to user oops, no, no, no, we're going to put var www dot html. So that's going to place it where we want it to be. And we're just going to make sure the permissions are as what we need them to be. So I'm going to do object, we're going to say var WW. Dot, we've already done this already. But I'm going to just force it anyway. And it's also necessary to see how we can use permissions to change permissions. You see to user, we're going to do group, EC to user as well. And the next thing we'll do is we'll put in some hooks. So for hooks, there's the application stop, that's something we definitely want to do. And we will just name this little save location. And we'll say stop app.sh will give it a timeout, it's always good to provide timeout. So that doesn't take forever. Stopping the app is super, super fast. So we'll give it I don't know, 10 seconds, which is maybe we don't even need 10 seconds, I think it takes like six seconds. And we'll run this as easy to user. And so that should be the premise of that one. We're gonna need that that stop app there. So I'm gonna go here and make a new file, say, Stop app.sh. Whoops, we will rename that because of its name wrong, we're gonna have a lot of trouble on our hands. So I'm going to just double click that there, we're going to go to user data. And we're going to grab this line here. And what we're going to do is paste that in there. Generally, when you run commands, you want to give it the absolute path. So we need to know where the absolute path of httpd is. So what I'm going to do is connect to the instance here, we're going to use sessions manager, I hit Connect. And we're going to here sudo sudo, Su hyphen EC to user. And then what we're going to do here is type in Where is httpd. And that's going to give us the full path. So this I believe is the full path. I really hope that's the case. I guess I could just run it. Yep, that's it. And what we'll do is we'll go back here, and I'm just going to paste this in fully here. Because I do not want to have any problems. And I think it's this. Here we go. So that's our stop. And then and we don't necessarily have to stop it, we could just restart it, I suppose. Maybe that's a little bit cleaner to do, I think that's what I'm going to do, actually. So I'm going to rename this to restart app. That's going to avoid us a bunch of problems. And so instead of doing this in the application stop, I'm gonna do this the application start, which is one of the last things that it does. So for application stop, we're not going to do much here. There's the before install that we can put in here. There's nothing really to do there, but we're definitely going to need an after install. So after install that means like after the code has been downloaded to the source directory. So wondering if this is a smart idea to place it here. Um, maybe I'll just put in a folder called revision. That's actually what I think I would prefer to do. And then what we can do here is we will just copy this here. And we are going to make a we'll rename this to restart and we'll rename this to update, update app. And What we'll do here is make that new file over here. And I call that update app.sh. And we will open that one up there. And what I need to do is I need to make a command that's going to copy that revision. So I'm just going to double check, make sure I put a forward slash in there. Yeah. So the revision should be stored here. So the first thing I would do is remove the old file. So I'd say remove var, www dot html index dot HTML. And then I'd say copy, revision to var. W Yeah, this one up here. Not gonna read the whole thing out there. So this should do it. Okay. And so yeah, this should be our code spec. Now, what I'm going to go ahead here and do is commit all this stuff to our repo. For a later step, actually, we'll leave it alone. For the time being, we don't have to do that. But I do need to zip the contents of the stuff that is here. So let me think about that. We'll go back to code deploy, I'm pretty sure it expects it to be a zip, copy and paste the Amazon s3 bucket where your revision is stored. And here you can see we have folder object dot zip, targ, or t g. Zed. So what I'm going to do is I'm just going to figure out what these command is. And I'll be back here in a moment, because I never remember that off the top my head. Okay, I'm back. I think I know what the command is. It's zip. And then you supply the name. And then we'll do this in the that actual folder there. So go into there, I'm going to type in zip, and then whatever we want, call this say this is revision, revision 000 0.1, maybe zip. And then we need to say what files we want. So we want the aspect camel, we want the index HTML, we want the restart app, we want the update, sh. Maybe we should match the version. Because it says version two, let's just call it version two, it'll be less confusing. So I think we have everything update, restart. Yep, we have all the files we need. What we'll do is we'll hit Enter. And that all looks good to me. So now we just need to get this revision file onto s3. And we're going to need a new s3 buckets. So using this, Eli, it's going to be AWS s3 API, create bucket. And we're going to give it a bucket name, of course. And the name is going to be my code deploy. app. If that doesn't work, you might have to choose a more unique name because s3 names are unique, like domain names. And we're going to specify the region always specify the region makes our life a lot easier, always use East one. And I did not use the command correctly. It was s3 API three. Hold on here. Eva's s3 API. Help. Yeah, there's a create bucket command. And so we'll do create bucket. Just scroll down here for a second. Just looking for an example. Okay, so it says colon colon bucket. I called it bucket name. So that's not the command that is supposed to do there. So I'm just going to hit up there. And we're going to make our way over here and just take out the hyphen name part. And there we go, it created the bucket. So now what we need to do is copy this revision to there. So I'm gonna do it was it was s3 cp, why it's not s3 API, why there's different ones, I have no idea why, but that's just how they have it. And so we'll specify that file which is revision two. And then we need to specify the bucket, so s3 colon, colon or colon slash slash, then we'll put the name of the bucket here. And we'll just maybe put in a new folder called revisions. And, yeah, maybe we'll call it version two. And we'll hit up, upload there. And so now it's uploaded. So if we make our way back over to here, it's now expecting that bucket path. And we are just going to go back to our cloud nine environment and we have it right here. So I'm going to grab it Okay, and paste that there. The reason type is zip That is correct. And there's that application stop lifecycle. I don't remember this option being here before don't fail to deploy to an instance, if this life cycle event on instance fails. That's that's a cool option here. So we don't have to do anything with that you we don't need to override anything there. We don't do anything like that. Normally, you have to add a A code deploy agent to the EC two instance, for this to work. So I'm just wondering if I need to give permissions for that there. Maybe we should do that before we proceed forward. Because I feel like that's going to give us trouble if we don't do that. So what we'll do is go back to our EC two instance, it's already running, and we have this SSM aect role that's here. So what I want you to do is I want you to go to I am, we're going to use create ourselves a new unique role for this app. So I'm just gonna go EC to next. And I'm going to, I'm going to add some policies. So I want the SSM one definitely. And then I want the code deploy agent. Yep. And we'll hit Next Next, and I'm going to call this my code deploy app role. And we will create that rule. What we'll do is we'll make our way back to EC to console to where that is, we'll just type that in again. We'll go to instances and left hand side will look up this instance, I'm pretty sure we can just replace the roll through here. Where are you attach replace I am role. So we will swap it out for my code deploy role? And we'll hit Apply. And we will close that. We don't know if we'll have to restart the AP. I don't think so. I don't think so. And the IM role is not part of the actual ami, we attach it when it launches. So that's another thing we don't have to worry about. So what we'll do is we will go back to code deploy. And I think we're all in good shape here. Deployment description, add a brief description, deploy version two. That's what we want to happen. Now whether that will work, I guess we'll find out here in a moment. And we'll hit create, deploy. And I'm so used to code deploy, deploy is failing. So if this fails, I'm totally okay with that. Now, this is an in place deploy. Yeah, so I was thinking I needed the code deploy agent, because code deploy might have to start up another auto scaling group or something. But I realized as an in place deployment, that means it's not taking, not creating a new server, it's just going to take the server out of service, and then apply the updates in place. And we will see we click View events here and see how that goes. Generally, this is super fast, so I'm not sure why it's taking its time. But I guess what I'll do here, I'm just gonna click back. Since it's in place, that should be super fast. What I'm gonna do is I'm just going to wait until we have the results here. If it fails, it fails or if the time's up, I guess we'll find out here shortly. Alright, so this failed, but that's okay. I figured out what the problem was. I hopped onto it with support, because I was really, really stuck. And I realized I didn't install the code deploy agent. I guess I just thought it was pre installed on Amazon Linux two, but I guess it's not which is a bit frustrating, and they don't have a run command for that either. So I guess we'll have to manually install that. Not a big deal because we were smart enough to set up SSM just in case we had to go into our server. So let's go ahead and hit Connect, go to sessions manager Connect. And then we'll go ahead and install code deploy. So
