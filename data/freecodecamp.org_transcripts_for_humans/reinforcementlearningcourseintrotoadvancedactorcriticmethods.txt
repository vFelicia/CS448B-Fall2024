With timestamps:

00:00 - Welcome to an introduction to advanced actor
critic methods. I am your instructor, Dr.
00:04 - Phil Taber. I'm a physicist and former semiconductor
engineer turn data scientist. In this course,
00:10 - you're gonna learn the fundamentals of advanced
actor critic methods. Now, if you've seen
00:14 - some of my prior work here on the Free Code
Camp, then you may have seen some work related
00:18 - to deep learning as well as actor critic methods,
there will be a little bit of overlap between
00:23 - the other actor critic courses and this material,
simply because I can't assume everyone has
00:28 - seen my earlier content, no need to go back
and re watch that though. Although you are
00:32 - free to do so if you wish, I will include
enough information in this particular course,
00:37 - for the motivated beginner to get started
in the field of deep reinforcement learning
00:42 - and actor critic methods in particular. Now,
why are actor critic methods important? That's
00:47 - a great question I'm glad you asked. The basic
idea is that things like cue learning are
00:53 - great for learning problems with a discrete
action sets like playing video games where
00:57 - you can move left or right or shoot your phaser
blast or the invading aliens. But it falls
01:02 - down when attempting to handle things like
continuous actions. So this is important in
01:07 - fields like robotics, where you are applying
continuous voltages to motors enjoins to actuate
01:14 - movement. And so we do technology to handle
robotic movement above and beyond deep learning.
01:20 - Now, far from being a theoretical exercise
in March of 2021. This year, group at Berkeley
01:26 - did, in fact, use deep reinforcement learning
to get to bipedal movement in a robot named
01:32 - Cassie, and I'm going to detail that on my
channel. By the time you see this, it may
01:36 - already be out. So go ahead, check me out
at machine learning with Phil, I'll leave
01:39 - a link in the description, you can go subscribe
if you're interested in more deep reinforcement
01:43 - learning content, but enough of the shameless
plugging. So actor critic methods are necessary
01:48 - for dealing with continuous action spaces.
And they work by approximating something called
01:54 - a policy. A policy is a mathematical function
that takes a state of the environment as input,
01:59 - and outputs some action. Now, in the case
of a robot, that action could be a just an
02:04 - actual voltage that we apply it to our motors,
or it could be a probability distribution
02:11 - that we sample to generate some action. So
for example, Gaussian distribution, you know,
02:16 - a normal bell curve that you sample to get
some value for your action. We're going to
02:22 - cover both cases. In this course, we're going
to cover a whole host of algorithms, starting
02:27 - with the vanilla actor, critic method, deep
deterministic policy gradients, twin delay
02:33 - deep deterministic policy gradients, proximal
policy optimization, soft actor, critic, as
02:38 - well as a synchronous advantage, actor critic,
try saying that five times fast, as far as
02:43 - software requirements go, they are relatively
light. So we will need NumPy pie torch and
02:49 - Matt plot line, I highly recommend using the
versions that I will leave for you in the
02:54 - description, because NumPy in particular,
likes to deprecate stuff. And that tends to
02:58 - break my code. So if you use the versions
that I've linked in the description, it's
03:02 - almost guaranteed to work provided we didn't
make some mistakes along the way. As far as
03:07 - hardware, you're going to need a GPU unfortunately,
and 2021 the GPU market is totally broken.
03:13 - So hopefully you already have one on hand.
If not, don't despair. In particular, the
03:18 - a three c algorithm, a synchronous advantage
actor critic is designed to run a multi core
03:23 - CPUs. So at the very least, you'll be able
to run that and get really good results. Other
03:27 - algorithms, you may be able to get something
that converges all of the timeline, the amount
03:31 - of time you're gonna have to train is going
to be a little bit longer. So you may want
03:34 - to leave things running overnight. With all
of that said, I will check in periodically
03:38 - for questions. Obviously, I don't get notifications
from the Free Code Camp channel, but I'll
03:42 - do my best to patrol the comments to see should
there be any confusions or need for clarification
03:47 - I can swoop in to render assistance. Once
again, if you like this type of content, check
03:52 - me out at my YouTube channel machine learning
with Phil where I go over all things, deep
03:56 - reinforcement learning, and occasionally natural
language processing as well. Let's go ahead
04:01 - and get started. And I look forward to seeing
you on the inside. Welcome back everybody.
04:06 - In today's tutorial, you are going to get
a mini crash course in actor critic methods
04:10 - in TensorFlow two, we're gonna have around
15 minutes of lecture followed by about 20
04:15 - minutes of coding, and you're gonna learn
everything you need to know to go from start
04:18 - to finish with actor critic methods. If you'd
like to know more, you can always pick up
04:22 - one of my Udemy courses on sale right now
link in the description. Let's get started.
04:29 - Welcome to the crash course in actor critic
methods, I'm going to give a relatively quick
04:34 - overview of the fundamentals of reinforcement
learning in general. And then of actor critic
04:39 - methods in particular, finally, work together
to code up our own actor, critic agent and
04:44 - TensorFlow two. This is geared toward beginners,
so feel free to ask some questions in the
04:49 - comment section. Reinforcement Learning deals
with agents acting on an environment, causing
04:55 - some change in that environment and receiving
a reward in the process. That All of our agent
05:00 - is to maximize his total reward over time,
even if it starts out knowing literally nothing
05:05 - about its environment. Fortunately, we have
some really useful mathematics at our disposal,
05:11 - which makes figuring out how to beat the environment
it difficult, yet solvable problem. The mathematics
05:17 - we're going to use relies on a very simple
property of the system, the Markov property.
05:21 - When a system depends only on its previous
state, and the last action of the agent, we
05:26 - say it is markovian. As we said earlier, the
agent is given some reward for its action.
05:32 - So the set of states the agencies, the actions
it takes, and the rewards it receives forms
05:37 - our Markov decision process. Let's take a
look at each of these components. In turn,
05:43 - the states are just some convenient representation
for the environment. So if we're talking about
05:48 - an agent trying to navigate a maze, the state
is just the position of the agent within that
05:52 - maze. The state can be more abstract, like
in the case of the lunar lander, where the
05:57 - state has an array of continuous numbers that
describe the position of the lander, the velocity
06:02 - of the lander, its angle and angular velocity,
as well as which legs are in contact with
06:06 - the ground. The main idea is that the state
describes exactly what about the environment
06:12 - is changing. And each time step. The rules
that govern how the states change are called
06:16 - the dynamics of the environment, the actions
are a little simpler to understand. In the
06:21 - case of a maze running robot, the actions
would be just move up, down, left and right.
06:27 - Pretty straightforward. In the case of a lunar
lander, the action is consist of doing nothing,
06:33 - firing the main engine, firing the left engine
and firing the right engine. In both these
06:38 - cases, the actions are discrete, meaning they're
either one or the other. You can't simultaneously
06:43 - not fire the engine and fire the right thruster.
For instance, this doesn't have to be the
06:47 - case, though. actions can in fact, be continuous.
And there are numerous videos on this channel
06:52 - dealing with continuous action spaces. Check
out my videos on soft actor critic deep deterministic
06:58 - policy gradients, and twin delayed deep deterministic
policy gradients. From the agents perspective,
07:05 - it seeing some set of states and trying to
decide what to do. How is our agent to decide?
07:11 - The answer is something called the agents
policy, a policy of the mathematical function
07:15 - that takes states as inputs and returns probabilities
as output. In particular, the policy assigns
07:21 - some probability to each action in the action
space for each state. It can be deterministic,
07:28 - meaning the probability of selecting one of
the actions is one and the others is zero.
07:32 - But in general, the probabilities will be
somewhere between zero and one. The policy
07:37 - is typically denoted by the Greek letter pi.
And learning to be the environment is then
07:41 - a matter of finding the policy pi that maximizes
the total return over time by increasing the
07:46 - chances of selecting the best actions and
reducing the chances of selecting the wrong
07:50 - ones. The reward tells the agent exactly what
is expected of it. These rewards can be either
07:56 - positive or negative. And the design of rewards
is actually a tricky issue. Let's take the
08:01 - maze running robot. If we give it a positive
reward for exiting the maze, and no other
08:05 - reward, what is the consequence of that? The
consequence is that the agent has no motivation
08:10 - to solve the maze quickly, it gets the same
reward if it takes a minimum number of steps.
08:14 - Or if it takes 100 times at number, we typically
want to solve the maze as quickly as possible.
08:20 - So the simple reward scheme fails. In this
example, we have to give a penalty or a reward
08:25 - of minus one for each step and a reward of
zero for exiting the maze, then the agent
08:30 - has a strong motivation to solve the maze
in as few steps as possible. This is because
08:35 - the agent be trying to maximize the negative
reward, meaning get it as close to zero as
08:39 - possible. So now that we have our basic definitions
out of the way, we can start to think through
08:44 - the mathematics of the reinforcement learning
problem. From the agents perspective, it has
08:48 - no idea how its actions affect the environment.
So we have to use a probability distribution
08:53 - to describe the dynamics is probability distribution
is denoted p of s prime and are given s and
08:59 - a was just raised as the probability of ending
up in state S prime and receiving a reward
09:03 - are given we're in state s and take action
a. In general, we won't know the value for
09:08 - this until we interact with the environment.
And that's really part of solving reinforcement
09:13 - learning. Since we're dealing with probabilities,
we have to start thinking in terms of expectation
09:18 - values. In general, the expectation value
is calculated by taking into account all possible
09:23 - outcomes and multiplying the probability of
that outcome by what you receive in that outcome.
09:29 - So in our Markov framework, the expected reward
for a state and action pair is given by the
09:33 - expectation value of that reward, which is
the sum over all possible rewards outcomes
09:39 - also by by the sum over the probabilities
of ending up in all possible resulting states.
09:45 - For a simple example, let's consider a simple
coin toss game. If we flip a coin and it comes
09:50 - up heads, you get one point. If it comes up
tails, you get minus one point. If we flip
09:55 - the coin two times, what is the expected number
of points? It's probably of getting heads
10:01 - multiplied by the reward for getting heads,
plus a probability of getting tails multiplied
10:05 - by the reward for getting tails. So 0.5 times
one plus 0.5 times negative one. This gives
10:13 - an expected reward of zero points, which is
what you would intuitively expect. This is
10:18 - a trivial example, but I hope it illustrates
the point, we have to consider all possible
10:22 - outcomes, their probabilities and what we
would expect to receive in each of those outcomes.
10:27 - When we go to put theory and practice, we're
going to be doing it in systems that are what
10:32 - we call episodic. This means that the agent
has some starting state, and there is some
10:36 - terminal state that causes the gameplay to
end. Of course, we can start over again with
10:41 - a new episode. But the agent takes no actions
in this terminal step and thus no future rewards
10:46 - follow. In this case, we're dealing with not
just individual rewards. But with the sequence
10:52 - of rewards over the course of that episode.
We call the cumulative reward the return and
10:57 - it's usually denoted by the letter G. Now
this discussion is for games that are broken
11:02 - into episodes. But it would be nice if we
could use the same mathematics for tasks that
11:06 - don't have a natural end, we have a game that
goes on and on, and the total sum of rewards
11:12 - will approach infinity. It's absurd to talk
about maximizing total rewards an environment
11:17 - where you can expect an infinite reward. So
we have to do a little modification to our
11:20 - expression for the returns, we need to introduce
the concept of discounting, we're going to
11:26 - reduce the contribution of each reward to
the sum based on how far away in time it is,
11:31 - from our current time step, we're going to
use a power law to describe this reduction
11:36 - so that the reward gets reduced by some additional
power of a new hyper parameter we'll denote
11:40 - as gamma, gamma is between zero and one. So
each time we increase the power, the contribution
11:46 - is reduced. If we introduce gamma into the
expression for the return at time step t,
11:52 - we get to the return is just a sum over k
of gamma to the K and multiplied by the rewards
11:57 - at time t plus one plus K. Besides being a
trick to make sure we can use the same mathematics
12:03 - for episodic and continuing tasks is counting
as a reasonable basis and first principles.
12:08 - Since our state transitions are defined in
terms of unknown probabilities, we can't really
12:14 - say how certain each of those rewards were
states that we encounter further out in time
12:18 - become less and less certain. And so the rewards
for reaching those states are also less and
12:23 - less certain. And less, we shouldn't wait
them as much as the reward we just received.
12:28 - If you've been following along carefully,
something may not quite add up here. All this
12:33 - math is for systems with a Markov property,
which means that they depend only on the previous
12:37 - state and action. So why do we want to keep
track of the entire history of rewards received?
12:42 - Well, it turns out that we don't have to,
if you do some factoring in the expression
12:47 - with a return a time step t, you find that
the return at time t is just the sum of the
12:52 - reward at time t plus one and the discounted
return for the T plus one time step. This
12:57 - is a recursive relationship between returns
at each time step. It's more consistent with
13:02 - the principles of the Markov decision process,
where we're just concerned with successive
13:06 - time steps. Now that we know exactly what
the agent wants to maximize the total returns,
13:11 - and then function for how it's going to act,
the policy, we can actually start to make
13:16 - useful mathematical predictions. One quantity
a particular interest is called the value
13:21 - function depends on the agents policy pi and
the current state of the environment, and
13:26 - gives us the expectation value of the agents
returns starting from time t and state s,
13:30 - assuming it follows the policy pi as a comparable
function for the value of state and action
13:36 - pairs, which tells us the value of taking
action a in state s and then following the
13:40 - policy pi afterwards, it's called the action
value function and is represented by the letter
13:45 - Q. So how are these values calculated in practice?
Well, in reality, we don't solve these equations,
13:53 - we estimate them, we can use neural networks
to approximate the value or action value function.
13:58 - Because neural networks are universal function.
approximator is the sample rewards from the
14:03 - environment and use us to update the weights
of our network to improve our estimate for
14:06 - the value or action value function. estimating
the value function is important because it
14:11 - tells us the value of the current state and
the value of any other state the agent may
14:16 - encounter. Solving the reinforcement learning
problem then becomes an issue of constructing
14:19 - a policy that allows the agent to seek out
the most profitable states. The policy that
14:25 - yields the best value function for all states
in the state space is called the optimal policy.
14:30 - In reality, it can be a set of policies, and
they're all effectively equivalent. various
14:34 - schemes to find these optimal policies exist,
and one such scheme is called the actor critic
14:39 - method. In actor critic methods, we're using
two deep neural networks. One of them is used
14:45 - to approximate the agents policy directly,
which we can do because it's just a mathematical
14:49 - function. We call that the policy is just
a probability distribution over the set of
14:54 - actions where we take a state as input and
output a probability of selecting each action.
14:59 - The other network called the critic is used
to approximate the value function. The Critic
15:03 - acts just like any other critic telling the
actor how good each action is based on whether
15:08 - or not the resulting state is valuable. The
two networks work together to find out how
15:12 - best to act in the environment. The actor
slugs actions, the critic evaluates the states
15:17 - and then the result as compared to the rewards
from the environment. Over time, the critic
15:21 - becomes more accurate in estimating the advisor
states, which allows the accurate slide the
15:25 - actions that lead to those states, from a
practical perspective, are going to be updating
15:30 - the weights of our deep neural network at
each time step. Because actor critic methods
15:34 - belong to a class of algorithms called temporal
difference learning. This is just a fancy
15:39 - way of saying that we're going to be estimating
the difference in values of successive states,
15:43 - meaning states that are one time step apart,
hence temporal difference. Just like with
15:48 - any deep learning problem, we're going to
be calculating cost functions. In particular,
15:53 - we're going to have two cost functions, one
for updating our critic and the other for
15:57 - updating our actor. To calculate our costs,
we want to generate a quantity we'll call
16:03 - Delta. And it's just given by the sum of the
current reward and the discounted estimate
16:07 - of the new state and then subtracting off
the value of the current state. Keep in mind
16:11 - that the value of the terminal state is identically
zero. So we need a way to take this into account.
16:17 - The cost for the critic is going to be delta
squared, just kind of like a typical linear
16:21 - regression problem. The cost for our actor
is a little more complex, we're going to multiply
16:26 - the delta by the log of the policy for the
current state and action the agent took. The
16:31 - reason behind this is a little complex, and
it's something I go into more detail about
16:35 - in my course, where you can look for it in
the chapter on policy gradient methods in
16:39 - the free textbook by Sutton and Barto. So
let's talk implementation details, we're going
16:44 - to implement the following algorithm. initialize
a deep neural network to model the actor and
16:49 - critic. Repeat for a large number of episodes,
we set the score on terminal flagging environment,
16:56 - all the state is not terminal, select an action
based on the current state of the environment.
17:02 - Take the action and receive the new state
reward and terminal flag from the environment.
17:05 - calculate delta and use it to update the actor
and critic networks. Set the current state
17:11 - to the new state and increment the episode
reward by the score. After all, the episodes
17:16 - are finished, plot the trend and scores to
look for evidence of learning, there should
17:20 - be an overall increase in score over time,
you will see lots of oscillations because
17:24 - actor critic methods aren't really stable,
but the overall trend should be upward. Another
17:29 - thing you may see is that the score can go
upward for a while and then fall off a cliff.
17:33 - This isn't uncommon, because actor critic
methods are quite brittle. And they're really
17:37 - not the best solution for all cases, but they
are a stepping stone to more advanced algorithms.
17:43 - Other important implementation details, you
can use a single network for both the actor
17:48 - and critic. So you have common input layers
and two outputs, one for the actor and one
17:53 - for the critic. This has the benefit that
we don't have to train two different networks
17:57 - to understand the environment, you can definitely
use an independent actor and critic, it just
18:01 - makes the learning more difficult for an algorithm
and is already pretty finicky. What to play
18:07 - almost 2000 games with a relatively large
deep neural network, something like about
18:11 - 1000 units in the first hidden layer and 500
units in the second. The hard part is going
18:16 - to be the actor. As I said earlier, the actor
models the policy, which is a probability
18:22 - distribution, the actor layer will have as
many outputs as our actions. And we use a
18:26 - softmax activation because we're modeling
probabilities, and they'd better sum to one.
18:31 - When selecting actions, we're going to be
dealing with a discrete action spaces. So
18:35 - this is what is called a categorical distribution.
We're going to want to use the TensorFlow
18:40 - underscore probability package for the categorical
distribution, and then use the probabilities
18:44 - generated by the actor layer to get this distribution,
which we can then sample and use the built
18:50 - in log prop function for our cost function.
As far as the structure of our code, we're
18:55 - going to have a class for our actor critic
network. And that will live in its own file.
18:59 - We'll also have a class for our agent and
they'll have that functionality, choose actions
19:03 - save models and learn from its experience,
Matt goes in a separate file. The main loop
19:08 - is pretty straightforward, but it does go
in its own file as well. Okay, now that we
19:13 - have all the details out of the way, let's
go ahead and get started coding this. So now
19:21 - that we have all of our lectures out of the
way, we're going to go ahead and proceed with
19:24 - the coding. We're going to start with the
network's begin as always with our imports.
19:32 - So we will need OS to handle file joining
operations for model checkpointing. We will
19:39 - need carols and we will need our layers which
for this example is just going to be a dense
19:52 - layer. So we will have our actor critic network
and you See a case of converging engineering
20:03 - here where TensorFlow and pytorch both have
you derive your model class from the base
20:09 - model class. And we can go ahead and define
our constructor. That will take a number of
20:17 - actions as input the number of dimensions
for the first fully connected layer, we will
20:23 - default that to 1024. And for the second,
we will default it to 512. We will have a
20:34 - name for model checkpointing purposes, and
a checkpoint directory. Very important, you
20:42 - must remember to do a make directory on this
temp slash actor critic before you attempt
20:47 - to save a model. Otherwise, you're going to
get an error. The first thing you want to
20:51 - do is call your super constructor. And then
go ahead and start saving your parameters.
21:06 - Now, also very important for our class that
we've derived from the base class. In this
21:17 - case, the actor critic network class, we have
to use model name instead of name because
21:21 - name is reserved by the base class. So just
be aware of that not a huge deal checkpoint
21:31 - directory. And then we'll have our file and
that will be OS path join the directory name
21:45 - plus underscore Pacey. I like to use underscore
algorithm, in this case, AC for actor critic,
21:52 - in case you have one directory that use for
many different algorithms, if you're just
21:56 - using like, say a working directory, you don't
want to confuse the model types. Otherwise,
22:01 - if you have a good model saved, you don't
want to override it with something else. Now
22:05 - we'll go ahead and define our layers. And
that will be fully connected dense layers.
22:15 - The neat thing about Kairos is that the number
of input dimensions are inferred so we don't
22:19 - have to specify it. That's what we don't have
an input dims for our constructor, and it
22:24 - will output FC one dims with an activation
of rally, FC two will be similar. And then
22:36 - we will have two separate outputs. So we have
two common layers and then two independent
22:42 - outputs, one for the value function. And that
is single valued with no activation. And the
22:51 - second is our policy pi. And that will output
an actions with a softmax activation. Recall
23:01 - that the policy is just a probability distribution.
So it assigns a probability to each action.
23:06 - And those probabilities have to add up to
one because that's kind of what probabilities
23:10 - do, right? Next, we have to define our call
function. This is really the feed forward.
23:18 - If you're familiar with that from pytorch.
So we'll just use some generic name like value
23:24 - doesn't really matter, and pass through the
second fully connected layer. And then get
23:31 - our value function and our policy pi and then
return both value function and the policy
23:41 - pi. So that is really it. For the actor critic
network. All of the interesting functionality
23:45 - happens in the agent class. So let's go ahead
and start writing the agent class. So we'll
23:56 - begin as always with our imports. We will
need TensorFlow, we will need our optimizers.
24:08 - In this case, we're going to use an atom optimizer.
Probability it t we will need TensorFlow probability
24:18 - to handle our categorical distribution to
model our policy directly. You have to do
24:25 - a pip install TensorFlow probability before
you can run this. This is a separate package
24:30 - from TensorFlow. And we also need our actor
critic network. Let's go ahead and code up
24:39 - our agent. So our initializer is pretty straightforward.
We will need some default learning rate news
24:50 - 0003 It doesn't really matter I'm going to
pass in a specific learning rate in the main
24:54 - file. We will have a gamma of 0.99 and Default
in actions have some number like, say two.
25:04 - So we're going to go ahead and save our parameters,
we'll call the gam is our discount factor,
25:13 - we're gonna need a variable to keep track
of the last action we took. This will be a
25:18 - little bit more clear when we get to the Learn
function, and has to do with the way we calculate
25:22 - the loss because we have to use a gradient
tape for TensorFlow two, which is a bit of
25:26 - a workaround for how TensorFlow two does things,
we need our action space for random action
25:34 - selection. Let's use a list of actions from
zero to n actions minus one, we need our actor
25:44 - critic 
want to make sure to specify the number of
25:52 - actions and we want to compile that model.
So after critic compile with an atom optimizer,
26:03 - and learning rate defined by alphab. Next,
we have the most basic functionality of our
26:10 - agent, the functionality to choose an action.
And that takes the current state of the environment
26:19 - as input, which we have to convert to a tensor.
And in particular, we have to add an extra
26:27 - dimension batch dimension, the reason being
that the deep neural network expects a batch
26:34 - of inputs. And so you have to have something
other than a 1d array SB two dimensional,
26:40 - so we just add an extra dimension along the
zeroeth dimension. So then we will feed that
26:46 - through our deep neural network, we don't
care about the value of the state for the
26:50 - purpose of choosing that action, so we just
use a blank. And we will get the probabilities
26:57 - by passing the state through the actor critic
network. And then we can use that output the
27:03 - probabilities defined by our neural network
to feed into the actual TensorFlow probabilities
27:09 - categorical distribution, and then use that
to select an action by sampling that distribution,
27:14 - and getting a log probability of selecting
that sample. Sorry, that's TFP, categorical,
27:30 - and probabilities given by prompts, and our
actual action will be a sample of that distribution.
27:43 - And we don't actually need the log prompt
at this stage, we will need the log problem
27:47 - we calculate the loss function for our deep
neural network. But we don't need it now.
27:53 - And it doesn't make sense or rather, doesn't
actually work. To save it to a list, let's
27:58 - say for use later here. Because this calculation
takes place outside of the gradient tape.
28:03 - TensorFlow two has this construct of the gradient
tape, it's pretty cool. It allows you to calculate
28:07 - gradients manually, which is really what we
want to do here. But anything outside of that
28:12 - tape doesn't get added to the calculation
for backpropagation. So the log prob doesn't
28:17 - matter at this point, so why bother calculating
it. One thing we do need, however, is the
28:24 - action that we selected. So we will save that
in the action variable. And we will return
28:31 - a NumPy version of our action because action
is TensorFlow tensor, which is incompatible
28:37 - with the open engine. It does, however, take
NumPy arrays, and we want the zeroeth element
28:42 - of that because we added in a batch dimension
for compatibility with our deep neural network
28:47 - and a little bit confusing, but that is what
we have to deal with. Next, let's do a couple
28:53 - of bookkeeping functions to save and load
models won't take any inputs. And so it will
29:15 - save the weights of the network to the checkpoint
file. We do the inverse operation to load
29:21 - models. And we will load weights from a checkpoint
file. So that is it for the basic bookkeeping
29:44 - operations. Next we have the real heart of
the problem and the functionality to learn.
29:50 - This will take a number of inputs, we'll take
the state reward received new state and terminal
29:58 - flag as input The first thing we want to do
is convert each of those to TensorFlow tensors.
30:09 - And make sure to add a batch dimension. And
I like to be really pedantic with my data
30:16 - type. So I will cast it to tf float 32. And
we don't have to add a batch dimension to
30:40 - the reward, because it is not fed to a deep
neural network. So now we get to calculate
30:47 - our actual gradients using something called
the gradient tape. And we'll set persistent
30:57 - to true. I'm not actually sure that's needed.
I'm going to go ahead and experiment with
31:06 - that. One, we go ahead and run the code. But
I have it that way, I might have just copied
31:11 - and pasted code from somewhere else. So let
me double check. But we want to feed our state
31:18 - a new state through the actor critic network,
and get back our quantities of interest. So
31:29 - we feed the current state and then the new
state. But for the new state, we don't care
31:36 - about the probabilities, we just care about
the value. And that is for calculation of
31:44 - our delta. But for the calculation of our
loss, we have to get rid of that batch dimension.
31:50 - So we have to squeeze these two parameters.
And the reason you have to do that is because
31:59 - the loss works best if it's on a one dimensional
quantity, or rather a scalar value rather
32:06 - than a scalar value inside of brackets. So
it has to be a scalar instead of a vector
32:10 - containing a single item. It's just something
we have to do, I encourage you to play around
32:19 - with it to double check me on that I move
between I move between distributions, excuse
32:24 - me a framework. So sometimes, stuff isn't
always 100% necessary, even if it doesn't
32:30 - hurt anything. So we need our action probabilities
for the calculation of the log prob. TFP distributions
32:38 - categorical. We define our props by the output
of our deep neural network. And then our log
32:46 - prop is action probs dot log prop of the self
dot action. And this is the action that we
32:54 - saved up at the top when we calculate the
action for the agent. So this is the most
32:59 - recent action, then we calculate our delta.
That is reward plus gamma multiplied by the
33:10 - value of the new state times one minus end
of done. And the reason for that is that the
33:16 - value of the terminal state is identically
zero, because no returns no rewards follow
33:20 - the terminal state, so it has no future value.
and subtract all the state value. So our actor
33:29 - loss is minus log prob times that delta. And
the critic loss is delta squared, and the
33:39 - total loss, he goes after loss plus critical
loss. And then we can go ahead and calculate
33:47 - our gradients. So our gradient is taped out
gradient, total loss with respect to the trainable
33:55 - variables. optimizer apply gradients, and
it's expects a zip as input. So we're going
34:11 - to zip the gradient and the trainable variables.
Alright, and that is it for the actor critic
34:24 - functionality. So I'm going to come back to
this in a few minutes to see if I need that
34:29 - persistent I don't believe I do. I believe
I need this in the case when we have. If we
34:35 - were to have, say, separate actor and critic
networks and had to calculate greatest perspective,
34:40 - two separate sets of trainable variables,
I believe that's when the persistent equals
34:44 - true would be necessary. And when when we
had coupling between the loss of one network
34:50 - and the other, it's so that it keeps track
of the gradients, average does the back propagation,
34:56 - kind of like in pytorch, where it throws it
away and you have to tell it to retain the
34:59 - graph. I'll double check on that though. So
let's go ahead and write and quit. And then
35:05 - we're ready to go ahead and code up our main
file. So with our imports, we will need, Jim,
35:14 - we will need NumPy we will need our agent,
we all need our plot learning curve function,
35:28 - I'm not going to go into any detail on this,
it's just a function to plot data using matplotlib.
35:33 - With some labeled axes, it's nothing really
worth going into. first thing I'll do is make
35:42 - our environment. And I'm using the card poll,
because it runs very quickly. And the actor
35:51 - critic method is quite brittle, quite finicky,
you will observe in many cases where it will
35:58 - achieve a pretty decent score then fall off
a cliff because the learning rate was just
36:02 - a little bit too high. So there are a number
of problems with the algorithm. And it's easiest
36:07 - to test in a very simple environment. In my
course, we use the lunar lander environment,
36:12 - and I did more hyper parameter tuning to get
it to actually get pretty close to beating
36:17 - the environment, I believe. In this case,
we won't quite beat it, we achieve a high
36:20 - score like 140 points or so when beating it
is 200. But I leave the exercise of hyper
36:26 - parameter tuning to you, the viewer, I gotta
leave something for you to do as well, right.
36:31 - So we'll define our agent with a learning
rate of one by 10 to the minus five, and a
36:40 - number of actions defined by our environment,
action space underscore, and then we'll have,
36:47 - say, 1800 games, about 2000. With a file name
of card poll dot png, I would encourage you
36:57 - if you do hyper parameter testing, to put
the string representations of those hyper
37:03 - parameters here in the file name, so that
way, when you look at it later, you don't
37:06 - get confused. And you know what hyper parameters
were used to generate which plot. So our figure
37:12 - file is just plots plus the file name, I split
it up, you don't have to do it that way. We
37:18 - want to keep track of the best score received.
And it'll default to the lowest range. So
37:24 - that way, the first score you get is better
than the lowest. So the range is a you save
37:29 - your models right away. And if you list to
keep track of the score history, I Boolean
37:36 - for whether or not we want to load a checkpoint.
So if we're going to load a checkpoint, then
37:40 - I'm going to load models. And then finally,
we want to go ahead and start playing our
37:49 - games. We want to reset our environment, we
set our terminal flag, set our score to zero.
38:02 - And while we're not done with the episode,
we can choose an action 
38:10 - get the new state reward done and info from
the environment, increment our score, we're
38:21 - not loading a checkpoint, then we want to
learn. Either way, we want to set the current
38:32 - state to the new state. Otherwise, you will
be constantly choosing an action based on
38:38 - the initial state of the environment, which
obviously will not work. You also want to
38:48 - append the score to the score history for
plotting purposes, and calculate a score an
38:56 - average score of the previous almost say 100
games. And if an average score is better than
39:04 - your best score, then set the best score to
the average score. And if we're not loading
39:11 - a checkpoint, then save your models. So this
inner conditional statement keeps you from
39:20 - overriding your models that had your best
scores when you're actually testing. If you
39:27 - just saved the model every time then you'd
be overriding your best model with whatever
39:31 - which you know, may not be the best model.
So at the end, if we're not loading a checkpoint,
39:37 - actually, we can just plot either way and
let's do that. It's got our x axis and plot
39:48 - learning curve x score. Figure file. Okay.
Now I have to do a major On plots, temp that
40:06 - I call it temp slash actor critic. Otherwise,
this stuff won't work. And you also want to
40:16 - do a pip install. Flow TensorFlow probability,
because that is a separate package. Of course,
40:26 - I already have it. So let's go ahead and try
to run this. And see if I made any typos.
40:35 - I'm certain almost certain I did. So it says
something. Something is not callable. Oh,
40:42 - that's because I have forgotten my multiplication
sign. So that is in line 49. Yeah, things
40:52 - I'm trying to call something here when I really
want to multiply. Oh, you know what I did
41:10 - forget. One thing I did forget, of course,
is down here, I forgot my debug statements.
41:18 - So let's do this. That's pretty funny. episode,
I score. Set one F. 
41:37 - Always have to forget something, of course.
Okay, there we go. So one other thing I want
41:45 - to do is come back here to actor critic, and
get rid of this persistent equals true, I
41:52 - don't think I actually need this. Sometimes
I just copy and paste code. And then as I'm
41:57 - doing the video, I realize, Oh, hey, I don't
always need all of that stuff. Okay, so yeah,
42:05 - it does run. All righty. So I'm going to go
ahead and switch over to another window where
42:11 - I have let this finish up. Because there's
no point letting it run for another 1800 games.
42:18 - So let's go ahead and check that out. So here
you can see the output of the other 1800 games
42:26 - I ran. And it does achieve a score of around
160 768, about 170 points or so which is almost
42:34 - beating the environment is pretty close, you
take a look at the learning plot here, you
42:38 - can see that it has an overall overall upward
trend, and it's linear. And the reason I don't
42:44 - let it continue is because as I alluded to
in the lecture, these models are very brittle.
42:50 - And so sometimes you can get on a very narrow
slice of parameter space where your model
42:56 - is doing well in any small step out of that
range blows up the model. One thing to fix
43:02 - that is replay memory, as you get a broader
sampling of, of experiences and get a little
43:07 - bit more stable learning. But that doesn't
work by bolting directly on to actor critic
43:12 - methods, at least from my experience, I've
tried it I have a video on that. I wasn't
43:16 - able to get it to work. Maybe some of you
can. That would be fantastic if you could,
43:20 - but in my case, I didn't get it to work. I
thought it would work. It does not. And in
43:24 - fact, there's a whole separate algorithm called
actor critic with experience replay that deals
43:29 - with bolting experience replay on to vanilla
actor critic methods. So I hope this was helpful.
43:35 - It's pretty hard to give a really solid overview
and like a 3040 minute YouTube video, but
43:42 - it serves to illustrate some of the finer
points of Agile critic methods and some of
43:46 - the foundational points of deep reinforcement
learning in general. In my courses, I go into
43:50 - much more depth. And in particular, I show
you how to actually read papers, how to turn
43:54 - papers into code, useful skill that's really
hard to find anywhere else on YouTube or Udemy.
43:59 - So if you like the content, make sure to leave
a like, subscribe if you haven't already.
44:04 - And leave a comment down below with any questions,
comments, criticisms, concerns, and I will
44:10 - see you in the next video. Oh, really quick
before we do, let's check in on the other
44:16 - terminal to make sure it's actually learning.
So here is the output. And you can see as
44:21 - it's going along, it is saving some models
and the score is generally trending upward
44:25 - over time. And that's why you get the saving
models because the score is best in the last
44:30 - best score. So we didn't make any fundamental
errors in our code. If it doesn't achieve
44:34 - the same result that isn't entirely surprising
because there is a significant amount of render
44:38 - run variation. But the code is functional.
What's up on my GitHub. If you want to go
44:44 - ahead check it out. I'll leave a link in the
description. See you in the next one. Welcome
44:50 - to a crash course in deep deterministic policy
gradients, or ddpg for short. In this lecture,
44:58 - we're going to briefly cover the fundamental
concepts and some notes on the implementation,
45:02 - so that the stuff we do in the coding portion
isn't such a mystery. So why do we even need
45:09 - ddpg? in the first place? Well, ddpg exists
to answer the question of how do we apply
45:14 - reinforcement learning to continuous action
spaces. This is particularly important in
45:20 - something like say robotics, where we are
applying continuous voltages to electronic
45:25 - motors that cause the robot to move in three
dimensional space. Now, you may think, Well,
45:31 - why can't we just use something awesome, like
deep q learning. And that's not such a terrible
45:36 - idea. However, the problem is, it doesn't
work. In particular, q learning can't handle
45:42 - continuous action spaces. If you've coded
up a Q Learning Network, a deep learning network
45:47 - that is, you know, that it outputs discrete
numbers has actions. And of course, that doesn't
45:53 - cut it when you're dealing with continuous
action spaces. Now, you might think, why can't
45:57 - we just discretize our action space. And that's
not such a bad idea. But the problem is, it
46:03 - doesn't work. And the basic idea here is you
have some finite interval for your action
46:08 - space, and then you just divide it up into
a number of discrete chunks. And then every
46:13 - time you want to output the action in that
chunk, you just use that discrete integer.
46:19 - The problem with this is that these robots
tend to have many degrees of freedom, meaning
46:23 - they can move in many different directions
in space, right, they can rotate around axes,
46:27 - multiple axes, in general, they can move up,
down left and right, they can rotate. And
46:32 - so your number of discrete actions approaches
the 1000s very, very quickly. And so q learning
46:37 - while it does handle discrete action spaces,
doesn't handle large numbers of discrete actions
46:42 - particularly well. However, that doesn't mean
that the innovations from cue learning can't
46:48 - be applied to actor critic methods. And in
fact, they can, and that was the motivation
46:53 - behind the work done in the ddpg paper. In
particular, we're going to make use of a replay
46:59 - memory, where instead of just learning from
the most recent state transition that the
47:04 - agent has experienced, is going to keep track
of the sum total of its experiences, and then
47:09 - randomly sample that memory at each time step
to get some batch of memories to update the
47:15 - weights of its deep neural networks. The other
innovation is the use of target networks.
47:21 - So in cue learning, we have to do two different
things, we have to use a network to determine
47:28 - the action to take. And then we have to use
a network to determine the value of that action.
47:34 - And that value is used to update the weights
of the deep neural network. Now, if you're
47:40 - using the same network to do both things,
the problem is you end up chasing your tail,
47:44 - you end up chasing a rapidly moving target,
because each time step those weights are getting
47:49 - updated. And so the evaluation of similar
actions, excuse me, the evaluation of similar
47:54 - states, changes rapidly over the course of
the simulation causing the learning to be
47:59 - unstable. The solution to this is to keep
two networks, one of which use the online
48:06 - network is called to choose actions at each
time step. And then another network called
48:10 - a target network, to evaluate the values of
those actions when performing the update for
48:16 - your deep neural network. Now, in this case,
you will be doing a hard update of your target
48:24 - network. So every, let's say 1000 steps, it's
a hyper parameter v region, but a typical
48:29 - value would be 1000 steps, you would take
the values for the network parameters from
48:34 - the online network, and directly copy those
over to the target network. It's what's called
48:40 - a hard update. The authors of the ddpg paper
took inspiration from this, but instead of
48:46 - doing a direct, hard update, they use something
called a soft copy of the target networks.
48:51 - All this means is that we're going to be doing
some multiplicative constant for our update,
48:58 - and we're going to be using a new hyper parameter
called tau. And it's going to be a very small
49:02 - number of order point 001. It's also worth
noting that we're gonna have more than one
49:07 - target number here. And the reason we need
more than one target network is because DDP
49:13 - G is a type of actor critic method. And so
you have two distinct networks, one for the
49:18 - actor and one for the critic. And in fact,
in this implementation, we're going to have
49:22 - four different networks, one actor, one critic,
and then one target actor and one target critic.
49:29 - Now, for simple problems with discrete action
spaces, you can get away with having a single
49:35 - network, where the lower layers learn the
features of the environment and the upper
49:39 - layer splits off into outputting, the Act,
the critic evaluation as well as the output
49:46 - for the actor network. But in this case, we
do in fact want to totally distinct networks,
49:51 - as well as two copies of those for the target
networks. The basic idea is that our critic
49:59 - network is going to evaluate State in action
pairs. And so we're going to be passing in
50:03 - states and actions. And it's going to say,
hey, given that state, the action we took
50:07 - was pretty good. Or maybe that action was
pretty terrible, we could probably do better
50:11 - next time. And similarly, the actor is going
to decide what to do based on the current
50:17 - state or whatever state we pass into it. Something
worth noting is that this network is going
50:24 - to output action values, in other words, a
continuous number that corresponds to the
50:29 - direct input for opening a gym environment,
rather than outputting probabilities. So if
50:35 - you've been doing this for a while, you may
know that the policy is actually a probability
50:42 - distribution, it is a function that tells
us what is the probability of selecting any
50:47 - action from the action space given an input
of a state or a set of states. The deterministic
50:54 - part comes from the fact that ddpg outputs
the action values themselves. And it's deterministic
51:00 - in the sense that if I pass in one state over
and over again, I'm going to get the same
51:05 - action value out every single time. Now, this
does have a bit of a problem. So the problem
51:12 - is that the agent has something called an
explore exploit dilemma. And this is present
51:17 - in all reinforcement learning problems. It's
a fundamental concept in the field. The basic
51:22 - idea is that our agent is attempting to build
out a model of the world, the agent wants
51:26 - to know how to maximize his total score over
time. But it starts out knowing absolutely
51:31 - nothing about its world, it has to figure
out how states transition from one end to
51:35 - another and how its actions affect those states,
and in particular, how its actions give it
51:39 - rewards. So it starts out knowing none of
this and has to build out that model over
51:44 - time. The problem is, the agent can never
be quite sure that its model is accurate.
51:49 - No matter how long it spends playing the game
interacting with the environment, isn't 100%
51:54 - certain that the action it thinks is best
is actually the best. Perhaps there's some
51:59 - other strategy, some other action out there
that is significantly better. And the degree
52:03 - to which the agent takes off optimal actions,
is called the Explore exploit dilemma. So
52:10 - of course, taking off optimal action is called
exploration. And taking the optimal action
52:15 - is called exploitation, because you're just
exploiting the best known action. And this
52:19 - is a dilemma that is present in all reinforcement
learning problems. And the solution here is
52:25 - to take the output of our actor network and
apply some extra noise to it. Now in our implementation,
52:33 - we're going to be using simple Gaussian noise,
because it's sufficient for the problem at
52:37 - hand. However, in the original paper, the
authors use something called Orenstein lundbeck
52:41 - noise. It's a model of Gaussian processes
in physical systems, it's overly complex,
52:47 - and it's not needed. So we're not going to
implement it. Although in the course, I do
52:51 - show you how to implement it exactly. But
for YouTube, it's not really necessary. And
52:55 - in fact, when other authors implement ddpg,
they just throw that right out the window,
53:00 - because it's pretty dumb. Next is the update
rule for our actor network. And it is somewhat
53:06 - complex. So I'm going to show you the equation,
and then I'm going to walk you through it.
53:10 - So this is the update for the actor network.
And don't panic, this is a little bit easier
53:17 - than it looks at first glance. So from left
to right, we have the Nabla operator that
53:22 - is the gradient and the subscript there theta
super mu means that we want to take the gradient
53:29 - of the cost function J with respect to the
network parameters of our actor network, or
53:34 - the actress denoted by mu, and its parameters
are denoted by theta super mu. So theta Super
53:40 - Q means the parameters for the critic network
or the critic is denoted by Q. And it's just
53:47 - given by an expectation value or an average
of the gradient of the critic network. Where
53:55 - we're going to input some states and actions
with the actions are chosen according to the
53:59 - current policy. Okay. So, in practice, what
this means is, we're going to do is randomly
54:06 - sample states from the agent's memory. Now,
the memory keeps track of everything we want,
54:11 - it keeps track of the states, the agent saw
the actions that took and the new states that
54:18 - resulted from those actions, as well as the
reward the agent received at that time step
54:23 - and the terminal flag to determine whether
or not the episode ended on the new time step.
54:28 - So the scent the memory keeps track of all
of that. But here, we just want to sample
54:33 - the states the agent saw, okay, and then we're
going to use the actor network to determine
54:38 - what actions it thinks it should take based
on those states. Now, these actions will probably
54:44 - probably be different from the actions we
have stored in memory and that's okay. This
54:49 - is off policy, meaning we're using a separate
policy to gather data and use that data to
54:55 - update a different policy, the current policy
so It's ok that these actions don't match.
55:01 - Don't worry about that. It all works out in
the end, then you then the next thing you
55:07 - want to do is plug those actions from the
actor into the critic network along with the
55:12 - states, we sample from the memory and get
the value for the critic network what it thinks
55:17 - that state action pair is worth. And then
we're going to use the gradient tape from
55:21 - TensorFlow to to take the gradient of the
actor network, excuse me, the critic network
55:27 - with respect to the parameters from the actor
network. And we can do that because they're
55:32 - coupled through this selection of the actions
based on the actor and network. Okay, so it's
55:38 - a little bit complex. It's much easier in
code just in code, you sample the the states,
55:44 - then you get the actions based on the critic
network, and then plug those states and actions
55:48 - into the critic network. And then you just
have the loss proportional to that it's actually
55:54 - much simpler in code than it is on paper.
But that's the basic idea. So then the next
56:00 - question is, how do we implement our critic
network? Well, fortunately, it's a little
56:04 - bit more straightforward, and it's more reminiscent
of deep q learning. So we have this relatively
56:10 - simple loss function, that is the mean squared
error between these, this target value y sub
56:16 - i, and this Q for the current state and action.
So what we're going to do here is randomly
56:26 - sample states new states actions and rewards.
And then we want to use the target actor network
56:33 - to determine the actions for the new states.
Then plug those actions into the target critic
56:41 - network, to get your why and multiply it by
the discount factor gamma, and add in the
56:46 - reward from that time step, which you sampled
from the memory. And then that is a target
56:51 - the via we want to shift the estimates for
our critic towards. And then we want to plug
56:57 - the states and actions into the critic network.
In other words, the actions the agent actually
57:02 - took that we sampled from our memory, This
is in contrast to the update for the actor
57:06 - network, and take that difference with the
target. And then we're going to input it into
57:11 - the mean squared error loss function for TensorFlow
two. So in code, this is going to be relatively
57:16 - straightforward as well, we're going to have
a sample function to get the state's new states
57:20 - actions and rewards, we're going to plug in
the new states into the target actor network,
57:26 - get some output, we're going to plug that
output into the target critic to get our target
57:33 - y, and note, it is S sub i plus one. So what
is the new states that we get from our memory
57:40 - buffer. And then we're going to plug the current
states and actions from our memory into the
57:45 - critic and take the difference with the target.
So it only looks kind of scary on paper, when
57:50 - you see it written in code, it'll make much
more sense. So the other piece of the puzzle
57:56 - is how we're going to handle the updates of
our target networks. So at the very beginning
58:01 - of the program, we are going to initialize
our actor and critic networks with some parameters,
58:06 - of course, are going to be random. And then
we're going to directly copy those parameters
58:11 - over to our target actor and target critic
networks, that'll be the only time in the
58:15 - simulation that we do an exact hard copy.
every other time step, we're going to use
58:20 - the soft update rule. So here you have the
two parameters, theta Q, theta Super Q prime,
58:28 - so the weights of the target critic network,
and theta super mu prime, which is the weights
58:34 - of the target actor network, you're going
to update those with tau multiplied by the
58:41 - respective value of the online network, the
critic or the actor, and add in one minus
58:47 - tau times the current value of your target
actor or target critic network. And so this
58:54 - will be a very slowly changing function, because
there's going to be some small number tau,
58:59 - multiplied by some parameters plus one minus
tau is, which is approximately one multiplied
59:04 - by the current value. So it's going to be
approximately equal to its last time step,
59:09 - just plus minus a little bit. So it's relatively
straightforward. We'll see it in code. It's
59:15 - not all that bad. So then the next question
is, what data structures are we going to need
59:21 - for all of this? So we're going to use a class
to encapsulate our replay buffer, and that
59:26 - will use NumPy arrays, there are a myriad
of different ways to handle the replay buffer.
59:33 - I like the NumPy arrays because it's easy
to enumerate. It's easy to know, what is being
59:38 - stored where, and it makes for an easier YouTube
video because it's easier for people to see
59:43 - what's going on. If you have a different way
of doing it, by all means use that it's not
59:48 - something for which there was only one right
answer. We will need one class each for the
59:55 - actual network and critic network and those
will be handled using the TensorFlow two framework,
60:01 - these will have the functionality to perform
a four pass as well as the usual initializer
60:06 - function. We will also need an agent class
to tie everything together, the agent will
60:12 - have a functionality for the memory right
it will have a memory buffer to store memories,
60:19 - it will have the actor network critic network
target actor or target critic network, it
60:24 - will also have a function for choosing actions
based on the current state of the environment.
60:28 - And that will involve passing a state through
the actor network, getting the output and
60:33 - adding in some Gaussian noise. It will also
have functionality to learn from the memories
60:40 - it has sampled. And it will also have functionality
for checkpointing models because this can
60:45 - take quite a while to train for complex environments,
we're going to use a simple environment. But
60:50 - if you want to go ahead and try something
more complex, the checkpointing functionality
60:55 - will come in handy. Finally, we will need
a main loop to train our network and to evaluate
61:01 - its performance. Or that has been a very brief
introduction. Let's go ahead and get into
61:06 - the coding portion of our video. Alright,
now that the lecture is out of the way, it
61:12 - is time for a shameless plug. In my courses,
I show you how to go from paper to code rather
61:17 - than relying on someone else to break down
the material and then paper for you. It's
61:21 - the best way to gain independence. And to
level up your skill as a machine learning
61:25 - engineer provided through Udemy, they're on
sale right now check the link in the description
61:30 - below. Let's go ahead and start with our buffer.
So the point of our buffer is to store the
61:37 - state's actions rewards new states and terminal
flags and the agent encounters in its adventures.
61:44 - And we're going to use as I stated in the
lecture NumPy for this is going to be relatively
61:49 - straightforward. The reason I use NumPy is
because it makes it much more clear what everything
61:56 - is, it's just a little bit simpler and cleaner
from an implementation perspective, although
62:01 - it is by no means the only way to do things.
So the first thing we'll need is a max size,
62:06 - the memory is bounded, and input shape from
our environment, and a number of actions for
62:14 - our action space. Now in this case, since
it is a continuous action space, number of
62:20 - actions is a bit of a misnomer. What it really
means is number of components to the action.
62:26 - The purpose of the meme size is that the memory
cannot be unbounded. And our memory will have
62:31 - the property that as we exceed the memory
size, we will override our earliest memories
62:37 - with new ones. So for that, we will need a
memory counter it starts at zero. And then
62:43 - we can go ahead and start with our actual
memories. First we have a memory size, excuse
62:49 - me a state memory. And that will be in shape
memory size by input shape. We have the new
62:56 - state memory which is in the same dimensions
we need an action memory. And that will be
63:08 - and shape mem size by n actions. And one thing
I want to make clear there was a question
63:16 - on the discord server. By the way, also check
the link in the description for the discord
63:21 - server, we have many really bright people
in there talking about a lot of complex stuff.
63:25 - It's a great little community, I would encourage
you to join. But the question popped up whether
63:29 - or not I'm writing this stuff on the fly.
And I am not I don't know if that was clear,
63:33 - you can often see me looking off to the side
here. That's because it's not a second monitor,
63:38 - which is a really large monitor where I have
a second window open with the already completed
63:42 - code. And if you've seen my tutorials before,
you know that I make a lot of typos. And so
63:47 - the probability of making a logical error
where I have state instead of new state, for
63:51 - instance, is incredibly high. And that when
you are making YouTube videos is enormously
63:56 - painful to try to do everything from memory
and then swap states with new states and then
64:02 - not have it work and then have to go back
and re record. So it's just easier to have
64:07 - the already written code and I'm kind of reading
off of it as I go along. And occasionally
64:10 - I'll make modifications. So all the code is
mine, but it is not written on the fly. I
64:18 - just want to make that clear. Next we'll need
a reward memory and then as your shape mem
64:24 - size and that is just going to be an array
of floating point numbers. And we will need
64:29 - a terminal memory and that will be and type
Boolean. I use Boolean because in in pytorch,
64:44 - we can use masking of tensors I don't think
it works in TensorFlow. So we'll have to do
64:51 - something slightly different in our learning
function, but I will note that when we get
64:56 - there we will need a function to store a trend.
Whereas transition is the state action reward
65:03 - new state and terminal flag, the first thing
we want to do is determine what is the position
65:09 - of the first available memory. And that's
given by the modulus of the current memory
65:14 - counter and the memory size. Once we have
the index, we can go ahead and start saving
65:20 - our transitions. Yeah, see, I've already made
a mistake here. Then we have actual memory
65:48 - and terminal memory. And we want to increment
our memory counter by one. Now, one thing
65:59 - I don't think I stated in the lecture, the
purpose of this terminal memory is that the
66:04 - value of the terminal state is zero. Because
no future rewards follow from that that terminal
66:10 - state, we have to reset the episode back to
the initial state. So we have to have a way
66:16 - for accounting for that in our learning function.
And we do that by using the terminal flags,
66:22 - excuse me by using the terminal flags, as
a multiplicative constant in our learning
66:28 - function as a function to sample our buffer,
and that will take a batch size as input,
66:36 - we want to know how much of our memory we
filled up because we have a memory and it
66:41 - starts out as entirely zeros. And until we
fill up that memory, some portion of it will
66:45 - be nothing but zeros. It doesn't do us any
good to learn from a bunch of zeros. So we
66:49 - have to know how much of the memory we've
actually filled up. And this is given by the
66:53 - minimum of the memory counter or the memory
size, that we can take a batch of numbers,
67:01 - random choice maximum batch size. And, you
know, I think I want to pass the Replace equals
67:11 - false flag in there. I don't have that in
my cheat sheet. But the point of passing and
67:17 - replace equals false is that once a memory
is sampled, from that range, it will not be
67:23 - sampled. Again, that prevents you from double
sampling memories. If you're dealing with
67:26 - a really large memory buffer, the probability
of sampling two memories, two identical memories
67:31 - is astronomically small, but up until that
point, it's non negligible. So we should be
67:36 - careful. Then we want to go ahead and dereference
our NumPy arrays. And we will return those
67:51 - at the end 
68:05 - is a typo. Alright, that wraps up our replay
buffer class. Now we're going to go ahead
68:19 - and handle the network classes. Alright, so
our imports are relatively light, we will
68:25 - need OS for file path joining operations for
model checkpointing. We will need the base
68:32 - TensorFlow package we will need Kairos and
we will need layers. Now we're going to be
68:45 - dealing with a very simple environment. So
we just need a dense layer. We don't need
68:48 - any convolutional layers. So we will start
with a critic network. And that derives from
68:57 - the Karol stop model class that will take
a number of actions fully connected dims.
69:10 - a name and a checkpoint directory. Now very
important, you must do a make der temp and
69:21 - make der temp slash ddpg. Before we run this,
so that you can actually save your models
69:27 - otherwise we'll get an error. Let's go ahead
and call a super constructor 
69:36 - and then start saving our variables. We have
to call our model model name due to the TensorFlow
69:56 - package keeping name as a reserved variable.
You can't just say dot name, it'll give you
70:01 - an error. Not a huge deal, just something
to be aware of, then we'll have our checkpoint
70:06 - file. And chaos models get saved with a dot
h file extension. And I throw in the model
70:21 - name, because we're going to want to distinguish
between the target and regular networks. And
70:28 - I add an underscore ddpg. Because if you do
development in a single directory, then you
70:34 - don't want to overwrite models from say, TT
three with ddpg models vice versa. It's just
70:41 - a way of keeping all of your models distinct
and very secure. Next, we need to define our
70:49 - network. So our first fully connected layer
have a lot of put self that FC one dims with
70:56 - a rail you activation, then we'll have FC
two dims. Rail you and our final output layer
71:08 - is single valued with no activation. A couple
of things to note here is that in the original
71:15 - paper, the author's use batch normalization.
Turns out that isn't necessary. In the course
71:21 - I show you how to implement that. But it's
use an overly complex implementation, it doesn't
71:26 - actually add much to it. They also do a number
of kind of tweaks with the initialization
71:34 - of the layers, we're not going to do that
here, I show you in the chorus, but it's not
71:38 - really necessary. Next, we need our call function.
This is the forward propagation operation.
71:46 - And this is the critic so it takes a state
and action as input. So we want to pass the
71:53 - concatenated. State and action through our
first fully connected layer and we'll concatenate
72:02 - it along the first axis. The zero axis is
the batch. And then we will pass the output
72:12 - of that through the second fully connected
layer. And we will go ahead and get our Q
72:17 - value out and return it. That is it for the
actor network. Excuse me, critic network.
72:23 - Very, very straightforward. Next, we have
our extra network. And that is pretty similar.
72:40 - equals 512. And actions are just defaulted
to name equals actor. And check pointer. And
72:53 - you know what, now that I'm looking at this,
we don't need the number of actions here,
73:01 - do we because we don't use it. Let's get rid
of that. Yeah, I'm looking at my cheat sheet
73:08 - as well. These are some of the modifications
I make as I go along. Sometimes I'll have
73:12 - I'll have stuff that doesn't always make 100%
sense. And then when I do the YouTube video,
73:20 - I go ahead and rectify it. So we'll go ahead
and call our super constructor and save our
73:30 - values. This time, we will need the number
of actions because the critic network does.
73:39 - Excuse me, the actor network does need to
know how many actions it has. And we will
73:49 - need our checkpoint actor. And then our model
is going to be pretty similar to the critic
74:15 - network, there's going to be a number of fully
connected layers with a row activation. Now,
74:35 - we don't want a neural activation here no
activation or linear activation, if you will,
74:41 - we do want an actual function here. And what
we want is a function that is bounded between
74:46 - plus and minus one. And the reason is that
most of our environments have an action boundary
74:53 - of plus or minus one. However, if the action
boundary is larger than plus or minus one,
74:58 - it's easy to get the The action within that
range by multiplying a function which is bounded
75:05 - by plus or minus one by the upper bound of
the environment. So, if your environment has
75:11 - bound to plus or minus two, then he would
just multiply the tan hyperbolic function
75:15 - which is bound to plus or minus one by to.
Next, we have our call function, we'll go
75:30 - ahead and call it prob, it's a bit of a misnomer.
These aren't really probabilities. And then
75:39 - we will get our mu, which is from the paper,
it's our actual action. Now, if you had not
75:50 - plus or minus, one can multiply here, you
can multiply it there, or you can multiply
75:58 - it in the agent class, when we do the Choose
action function. Either way, is logically
76:03 - the same, I would probably do it in the agent
class, just for my own personal preferences,
76:09 - but either way, you want to multiply the output
of the deep neural network by the bounds if
76:15 - those bounds are not plus or minus one. All
right, that is actually it for the network
76:20 - classes. Let's go ahead and code the agent
class and tie all of this together. Okay,
76:30 - our imports will be quite numerous, we will
need NumPy. TensorFlow, we will need Kairos.
76:44 - We will need our optimizers. Which we use
Adam for this project, we will need our replay
76:54 - buffer. And we will need our actor and critic
network. I misspelled it not belux. Correct.
77:08 - So here's our agent class. And we have an
initializer. And that will take input dims.
77:17 - A learning rate for the actor network alpha,
a learning rate for the critic network beta.
77:25 - These are distinct, they're not the same.
And in fact that the critic networking can
77:30 - get away with a slightly higher learning rate
than the actor network. And that is because
77:35 - in general and policy gradient type methods,
the policy approximation is a little bit a
77:41 - little bit more sensitive to the perturbation
and parameters. So he wiggle around the parameters
77:46 - of your deep neural network, you can get big
changes in the output of the actor network.
77:51 - And so it has to have a slightly smaller learning
rate with respect to say, like the critic
77:56 - network, we will need our environment for
the max admin actions because as I said, on
78:03 - lecture, we're going to be adding noise to
the output of our deep neural network for
78:06 - for some exploration, and we have to clip
that into the maximum actions for environments
78:11 - we don't trigger an error. When we try to
pass that action into the open AI gym. We
78:17 - will need a gamma. And that is the discount
factor for update equation. number of actions.
78:24 - A MAX SIZE for our replay buffer defaulted
to a million a default value for our soft
78:33 - update of 0.005. That's from the paper. default
for FC one and FC two. In the paper, they
78:44 - actually use 403 100. I'm going to go ahead
and do that now rather than in the main program
78:51 - that makes life a little easier. A batch size
for a memory sampling and a noise for our
78:58 - exploration. So let's go ahead and save our
parameters gammon towel, we can instantiate
79:06 - our memory. We can save our batch size and
our noise 
79:31 - and then we'll go ahead and get the maximum
and minimum actions for our environment. Then
79:38 - we can instantiate our actual networks. So
our actor, our critic or target actor, And
80:14 - our target critic. That is it for our networks,
now we have to actually compile those we'll
80:36 - use our atom optimizer learning rate to find
by alpha. Similar we for our critic network,
80:53 - we will give it a learning rate of beta. And
then we have our target networks. That is
80:58 - a bit of a misnomer, because we aren't going
to be calling the we are going to be doing
81:13 - gradient descent for these networks, we're
going to be doing the salt network update.
81:17 - But we have to compile the network, so we
have to pass it a learning rate. It's just
81:22 - a feature of TensorFlow. Don't get confused.
If down in the Learn function, we don't actually
81:32 - call an update for the loss function for these
target actor and target critics, then we have
81:41 - to call our update network parameters function.
And this is where we do the hard copy of the
81:49 - initial weights of our actor and critic network
to the target actor and target critic network.
81:54 - And the passing of tau equals one to facilitate
that hard copy. Let's go ahead and write that
81:59 - function now. And so we have to deal with
the base case of the hard copy on the first
82:13 - call the function and the soft copy on every
other time we call the function. So tau is
82:20 - none. In other words, if we don't supply a
value for towel, and just use the default
82:25 - defined in the constructor, and so you notice
here, the first time I call it towel is one,
82:30 - so tau is not none. So we're going to be using
a value of one for tau instead of the 0.005.
82:37 - So we'll say weights is an empty list. And
our targets will be target actor weights,
82:46 - or eye weight. We're going to iterate over
the actor weights and append the weight for
83:00 - the actor multiplied by towel, plus targets
sub i know, which is the weight from the target
83:06 - actor times one minus towel. And then after
we go through every iteration of the loop,
83:14 - we're going to set our weights for the target
actor to that list of weights. So in the first
83:23 - iteration, towel starts out as one, and so
you have weight times tau, which is one, so
83:32 - just the actor weight plus the target actor
weight, multiply by one minus tau, which is
83:39 - one minus one or zero. So on the first iteration,
we get a hard copy. And then we do the same
83:46 - thing for the critic network. And that is
all we need to do for our software update
84:15 - rule. So now we'll go ahead and write an interface
function for our agents memory. They'll take
84:21 - a state action reward new state, and terminal
flag as input. And then we'll just call the
84:29 - store transition function. And this is just
good clean coding hygiene. It's an interface
84:39 - function. When you have interdependent classes,
the in theory the we shouldn't be able to
84:45 - call the agent dot memory dot store transition
from anywhere else in the program except within
84:51 - its own member function. It's just basic,
object oriented programming stuff. Now let's
84:58 - deal with model checkpointing. And we'll say
self dot actors save weights after a checkpoint
85:12 - file. And likewise for our other networks
should be a dot underscore. I think I've done
85:27 - that before. Specific mistake also have an
extra space here. And the load models function
85:51 - is just the inverse operation. We say what
is it self dot, dot load weights? dot checkpoint
86:07 - file. Critic check one fall Yep. Okay, that
is it for our basic bookkeeping type functions.
86:41 - Now we can get into the functionality for
choosing an action. And that will take the
86:47 - observation of the current state of our environment
as input, as well as a flag I call evaluate.
86:55 - This has to do with training versus testing
our agent, remember that we use the addition
87:01 - of noise for the exploration part of the Explore
exploit lm, if you're just testing the agent
87:07 - to see how well it learned, you don't necessarily
want to add that noise, you can just do the
87:13 - purely deterministic output of your actor
network. And I facilitate that with a Boolean
87:18 - flag here. First thing you want to do is convert
our state to a tensor. And we have to add
87:28 - an extra dimension to our observation to give
it a batch dimension. It's just what the deep
87:36 - neural networks expect as input, they expect
the batch dimension. And I specify a float
87:42 - 32 data type to be pedantic. And then we pass
the state through the actor network to get
87:48 - the actions out. If we are not evaluating,
in other words, if we are training, then we
87:56 - want to get some random normal noise in the
shape of self dot n actions, with a mean of
88:05 - 0.0 and a standard deviation of whatever our
noise parameter is. Now, it's entirely possible
88:14 - that the output of our deep neural network
was one or point 999. And then when you add
88:21 - in the noise, you're adding in something like
let's say 0.1. And you end up with an action
88:26 - that is outside the bounds of your environment
biggest perhaps is bounded by plus or minus
88:31 - one. And so you want to go ahead and clip
that to make sure you don't pass any legal
88:35 - action to your environment. We'll say actions
equals TF clip, by value as a function we
88:42 - want actions as between self min action and
Max. Action. And then we want to return the
88:52 - zeroeth element because it is a tensor and
the value is the zeroeth element, which is
88:57 - a NumPy array. Then we want our learning function.
And this is where the bulk of the functionality
89:04 - comes in. And right away, we are faced with
a dilemma. So what if it's the case that we
89:11 - haven't filled up at least batch size of our
memories. So remember that the memory starts
89:16 - out as all zeros. And if you've only filled
up, let's say 10 memories, you don't really
89:21 - want to sample those 10 memories, you know,
batch number of times batch size number of
89:28 - times. So you can just go ahead and say well,
I'm not going to learn I'm going to wait until
89:32 - I fill up my memory into until at least batch
size number of memories. Alternatively, you
89:41 - can play batch size number of steps with random
actions, and then call the Learn function.
89:49 - That's another solution as well and they're
both valid. I just find this to be a little
90:00 - Bit more straightforward. So then we have
to go ahead and sample our memory. And then
90:15 - we can go ahead and convert these two tensors.
New states, singular, not plural. And we don't
90:58 - have to convert the terminal flags to a tensor.
Because we're not going to be doing tensor
91:03 - operations with it, we're going to be doing
regular NumPy array type operations. So we're
91:11 - going to use a gradient tape for the calculation
of our gradients. If you're not familiar with
91:16 - a gradient tape, the basic idea is we're going
to go ahead and load up operations on to our
91:22 - computational graph for calculation of gradients.
So when we call the Choose action function
91:28 - above, those operations aren't stored anywhere
that is used for calculation of gradients.
91:35 - So that's effectively detached from the graph.
So only things within this context manager
91:40 - are used for the calculation of our gradient.
And so this is where we're going to stick
91:43 - the update rule from our lecture. So let's
go ahead and start with the critic network,
91:49 - where recall that we have to take the new
states and pass it through a target actor
91:53 - network, and then get the target critics evaluation
of the new states and those target actions.
92:00 - And then we can calculate the target, which
is the reward plus gamma multiplied by the
92:07 - critic value for the new states times one
minus a terminal flag, and then take the mean
92:11 - squared error between the target value and
the critic values for the states and actions
92:17 - the agent actually took. So let's go ahead
and write that out. So target actions is given
92:25 - by a target actor. What are the things we
should do for the new states and the critic
92:33 - value for those new states, that's going to
be given by the target critic evaluation of
92:40 - those new states and target actions. And squeezed
along the first dimension, we have to put
92:52 - in this squeeze because we have the batch
dimension and it doesn't actually learn if
92:56 - you pass in the batch dimension, I feel like
I am I missing there we go parenthese. So
93:04 - we'll have a squeeze everywhere that we had
to actually pass up through the network. So
93:09 - then the critic value, which is the value
of the current states, the original states
93:13 - and the actions the agent actually took during
the course of the episode is given by the
93:18 - squeezed output of the critic, or the state's
actions along the first dimension. I guess
93:31 - it is right. Yep. And then we have our targets,
and that's reward plus gamma, times this critic
93:40 - value underscore times one minus done. So
this one minus done is one minus true or false.
93:49 - So when the episode is over, done is true.
And so you have one minus one or zero. And
93:55 - so the target for the terminal new state is
just the reward, whereas for every other state,
94:01 - it is reward plus the discounted value of
the resulting state, according to the target
94:05 - critic network. I'm sorry, you can hear the
landscapers outside is Tuesday they're doing
94:10 - the neighbor's yard. Hopefully the noise suppression
filter takes care of that, but if not, I apologize.
94:17 - Next we have a critic loss. And that's the
mean squared error between our target and
94:24 - the critic value. outside of the context manager,
we want to go ahead and calculate the gradient
94:30 - so we'll say credit network gradient was taped
out gradient critic loss self critic trainable
94:41 - variable so it is the gradient of the critic
loss with respect to those critics trainable
94:49 - variables, and then we want to apply our gradients.
So our optimizer dot apply gradients and that
95:01 - expects a zip as input. We want to zip up
the critic network gradient and the critic
95:09 - trainable variables and that is it for the
critic loss. Now we have the actor loss. So
95:18 - with, we want to do essentially the same thing
we want our context manager gradient tape
95:23 - as tape then we say new policy actions accurate
states. So, this is the these are the actions
95:33 - according to the actor based on its current
set of weights, not based on the weights it
95:39 - had at the time of whatever memory we stored
in the agent's memory. So, then we have our
95:45 - actor loss. And that is the negative of the
critic output of the states and the new policy
95:52 - actions, it's negative because we're doing
gradient ascent and policy gradient methods,
95:59 - you typically want to take the, you don't
want to do a gradient descent because that
96:04 - would minimize the total score over time you
want to maximize total score over time. So
96:08 - you do gradient ascent, gradient descent is
just the negative of gradient descent. So
96:12 - we stick a negative sign in here. And then
our loss of just the reduced mean of that
96:22 - after loss. And then we can go ahead and calculate
our gradients and apply them. So the actor
96:33 - network gradient is taped out gradient of
the actor loss with respect to the actor trainable
96:40 - variables, and this is how we're going to
get that gradient of the critic loss with
96:46 - respect to the mu parameters of theta super
mu, is by taking this actor loss, which is
96:52 - proportional to the output of the critic network.
And that is coupled, the gradient is nonzero
96:58 - because it has this dependency on the output
of our actor network. So the dependence that
97:03 - gives you a nonzero gradient comes from the
fact that we are taking actions with respect
97:09 - to the actor network, which is calculated
according to faders theta super mu. And that
97:17 - gets fed forward through here through the
critic network. That's what allows you to
97:20 - take the gradient of the output of the critic
network with respect to the variables of the
97:25 - actual network, it's how we get that coupling.
And if you read the paper, they actually apply
97:30 - the chain rule and you get the gradient of
the critic network and the grading of the
97:35 - actual network. This form is actually easier
to implement in code. That's why I do it based
97:39 - on the first equation, not the second equation
in the paper, it's just easier to do. So why
97:45 - not do it the easy way. Then you want to apply
those gradients, which again, takes a zip
97:54 - as inputs. We want to apply zip up our attic
radio network, and the actor dot trainable
98:05 - variables. One other thing I want to point
out is that this accurate, trainable variables
98:10 - we didn't define, it comes from the fact that
we derive our actor network class from the
98:16 - Kairos dot model class. It's just comes from
the properties of object oriented programming.
98:24 - Once we have updated the main networks, we
want to go ahead and perform the soft update
98:28 - of our target networks. And since this is
not the first time we're calling the function
98:36 - that gets no input, so we'll use the default
value for tau of 0.005. And that is it 113
98:43 - lines for our agent class. Now we're ready
to write up the main loop and test it out
98:48 - to see how well it does. Okay, I have an error,
it says keyword cannot be an expression. Let's
98:56 - see rewards to convert to tensor. Where have
I gone? Wrong? Right here, it's a period instead
99:08 - of a comma. All right. Now we're good. So
let's go ahead and start with our imports.
99:19 - We have Jim we have NumPy. We have our agent
and are plot learning curves. So go to my
99:35 - GitHub, do a git clone and get those utils.
It is just a matplotlib function to plot our
99:42 - learning curve, which is the score versus
time the running average of the previous 100
99:46 - games. Over time, it's nothing magical. I
don't go over it because it's relatively trivial
99:52 - doesn't really contribute to your understanding.
You can just do a plot of the scores over
99:56 - time to see if it's learning 
So we start with our making our environment.
100:07 - And we use the pendulum and one stanchion,
our agent, getting the observation space,
100:18 - from our environment for input dims, passing
on the environment, shape. And we will use
100:31 - the default value for our noise and every
other parameter because those were good defaults.
100:41 - let it play 250 games. Let's go ahead and
to find a figure file, pendulum dot png, we
100:53 - need to keep track have the best score. That's
the lower bound of our reward range. And to
101:00 - keep track of the history of scores, Agent
receives, and a load checkpoint, that'll be
101:07 - false. That's if you want to set up our training
versus testing. If we're going to load that
101:15 - checkpoint, then 
we want to set number of steps to zero while
101:23 - n steps is less thing agent dot batch size.
What we're doing here, okay, so I should explain
101:34 - this. So my understanding and this could be
wrong. If it's wrong, drop a comment down
101:38 - below to correct me, I don't profess to know
everything about TensorFlow. But from what
101:43 - I've read from Google, the model loading is
set up such that you have to call the learning
101:53 - function before you can load your model. That's
because when you instantiate the agent, you
102:00 - aren't actually loading any values onto the
graph. And so it's basically an empty network
102:04 - with no values loaded into that network, it
doesn't load any violations, he tried to do
102:08 - something with it. And so we're going to go
ahead and fill up the agents memory with dummy
102:13 - variables, dummy values, they don't really
matter, we're just going to go and load it
102:18 - up with dummy values, and then call the agents
learn function so we can load our models.
102:25 - And so we can do a random action doesn't really
matter. Get our new state, reward dawn info
102:34 - from our environment. And then remember that
very important one increment number of steps.
102:49 - Once you've done that, call the Learn function
and load your models. And said, evaluate.
102:57 - Value eight true. And if we're not going to
be loading our checkpoint, we'll just say
103:07 - evaluate equals false, I guess we could just
use load checkpoint in place of evaluate our
103:14 - call it evaluate, but whatever I've used to
separate variable assuming. So for i in range
103:20 - and games, we want to go ahead and reset our
environment. At the top of every episode,
103:26 - reset the terminal flag and the score to zero.
While the episode is not done, Beijing can
103:37 - choose an action based on the observation
and the evaluate flag. Good the new state
103:45 - reward Don and info from the environment.
Incorrect our scoring, and store that transition
103:58 - for not loading a checkpoint that we want
to learn. And the reason I put in that conditional
104:18 - statement is because if you're evaluating
the performance of the agent, you probably
104:22 - don't want to disturb its parameters, you
want to just go ahead and see how it performs.
104:28 - As of the last time you saved it, rather than
trying to get to learn a little bit more.
104:33 - Feel free to change that. And very importantly,
we want to set the current state of the environment
104:38 - to the new state after the agent took its
action. So then we want to at the end of every
104:47 - episode, we want to append the score and calculate
the average to get an idea of whether or not
104:58 - our agent is learning If the average score
is better than the best known score, then
105:04 - set the best score to that average score.
And, again, if we aren't loading a checkpoint,
105:13 - go ahead and save your models. And at the
end of every episode, we want to print some
105:20 - basic debug information. So I score 
105:35 - and at the end of all the episodes we want
to print our plot our learning curve. So our
105:43 - x axis number of games. Okay, that is it for
our main loop. Let's go ahead and test it
105:59 - out to make sure I made a sufficient number
of typos. But as I said, the first thing I
106:05 - want to do is make dir temp. Okay, I already
have that to make dir temp slash ddpg. I didn't
106:12 - have that and make der plots. I think that
already exists. Okay. Now we can go ahead
106:17 - and run the main file and see what I messed
up. Okay, so Oh, yeah, of course our critic
106:26 - network 
does not get a number of actions. That's something
106:34 - I changed on the fly. So let's fix that here
as well. We go ahead and put this back up
106:54 - here. I think that is right. Let's try it
again. agent has no it's because its target
107:10 - actor dot weights not target actor underscore
weights. That is in line 39. So right here
107:22 - that I do. Nope, the target critic was correct.
Okay. has no attribute men sighs Okay. Oh,
107:42 - it's in action. That's in line. 73. Yeah,
it's mean action. And Max action. Of course.
107:59 - All right, that's in main line. 41. See, this
is why I have a cheat sheet because even even
108:10 - with the cheat sheet, I make a number of typos.
So you can imagine what it's like if I were
108:14 - to try to do it on camera that is in line
41 if not load checkpoint. There we go. Good
108:29 - grief call takes three positional arguments,
but four were given all that's because I have
108:35 - my parentheses in a wrong place. Okay. That
is in line 92 so the 
108:50 - state's target actions so the parenthese goes
here. Yeah, that's right. Oh my goodness concat
109:06 - missing one required positional argument Oh,
because I have the because I have an extra
109:14 - Brenda z okay. So, this is bad, even for me,
line 23. So, then we need a second parenthese
109:32 - there. Critic trainable underscore variables
once again that is a trainable dot variables.
109:43 - That is in line 90. I thought I looked for
that. Oh, no, I didn't critic dot trainable
109:56 - variables. I did the same thing here. Alright,
what I just fixed that, did I not? line 100?
110:12 - Did I just fix that? No, I did not just fix
that. Right? Okay. Actor dot critic dots.
110:21 - Okay, this is what happens when you don't
do this for a month. making YouTube videos
110:28 - is a perishable skill. Okay, perfect. Now
what is actually working, I've got gotten
110:36 - through all of those typos, and it has started
to run. So I'm gonna let this go ahead and
110:41 - finish up and we're going to see how it does.
Alright, so it has finished running. And you
110:49 - can see that at the end, it kind of tapered
off a little bit in performance. If we scroll
110:54 - up, you can see that about halfway through
it was achieving record performance with pretty
110:59 - much every single simulation that isn't entirely
a typical with actor critic type methods.
111:07 - So oftentimes, what will happen is the agent
will achieve some reasonable performance and
111:11 - then kind of started to taper off. Because
the as I said, the actor network is relatively
111:16 - sensitive to changes in its parameters. In
this case, it didn't fall off a cliff like
111:20 - I've seen with things like actor critic or
policy gradient methods, but it is still nonetheless
111:25 - sensitive to changes in its weights, and is
prone to deteriorations of performance late
111:32 - in the number of simulations. If you take
a look at the learning curve, you can see
111:37 - pretty clearly that it has an overall upward
trend over time. So it is in fact learning
111:41 - our technique is working. It's doing its job,
but it's not, you know, it's nothing, at least
111:47 - for this environment. It's not the greatest
thing since sliced bread, we could do a little
111:51 - bit more tuning to get it even better. But
for now, I think this is sufficient to a demonstrate
111:57 - that it works and to be have a solid tutorial
on how to implement deep deterministic policy
112:01 - gradients. Once again, shameless plug if you
want to know how to go from paper to code,
112:06 - I show you how in my two Udemy courses on
deep reinforcement learning, where we go through
112:11 - several papers per course, one on deep learning
one on actor critic methods, and implement
112:16 - all these algorithms from scratch, I show
you how to implement pretty much everything
112:19 - from the papers minus some super superfluous
features. Either way, if you made it this
112:27 - far, please leave a like, subscribe, drop
a comment down below, and I'll see you in
112:32 - the next video. Welcome to a crash course
in 20, late deep deterministic policy gradients
112:41 - or TD three for short. This is a very brief
lecture that's going to cover the fundamental
112:45 - concepts and implementation notes for our
coding tutorial, which will follow this lecture.
112:51 - If you want the full details of how this algorithm
works, I highly recommend you read the paper
112:55 - titled addressing function approximation error
and actor critic methods. It's very well written
113:01 - very technical, very detailed, and it covers
significantly more detail and depth than I
113:06 - will do here. I'm just going to give you broad
strokes, and some idea of what we're going
113:09 - to be doing in our coding tutorial. And we're
going to be using TensorFlow too, by the way
113:14 - for our coding tutorial, so make sure you
have that installed. So TD three exists to
113:21 - deal with a fairly straightforward issue.
How do we deal with overestimation, bias and
113:27 - continuous action space actor critic methods,
if you're not familiar with over estimation
113:32 - bias is a tendency of agents to incorrectly
estimate the value of a state on the high
113:38 - end. So it says this state is worth more than
it actually is. And this is a problem because
113:43 - the agent will attempt to access that state
in the future, which leads to a suboptimal
113:48 - policy because there are other more profitable
states out there. So this is a pretty big
113:54 - problem, particularly when you're trying to
approximate the agents policy as we are in
113:58 - actor critic methods. But it's not clear where
this would even come from. in Q learning,
114:04 - we get biased because we take a max over our
actions in our update rule for the either
114:08 - the table or our deep neural network. So there
is some maximization or overestimation built
114:13 - in right from the beginning. And that's pretty
easy to see. But there is no max in our update
114:19 - rule for actor critic methods. So what gives?
Where could this overestimation bias come
114:23 - from? Well, on a somewhat theoretical level,
I kind of argue they don't do this in the
114:30 - paper, but the way I think of it is that we
have Stochastic gradient descent, or we're
114:35 - attempting to maximize the product of our
probabilities or policy and the rewards or
114:42 - returns the agent receives over time. So there
is some implicit drive to maximize score over
114:50 - time. And so when you get natural variation
in your rewards, or in the trajectory through
114:55 - the state space, you can end up with some
incorrect estimations. of the visor states
115:01 - because of natural, you know high variance,
which is typical for deep reinforcement learning
115:06 - problems. However, more fundamentally, overestimation
comes from approximation errors. Now, this
115:13 - isn't something they prove in the paper, because
this is a result that dates all the way back
115:16 - to the early 1990s, when people were just
starting to talk about q learning. And so
115:22 - it's been known for a while. But the basic
idea is that when you attempt to use some
115:28 - mathematical apparatus, to estimate a function
in high dimensional space, you get approximation
115:33 - errors. And that results in over estimation.
Now, what is our source of overestimation
115:40 - or approximation error, in this case, it's
going to be a deep neural network. Neural
115:45 - Networks are, of course, universal function
approximator. Mostly, there are obviously
115:49 - exceptions, you know, discontinuous functions,
they can choke on the function has to be differentiable
115:53 - and all that good stuff. But for all the stuff
we deal with, neural networks are going to
115:58 - be a universal function approximator. And
so that leads to a source of error. Now, this
116:04 - is inherent to any approximation method, either
tiling, like, binning anything like that.
116:12 - So it's not that neural networks are bad,
it's just whenever you have a function approximation,
116:17 - you're going to have some error. Okay, because
you don't have an infinite amount of time
116:20 - to collect an infinite number of samples to
get infinite precision, you're always going
116:24 - to have to lop off your estimate, at some
point where there are more decimal points
116:28 - waiting for you, should you be able to collect
more data. Worse yet, actor critic methods
116:34 - are bootstrapped? Well, this means is that
we're going to be using our initial estimates
116:39 - to perform updates to later estimates. And
so you have some errors in your early estimates
116:45 - that propagate to your later estimates over
time, so you get an accumulation of error
116:51 - over time. Naturally, there are other ways
of dealing with this. And in fact, double
116:56 - q learning uses a rather ingenious solution
they use in that algorithm, two different
117:02 - cue networks, and you alternate their use
in the update rule. And so you're never taking
117:06 - a max over actions of the same network that
you use to choose an action when you're trying
117:11 - to update the value of that action. And so
it seems like it's a reasonable thing to try.
117:16 - So in particular, that's what they do. Now,
I want to point something out that in double
117:20 - q learning, deep, double, deep, deep double
q learning, they use a not exactly analogous
117:28 - solution to the tabular case of double q learning,
they use something slightly different, which
117:32 - doesn't work an actor critic methods, as they
detail in the paper, they're going to use
117:37 - a more exact replica of the tabular version
of double q learning. And they're going to
117:44 - perform a slight modification where they're
going to clip the inputs of the actions to
117:49 - the cue networks, the double cue networks,
which is going to tend to underestimate and
117:55 - we're going to be taking a min. So later on,
you'll see in the algorithm that we're going
117:59 - to feed some clipped actions to our cue networks,
and then take the minimum. So whatever the
118:05 - minimum value is, we're going to take that
which tends to underestimate. Now you may
118:10 - say, Dr. Phil, isn't that a problem? Not really.
And the reason it's not a problem is because
118:16 - if we underestimate actions, those actions
become less attractive to the agent. And so
118:23 - it's not likely to take those actions again.
And so that kind of dampens that out over
118:26 - time. It's a natural feedback mechanism to
deal with that issue. So that is rather nice.
118:32 - And it's baked right into the algorithm. The
other innovation is that they're going to
118:37 - delay policy updates, to give the credit Network
Time to converge. So the Policy Network is
118:46 - very slowly changing function, or as the queue
network can change much more quickly. So you
118:52 - have two different timescales there. And that's
where the delayed part comes from. Another
118:57 - innovation is we're going to use target networks
for the actor and both critics. Since we're
119:02 - doing an analogue of double q learning, which
uses two cue networks, it stands to reason
119:06 - we're gonna have two critics. And we're going
to target networks for all the things so we're
119:10 - going to have an actor, a target actor, to
critics, and to target critics. So total of
119:16 - six deep neural networks, we're gonna have
all the neural networks in the world. The
119:23 - other thing we're going to need is a salt
update to the target networks. So you have
119:28 - a couple of options for updating the weights
of retarget networks. One is you could do
119:33 - Stochastic gradient descent on those directly.
Another option is to take the parameters from
119:40 - your online networks that you're actively
performing gradient descent on and copy those
119:44 - to the target networks directly. Or a third
option is to do some slowly varying interposed
119:50 - between the two. So you're going to update
the online networks every time separately
119:56 - n time steps in the case of the policy, and
then you're going to do some slowly Changing
120:00 - update to those target networks. And that'll
introduce an additional hyper parameter called
120:05 - tout, or agent, which you will see later.
So this is an actor critic method. And we're
120:12 - going to have actually, six distinct, I have
a typo there. Sorry, I can't count despite
120:17 - having a PhD in physics. So we're going to
have an actor to critics target actor. And
120:23 - then to target critics. As I said, the purpose
of the critic is to be critical it is to evaluate
120:29 - the values of states and action pairs. So
it says, Hey, in this particular state, we
120:33 - took some action, I think this was valuable
or not. And that will help to update the agents
120:39 - estimate of the VI's of those pairs, and to
choose better actions for given states over
120:44 - time, the actor will decide what to do based
on the current state. And it's important to
120:52 - note here that our deep neural network is
going to output action values, which are continuous
120:56 - numbers, not discrete numbers, not probabilities.
Now, that makes us a deterministic algorithm.
121:02 - That's where the name of the algorithm comes
in twin delay deep deterministic policy gradients
121:07 - is an extension of deep deterministic policy
gradients. And so we have a bit of a problem
121:13 - there. Because we're dealing with approximation,
right? We're approximating a cue function,
121:17 - we're approximating a policy, we're approximating
all the things, and we never know exactly
121:22 - how accurate our approximations are. And so
we never know if we are correctly evaluating
121:29 - states and action pairs. So we never know
if given some state if this action is really
121:33 - the most beneficial action we could possibly
take. Or if there's some other, more beneficial
121:38 - action out there waiting to be discovered.
That is called the Explore exploit dilemma.
121:43 - Do we explore sub optimal actions or exploit
what we think are the most beneficial actions.
121:48 - And the extent to which you engage in those
two activities is the dilemma. And there are
121:52 - a number of solutions to that deep learning
uses epsilon greedy action selection, where
121:56 - you just take random actions, some fixed proportion
of the time, but in this case, we're going
122:01 - to be adding noise to the output of our actor
network, when we decide what actions to take.
122:10 - The update rule for actor looks a little bit
scary, but it's actually not. So this is the
122:17 - update rule from the paper j is going to be
our loss function for our actor. It's a function
122:23 - of phi, the parameters of the Policy Network.
And you want to take the gradient of the cost
122:29 - function with respect to the parameters of
the Policy Network. And it's given by one
122:35 - over m times the sun, the sum, or a mean an
average. So our loss function is going to
122:40 - be a mean. And it's going to be the product
in this particular equation of the gradient
122:46 - of the first critic network with respect to
the actions chosen by the Policy Network,
122:52 - multiplied by the gradient of the policy network
with respect to its parameters. Now, this
122:57 - looks intimidating, but it's actually not
this is the application of the chain rule
123:02 - to the loss function. So they have taken the
gradient of the loss function with respect
123:09 - to phi. But the loss function is proportional
to the output of the first critic network.
123:15 - Of course, the first critic deep neural network
has its own set of of neural network parameters,
123:20 - it doesn't have an explicit dependence on
the neural network parameters of the Policy
123:25 - Network. So it's very difficult to take a
gradient write something that doesn't depend
123:28 - on something else. So the dependence is implicit,
it comes from the fact that those actions
123:35 - are chosen according to the output of our
policy network. And so you have to apply the
123:39 - chain rule. In reality, all we're going to
do is the following. We're going to randomly
123:47 - sample states from our memory, we're going
to use our active network to determine actions
123:52 - for those states. So we're not going to be
using the actions from our memory, we're going
123:55 - to figure out what actions the agent thinks
we should take. Now, we're going to plug those
124:00 - actions into our critic and get some value,
specifically, the first critic never the second
124:04 - only the first, that's just by design. And
then we're going to take the gradient with
124:08 - respect to the accurate network parameters.
Now we don't have to calculate that gradient,
124:12 - TensorFlow is going to do it for us, we just
have to do the first three things where we
124:16 - have sample states. Use the actor determined
actions for those states and plug those into
124:21 - our critic along with the states to get some
value, and then take the gradient with respect
124:26 - to the accurate network parameters. Now, keep
in mind that this update isn't performed every
124:30 - time step is performed every other time step.
How often you perform it is a hyper parameter
124:36 - of the algorithm. But it is not every single
time step. Now, nominally the update rule
124:44 - for the critic is a little more straightforward.
So again, you're going to sample a batch of
124:49 - transitions from your memory, you're going
to put the new states that the agent received
124:53 - observed after taking some action through
the target actor network, that's at a tilde
124:59 - a parameter. And then you're going to add
in some clips noise, it's just going to be
125:03 - a normally distributed noise with mean zero,
and some standard deviation, something like
125:08 - 0.2. And we're going to clip it in the range
of minus 0.5 to positive 0.5. So that's where
125:14 - the clipping comes in for our double q learning,
and the actual double q part comes in. And
125:21 - when we calculate our targets y, so we're
gonna take the reward that we sample from
125:24 - our buffer, and add it to the product of the
gamma, which is the discount factor 0.99 or
125:31 - so. And we're going to take the minimum of
the output of the two target critic networks.
125:38 - So we'll say, we're going to feed the new
states through the new states and those clipped
125:43 - actions through both target critic networks.
And we're going to see which one is the minimum
125:47 - and take that for our target value. And then
we're going to input that target value into
125:54 - our loss function. Again, you have a one over
n multiplied by some multiplied by something
125:59 - squared, that has a mean squared error. And
it's the mean squared error between that target
126:03 - y and the output of both of our critic networks.
So our loss is going to have two different
126:11 - components, it's going to have a loss for
critical one, we have q sub theta sub one,
126:15 - and then our have a loss for Q sub theta sub
two. So our two losses, and in TensorFlow
126:21 - two, when we do our gradient tape, we're gonna
have to pass in that persistent equals true
126:26 - flag to our function call, so that it keeps
track of network parameters between gradient
126:32 - ascent steps. And so these, the rest of this
verbiage is just kind of the verbal description
126:39 - of what we want to do, it's going to be much
easier once you see it written in code, I
126:43 - assure you, it's not that difficult. Next,
we have to handle the question of target network
126:49 - updates. So at the very beginning, in our
constructor for our agent class, we're going
126:56 - to go ahead and initialize actor to critic
networks, and then to target then a target
127:01 - actor and to target critic networks. And we
first start out, we want to initialize those
127:07 - target networks with the exact values of the
online networks. And so we're gonna have a
127:13 - special case in our target network update
function that handles the very beginning of
127:18 - the program. every other time step, we're
gonna use the following expression to update
127:23 - the weights. So on the left side, you have
theta and five prime where the AI on the theta
127:28 - denotes either critic one or two, phi is the
parameter for our critic network, excuse me
127:36 - Policy Network. And the thetas are the parameters
for our critic networks. And so you're going
127:40 - to multiply town some small number of point
005, in this case, by the values of the current
127:46 - online network, and add in one minus tau times
the old values of the on the critic network.
127:53 - So it'll be a small number multiplied by the
current values of your online networks plus
127:59 - something that's almost one, multiplied by
the old values of your target networks, it's
128:04 - going to be a slowly changing update to our
target networks. Another thing to note is
128:08 - that we're only going to be performing this
update when we update the actual network.
128:12 - So it's not every time step. In this case,
it will be every other time step very, very
128:17 - important. So for this program, we're going
to need a number of data structures, we're
128:22 - going to need a class for our replay buffer
the agent's memory. Now, I like to use NumPy
128:27 - arrays is not the best way or the only way
to do it, it's just my preferred way. So follow
128:32 - along with me in the tutorial, do it that
way. And then when you play around with the
128:36 - code later, to understand it better, go ahead
and rewrite the replay buffer to something
128:40 - that makes more sense to you. That's a great
way to get started with modifying the program
128:44 - is with the replay buffer. Next, we're gonna
have classes for our actor network and our
128:51 - critic network. And those are of course written
in TensorFlow two, we have another class for
128:57 - our agent, and that is really going to tie
everything together, it's going to have a
129:01 - memory that keeps track of transitions, it's
going to have an actor to critics target networks
129:07 - for each of those a function to choose an
action based on the current state, a function
129:11 - to learn that performs the update rules, we
just went over an interface function with
129:16 - his memory that I call remember, just to store
transitions in the agent's memory, as well
129:21 - as functionality to save models and perform
target network updates, which I forgot to
129:26 - write here. Finally, we're gonna need a main
loop to train and evaluate our algorithm.
129:32 - So we're going to be using the open AI gym
and the bipedal Walker in particular, because
129:37 - this is a kind of difficult environment for
other algorithms. Now, it's a continuous action
129:44 - space with a pretty large state space. I think
it has 24 different components in the state
129:49 - space, if I'm not mistaken, some relatively
large number. So it's a bit difficult for
129:53 - agents to learn. And in fact, it's going to
take my computer around six or seven hours
129:57 - to complete the evaluation. filming this after
I do the code, so it'll take a while to run.
130:04 - So if it takes forever on your computer, don't
Don't panic. That's normal, quite normal.
130:09 - So all that out of the way, let's go ahead
and get started in the coding portion of this
130:14 - tutorial. Alright, so we begin as usual with
our imports, we will need NumPy the base TensorFlow
130:24 - package, we will need TensorFlow dot Kairos.
We will need layers from important dense that
130:36 - is for constructing our deep neural networks.
And we will need our atom optimizer for the
130:46 - gradient descent. And we will need LS for
file joining operations for model checkpointing.
130:52 - Let's start with our replay buffer class.
Now, this should be very familiar to you for
131:00 - deep use to deep q learning will need a max
size and shape and number of actions as input
131:07 - to our constructor. Now remember, we're dealing
with continuous action spaces. So there's
131:11 - number of actions is really a number of components
to our continuous action. I just named it
131:16 - that for consistency with my deep q learning
code. So we will save the appropriate number
131:24 - of variables. We use a memory counter instantiated
at zero because our memory is finite. We'll
131:33 - need our state memory, which will initialize
as zeros. The shape memory size and star input
131:40 - shape the star idiom just unpacks a, an array
in this case, whatever the shape of our input,
131:49 - dimensionality is from our environment, we
will of course need a new state memory. And
131:54 - that's the same shape that keeps track of
the new states that we are going to see. And
132:02 - actually memory. And remember, of course,
again, number of actions is number of components
132:09 - to or action. Reward memory in the shape of
memory size, and a terminal memory. We're
132:26 - going to use NumPy NumPy NumPy bool as our
data type, but don't keep track of our terminal
132:34 - flags, the reason being that the value of
the terminal state is always zero. And so
132:38 - we keep track of the done flags from our environment
to accommodate that. So let's store a transition
132:49 - takes the state observed action taken reward
received new state observed and terminal flag
132:55 - received as input. We want to know what the
first available memory position is. And that's
133:02 - the memory counter modules mem size that has
a property that it will overwrite earlier
133:07 - memories with newer memories as soon as the
memory fills up. And then go ahead and save
133:15 - our variables. All state underscore trawl
memory. And that's all of them I believe.
133:39 - And very important, we want to increment our
memory counter by one. Next we have to handle
133:46 - the function to sample our buffer. And that'll
just take a batch size as input. We want to
133:56 - know what the position of our maximum filled
memory is. And that's given by this minimum
134:02 - of memory counter and men size. Because we
don't want to sample zeros, we initialize
134:09 - our memory with zero so we just sample the
entire buffer, then we're probably going to
134:13 - end up sampling zeros until we fill up that
buffer which is totally useless. So then the
134:19 - batch is going to be a random choice zero
to maximum in shape batch size. Then go ahead
134:29 - and do you reference our variables 
134:48 - and DUNS All right. wraps up our replay memory
that's very, very simple, probably the most
135:04 - straightforward class in the entire project.
Next, we're gonna move on to our critic network.
135:13 - And that will derive from Kerris dot model.
So we get access to all of the properties
135:18 - of that particular base class. That'll help
with using the gradient tape for learning
135:23 - later on. We're going to take some inputs
for the number of dimensions for the first
135:34 - and second, second fully connected layers,
number of actions again, number of components,
135:41 - a name for the purpose of model checkpointing,
and a checkpoint directory, we will have to
135:48 - do a make directory on that Before you begin,
otherwise, you will get an error and it will
135:53 - not work. Call our super constructor and start
saving stuff. Do I need to do that? Now why
136:06 - not. So the purpose of the name is the fact
that we're going to be saving target networks
136:16 - as well as regular networks. And they're going
to be to critics. So we want to be able to
136:20 - keep all of those straight when we handle
model checkpointing. So we'll save our model
136:28 - name, checkpoint directory, and the checkpoint
file. And I like to append the algorithm name
136:47 - to the checkpoint files so that if I do everything
in one working directory, when I'm experimenting,
136:54 - all the names, tell me exactly which file
correspond to which algorithm, you don't have
137:00 - to do that. It's just my own personal convention.
So for our deep neural network, we'll start
137:06 - with a dense layer with a rail you activation.
Second dense layer with value activation and
137:19 - an output that will be single valued with
no activation. Now keep in mind, one interesting
137:31 - thing about TensorFlow two is that we don't
have to specify the number of input dimensions
137:37 - it infers it from the inputs. That's a pretty
nice feature. So now we define our feed forward,
137:43 - on this case, we call it call and allow us
to use the name of an object as a function
137:50 - call and basically need a state and action
as input will have a q1 action value. And
138:00 - we will want to concatenate our state and
action along the first axis. And we will feed
138:10 - that through FC to q1 action value. pass it
through the final layer, action, value, and
138:25 - return. Now keep in mind that the critic evaluates
the value of both the action and state. So
138:32 - that's why we have to concatenate the two
values. That is it for our critic network.
138:36 - Very, very straightforward. Next, we're going
to handle our actor network. And that, again,
138:44 - derives from Kerris dot model just the same
as a critic network. Our initializer takes
138:54 - dimensionality as input again, as well. number
of actions. And you know, what did I I'm sorry,
139:01 - I'm checking something here. No, I did not.
For a second there, I thought I passed in
139:05 - in good shape to the critic network that would
have been totally unnecessary. checkpoint
139:10 - directory equals temp, TD three, we want all
of the models to live in the same directory
139:16 - very, very helpful. And then we can go ahead
and start saving stuff. Looking at this and
139:40 - as I look at the what I'm doing here, so I
like to modify stuff on the fly. I don't think
139:49 - I actually need this actions here. So let's
go ahead and delete that. Just for the sake
139:57 - of cleanliness. We will need it on the actor
of course. Yeah, let's keep it nice and clean
140:04 - model name, checkpoint directory, plus TD
three. And just for clarity, that name will
140:22 - have stuff like Target actor, target critic,
actor or critic, so we can keep all of those
140:26 - particular networks straight. And of course,
the two critics will be critic one or critic
140:30 - two, because we have some very interesting
naming conventions. So now we'll have our
140:36 - deep neural network. Again, a simple dense
layer with raw you activations for the first
140:49 - two layers, and then you for our output. And
that will take actions as our output dimensionality,
140:58 - with a tan hyperbolic activation, the tan
hyperbolic is bound between minus one and
141:04 - plus one, if you want to take into account
boundaries of actions that are beyond plus
141:10 - or minus one, you can multiply this output
by the maximum balance for your environment.
141:16 - So some environments have a max action of
plus or minus two, which of course, plus or
141:20 - minus one is oftentimes less than, you know,
two. So you want to take that into account
141:26 - depending on the environment. So again, we
did a call function to handle the feed forward.
141:35 - So pass our state through the first fully
connected layer, second, fully connected layer,
141:43 - and pass that through the final layer and
return it. So that is it for our actor network.
141:54 - Next, we need an agent class to tie everything
together and to handle all the really interesting
141:59 - functionality. So our agent doesn't derive
from anything. But our super constructor,
142:07 - excuse me, our constructor is going to take
a whole slew of inputs. So we need a couple
142:12 - different learning rates. Reason being you
want to accommodate the capacity for different
142:17 - learning rates for your actor and critic network.
Sometimes they learn best with different learning
142:22 - rates. Input demos, you'll need that for your
memory towel for your software update rule,
142:32 - your environment for a number of important
variables from the environment. Default gamma
142:36 - is 0.99. The update act date up date actor
interval will default it to every other iteration,
142:45 - a warm up of 1000 steps. Just a default value
for an actions max size of a million transitions.
142:59 - layer one size 400 layer two size that's RFC
one and two tins respectively. A batch size
143:07 - default on 300 and a noise of 0.1. Let's go
ahead and start saving stuff. Since we will
143:21 - be adding in noise, we're gonna have to perform
a clamping on our actions to make sure that
143:25 - the actions the action plus the noise don't
fall outside of the allowable bounds of the
143:30 - environment, or below our memory, that's a
replay buffer. Then we need batch size. We
143:50 - need a learn step counter that will need that
because we're doing the delayed part of TD
143:55 - three we're going to delay the updates of
the actor network. Every two every two different
144:01 - iterations of the update of the critic network
to get the critic network time to converge.
144:07 - Then we have you know, I'm looking at my cheat
sheet here are all a time step. Why do I have
144:13 - a time step? Excuse me one moment now I believe
the time step is for the warm up procedure.
144:21 - We shall double check that later. If not,
I'll come back and delete it in the GitHub.
144:27 - And we don't want to forget number of actions.
Sorry, I write this code you know sometimes
144:31 - well in advance of doing the video because
I get distracted by other stuff. And so when
144:36 - I come back to it, I don't always know what
I was thinking that is a benefit of comments
144:41 - which I don't really do for this stuff. Sue
me I probably should. We do need our update
144:47 - after iteration. And that is update actor.
Interval excuse me, let me close my door.
144:55 - My toddler is rampaging. And next we can go
ahead and start defining our actors And critics
145:00 - and the name will just be actor. Because we
are quite creative. To maintain compliance
145:20 - with the pepp eight style guide. Let's go
ahead and delete a couple spaces. Critic one
145:25 - is a critic network or one size layer to size.
And we don't need number of actions there
145:36 - because I deleted it. So its name will be
critic one. Likewise for critic two and again,
145:51 - the purpose of this is to handle the double
q learning update rule. Next, we will need
145:56 - a target actor layer to size. Let me go ahead
and delete spaces. What silly style guides
146:17 - and finally and name of target actor then
we'll need target predict one 
146:36 - with a very original name of target critic
one. Similarly, critic net target critic two.
146:55 - And that is it for our network's Next we have
to compile them because this is a TensorFlow
147:00 - two. And that is where our learning rates
come into play. So we will use our atom optimizer
147:08 - with the learning rate defined by alpha for
our critic, our loss would just be a mean.
147:17 - And our critic one learning rate of beta a
loss of mean squared error and critic to have
147:35 - to do the same thing. Where equals beta do
I need? Yes, I do. I do need to parentheses
147:47 - there. mean squared error. And then we will
handle our target networks ness. Target networks
147:59 - nest. Next that is a tongue twister. So we
have to compile the target networks, just
148:05 - by convention with TensorFlow two, we're not
going to be performing any Stochastic gradient
148:10 - descent or Adam. In this case, on those particular
networks, we're gonna be doing the salt network
148:15 - updates, but we still have to compile them.
Nevertheless. That is just by convention.
148:23 - We read alpha loss equals mean target critic
one. Okay, so that is all of our networks.
149:11 - So that noise will keep track of as well update
network parameters with a default value equals
149:20 - one. I do that because on the first step of
the update, we have to set the values of our
149:27 - target networks equal to the starting values
of the online networks. And so we pass in
149:33 - a value tau equals one to perform an exact
update or a heart update instead of a soft
149:37 - update. We'll handle that function toward
the end. For now I want to get to the Choose
149:43 - action remember and learn functionality because
that's where all the really interesting stuff
149:47 - is. So let's go ahead and choose an action
based upon an observation of the current state
149:53 - of the environment as input. So far, a time
step less than our warmup period. I yeah,
150:02 - that's why we need the time step to handle
the warm up. As I suspected, lad, I didn't
150:07 - delete that, we're going to select an action
at random, with just a normal distribution
150:16 - with a scale defined by our noise parameter
in the shape of number of actions, comma,
150:24 - so we get a batch fare. And sorry, an array
of scalars. Otherwise, we want to go ahead
150:31 - and convert our state to a tensor. And add
on a batch dimension, that's just all the
150:39 - way that the inputs are expected to be fed
into the deep neural network, we have to add
150:43 - that batch dimensionality. And I have to float
32 here must be due to some sort of precision
150:51 - thing that makes TensorFlow happy. So then
when we want to pass our state through our
150:57 - actor network, and receive our mu, and we're
doing that because it returns batch size of
151:03 - one, one scalar. Then we'll say mu prime,
which is where we handle the noise equals
151:11 - mu plus MP random, normal scale equals self
dot noise. And your prime is TF clip. Bye
151:24 - bye, because again, that noise could take
us outside the bounds of our environment.
151:28 - So we'll plant mu prime between min action
and Max action, increase our time step by
151:37 - one very important. And we want to return
mu prime. Okay, that's it for our choose action.
151:47 - Now let's handle the simple interface function
to remember a transition. So remember, state
151:55 - action reward, new state done. And we'll say
memory dot store, transition, state action
152:05 - reward new state done nice just because we
have to interface with the memory in some
152:13 - way, we don't want to have the agent class
calling. You don't want to have the agent
152:18 - class interacting with private variables from
your memory, that would be poor software design.
152:25 - So next, we handle the most interesting function
in the whole program, which is the Learn function.
152:32 - And the very first thing we want to do is
say, hey, if we haven't filled up at least
152:37 - batch size of memory, we probably don't want
to be learning. So we'll say self dot memory,
152:42 - that meme counter loss and batch size. Now,
if it's not, if it's not greater than the
152:49 - batch size, or equal to go ahead and return
essence, we're doing a warm up, that won't
152:54 - be the case, because the batch size is just
a few 100, the warm up is 1000. So by the
152:58 - time we get through the warm up, then we're
already well into filling up batch size of
153:05 - memories. But if you decided not to do a warm
up, then that would be important. So we'll
153:09 - start by sampling our memory. So memory, sample
buffer, pass in our batch size. And then we
153:21 - want to convert all of those to TensorFlow
tensors. And I have to be very pedantic with
153:30 - data types here. I think. There could be issues
if you do not, as I recall, I think it barks
153:38 - at you about data types, because it expects
certain types of floating point variables
153:43 - in some places and other types elsewhere.
And we don't have to convert the Dunn's to
154:04 - a tensor because rod sticking that in the
deep neural network, we're just using that
154:08 - as a multiplicative factor. So we can leave
it as a NumPy array. Now we're going to handle
154:14 - our update to the critic network, because
we do that every time step. And then we'll
154:22 - handle the update to our actor network. So
we'll say with TF three and tape, and I'll
154:30 - have persistent equals true Oh, because I
have two different networks. Yeah, so you
154:35 - need two different if you're using two different
updates for one group, excuse me, we're using
154:40 - two different apply gradients for a single
tape. And you need to pass any persistent
154:46 - equals true variable, parameter excuse me,
or argument. Otherwise, you don't need that
154:52 - versus n equals true. We just have say a single
network that you're performing an update on.
154:56 - So we want the actions according to Our target
actor for the new states. And then we're going
155:07 - to go ahead and add on a noise parameter to
that, that we're going to clip between the
155:15 - range of minus point five and positive point
five. So clip by value MP random, normal 0.2
155:26 - minus 0.5 4.5. And then we're going to go
ahead and clip that again, because again,
155:33 - the addition of that noise could take the
action outside of the bounds of our environment.
155:54 - And then we are free to go ahead and start
calculating our critic values. So q1 underscore
156:00 - the critic value, according to the first target
critic is the feed forward of the new states
156:10 - and target actions through the first target
critic, cue to underscore is very similar.
156:17 - It's just the evaluation of the new states
and target actions according to the second
156:24 - critic. Now, again, we're gonna have to go
ahead and squeeze that output. And the reason
156:33 - is, is that our shape is batch size by one
want to collapse to batch size. And we have
156:48 - to do that for q2 as well, excuse me, q2 underscore.
And then we're going to need the the value
156:59 - of the states and actions the agent actually
took, according to the regular critical one,
157:05 - excuse me, one and two networks. So we'll
call those q one. And we'll just go ahead
157:11 - and squeeze those right away. Critic one state's
actions squeezed along the first dimension
157:23 - critic to states actions, one and then we're
going to say that our critic value for the
157:32 - new states is the minimum of q1 underscore,
q2, underscore and then we're going to need
157:40 - our target value the Y from our paper rewards
plus gamma times critic value times one minus
157:49 - dunnes that will set the value of the second
term here gamma times critic value there should
157:57 - be an underscore there sorry to zero everywhere
the done flag is true. And then we have our
158:04 - losses. So critic one loss karass losses got
mean squared error between the target and
158:12 - Q one mean squared error target and que tu
so that is it for the calculation of the critic
158:22 - losses. Now we have to handle the calculation
of the gradients. Now, we don't have to do
158:26 - anything special for that. The TensorFlow
package handles that for us. So tape dot gradient,
158:33 - the gradient critical one loss with respect
to the critical one trainable variables. So
158:42 - I should be dot not an underscore I make that
mistake frequently. Critic one critic underscore
158:49 - one dot trainable variables yeah that is right.
And then we need the critic to great critic
158:59 - to loss critic to trainable variables Same
deal then we need to go ahead and apply those
159:11 - gradients. So calling our optimizer dot apply
grip gradients function and that expects a
159:22 - zip as input. We're going to zip The Critic
one gradient and the self critic one trainable
159:32 - variables simply recruited to optimize or
to gradient Okay. And then we want to increment
159:58 - our Learn step counter Because that gets incremented
every time we update our critic networks,
160:05 - and then we have to address the question of,
is it time to update our actor network. So
160:11 - we'll say if that learns step, counter modulus,
self update actor, interval, it or sorry,
160:21 - is not equal to zero, then return so it's
not every n steps, then go ahead and return.
160:31 - And if we haven't returned, then we're going
to go ahead and calculate the loss for our
160:35 - actor network. So with TF gradient tape, as
tape, and here, since we're just dealing with
160:42 - one loss, we don't have to call the processing
equals true. We don't have to pass in the
160:47 - verses and equals true argument. So what's
our new actions are the actions chosen by
160:54 - the current parameters or actor network for
the current set of states, the states the
161:00 - agent saw along the way, the critic one value
is self critic, one of those states and new
161:08 - actions. And then our actor loss. There's
negative TF math reduce mean, critical one
161:19 - value. And this may look a little strange
to you. But this is how we're going to handle
161:26 - the gradient of the output of one network
respect to the parameters of a network, it's
161:31 - kind of like how you apply the chain rule
to the to the loss of the output of the critic
161:39 - network with respect to the parameters of
your target actor, your actor network, sorry.
161:44 - Oh, that makes sense. So then we do the same
thing where we calculate our gradient gradient,
161:51 - tape dot gradient. Sorry, taped out gradient
that I call, let me make sure I didn't make
162:00 - a mistake, I did make a mistake up here. Sorry.
So this should be taped out gradient. Not
162:08 - TF. That is one less error to worry about
when we get to running the program. Sorry
162:15 - about that. So take that gradient, factor
loss factor trainable variables, step our
162:26 - optimizer by applying our gradients, again
a Texas zip as input actor, gradient cell
162:36 - dot actor trainable variables. Okay, so then
finally, at the end of the learning function,
162:49 - we want to update our network parameters.
Okay, so that really handles all of the learning
163:00 - algorithm for our agent. All that's left now
is to update our network parameters, and then
163:06 - handle the model saving so just a few functions
left and then we can go right our main loop
163:10 - and see how it does. So network parameters.
So we're going to pass in a default value
163:22 - of towel, Fernand, remember that at the top
of our rather the end of our initializer we
163:28 - pass in tau equals one to handle heart update.
every other time we're going to pass in a
163:35 - nun. So we'll say if tau is none, then tau
i go self dot pal. So every time other than
163:42 - the first time we call this we're going to
use the stored value for towel. So weights
163:50 - equals the list targets equal self dot target
actor dot waits, I wait in enumerate self
164:00 - actor waits waits dot append, wait times tau
plus targets sub i times one minus tau. There
164:18 - we go. And then sell dot target actor dot
set weights and I am going to yank this and
164:31 - paste and paste again and then say target
critic. One self dot critic one weights. Set
164:51 - wait Sorry, I forgot to set the actual via
the weights how sloppy of mean, and then say
164:57 - CELTA Target critic, one set weights weights.
And then we have target critic to numerate
165:16 - critic to weights, weight start append. And
then so got target critic to set weights.
165:32 - And that handles our update rule for our two
networks. Sorry, my Num Lock key is off there.
165:44 - Okay, so if it isn't clear what's going on
here we are iterating over the weights of
165:48 - our actor and critic one critic two networks,
then we're doing the calculation for the soft
165:53 - update rule saving that in a temporary list
and uploading that list to the target actor
165:58 - or target critic one or critic two networks.
Now, we can handle the Save model functionality.
166:06 - This is the easiest part of the whole project.
So print saving models just a little debug
166:13 - statement to let us know something is going
on. Save weights self dot accurate a checkpoint
166:22 - file that critic one checkpoint file, checkpoint
file, and then we have our target numbers
166:42 - as well. Good grief. Then we do the inverse
operation of loading our models. So let actor
167:26 - load weights from the accurate Check Point
file. We have critic one load weights. So
167:41 - blog critic one dot checkpoint, firewall.
And then we have our target networks. I should
168:03 - have chosen shorter variable names to save
my risks a little bit of work here. But hindsight
168:10 - is always 2020 I guess. Target one, check
one file. Okay, so that is it for the main
168:35 - code for our TD three algorithm. Now we get
to handle the main loop. So let's go ahead
168:40 - and code that up. Of course, before we can
have an invalid syntax right here at the very
168:48 - beginning. I'm TensorFlow dot Kairos dot layers.
Oh, sorry. That's because it's a from import
168:59 - dense. I'll have to notate that in the video.
And I have another issue here. def store transition,
169:14 - where is it unhappy? I am missing a comma.
Of course. And I have another issue Oh s path
169:28 - joined. name equals my wrists are nonfunctional
today. Cell dot target actor dot set weights.
169:40 - Oh, ah. Why did that happen? Interesting.
Did I type those at the end and have a stroke
169:57 - or something to remember very strange Okay,
unexpected and a file, let's delete their
170:07 - same. am I forgetting a parentheses somewhere?
I am. Because I have right there. All right,
170:27 - finally, good green. That's a whole lot of
typos. Now some people suggest that I upgrade
170:32 - my vim to actually catch that stuff on the
fly. And you're absolutely right, I'm gonna
170:36 - do that. When I finally forced myself to do
it, let's go ahead in the meantime and write
170:41 - our main loop. So we want to import Jim NumPy
we'll need our agent. And we'll need our utility
170:56 - file plot learning curve, you can do a git
clone on my GitHub to get that it's just a
171:04 - map plot live pie plot with some labeled axes,
where we're taking an average of the previous
171:11 - 100 games running. I don't include that in
all my videos, I just kind of reference my
171:17 - Get up. You can just do a plot, if you wish,
name equals main gym dot make. We're going
171:28 - to be doing the bipedal Walker, v2. And we're
gonna call our agent constructor, alpha 0.01,
171:39 - a beta of 0.001. Our input dimensions will
be determined by our environment. So we don't
171:49 - have to hard code anything. Tau of 0.005 pass
in our environment, batch size, I have 100
171:58 - here equals 400 300. And n actions determined
again, by our environment, all this bad would
172:13 - play 1000 games, and the call our file name
plots, plus locker underscore. Now keep in
172:23 - mind, you have to do a make der plots and
make your temp slash gt three to correctly
172:29 - execute the code. Because it'll expect that
those directories exist. Pass a number of
172:36 - games as a variable for your file name. That
way, if you run it over and over again, with
172:43 - different numbers of games, it won't overwrite
the same plot. You can also include things
172:47 - like learning rates as part of your variable
name for your file names, I recommend doing
172:52 - that. I'm just not doing it here. So we need
to keep track of our best score subminimum
172:59 - the score of our environment. And the reason
is, we want to save our best models. Keep
173:06 - track of your score history. If you want to
load models, now's the appropriate time to
173:12 - do that. Actually, you know what we may need,
there may be an issue where we have to actually
173:20 - instantiate our network with some variables
to load the models. Open up an issue on my
173:28 - GitHub. If that's the case, and I will write
the correct code. I won't bother with the
173:33 - video, I'll leave that as an exercise for
the viewer. But if it turns out to be a problem,
173:37 - raise an issue and I can fix that. Not a huge
deal. Let's go ahead and play our games. Start
173:45 - by resetting the environment at the top of
every episode, we set the done flag and zero.
173:51 - And let's play our episode of Walmart done.
action equals agent dot choose action based
174:01 - on the observation. Let's take that action,
get the new state reward Don and debuginfo
174:08 - from our environment, call our Learn function.
Keep track of our score and set the current
174:21 - state to the new state very important. If
you don't do that, nothing is going to go
174:25 - well for you. As a punter a score at the bottom
of every episode and calculate our average
174:37 - minus 100 onward. If our average score is
greater than our best score, then set the
174:44 - best score to that average. And save our models
and then we want to print some debug information
174:56 - episode on Score, one of average score 
average score. At the end of all the games,
175:16 - let's go ahead and handle our plotting our
x axis is just the number of games and called
175:26 - plot learning curve. Alright, so moment of
truth. Let's go ahead and see where I have
175:38 - my invalid syntax. I forgot an addition sign
there. Very simple. Alright, let's try it.
175:45 - Oh, you know what I'm running it over in this
other terminal. And okay, so it saves models
175:50 - it is learning. That is good to know. Let's
do a make der temp slash TD. Three. I should
176:00 - already have plots. Python main Td three.pi.
Moment of truth. Okay, so it says actor network
176:10 - object has no attribute checkpoint file. I
didn't Oh, it's checkpoint directory. So that
176:17 - is in TD three. Do you have to that is in
line 70. So that is here. Okay. Try it again.
176:39 - Got an unexpected argument name. That is in.
Oh, that's because Okay, that is in line 48.
177:00 - That is super trivial. Thought name equals
hits name. Plus that I do that down here as
177:11 - well. No, I did not. Not looking forward to
editing this, this is going to be a lot of
177:20 - work. Okay, so it saves models right off the
bat and starts running. Okay, so I'm gonna
177:25 - let this run for a while. And then we're gonna
see how it does. Okay, so I did something
177:33 - very stupid. And I let it run and noticed
it wasn't actually learning. And so that's
177:41 - a problem. And the reason it's not learning
is because I forgot to store the transition.
177:47 - So we have to say agent, remember, observation,
action, reward, observation, underscore and
177:56 - done. Okay, and then I got to get rid of the
print statements I stuck in here, for debug
178:04 - purposes, because I'm a noob. And use debug
statements. Okay. Now let's do Python, main
178:15 - Td three.pi. And now it should work without
any funky print statements. Okay. Now I'm
178:24 - going to take off for a little bit and see
what's going on. And the reason I noticed
178:29 - this wasn't learning is because it was executing
much too quickly. I blasted through 350 games
178:34 - in just about a minute, which tells me it's
not doing anything useful on the GPU. So let
178:41 - this run out, or probably take an hour or
two, and then I'm gonna come back and see
178:45 - how it did. And we'll take a look at his performance.
Now, here we are, it's the next morning. This
178:51 - took around six or seven hours to run. So
I just waited until morning to film this.
178:57 - But in typical Phil's style, the filename
for the function call for the plot learning
179:04 - curve function has a typo in it. And so we
don't have an actual plot from the performance
179:10 - of this particular run. However, I will show
you a plot of a similar run, where it achieved
179:15 - an approximately similar score, you can see
that it does indeed learn it issues a high
179:21 - score of around 285 to 88, about 290 or so
depending on the run, you get some run a run
179:28 - variation. And 300 is the highest possible
score you can get for this environment. So
179:36 - I would consider this pretty much strong evidence
of learning. It's not a world class results.
179:41 - But that's not what we were aiming for any
way we just wanted to understand the gist
179:44 - of the algorithm and implement it correctly,
and demonstrate that we do in fact understand
179:49 - how it works. Mission accomplished. You can
pat yourself on the back for that. I'll also
179:55 - show you some footage of the walker kind of
stumbling along so you can see how it looks
180:01 - once it's fully trained. You can see it has
kind of a funny gait, but it does in fact
180:06 - managed to learn to walk that is pretty impressive
starting from just totally random actions,
180:11 - learning how to walk within just six or seven
hours only humans. Were so competent. I hope
180:17 - that was helpful. Leave a comment down below
with any questions, suggestions, anything
180:21 - you'd like to see next, give a thumbs up,
subscribe if you haven't already, and I'll
180:25 - see you in the next video. Welcome to a crash
course in proximal policy optimization. Before
180:34 - we begin a quick shameless plug my Udemy courses
on deep reinforcement learning specifically
180:39 - actor critic methods and deep q learning are
on sale right now, learn how to turn papers
180:44 - into code, link in the description below.
So proximal policy optimization or PPO for
180:50 - short, was created for pretty simple reason.
And that is that an actor critic methods,
180:55 - oftentimes we see that the performance can
fall off a cliff, the agent will be doing
180:59 - really well for a little while. And suddenly,
an update to the neural network will cause
181:03 - the agent to simply lose. Its its understanding
of how to play the game. And so performance
181:10 - tanks and never really recovers. Now this
happens because actor critic methods are incredibly
181:15 - sensitive to perturbations. The reason being
that small changes in the underlying parameters
181:21 - to our deep neural network, the weights, for
instance, can cause large jumps in policy
181:26 - space. And so you can go from a region of
policy space where performance is good to
181:30 - a region of policy space where performance
is bad, just by a small tweak to the underlying
181:35 - parameters of your deep neural network. PPO
addresses this by limiting the updates to
181:41 - the Policy Network. It has a number of mechanisms
for doing this. But the basic idea is we're
181:46 - going to base the update at each step on the
ratio of the new policy to the old. And we're
181:52 - going to constrain that ratio to be within
a specific range to make sure we're not taking
181:56 - really huge steps and parameter space for
our deep neural network. Of course, we also
182:03 - have to account for the goodness of state.
In other words, the advantage how valuable
182:07 - each state is, and the reason being naturally
that we want the agent to select states that
182:12 - are highly profitable to it over time, so
wants to find the best possible states. Now
182:18 - taking into account the advantage can cause
the the loss function to grow a little bit
182:25 - too large. And so we're going to be introducing
a way of dealing with that by clipping the
182:29 - loss function and taking the lower bound with
the minimum function. Something else we're
182:35 - going to be doing that's different than what
you may be used to is that instead of keeping
182:38 - track of something like say, a million transitions,
and then sampling a subset of those at random,
182:45 - we're going to be keeping a very small fixed
length trajectory of memories. And we're going
182:49 - to be doing multiple network updates per data
sample using mini batch Stochastic gradient
182:56 - descent. It's worth noting that you can also
use multiple parallel actors on the CPU something
183:02 - like what you would do in a three C, but we're
not going to deal with that, in this particular
183:07 - tutorial, I'm just going to show you how to
do the GPU implementation. So let's talk about
183:13 - the mini batch gradient sent for a second.
So we're going to keep track of a list of
183:19 - memory indices from say zero to 19. And that's
for the case of taking a look at 20 transitions.
183:25 - And let's say we want to take a batch of size
five. And so those batches could start at
183:31 - position 05 10, or 15. Those are the only
possible positions were distorted such that
183:37 - you get all the memories, you don't get any
overlap, and that it all works out evenly.
183:43 - So what we're going to do is we're going to
shuffle our memories, and then take batch
183:47 - size chunks, so we'll start at position zero,
from zero all the way up to four, that is
183:51 - one batch. And then position four, five, up
to nine is the next batch, and so on and so
183:56 - forth. It's relatively straightforward when
you see it in code, but it's kind of difficult
184:00 - to explain as you're coding it. So just know
that we're taking batch size chunks of shuffled
184:04 - memories for mini batch Stochastic gradient
descent. Other things we need to know is that
184:11 - we're going to be using two distinct networks
for actor and our critic instead of having
184:14 - a single network with shared inputs and multiple
outputs. Now, you certainly can use a shared
184:21 - input with multiple outputs, but it complicates
the loss function a little bit. And I found
184:25 - that performance is generally adequate with
two distinct networks for simple environments.
184:31 - So the critic will evaluate the states that
the agent encounters and it gets the name
184:36 - critic because it literally criticizes the
decisions that the actor makes, based on which
184:40 - states it ends up in. So it says, Hey, this
particular state was valuable. We did good,
184:45 - or this state is stupid. We did bad do better
next time. Now this is in contrast to state
184:51 - and action pairs for something like say deep
q learning, but it's in line with what other
184:56 - actor critic methods use And of course, the
actor decides what to do based on its current
185:05 - state. So our network is going to output probabilities
using a softmax activation. And we'll use
185:11 - that for a categorical distribution and pytorch.
So we'll have, in the case of the card poll,
185:17 - we'll have a couple actions, and some probabilities
selecting each action. And then we will use
185:21 - the probabilities determined by our deep neural
network to feed into a distribution that we
185:26 - can sample and use for the calculation of
the log probabilities more on that momentarily.
185:32 - It's also worth noting that exploration is
going to be taken care of for us, due to the
185:37 - fact that we're using a distribution. So it's
probabilistic and is set up so that each element
185:43 - has some finite probability. So even if the
probability of one action and goes arbitrarily
185:48 - close to one, the probability selecting the
other action stays finite so that at least
185:55 - some of the time it's going to get some exploration.
This is in contrast to something like say
185:59 - epsilon greedy action selection and deep q
learning, where you select off off optimal
186:05 - actions about 10% of the time. As I said earlier,
our memory is going to be fixed to a length
186:11 - of capital T. In this case, we'll use 20 different
steps, we're going to keep track of the state
186:17 - c agencies, the actions, it takes rewards,
it receives the terminal flags, the values
186:22 - of those states, according to the critic network,
and the log of the probability of selecting
186:27 - those actions that'll become important later
in our update rule. As I said, we're going
186:32 - to shuffle those memories and sample a batch
size of five. And we're going to perform a
186:36 - four epochs of updates on each batch. Now,
these parameters are chosen specifically for
186:41 - this particular environment. And that's one
of my criticisms of PPO is that there are
186:46 - a number of parameters to play with hyper
parameters. The memory length is one hyper
186:50 - parameter, the batch size, and number of epochs,
as well as learning rate. And another parameter
186:56 - we're going to see later, all play roles have
hyper parameters in our model. And so there
187:01 - is a lot to tune here. But these parameters
work really well for the carpool environment,
187:08 - so you won't have to do any tweaking for that.
Other thing to note is that this memory length,
187:15 - capital T, should be much less than the length
of the episode. So in the case of the carpool,
187:20 - the maximum episode length is 200 steps. And
so 20 steps is significantly less than that.
187:25 - So I think it qualifies, you wouldn't want
to use something that encompass more than
187:29 - one episode, for instance, that would probably
break the algorithm and result in poor performance
187:36 - relative to using a capital T much less than
the episode length. So all this is relatively
187:41 - simple. But what isn't so simple is the update
rule for our actor. So here's where all the
187:46 - math comes in. So we have this quantity loss,
the CPI, the stands for conservative policy
187:53 - iteration. And it's given by the expectation
value, which is just an average of the product
187:58 - of the ratio of the policy under the current
parameters to the policy under the old parameters
188:04 - multiplied by this a hat sub t, one that in
a second. When they do that, and they just
188:09 - abbreviate that ratio is r sub t. Now, if
you're not familiar with deep reinforcement
188:15 - learning or reinforcement learning, in general,
the policy is a probability distribution.
188:20 - That is what our actor is attempting to model
is a probability distribution, the policy.
188:25 - And this policy is a mapping between states
and actions and probabilities. So given your
188:31 - in state SMT, and you took action a sub t,
what was the probability of selecting that
188:36 - action, according to the distribution, and
so in the denominator, we have theta old,
188:41 - that is the probability of selecting action
a sub t given state SMT, under the old parameters
188:48 - of your deep neural network, so we're going
to play 20 steps, and then the agent is going
188:52 - to perform a learning update. And it's going
to do mini batch Stochastic gradient descent.
188:59 - And so after computing that first batch, the
parameters of the deep neural network change,
189:03 - right, that's all the batches work, you compute
the loss with each batch and update your parameters.
189:10 - And so right after you've calculated that
first batch of memories, the loss for that
189:14 - and updated your deep neural network, the
theta changes, and so the policy pi is going
189:19 - to change as well. So we have to keep track
of the parameters, excuse me of the the probabilities,
189:26 - of selecting each action at each time step
in our memory. And then on a learning function,
189:33 - we're going to pass those states through our
actor network, get the probabilities, the
189:39 - probability distribution and find out what
the probability of selecting action a sub
189:42 - T is sampled from our memory according to
the current values of the deep neural network.
189:48 - It'll be a little bit more clear in code.
Just know that we have to keep track of log
189:52 - prompts. As we go along and we're going to
be recalculating them in the learning loop.
189:57 - One thing we also see is that it takes into
account the Vantage, which is at a hat sub
190:02 - t. So the advantage is just a measure of the
goodness of each state, we'll get to the calculation
190:06 - of that in a few minutes. But one thing to
note is that this ratio, pi sub theta, or
190:13 - pi sub theta old, can have an arbitrary value,
right? Because you could have, let's say,
190:19 - pi, theta being point nine, nine, pi theta
old point 01. And so that's a pretty large
190:25 - number. And in particular, if you multiply
it by an advantage, that is like, say, 1020,
190:30 - of whatever, then that can also still be a
large number. And so we have to deal with
190:35 - that, right? Because the whole point of this
is that we want to constrain the updates to
190:39 - our deep neural network to be some relatively
small amount. And so the way you deal with
190:45 - that is by adding an additional hyper parameter
epsilon that you use to clip that ratio. So
190:52 - what we're going to do is we're going to clip
that ratio within the range one minus epsilon
190:56 - two plus one plus epsilon. So let's say from
0.8, to 1.2. So that ratio is going to be
191:02 - constrained to be close to one. And you're
going to multiply that by the advantage. And
191:06 - so that'll give you some number. And then
you want to take the minimum of that clips
191:12 - number, the clipped ratio multiplied by the
advantage, and the unclipped ratio multiplied
191:17 - by the advantage, take the minimum, and that
is what we will use for the loss for our actor
191:22 - network. So this serves as a pessimistic lower
bound to the loss. And they don't go into
191:28 - any real depth in the paper on the reasoning
for this. But to my mind, and this could be
191:33 - wrong, you know, I am an idiot sometimes,
but my understanding is smaller loss, smaller
191:37 - range and smaller update. That's the whole
point of it. So let's talk about this advantage
191:41 - now. So this advantage has to be calculated
at each time step and is given by this equation,
191:50 - don't freak out, this is relatively straightforward.
Once again, it tells us the benefit of the
191:55 - new state over the old. Well, how do we know
that we know that because it's proportional
191:59 - to or equal to the sum of the Delta sub t
with a Delta sub T is just the reward at a
192:05 - time step, plus a difference in the estimated
value of the new state and the current state.
192:11 - So it tells you, what is the difference in
the value between the next the next state
192:15 - we encounter and the current state. And of
course, you have the gamma in front of the
192:19 - V, which is the output of the critic network,
because we always discount the values of the
192:26 - next states, because we don't know the full
dynamics of the environment. And so that that
192:30 - reward is uncertain, there's always some uncertainty
around state transitions. And then in the
192:36 - top equation, you just sum that, where you're
going to be summing over gamma multiplied
192:41 - by lambda. So this quantity gamma is again,
the normal gamma 0.99 that we typically use.
192:48 - But this parameter lambda is a type of smoothing
parameter, it helps to reduce variance, and
192:54 - we're going to use a value of 0.95. And for
implementation, we're just going to use a
193:01 - couple of nested for loops. So you're going
to start out at time t equals zero, and then
193:05 - some from that step all the way up to capital
T minus one. So if we have 20 states, you're
193:11 - going to go from zero to capital T minus one,
zero to 18. And you have to do that because
193:18 - you have the V of S sub t plus one, you don't
want to try to evaluate something beyond the
193:23 - number of actual states that you have, that
won't work out, right. And so it's going to
193:28 - be relatively straightforward once you see
it in action. And we're going to be keeping
193:32 - track of that gamma times lambda, which is
a multiplicative constant that increases its
193:37 - power by one with each iteration of the inner
loop. All that'll be made clear in the code.
193:45 - But fortunately, the critic loss is a little
bit more straightforward. So we need something
193:50 - called the return. So the return is just equal
to the sum of the advantage and the critic
193:57 - value based on the memory. So whatever the
agent estimated the value of a particular
194:02 - state to be at the time that it took it is
what we're going to be using for the critic
194:07 - value in our return. And then the loss of
the critic is just going to be the mean squared
194:12 - error between the return and the critic value
based on the current values of the deep neural
194:19 - network. So once again, we're going to be
passing the state to the critic network to
194:21 - get its estimate of values. And we're going
to be also using the values from the memory
194:27 - as well. So relatively straightforward, even
easier when you see it in code. So we have
194:32 - two different losses, and we have to sum them,
and so that'll be the sum of the clipped actor
194:37 - and critic. So a couple things to note here
is that one, we're actually doing gradient
194:44 - descent. And so the coefficient of C one for
the loss of our critic is going to be positive
194:53 - and the loss of our actor is going to be negative
because We are doing gradient ascent and non
195:02 - gradient descent we have to multiply by negative
one other thing to note is that we have this
195:07 - other parameter here, C to this coefficient
multiplied by S, S is an entropy term. And
195:14 - that only comes into play when you have a
deep neural network with shared lower layers
195:19 - and actor and critic outputs at the top. So
we don't have to worry about that, in our
195:24 - particular implementation in this tutorial,
because we're doing two separate networks
195:28 - for the actor and the critic. And I'm going
to use a coefficient of 0.5 for the loss for
195:37 - the critic. As I said, we're not going to
be implementing the entropy term, because
195:43 - we're doing two distinct networks. We can
also use this for continuous actions there,
195:49 - you would use a different output for your
actor network. And indeed, that's what the
195:54 - paper really is geared for, is for continuous
action spaces. But we're going to be doing
196:00 - the very simple discrete case. Other thing
we don't implement is the multi core CPU implementation.
196:05 - Because that introduces even more complexity,
we're just going to be using the GPU. So what
196:12 - do we need for this project, we're going to
need a class for the replay buffer. And we're
196:15 - just going to use lists for this. Normally,
I like to use NumPy arrays, but in this case,
196:21 - lists turn out to be a simpler implementation.
So that's what we're going to go with. We're
196:25 - also going to need a class for our actor network
and a class for the critic network. We'll
196:30 - need a class for agent that's going to tie
everything together that'll have actor and
196:34 - critics that invoke actor and critic constructors
as well as a memory for storing the appropriate
196:41 - data. It also functions for choosing actions,
storing memories, saving models, and learning
196:47 - from its experiences. And then a separate
file, we're gonna have a main loop to train
196:51 - and evaluate the performance of our agent.
Before we get into the coding section, I want
196:56 - to do a quick shout out to William Woodall,
he hangs out in our discord channel, which
197:00 - is also linked in the description below if
you want to come hang out with some really,
197:05 - really smart people who talk about artificial
intelligence ranging from all sorts of different
197:10 - things every single day, check out link in
the description for the discord. So William
197:14 - came to me and said, Hey, Phil, I found an
implementation of PPO that I find to be in
197:18 - line with your general philosophy of software
minimalism. And he showed it to me, and I
197:24 - looked at it, and it helped clarify quite
a few questions I had after reading the paper.
197:28 - Now, the software you see here is pretty much
my own code. But it was inspired by Wayne
197:34 - Woodhouse code. So shout out to him for helping
me out on this because the paper really isn't
197:39 - all that clear to me, even after reading it
a few times. Other thing I want to say, and
197:46 - I'll talk a little bit more about this in
the coding section is that when I normally
197:50 - define deep neural networks, actors and critics,
in particular, I will use the convention of
197:56 - saying self dot layer name equals n n dot
linear self dot layer name Next, you know,
198:02 - equals and n dot linear. And then I'll write
the feed forward function where you use the
198:08 - member variables that self dot layer one as
something you can call as an object to call,
198:13 - and then calling activation functions within
that. Now, what I found is that doesn't really
198:18 - work very well. In fact, I have to use an
n dot sequential to create the models for
198:22 - this. And that's one of the biggest takeaways
I had from we would always code is that by
198:27 - using the nn da sequence where you really
get this thing to work. And for whatever reason,
198:31 - I cannot get as good a performance using my
conventional, typical way of writing these
198:36 - networks. Now that I can't think of any reason
why that should be the case. But it is something
198:41 - I've observed, I tested it, just altering
that one chunk of code, how I defined the
198:46 - models, and running it several times to take
into account run to run variation. And it
198:50 - seems to be repeatable for me. So maybe, I
don't know, maybe it's a configuration issue
198:54 - on my system. Maybe it's something I'm doing
wrong elsewhere. I don't know I don't think
198:58 - so. All of that out of the way. Let's go ahead
and get into the coding portion. All right,
199:05 - so let's go ahead and jump right into it with
our imports. They're going to be pretty light
199:10 - will need us to handle file joining operations
NumPy for NumPy type stuff, and all of the
199:18 - torch packages will need an N for sequential
model. We will need up Tim and we will also
199:29 - need our categorical distribution. So we'll
start with our PPO memory class. And this
199:44 - will be pretty simple. For the most part.
The only input for our constructor is a batch
199:50 - size. And we will just implement the memory
with lists. So we'll keep track of the states
199:58 - encountered the log process. I'll just call
it prompts for brevity, the values that our
200:03 - critic calculates the actions we actually
took the rewards received, and the terminal
200:14 - flags. So next, we need our function to generate
our batches. So our strategy is going to be
200:27 - the following, we're going to have a list
of integers a correspond to the indices of
200:31 - our memories. And then we're going to have
batch size chunks of those memories. So indices
200:37 - from zero to say, four, and then five to attend,
so on and so forth, or whatever our batch
200:43 - size is, we're going to shuffle up those indices,
and take those batch size chunks of those
200:47 - shuffled indices. So the first thing we need
to know are the number of states we are going
200:55 - to want to get our batch start list or array,
I suppose that'll go from zero to n states
201:02 - and batch size, steps, batch size. It would
help if I could type our indices. And that
201:16 - is just the number of states in our trajectory.
We run on a shuffle that so that we handle
201:25 - the stochastic part of the mini batch and
Stochastic gradient descent. And then we can
201:30 - go ahead and take our batches using a list
comprehension. So it's going to be those indices,
201:35 - from eye to eye plus self dot batch size for
i n, brain for AI, N, batch start. So it's
201:45 - going to take all of the possible starting
points of the batches, either 05 10, etc.
201:50 - and go for in the indices from that all the
way up to AI plus batch size. So we're going
201:55 - to get the whole batch from our indices, then
we're going to want to return an array for
202:01 - each of those. And this gets a little bit
messy. Let's be very careful not to mess up
202:13 - the order. Because of course, the order in
which you returned the memories definitely
202:17 - matters later on. We'll need rewards. And
then we're also going to want to return the
202:35 - batches. And the reason why will become apparent
later. It's because we're returning the entire
202:39 - array here. And we're going to want to iterate
over the batches. So now we need a function
202:46 - to store a memory. And that'll take a state
action, probability, value, reward, and done
202:54 - as input. And all we're going to do is append
each of those elements to the respective list.
203:05 - Not as reward singular. And then finally,
we need a function to clear the memory at
203:24 - the end of every trajectory. And I forgot
the self argument here. And mini rant here,
203:44 - I really don't like some aspects of Python.
It took me much longer than I would care to
203:51 - admit to get this to run, not because the
algorithm I implemented was incorrect. But
203:57 - because I had a mismatch. So here I had, I
believe action. And up here it was actions
204:02 - or promise vice versa, and my original implementation,
so it didn't flag as an error. Because it's
204:08 - not really an error, particularly where Python
is concerned. And so it was quite a nuisance.
204:14 - Pretty, pretty painful to track that down.
Of course, if we're more strongly typed language,
204:20 - then that wouldn't be an issue. But I digress.
So now let's handle our actor network. And
204:27 - that will derive from the base nn dot module
class. Our initializer is going to be pretty
204:34 - straightforward. We're after we will need
the number of actions. The input dims a learning
204:39 - rate alpha number of fully connected dims
for the first and second fully connected layers
204:48 - and a checkpoint directory. And we're also
going to need to call our super constructor
205:02 - And then create our checkpoint file, checkpoint
directory and actor, torch PPO. Now, I do
205:15 - it this way, because I'll often do development
in a single root directory. And I don't want
205:19 - to get models mixed up. If you have a different
way a more organized way of writing software,
205:24 - then you could perhaps skip this path joint
operation and just use a file by itself. But
205:32 - let's move on to the actual deep neural network.
We're going to want a linear layer that takes
205:42 - starred but dim, so we're going to unpack
the input dim, so we have to pass in a list.
205:47 - And it's going to output FC one dims. Array
you activation function, another linear layer
205:55 - that takes FC one dims as input outputs FC
two dims. That gets a raw you activation as
206:05 - well. Another linear layer that takes FC two
dims as inputs and outputs a number of actions.
206:11 - And then we're gonna use a softmax activation
along the minus one dimension. So that's the
206:21 - whole of our actor network. The softmax takes
care of the fact that we're dealing with probabilities,
206:26 - and they have to sum to one. So our optimizer
is going to be an atom optimizer, what are
206:34 - we going to optimize the parameters with learning
rate of alpha, of course, we need to handle
206:41 - the device, which would be our GPU if possible,
then we want to send the entire network to
206:57 - the device. Next, we have our feed forward
function. And that'll take a single state
207:04 - or batch of states as input. So we want to
pass that state through our knee deep neural
207:10 - network and get the distribution out. And
then use that to define a categorical distribution,
207:18 - which we are going to return. So what this
is doing is it is calculating a series of
207:22 - probabilities that we're going to use to draw
from a distribution to get our actual action.
207:27 - And then we can use that to get the log probabilities
for the calculation of the ratio of the two
207:32 - probabilities in our update for our learning
function. Then we have a couple of bookkeeping
207:38 - functions save checkpoint. We're going to
want to say, torch dot save the state dictionary
207:47 - for our network. And we're going to say that
into a checkpoint file, then we need to load
207:54 - checkpoint. And that is self dot load state
dictionary. What are we going to load a checkpoint
208:03 - file. And that's really it for the actor network,
it's pretty straightforward. The critic network
208:11 - is also straightforward. And that also derives
from nn module. Here, we don't need the number
208:22 - of actions because the output of the critic
is single valued, it just outputs a value
208:27 - of a particular state. So it doesn't care
how many actions are in the actual space.
208:32 - But it does need a learning rate alpha. It
does need some dimensions 256 and a checkpoint
208:45 - directory. And then we need to call the super
constructor. And same deal the checkpoint
209:01 - file the check point directory and critic
torch. So that way, we can differentiate between
209:13 - the actor and critic model files. And we will
again use a sequential model. And so in the
209:23 - shout out I was talking about William moodles
implementation, as well as something else
209:27 - I observed. So what I meant by the alternate
method of doing a model was if you say self.fc,
209:34 - one and in linear you know, if you do it that
way, you have FC one FC two the separate layers
209:43 - defined without the sequential model. It actually
does significantly worse than if you do it
209:49 - with the sequential model and I don't know
why I don't have a certainly there's no theoretical
209:55 - reason they should do it. It must be something
under the hood with the way in which pi torches
209:59 - and Limiting things. And it's no disrespect
to the creators of pytorch. But this is one
210:06 - of my, you know, one of my biggest gripes
with using these third party libraries is
210:11 - you never know how they're implemented. So
something doesn't operate the way you expect,
210:15 - you can certainly go look it up, it's open
source. But that is much easier said than
210:19 - done, right? You have to be familiar with
not the entire code base, but a really significant
210:23 - portion of it to be able to make sense of
a single file or a single way of doing things.
210:28 - So it really makes things opaque. It's an
abstraction on top of an abstraction. And
210:32 - so I don't know, it's part of the good part
of the it's the bad that comes with a good
210:37 - for having, you know, a robust library like
pytorch. But I do it this way, because it
210:45 - seems to work the best. And as an aside, I
also can't get it to work very well in TensorFlow
210:53 - two. And I suspect the The reasons are related
because the performance of the TensorFlow
210:58 - two is on par with the type of performance
I get from doing it the other way, where you
211:04 - just define individual layers instead of a
sequential model. So pretty interesting stuff,
211:09 - maybe one day, I'll get super motivated, and
decide to go ahead and figure it out. But
211:14 - I wouldn't hold my breath on that. So this
is going to be very similar model, linear
211:21 - layers with relu activations in between, the
main difference is that our output layer is
211:27 - going to be a linear layer with no activation,
and a single value output. Now, of course,
211:35 - it handles the batch size automatically. So
if you pass in a batch, you're gonna get the
211:38 - batch of outputs as well. Again, we need our
optimizer with learning rate of alpha. As
212:02 - an aside about the optimizer, I'm going to
use the same learning rate for both the actor
212:07 - and the critic. And it's entirely feasible
and possible, and perhaps even advisable to
212:14 - use separate learning rates for both the actor
and the critic. At least in something like
212:18 - deep deterministic policy gradients, you get
away with a much larger, you know, by a factor
212:22 - of three or so learning rate for your critic
than you do the actor. Reason being. As we
212:28 - outlined in the lecture, the actor is much
more sensitive to changes in the underlying
212:32 - parameters of its deep neural network. Now,
ostensibly, or theoretically, the, the PPO
212:38 - method should account for that and allow you
to use a similar learning rate, because the
212:44 - actor should be less sensitive than in the
case of ddpg, but I haven't tested it. So
212:49 - one thing you can do in your spare time is
play around with different learning rates
212:52 - for both the actor and the critic. Our forward,
feed forward function is pretty straightforward.
213:00 - You want to pass a state through your critic
network and return that value. And we're going
213:10 - to need saving and loading checkpoints, I
am just going to yank and paste those because
213:18 - the functions are otherwise identical. And
so that is it for our two networks. Now we
213:25 - come to the heart of the problem, which is
the agent class. Do I have an extra? Yes,
213:31 - I do. And this, of course, does not derive
from anything. This is our base agent class.
213:38 - With a number of actions, a default value
for gamma, which is the discount factor in
213:44 - the calculation of our advantages. Typically,
we use something like 0.99, a learning rate
213:51 - of 0.003 minus four, I got this from the paper.
So if you read the paper, they do give you
214:00 - the hyper parameters and a little bit of detail
on the networks they used. But it is not a
214:04 - very well written paper, it's rather obtuse.
So I'm not a huge fan of it. But we do have
214:13 - some good default values from it. So policy
clip. So in my cheat sheet here, I have a
214:19 - value of 0.1 as a default, although in the
paper, they use 0.2, perhaps I was experimenting,
214:27 - I will have to be careful with that a batch
size of 64, a default and a 2048. And so that
214:34 - is the horizon, the number of steps before
we perform an update, and the default for
214:39 - the number of epochs. Now these parameters
come from these parameters come from the values
214:48 - for continuous environments. So the actual
numbers we're going to be using are going
214:53 - to be significantly smaller, as I said, we
use an N of 23 ybox batch size of five instead
214:59 - of 64. And I'm going to go ahead and set that
policy clip to 0.2. Now that I'm looking at
215:06 - it, we need to ga lambda that is the lambda
parameter. But of course you can't use lambda
215:13 - because that is a reserved word for Python.
What else do we need? Um, yeah, I think I'm
215:20 - missing something in my other file here. That's
okay. I'll fix it on the fly. So then we go
215:25 - ahead and save our parameters, number of epochs
and RGA, lambda. Meet our actor network dems
215:52 - and learning rate 
takes input dims. And alpha, baby memory.
216:07 - batch size input one second. All right, hopefully
that is not as loud now. The toddler is playing
216:16 - with his grandparents always a hoot. So now
we need a function that handles the interface
216:22 - between the age and its memory. And it's just
going to be very simple self memory store
216:34 - memory. It's just an interface function, then
we need a function to save our models. Print
216:49 - saving models 
is just going to be an interface function
216:59 - between the agent and the Save checkpoint
functions for the underlying deep neural networks.
217:09 - And very similar for the load models function.
We have a visitor. All right, that is it for
217:25 - our bookkeeping functions. Next, we need something
to handle choosing an action. That'll take
217:32 - an observation of the current state of the
environment as input. And we want to convert
217:38 - that NumPy array to a torch tensor. And we're
going to add a batch dimension because the
217:45 - deep neural network expects a batch dimension.
And we'll be sure to specify that it is float.
217:58 - And then we're going to go ahead and pass
that through our neural networks. So dist
218:01 - equals self dot actor state, that'll give
us our distribution for choosing an action,
218:06 - we need the value of that particular state.
And then to get our action, we just sample
218:12 - our distribution. And then what we want to
do is go ahead and squeeze to get rid of those
218:19 - bash commands. And this might be something
I added. For TensorFlow two, I'm not I don't
218:23 - remember if torch requires it, but it doesn't
hurt anything. So for the prompts, you want
218:30 - to go ahead and return the log probability
of the action, we actually took that item,
218:36 - so that item will give you an integer. And
likewise, for the action, we want to squeeze
218:47 - it and get the action the item out. And similarly
for the value and then just return all three.
218:59 - So this will make our main function look a
little bit different than we're used to, because
219:02 - we're going to be accepting three values from
our transaction function instead of one. But
219:07 - that's necessary for keeping track of the
probabilities and values as well. Next, we
219:13 - come to the meat of the problem, so to speak,
our learning function, so we want to iterate
219:20 - over the number of epochs. So we're going
to have, in this case three epochs. At the
219:26 - top of every epoch, we want to get our arrays,
the old probabilities, the values, the reward,
219:41 - the dance and the batches. Do that and then
I'm just going to use a different note. tation
220:00 - here, and go ahead and start calculating our
advantages. So our advantage is just going
220:08 - to be a NumPy array of zeros. Len reward,
type MP float 32. And we're going to say for
220:22 - T and range, so for each time step, Len a
reward array minus one, because we don't want
220:29 - to overwrite the, our go beyond the bounds
of our array, our discount factor is going
220:36 - to be one, the advantage of these times have
starts out as zero. So we're okay in range.
220:42 - So we're going to start out at T and go from
t to the same reward array minus one and say
220:49 - a sub t plus equals discount so that ga times
lamda factor, which starts out as one times
220:59 - we need parentheses, or array sub k, plus
saltdogg, gamma times values k plus one times
221:14 - one minus and dot array, okay, minus values
sub k, then we say discount times equals self
221:27 - dot gamma times G, or lambda, at the end of
every calculation, the end of every case steps
221:37 - advantage sub t equals at a sub t. And at
the end, we're going to turn advantage to
221:44 - a tensor. In particular, a CUDA tensor. And
this is just a strict implementation of the
221:55 - equation from the paper. So this, right here,
in parentheses, is the Delta sub t. So it's
222:02 - a reward plus gamma times v sub t plus one
minus V sub t, where, you know, we swapped
222:09 - the K and T here, and you need the one minus
dunnes on the values as a multiplicative factor
222:16 - the vize of a T sub t plus one, because the
value of the terminal state is identically
222:21 - zero, that's just a convention in reinforcement
learning predates the deep neural network
222:26 - stuff is just how we handle it, it's assumed
that's why they don't put it in the calculation,
222:31 - it is assumed it's just a matter of convention.
And then that discount is the GA the lambda
222:36 - multiplied by the gamma, that takes care of
the multiplicative factor. So it is the the
222:43 - gamma lambda to the T minus one power, or
is it t minus k minus one, something like
222:51 - that power, multiplied by the Delta, and then
you're summing it all up. So now we have our
222:58 - advantage. I'm going to convert the values
to a tensor as well. And I fully admit here
223:07 - that going from Val's array, to you know what,
in fact, let's do this. Now, let's see what
223:17 - the weight is, may not be the most effective,
or excuse me, the most efficient way of doing
223:24 - it, but sometimes, I just get stuff to work.
And then don't go back and clean it up. If
223:29 - you want to clean it up, please do. So I always
invite that. And it looks like I'm missing
223:37 - something here because it is not automatically
indenting. So I'm probably missing a parenthese
223:47 - somewhere and it is right here, I believe,
if I'm not mistaken. Yeah, there it goes.
223:54 - Alright, so then states, it's just going to
be a tensor state array, sub batch, the type
224:03 - to float to salt actor dot device. And we're
kind of violating the pep eight style guides
224:12 - their style guide by going beyond 80 characters.
But I think we'll be all right old probabilities
224:18 - gets converted to a tensor. And I don't need
an explicit D type there. I don't think that
224:31 - vice to salt that accurate advice that works
and then actions. Okay, and then. So we have
224:55 - the states we encountered the old probabilities
according to our old Vector parameters, the
225:02 - actions we actually took the next parameter
we need. So we have the bottom of that numerator
225:07 - pi theta old, we need pi theta nu. So we have
to take the states that we encountered and
225:14 - pass them through the actor network and get
a new distribution to calculate that new,
225:18 - those new probabilities 
will also need the value of the the new values
225:32 - of the states according to the updated values
of the critic network. So you may as well
225:36 - get those now. And we can squeeze those. And
then we can calculate our new probabilities
225:51 - and take the prob ratio. So here, I'm going
to exponentiate the log probs to get the probabilities
226:04 - and take the ratio, you could also do this.
Those two are equivalent by the properties
226:13 - of exponent, exponent exponentials, excuse
me. And then we're going to calculate our
226:18 - weighted probabilities. And sorry, our probability
ratio. Now, yeah, the weighted probabilities,
226:27 - think I have two lines that do the same thing
in there, that's fine. That's going to be
226:31 - the advantage batch times a probability ratio,
and we need the weighted clipped probabilities.
226:42 - And that is going to be the clamp of the proper
ratio between one minor self dot policy clip
226:51 - and one plus self dot policy clip multiplied
by advantage sub batch. Now our actor loss
227:02 - is going to be the negative minimum of the
weighted probs or the weighted clipped Prague's
227:15 - that mean, and our returns for our critic
loss are going to be the advantage plus the
227:27 - proviso for that particular batch. And so
our critic loss, then is going to be the returns
227:39 - minus critic value squared. And the mean value,
our total loss after loss plus 0.5 Times critic
227:59 - loss. Remember, we're doing gradient ascent
and there's a negative sign in front of the
228:04 - actor. So we're not doing descent that's another
thing that's kind of suboptimal by the way
228:11 - the paper is written, you can get kind of
confused about negative signs if you're not
228:15 - paying very careful attention. Next, we have
to zero our gradients. I think you can probably
228:27 - hear that my son is giving a concert downstairs,
he's playing the drums by whacking on his
228:33 - toy box with some drumsticks. So we're going
to back propagate our total loss. And then
228:42 - step our optimizers. And finally, at the end
of every epoch Yeah, I think that's the right
228:58 - indentation, we want to clear our memory.
So at the end of all whoops, at the end of
229:03 - all the epochs we want to clear our memory.
Let me just make sure I'm not doing that.
229:10 - Eg POC No, I'm not okay. That is good. So
now let's do a write quit. And I have an indentation
229:19 - error here. I see. Oh, that came in when I
did the yank and paste. Alright, so that is
229:30 - it. For our agent file. Let's go ahead and
take a look at Main. So we start with our
229:44 - imports. We'll need a gym will need NumPy
to keep track of the right
229:50 - Average our scores from PPO torch will need
our agent. And if you're new here, I have
230:03 - a utility file that I use a map plot live
pie plot function to plot the running average
230:09 - of the previous 100 games. For the learning
curve, it's pretty trivial, you can just do
230:14 - a plot of the running average, just do a git
clone. If you want to use my exact version,
230:20 - I don't go over it in every video because
it's kind of redundant, but I'll leave a link
230:25 - in the description to the GitHub. So go ahead
and do a clone of that. So you have that file
230:31 - or just write your own. So we're going to
use the very basic cardpool v zero. Reason
230:39 - being, we don't need to spend a whole lot
of time on a very computationally complex
230:44 - environment to realize we made a mistake.
So it's very easy to see if something got
230:48 - screwed up with the card pole environment.
This certainly will work on more advanced
230:53 - environments. But it does require a little
bit of fine tuning. So we'll just start with
231:01 - the card pool. And then you can play around
with other environments at your leisure. So
231:05 - we'll use parameters I dictated in the lecture,
I think I change the number of epochs to four
231:17 - to three, we get the number of actions directly
from our environment, very handy. Pass in
231:30 - all the other relevant parameters got a number
of input dimensions from our environment.
231:53 - And we're only going to play 300 games as
I'm looking at this, I do realize that the
232:00 - parameters I did the last time I ran, I did
do a policy clip of 0.1. The 0.2 comes from
232:06 - the paper at hand. I'm pretty sure it works
both ways. So we will find out if we need
232:10 - to, we can go back and change a policy clip.
Not a big deal. So plot slash card poll dot
232:18 - png need to keep track of our best score this
minimum score for the environment, empty list
232:28 - for a score history. And number of learning
times we call the Learn function, you can
232:40 - make this a member variable of your agent
if you want. And an average score starting
232:47 - at a zero we don't actually need that, but
whatever. So we'll say at the top of every
232:56 - episode to reset our environment. So the terminal
flag to false and a cumulative score to zero
233:06 - or we're not done, we need to choose an action
based on the current state of the environment,
233:18 - get the new state reward done and debuginfo
back from the environment, increment our score
233:27 - by the reward and store that transition and
the agents memory worn and done. And if n
233:42 - and do need an extra variable here say n steps
equals zero and that's the number of steps
233:50 - to take. And we need that because we have
to know how often or when it's time to perform
233:57 - the learning function. So every time we take
an action, the number of steps goes up by
234:05 - one. So then steps modulus, n equals zero
then agent dot learn fitters plus equals one.
234:20 - And then no matter what happens, we want to
set the current state to the new state of
234:25 - the environment. At the end of every episode,
append our score and calculate our mean that's
234:38 - the previous 100 games then average score
is better than the best known score then set
234:50 - that The best score to the current average,
and save your models. And we also want some
234:59 - debug information is so I score pore size
should be an average score. The I like to
235:22 - pronounce this isn't necessary. But the number
of steps that have transpired in total, and
235:31 - the number of times the agent has called the
learning function. This gives you an idea.
235:40 - This is I did this because when I compare
with the results of the paper, it wasn't clear
235:46 - to me if they were talking about the number
of times they called the learning function
235:50 - or the actual absolute number of time steps
in the environment. So I print out both these
235:57 - time steps and learning steps are totally
optional. You don't have to print it out.
236:01 - It's not something that is required. So I
just do it for my own clarification. We need
236:08 - an X axis for our plot. One score history
and plot learning. Oh, certainly. Let's do
236:22 - this. We don't want to do it every single
game. We want to do it at the end of all games.
236:30 - All right. Now moment of truth. Let's see
how many typos I made. So it's telling me
236:46 - it got an unexpected argument, input dims.
That's interesting. What do I call it? I don't
236:57 - have it there. And the reason is, my computer
had a hard lock up and I had to do a reboot.
237:07 - And it mutilated my cheat sheet for this.
So there's bound to be some errors in here.
237:16 - I didn't do my make directories. So temp,
PPO and plots. Let's try it again. Named Dunn's
237:30 - array is not defined, it's probably done array
that is in line 161. So it is Dunn's array.
237:43 - Yeah, just change it there. I guess. Old property
is not defined, that's probably the same thing.
237:56 - Where am I? Yeah, old prop array. I'll do
the opposite here. I'll make it singular,
238:09 - just for just for the sake of not being consistent.
Index eight is out of bounds. Okay. So then
238:18 - something has gone extremely wonky with the
generation of the batches. Okay, let's take
238:25 - a look at that. Oh, wait, let's read this
a little bit more carefully. It says index
238:29 - eight is out of bounds for access zero with
size zero. So our action array Oh, you know
238:35 - what? Let's take a look at our memory. So
it's action array. So here we have self dot
238:41 - actions. We return the self actions. Self
thought, there we go. That's why. So action
238:57 - and action. All right. Now let's try it. Name
advantage is not defined that as a typo, that
239:08 - is in line 183. A advantage today, yes, try
that. try once more. has no memory underscore
239:31 - clear memory, it's memory. Clear memory 197.
Alright, so now it is running. So I'll let
239:47 - that go for a few minutes and we will see
how it does. Alright, so it has been running
239:56 - for a little bit, maybe just a few minutes.
Now it runs relatively quickly. And what I'm
240:01 - seeing is that we do get some oscillations
in performance, you see, it'll hit 200 for,
240:09 - you know, several games in a row, and then
it'll drop down into the mid one hundreds,
240:15 - even, you know, 66, something relatively low
like that. And there's a little chunk here
240:20 - where it dips below 100 points. So it's not
a silver bullet, but it looks to be recovering.
240:25 - So we'll give it another 80 rounds and see
how it does. Okay, so it has finished up.
240:34 - And you can see that it finished strong with
a long run of about 50 games 45 games have
240:41 - a score of 200. So I commented, when I was
writing the agent that I was looking at my
240:47 - cheat sheet and had a policy clip value of
0.1, it could be that I'd settled on that
240:53 - value based on some experimentation, and then
changed it back just to be more consistent
240:58 - with the paper for this particular video.
So that's something I would play with. Other
241:03 - thing to consider is that there is significant
run to run variation that is a facet of pretty
241:07 - much every algorithm in deep reinforcement
learning, it just has to do with the way the
241:11 - neural networks are initialized, as well as
how they number a random number generators
241:17 - initialize the environment. So when you see
papers, you'll typically report average values
241:24 - for say, five or 10, or whatever number of
games and then a band to show the range of
241:29 - variants for run to run variation. But this
is clear evidence of learning in order to
241:33 - achieve the score of 200 and under 300 games.
So I call this good to me this is fully functional.
241:40 - Now there are a number of things you can do
to improve on this, you can get it to work
241:43 - with continuous environments, you can bolt
on some stuff for doing Atari Games, where
241:50 - you would need to add in convolutional neural
networks as your input layers, and then flatten
241:56 - them out to pass them to a linear layer for
the actor and the critic. And you can see
242:03 - my earlier video on an AI learns to be Pong
for Q learning. There, I go over all of the
242:10 - a lot of the stuff you need to do to modify
the open AI, gym Atari environment to do a
242:16 - frame repeating that something they do in
Q learning. That's an exercise to the reader.
242:21 - Actually, I don't know, thinking back to the
paper, I don't recall if they actually do
242:25 - any frame repeating or not in this particular
algorithm PbO. But it's just something to
242:30 - look at anyway. So there's a number of things
you can do to improve upon it. I haven't added
242:35 - this module to my course yet. I'm still working
on it, I really want to take some time to
242:42 - give more thought to the paper because the
paper isn't very well written. And I'll probably
242:46 - have to do a lecture like what I did for this
YouTube video in the course, because the paper
242:51 - isn't very easy to implement just by reading
it. So I hope that was helpful. That is PPO
242:56 - and just a few 100 lines full implementation
in pytorch. Solving the carpool environment,
243:03 - then you can easily modify this to do other
environments as you wish. If you've made it
243:07 - this far, please consider subscribing hit
the bell icon, leave a like a comment down
243:12 - below. And I'll see you in the next video.
Welcome, do a crash course and soft actor
243:18 - critic methods, you're going to learn the
least painful way to quickly implement the
243:22 - salt actor critic algorithm using TensorFlow
two, we're going to implement this and tested
243:27 - on the PI board environment, the inverted
pendulum, because it's relatively quick to
243:33 - compute, it runs pretty fast. So we'll know
whether or not we got it right relatively
243:37 - quickly. So what exactly is assault actor
critic algorithm? So this algorithm sets out
243:44 - to address a pretty fundamental issue in deep
reinforcement learning, which is how do we
243:49 - use maximum entropy framework in actor critic
methods? We're going to go a little more detail
243:54 - into this in a moment, but that is the basic
idea behind what we want to do. So why would
244:02 - this even be something worth considering?
Well, as you may be aware, actor critic methods
244:07 - have a number of fundamental limitations,
not the least of which is the fact that they
244:12 - have what is called brutal convergence, meaning
that they suffer from a high degree of hyper
244:17 - parameter tuning. If you go monkeying around
with hyper parameters, the agent breaks and
244:21 - doesn't know what to do in the environment.
And so once you find a set of hyper parameters,
244:24 - you really have to stick with them. Worse
than that, you have a problem called high
244:29 - sample complexity, which is just a fancy way
of saying you need to play a whole bunch of
244:32 - games for the agent to figure out how it works.
It's not very efficient for environments in
244:38 - which you have a large number of large state
spaces and high number of actions in a continuous
244:44 - action space. Worst of all, these two fundamental
drawbacks really limit room replicability?
244:52 - And is probably one of the big reasons behind
why we haven't seen any widespread adoption
244:57 - of deep reinforcement learning something like
say robotics. So sometimes your critic, as
245:04 - I said, is a maximum entropy framework. And
what this means specifically is that the agent
245:10 - is going to maximize both long term rewards
and entropy. Well, what does entropy even
245:15 - mean in this context? Well, if you're not
familiar with the concept of entropy, it's
245:20 - strictly speaking, a measure of disorder in
your system. It stems from statistical physics
245:25 - and thermodynamics. It's basically the log
of the multiplicity of your system, the number
245:29 - of ways of arranging the components of your
system. What does that mean in the context
245:33 - of deep reinforcement learning? It means the
randomness of the actions of your agent. And
245:39 - you might wonder, why would you want to maximize
both long term rewards and randomness of your
245:45 - agent, the region, the reason is that we need
to have some degree of random actions to test
245:51 - our model of the world, the agent starts out
knowing absolutely nothing about the world,
245:55 - and so should act as randomly as possible.
And then as it starts to figure out what actions
245:59 - lead to rewards, it should eventually start
to converge on taking mostly those actions,
246:04 - but still spend some time exploring to make
sure that there isn't some better action out
246:08 - there. This isn't a totally alien concept
to you, if you're familiar to cue learning,
246:14 - or familiar with cue learning, there, we use
what's called epsilon greedy action selection,
246:19 - where some proportion of the time say 10%
of the time, we take a random action no matter
246:24 - what even if we know what the best possible
action is, we may still take a totally random
246:29 - action, simply because you can never be 100%
certain that you're right. In this algorithm,
246:36 - we're gonna be modeling entropy by reward
scaling, so we're gonna have a multiplicative
246:39 - factor for our reward. And there's going to
be an inverse relationship between our reward
246:45 - scale and the degree of entropy in the system.
And you can kind of think of this intuitively,
246:51 - because if you increase the scaling on the
reward, you're going to increase the contribution
246:55 - of that reward to the cost function. And so
you're going to kind of tilt the neural network
246:59 - parameters towards maximizing that reward.
Whereas if you lower that parameter, then
247:04 - you're going to lessen the contribution of
the signal of the reward to the updates of
247:10 - the neural network, and so decrease its overall
importance to the cost function. This algorithm
247:16 - is also going to leverage all the neural networks
possible around networks for actors, value
247:21 - network and critic networks. And in particular,
we're actually going to have two critic networks,
247:26 - which is going to be an exact analogue of
the double q learning algorithm, as well as
247:30 - twin delayed deep deterministic policy gradients,
another awesome algorithm for continuous action
247:35 - space environments. Please see other videos
on this channel. If you don't know much about
247:41 - those, I have a multitude of videos covering
both of those topics. They also make use of
247:47 - another innovation from Q learning, which
is the use of a target value function. So
247:53 - the idea here is that we're gonna be using
our value function to track the values of
247:58 - states to tell us which states are viable.
So we can, you know, seek out those states
248:02 - again, that's the idea behind reinforcement
learning. But the problem is that we're going
248:06 - to be updating that value function with each
time step. And so if you're updating at each
248:10 - time step and using it to evaluate the values
of the states, then you're really chasing
248:16 - a moving target. And so the algorithm will
suffer from instability. And that happens
248:22 - a lot in Q learning as well. The solution
is to have a target value function, which
248:27 - is updated only periodically or very slowly.
So in queue learning, we do a periodic update,
248:34 - where we just exactly copy the networks, the
network parameters from one network to another
248:39 - from the online to the target network, here,
we're going to make use of a soft that date,
248:43 - where it's going to be some kind of moving
average of the online and previous values
248:48 - of this target value network. I'll show you
that equation later in this little lecture.
248:56 - So our actor network is going to model both
the mean and sigma of a distribution. So this
249:00 - is a probabilistic policy, it is not a deterministic
policy, like in ddpg. So we're our actor network
249:09 - is going to output two parameters, a mean
and sigma. And then we're going to put that
249:12 - mean and sigma into a normal distribution
and sample a to get an action. Now the original
249:18 - paper use what they call a re parameterization
trick. And I will fully admit I'm a little
249:23 - fuzzy on exactly what that means. It kind
of sounds like they are sampling some normal
249:28 - distribution, then adding on some additional
noise to it. I implement this in what I do
249:35 - a what I believe to be a faithful implementation
of this and pytorch but I'm not going to do
249:39 - it in this tutorial. I think it's a little
superfluous and I'm actually getting, you
249:44 - know, really good results without it so I'm
kind of throwing it away. We do have a special
249:49 - function to enforce And bounds as well as
to calculate the log of the probabilities
249:54 - of taking some actions because the log of
the probability will play a big role in our
249:58 - update rules for our networks, which we'll
get to momentarily, and I'll show you that
250:03 - function later on in lecture as well. Another
thing to note is that we can use multiple
250:09 - steps of gradient descent like we do in proximal
policy optimization or PPO. But we're not
250:15 - going to do that, we're going to just use
one step of Adam opposite Adam optimization
250:21 - per time step of the environment. Some other
implementation notes, we're going to have
250:28 - a replay buffer based on NumPy arrays, I prefer
NumPy. arrays, that is by no means the only
250:33 - way to do it not even necessarily the best
way, it's just my preferred solution, change
250:38 - that if you want. Now we're going to keep
track of the states the agent sees the actions
250:43 - it took the rewards that received the new
states that resulted from those actions, as
250:49 - well as the terminal flags from our environment.
And the terminal flags are important, because
250:53 - we have to use that in the update for our
deep neural networks. Because when we're evaluating
250:58 - device states, we have to take into account
whether or not that state was terminal. The
251:02 - reason is that terminal states have no value.
Because the episode is over no feature rewards
251:06 - follow the value is just the present value
of the discounted future reward. So the reward
251:12 - is the episode is done, it's zero by default.
Now, this is the equation I alluded to earlier,
251:19 - where we are calculating the log of the probability
of selecting some action given some state.
251:26 - That's what that pie means. It's a probability
slotting an action which is good continuous
251:30 - parameter, given some state, this mu is our
actual sample of a of a distribution with
251:36 - mean and segment given by our deep neural
network. So what is not the output of our
251:40 - deep neural network, it is the sampling of
a distribution where the mean and sigma are
251:45 - given by our deep neural network. And another
thing to note here is that we are going to
251:51 - have to multiply our action by the max action
from the environment. Reason being is that
252:00 - the action is going to be proportional to
the tan hyperbolic function. And that is bounded
252:04 - by plus or minus one, which not all environments
are bound by plus or minus one. So you want
252:10 - to take into account that fact, we don't want
to cut off half of our action space arbitrarily.
252:15 - So let's talk about updating our deep neural
networks, our actor network update is pretty
252:22 - straightforward. What we're going to do is
sample states from our buffer, but compute
252:26 - new actions. So this is an off policy learning
method, where we're going to be using samples
252:31 - generated by one policy to update some newer
policy. We're going to stick those states
252:38 - through our actor network and get new actions
out on the other end, compute that log prob
252:45 - based on what we saw on the previous slide.
And then we're going to subtract off the minimum
252:50 - value of the two critics. So we're going to
pass the state sampled from our buffer the
252:54 - actions computed in our update, according
to the current visor actor network, and then
253:00 - take the minimum of the critic evaluation
of those state and action pairs. This is kind
253:05 - of the double q learning rule in action. That's
why as cute men, and the one over N some tells
253:12 - you that we're taking a mean of that quantity,
that difference. Our via network update is
253:20 - a little bit more convoluted. So we're going
to have one half the mean squared error, that's
253:26 - what the one over N, the one half of the difference
squared, tells you of the difference between
253:33 - the value function using the current parameters
or a value function for the state sample from
253:38 - our buffer. Again, we're going to subtract
off the minimum of the Q values, where the
253:44 - actions are chosen according to the new values
of our hacker network. And we're gonna again,
253:50 - subtract off that log of pi or again, the
action that is computed according to the current
253:55 - values of our actor network. And again, the
log is computed according to a couple slides
254:00 - ago. The target value network, on the other
hand, doesn't get any gradient descent to
254:06 - determine its new parameters, we're going
to be updating its parameters with the following
254:11 - equation. So we're going to take this hyper
parameter tau and multiplied by the values
254:17 - of our online via network given by Psy that's
just the symbol for the parameters for our
254:24 - value network. And we're going to add on one
minus tau. So point 995, multiplied by the
254:30 - current values of the target value network,
take the sum of those two and upload those
254:35 - values to our target value network. And it's
a slowly moving average of online and target
254:40 - networks. Our critic network gets rather interesting
update. So here both critics get updated So
254:50 - there's no minimum opera operation here. So
there's two cost functions, one for each critic.
254:56 - And it's just given by the mean squared error
of the difference between the output of the
255:01 - critic for the states and actions sampled
from the buffer. And we're not calculating
255:06 - new actions here, that is one difference between
the update of the critic and the actor advised
255:10 - networks. And we're subtracting off this quantity
Q hat. And that is where the actual entropy
255:15 - comes into play. So it is a scaled reward,
plus the values of the new states according
255:21 - to our target value network. So this is where
all the magic of the algorithm happens. That's
255:27 - where the maximum entropy framework comes
into play in this one little equation. kind
255:32 - of neat, huh? So we're going to need a whole
bunch of data structures. class for our replay
255:38 - buffer, again, that'll be NumPy. arrays, the
actor network gets a class a critic network
255:43 - gets a class, so does the value network, our
agent gets its own class. And that's going
255:47 - to tie everything together that'll have functionality
for choosing actions, saving models, learning
255:53 - as well as saving memories. And we will have
a main loop to train and evaluate the performance
255:59 - of our agent. Now we're going to need a number
of packages will need TensorFlow, I prefer
256:06 - GPU obviously, this is a highly computationally
expensive algorithm. So if you have the GPU,
256:11 - please use it, but need pi bullet for our
environment, Jim, of course, NumPy and TensorFlow
256:17 - dash probability, that is, the only way we
can get access to this probability distributions
256:23 - are not built into the base TensorFlow package
for some reason. Okay, that's it for mini
256:29 - lecture, your Crash Course. Now let's go ahead
and throw you in the water and start coding.
256:34 - Alright, so let's go ahead and get started
with our buffer class. And then when we want
256:41 - to do our networks and our aging class, so
we're going to be using NumPy arrays for our
256:50 - memories and so NumPy as our NumPy will be
our only package dependence for this particular
256:55 - part of the agent will need a max size, in
good shape and number of actions as input,
257:04 - we will need to save our MAX SIZE. And we
will also need a meme counter to keep track
257:11 - of the first available memory position. And
our agents memory is represented by NumPy
257:18 - arrays that we will initialize it as zeros
in the shape mem size by input shape or input
257:25 - shape is some tuple or list. We need the new
state memory as well that are the new states
257:34 - that the agent sees as a consequence of the
actions it takes in the environment, we do
257:39 - the action memory. And that is in shape men
size by a number of actions. Keep in mind
257:49 - number of actions means number of components
to the actions since these are continuous
257:52 - actions, they will have components along some
dimension and action space. It's not a actual
258:00 - number of discrete actions, it's a little
bit Miss aptly named reward memory. And that's
258:07 - just a vector and an array to keep track of
the terminal flags we received from the environment.
258:19 - And I'll go into that as num, type NumPy bool.
Next, we need a function to store Trent store
258:26 - transition. And if it isn't clear, the reason
we need to store the terminal flags is because
258:30 - the value of the terminal state is always
zero because no future rewards follow the
258:35 - terminal state and so it has no value. So
we will need to save all those parameters
258:43 - in our agents memory. And we want to know
what is the position of the first available
258:48 - memory. And that's given by the modulus of
memory counter and mem size. This has the
258:53 - property that when we overwrite the agent's
memory, we will go back to the very beginning.
258:57 - So we don't have to worry about going beyond
the bounds of our array. So why don't we just
259:03 - go ahead and save our variables 
259:21 - and terminal memory. And most importantly,
we need to increment our memory counter by
259:31 - one. Next we need to function to sample our
buffer. And I want to make a quick I want
259:38 - to make a quick aside here. So in a question
in the comment section someone asked Is there
259:44 - a way to sample memories according to how
viable they were reading our early memories
259:48 - agents acting randomly doesn't know what it's
doing. So what You know, what good is it to
259:51 - sample those memories, isn't it more important
to sample memories where we kind of knew what
259:55 - we were doing and had some reasonable model
the environment. And you can do that in something
260:00 - like prioritized experience replay where you
prioritize memories based on their utility.
260:05 - But in this case, we're just doing uniform
sampling. So we just take the good with the
260:12 - bad. So we want to know the first available
memory how far we have gone in our memory.
260:23 - And that's given by the minimum of the meme
size and the meme counter, so that we're not,
260:28 - you know, if we've filled up half of our memory,
we don't want to sample the latter half, which
260:31 - is all zeros, we only want to sample the first
half, which is filled up with useful transitions.
260:39 - So then we're going to go ahead and take a
batch size of memories from from our memory
260:46 - bank, and go ahead and dereference. Sorry,
we need an extra space there. And then go
261:13 - ahead and return those states actions towards
new states and terminal flags. And that is
261:20 - it for our memory buffer class. Next up, we're
going to handle our networks for the agent.
261:27 - So the first thing obviously, we want to do
is come up and fix our imports. Because we're
261:34 - going to need more stuff than just NumPy.
We're going to need OS, the base TensorFlow
261:40 - package, care OS and we will need that flow
probability has TFP now, you will have to
261:52 - install this package separately, it is not
included in the base TensorFlow package. So
261:59 - see the TensorFlow documentation if that is
something you don't know how to do. But you
262:05 - do need to do that installation separately.
So our critic network will derive from Kerris
262:11 - dot model, I guess has access to making use
of the gradients and the gradient tape and
262:18 - all the other good stuff that we get from
the base class. We will need a number of actions
262:29 - number of F's DIMMs FC one FC two dims a name
so the name is useful for model checkpointing.
262:39 - So we can keep track of which file corresponds
to which network and a directory for saving
262:48 - models you have to do make dir on that otherwise
you'll get an error. First thing you want
262:56 - to do is call your super constructor. Save
the relevant parameters. One of these days
263:17 - I'm going to shorten this checkpoint dur name,
but not today. So then we want to join the
263:26 - checkpoint file name checkpoint directory,
excuse me, with the name and underscore assault
263:37 - actor critic so that when we save a bunch
of stuff in a single directory, we know which
263:40 - file corresponds to which algorithm. Next
we can define our model. Our first fully connected
263:48 - layer is just a dense layer that has implied
input dimensionality and takes FC one dims
263:55 - as output with a row activation. Likewise
for a second layer. And our final output is
264:09 - single valued with excuse me a single node
with no activation. So, we have to input a
264:20 - state and action to our call function, because
the critic network evaluates the values of
264:24 - state action pairs. And so the first thing
we want to do is pass the concatenated state
264:32 - and action pair through the first fully connected
layer. And we want to concatenate along the
264:41 - first axis and pass out through the second
fully connected layer. Pass through The topic
264:51 - layer and return. Pretty straightforward there.
Next, let's handle our value network. And
264:59 - again, that arise from karass model. We don't
have to specify any dimensionality here other
265:12 - than for our first and second fully connected
layers. And we want to call our super constructor
265:27 - again. And if you want to get really fancy,
you could write a base model based network
265:57 - class, and then have everything derived from
both the I guess the base model class would
266:03 - derive from the karass model. And then everything
that derives from that would derive from the
266:10 - base class base model, sorry, base network
class. But that can get kind of messy really
266:16 - quickly, you have to deal with multiple inheritance.
Maybe it's not as hard as I initially think
266:21 - it is. But it's an alternative paths, it's
an alternative way so that you don't have
266:25 - to keep writing like the same stuff over and
over again. Because you notice there's a lot
266:33 - of overlap between the code of the network
so you can fix that with inheritance in our
266:50 - output is again going to be single valued.
No activation. Now, the call function here
267:02 - only takes the current state of the environment
as input, the value function is only concerned
267:06 - with the values of states not state action
pairs. Then we just go ahead and pass all
267:24 - of those through and return them. Pretty straightforward.
The real difficulty doesn't occur until the
267:31 - actor network and the difficulty will come
in the call function as well as the sample
267:42 - normal. So we want to pass in a max action
all using that here. Sorry, I always kind
267:50 - of make stuff on the fly kind of modify stuff
on the fly. Yeah, so we do need that FC one
267:57 - dims. FC two dims. A name number of actions
are used to by default again, that's number
268:08 - of components. And a checkpoint directory.
protocol are super constructor and save the
268:23 - relevant parameters. Okay, then we have our
model name. And just as an aside, you can't
268:38 - use name, it's a reserved variable, we can
say self dot name equals name, because name
268:44 - is reserved by the base class karass model,
so you have to use modeling. So we're not
269:09 - going to be using three parameterization.
But we will need some noise factor to make
269:14 - sure we don't get something that blows up
when we take the log. You'll see what I mean
269:18 - very shortly. Our neural network by now should
look pretty familiar. Row your activations.
269:33 - Here's where things get interesting. We're
going to have a new for our distribution as
269:39 - well as a sigma no activation So let's go
and handle our call function. So we're gonna
269:54 - get our I've called it prop here, but I don't
know if I like that name now that I'm rewriting
270:02 - it. But I'm gonna roll with it. So mu, really,
that problem is just the input of our first
270:12 - two layers, you know, the pastor of the state
through the first couple layers. Why are sigma
270:18 - which is the standard deviation of our distribution.
And we're going to want to clip this. It may
270:27 - be the case that TensorFlow two doesn't need
it. But when I was coding this up and pie
270:32 - torch, it, the distribution would blow up.
If we had a noise of sorry, a sigma of zero,
270:39 - it didn't like zero standard deviation distribution,
which I can understand. I guess that's a direct
270:45 - delta function, it's not exactly something
that you would encounter every day. And I
270:51 - think the TensorFlow might actually fail a
little bit more gracefully than the PI torture
270:56 - in that respect. But I go ahead and clip it.
So that goes between one by 10 to the minus
271:00 - six and one, so we're going to constrain the
standard deviation here Do not be too large.
271:04 - That is something you can optimize on your
own. I don't do extensive and detailed studies
271:09 - on all this stuff, I just get it working.
Check that it conforms to the specifications
271:14 - laid out the paper, and then I get something
reasonable on the output. But we're not done
271:21 - for the actor network, we have the output
of our deep neural network, which is a mean
271:25 - and standard deviation for our distribution.
Now we have to sample that distribution to
271:29 - get the actual action for our agent. And I've
stuck it on a function called sample normal.
271:37 - And as I stated, we're not going to do in
the re parameterization trick, show students
271:40 - how to do that in the course. And in fact,
in my pie torch video, I show you how to do
271:44 - it. Pie torch has a very simple function called
our sample that does basically the parameterization
271:49 - for you. You can implement the code yourself,
if you really want to be completionist about
271:56 - it. But I'm not bothering here because I do
get good results without it. It's kind of
272:03 - like what the ddpg paper where they use the
Orenstein will lindbeck noise. And then people
272:07 - that implemented a later kind of threw it
away, because they're like, Wait a second,
272:10 - this is unnecessary complication. That kind
of seems to be the case here where it worked
272:14 - without it. And it's an unnecessary complication.
But I mean, I could always be wrong. But I
272:21 - do get solid results without it. So here,
we're going to go ahead and instantiate our
272:28 - normal distribution defined by mu, and sigma.
And our action is going to be our actions
272:36 - are going to be a sample problem bill it T's
dot sample, if you want to do the re parameterization.
272:44 - You could re parameterize it there. And the
action is math, a tan hyperbolic times max
272:55 - action, that max action takes care of the
fact that our environment may have max actions
272:59 - outside of the bounds plus or minus one, which
are of course, the bounds of the tan hyperbolic
273:03 - function. So you want to not arbitrarily cut
off half of your action space if you don't
273:09 - have to. Log probs it's probabilities, bilities
dot log prob of actions, and then go ahead
273:20 - and subtract off math. Log one minus Tf dot
map dot path, excuse me. Action, so we're
273:31 - going to square the action plus self dot REAP
program. Call it noise did self dot noise.
273:42 - And so then I had to change it here as well,
don't I? Yeah. self dot noise plus self dot
273:51 - noise. Do I have the right number of parentheses
here? Yes, I do. And so since you have a logarithmic
274:00 - function here, you don't want to take the
log of zero that is oftentimes not advisable.
274:05 - So I just add in some really small number
to make sure we're taking the log of something
274:08 - finite. So then we're going to go ahead and
do a reduce some on that. Log probs x equals
274:18 - one to keep them true, in return the actual
and log props. So that wraps up the network's
274:28 - portion of our code. Now we have to handle
the agent class, which will tie everything
274:32 - all together. So of course, again, we have
to come back up to the top and add in some
274:38 - imports. We're going to need our optimizer.
So we'll say from TensorFlow Kerris optimizers
274:47 - import Adam, and I think that's the only other
import we will need. And then we come back
274:57 - down here and start our agent class, which
derives from nothing. Our initializer is going
275:06 - to take a number of learning rates. Now we're
going to use two separate learning rates,
275:13 - you could in principle, use three one for
the critic one for the actor, one for the
275:18 - value network. In this case, we're gonna use
one single learning rate for the actor and
275:23 - then the same, the beta learning rate will
be for the value and critic networks. And
275:29 - book dims something like eight, that's just
the default for the lunar lander, it's the
275:33 - only default I know off the top of my head,
we're gonna want to pass in the environment
275:39 - to get some useful information from it a gamma,
the discount factor for our update equation
275:46 - backsies for a memory, a million transactions.
Default Value of towel was 0.005. layer one
276:01 - size layer two size to fit the six bite size
default to 36. And a reward scale default
276:14 - of two. Let's go ahead and start saving replay
buffer, Mac size and input dims interactions
276:26 - pretty straightforward. Then we're going to
go ahead and define our networks. Sir Max,
276:53 - Max action is going to be the high value of
the access space from our environment. We
277:00 - need to our first critic, second critic. And
these are you know, identical in terms of
277:18 - their definitions, except for the name. Because
again, when we save files, we want to save
277:23 - separate files for both critics otherwise,
we're going to get the most value network
277:35 - in our target value. Okay, so now we have
to compile our networks. We'll need our critical
277:53 - one. Great a beta, or environment network
and our target. Now, of course, the target
278:22 - via network doesn't perform any optimization,
we just copy of weights using our soft update
278:30 - rule. But it is part of a which is part of
the framework that we have to compile the
278:35 - model before we can use it our scale factor
and our initial update network parameters,
278:49 - where we're going to do a hard copy of the
parameters from the online network to the
278:54 - target value network. The first function I
want to handle is the Choose action function.
279:00 - Then we'll get to the remember function, the
update network parameters. And our saved model
279:05 - parameters. Finally, we'll get to the Learn
function will save the best for last. So we
279:13 - want to convert our state our observation
tensor. And the neural network expects that
279:23 - we want to have a batch dimension. So we have
to do that. We have to add an extra dimension
279:29 - to get our batch. We want to get the actions.
We don't really care about the log prompts
279:34 - at this stage. And then we just go ahead and
return actions zero, because the tensor I
279:43 - believe is the output and we want to return
a NumPy array because the environment doesn't
279:48 - accept the TensorFlow tensor as input to this
function. So this is a simple interstate interface
279:57 - function between the agent and its memory.
And this is necessary because you don't want
280:04 - to directly access the values of the memory
class from the aging class. It's just bad
280:12 - software design, you don't want to go overriding
parameters of one class with another, you
280:16 - want to have an interface function that handles
all that for you that where you use one function
280:20 - to call another, it's just clean software
design. Always keep that in mind. I think
280:29 - state action reward new state. And it looks
kind of silly, we could get away with it in
280:35 - this context, because we're not going to be
building on this codebase later. But I always
280:38 - find it important to use strong and consistent
software design principles wherever possible.
280:45 - Our update network parameters function. Pretty
straightforward. So we're going to pass in
280:53 - a towel, which is a default value of none.
So tau, is none, then we want to go ahead
281:00 - and use our default value for towel. And this
has to do with the fact that appear on line
281:05 - 153. We're calling update now parameters and
the value of tau equals one to facilitate
281:09 - the hard network of hard network weight copy.
So we're basically going to just go ahead
281:15 - and iterate over our network weights, do the
calculation, append those to a list, and then
281:22 - upload that list to or by you target by your
network. wait times to helpless targets, times
281:45 - one minus tau. one too many parentheses there.
That looks right. weights. And that is it
282:01 - for update network parameters. Pretty straightforward.
Next, we have two bookkeeping functions to
282:07 - save our models. Those don't take any inputs.
Sylvain is not used word formation. So we
282:24 - just want to save the weights to the checkpoint
final checkpoint, I'll take a point that sounds
282:36 - pretty good. And then the load models function
is basically the inverse. So I'm just going
283:07 - to go ahead and yank and paste then make sure
to change save to load. So that way, we don't
283:17 - do anything wonky. After dot save weights,
we want to change to load weights. And same
283:36 - deal we want to load from the check point
files. Okay, now we come to the hard part
283:44 - of the problem, the Learn function, we've
got a kitty cat joining us, perhaps she will
283:50 - hop up on the desk, the first consideration
we have. So the first consideration we have
283:56 - is what we would do what we do in the event
that we haven't filled up enough of our memories
284:02 - to actually load a batch size of those memories.
And you can do many different things. You
284:07 - can play batch size of transitions randomly
and store the transitions and then call your
284:14 - learning function. Or in this case, you can
just say, hey, if I haven't filled up at least
284:17 - batch size of memories, just go ahead and
return. So that's what we're going to do.
284:22 - And this is a this violates my principles
of good software design where I'm calling
284:27 - the mem counter or the memory class directly
here instead of using a function to get it.
284:32 - This is bad design. So don't do this. If you
have an option not to I'm just doing this,
284:40 - because it's like I said, it's not gonna matter
if this isn't a huge code base that we're
284:44 - going to be building upon later. But just
know that this isn't consistent with what
284:48 - I said earlier. I'm kind of backpedaling.
A little bit. Now we have a sample our memory,
284:57 - we're gonna pass in our batch size, then we
want to go ahead and convert those two tensors.
285:07 - And I want to be really consistent with my
data types here. Reason being a lot of these
285:20 - frameworks, TensorFlow and pytorch, specifically,
get a little bit finicky when you start mixing
285:28 - up data types. They don't like to mix data
types and calculations, because that screws
285:34 - up the precision of the calculation. They
want everything to be. They want everything
285:39 - to be consistent and explicit, so that you
get exactly what you expect, because of course,
285:43 - there is a rather a significant difference
in large calculations between floating point
285:48 - 32 floating point 64, even in floating point
16 calculations. So it's very beneficial to
285:55 - go ahead and specify what data type you're
using. In this case, we use float 32. And
286:02 - we even in theory could get away with sorry,
these are TF not NP, we could get away with
286:12 - 16 point because 16 point precision 16 bit
precision, excuse me, because we don't need,
286:20 - you know, a whole bunch of decimal places
in our rewards or anything like that. But
286:26 - we use 32. Just to be safe. So now we're going
to handle the update rule for our value network.
286:42 - So we need our gradient tape. So we're going
to pass the state states through our network
286:56 - and then squeeze to get rid of that batch
dimensionality, then we're going to do the
287:01 - same thing with our new states. except we're
going to pass through the target by a network.
287:10 - Squeeze again, what are the actions according
according to our current policy, and the log
287:17 - probs. So we're going to sample normal states.
And we of course, don't do any real parameterization.
287:26 - Within our log probs, we have to squeeze and
we get the values according to the new policy
287:38 - critic one state's policy actions, and then
the value according to the second critic,
287:53 - then our critic value you TF that squeeze.
Do you have math minimum between the q1 new
288:08 - policy and q2 policy and I'm missing a privacy
there. There we go. Now we're to squeeze along
288:25 - the first dimension. So then our value target
is going to be our critic value minus our
288:32 - log prompts. That's from the paper value loss.
One and a half keros losses dot mean squared
288:42 - error between the value and the target value
value targets sorry. Sorry, my toddler's rampaging
288:56 - again. So now we need to calculate our gradient.
Good grief. I have the cat on my lap as well.
289:06 - It's making it very difficult to type value
loss self dot value dot train abort variables,
289:16 - and we can stick that on new line to be good
little programmers. Then we have to apply
289:23 - our gradients optimizer dot apply gradients
zip by network gradient value trainable variables,
289:40 - and this is just how we do gradient descent
using the gradient tape. And so that is it
289:46 - for our value network loss. Now, we have our
Actor, network and critic networks to worry
289:53 - about. So we need more gradient tapes. So
we need our new policy actions and their log
290:03 - probs on a sample are states Oh, that's interesting.
I have I see. That's interesting in my cheat
290:20 - sheet here I have the NumPy arrays, and it
works. Interesting. I'll go ahead and fix
290:28 - that. So let's see if it breaks when I pass
in the TensorFlow tensors it shouldn't I just
290:33 - have a typo in my cheat sheet. So let's go
ahead and squeeze our log probs and get some
290:44 - new policy values one policy actions here
two new policy 
291:08 - we have our critic value squeeze math minimum
291:24 - then I'll actor loss just the log probs minus
i critic by Michael loss goals. Math reduce
291:35 - me Petra loss. Then we have our actor network
gradient. Tape gradients after loss. The gradient
291:54 - of our loss with respect to our accurate trainable
variables that we want to apply our gradients,
292:00 - actor optimizer. And of course that expects
zip as input. And that handles our actor loss.
292:19 - And finally we come to the critic loss. Now
we have to pass the persistent evil true parameter
292:30 - to our function call, because the loss is
going to have two components. So we'll have
292:37 - a critic one and critic two loss. If you don't
pass versus an equals true flag, it only keeps
292:42 - track of stuff for the application of a single
set of gradients. So you can only do the update
292:47 - to one of your critic networks instead of
both. So you tell it to just keep track of
292:52 - the gradients even after it Go ahead, even
after it applies gradients one time. So you
292:56 - can apply gradients twice. Or q hat is our
scale factor and multiplied by reward. That's
293:03 - where the entropy comes in. gamma times value
underscore one minus done. That is interesting.
293:15 - So this gives me a little bit of pause, because
first of all, I know the code works, I've
293:20 - tested this, and I haven't accidentally deleted
a line. But what gives me pause here is that
293:25 - I have this value underscore, which is defined
up in this other scope here. I'm actually
293:31 - missing a equal sign there. That would have
triggered an error. So it does work. That's
293:42 - interesting. I didn't know that the context
manager shared the scoping of variables. I
293:49 - didn't know that that is new information to
me. So I want to go ahead and roll with it
293:56 - for now. If this is suddenly broken and doesn't
work, then I will come back and re edit this
294:03 - and put in the new code that actually works,
of course. But that is new information to
294:09 - me. Say I learned stuff even while making
content. So we have to get the values according
294:15 - to the old policy. Why do I call it old policy?
Perhaps go ahead and check the GitHub for
294:34 - this when I upload it. I will probably do
some variable name swapping to make things
294:39 - a little bit more logical. If I have to scratch
my head while I'm typing out the code here.
294:44 - You're probably scratching your head watching
it Of course, the losses are pretty straightforward,
294:53 - just mean squared error between the q1 and
q2 old policies and this Q hat value, then
295:06 - we can go ahead and calculate our gradients.
Critical network gradients. Want to go ahead
295:43 - and apply our bruneians. Sorry, so blood critic
to network. Brilliant, predict two trainable
296:17 - variables. And then we want to call our update
number of parameters function after we've
296:29 - done all of our updates, okay, so that wraps
up all of the heavy lifting for our code.
296:35 - Now we can move on to coding up the main loop,
which is going to be a cakewalk by comparison.
296:44 - And of course, I have an invalid syntax error.
Oh, that's easy. There is no self there actually
296:53 - is blank equals. And I've done it yet again.
And oh, because it's a sorry, my cat wants
297:16 - to steal the limelight. There we go. I did
the same thing here. Of course, why wouldn't
297:25 - I do the same thing there, I like to be consistent.
Okay, so now we're going to handle the main
297:29 - function. So of course, we will need our pie
bullets. And these could be because we're
297:41 - going to be dealing with the inverted pendulum
and bullet environment, we need our gym we
297:48 - need NumPy we need our agent. We need our
plot learning curve. And that is it. Want
298:09 - to make our environment inverted? pendulum
board envy me zero. Need to make far agent
298:24 - passing all of our roles and parameters to
play 250 games. Figure file 
298:53 - is just the directory plus the file name,
you can condense that into one line if you
298:57 - want, it's not a problem. We save best score
in the reward range zero score history want
299:07 - to keep track of the scores the agent receives
over time so we can see if it's learning as
299:10 - well as to plot them later. And a load checkpoint
variable, which you can set to false if you
299:18 - want to load a checkpoint. So we're going
to load the checkpoint and load your models.
299:26 - And one thing I like to do is set the render
mode to human so that we can see the agent
299:32 - and play the game because if you're loading
a checkpoint, you probably want to evaluate
299:37 - the performance. If not just comment out this
line. In other words, if you're doing checkpointing
299:42 - so that you can do more training later than
just get rid of that line. No big deal. Let's
299:49 - go ahead and play our game. To reset your
environment, at the top of every episode,
299:57 - you set your done flag and set your score
to zero. Play your episode by choosing an
300:06 - action. Take your action, get the new state
reward done and debuginfo back from the environment
300:20 - to crack your score. Remember your transition
300:33 - not load checkpoint, then you want to learn
the logic here being that if you're loading
300:39 - a checkpoint, you're probably evaluating.
If it's the case that you're just loading
300:43 - a checkpoint to perform more training later,
then you're going to need to put that agent
300:47 - dot learn outside of that if statement, just
get rid of the if statement. Okay, so that
300:53 - you're, you know, gonna do what you actually
intend. No matter what you need to set the
300:57 - current state to the new state. And at the
end of the episode, you want to append the
301:04 - score to the score history, calculate an average
of the previous 100 games. If that average
301:16 - score is better than your best score, then
set the best score to that average. So that
301:22 - you know you're learning. And again, if not
load checkpoint agent does save models and
301:31 - you want to print episode score average score
and then when all the games are over. Go ahead
301:55 - and what's your learning curve? Okay, moment
of truth. Let's see how many typos I made.
302:17 - Hello. That's easy. plog replay buffer as
attribute new size. It should be mem size
302:42 - self dot new size. I didn't do that elsewhere.
I don't think Okay, once that I not call it
303:01 - call that is interesting. So that is for my
value network. Oh, okay. replay buffer critic
303:12 - and value network. Oh, ah. There we go. Sorry
about that. indentation error. As no attribute
303:30 - sample. That's because it is sample normal.
That is in line 223. It does sample normal.
303:44 - persistent. Because I forgot to t that is
in line 239. Verses 10. equals true that's
304:03 - another typo perfect. Your ops is not callable.
See self dot gamma times value underscore
304:18 - oh I'm missing a multiplication sign. That
is in line 240 Sorry about that. On 241 mine
304:38 - is done. Good grief critic. One underscore
That is in line 251. Oh, because it's not
304:57 - there's no self in there. Oh, okay. Good grief.
There is no self there. Yep. Okay. Greens
305:22 - do not exist for one of my layers. Oh, okay.
means I have forgotten something in deed that's
305:35 - problematic because it doesn't tell me. Well,
these warnings here, don't worry about those
305:39 - those aren't a problem this Grady does not
exist is in fact a problem. So let me see,
305:44 - I don't even though which network it is talking
about there doesn't tell me says dense kernel
305:52 - zero. So that's probably whichever network
I made first. So I instantiate my Acura network
306:03 - first. So here is value after a loss. Let's
check out our handy dandy cheat sheet here.
306:19 - And so I have located the source of the issue.
And I will annotate this when I edit the video.
306:27 - But the issue here is that I was passing the
state through both the first fully connected
306:32 - and second fully connected layer, instead
of allowing the output of the first fully
306:35 - connected layer defeat into the second layer.
So of course, that's how deep neural networks
306:39 - work. And you won't get anything useful if
you don't feed things through properly. So
306:46 - let's go ahead and try it again. Okay, so
now we got the same warning. And that's not
306:55 - concerning. It's just has to do with the precision.
So that's something you can deal with on your
307:02 - own. If you want, I'm not going to go changing
the backend settings, I could suppress the
307:07 - warning, but it doesn't really bother me that
much. So I'm going to come back in a few minutes
307:11 - and make sure that this is learning. And when
I do, I'll go ahead and show you the output
307:15 - of a fully trained network. And if it doesn't
learn them to go back and debug it, and you'll
307:20 - know because I'll tell you in a few minutes
from your perspective. Okay, so here we are,
307:26 - it's just a few minutes later, by game 60,
we can see that the average score is improving
307:31 - with each episode, meaning that it is in fact
learning and it's getting well over 100. And
307:36 - so I'm gonna switch to the other terminal
where I have finished running it, I'm not
307:40 - gonna wait for it to play all 250 games, because
it does take quite a while once it gets closer
307:44 - to 1000 steps. So hold on one moment. So here
is the output of an earlier model I trained.
307:53 - And just checking the other window to make
sure it is indeed still learning it is it's
307:57 - saving models pretty much every game. So this
is an output of a model I ran earlier, where
308:02 - you can see that for the last several games,
it gets a consistent score with 1000, with
308:07 - a little bit of a low flyer here at 751. Still
a respectable score. It doesn't have 100 games
308:13 - 1000. So the average score hasn't hit 1000.
But it is trending well up on its way. So
308:18 - I consider that to be a fully trained model.
If it ran another 50 games 100 games, then
308:23 - the average score would be 100 new, and we
would have beaten the environment. So that
308:28 - assault after critic in TensorFlow two, I
left out the re parameterization. Because
308:32 - I don't think it's entirely necessary. I don't
know why it was put in the original paper,
308:37 - I have implemented it in pytorch. You can
see that video, if you want to see how that
308:41 - works. It's just you passing in a parameter
to your sample function. And if that parameter
308:47 - is true, then you use the our sample instead
of the sample function from your distribution,
308:52 - if you want to implement that in TensorFlow
to leave that as an exercise to the viewer,
308:55 - but you would have to do some simple calculation
of adding in some spherically sampled noise
308:59 - to it. So that's not you know, what's not
particularly difficult, which is something
309:03 - I didn't want to bother with. So this agent
learn that is sought after critic in TensorFlow
309:08 - to an incredibly powerful algorithm. Indeed,
state of the art I prefer TD three, I think
309:14 - it tends to perform a little bit better, but
this is certainly no slouch in and of itself.
309:18 - I hope that was helpful. Leave a comment a
question? Subscribe, certainly, if you made
309:23 - it this far, and I will see you in the next
video. If you give me about 45 minutes of
309:29 - your time, I will show you how to code a fully
functional asynchronous advantage actor critic
309:33 - agent in the pytorch framework starting from
scratch. We're gonna have about 10 to 15 minutes
309:38 - of lecture followed by an about 30 minute
interactive coding tutorial. Let's get started.
309:49 - Really quick if you're the type A person that
likes to read content, I have an associated
309:53 - blog post where I'm going to go into much
more detail, check the link in the description.
309:57 - Deep reinforcement learning really exploded
in 2015. With the development of the deep
310:01 - q learning algorithm. One of the main innovations
in this algorithm that helped it to achieve
310:06 - such popularity is the use of a replay buffer.
The replay buffer solves a very fundamental
310:11 - problem in deep reinforcement learning. And
that problem is that neural networks tend
310:16 - to produce garbage output when their inputs
are correlated. What could be more correlated
310:20 - than an agent playing a game where each time
step depends on the one taken immediately
310:24 - before it, these correlations cause the agent
to exhibit very strange behaviors, where we'll
310:29 - know how to play the game and suddenly forget,
when an account or some new set of states
310:33 - that have never seen before, the neural network
really isn't able to generalize from previously
310:36 - seen states to unseen states. Due to the complexity
of the parameter space of the underlying problems.
310:42 - The replay buffer fixes this problem by allowing
the agent to randomly sample agents from many
310:47 - many different episodes. This guarantees that
those time steps taken are totally uncorrelated.
310:53 - And so the agent gets a broad sampling of
parameter space and is therefore able to learn
310:57 - a more robust policy with respect to new inputs.
As I've shown before on this channel problems
311:02 - arise when you attempt to simply bolt on a
replay buffer onto the actor critic algorithm,
311:07 - it doesn't really seem to work. And in fact,
it's not very robust. Actor critic methods
311:11 - in particular, suffer from being especially
brittle. And so adding on a replay buffer
311:17 - really doesn't help to address that problem.
In 2016, a group of researchers managed to
311:22 - solve this problem using something called
asynchronous deep reinforcement learning.
311:27 - It's a totally different paradigm for approaching
the deep reinforcement learning problem. And
311:30 - in fact, the technology can be applied to
a wide variety of algorithms, and the original
311:35 - paper they detail solutions for deep q learning.
And step sarsa. Excuse me, instead, q learning
311:41 - as well as sarsa, and actor critic methods
as well. So what is this big innovation? Well,
311:47 - instead of having a replay buffer, we're going
to allow a large number of agents to play
311:51 - independently on totally separate and self
contained environments. Each of these environments
311:56 - will live on a CPU thread, in contrast to
a GPU for most deep learning applications.
312:01 - This has the additional benefit that while
if we don't use a replay buffer, we don't
312:05 - have to store a million transitions, we were
trivial environments really doesn't matter.
312:09 - But if you're dealing with something like
say, the Atari library, a million transitions
312:14 - can take up a significant amount of RAM, which
can be a limiting factor for enthusiast. So
312:18 - having the agent play a bunch of different
games in parallel on separate environments,
312:22 - only keeping track of a small number of transitions,
vastly reduces the memory footprint required
312:28 - for deep reinforcement learning. So in what
sense exactly is this algorithm a synchronous?
312:32 - Well, this means exactly in this context is
that we're going to have a large number of
312:36 - parallel CPU threads with agents playing in
their own environments, they're going to be
312:41 - acting at the same time, but at various times,
they're going to be deciding what to do as
312:45 - well as updating their deep neural network
parameters. And so we're not going to have
312:49 - any one agent sitting around waiting on another
agent to finish playing the game to update
312:53 - its own set of deep neural network parameters.
Each one will be totally independent, and
312:57 - learning on its own. Now, we're not going
to be simply throwing away the learning from
313:01 - each agent average finishes the episode, rather,
we're going to be updating the network parameters
313:07 - of some global optimizer as well as some global
actor, critic agent to have one actor, critic
313:12 - agent that sits atop all the others, and the
local agents that do all the learning by interacting
313:16 - with our environments. So what is the advantage
part of a three scene. So the advantage essentially
313:22 - means what is the relative advantage of one
state over another, it stands to reason that
313:27 - an agent can maximize his total score over
time by seeking out those states which are
313:32 - most advantageous or have the highest expected
future return. The paper gives a relatively
313:37 - straightforward calculation for this, all
we have to do is take the discounted sum of
313:41 - the rewards received over some fixed length
trajectory, and then add on an appropriately
313:46 - discounted value estimate for the final state,
the agent saw in that trajectory. Please note
313:51 - that this could be some fixed number, like
say five steps, or it could be three steps
313:55 - that the agent encountered in Terminal Terminal
state along the way, we're then going to go
313:59 - ahead and subtract off the agents estimate
of the value of whatever current time step
314:03 - it's in, in the trajectory. So that way, we're
always taking the value of the next state
314:08 - minus the current state. That's what gives
us the relative advantage. So what does the
314:12 - actor critic portion of a three c mean? Specifically,
this refers to a class of algorithms that
314:18 - use two separate neural networks to do two
separate things. So the actor network is responsible
314:23 - for telling the agent how to act kind of a
clever name, right? It does is by approximating
314:28 - a mathematical function known as the policy,
the policy is just the probability of selecting
314:32 - any of the available actions for the agent
given it's in some state. And so for discrete
314:37 - action space, it's going to be relative probability
selecting one action over another. So in our
314:42 - car pool, it's going to be say 60%, move left
40% move right, so on and so forth. We're
314:48 - going to facilitate this by having two separate
networks, the actor network will take a state
314:53 - or set of states as input and output a softmax
probability distribution that we're going
314:57 - to be feeding into a categorical distribution
from the pytorch. framework, we can then sample
315:02 - that categorical distribution to get the actual
action for our agent. And we can also use
315:07 - that to calculate the log of the probability
of selecting that action according to the
315:11 - distribution, probability distribution. And
we use that for the update rule for our actor.
315:17 - Now, the critic has a little bit of a different
role, the critic essentially criticizes what
315:22 - the agent the actor did, it said, you know,
that action you took gave us a pretty lousy
315:26 - state that doesn't have a very large expected
future return. And so we shouldn't really
315:31 - try to take that action given that state any
other time that we encounter it. So the critic
315:36 - essentially criticizes what the actor does,
and the to kind of play off of each other
315:40 - to access more and more advantageous states
over time. Before we go ahead and talk about
315:46 - the specifics of each class, let's get some
idea of the general structure and flow of
315:49 - the program. The basic idea is that we're
going to have some global optimizer and global
315:54 - actor critic agent that sits on top that keeps
track of everything the the local agents learn
315:59 - in their own individual threads. Each agent
will get its own specific thread work and
316:04 - interact with its own totally distinct and
separate environment, the agent will play
316:09 - either some fixed number of time steps or
until it encounters a terminal state, at which
316:13 - point it will perform the loss calculation
to do the gradient descent on the global optimizer.
316:19 - Once it calculates those gradients, it's going
to upload it to the global optimizer, and
316:23 - then redownload the parameters from that global
optimizer. Now keep in mind, each agent is
316:28 - going to be doing this asynchronously. So
while one agent is performing its loss calculations,
316:33 - another agent may have already finished that
loss calculation and updated the global optimizer.
316:38 - That's why right after calculating the gradients,
we want to go ahead and download the global
316:42 - parameters from the global actor critic. So
that way, we make sure we are always operating
316:47 - with the most up to date parameters. After
each time the agent performs an update to
316:52 - its deep neural network, we're going to want
to go ahead and zero out its memory so that
316:56 - it can start fresh for another sequence of
five or until it encounters a terminal state
317:00 - number of steps. So now let's talk implementation
details. We're gonna have a few separate distinct
317:05 - classes for this, the first of which is going
to be overriding the atom optimizer from the
317:10 - base pie torch package. So we're gonna have
a shared atom class that derives from the
317:14 - base torch optim, Adam class. And this will
have the simple functionality of telling pytorch
317:21 - me want to share the parameters of a global
optimizer among a pool of threads, it's only
317:26 - going to be a few lines long, and it's much
easier than it sounds, and I'll show you how
317:29 - to do it in code. Our next class will be the
actor critic network. Now, typically, we would
317:35 - use shared input layers between an actor and
critic where we simply have one input layer
317:39 - and two outputs correspond to the probability
distribution pie and the value network meme.
317:45 - But in this case, we're going to host two
totally separate distinct networks within
317:49 - one class. It's a relatively simple problem,
the car pole. And so we're going to be able
317:53 - to get away with this. The reason I'm doing
it this way is because I frankly could not
317:57 - get shared input layers to work with the pytorch
multi processing framework. Our agent will
318:04 - also have a memory which we're just going
to use simple lists. For that, we're going
318:09 - to append states actions and rewards to those
lists, and then go ahead and set those lists
318:14 - back to empty lists. When we need to clear
the agent's memory, we're going to have a
318:19 - function for calculating the returns where
we're going to use the calculation according
318:22 - to the algorithm presented within the paper.
So the idea is that we're going to start at
318:28 - the terminal step, or the final step and the
trajectory. If that step is terminal, the
318:32 - R or the return gets set to zero, if it's
not, it gets set to the current estimate of
318:37 - the value of that particular state, then we're
going to work backward from the T minus one
318:41 - time step all the way to the beginning. And
we're going to update our as r sub i plus
318:47 - gamma times the previous value of r, I'm going
to do a calculation in the video, the coding
318:51 - portion to show you that these two are equivalent,
meaning this calculation as well as the earlier
318:56 - advantage description I gave you, I'm going
to make sure that you understand that those
319:00 - are actually equivalent. And it's just a few
lines of mathematics. So it's not really that
319:04 - difficult. And I've taken the liberty of doing
it for you, then we're going to be calculating
319:07 - the loss functions. And these will be done
according to the loss functions given in the
319:13 - paper. So for our critic, we're going to be
taking the delta between those returns and
319:17 - the values and taking the mean squared error.
For our actor, we're going to be taking the
319:21 - log prop of the policy and multiplying it
by the advantage. And with the negative one
319:25 - factor thrown in there as well. Now that's
a really cool way of calculating the loss
319:29 - for the actor because it has a pretty neat
property. So when we multiply the advantage
319:33 - by the log of the probability, what we're
actually doing is waiting at probabilities
319:38 - according to the advantage they produce. So
actions that produce a high advantage are
319:44 - going to get naturally weighted higher and
higher over time. And so we're going to naturally
319:48 - evolve our policy towards being better Over
time, which is precisely what we want, right?
319:53 - Our final class will be the agent class. And
this will derive from the multi processing
319:57 - process subclass. So here's where all of the
real main type functionality is going to happen.
320:04 - So we're going to be passing in our global
optimizer as well as our global actor, critic
320:08 - agent, instantiating 16, in the case of 16,
threads for a CPU, local critics with 16 separate
320:17 - environments. And then each one of those is
going to have you know, two separate loops,
320:20 - where it's going to go up until the number
of episodes that we dictate, and it's going
320:23 - to play each episode, as I described earlier,
within each episode is going to play some
320:29 - fixed sequence number of steps. And then it
is going to perform some update to the global
320:35 - optimizer and then download the parameters
from the global actor, critic agent. Our main
320:40 - loop is basically going to set everything
up, we're going to go ahead and define all
320:44 - of our parameters, create our global actor,
critic, our global optimizer and tell pytorch
320:49 - if we want to share the memory for our global
actor, critic, agent, and then we're going
320:54 - to make a list of workers or agents. And then
we're going to go ahead and send each of those
320:59 - a start command as well as a join command
so that we can get everything rockin and rollin.
321:03 - So what are some critiques of this algorithm
overall? Well, one is that it is exceptionally
321:08 - brittle. Most actor critic methods require
a fair amount of hyper parameter tuning. And
321:14 - this one is no exception. I tried to use the
lunar lander environment, but couldn't really
321:18 - get a good set of parameters to make it run
effectively and get a you know, a consistent
321:23 - score of 200 or above, or Heck, even a consistent
score of over 100 out of call that good enough
321:28 - for YouTube. Another one is that there is
a significant amount of run to run variation.
321:33 - So it's highly sensitive to initial parameters.
You can solve this by setting global seeds
321:38 - for the random number generators so that you're
getting consistent random numbers over time,
321:43 - and so you're going to know exactly how you're
starting. But to me, it's a little bit kind
321:48 - of like cheating. So I don't do it in this
video, but it is something to take note of.
321:52 - And in the original paper, I think they do
something like 50 different runs of each evaluation
321:57 - some large number to get a pretty tight or
to get a pretty solid distribution of scores.
322:01 - And that is, I think, because of the high
degree of run to run variation. Okay, I have
322:07 - lectured at you enough. Again, if you'd like
to read written content, I have a link in
322:11 - the description to a blog post, where I talk
about this in a little bit more detail. But
322:15 - nonetheless, let's go ahead and jump right
into the coding tutorial. Let's go ahead and
322:26 - start with our inputs to the gym for our environment.
They are based towards package we'll need
322:35 - torture, multi processing, to handle all the
multi processing type stuff, we will need
322:40 - torch penon to handle our layers 
will need nn functional to handle our activation
322:51 - functions. And we're going to need our distribution
as well. And in this case, we're going to
323:00 - need a categorical distribution. All this
does is takes a probability output from a
323:08 - deep neural network maps into a distribution
so that you can do some actual sampling to
323:12 - get the real actions for your agent. Now I
want to start with a shared Adam class. This
323:17 - will handle the fact that we are going to
be sharing a single optimizer among all of
323:21 - our different agents that interact with separate
environments. All we're going to do here is
323:26 - called the base Adam initializer. And then
iterate over the parameters in our parameter
323:32 - groups, setting the steps exponential average
and exponential average squared to zeros effectively
323:38 - and then telling it to share those parameters
amongst the different pools in our multi threading
323:44 - pool. And this will derive from the base atom
class. Our default values are going to be
324:00 - I believe, identical to the defaults for the
atom class. And then we want to call our super
324:14 - constructor. Now we're going to 
handle setting our initial values 
325:10 - And then we're going to tell torchiere, we
want to share the memory for our parameters
325:15 - are for our gradient descent. And note the
presence of the underscore at the end of memory
325:26 - there. Okay, that is it for the shared atom,
pretty straightforward. Next up, we want to
325:38 - handle the actor critic network, which will
also encapsulate a lot of the functionality
325:43 - I would typically put into an agent class
because of my understanding of the design
325:48 - principles of object oriented software programming.
In this case, I do shimmy a few things around,
325:54 - because the agent class is going to handle
the multi processing elements of our problem.
326:00 - And so it doesn't really make sense to stick
this, like the Choose action, or memory, or
326:06 - memory functionality in the agent class. So
we're gonna stick it in the network class,
326:09 - it's not a huge deal. It's just a departure
from how I normally do things. And certainly
326:14 - not everybody does things the same way I do.
So our initializer takes input dims from our
326:26 - environment, number of actions from our agent,
and a default value for gamma of 0.99. We
326:42 - also have to save our gamma. And the next
thing we want to handle is writing our actual
326:48 - deep neural network. Now, this is also a little
bit different than the way I normally do things.
326:53 - Normally, I would have a shared input layer
that branches out into a policy and evaluate
326:58 - network as two separate outputs with that
shared input layer. When I tried to do that,
327:02 - I found that the software doesn't actually
run, it doesn't handle the threading aspect
327:07 - very well. In that case, when you have shared
input layers from a deep neural network, I
327:12 - don't know exactly why that is, if you know,
please leave a comment down below, because
327:15 - I'd be very curious to hear the explanation,
and simply what I found out through my own
327:18 - experimentation, so we're gonna have two separate
inputs, one for the policy and one for the
327:23 - value network, as well as two separate outputs.
So they're effectively two distinct networks
327:27 - within one single class. We're only going
to be using 128 neurons here and not a very
327:38 - large network. And our output will take those
128 hidden neurons and converted into number
327:53 - of actions. And our value function will take
likewise, 120 in hidden layers, hidden elements
328:01 - and convert it to a single value. Or if you
pass in a batch of states a batch of values.
328:06 - The agent also, excuse me, the network also
has some basic memory, so rewards, actions,
328:16 - and states. These we will handle just by appending
stuff to a list and then and each time we
328:22 - call the learning function, we're going to
want to reset that memory. So let's go ahead
328:26 - and handle that functionality first. So the
remember just appends a state action reward
328:34 - to the relevant list. And a clear memory function
just zeros out all those lists. Pretty straightforward.
329:03 - Next, we have our feed forward function that
takes a state as input. So we're going to
329:14 - pass that state through our first input layer
for our policy and perform a value activation
329:23 - on that and do something similar for the value
input layer. And the outputs of those two
329:32 - are going to be passed to the roles to the
relevant policy and value outputs. And then
329:44 - we just returned pi and V. Pretty straightforward
yet again. Next, we're going to have our function
329:48 - to calculate the returns from are a sequence
of steps. So this will only take a single
329:55 - input, and that will be the terminal flag.
Recall that the return for the terminal step
330:01 - is identically zero. So we need the terminal
flag or the done flag to accommodate that.
330:06 - So we want to go ahead and convert the states
from our memory to a torch tensor of T dot
330:15 - float data type, because it is a little particular
about the data type, you don't want to pass
330:19 - it in double it gives you an error. So best
to take care of it. Now, we're going to go
330:25 - ahead and pass that through our neural network.
And we're not going to be concerned with the
330:31 - policy output at this stage, we just want
to know what the value evaluations The Critic
330:36 - has for that set of states. So our return
is going to be is going to start out as the
330:44 - last element of that list, so the terminal
step, or the last step in the sequence of
330:49 - steps. And we're going to multiply that by
one minus done so that if the episode is over,
330:56 - one minus done is zero, so you're multiplying
by zero, you get zero. Pretty handy way of
331:01 - handling that, then we're going to handle
the calculation of the returns at all the
331:06 - other time steps. So we're going to go ahead
and iterate over the reversed memory. And
331:15 - say that our return is the reward at that
time step plus gamma times are and then just
331:22 - return actually append that return to the
list of batch returns. And then finally, at
331:29 - the end, you want to go ahead and reverse
that list again. So that's in the same order
331:35 - in which you encounter the states. This calculation
reverses it up as you're starting at the end,
331:40 - when you know the value of the final state,
or at least the estimate of the value according
331:44 - to the critic, and then reversing it to get
it back in order for passing it into our loss
331:52 - calculation function. Now, this may be a strange
form to you. If you write it out by hand,
331:58 - maybe I can show you something here where
I did it for you. If you write it out by hand,
332:02 - this particular chunk of code, you can see
that it's identical to what they tell you
332:05 - the calculation is in the paper. So you can
do that exercise on your own to convince yourself
332:10 - or I can just show it to you so that I can
convince you of it. But this is indeed, the
332:16 - return calculation from the paper, everything
is as it should be. And then I want to convert
332:22 - that to a tensor and return. Next, we have
to handle the calculation of our loss function.
332:36 - And this, again, is only a single input a
terminal flag from the environment, we're
332:44 - going to go ahead and get the value, excuse
me the tensor representations of our state's
332:51 - actions right at the beginning. And then we're
going to go ahead and calculate our returns.
333:09 - And then we're going to perform the update.
So we're going to be passing the states through
333:14 - our actor critic network to get the new values
as well as then a distribution according to
333:20 - the current values of our deep neural network,
we're going to use that distribution to get
333:24 - the log problems of the actions the agent
actually took at the time it took them. And
333:29 - then we're going to use those quantities for
our loss functions via squeeze. Now, this
333:43 - squeeze is very important. If you don't squeeze
here, it won't trigger an error, but it will
333:49 - give you the wrong answer. The reason it will
do that is because the actor loss and the
333:58 - critic loss, I believe, will come out as a
shape of five by five. And that is not the
334:04 - shape we want, we want something in the case
of five time steps, team x equals five, so
334:09 - it'll give you a five by five matrix instead
of a five element vector. So you have to perform
334:14 - the squeeze here to get the five by one output
of the depot network into some advantages
334:20 - five, a list of five elements or vector of
five elements instead of five by one. So definitely
334:25 - that squeeze. If you don't believe me, by
all means, raised a line or commented out
334:29 - and print out the shapes of things to the
terminal. I always recommend doing that. It's
334:34 - a good way of solidifying your understanding
of how all this stuff works. So then our credit
334:41 - loss. It's just the returns minus values,
squared, pretty straightforward. So now let's
334:49 - go ahead and say We want the softmax activation
of our output. And that has a property that,
335:01 - of course, the softmax guarantees that every
action has a finite value. And that's the
335:07 - by the probabilities add up to one as all
probability distributions, distributions should.
335:14 - So then we use that output to create a categorical
distribution, and calculate the log probability
335:24 - distribution of our actions actually taken
than our actual loss, minus log probs times
335:35 - the quantity turns minus values that's from
the paper, and then our total loss is just
335:43 - predict loss after loss. That mean, we have
to sum the two together, because of the way
335:53 - that the backpropagation is handled by pytorch.
And, of course, I did forget to choose action
336:00 - function. But that is not a big deal, we'll
just go ahead and handle that now. So that
336:07 - will take, we're going to call it observation
as input, because we're gonna be passing on
336:12 - the raw observation from our environment.
So we have to convert that to a tensor right
336:17 - off the bat. And we have to add a batch dimension
to that, for compatibility with the inputs
336:26 - of our deep neural net. And we're gonna call
it a D type of float, we pass that through
336:33 - our neural network, get our policy and value
function out, perform a softmax activation
336:44 - on our policy along the first dimension, then
we're going to create our distribution based
336:52 - on those probabilities and sample it and convert
it to a NumPy quantity. Take the zeroeth element
337:04 - and return it. So that is it for our actor
network class. It encapsulates most of the
337:10 - functionality I would associate with the agent,
but it does everything we're going to need
337:13 - each of the independent actors within their
own thread to do now we're going to move on
337:17 - to the agent classes going to handle our multi
processing functionality. So I'm going to
337:23 - call this agent, you will sometimes see it
referred to as worker and that is that fine
337:28 - name, that is kind of precisely what it is.
I'm just using agent to be consistent with
337:33 - my previous nomenclature, it acid derived
from the NP dot process class. So we get some
337:39 - access to some goodies there. So we will need
our global actor predict that is what is going
337:47 - to handle the functionality of keeping track
of all the learning from all of our environment
337:53 - specific agents. The optimizer that is going
to be the shared atom optimizer that we wrote
338:00 - earlier. Input damns number of actions, gamma
in case you want to use something other than
338:09 - 0.99 a learning rate, a name to keep track
of each of the workers from our multi processing,
338:17 - global episode, index. So this will keep track
of the total number of episodes run by all
338:24 - of our agents. It's not as easy as our content
is it may seem because you're doing asynchronous
338:31 - processing, as well as an environment ID.
So the first thing we want to do is call our
338:36 - super constructor. And go ahead and start
saving stuff. So our local actor critic, is
338:47 - just going to be our new actor critic, with
inputs of input demos, number of actions and
338:57 - gamma. You want to save our global actor critic.
So that we can update its parameters. We're
339:07 - going to have a name for each worker and its
own independent thread. And that's just going
339:12 - to be just work or a number name. And then
we'll have an episode index. Well, welcome
339:25 - so that it backs our environment. So environment
it is a string here if that isn't clear. And
339:38 - our optimizer, I spell that correctly. Yes,
I believe I did. So we have to define a very
339:44 - specific function. So that stuff actually
works and that function is the run function.
339:48 - This gets called behind the scenes. By the
worker dot start function that we're going
339:53 - to handle in the main loop. But this handles
effectively all the main loop type functionality
340:01 - of our problems. So our global time step is
going to get set to one, while our episode
340:11 - ID x dot value, so episode ID x is a global
parameter from the multiprocessing class,
340:21 - and that we want to get the value by using
the dot value dereference. And while that's
340:26 - less than number of games, some some global
variable we're going to define. In other words,
340:34 - what while we have not completed all of our
games, set your terminal flag to false, reset
340:40 - your environment and set the score to zero.
And go ahead and clear the agent's memory
340:49 - at the top of every episode. And then while
you're not done, so go ahead and play your
340:59 - sequence of episodes. I'm sorry, a sequence
of steps within an episode. Action is chosen
341:05 - by a local actor, critic to pass the observation
into each local actor critic. So each of the
341:16 - 16 in this case, threads will get its own
local actor critic and that'll synchronize
341:21 - took a global but you never actually use the
global network directly to do things like
341:27 - choosing actions. So get the new state reward
done and a bug info back from your vironment.
341:39 - Keep track, other award received as part of
the total score. Remember that observation,
341:53 - action reward, then we have to say, if the
number of steps modulus, the maximum number
342:04 - of steps is zero, in other words, if it is
every if it is every fifth time step, or we
342:10 - have finished the episode with that last time
step, then we're going to go ahead and perform
342:14 - our learning step. Last T, Max, or we're done,
then we're going to go ahead and handle the
342:25 - learning functionality. So we'll say loss
equals local actor predict loss. Of course,
342:33 - we need the most recent terminal flag as the
parameter for that function, the argument,
342:43 - go ahead and zero your gradient and back propagate
your loss. So we're going to set the parameter
343:16 - for the global gradient to the local agents
gradient at this step. And then, after doing
343:25 - that, we can tell our optimizer to step and
then we can go ahead and synchronize our networks.
343:35 - And we do that by loading the state dictionary
from our global actor critic to our local
343:49 - one. And then we want to go ahead and clear
the memory for each learning step, and then
344:00 - tell the algorithm to increment t step by
one the global time step and set the current
344:08 - state to the new state. Then at the end of
every episode, we have to handle the the fact
344:15 - that we may have finished an episode from
another agent while one thread was running,
344:20 - because this is running a synchronous asynchronously.
So we say with self dot global sorry, self
344:28 - dot episode ID x get locked. So we want to
make sure that no other thread is trying to
344:35 - access that variable right now. And if we
can go ahead and increment that episode value
344:43 - by one. Then we do the usual print the debug
stuff to the terminal. We're going to print
344:49 - out the name of The worker, the episode that
is self episode, Id x and the reward the total
345:08 - score from the episode, Matt, is it for our
agent class. So this is relatively straightforward.
345:16 - All we're doing is handling the fact that
we have our local parameter, our local agent
345:22 - that uses actions, we're going to perform
the gradient descent optimizer update using
345:28 - the gradients calculated by the actions of
that agent. And then upload that to our global
345:33 - network. So that every every agent uploads
this learnings to the global network, and
345:38 - then we're going to go ahead and download
that from our global network as well so that
345:42 - we make each agent in its own thread better
over time. Now we have our main loop. And
345:51 - we're going to do some stuff like declaring
our learning rate, or environment ID. Card
346:01 - poll, the zero number of actions for that
environment is just to left and right, input
346:07 - them. So it's just a four vector, we'll say
number of games 5000. You know, we can actually
346:16 - get away with 3000, I believe, team Max, every
five steps that comes from the paper, a global
346:25 - actor critic, gets initialized here. And that
takes input demos and number of actions. Want
346:35 - to tell that global actor critic object that
we want to share its memory? And we have our
346:42 - optimizer? What are we going to be optimizing
and sharing the parameters of a global actor
346:51 - critic network learning rate defined by learning
rate and betas? 0.920999 I get that from more
347:02 - banjos stuff, I haven't experimented with
that too much. A little bit of cargo cargo
347:07 - called programming here, I apologize for that.
Now we have our episode nine is just going
347:14 - to be a value of Type II, that means unsigned
integer, doesn't mean I as an account or variable.
347:20 - You could also have D for a double, if you
wanted to keep track of like a score or something
347:25 - like that. I just mean unsigned integer, or
no, it's a it's a signed integer can be negative
347:32 - as well as positive. Now we have to create
a list of workers. And that's going to be
347:37 - a list of our agents. And this is what's this
is the construct that we're going to use to
347:42 - handle the starting and running of our specific
threads. So that's an agent, we have to pass
347:52 - on our global actor, critic, or optimizer,
our input number of actions, a gamma learning
348:02 - rate, it's helpful to use commas, our name
is just going to be the the name is just going
348:12 - to be the AI in our list comprehension. Our
global episode ID x is going to be that global
348:23 - variable we just defined. And we're going
to pass in our environment ID for i in range,
348:31 - multi processing dot CSV count. Now, I haven't
noticed a huge difference. And running this
348:40 - with different numbers. So something other
than CPU count and the range, I have 16 threads,
348:45 - if I tell it eight, it still uses all 16.
So I'm probably missing something here, I
348:51 - could do a deeper dive into the documentation
to figure it out. But for the purposes of
348:55 - the video, I'm just going to go ahead and
roll with it. So now we want to go ahead and
349:01 - start all of our workers and then go ahead
and do our join operation. Okay, so now moment
349:16 - of truth, I want to find out how many typos
I made in the recording of this. Let's go
349:21 - ahead and do a right quit. I already have
an invalid syntax. So I say for reward in
349:29 - that's an easy one. No others. So let's say
a through c.pi. Okay, name, envy is not defined
349:40 - that's because it is self thought envy. So
that is in line 114 self thought can be reset
349:51 - cannot assign torch flow tensor as child module
pi one torch and in module or non expected
350:01 - so that is when I handle the feed forward
in our choose action function who this is
350:10 - an interesting one. I haven't encountered
this before let's take a look cannot assign
350:15 - as child module pi one. Oh, it's because interesting.
Oh, of course. Yeah. Obviously, I don't want
350:37 - that self here. odd to annotate that one I
am editing the video, that is obviously going
350:45 - to be problematic. So you'll probably have
seen that already that I put in a no self.in.
350:50 - There. Okay. As the beauty of doing things
for an O scoters. Log problem, that log probs
351:01 - that's in 85. Problem. Oh, by Remember, all
these non type object has no attribute backward,
351:16 - I probably forgot to return my loss. Yeah,
that's right. There we go. All right, now
351:30 - it's running. Hopefully, it's still gonna
record, even though it is running across all
351:37 - of our threads. Looks like we're still good.
Now you see that executed really, really fast.
351:42 - It was so fast, I didn't even have time to
get up the thread count the Resource Monitor
351:49 - to show you guys. But you can see here that
it achieves a non negligible scores that indicates
351:57 - some learning is going on. Now. Let's go ahead
and run it again. And you can see now this
352:05 - time, it's not doing any learning at all.
So there is a significant amount of runner
352:08 - run variation. So I'll let it run and finish.
And we'll try one more time. And maybe we'll
352:15 - get lucky. And it will actually approach a
score of 200. Now, that's one of my big criticisms
352:26 - of this algorithm is that there is a significant
amount of run to run variation. And it is
352:31 - pretty brittle with respect to some other
algorithms that came later. Basically, the
352:35 - main advantage here is that it's a synchronous.
And this paradigm can be employed on many
352:41 - different types of algorithms. As you read
in the paper, you know, it can be applied
352:44 - to deep learning, as well as and step learn
stuff like that. Okay, so it's getting a little
352:55 - closer. Okay, so this one took a little bit
longer, because the episodes actually achieve
353:22 - scores of 200, which is the max, not every
thread manages to be a winner, not all of
353:28 - us can be winners in life. That's a nice little
microcosm of the universe. But you can see
353:33 - that for the most part, some of the workers
actually one throws consistency amongst so
353:39 - worker 11, not 200. And if I scan down here,
another worker 11 140 to 161. Interesting,
353:52 - so you do get actual learning across your
agents. It's just not super consistent. And
353:58 - that is my biggest criticism of this algorithm
is that it has a high degree of render and
354:04 - variation, as you can see, but it's still
pretty good. It's an interesting take on deep
354:10 - reinforcement learning. And I do enjoy the
algorithm. It's just not what I would categorize
354:16 - as the top two or three algorithm and actor
critic methods. I hope this was illustrative
354:20 - for you. I hope you found it very helpful.
If you have made it this far, please consider
354:24 - subscribing, leave a like a comment down below
and I'll see you in the next video.

Cleaned transcript:

Welcome to an introduction to advanced actor critic methods. I am your instructor, Dr. Phil Taber. I'm a physicist and former semiconductor engineer turn data scientist. In this course, you're gonna learn the fundamentals of advanced actor critic methods. Now, if you've seen some of my prior work here on the Free Code Camp, then you may have seen some work related to deep learning as well as actor critic methods, there will be a little bit of overlap between the other actor critic courses and this material, simply because I can't assume everyone has seen my earlier content, no need to go back and re watch that though. Although you are free to do so if you wish, I will include enough information in this particular course, for the motivated beginner to get started in the field of deep reinforcement learning and actor critic methods in particular. Now, why are actor critic methods important? That's a great question I'm glad you asked. The basic idea is that things like cue learning are great for learning problems with a discrete action sets like playing video games where you can move left or right or shoot your phaser blast or the invading aliens. But it falls down when attempting to handle things like continuous actions. So this is important in fields like robotics, where you are applying continuous voltages to motors enjoins to actuate movement. And so we do technology to handle robotic movement above and beyond deep learning. Now, far from being a theoretical exercise in March of 2021. This year, group at Berkeley did, in fact, use deep reinforcement learning to get to bipedal movement in a robot named Cassie, and I'm going to detail that on my channel. By the time you see this, it may already be out. So go ahead, check me out at machine learning with Phil, I'll leave a link in the description, you can go subscribe if you're interested in more deep reinforcement learning content, but enough of the shameless plugging. So actor critic methods are necessary for dealing with continuous action spaces. And they work by approximating something called a policy. A policy is a mathematical function that takes a state of the environment as input, and outputs some action. Now, in the case of a robot, that action could be a just an actual voltage that we apply it to our motors, or it could be a probability distribution that we sample to generate some action. So for example, Gaussian distribution, you know, a normal bell curve that you sample to get some value for your action. We're going to cover both cases. In this course, we're going to cover a whole host of algorithms, starting with the vanilla actor, critic method, deep deterministic policy gradients, twin delay deep deterministic policy gradients, proximal policy optimization, soft actor, critic, as well as a synchronous advantage, actor critic, try saying that five times fast, as far as software requirements go, they are relatively light. So we will need NumPy pie torch and Matt plot line, I highly recommend using the versions that I will leave for you in the description, because NumPy in particular, likes to deprecate stuff. And that tends to break my code. So if you use the versions that I've linked in the description, it's almost guaranteed to work provided we didn't make some mistakes along the way. As far as hardware, you're going to need a GPU unfortunately, and 2021 the GPU market is totally broken. So hopefully you already have one on hand. If not, don't despair. In particular, the a three c algorithm, a synchronous advantage actor critic is designed to run a multi core CPUs. So at the very least, you'll be able to run that and get really good results. Other algorithms, you may be able to get something that converges all of the timeline, the amount of time you're gonna have to train is going to be a little bit longer. So you may want to leave things running overnight. With all of that said, I will check in periodically for questions. Obviously, I don't get notifications from the Free Code Camp channel, but I'll do my best to patrol the comments to see should there be any confusions or need for clarification I can swoop in to render assistance. Once again, if you like this type of content, check me out at my YouTube channel machine learning with Phil where I go over all things, deep reinforcement learning, and occasionally natural language processing as well. Let's go ahead and get started. And I look forward to seeing you on the inside. Welcome back everybody. In today's tutorial, you are going to get a mini crash course in actor critic methods in TensorFlow two, we're gonna have around 15 minutes of lecture followed by about 20 minutes of coding, and you're gonna learn everything you need to know to go from start to finish with actor critic methods. If you'd like to know more, you can always pick up one of my Udemy courses on sale right now link in the description. Let's get started. Welcome to the crash course in actor critic methods, I'm going to give a relatively quick overview of the fundamentals of reinforcement learning in general. And then of actor critic methods in particular, finally, work together to code up our own actor, critic agent and TensorFlow two. This is geared toward beginners, so feel free to ask some questions in the comment section. Reinforcement Learning deals with agents acting on an environment, causing some change in that environment and receiving a reward in the process. That All of our agent is to maximize his total reward over time, even if it starts out knowing literally nothing about its environment. Fortunately, we have some really useful mathematics at our disposal, which makes figuring out how to beat the environment it difficult, yet solvable problem. The mathematics we're going to use relies on a very simple property of the system, the Markov property. When a system depends only on its previous state, and the last action of the agent, we say it is markovian. As we said earlier, the agent is given some reward for its action. So the set of states the agencies, the actions it takes, and the rewards it receives forms our Markov decision process. Let's take a look at each of these components. In turn, the states are just some convenient representation for the environment. So if we're talking about an agent trying to navigate a maze, the state is just the position of the agent within that maze. The state can be more abstract, like in the case of the lunar lander, where the state has an array of continuous numbers that describe the position of the lander, the velocity of the lander, its angle and angular velocity, as well as which legs are in contact with the ground. The main idea is that the state describes exactly what about the environment is changing. And each time step. The rules that govern how the states change are called the dynamics of the environment, the actions are a little simpler to understand. In the case of a maze running robot, the actions would be just move up, down, left and right. Pretty straightforward. In the case of a lunar lander, the action is consist of doing nothing, firing the main engine, firing the left engine and firing the right engine. In both these cases, the actions are discrete, meaning they're either one or the other. You can't simultaneously not fire the engine and fire the right thruster. For instance, this doesn't have to be the case, though. actions can in fact, be continuous. And there are numerous videos on this channel dealing with continuous action spaces. Check out my videos on soft actor critic deep deterministic policy gradients, and twin delayed deep deterministic policy gradients. From the agents perspective, it seeing some set of states and trying to decide what to do. How is our agent to decide? The answer is something called the agents policy, a policy of the mathematical function that takes states as inputs and returns probabilities as output. In particular, the policy assigns some probability to each action in the action space for each state. It can be deterministic, meaning the probability of selecting one of the actions is one and the others is zero. But in general, the probabilities will be somewhere between zero and one. The policy is typically denoted by the Greek letter pi. And learning to be the environment is then a matter of finding the policy pi that maximizes the total return over time by increasing the chances of selecting the best actions and reducing the chances of selecting the wrong ones. The reward tells the agent exactly what is expected of it. These rewards can be either positive or negative. And the design of rewards is actually a tricky issue. Let's take the maze running robot. If we give it a positive reward for exiting the maze, and no other reward, what is the consequence of that? The consequence is that the agent has no motivation to solve the maze quickly, it gets the same reward if it takes a minimum number of steps. Or if it takes 100 times at number, we typically want to solve the maze as quickly as possible. So the simple reward scheme fails. In this example, we have to give a penalty or a reward of minus one for each step and a reward of zero for exiting the maze, then the agent has a strong motivation to solve the maze in as few steps as possible. This is because the agent be trying to maximize the negative reward, meaning get it as close to zero as possible. So now that we have our basic definitions out of the way, we can start to think through the mathematics of the reinforcement learning problem. From the agents perspective, it has no idea how its actions affect the environment. So we have to use a probability distribution to describe the dynamics is probability distribution is denoted p of s prime and are given s and a was just raised as the probability of ending up in state S prime and receiving a reward are given we're in state s and take action a. In general, we won't know the value for this until we interact with the environment. And that's really part of solving reinforcement learning. Since we're dealing with probabilities, we have to start thinking in terms of expectation values. In general, the expectation value is calculated by taking into account all possible outcomes and multiplying the probability of that outcome by what you receive in that outcome. So in our Markov framework, the expected reward for a state and action pair is given by the expectation value of that reward, which is the sum over all possible rewards outcomes also by by the sum over the probabilities of ending up in all possible resulting states. For a simple example, let's consider a simple coin toss game. If we flip a coin and it comes up heads, you get one point. If it comes up tails, you get minus one point. If we flip the coin two times, what is the expected number of points? It's probably of getting heads multiplied by the reward for getting heads, plus a probability of getting tails multiplied by the reward for getting tails. So 0.5 times one plus 0.5 times negative one. This gives an expected reward of zero points, which is what you would intuitively expect. This is a trivial example, but I hope it illustrates the point, we have to consider all possible outcomes, their probabilities and what we would expect to receive in each of those outcomes. When we go to put theory and practice, we're going to be doing it in systems that are what we call episodic. This means that the agent has some starting state, and there is some terminal state that causes the gameplay to end. Of course, we can start over again with a new episode. But the agent takes no actions in this terminal step and thus no future rewards follow. In this case, we're dealing with not just individual rewards. But with the sequence of rewards over the course of that episode. We call the cumulative reward the return and it's usually denoted by the letter G. Now this discussion is for games that are broken into episodes. But it would be nice if we could use the same mathematics for tasks that don't have a natural end, we have a game that goes on and on, and the total sum of rewards will approach infinity. It's absurd to talk about maximizing total rewards an environment where you can expect an infinite reward. So we have to do a little modification to our expression for the returns, we need to introduce the concept of discounting, we're going to reduce the contribution of each reward to the sum based on how far away in time it is, from our current time step, we're going to use a power law to describe this reduction so that the reward gets reduced by some additional power of a new hyper parameter we'll denote as gamma, gamma is between zero and one. So each time we increase the power, the contribution is reduced. If we introduce gamma into the expression for the return at time step t, we get to the return is just a sum over k of gamma to the K and multiplied by the rewards at time t plus one plus K. Besides being a trick to make sure we can use the same mathematics for episodic and continuing tasks is counting as a reasonable basis and first principles. Since our state transitions are defined in terms of unknown probabilities, we can't really say how certain each of those rewards were states that we encounter further out in time become less and less certain. And so the rewards for reaching those states are also less and less certain. And less, we shouldn't wait them as much as the reward we just received. If you've been following along carefully, something may not quite add up here. All this math is for systems with a Markov property, which means that they depend only on the previous state and action. So why do we want to keep track of the entire history of rewards received? Well, it turns out that we don't have to, if you do some factoring in the expression with a return a time step t, you find that the return at time t is just the sum of the reward at time t plus one and the discounted return for the T plus one time step. This is a recursive relationship between returns at each time step. It's more consistent with the principles of the Markov decision process, where we're just concerned with successive time steps. Now that we know exactly what the agent wants to maximize the total returns, and then function for how it's going to act, the policy, we can actually start to make useful mathematical predictions. One quantity a particular interest is called the value function depends on the agents policy pi and the current state of the environment, and gives us the expectation value of the agents returns starting from time t and state s, assuming it follows the policy pi as a comparable function for the value of state and action pairs, which tells us the value of taking action a in state s and then following the policy pi afterwards, it's called the action value function and is represented by the letter Q. So how are these values calculated in practice? Well, in reality, we don't solve these equations, we estimate them, we can use neural networks to approximate the value or action value function. Because neural networks are universal function. approximator is the sample rewards from the environment and use us to update the weights of our network to improve our estimate for the value or action value function. estimating the value function is important because it tells us the value of the current state and the value of any other state the agent may encounter. Solving the reinforcement learning problem then becomes an issue of constructing a policy that allows the agent to seek out the most profitable states. The policy that yields the best value function for all states in the state space is called the optimal policy. In reality, it can be a set of policies, and they're all effectively equivalent. various schemes to find these optimal policies exist, and one such scheme is called the actor critic method. In actor critic methods, we're using two deep neural networks. One of them is used to approximate the agents policy directly, which we can do because it's just a mathematical function. We call that the policy is just a probability distribution over the set of actions where we take a state as input and output a probability of selecting each action. The other network called the critic is used to approximate the value function. The Critic acts just like any other critic telling the actor how good each action is based on whether or not the resulting state is valuable. The two networks work together to find out how best to act in the environment. The actor slugs actions, the critic evaluates the states and then the result as compared to the rewards from the environment. Over time, the critic becomes more accurate in estimating the advisor states, which allows the accurate slide the actions that lead to those states, from a practical perspective, are going to be updating the weights of our deep neural network at each time step. Because actor critic methods belong to a class of algorithms called temporal difference learning. This is just a fancy way of saying that we're going to be estimating the difference in values of successive states, meaning states that are one time step apart, hence temporal difference. Just like with any deep learning problem, we're going to be calculating cost functions. In particular, we're going to have two cost functions, one for updating our critic and the other for updating our actor. To calculate our costs, we want to generate a quantity we'll call Delta. And it's just given by the sum of the current reward and the discounted estimate of the new state and then subtracting off the value of the current state. Keep in mind that the value of the terminal state is identically zero. So we need a way to take this into account. The cost for the critic is going to be delta squared, just kind of like a typical linear regression problem. The cost for our actor is a little more complex, we're going to multiply the delta by the log of the policy for the current state and action the agent took. The reason behind this is a little complex, and it's something I go into more detail about in my course, where you can look for it in the chapter on policy gradient methods in the free textbook by Sutton and Barto. So let's talk implementation details, we're going to implement the following algorithm. initialize a deep neural network to model the actor and critic. Repeat for a large number of episodes, we set the score on terminal flagging environment, all the state is not terminal, select an action based on the current state of the environment. Take the action and receive the new state reward and terminal flag from the environment. calculate delta and use it to update the actor and critic networks. Set the current state to the new state and increment the episode reward by the score. After all, the episodes are finished, plot the trend and scores to look for evidence of learning, there should be an overall increase in score over time, you will see lots of oscillations because actor critic methods aren't really stable, but the overall trend should be upward. Another thing you may see is that the score can go upward for a while and then fall off a cliff. This isn't uncommon, because actor critic methods are quite brittle. And they're really not the best solution for all cases, but they are a stepping stone to more advanced algorithms. Other important implementation details, you can use a single network for both the actor and critic. So you have common input layers and two outputs, one for the actor and one for the critic. This has the benefit that we don't have to train two different networks to understand the environment, you can definitely use an independent actor and critic, it just makes the learning more difficult for an algorithm and is already pretty finicky. What to play almost 2000 games with a relatively large deep neural network, something like about 1000 units in the first hidden layer and 500 units in the second. The hard part is going to be the actor. As I said earlier, the actor models the policy, which is a probability distribution, the actor layer will have as many outputs as our actions. And we use a softmax activation because we're modeling probabilities, and they'd better sum to one. When selecting actions, we're going to be dealing with a discrete action spaces. So this is what is called a categorical distribution. We're going to want to use the TensorFlow underscore probability package for the categorical distribution, and then use the probabilities generated by the actor layer to get this distribution, which we can then sample and use the built in log prop function for our cost function. As far as the structure of our code, we're going to have a class for our actor critic network. And that will live in its own file. We'll also have a class for our agent and they'll have that functionality, choose actions save models and learn from its experience, Matt goes in a separate file. The main loop is pretty straightforward, but it does go in its own file as well. Okay, now that we have all the details out of the way, let's go ahead and get started coding this. So now that we have all of our lectures out of the way, we're going to go ahead and proceed with the coding. We're going to start with the network's begin as always with our imports. So we will need OS to handle file joining operations for model checkpointing. We will need carols and we will need our layers which for this example is just going to be a dense layer. So we will have our actor critic network and you See a case of converging engineering here where TensorFlow and pytorch both have you derive your model class from the base model class. And we can go ahead and define our constructor. That will take a number of actions as input the number of dimensions for the first fully connected layer, we will default that to 1024. And for the second, we will default it to 512. We will have a name for model checkpointing purposes, and a checkpoint directory. Very important, you must remember to do a make directory on this temp slash actor critic before you attempt to save a model. Otherwise, you're going to get an error. The first thing you want to do is call your super constructor. And then go ahead and start saving your parameters. Now, also very important for our class that we've derived from the base class. In this case, the actor critic network class, we have to use model name instead of name because name is reserved by the base class. So just be aware of that not a huge deal checkpoint directory. And then we'll have our file and that will be OS path join the directory name plus underscore Pacey. I like to use underscore algorithm, in this case, AC for actor critic, in case you have one directory that use for many different algorithms, if you're just using like, say a working directory, you don't want to confuse the model types. Otherwise, if you have a good model saved, you don't want to override it with something else. Now we'll go ahead and define our layers. And that will be fully connected dense layers. The neat thing about Kairos is that the number of input dimensions are inferred so we don't have to specify it. That's what we don't have an input dims for our constructor, and it will output FC one dims with an activation of rally, FC two will be similar. And then we will have two separate outputs. So we have two common layers and then two independent outputs, one for the value function. And that is single valued with no activation. And the second is our policy pi. And that will output an actions with a softmax activation. Recall that the policy is just a probability distribution. So it assigns a probability to each action. And those probabilities have to add up to one because that's kind of what probabilities do, right? Next, we have to define our call function. This is really the feed forward. If you're familiar with that from pytorch. So we'll just use some generic name like value doesn't really matter, and pass through the second fully connected layer. And then get our value function and our policy pi and then return both value function and the policy pi. So that is really it. For the actor critic network. All of the interesting functionality happens in the agent class. So let's go ahead and start writing the agent class. So we'll begin as always with our imports. We will need TensorFlow, we will need our optimizers. In this case, we're going to use an atom optimizer. Probability it t we will need TensorFlow probability to handle our categorical distribution to model our policy directly. You have to do a pip install TensorFlow probability before you can run this. This is a separate package from TensorFlow. And we also need our actor critic network. Let's go ahead and code up our agent. So our initializer is pretty straightforward. We will need some default learning rate news 0003 It doesn't really matter I'm going to pass in a specific learning rate in the main file. We will have a gamma of 0.99 and Default in actions have some number like, say two. So we're going to go ahead and save our parameters, we'll call the gam is our discount factor, we're gonna need a variable to keep track of the last action we took. This will be a little bit more clear when we get to the Learn function, and has to do with the way we calculate the loss because we have to use a gradient tape for TensorFlow two, which is a bit of a workaround for how TensorFlow two does things, we need our action space for random action selection. Let's use a list of actions from zero to n actions minus one, we need our actor critic want to make sure to specify the number of actions and we want to compile that model. So after critic compile with an atom optimizer, and learning rate defined by alphab. Next, we have the most basic functionality of our agent, the functionality to choose an action. And that takes the current state of the environment as input, which we have to convert to a tensor. And in particular, we have to add an extra dimension batch dimension, the reason being that the deep neural network expects a batch of inputs. And so you have to have something other than a 1d array SB two dimensional, so we just add an extra dimension along the zeroeth dimension. So then we will feed that through our deep neural network, we don't care about the value of the state for the purpose of choosing that action, so we just use a blank. And we will get the probabilities by passing the state through the actor critic network. And then we can use that output the probabilities defined by our neural network to feed into the actual TensorFlow probabilities categorical distribution, and then use that to select an action by sampling that distribution, and getting a log probability of selecting that sample. Sorry, that's TFP, categorical, and probabilities given by prompts, and our actual action will be a sample of that distribution. And we don't actually need the log prompt at this stage, we will need the log problem we calculate the loss function for our deep neural network. But we don't need it now. And it doesn't make sense or rather, doesn't actually work. To save it to a list, let's say for use later here. Because this calculation takes place outside of the gradient tape. TensorFlow two has this construct of the gradient tape, it's pretty cool. It allows you to calculate gradients manually, which is really what we want to do here. But anything outside of that tape doesn't get added to the calculation for backpropagation. So the log prob doesn't matter at this point, so why bother calculating it. One thing we do need, however, is the action that we selected. So we will save that in the action variable. And we will return a NumPy version of our action because action is TensorFlow tensor, which is incompatible with the open engine. It does, however, take NumPy arrays, and we want the zeroeth element of that because we added in a batch dimension for compatibility with our deep neural network and a little bit confusing, but that is what we have to deal with. Next, let's do a couple of bookkeeping functions to save and load models won't take any inputs. And so it will save the weights of the network to the checkpoint file. We do the inverse operation to load models. And we will load weights from a checkpoint file. So that is it for the basic bookkeeping operations. Next we have the real heart of the problem and the functionality to learn. This will take a number of inputs, we'll take the state reward received new state and terminal flag as input The first thing we want to do is convert each of those to TensorFlow tensors. And make sure to add a batch dimension. And I like to be really pedantic with my data type. So I will cast it to tf float 32. And we don't have to add a batch dimension to the reward, because it is not fed to a deep neural network. So now we get to calculate our actual gradients using something called the gradient tape. And we'll set persistent to true. I'm not actually sure that's needed. I'm going to go ahead and experiment with that. One, we go ahead and run the code. But I have it that way, I might have just copied and pasted code from somewhere else. So let me double check. But we want to feed our state a new state through the actor critic network, and get back our quantities of interest. So we feed the current state and then the new state. But for the new state, we don't care about the probabilities, we just care about the value. And that is for calculation of our delta. But for the calculation of our loss, we have to get rid of that batch dimension. So we have to squeeze these two parameters. And the reason you have to do that is because the loss works best if it's on a one dimensional quantity, or rather a scalar value rather than a scalar value inside of brackets. So it has to be a scalar instead of a vector containing a single item. It's just something we have to do, I encourage you to play around with it to double check me on that I move between I move between distributions, excuse me a framework. So sometimes, stuff isn't always 100% necessary, even if it doesn't hurt anything. So we need our action probabilities for the calculation of the log prob. TFP distributions categorical. We define our props by the output of our deep neural network. And then our log prop is action probs dot log prop of the self dot action. And this is the action that we saved up at the top when we calculate the action for the agent. So this is the most recent action, then we calculate our delta. That is reward plus gamma multiplied by the value of the new state times one minus end of done. And the reason for that is that the value of the terminal state is identically zero, because no returns no rewards follow the terminal state, so it has no future value. and subtract all the state value. So our actor loss is minus log prob times that delta. And the critic loss is delta squared, and the total loss, he goes after loss plus critical loss. And then we can go ahead and calculate our gradients. So our gradient is taped out gradient, total loss with respect to the trainable variables. optimizer apply gradients, and it's expects a zip as input. So we're going to zip the gradient and the trainable variables. Alright, and that is it for the actor critic functionality. So I'm going to come back to this in a few minutes to see if I need that persistent I don't believe I do. I believe I need this in the case when we have. If we were to have, say, separate actor and critic networks and had to calculate greatest perspective, two separate sets of trainable variables, I believe that's when the persistent equals true would be necessary. And when when we had coupling between the loss of one network and the other, it's so that it keeps track of the gradients, average does the back propagation, kind of like in pytorch, where it throws it away and you have to tell it to retain the graph. I'll double check on that though. So let's go ahead and write and quit. And then we're ready to go ahead and code up our main file. So with our imports, we will need, Jim, we will need NumPy we will need our agent, we all need our plot learning curve function, I'm not going to go into any detail on this, it's just a function to plot data using matplotlib. With some labeled axes, it's nothing really worth going into. first thing I'll do is make our environment. And I'm using the card poll, because it runs very quickly. And the actor critic method is quite brittle, quite finicky, you will observe in many cases where it will achieve a pretty decent score then fall off a cliff because the learning rate was just a little bit too high. So there are a number of problems with the algorithm. And it's easiest to test in a very simple environment. In my course, we use the lunar lander environment, and I did more hyper parameter tuning to get it to actually get pretty close to beating the environment, I believe. In this case, we won't quite beat it, we achieve a high score like 140 points or so when beating it is 200. But I leave the exercise of hyper parameter tuning to you, the viewer, I gotta leave something for you to do as well, right. So we'll define our agent with a learning rate of one by 10 to the minus five, and a number of actions defined by our environment, action space underscore, and then we'll have, say, 1800 games, about 2000. With a file name of card poll dot png, I would encourage you if you do hyper parameter testing, to put the string representations of those hyper parameters here in the file name, so that way, when you look at it later, you don't get confused. And you know what hyper parameters were used to generate which plot. So our figure file is just plots plus the file name, I split it up, you don't have to do it that way. We want to keep track of the best score received. And it'll default to the lowest range. So that way, the first score you get is better than the lowest. So the range is a you save your models right away. And if you list to keep track of the score history, I Boolean for whether or not we want to load a checkpoint. So if we're going to load a checkpoint, then I'm going to load models. And then finally, we want to go ahead and start playing our games. We want to reset our environment, we set our terminal flag, set our score to zero. And while we're not done with the episode, we can choose an action get the new state reward done and info from the environment, increment our score, we're not loading a checkpoint, then we want to learn. Either way, we want to set the current state to the new state. Otherwise, you will be constantly choosing an action based on the initial state of the environment, which obviously will not work. You also want to append the score to the score history for plotting purposes, and calculate a score an average score of the previous almost say 100 games. And if an average score is better than your best score, then set the best score to the average score. And if we're not loading a checkpoint, then save your models. So this inner conditional statement keeps you from overriding your models that had your best scores when you're actually testing. If you just saved the model every time then you'd be overriding your best model with whatever which you know, may not be the best model. So at the end, if we're not loading a checkpoint, actually, we can just plot either way and let's do that. It's got our x axis and plot learning curve x score. Figure file. Okay. Now I have to do a major On plots, temp that I call it temp slash actor critic. Otherwise, this stuff won't work. And you also want to do a pip install. Flow TensorFlow probability, because that is a separate package. Of course, I already have it. So let's go ahead and try to run this. And see if I made any typos. I'm certain almost certain I did. So it says something. Something is not callable. Oh, that's because I have forgotten my multiplication sign. So that is in line 49. Yeah, things I'm trying to call something here when I really want to multiply. Oh, you know what I did forget. One thing I did forget, of course, is down here, I forgot my debug statements. So let's do this. That's pretty funny. episode, I score. Set one F. Always have to forget something, of course. Okay, there we go. So one other thing I want to do is come back here to actor critic, and get rid of this persistent equals true, I don't think I actually need this. Sometimes I just copy and paste code. And then as I'm doing the video, I realize, Oh, hey, I don't always need all of that stuff. Okay, so yeah, it does run. All righty. So I'm going to go ahead and switch over to another window where I have let this finish up. Because there's no point letting it run for another 1800 games. So let's go ahead and check that out. So here you can see the output of the other 1800 games I ran. And it does achieve a score of around 160 768, about 170 points or so which is almost beating the environment is pretty close, you take a look at the learning plot here, you can see that it has an overall overall upward trend, and it's linear. And the reason I don't let it continue is because as I alluded to in the lecture, these models are very brittle. And so sometimes you can get on a very narrow slice of parameter space where your model is doing well in any small step out of that range blows up the model. One thing to fix that is replay memory, as you get a broader sampling of, of experiences and get a little bit more stable learning. But that doesn't work by bolting directly on to actor critic methods, at least from my experience, I've tried it I have a video on that. I wasn't able to get it to work. Maybe some of you can. That would be fantastic if you could, but in my case, I didn't get it to work. I thought it would work. It does not. And in fact, there's a whole separate algorithm called actor critic with experience replay that deals with bolting experience replay on to vanilla actor critic methods. So I hope this was helpful. It's pretty hard to give a really solid overview and like a 3040 minute YouTube video, but it serves to illustrate some of the finer points of Agile critic methods and some of the foundational points of deep reinforcement learning in general. In my courses, I go into much more depth. And in particular, I show you how to actually read papers, how to turn papers into code, useful skill that's really hard to find anywhere else on YouTube or Udemy. So if you like the content, make sure to leave a like, subscribe if you haven't already. And leave a comment down below with any questions, comments, criticisms, concerns, and I will see you in the next video. Oh, really quick before we do, let's check in on the other terminal to make sure it's actually learning. So here is the output. And you can see as it's going along, it is saving some models and the score is generally trending upward over time. And that's why you get the saving models because the score is best in the last best score. So we didn't make any fundamental errors in our code. If it doesn't achieve the same result that isn't entirely surprising because there is a significant amount of render run variation. But the code is functional. What's up on my GitHub. If you want to go ahead check it out. I'll leave a link in the description. See you in the next one. Welcome to a crash course in deep deterministic policy gradients, or ddpg for short. In this lecture, we're going to briefly cover the fundamental concepts and some notes on the implementation, so that the stuff we do in the coding portion isn't such a mystery. So why do we even need ddpg? in the first place? Well, ddpg exists to answer the question of how do we apply reinforcement learning to continuous action spaces. This is particularly important in something like say robotics, where we are applying continuous voltages to electronic motors that cause the robot to move in three dimensional space. Now, you may think, Well, why can't we just use something awesome, like deep q learning. And that's not such a terrible idea. However, the problem is, it doesn't work. In particular, q learning can't handle continuous action spaces. If you've coded up a Q Learning Network, a deep learning network that is, you know, that it outputs discrete numbers has actions. And of course, that doesn't cut it when you're dealing with continuous action spaces. Now, you might think, why can't we just discretize our action space. And that's not such a bad idea. But the problem is, it doesn't work. And the basic idea here is you have some finite interval for your action space, and then you just divide it up into a number of discrete chunks. And then every time you want to output the action in that chunk, you just use that discrete integer. The problem with this is that these robots tend to have many degrees of freedom, meaning they can move in many different directions in space, right, they can rotate around axes, multiple axes, in general, they can move up, down left and right, they can rotate. And so your number of discrete actions approaches the 1000s very, very quickly. And so q learning while it does handle discrete action spaces, doesn't handle large numbers of discrete actions particularly well. However, that doesn't mean that the innovations from cue learning can't be applied to actor critic methods. And in fact, they can, and that was the motivation behind the work done in the ddpg paper. In particular, we're going to make use of a replay memory, where instead of just learning from the most recent state transition that the agent has experienced, is going to keep track of the sum total of its experiences, and then randomly sample that memory at each time step to get some batch of memories to update the weights of its deep neural networks. The other innovation is the use of target networks. So in cue learning, we have to do two different things, we have to use a network to determine the action to take. And then we have to use a network to determine the value of that action. And that value is used to update the weights of the deep neural network. Now, if you're using the same network to do both things, the problem is you end up chasing your tail, you end up chasing a rapidly moving target, because each time step those weights are getting updated. And so the evaluation of similar actions, excuse me, the evaluation of similar states, changes rapidly over the course of the simulation causing the learning to be unstable. The solution to this is to keep two networks, one of which use the online network is called to choose actions at each time step. And then another network called a target network, to evaluate the values of those actions when performing the update for your deep neural network. Now, in this case, you will be doing a hard update of your target network. So every, let's say 1000 steps, it's a hyper parameter v region, but a typical value would be 1000 steps, you would take the values for the network parameters from the online network, and directly copy those over to the target network. It's what's called a hard update. The authors of the ddpg paper took inspiration from this, but instead of doing a direct, hard update, they use something called a soft copy of the target networks. All this means is that we're going to be doing some multiplicative constant for our update, and we're going to be using a new hyper parameter called tau. And it's going to be a very small number of order point 001. It's also worth noting that we're gonna have more than one target number here. And the reason we need more than one target network is because DDP G is a type of actor critic method. And so you have two distinct networks, one for the actor and one for the critic. And in fact, in this implementation, we're going to have four different networks, one actor, one critic, and then one target actor and one target critic. Now, for simple problems with discrete action spaces, you can get away with having a single network, where the lower layers learn the features of the environment and the upper layer splits off into outputting, the Act, the critic evaluation as well as the output for the actor network. But in this case, we do in fact want to totally distinct networks, as well as two copies of those for the target networks. The basic idea is that our critic network is going to evaluate State in action pairs. And so we're going to be passing in states and actions. And it's going to say, hey, given that state, the action we took was pretty good. Or maybe that action was pretty terrible, we could probably do better next time. And similarly, the actor is going to decide what to do based on the current state or whatever state we pass into it. Something worth noting is that this network is going to output action values, in other words, a continuous number that corresponds to the direct input for opening a gym environment, rather than outputting probabilities. So if you've been doing this for a while, you may know that the policy is actually a probability distribution, it is a function that tells us what is the probability of selecting any action from the action space given an input of a state or a set of states. The deterministic part comes from the fact that ddpg outputs the action values themselves. And it's deterministic in the sense that if I pass in one state over and over again, I'm going to get the same action value out every single time. Now, this does have a bit of a problem. So the problem is that the agent has something called an explore exploit dilemma. And this is present in all reinforcement learning problems. It's a fundamental concept in the field. The basic idea is that our agent is attempting to build out a model of the world, the agent wants to know how to maximize his total score over time. But it starts out knowing absolutely nothing about its world, it has to figure out how states transition from one end to another and how its actions affect those states, and in particular, how its actions give it rewards. So it starts out knowing none of this and has to build out that model over time. The problem is, the agent can never be quite sure that its model is accurate. No matter how long it spends playing the game interacting with the environment, isn't 100% certain that the action it thinks is best is actually the best. Perhaps there's some other strategy, some other action out there that is significantly better. And the degree to which the agent takes off optimal actions, is called the Explore exploit dilemma. So of course, taking off optimal action is called exploration. And taking the optimal action is called exploitation, because you're just exploiting the best known action. And this is a dilemma that is present in all reinforcement learning problems. And the solution here is to take the output of our actor network and apply some extra noise to it. Now in our implementation, we're going to be using simple Gaussian noise, because it's sufficient for the problem at hand. However, in the original paper, the authors use something called Orenstein lundbeck noise. It's a model of Gaussian processes in physical systems, it's overly complex, and it's not needed. So we're not going to implement it. Although in the course, I do show you how to implement it exactly. But for YouTube, it's not really necessary. And in fact, when other authors implement ddpg, they just throw that right out the window, because it's pretty dumb. Next is the update rule for our actor network. And it is somewhat complex. So I'm going to show you the equation, and then I'm going to walk you through it. So this is the update for the actor network. And don't panic, this is a little bit easier than it looks at first glance. So from left to right, we have the Nabla operator that is the gradient and the subscript there theta super mu means that we want to take the gradient of the cost function J with respect to the network parameters of our actor network, or the actress denoted by mu, and its parameters are denoted by theta super mu. So theta Super Q means the parameters for the critic network or the critic is denoted by Q. And it's just given by an expectation value or an average of the gradient of the critic network. Where we're going to input some states and actions with the actions are chosen according to the current policy. Okay. So, in practice, what this means is, we're going to do is randomly sample states from the agent's memory. Now, the memory keeps track of everything we want, it keeps track of the states, the agent saw the actions that took and the new states that resulted from those actions, as well as the reward the agent received at that time step and the terminal flag to determine whether or not the episode ended on the new time step. So the scent the memory keeps track of all of that. But here, we just want to sample the states the agent saw, okay, and then we're going to use the actor network to determine what actions it thinks it should take based on those states. Now, these actions will probably probably be different from the actions we have stored in memory and that's okay. This is off policy, meaning we're using a separate policy to gather data and use that data to update a different policy, the current policy so It's ok that these actions don't match. Don't worry about that. It all works out in the end, then you then the next thing you want to do is plug those actions from the actor into the critic network along with the states, we sample from the memory and get the value for the critic network what it thinks that state action pair is worth. And then we're going to use the gradient tape from TensorFlow to to take the gradient of the actor network, excuse me, the critic network with respect to the parameters from the actor network. And we can do that because they're coupled through this selection of the actions based on the actor and network. Okay, so it's a little bit complex. It's much easier in code just in code, you sample the the states, then you get the actions based on the critic network, and then plug those states and actions into the critic network. And then you just have the loss proportional to that it's actually much simpler in code than it is on paper. But that's the basic idea. So then the next question is, how do we implement our critic network? Well, fortunately, it's a little bit more straightforward, and it's more reminiscent of deep q learning. So we have this relatively simple loss function, that is the mean squared error between these, this target value y sub i, and this Q for the current state and action. So what we're going to do here is randomly sample states new states actions and rewards. And then we want to use the target actor network to determine the actions for the new states. Then plug those actions into the target critic network, to get your why and multiply it by the discount factor gamma, and add in the reward from that time step, which you sampled from the memory. And then that is a target the via we want to shift the estimates for our critic towards. And then we want to plug the states and actions into the critic network. In other words, the actions the agent actually took that we sampled from our memory, This is in contrast to the update for the actor network, and take that difference with the target. And then we're going to input it into the mean squared error loss function for TensorFlow two. So in code, this is going to be relatively straightforward as well, we're going to have a sample function to get the state's new states actions and rewards, we're going to plug in the new states into the target actor network, get some output, we're going to plug that output into the target critic to get our target y, and note, it is S sub i plus one. So what is the new states that we get from our memory buffer. And then we're going to plug the current states and actions from our memory into the critic and take the difference with the target. So it only looks kind of scary on paper, when you see it written in code, it'll make much more sense. So the other piece of the puzzle is how we're going to handle the updates of our target networks. So at the very beginning of the program, we are going to initialize our actor and critic networks with some parameters, of course, are going to be random. And then we're going to directly copy those parameters over to our target actor and target critic networks, that'll be the only time in the simulation that we do an exact hard copy. every other time step, we're going to use the soft update rule. So here you have the two parameters, theta Q, theta Super Q prime, so the weights of the target critic network, and theta super mu prime, which is the weights of the target actor network, you're going to update those with tau multiplied by the respective value of the online network, the critic or the actor, and add in one minus tau times the current value of your target actor or target critic network. And so this will be a very slowly changing function, because there's going to be some small number tau, multiplied by some parameters plus one minus tau is, which is approximately one multiplied by the current value. So it's going to be approximately equal to its last time step, just plus minus a little bit. So it's relatively straightforward. We'll see it in code. It's not all that bad. So then the next question is, what data structures are we going to need for all of this? So we're going to use a class to encapsulate our replay buffer, and that will use NumPy arrays, there are a myriad of different ways to handle the replay buffer. I like the NumPy arrays because it's easy to enumerate. It's easy to know, what is being stored where, and it makes for an easier YouTube video because it's easier for people to see what's going on. If you have a different way of doing it, by all means use that it's not something for which there was only one right answer. We will need one class each for the actual network and critic network and those will be handled using the TensorFlow two framework, these will have the functionality to perform a four pass as well as the usual initializer function. We will also need an agent class to tie everything together, the agent will have a functionality for the memory right it will have a memory buffer to store memories, it will have the actor network critic network target actor or target critic network, it will also have a function for choosing actions based on the current state of the environment. And that will involve passing a state through the actor network, getting the output and adding in some Gaussian noise. It will also have functionality to learn from the memories it has sampled. And it will also have functionality for checkpointing models because this can take quite a while to train for complex environments, we're going to use a simple environment. But if you want to go ahead and try something more complex, the checkpointing functionality will come in handy. Finally, we will need a main loop to train our network and to evaluate its performance. Or that has been a very brief introduction. Let's go ahead and get into the coding portion of our video. Alright, now that the lecture is out of the way, it is time for a shameless plug. In my courses, I show you how to go from paper to code rather than relying on someone else to break down the material and then paper for you. It's the best way to gain independence. And to level up your skill as a machine learning engineer provided through Udemy, they're on sale right now check the link in the description below. Let's go ahead and start with our buffer. So the point of our buffer is to store the state's actions rewards new states and terminal flags and the agent encounters in its adventures. And we're going to use as I stated in the lecture NumPy for this is going to be relatively straightforward. The reason I use NumPy is because it makes it much more clear what everything is, it's just a little bit simpler and cleaner from an implementation perspective, although it is by no means the only way to do things. So the first thing we'll need is a max size, the memory is bounded, and input shape from our environment, and a number of actions for our action space. Now in this case, since it is a continuous action space, number of actions is a bit of a misnomer. What it really means is number of components to the action. The purpose of the meme size is that the memory cannot be unbounded. And our memory will have the property that as we exceed the memory size, we will override our earliest memories with new ones. So for that, we will need a memory counter it starts at zero. And then we can go ahead and start with our actual memories. First we have a memory size, excuse me a state memory. And that will be in shape memory size by input shape. We have the new state memory which is in the same dimensions we need an action memory. And that will be and shape mem size by n actions. And one thing I want to make clear there was a question on the discord server. By the way, also check the link in the description for the discord server, we have many really bright people in there talking about a lot of complex stuff. It's a great little community, I would encourage you to join. But the question popped up whether or not I'm writing this stuff on the fly. And I am not I don't know if that was clear, you can often see me looking off to the side here. That's because it's not a second monitor, which is a really large monitor where I have a second window open with the already completed code. And if you've seen my tutorials before, you know that I make a lot of typos. And so the probability of making a logical error where I have state instead of new state, for instance, is incredibly high. And that when you are making YouTube videos is enormously painful to try to do everything from memory and then swap states with new states and then not have it work and then have to go back and re record. So it's just easier to have the already written code and I'm kind of reading off of it as I go along. And occasionally I'll make modifications. So all the code is mine, but it is not written on the fly. I just want to make that clear. Next we'll need a reward memory and then as your shape mem size and that is just going to be an array of floating point numbers. And we will need a terminal memory and that will be and type Boolean. I use Boolean because in in pytorch, we can use masking of tensors I don't think it works in TensorFlow. So we'll have to do something slightly different in our learning function, but I will note that when we get there we will need a function to store a trend. Whereas transition is the state action reward new state and terminal flag, the first thing we want to do is determine what is the position of the first available memory. And that's given by the modulus of the current memory counter and the memory size. Once we have the index, we can go ahead and start saving our transitions. Yeah, see, I've already made a mistake here. Then we have actual memory and terminal memory. And we want to increment our memory counter by one. Now, one thing I don't think I stated in the lecture, the purpose of this terminal memory is that the value of the terminal state is zero. Because no future rewards follow from that that terminal state, we have to reset the episode back to the initial state. So we have to have a way for accounting for that in our learning function. And we do that by using the terminal flags, excuse me by using the terminal flags, as a multiplicative constant in our learning function as a function to sample our buffer, and that will take a batch size as input, we want to know how much of our memory we filled up because we have a memory and it starts out as entirely zeros. And until we fill up that memory, some portion of it will be nothing but zeros. It doesn't do us any good to learn from a bunch of zeros. So we have to know how much of the memory we've actually filled up. And this is given by the minimum of the memory counter or the memory size, that we can take a batch of numbers, random choice maximum batch size. And, you know, I think I want to pass the Replace equals false flag in there. I don't have that in my cheat sheet. But the point of passing and replace equals false is that once a memory is sampled, from that range, it will not be sampled. Again, that prevents you from double sampling memories. If you're dealing with a really large memory buffer, the probability of sampling two memories, two identical memories is astronomically small, but up until that point, it's non negligible. So we should be careful. Then we want to go ahead and dereference our NumPy arrays. And we will return those at the end is a typo. Alright, that wraps up our replay buffer class. Now we're going to go ahead and handle the network classes. Alright, so our imports are relatively light, we will need OS for file path joining operations for model checkpointing. We will need the base TensorFlow package we will need Kairos and we will need layers. Now we're going to be dealing with a very simple environment. So we just need a dense layer. We don't need any convolutional layers. So we will start with a critic network. And that derives from the Karol stop model class that will take a number of actions fully connected dims. a name and a checkpoint directory. Now very important, you must do a make der temp and make der temp slash ddpg. Before we run this, so that you can actually save your models otherwise we'll get an error. Let's go ahead and call a super constructor and then start saving our variables. We have to call our model model name due to the TensorFlow package keeping name as a reserved variable. You can't just say dot name, it'll give you an error. Not a huge deal, just something to be aware of, then we'll have our checkpoint file. And chaos models get saved with a dot h file extension. And I throw in the model name, because we're going to want to distinguish between the target and regular networks. And I add an underscore ddpg. Because if you do development in a single directory, then you don't want to overwrite models from say, TT three with ddpg models vice versa. It's just a way of keeping all of your models distinct and very secure. Next, we need to define our network. So our first fully connected layer have a lot of put self that FC one dims with a rail you activation, then we'll have FC two dims. Rail you and our final output layer is single valued with no activation. A couple of things to note here is that in the original paper, the author's use batch normalization. Turns out that isn't necessary. In the course I show you how to implement that. But it's use an overly complex implementation, it doesn't actually add much to it. They also do a number of kind of tweaks with the initialization of the layers, we're not going to do that here, I show you in the chorus, but it's not really necessary. Next, we need our call function. This is the forward propagation operation. And this is the critic so it takes a state and action as input. So we want to pass the concatenated. State and action through our first fully connected layer and we'll concatenate it along the first axis. The zero axis is the batch. And then we will pass the output of that through the second fully connected layer. And we will go ahead and get our Q value out and return it. That is it for the actor network. Excuse me, critic network. Very, very straightforward. Next, we have our extra network. And that is pretty similar. equals 512. And actions are just defaulted to name equals actor. And check pointer. And you know what, now that I'm looking at this, we don't need the number of actions here, do we because we don't use it. Let's get rid of that. Yeah, I'm looking at my cheat sheet as well. These are some of the modifications I make as I go along. Sometimes I'll have I'll have stuff that doesn't always make 100% sense. And then when I do the YouTube video, I go ahead and rectify it. So we'll go ahead and call our super constructor and save our values. This time, we will need the number of actions because the critic network does. Excuse me, the actor network does need to know how many actions it has. And we will need our checkpoint actor. And then our model is going to be pretty similar to the critic network, there's going to be a number of fully connected layers with a row activation. Now, we don't want a neural activation here no activation or linear activation, if you will, we do want an actual function here. And what we want is a function that is bounded between plus and minus one. And the reason is that most of our environments have an action boundary of plus or minus one. However, if the action boundary is larger than plus or minus one, it's easy to get the The action within that range by multiplying a function which is bounded by plus or minus one by the upper bound of the environment. So, if your environment has bound to plus or minus two, then he would just multiply the tan hyperbolic function which is bound to plus or minus one by to. Next, we have our call function, we'll go ahead and call it prob, it's a bit of a misnomer. These aren't really probabilities. And then we will get our mu, which is from the paper, it's our actual action. Now, if you had not plus or minus, one can multiply here, you can multiply it there, or you can multiply it in the agent class, when we do the Choose action function. Either way, is logically the same, I would probably do it in the agent class, just for my own personal preferences, but either way, you want to multiply the output of the deep neural network by the bounds if those bounds are not plus or minus one. All right, that is actually it for the network classes. Let's go ahead and code the agent class and tie all of this together. Okay, our imports will be quite numerous, we will need NumPy. TensorFlow, we will need Kairos. We will need our optimizers. Which we use Adam for this project, we will need our replay buffer. And we will need our actor and critic network. I misspelled it not belux. Correct. So here's our agent class. And we have an initializer. And that will take input dims. A learning rate for the actor network alpha, a learning rate for the critic network beta. These are distinct, they're not the same. And in fact that the critic networking can get away with a slightly higher learning rate than the actor network. And that is because in general and policy gradient type methods, the policy approximation is a little bit a little bit more sensitive to the perturbation and parameters. So he wiggle around the parameters of your deep neural network, you can get big changes in the output of the actor network. And so it has to have a slightly smaller learning rate with respect to say, like the critic network, we will need our environment for the max admin actions because as I said, on lecture, we're going to be adding noise to the output of our deep neural network for for some exploration, and we have to clip that into the maximum actions for environments we don't trigger an error. When we try to pass that action into the open AI gym. We will need a gamma. And that is the discount factor for update equation. number of actions. A MAX SIZE for our replay buffer defaulted to a million a default value for our soft update of 0.005. That's from the paper. default for FC one and FC two. In the paper, they actually use 403 100. I'm going to go ahead and do that now rather than in the main program that makes life a little easier. A batch size for a memory sampling and a noise for our exploration. So let's go ahead and save our parameters gammon towel, we can instantiate our memory. We can save our batch size and our noise and then we'll go ahead and get the maximum and minimum actions for our environment. Then we can instantiate our actual networks. So our actor, our critic or target actor, And our target critic. That is it for our networks, now we have to actually compile those we'll use our atom optimizer learning rate to find by alpha. Similar we for our critic network, we will give it a learning rate of beta. And then we have our target networks. That is a bit of a misnomer, because we aren't going to be calling the we are going to be doing gradient descent for these networks, we're going to be doing the salt network update. But we have to compile the network, so we have to pass it a learning rate. It's just a feature of TensorFlow. Don't get confused. If down in the Learn function, we don't actually call an update for the loss function for these target actor and target critics, then we have to call our update network parameters function. And this is where we do the hard copy of the initial weights of our actor and critic network to the target actor and target critic network. And the passing of tau equals one to facilitate that hard copy. Let's go ahead and write that function now. And so we have to deal with the base case of the hard copy on the first call the function and the soft copy on every other time we call the function. So tau is none. In other words, if we don't supply a value for towel, and just use the default defined in the constructor, and so you notice here, the first time I call it towel is one, so tau is not none. So we're going to be using a value of one for tau instead of the 0.005. So we'll say weights is an empty list. And our targets will be target actor weights, or eye weight. We're going to iterate over the actor weights and append the weight for the actor multiplied by towel, plus targets sub i know, which is the weight from the target actor times one minus towel. And then after we go through every iteration of the loop, we're going to set our weights for the target actor to that list of weights. So in the first iteration, towel starts out as one, and so you have weight times tau, which is one, so just the actor weight plus the target actor weight, multiply by one minus tau, which is one minus one or zero. So on the first iteration, we get a hard copy. And then we do the same thing for the critic network. And that is all we need to do for our software update rule. So now we'll go ahead and write an interface function for our agents memory. They'll take a state action reward new state, and terminal flag as input. And then we'll just call the store transition function. And this is just good clean coding hygiene. It's an interface function. When you have interdependent classes, the in theory the we shouldn't be able to call the agent dot memory dot store transition from anywhere else in the program except within its own member function. It's just basic, object oriented programming stuff. Now let's deal with model checkpointing. And we'll say self dot actors save weights after a checkpoint file. And likewise for our other networks should be a dot underscore. I think I've done that before. Specific mistake also have an extra space here. And the load models function is just the inverse operation. We say what is it self dot, dot load weights? dot checkpoint file. Critic check one fall Yep. Okay, that is it for our basic bookkeeping type functions. Now we can get into the functionality for choosing an action. And that will take the observation of the current state of our environment as input, as well as a flag I call evaluate. This has to do with training versus testing our agent, remember that we use the addition of noise for the exploration part of the Explore exploit lm, if you're just testing the agent to see how well it learned, you don't necessarily want to add that noise, you can just do the purely deterministic output of your actor network. And I facilitate that with a Boolean flag here. First thing you want to do is convert our state to a tensor. And we have to add an extra dimension to our observation to give it a batch dimension. It's just what the deep neural networks expect as input, they expect the batch dimension. And I specify a float 32 data type to be pedantic. And then we pass the state through the actor network to get the actions out. If we are not evaluating, in other words, if we are training, then we want to get some random normal noise in the shape of self dot n actions, with a mean of 0.0 and a standard deviation of whatever our noise parameter is. Now, it's entirely possible that the output of our deep neural network was one or point 999. And then when you add in the noise, you're adding in something like let's say 0.1. And you end up with an action that is outside the bounds of your environment biggest perhaps is bounded by plus or minus one. And so you want to go ahead and clip that to make sure you don't pass any legal action to your environment. We'll say actions equals TF clip, by value as a function we want actions as between self min action and Max. Action. And then we want to return the zeroeth element because it is a tensor and the value is the zeroeth element, which is a NumPy array. Then we want our learning function. And this is where the bulk of the functionality comes in. And right away, we are faced with a dilemma. So what if it's the case that we haven't filled up at least batch size of our memories. So remember that the memory starts out as all zeros. And if you've only filled up, let's say 10 memories, you don't really want to sample those 10 memories, you know, batch number of times batch size number of times. So you can just go ahead and say well, I'm not going to learn I'm going to wait until I fill up my memory into until at least batch size number of memories. Alternatively, you can play batch size number of steps with random actions, and then call the Learn function. That's another solution as well and they're both valid. I just find this to be a little Bit more straightforward. So then we have to go ahead and sample our memory. And then we can go ahead and convert these two tensors. New states, singular, not plural. And we don't have to convert the terminal flags to a tensor. Because we're not going to be doing tensor operations with it, we're going to be doing regular NumPy array type operations. So we're going to use a gradient tape for the calculation of our gradients. If you're not familiar with a gradient tape, the basic idea is we're going to go ahead and load up operations on to our computational graph for calculation of gradients. So when we call the Choose action function above, those operations aren't stored anywhere that is used for calculation of gradients. So that's effectively detached from the graph. So only things within this context manager are used for the calculation of our gradient. And so this is where we're going to stick the update rule from our lecture. So let's go ahead and start with the critic network, where recall that we have to take the new states and pass it through a target actor network, and then get the target critics evaluation of the new states and those target actions. And then we can calculate the target, which is the reward plus gamma multiplied by the critic value for the new states times one minus a terminal flag, and then take the mean squared error between the target value and the critic values for the states and actions the agent actually took. So let's go ahead and write that out. So target actions is given by a target actor. What are the things we should do for the new states and the critic value for those new states, that's going to be given by the target critic evaluation of those new states and target actions. And squeezed along the first dimension, we have to put in this squeeze because we have the batch dimension and it doesn't actually learn if you pass in the batch dimension, I feel like I am I missing there we go parenthese. So we'll have a squeeze everywhere that we had to actually pass up through the network. So then the critic value, which is the value of the current states, the original states and the actions the agent actually took during the course of the episode is given by the squeezed output of the critic, or the state's actions along the first dimension. I guess it is right. Yep. And then we have our targets, and that's reward plus gamma, times this critic value underscore times one minus done. So this one minus done is one minus true or false. So when the episode is over, done is true. And so you have one minus one or zero. And so the target for the terminal new state is just the reward, whereas for every other state, it is reward plus the discounted value of the resulting state, according to the target critic network. I'm sorry, you can hear the landscapers outside is Tuesday they're doing the neighbor's yard. Hopefully the noise suppression filter takes care of that, but if not, I apologize. Next we have a critic loss. And that's the mean squared error between our target and the critic value. outside of the context manager, we want to go ahead and calculate the gradient so we'll say credit network gradient was taped out gradient critic loss self critic trainable variable so it is the gradient of the critic loss with respect to those critics trainable variables, and then we want to apply our gradients. So our optimizer dot apply gradients and that expects a zip as input. We want to zip up the critic network gradient and the critic trainable variables and that is it for the critic loss. Now we have the actor loss. So with, we want to do essentially the same thing we want our context manager gradient tape as tape then we say new policy actions accurate states. So, this is the these are the actions according to the actor based on its current set of weights, not based on the weights it had at the time of whatever memory we stored in the agent's memory. So, then we have our actor loss. And that is the negative of the critic output of the states and the new policy actions, it's negative because we're doing gradient ascent and policy gradient methods, you typically want to take the, you don't want to do a gradient descent because that would minimize the total score over time you want to maximize total score over time. So you do gradient ascent, gradient descent is just the negative of gradient descent. So we stick a negative sign in here. And then our loss of just the reduced mean of that after loss. And then we can go ahead and calculate our gradients and apply them. So the actor network gradient is taped out gradient of the actor loss with respect to the actor trainable variables, and this is how we're going to get that gradient of the critic loss with respect to the mu parameters of theta super mu, is by taking this actor loss, which is proportional to the output of the critic network. And that is coupled, the gradient is nonzero because it has this dependency on the output of our actor network. So the dependence that gives you a nonzero gradient comes from the fact that we are taking actions with respect to the actor network, which is calculated according to faders theta super mu. And that gets fed forward through here through the critic network. That's what allows you to take the gradient of the output of the critic network with respect to the variables of the actual network, it's how we get that coupling. And if you read the paper, they actually apply the chain rule and you get the gradient of the critic network and the grading of the actual network. This form is actually easier to implement in code. That's why I do it based on the first equation, not the second equation in the paper, it's just easier to do. So why not do it the easy way. Then you want to apply those gradients, which again, takes a zip as inputs. We want to apply zip up our attic radio network, and the actor dot trainable variables. One other thing I want to point out is that this accurate, trainable variables we didn't define, it comes from the fact that we derive our actor network class from the Kairos dot model class. It's just comes from the properties of object oriented programming. Once we have updated the main networks, we want to go ahead and perform the soft update of our target networks. And since this is not the first time we're calling the function that gets no input, so we'll use the default value for tau of 0.005. And that is it 113 lines for our agent class. Now we're ready to write up the main loop and test it out to see how well it does. Okay, I have an error, it says keyword cannot be an expression. Let's see rewards to convert to tensor. Where have I gone? Wrong? Right here, it's a period instead of a comma. All right. Now we're good. So let's go ahead and start with our imports. We have Jim we have NumPy. We have our agent and are plot learning curves. So go to my GitHub, do a git clone and get those utils. It is just a matplotlib function to plot our learning curve, which is the score versus time the running average of the previous 100 games. Over time, it's nothing magical. I don't go over it because it's relatively trivial doesn't really contribute to your understanding. You can just do a plot of the scores over time to see if it's learning So we start with our making our environment. And we use the pendulum and one stanchion, our agent, getting the observation space, from our environment for input dims, passing on the environment, shape. And we will use the default value for our noise and every other parameter because those were good defaults. let it play 250 games. Let's go ahead and to find a figure file, pendulum dot png, we need to keep track have the best score. That's the lower bound of our reward range. And to keep track of the history of scores, Agent receives, and a load checkpoint, that'll be false. That's if you want to set up our training versus testing. If we're going to load that checkpoint, then we want to set number of steps to zero while n steps is less thing agent dot batch size. What we're doing here, okay, so I should explain this. So my understanding and this could be wrong. If it's wrong, drop a comment down below to correct me, I don't profess to know everything about TensorFlow. But from what I've read from Google, the model loading is set up such that you have to call the learning function before you can load your model. That's because when you instantiate the agent, you aren't actually loading any values onto the graph. And so it's basically an empty network with no values loaded into that network, it doesn't load any violations, he tried to do something with it. And so we're going to go ahead and fill up the agents memory with dummy variables, dummy values, they don't really matter, we're just going to go and load it up with dummy values, and then call the agents learn function so we can load our models. And so we can do a random action doesn't really matter. Get our new state, reward dawn info from our environment. And then remember that very important one increment number of steps. Once you've done that, call the Learn function and load your models. And said, evaluate. Value eight true. And if we're not going to be loading our checkpoint, we'll just say evaluate equals false, I guess we could just use load checkpoint in place of evaluate our call it evaluate, but whatever I've used to separate variable assuming. So for i in range and games, we want to go ahead and reset our environment. At the top of every episode, reset the terminal flag and the score to zero. While the episode is not done, Beijing can choose an action based on the observation and the evaluate flag. Good the new state reward Don and info from the environment. Incorrect our scoring, and store that transition for not loading a checkpoint that we want to learn. And the reason I put in that conditional statement is because if you're evaluating the performance of the agent, you probably don't want to disturb its parameters, you want to just go ahead and see how it performs. As of the last time you saved it, rather than trying to get to learn a little bit more. Feel free to change that. And very importantly, we want to set the current state of the environment to the new state after the agent took its action. So then we want to at the end of every episode, we want to append the score and calculate the average to get an idea of whether or not our agent is learning If the average score is better than the best known score, then set the best score to that average score. And, again, if we aren't loading a checkpoint, go ahead and save your models. And at the end of every episode, we want to print some basic debug information. So I score and at the end of all the episodes we want to print our plot our learning curve. So our x axis number of games. Okay, that is it for our main loop. Let's go ahead and test it out to make sure I made a sufficient number of typos. But as I said, the first thing I want to do is make dir temp. Okay, I already have that to make dir temp slash ddpg. I didn't have that and make der plots. I think that already exists. Okay. Now we can go ahead and run the main file and see what I messed up. Okay, so Oh, yeah, of course our critic network does not get a number of actions. That's something I changed on the fly. So let's fix that here as well. We go ahead and put this back up here. I think that is right. Let's try it again. agent has no it's because its target actor dot weights not target actor underscore weights. That is in line 39. So right here that I do. Nope, the target critic was correct. Okay. has no attribute men sighs Okay. Oh, it's in action. That's in line. 73. Yeah, it's mean action. And Max action. Of course. All right, that's in main line. 41. See, this is why I have a cheat sheet because even even with the cheat sheet, I make a number of typos. So you can imagine what it's like if I were to try to do it on camera that is in line 41 if not load checkpoint. There we go. Good grief call takes three positional arguments, but four were given all that's because I have my parentheses in a wrong place. Okay. That is in line 92 so the state's target actions so the parenthese goes here. Yeah, that's right. Oh my goodness concat missing one required positional argument Oh, because I have the because I have an extra Brenda z okay. So, this is bad, even for me, line 23. So, then we need a second parenthese there. Critic trainable underscore variables once again that is a trainable dot variables. That is in line 90. I thought I looked for that. Oh, no, I didn't critic dot trainable variables. I did the same thing here. Alright, what I just fixed that, did I not? line 100? Did I just fix that? No, I did not just fix that. Right? Okay. Actor dot critic dots. Okay, this is what happens when you don't do this for a month. making YouTube videos is a perishable skill. Okay, perfect. Now what is actually working, I've got gotten through all of those typos, and it has started to run. So I'm gonna let this go ahead and finish up and we're going to see how it does. Alright, so it has finished running. And you can see that at the end, it kind of tapered off a little bit in performance. If we scroll up, you can see that about halfway through it was achieving record performance with pretty much every single simulation that isn't entirely a typical with actor critic type methods. So oftentimes, what will happen is the agent will achieve some reasonable performance and then kind of started to taper off. Because the as I said, the actor network is relatively sensitive to changes in its parameters. In this case, it didn't fall off a cliff like I've seen with things like actor critic or policy gradient methods, but it is still nonetheless sensitive to changes in its weights, and is prone to deteriorations of performance late in the number of simulations. If you take a look at the learning curve, you can see pretty clearly that it has an overall upward trend over time. So it is in fact learning our technique is working. It's doing its job, but it's not, you know, it's nothing, at least for this environment. It's not the greatest thing since sliced bread, we could do a little bit more tuning to get it even better. But for now, I think this is sufficient to a demonstrate that it works and to be have a solid tutorial on how to implement deep deterministic policy gradients. Once again, shameless plug if you want to know how to go from paper to code, I show you how in my two Udemy courses on deep reinforcement learning, where we go through several papers per course, one on deep learning one on actor critic methods, and implement all these algorithms from scratch, I show you how to implement pretty much everything from the papers minus some super superfluous features. Either way, if you made it this far, please leave a like, subscribe, drop a comment down below, and I'll see you in the next video. Welcome to a crash course in 20, late deep deterministic policy gradients or TD three for short. This is a very brief lecture that's going to cover the fundamental concepts and implementation notes for our coding tutorial, which will follow this lecture. If you want the full details of how this algorithm works, I highly recommend you read the paper titled addressing function approximation error and actor critic methods. It's very well written very technical, very detailed, and it covers significantly more detail and depth than I will do here. I'm just going to give you broad strokes, and some idea of what we're going to be doing in our coding tutorial. And we're going to be using TensorFlow too, by the way for our coding tutorial, so make sure you have that installed. So TD three exists to deal with a fairly straightforward issue. How do we deal with overestimation, bias and continuous action space actor critic methods, if you're not familiar with over estimation bias is a tendency of agents to incorrectly estimate the value of a state on the high end. So it says this state is worth more than it actually is. And this is a problem because the agent will attempt to access that state in the future, which leads to a suboptimal policy because there are other more profitable states out there. So this is a pretty big problem, particularly when you're trying to approximate the agents policy as we are in actor critic methods. But it's not clear where this would even come from. in Q learning, we get biased because we take a max over our actions in our update rule for the either the table or our deep neural network. So there is some maximization or overestimation built in right from the beginning. And that's pretty easy to see. But there is no max in our update rule for actor critic methods. So what gives? Where could this overestimation bias come from? Well, on a somewhat theoretical level, I kind of argue they don't do this in the paper, but the way I think of it is that we have Stochastic gradient descent, or we're attempting to maximize the product of our probabilities or policy and the rewards or returns the agent receives over time. So there is some implicit drive to maximize score over time. And so when you get natural variation in your rewards, or in the trajectory through the state space, you can end up with some incorrect estimations. of the visor states because of natural, you know high variance, which is typical for deep reinforcement learning problems. However, more fundamentally, overestimation comes from approximation errors. Now, this isn't something they prove in the paper, because this is a result that dates all the way back to the early 1990s, when people were just starting to talk about q learning. And so it's been known for a while. But the basic idea is that when you attempt to use some mathematical apparatus, to estimate a function in high dimensional space, you get approximation errors. And that results in over estimation. Now, what is our source of overestimation or approximation error, in this case, it's going to be a deep neural network. Neural Networks are, of course, universal function approximator. Mostly, there are obviously exceptions, you know, discontinuous functions, they can choke on the function has to be differentiable and all that good stuff. But for all the stuff we deal with, neural networks are going to be a universal function approximator. And so that leads to a source of error. Now, this is inherent to any approximation method, either tiling, like, binning anything like that. So it's not that neural networks are bad, it's just whenever you have a function approximation, you're going to have some error. Okay, because you don't have an infinite amount of time to collect an infinite number of samples to get infinite precision, you're always going to have to lop off your estimate, at some point where there are more decimal points waiting for you, should you be able to collect more data. Worse yet, actor critic methods are bootstrapped? Well, this means is that we're going to be using our initial estimates to perform updates to later estimates. And so you have some errors in your early estimates that propagate to your later estimates over time, so you get an accumulation of error over time. Naturally, there are other ways of dealing with this. And in fact, double q learning uses a rather ingenious solution they use in that algorithm, two different cue networks, and you alternate their use in the update rule. And so you're never taking a max over actions of the same network that you use to choose an action when you're trying to update the value of that action. And so it seems like it's a reasonable thing to try. So in particular, that's what they do. Now, I want to point something out that in double q learning, deep, double, deep, deep double q learning, they use a not exactly analogous solution to the tabular case of double q learning, they use something slightly different, which doesn't work an actor critic methods, as they detail in the paper, they're going to use a more exact replica of the tabular version of double q learning. And they're going to perform a slight modification where they're going to clip the inputs of the actions to the cue networks, the double cue networks, which is going to tend to underestimate and we're going to be taking a min. So later on, you'll see in the algorithm that we're going to feed some clipped actions to our cue networks, and then take the minimum. So whatever the minimum value is, we're going to take that which tends to underestimate. Now you may say, Dr. Phil, isn't that a problem? Not really. And the reason it's not a problem is because if we underestimate actions, those actions become less attractive to the agent. And so it's not likely to take those actions again. And so that kind of dampens that out over time. It's a natural feedback mechanism to deal with that issue. So that is rather nice. And it's baked right into the algorithm. The other innovation is that they're going to delay policy updates, to give the credit Network Time to converge. So the Policy Network is very slowly changing function, or as the queue network can change much more quickly. So you have two different timescales there. And that's where the delayed part comes from. Another innovation is we're going to use target networks for the actor and both critics. Since we're doing an analogue of double q learning, which uses two cue networks, it stands to reason we're gonna have two critics. And we're going to target networks for all the things so we're going to have an actor, a target actor, to critics, and to target critics. So total of six deep neural networks, we're gonna have all the neural networks in the world. The other thing we're going to need is a salt update to the target networks. So you have a couple of options for updating the weights of retarget networks. One is you could do Stochastic gradient descent on those directly. Another option is to take the parameters from your online networks that you're actively performing gradient descent on and copy those to the target networks directly. Or a third option is to do some slowly varying interposed between the two. So you're going to update the online networks every time separately n time steps in the case of the policy, and then you're going to do some slowly Changing update to those target networks. And that'll introduce an additional hyper parameter called tout, or agent, which you will see later. So this is an actor critic method. And we're going to have actually, six distinct, I have a typo there. Sorry, I can't count despite having a PhD in physics. So we're going to have an actor to critics target actor. And then to target critics. As I said, the purpose of the critic is to be critical it is to evaluate the values of states and action pairs. So it says, Hey, in this particular state, we took some action, I think this was valuable or not. And that will help to update the agents estimate of the VI's of those pairs, and to choose better actions for given states over time, the actor will decide what to do based on the current state. And it's important to note here that our deep neural network is going to output action values, which are continuous numbers, not discrete numbers, not probabilities. Now, that makes us a deterministic algorithm. That's where the name of the algorithm comes in twin delay deep deterministic policy gradients is an extension of deep deterministic policy gradients. And so we have a bit of a problem there. Because we're dealing with approximation, right? We're approximating a cue function, we're approximating a policy, we're approximating all the things, and we never know exactly how accurate our approximations are. And so we never know if we are correctly evaluating states and action pairs. So we never know if given some state if this action is really the most beneficial action we could possibly take. Or if there's some other, more beneficial action out there waiting to be discovered. That is called the Explore exploit dilemma. Do we explore sub optimal actions or exploit what we think are the most beneficial actions. And the extent to which you engage in those two activities is the dilemma. And there are a number of solutions to that deep learning uses epsilon greedy action selection, where you just take random actions, some fixed proportion of the time, but in this case, we're going to be adding noise to the output of our actor network, when we decide what actions to take. The update rule for actor looks a little bit scary, but it's actually not. So this is the update rule from the paper j is going to be our loss function for our actor. It's a function of phi, the parameters of the Policy Network. And you want to take the gradient of the cost function with respect to the parameters of the Policy Network. And it's given by one over m times the sun, the sum, or a mean an average. So our loss function is going to be a mean. And it's going to be the product in this particular equation of the gradient of the first critic network with respect to the actions chosen by the Policy Network, multiplied by the gradient of the policy network with respect to its parameters. Now, this looks intimidating, but it's actually not this is the application of the chain rule to the loss function. So they have taken the gradient of the loss function with respect to phi. But the loss function is proportional to the output of the first critic network. Of course, the first critic deep neural network has its own set of of neural network parameters, it doesn't have an explicit dependence on the neural network parameters of the Policy Network. So it's very difficult to take a gradient write something that doesn't depend on something else. So the dependence is implicit, it comes from the fact that those actions are chosen according to the output of our policy network. And so you have to apply the chain rule. In reality, all we're going to do is the following. We're going to randomly sample states from our memory, we're going to use our active network to determine actions for those states. So we're not going to be using the actions from our memory, we're going to figure out what actions the agent thinks we should take. Now, we're going to plug those actions into our critic and get some value, specifically, the first critic never the second only the first, that's just by design. And then we're going to take the gradient with respect to the accurate network parameters. Now we don't have to calculate that gradient, TensorFlow is going to do it for us, we just have to do the first three things where we have sample states. Use the actor determined actions for those states and plug those into our critic along with the states to get some value, and then take the gradient with respect to the accurate network parameters. Now, keep in mind that this update isn't performed every time step is performed every other time step. How often you perform it is a hyper parameter of the algorithm. But it is not every single time step. Now, nominally the update rule for the critic is a little more straightforward. So again, you're going to sample a batch of transitions from your memory, you're going to put the new states that the agent received observed after taking some action through the target actor network, that's at a tilde a parameter. And then you're going to add in some clips noise, it's just going to be a normally distributed noise with mean zero, and some standard deviation, something like 0.2. And we're going to clip it in the range of minus 0.5 to positive 0.5. So that's where the clipping comes in for our double q learning, and the actual double q part comes in. And when we calculate our targets y, so we're gonna take the reward that we sample from our buffer, and add it to the product of the gamma, which is the discount factor 0.99 or so. And we're going to take the minimum of the output of the two target critic networks. So we'll say, we're going to feed the new states through the new states and those clipped actions through both target critic networks. And we're going to see which one is the minimum and take that for our target value. And then we're going to input that target value into our loss function. Again, you have a one over n multiplied by some multiplied by something squared, that has a mean squared error. And it's the mean squared error between that target y and the output of both of our critic networks. So our loss is going to have two different components, it's going to have a loss for critical one, we have q sub theta sub one, and then our have a loss for Q sub theta sub two. So our two losses, and in TensorFlow two, when we do our gradient tape, we're gonna have to pass in that persistent equals true flag to our function call, so that it keeps track of network parameters between gradient ascent steps. And so these, the rest of this verbiage is just kind of the verbal description of what we want to do, it's going to be much easier once you see it written in code, I assure you, it's not that difficult. Next, we have to handle the question of target network updates. So at the very beginning, in our constructor for our agent class, we're going to go ahead and initialize actor to critic networks, and then to target then a target actor and to target critic networks. And we first start out, we want to initialize those target networks with the exact values of the online networks. And so we're gonna have a special case in our target network update function that handles the very beginning of the program. every other time step, we're gonna use the following expression to update the weights. So on the left side, you have theta and five prime where the AI on the theta denotes either critic one or two, phi is the parameter for our critic network, excuse me Policy Network. And the thetas are the parameters for our critic networks. And so you're going to multiply town some small number of point 005, in this case, by the values of the current online network, and add in one minus tau times the old values of the on the critic network. So it'll be a small number multiplied by the current values of your online networks plus something that's almost one, multiplied by the old values of your target networks, it's going to be a slowly changing update to our target networks. Another thing to note is that we're only going to be performing this update when we update the actual network. So it's not every time step. In this case, it will be every other time step very, very important. So for this program, we're going to need a number of data structures, we're going to need a class for our replay buffer the agent's memory. Now, I like to use NumPy arrays is not the best way or the only way to do it, it's just my preferred way. So follow along with me in the tutorial, do it that way. And then when you play around with the code later, to understand it better, go ahead and rewrite the replay buffer to something that makes more sense to you. That's a great way to get started with modifying the program is with the replay buffer. Next, we're gonna have classes for our actor network and our critic network. And those are of course written in TensorFlow two, we have another class for our agent, and that is really going to tie everything together, it's going to have a memory that keeps track of transitions, it's going to have an actor to critics target networks for each of those a function to choose an action based on the current state, a function to learn that performs the update rules, we just went over an interface function with his memory that I call remember, just to store transitions in the agent's memory, as well as functionality to save models and perform target network updates, which I forgot to write here. Finally, we're gonna need a main loop to train and evaluate our algorithm. So we're going to be using the open AI gym and the bipedal Walker in particular, because this is a kind of difficult environment for other algorithms. Now, it's a continuous action space with a pretty large state space. I think it has 24 different components in the state space, if I'm not mistaken, some relatively large number. So it's a bit difficult for agents to learn. And in fact, it's going to take my computer around six or seven hours to complete the evaluation. filming this after I do the code, so it'll take a while to run. So if it takes forever on your computer, don't Don't panic. That's normal, quite normal. So all that out of the way, let's go ahead and get started in the coding portion of this tutorial. Alright, so we begin as usual with our imports, we will need NumPy the base TensorFlow package, we will need TensorFlow dot Kairos. We will need layers from important dense that is for constructing our deep neural networks. And we will need our atom optimizer for the gradient descent. And we will need LS for file joining operations for model checkpointing. Let's start with our replay buffer class. Now, this should be very familiar to you for deep use to deep q learning will need a max size and shape and number of actions as input to our constructor. Now remember, we're dealing with continuous action spaces. So there's number of actions is really a number of components to our continuous action. I just named it that for consistency with my deep q learning code. So we will save the appropriate number of variables. We use a memory counter instantiated at zero because our memory is finite. We'll need our state memory, which will initialize as zeros. The shape memory size and star input shape the star idiom just unpacks a, an array in this case, whatever the shape of our input, dimensionality is from our environment, we will of course need a new state memory. And that's the same shape that keeps track of the new states that we are going to see. And actually memory. And remember, of course, again, number of actions is number of components to or action. Reward memory in the shape of memory size, and a terminal memory. We're going to use NumPy NumPy NumPy bool as our data type, but don't keep track of our terminal flags, the reason being that the value of the terminal state is always zero. And so we keep track of the done flags from our environment to accommodate that. So let's store a transition takes the state observed action taken reward received new state observed and terminal flag received as input. We want to know what the first available memory position is. And that's the memory counter modules mem size that has a property that it will overwrite earlier memories with newer memories as soon as the memory fills up. And then go ahead and save our variables. All state underscore trawl memory. And that's all of them I believe. And very important, we want to increment our memory counter by one. Next we have to handle the function to sample our buffer. And that'll just take a batch size as input. We want to know what the position of our maximum filled memory is. And that's given by this minimum of memory counter and men size. Because we don't want to sample zeros, we initialize our memory with zero so we just sample the entire buffer, then we're probably going to end up sampling zeros until we fill up that buffer which is totally useless. So then the batch is going to be a random choice zero to maximum in shape batch size. Then go ahead and do you reference our variables and DUNS All right. wraps up our replay memory that's very, very simple, probably the most straightforward class in the entire project. Next, we're gonna move on to our critic network. And that will derive from Kerris dot model. So we get access to all of the properties of that particular base class. That'll help with using the gradient tape for learning later on. We're going to take some inputs for the number of dimensions for the first and second, second fully connected layers, number of actions again, number of components, a name for the purpose of model checkpointing, and a checkpoint directory, we will have to do a make directory on that Before you begin, otherwise, you will get an error and it will not work. Call our super constructor and start saving stuff. Do I need to do that? Now why not. So the purpose of the name is the fact that we're going to be saving target networks as well as regular networks. And they're going to be to critics. So we want to be able to keep all of those straight when we handle model checkpointing. So we'll save our model name, checkpoint directory, and the checkpoint file. And I like to append the algorithm name to the checkpoint files so that if I do everything in one working directory, when I'm experimenting, all the names, tell me exactly which file correspond to which algorithm, you don't have to do that. It's just my own personal convention. So for our deep neural network, we'll start with a dense layer with a rail you activation. Second dense layer with value activation and an output that will be single valued with no activation. Now keep in mind, one interesting thing about TensorFlow two is that we don't have to specify the number of input dimensions it infers it from the inputs. That's a pretty nice feature. So now we define our feed forward, on this case, we call it call and allow us to use the name of an object as a function call and basically need a state and action as input will have a q1 action value. And we will want to concatenate our state and action along the first axis. And we will feed that through FC to q1 action value. pass it through the final layer, action, value, and return. Now keep in mind that the critic evaluates the value of both the action and state. So that's why we have to concatenate the two values. That is it for our critic network. Very, very straightforward. Next, we're going to handle our actor network. And that, again, derives from Kerris dot model just the same as a critic network. Our initializer takes dimensionality as input again, as well. number of actions. And you know, what did I I'm sorry, I'm checking something here. No, I did not. For a second there, I thought I passed in in good shape to the critic network that would have been totally unnecessary. checkpoint directory equals temp, TD three, we want all of the models to live in the same directory very, very helpful. And then we can go ahead and start saving stuff. Looking at this and as I look at the what I'm doing here, so I like to modify stuff on the fly. I don't think I actually need this actions here. So let's go ahead and delete that. Just for the sake of cleanliness. We will need it on the actor of course. Yeah, let's keep it nice and clean model name, checkpoint directory, plus TD three. And just for clarity, that name will have stuff like Target actor, target critic, actor or critic, so we can keep all of those particular networks straight. And of course, the two critics will be critic one or critic two, because we have some very interesting naming conventions. So now we'll have our deep neural network. Again, a simple dense layer with raw you activations for the first two layers, and then you for our output. And that will take actions as our output dimensionality, with a tan hyperbolic activation, the tan hyperbolic is bound between minus one and plus one, if you want to take into account boundaries of actions that are beyond plus or minus one, you can multiply this output by the maximum balance for your environment. So some environments have a max action of plus or minus two, which of course, plus or minus one is oftentimes less than, you know, two. So you want to take that into account depending on the environment. So again, we did a call function to handle the feed forward. So pass our state through the first fully connected layer, second, fully connected layer, and pass that through the final layer and return it. So that is it for our actor network. Next, we need an agent class to tie everything together and to handle all the really interesting functionality. So our agent doesn't derive from anything. But our super constructor, excuse me, our constructor is going to take a whole slew of inputs. So we need a couple different learning rates. Reason being you want to accommodate the capacity for different learning rates for your actor and critic network. Sometimes they learn best with different learning rates. Input demos, you'll need that for your memory towel for your software update rule, your environment for a number of important variables from the environment. Default gamma is 0.99. The update act date up date actor interval will default it to every other iteration, a warm up of 1000 steps. Just a default value for an actions max size of a million transitions. layer one size 400 layer two size that's RFC one and two tins respectively. A batch size default on 300 and a noise of 0.1. Let's go ahead and start saving stuff. Since we will be adding in noise, we're gonna have to perform a clamping on our actions to make sure that the actions the action plus the noise don't fall outside of the allowable bounds of the environment, or below our memory, that's a replay buffer. Then we need batch size. We need a learn step counter that will need that because we're doing the delayed part of TD three we're going to delay the updates of the actor network. Every two every two different iterations of the update of the critic network to get the critic network time to converge. Then we have you know, I'm looking at my cheat sheet here are all a time step. Why do I have a time step? Excuse me one moment now I believe the time step is for the warm up procedure. We shall double check that later. If not, I'll come back and delete it in the GitHub. And we don't want to forget number of actions. Sorry, I write this code you know sometimes well in advance of doing the video because I get distracted by other stuff. And so when I come back to it, I don't always know what I was thinking that is a benefit of comments which I don't really do for this stuff. Sue me I probably should. We do need our update after iteration. And that is update actor. Interval excuse me, let me close my door. My toddler is rampaging. And next we can go ahead and start defining our actors And critics and the name will just be actor. Because we are quite creative. To maintain compliance with the pepp eight style guide. Let's go ahead and delete a couple spaces. Critic one is a critic network or one size layer to size. And we don't need number of actions there because I deleted it. So its name will be critic one. Likewise for critic two and again, the purpose of this is to handle the double q learning update rule. Next, we will need a target actor layer to size. Let me go ahead and delete spaces. What silly style guides and finally and name of target actor then we'll need target predict one with a very original name of target critic one. Similarly, critic net target critic two. And that is it for our network's Next we have to compile them because this is a TensorFlow two. And that is where our learning rates come into play. So we will use our atom optimizer with the learning rate defined by alpha for our critic, our loss would just be a mean. And our critic one learning rate of beta a loss of mean squared error and critic to have to do the same thing. Where equals beta do I need? Yes, I do. I do need to parentheses there. mean squared error. And then we will handle our target networks ness. Target networks nest. Next that is a tongue twister. So we have to compile the target networks, just by convention with TensorFlow two, we're not going to be performing any Stochastic gradient descent or Adam. In this case, on those particular networks, we're gonna be doing the salt network updates, but we still have to compile them. Nevertheless. That is just by convention. We read alpha loss equals mean target critic one. Okay, so that is all of our networks. So that noise will keep track of as well update network parameters with a default value equals one. I do that because on the first step of the update, we have to set the values of our target networks equal to the starting values of the online networks. And so we pass in a value tau equals one to perform an exact update or a heart update instead of a soft update. We'll handle that function toward the end. For now I want to get to the Choose action remember and learn functionality because that's where all the really interesting stuff is. So let's go ahead and choose an action based upon an observation of the current state of the environment as input. So far, a time step less than our warmup period. I yeah, that's why we need the time step to handle the warm up. As I suspected, lad, I didn't delete that, we're going to select an action at random, with just a normal distribution with a scale defined by our noise parameter in the shape of number of actions, comma, so we get a batch fare. And sorry, an array of scalars. Otherwise, we want to go ahead and convert our state to a tensor. And add on a batch dimension, that's just all the way that the inputs are expected to be fed into the deep neural network, we have to add that batch dimensionality. And I have to float 32 here must be due to some sort of precision thing that makes TensorFlow happy. So then when we want to pass our state through our actor network, and receive our mu, and we're doing that because it returns batch size of one, one scalar. Then we'll say mu prime, which is where we handle the noise equals mu plus MP random, normal scale equals self dot noise. And your prime is TF clip. Bye bye, because again, that noise could take us outside the bounds of our environment. So we'll plant mu prime between min action and Max action, increase our time step by one very important. And we want to return mu prime. Okay, that's it for our choose action. Now let's handle the simple interface function to remember a transition. So remember, state action reward, new state done. And we'll say memory dot store, transition, state action reward new state done nice just because we have to interface with the memory in some way, we don't want to have the agent class calling. You don't want to have the agent class interacting with private variables from your memory, that would be poor software design. So next, we handle the most interesting function in the whole program, which is the Learn function. And the very first thing we want to do is say, hey, if we haven't filled up at least batch size of memory, we probably don't want to be learning. So we'll say self dot memory, that meme counter loss and batch size. Now, if it's not, if it's not greater than the batch size, or equal to go ahead and return essence, we're doing a warm up, that won't be the case, because the batch size is just a few 100, the warm up is 1000. So by the time we get through the warm up, then we're already well into filling up batch size of memories. But if you decided not to do a warm up, then that would be important. So we'll start by sampling our memory. So memory, sample buffer, pass in our batch size. And then we want to convert all of those to TensorFlow tensors. And I have to be very pedantic with data types here. I think. There could be issues if you do not, as I recall, I think it barks at you about data types, because it expects certain types of floating point variables in some places and other types elsewhere. And we don't have to convert the Dunn's to a tensor because rod sticking that in the deep neural network, we're just using that as a multiplicative factor. So we can leave it as a NumPy array. Now we're going to handle our update to the critic network, because we do that every time step. And then we'll handle the update to our actor network. So we'll say with TF three and tape, and I'll have persistent equals true Oh, because I have two different networks. Yeah, so you need two different if you're using two different updates for one group, excuse me, we're using two different apply gradients for a single tape. And you need to pass any persistent equals true variable, parameter excuse me, or argument. Otherwise, you don't need that versus n equals true. We just have say a single network that you're performing an update on. So we want the actions according to Our target actor for the new states. And then we're going to go ahead and add on a noise parameter to that, that we're going to clip between the range of minus point five and positive point five. So clip by value MP random, normal 0.2 minus 0.5 4.5. And then we're going to go ahead and clip that again, because again, the addition of that noise could take the action outside of the bounds of our environment. And then we are free to go ahead and start calculating our critic values. So q1 underscore the critic value, according to the first target critic is the feed forward of the new states and target actions through the first target critic, cue to underscore is very similar. It's just the evaluation of the new states and target actions according to the second critic. Now, again, we're gonna have to go ahead and squeeze that output. And the reason is, is that our shape is batch size by one want to collapse to batch size. And we have to do that for q2 as well, excuse me, q2 underscore. And then we're going to need the the value of the states and actions the agent actually took, according to the regular critical one, excuse me, one and two networks. So we'll call those q one. And we'll just go ahead and squeeze those right away. Critic one state's actions squeezed along the first dimension critic to states actions, one and then we're going to say that our critic value for the new states is the minimum of q1 underscore, q2, underscore and then we're going to need our target value the Y from our paper rewards plus gamma times critic value times one minus dunnes that will set the value of the second term here gamma times critic value there should be an underscore there sorry to zero everywhere the done flag is true. And then we have our losses. So critic one loss karass losses got mean squared error between the target and Q one mean squared error target and que tu so that is it for the calculation of the critic losses. Now we have to handle the calculation of the gradients. Now, we don't have to do anything special for that. The TensorFlow package handles that for us. So tape dot gradient, the gradient critical one loss with respect to the critical one trainable variables. So I should be dot not an underscore I make that mistake frequently. Critic one critic underscore one dot trainable variables yeah that is right. And then we need the critic to great critic to loss critic to trainable variables Same deal then we need to go ahead and apply those gradients. So calling our optimizer dot apply grip gradients function and that expects a zip as input. We're going to zip The Critic one gradient and the self critic one trainable variables simply recruited to optimize or to gradient Okay. And then we want to increment our Learn step counter Because that gets incremented every time we update our critic networks, and then we have to address the question of, is it time to update our actor network. So we'll say if that learns step, counter modulus, self update actor, interval, it or sorry, is not equal to zero, then return so it's not every n steps, then go ahead and return. And if we haven't returned, then we're going to go ahead and calculate the loss for our actor network. So with TF gradient tape, as tape, and here, since we're just dealing with one loss, we don't have to call the processing equals true. We don't have to pass in the verses and equals true argument. So what's our new actions are the actions chosen by the current parameters or actor network for the current set of states, the states the agent saw along the way, the critic one value is self critic, one of those states and new actions. And then our actor loss. There's negative TF math reduce mean, critical one value. And this may look a little strange to you. But this is how we're going to handle the gradient of the output of one network respect to the parameters of a network, it's kind of like how you apply the chain rule to the to the loss of the output of the critic network with respect to the parameters of your target actor, your actor network, sorry. Oh, that makes sense. So then we do the same thing where we calculate our gradient gradient, tape dot gradient. Sorry, taped out gradient that I call, let me make sure I didn't make a mistake, I did make a mistake up here. Sorry. So this should be taped out gradient. Not TF. That is one less error to worry about when we get to running the program. Sorry about that. So take that gradient, factor loss factor trainable variables, step our optimizer by applying our gradients, again a Texas zip as input actor, gradient cell dot actor trainable variables. Okay, so then finally, at the end of the learning function, we want to update our network parameters. Okay, so that really handles all of the learning algorithm for our agent. All that's left now is to update our network parameters, and then handle the model saving so just a few functions left and then we can go right our main loop and see how it does. So network parameters. So we're going to pass in a default value of towel, Fernand, remember that at the top of our rather the end of our initializer we pass in tau equals one to handle heart update. every other time we're going to pass in a nun. So we'll say if tau is none, then tau i go self dot pal. So every time other than the first time we call this we're going to use the stored value for towel. So weights equals the list targets equal self dot target actor dot waits, I wait in enumerate self actor waits waits dot append, wait times tau plus targets sub i times one minus tau. There we go. And then sell dot target actor dot set weights and I am going to yank this and paste and paste again and then say target critic. One self dot critic one weights. Set wait Sorry, I forgot to set the actual via the weights how sloppy of mean, and then say CELTA Target critic, one set weights weights. And then we have target critic to numerate critic to weights, weight start append. And then so got target critic to set weights. And that handles our update rule for our two networks. Sorry, my Num Lock key is off there. Okay, so if it isn't clear what's going on here we are iterating over the weights of our actor and critic one critic two networks, then we're doing the calculation for the soft update rule saving that in a temporary list and uploading that list to the target actor or target critic one or critic two networks. Now, we can handle the Save model functionality. This is the easiest part of the whole project. So print saving models just a little debug statement to let us know something is going on. Save weights self dot accurate a checkpoint file that critic one checkpoint file, checkpoint file, and then we have our target numbers as well. Good grief. Then we do the inverse operation of loading our models. So let actor load weights from the accurate Check Point file. We have critic one load weights. So blog critic one dot checkpoint, firewall. And then we have our target networks. I should have chosen shorter variable names to save my risks a little bit of work here. But hindsight is always 2020 I guess. Target one, check one file. Okay, so that is it for the main code for our TD three algorithm. Now we get to handle the main loop. So let's go ahead and code that up. Of course, before we can have an invalid syntax right here at the very beginning. I'm TensorFlow dot Kairos dot layers. Oh, sorry. That's because it's a from import dense. I'll have to notate that in the video. And I have another issue here. def store transition, where is it unhappy? I am missing a comma. Of course. And I have another issue Oh s path joined. name equals my wrists are nonfunctional today. Cell dot target actor dot set weights. Oh, ah. Why did that happen? Interesting. Did I type those at the end and have a stroke or something to remember very strange Okay, unexpected and a file, let's delete their same. am I forgetting a parentheses somewhere? I am. Because I have right there. All right, finally, good green. That's a whole lot of typos. Now some people suggest that I upgrade my vim to actually catch that stuff on the fly. And you're absolutely right, I'm gonna do that. When I finally forced myself to do it, let's go ahead in the meantime and write our main loop. So we want to import Jim NumPy we'll need our agent. And we'll need our utility file plot learning curve, you can do a git clone on my GitHub to get that it's just a map plot live pie plot with some labeled axes, where we're taking an average of the previous 100 games running. I don't include that in all my videos, I just kind of reference my Get up. You can just do a plot, if you wish, name equals main gym dot make. We're going to be doing the bipedal Walker, v2. And we're gonna call our agent constructor, alpha 0.01, a beta of 0.001. Our input dimensions will be determined by our environment. So we don't have to hard code anything. Tau of 0.005 pass in our environment, batch size, I have 100 here equals 400 300. And n actions determined again, by our environment, all this bad would play 1000 games, and the call our file name plots, plus locker underscore. Now keep in mind, you have to do a make der plots and make your temp slash gt three to correctly execute the code. Because it'll expect that those directories exist. Pass a number of games as a variable for your file name. That way, if you run it over and over again, with different numbers of games, it won't overwrite the same plot. You can also include things like learning rates as part of your variable name for your file names, I recommend doing that. I'm just not doing it here. So we need to keep track of our best score subminimum the score of our environment. And the reason is, we want to save our best models. Keep track of your score history. If you want to load models, now's the appropriate time to do that. Actually, you know what we may need, there may be an issue where we have to actually instantiate our network with some variables to load the models. Open up an issue on my GitHub. If that's the case, and I will write the correct code. I won't bother with the video, I'll leave that as an exercise for the viewer. But if it turns out to be a problem, raise an issue and I can fix that. Not a huge deal. Let's go ahead and play our games. Start by resetting the environment at the top of every episode, we set the done flag and zero. And let's play our episode of Walmart done. action equals agent dot choose action based on the observation. Let's take that action, get the new state reward Don and debuginfo from our environment, call our Learn function. Keep track of our score and set the current state to the new state very important. If you don't do that, nothing is going to go well for you. As a punter a score at the bottom of every episode and calculate our average minus 100 onward. If our average score is greater than our best score, then set the best score to that average. And save our models and then we want to print some debug information episode on Score, one of average score average score. At the end of all the games, let's go ahead and handle our plotting our x axis is just the number of games and called plot learning curve. Alright, so moment of truth. Let's go ahead and see where I have my invalid syntax. I forgot an addition sign there. Very simple. Alright, let's try it. Oh, you know what I'm running it over in this other terminal. And okay, so it saves models it is learning. That is good to know. Let's do a make der temp slash TD. Three. I should already have plots. Python main Td three.pi. Moment of truth. Okay, so it says actor network object has no attribute checkpoint file. I didn't Oh, it's checkpoint directory. So that is in TD three. Do you have to that is in line 70. So that is here. Okay. Try it again. Got an unexpected argument name. That is in. Oh, that's because Okay, that is in line 48. That is super trivial. Thought name equals hits name. Plus that I do that down here as well. No, I did not. Not looking forward to editing this, this is going to be a lot of work. Okay, so it saves models right off the bat and starts running. Okay, so I'm gonna let this run for a while. And then we're gonna see how it does. Okay, so I did something very stupid. And I let it run and noticed it wasn't actually learning. And so that's a problem. And the reason it's not learning is because I forgot to store the transition. So we have to say agent, remember, observation, action, reward, observation, underscore and done. Okay, and then I got to get rid of the print statements I stuck in here, for debug purposes, because I'm a noob. And use debug statements. Okay. Now let's do Python, main Td three.pi. And now it should work without any funky print statements. Okay. Now I'm going to take off for a little bit and see what's going on. And the reason I noticed this wasn't learning is because it was executing much too quickly. I blasted through 350 games in just about a minute, which tells me it's not doing anything useful on the GPU. So let this run out, or probably take an hour or two, and then I'm gonna come back and see how it did. And we'll take a look at his performance. Now, here we are, it's the next morning. This took around six or seven hours to run. So I just waited until morning to film this. But in typical Phil's style, the filename for the function call for the plot learning curve function has a typo in it. And so we don't have an actual plot from the performance of this particular run. However, I will show you a plot of a similar run, where it achieved an approximately similar score, you can see that it does indeed learn it issues a high score of around 285 to 88, about 290 or so depending on the run, you get some run a run variation. And 300 is the highest possible score you can get for this environment. So I would consider this pretty much strong evidence of learning. It's not a world class results. But that's not what we were aiming for any way we just wanted to understand the gist of the algorithm and implement it correctly, and demonstrate that we do in fact understand how it works. Mission accomplished. You can pat yourself on the back for that. I'll also show you some footage of the walker kind of stumbling along so you can see how it looks once it's fully trained. You can see it has kind of a funny gait, but it does in fact managed to learn to walk that is pretty impressive starting from just totally random actions, learning how to walk within just six or seven hours only humans. Were so competent. I hope that was helpful. Leave a comment down below with any questions, suggestions, anything you'd like to see next, give a thumbs up, subscribe if you haven't already, and I'll see you in the next video. Welcome to a crash course in proximal policy optimization. Before we begin a quick shameless plug my Udemy courses on deep reinforcement learning specifically actor critic methods and deep q learning are on sale right now, learn how to turn papers into code, link in the description below. So proximal policy optimization or PPO for short, was created for pretty simple reason. And that is that an actor critic methods, oftentimes we see that the performance can fall off a cliff, the agent will be doing really well for a little while. And suddenly, an update to the neural network will cause the agent to simply lose. Its its understanding of how to play the game. And so performance tanks and never really recovers. Now this happens because actor critic methods are incredibly sensitive to perturbations. The reason being that small changes in the underlying parameters to our deep neural network, the weights, for instance, can cause large jumps in policy space. And so you can go from a region of policy space where performance is good to a region of policy space where performance is bad, just by a small tweak to the underlying parameters of your deep neural network. PPO addresses this by limiting the updates to the Policy Network. It has a number of mechanisms for doing this. But the basic idea is we're going to base the update at each step on the ratio of the new policy to the old. And we're going to constrain that ratio to be within a specific range to make sure we're not taking really huge steps and parameter space for our deep neural network. Of course, we also have to account for the goodness of state. In other words, the advantage how valuable each state is, and the reason being naturally that we want the agent to select states that are highly profitable to it over time, so wants to find the best possible states. Now taking into account the advantage can cause the the loss function to grow a little bit too large. And so we're going to be introducing a way of dealing with that by clipping the loss function and taking the lower bound with the minimum function. Something else we're going to be doing that's different than what you may be used to is that instead of keeping track of something like say, a million transitions, and then sampling a subset of those at random, we're going to be keeping a very small fixed length trajectory of memories. And we're going to be doing multiple network updates per data sample using mini batch Stochastic gradient descent. It's worth noting that you can also use multiple parallel actors on the CPU something like what you would do in a three C, but we're not going to deal with that, in this particular tutorial, I'm just going to show you how to do the GPU implementation. So let's talk about the mini batch gradient sent for a second. So we're going to keep track of a list of memory indices from say zero to 19. And that's for the case of taking a look at 20 transitions. And let's say we want to take a batch of size five. And so those batches could start at position 05 10, or 15. Those are the only possible positions were distorted such that you get all the memories, you don't get any overlap, and that it all works out evenly. So what we're going to do is we're going to shuffle our memories, and then take batch size chunks, so we'll start at position zero, from zero all the way up to four, that is one batch. And then position four, five, up to nine is the next batch, and so on and so forth. It's relatively straightforward when you see it in code, but it's kind of difficult to explain as you're coding it. So just know that we're taking batch size chunks of shuffled memories for mini batch Stochastic gradient descent. Other things we need to know is that we're going to be using two distinct networks for actor and our critic instead of having a single network with shared inputs and multiple outputs. Now, you certainly can use a shared input with multiple outputs, but it complicates the loss function a little bit. And I found that performance is generally adequate with two distinct networks for simple environments. So the critic will evaluate the states that the agent encounters and it gets the name critic because it literally criticizes the decisions that the actor makes, based on which states it ends up in. So it says, Hey, this particular state was valuable. We did good, or this state is stupid. We did bad do better next time. Now this is in contrast to state and action pairs for something like say deep q learning, but it's in line with what other actor critic methods use And of course, the actor decides what to do based on its current state. So our network is going to output probabilities using a softmax activation. And we'll use that for a categorical distribution and pytorch. So we'll have, in the case of the card poll, we'll have a couple actions, and some probabilities selecting each action. And then we will use the probabilities determined by our deep neural network to feed into a distribution that we can sample and use for the calculation of the log probabilities more on that momentarily. It's also worth noting that exploration is going to be taken care of for us, due to the fact that we're using a distribution. So it's probabilistic and is set up so that each element has some finite probability. So even if the probability of one action and goes arbitrarily close to one, the probability selecting the other action stays finite so that at least some of the time it's going to get some exploration. This is in contrast to something like say epsilon greedy action selection and deep q learning, where you select off off optimal actions about 10% of the time. As I said earlier, our memory is going to be fixed to a length of capital T. In this case, we'll use 20 different steps, we're going to keep track of the state c agencies, the actions, it takes rewards, it receives the terminal flags, the values of those states, according to the critic network, and the log of the probability of selecting those actions that'll become important later in our update rule. As I said, we're going to shuffle those memories and sample a batch size of five. And we're going to perform a four epochs of updates on each batch. Now, these parameters are chosen specifically for this particular environment. And that's one of my criticisms of PPO is that there are a number of parameters to play with hyper parameters. The memory length is one hyper parameter, the batch size, and number of epochs, as well as learning rate. And another parameter we're going to see later, all play roles have hyper parameters in our model. And so there is a lot to tune here. But these parameters work really well for the carpool environment, so you won't have to do any tweaking for that. Other thing to note is that this memory length, capital T, should be much less than the length of the episode. So in the case of the carpool, the maximum episode length is 200 steps. And so 20 steps is significantly less than that. So I think it qualifies, you wouldn't want to use something that encompass more than one episode, for instance, that would probably break the algorithm and result in poor performance relative to using a capital T much less than the episode length. So all this is relatively simple. But what isn't so simple is the update rule for our actor. So here's where all the math comes in. So we have this quantity loss, the CPI, the stands for conservative policy iteration. And it's given by the expectation value, which is just an average of the product of the ratio of the policy under the current parameters to the policy under the old parameters multiplied by this a hat sub t, one that in a second. When they do that, and they just abbreviate that ratio is r sub t. Now, if you're not familiar with deep reinforcement learning or reinforcement learning, in general, the policy is a probability distribution. That is what our actor is attempting to model is a probability distribution, the policy. And this policy is a mapping between states and actions and probabilities. So given your in state SMT, and you took action a sub t, what was the probability of selecting that action, according to the distribution, and so in the denominator, we have theta old, that is the probability of selecting action a sub t given state SMT, under the old parameters of your deep neural network, so we're going to play 20 steps, and then the agent is going to perform a learning update. And it's going to do mini batch Stochastic gradient descent. And so after computing that first batch, the parameters of the deep neural network change, right, that's all the batches work, you compute the loss with each batch and update your parameters. And so right after you've calculated that first batch of memories, the loss for that and updated your deep neural network, the theta changes, and so the policy pi is going to change as well. So we have to keep track of the parameters, excuse me of the the probabilities, of selecting each action at each time step in our memory. And then on a learning function, we're going to pass those states through our actor network, get the probabilities, the probability distribution and find out what the probability of selecting action a sub T is sampled from our memory according to the current values of the deep neural network. It'll be a little bit more clear in code. Just know that we have to keep track of log prompts. As we go along and we're going to be recalculating them in the learning loop. One thing we also see is that it takes into account the Vantage, which is at a hat sub t. So the advantage is just a measure of the goodness of each state, we'll get to the calculation of that in a few minutes. But one thing to note is that this ratio, pi sub theta, or pi sub theta old, can have an arbitrary value, right? Because you could have, let's say, pi, theta being point nine, nine, pi theta old point 01. And so that's a pretty large number. And in particular, if you multiply it by an advantage, that is like, say, 1020, of whatever, then that can also still be a large number. And so we have to deal with that, right? Because the whole point of this is that we want to constrain the updates to our deep neural network to be some relatively small amount. And so the way you deal with that is by adding an additional hyper parameter epsilon that you use to clip that ratio. So what we're going to do is we're going to clip that ratio within the range one minus epsilon two plus one plus epsilon. So let's say from 0.8, to 1.2. So that ratio is going to be constrained to be close to one. And you're going to multiply that by the advantage. And so that'll give you some number. And then you want to take the minimum of that clips number, the clipped ratio multiplied by the advantage, and the unclipped ratio multiplied by the advantage, take the minimum, and that is what we will use for the loss for our actor network. So this serves as a pessimistic lower bound to the loss. And they don't go into any real depth in the paper on the reasoning for this. But to my mind, and this could be wrong, you know, I am an idiot sometimes, but my understanding is smaller loss, smaller range and smaller update. That's the whole point of it. So let's talk about this advantage now. So this advantage has to be calculated at each time step and is given by this equation, don't freak out, this is relatively straightforward. Once again, it tells us the benefit of the new state over the old. Well, how do we know that we know that because it's proportional to or equal to the sum of the Delta sub t with a Delta sub T is just the reward at a time step, plus a difference in the estimated value of the new state and the current state. So it tells you, what is the difference in the value between the next the next state we encounter and the current state. And of course, you have the gamma in front of the V, which is the output of the critic network, because we always discount the values of the next states, because we don't know the full dynamics of the environment. And so that that reward is uncertain, there's always some uncertainty around state transitions. And then in the top equation, you just sum that, where you're going to be summing over gamma multiplied by lambda. So this quantity gamma is again, the normal gamma 0.99 that we typically use. But this parameter lambda is a type of smoothing parameter, it helps to reduce variance, and we're going to use a value of 0.95. And for implementation, we're just going to use a couple of nested for loops. So you're going to start out at time t equals zero, and then some from that step all the way up to capital T minus one. So if we have 20 states, you're going to go from zero to capital T minus one, zero to 18. And you have to do that because you have the V of S sub t plus one, you don't want to try to evaluate something beyond the number of actual states that you have, that won't work out, right. And so it's going to be relatively straightforward once you see it in action. And we're going to be keeping track of that gamma times lambda, which is a multiplicative constant that increases its power by one with each iteration of the inner loop. All that'll be made clear in the code. But fortunately, the critic loss is a little bit more straightforward. So we need something called the return. So the return is just equal to the sum of the advantage and the critic value based on the memory. So whatever the agent estimated the value of a particular state to be at the time that it took it is what we're going to be using for the critic value in our return. And then the loss of the critic is just going to be the mean squared error between the return and the critic value based on the current values of the deep neural network. So once again, we're going to be passing the state to the critic network to get its estimate of values. And we're going to be also using the values from the memory as well. So relatively straightforward, even easier when you see it in code. So we have two different losses, and we have to sum them, and so that'll be the sum of the clipped actor and critic. So a couple things to note here is that one, we're actually doing gradient descent. And so the coefficient of C one for the loss of our critic is going to be positive and the loss of our actor is going to be negative because We are doing gradient ascent and non gradient descent we have to multiply by negative one other thing to note is that we have this other parameter here, C to this coefficient multiplied by S, S is an entropy term. And that only comes into play when you have a deep neural network with shared lower layers and actor and critic outputs at the top. So we don't have to worry about that, in our particular implementation in this tutorial, because we're doing two separate networks for the actor and the critic. And I'm going to use a coefficient of 0.5 for the loss for the critic. As I said, we're not going to be implementing the entropy term, because we're doing two distinct networks. We can also use this for continuous actions there, you would use a different output for your actor network. And indeed, that's what the paper really is geared for, is for continuous action spaces. But we're going to be doing the very simple discrete case. Other thing we don't implement is the multi core CPU implementation. Because that introduces even more complexity, we're just going to be using the GPU. So what do we need for this project, we're going to need a class for the replay buffer. And we're just going to use lists for this. Normally, I like to use NumPy arrays, but in this case, lists turn out to be a simpler implementation. So that's what we're going to go with. We're also going to need a class for our actor network and a class for the critic network. We'll need a class for agent that's going to tie everything together that'll have actor and critics that invoke actor and critic constructors as well as a memory for storing the appropriate data. It also functions for choosing actions, storing memories, saving models, and learning from its experiences. And then a separate file, we're gonna have a main loop to train and evaluate the performance of our agent. Before we get into the coding section, I want to do a quick shout out to William Woodall, he hangs out in our discord channel, which is also linked in the description below if you want to come hang out with some really, really smart people who talk about artificial intelligence ranging from all sorts of different things every single day, check out link in the description for the discord. So William came to me and said, Hey, Phil, I found an implementation of PPO that I find to be in line with your general philosophy of software minimalism. And he showed it to me, and I looked at it, and it helped clarify quite a few questions I had after reading the paper. Now, the software you see here is pretty much my own code. But it was inspired by Wayne Woodhouse code. So shout out to him for helping me out on this because the paper really isn't all that clear to me, even after reading it a few times. Other thing I want to say, and I'll talk a little bit more about this in the coding section is that when I normally define deep neural networks, actors and critics, in particular, I will use the convention of saying self dot layer name equals n n dot linear self dot layer name Next, you know, equals and n dot linear. And then I'll write the feed forward function where you use the member variables that self dot layer one as something you can call as an object to call, and then calling activation functions within that. Now, what I found is that doesn't really work very well. In fact, I have to use an n dot sequential to create the models for this. And that's one of the biggest takeaways I had from we would always code is that by using the nn da sequence where you really get this thing to work. And for whatever reason, I cannot get as good a performance using my conventional, typical way of writing these networks. Now that I can't think of any reason why that should be the case. But it is something I've observed, I tested it, just altering that one chunk of code, how I defined the models, and running it several times to take into account run to run variation. And it seems to be repeatable for me. So maybe, I don't know, maybe it's a configuration issue on my system. Maybe it's something I'm doing wrong elsewhere. I don't know I don't think so. All of that out of the way. Let's go ahead and get into the coding portion. All right, so let's go ahead and jump right into it with our imports. They're going to be pretty light will need us to handle file joining operations NumPy for NumPy type stuff, and all of the torch packages will need an N for sequential model. We will need up Tim and we will also need our categorical distribution. So we'll start with our PPO memory class. And this will be pretty simple. For the most part. The only input for our constructor is a batch size. And we will just implement the memory with lists. So we'll keep track of the states encountered the log process. I'll just call it prompts for brevity, the values that our critic calculates the actions we actually took the rewards received, and the terminal flags. So next, we need our function to generate our batches. So our strategy is going to be the following, we're going to have a list of integers a correspond to the indices of our memories. And then we're going to have batch size chunks of those memories. So indices from zero to say, four, and then five to attend, so on and so forth, or whatever our batch size is, we're going to shuffle up those indices, and take those batch size chunks of those shuffled indices. So the first thing we need to know are the number of states we are going to want to get our batch start list or array, I suppose that'll go from zero to n states and batch size, steps, batch size. It would help if I could type our indices. And that is just the number of states in our trajectory. We run on a shuffle that so that we handle the stochastic part of the mini batch and Stochastic gradient descent. And then we can go ahead and take our batches using a list comprehension. So it's going to be those indices, from eye to eye plus self dot batch size for i n, brain for AI, N, batch start. So it's going to take all of the possible starting points of the batches, either 05 10, etc. and go for in the indices from that all the way up to AI plus batch size. So we're going to get the whole batch from our indices, then we're going to want to return an array for each of those. And this gets a little bit messy. Let's be very careful not to mess up the order. Because of course, the order in which you returned the memories definitely matters later on. We'll need rewards. And then we're also going to want to return the batches. And the reason why will become apparent later. It's because we're returning the entire array here. And we're going to want to iterate over the batches. So now we need a function to store a memory. And that'll take a state action, probability, value, reward, and done as input. And all we're going to do is append each of those elements to the respective list. Not as reward singular. And then finally, we need a function to clear the memory at the end of every trajectory. And I forgot the self argument here. And mini rant here, I really don't like some aspects of Python. It took me much longer than I would care to admit to get this to run, not because the algorithm I implemented was incorrect. But because I had a mismatch. So here I had, I believe action. And up here it was actions or promise vice versa, and my original implementation, so it didn't flag as an error. Because it's not really an error, particularly where Python is concerned. And so it was quite a nuisance. Pretty, pretty painful to track that down. Of course, if we're more strongly typed language, then that wouldn't be an issue. But I digress. So now let's handle our actor network. And that will derive from the base nn dot module class. Our initializer is going to be pretty straightforward. We're after we will need the number of actions. The input dims a learning rate alpha number of fully connected dims for the first and second fully connected layers and a checkpoint directory. And we're also going to need to call our super constructor And then create our checkpoint file, checkpoint directory and actor, torch PPO. Now, I do it this way, because I'll often do development in a single root directory. And I don't want to get models mixed up. If you have a different way a more organized way of writing software, then you could perhaps skip this path joint operation and just use a file by itself. But let's move on to the actual deep neural network. We're going to want a linear layer that takes starred but dim, so we're going to unpack the input dim, so we have to pass in a list. And it's going to output FC one dims. Array you activation function, another linear layer that takes FC one dims as input outputs FC two dims. That gets a raw you activation as well. Another linear layer that takes FC two dims as inputs and outputs a number of actions. And then we're gonna use a softmax activation along the minus one dimension. So that's the whole of our actor network. The softmax takes care of the fact that we're dealing with probabilities, and they have to sum to one. So our optimizer is going to be an atom optimizer, what are we going to optimize the parameters with learning rate of alpha, of course, we need to handle the device, which would be our GPU if possible, then we want to send the entire network to the device. Next, we have our feed forward function. And that'll take a single state or batch of states as input. So we want to pass that state through our knee deep neural network and get the distribution out. And then use that to define a categorical distribution, which we are going to return. So what this is doing is it is calculating a series of probabilities that we're going to use to draw from a distribution to get our actual action. And then we can use that to get the log probabilities for the calculation of the ratio of the two probabilities in our update for our learning function. Then we have a couple of bookkeeping functions save checkpoint. We're going to want to say, torch dot save the state dictionary for our network. And we're going to say that into a checkpoint file, then we need to load checkpoint. And that is self dot load state dictionary. What are we going to load a checkpoint file. And that's really it for the actor network, it's pretty straightforward. The critic network is also straightforward. And that also derives from nn module. Here, we don't need the number of actions because the output of the critic is single valued, it just outputs a value of a particular state. So it doesn't care how many actions are in the actual space. But it does need a learning rate alpha. It does need some dimensions 256 and a checkpoint directory. And then we need to call the super constructor. And same deal the checkpoint file the check point directory and critic torch. So that way, we can differentiate between the actor and critic model files. And we will again use a sequential model. And so in the shout out I was talking about William moodles implementation, as well as something else I observed. So what I meant by the alternate method of doing a model was if you say self.fc, one and in linear you know, if you do it that way, you have FC one FC two the separate layers defined without the sequential model. It actually does significantly worse than if you do it with the sequential model and I don't know why I don't have a certainly there's no theoretical reason they should do it. It must be something under the hood with the way in which pi torches and Limiting things. And it's no disrespect to the creators of pytorch. But this is one of my, you know, one of my biggest gripes with using these third party libraries is you never know how they're implemented. So something doesn't operate the way you expect, you can certainly go look it up, it's open source. But that is much easier said than done, right? You have to be familiar with not the entire code base, but a really significant portion of it to be able to make sense of a single file or a single way of doing things. So it really makes things opaque. It's an abstraction on top of an abstraction. And so I don't know, it's part of the good part of the it's the bad that comes with a good for having, you know, a robust library like pytorch. But I do it this way, because it seems to work the best. And as an aside, I also can't get it to work very well in TensorFlow two. And I suspect the The reasons are related because the performance of the TensorFlow two is on par with the type of performance I get from doing it the other way, where you just define individual layers instead of a sequential model. So pretty interesting stuff, maybe one day, I'll get super motivated, and decide to go ahead and figure it out. But I wouldn't hold my breath on that. So this is going to be very similar model, linear layers with relu activations in between, the main difference is that our output layer is going to be a linear layer with no activation, and a single value output. Now, of course, it handles the batch size automatically. So if you pass in a batch, you're gonna get the batch of outputs as well. Again, we need our optimizer with learning rate of alpha. As an aside about the optimizer, I'm going to use the same learning rate for both the actor and the critic. And it's entirely feasible and possible, and perhaps even advisable to use separate learning rates for both the actor and the critic. At least in something like deep deterministic policy gradients, you get away with a much larger, you know, by a factor of three or so learning rate for your critic than you do the actor. Reason being. As we outlined in the lecture, the actor is much more sensitive to changes in the underlying parameters of its deep neural network. Now, ostensibly, or theoretically, the, the PPO method should account for that and allow you to use a similar learning rate, because the actor should be less sensitive than in the case of ddpg, but I haven't tested it. So one thing you can do in your spare time is play around with different learning rates for both the actor and the critic. Our forward, feed forward function is pretty straightforward. You want to pass a state through your critic network and return that value. And we're going to need saving and loading checkpoints, I am just going to yank and paste those because the functions are otherwise identical. And so that is it for our two networks. Now we come to the heart of the problem, which is the agent class. Do I have an extra? Yes, I do. And this, of course, does not derive from anything. This is our base agent class. With a number of actions, a default value for gamma, which is the discount factor in the calculation of our advantages. Typically, we use something like 0.99, a learning rate of 0.003 minus four, I got this from the paper. So if you read the paper, they do give you the hyper parameters and a little bit of detail on the networks they used. But it is not a very well written paper, it's rather obtuse. So I'm not a huge fan of it. But we do have some good default values from it. So policy clip. So in my cheat sheet here, I have a value of 0.1 as a default, although in the paper, they use 0.2, perhaps I was experimenting, I will have to be careful with that a batch size of 64, a default and a 2048. And so that is the horizon, the number of steps before we perform an update, and the default for the number of epochs. Now these parameters come from these parameters come from the values for continuous environments. So the actual numbers we're going to be using are going to be significantly smaller, as I said, we use an N of 23 ybox batch size of five instead of 64. And I'm going to go ahead and set that policy clip to 0.2. Now that I'm looking at it, we need to ga lambda that is the lambda parameter. But of course you can't use lambda because that is a reserved word for Python. What else do we need? Um, yeah, I think I'm missing something in my other file here. That's okay. I'll fix it on the fly. So then we go ahead and save our parameters, number of epochs and RGA, lambda. Meet our actor network dems and learning rate takes input dims. And alpha, baby memory. batch size input one second. All right, hopefully that is not as loud now. The toddler is playing with his grandparents always a hoot. So now we need a function that handles the interface between the age and its memory. And it's just going to be very simple self memory store memory. It's just an interface function, then we need a function to save our models. Print saving models is just going to be an interface function between the agent and the Save checkpoint functions for the underlying deep neural networks. And very similar for the load models function. We have a visitor. All right, that is it for our bookkeeping functions. Next, we need something to handle choosing an action. That'll take an observation of the current state of the environment as input. And we want to convert that NumPy array to a torch tensor. And we're going to add a batch dimension because the deep neural network expects a batch dimension. And we'll be sure to specify that it is float. And then we're going to go ahead and pass that through our neural networks. So dist equals self dot actor state, that'll give us our distribution for choosing an action, we need the value of that particular state. And then to get our action, we just sample our distribution. And then what we want to do is go ahead and squeeze to get rid of those bash commands. And this might be something I added. For TensorFlow two, I'm not I don't remember if torch requires it, but it doesn't hurt anything. So for the prompts, you want to go ahead and return the log probability of the action, we actually took that item, so that item will give you an integer. And likewise, for the action, we want to squeeze it and get the action the item out. And similarly for the value and then just return all three. So this will make our main function look a little bit different than we're used to, because we're going to be accepting three values from our transaction function instead of one. But that's necessary for keeping track of the probabilities and values as well. Next, we come to the meat of the problem, so to speak, our learning function, so we want to iterate over the number of epochs. So we're going to have, in this case three epochs. At the top of every epoch, we want to get our arrays, the old probabilities, the values, the reward, the dance and the batches. Do that and then I'm just going to use a different note. tation here, and go ahead and start calculating our advantages. So our advantage is just going to be a NumPy array of zeros. Len reward, type MP float 32. And we're going to say for T and range, so for each time step, Len a reward array minus one, because we don't want to overwrite the, our go beyond the bounds of our array, our discount factor is going to be one, the advantage of these times have starts out as zero. So we're okay in range. So we're going to start out at T and go from t to the same reward array minus one and say a sub t plus equals discount so that ga times lamda factor, which starts out as one times we need parentheses, or array sub k, plus saltdogg, gamma times values k plus one times one minus and dot array, okay, minus values sub k, then we say discount times equals self dot gamma times G, or lambda, at the end of every calculation, the end of every case steps advantage sub t equals at a sub t. And at the end, we're going to turn advantage to a tensor. In particular, a CUDA tensor. And this is just a strict implementation of the equation from the paper. So this, right here, in parentheses, is the Delta sub t. So it's a reward plus gamma times v sub t plus one minus V sub t, where, you know, we swapped the K and T here, and you need the one minus dunnes on the values as a multiplicative factor the vize of a T sub t plus one, because the value of the terminal state is identically zero, that's just a convention in reinforcement learning predates the deep neural network stuff is just how we handle it, it's assumed that's why they don't put it in the calculation, it is assumed it's just a matter of convention. And then that discount is the GA the lambda multiplied by the gamma, that takes care of the multiplicative factor. So it is the the gamma lambda to the T minus one power, or is it t minus k minus one, something like that power, multiplied by the Delta, and then you're summing it all up. So now we have our advantage. I'm going to convert the values to a tensor as well. And I fully admit here that going from Val's array, to you know what, in fact, let's do this. Now, let's see what the weight is, may not be the most effective, or excuse me, the most efficient way of doing it, but sometimes, I just get stuff to work. And then don't go back and clean it up. If you want to clean it up, please do. So I always invite that. And it looks like I'm missing something here because it is not automatically indenting. So I'm probably missing a parenthese somewhere and it is right here, I believe, if I'm not mistaken. Yeah, there it goes. Alright, so then states, it's just going to be a tensor state array, sub batch, the type to float to salt actor dot device. And we're kind of violating the pep eight style guides their style guide by going beyond 80 characters. But I think we'll be all right old probabilities gets converted to a tensor. And I don't need an explicit D type there. I don't think that vice to salt that accurate advice that works and then actions. Okay, and then. So we have the states we encountered the old probabilities according to our old Vector parameters, the actions we actually took the next parameter we need. So we have the bottom of that numerator pi theta old, we need pi theta nu. So we have to take the states that we encountered and pass them through the actor network and get a new distribution to calculate that new, those new probabilities will also need the value of the the new values of the states according to the updated values of the critic network. So you may as well get those now. And we can squeeze those. And then we can calculate our new probabilities and take the prob ratio. So here, I'm going to exponentiate the log probs to get the probabilities and take the ratio, you could also do this. Those two are equivalent by the properties of exponent, exponent exponentials, excuse me. And then we're going to calculate our weighted probabilities. And sorry, our probability ratio. Now, yeah, the weighted probabilities, think I have two lines that do the same thing in there, that's fine. That's going to be the advantage batch times a probability ratio, and we need the weighted clipped probabilities. And that is going to be the clamp of the proper ratio between one minor self dot policy clip and one plus self dot policy clip multiplied by advantage sub batch. Now our actor loss is going to be the negative minimum of the weighted probs or the weighted clipped Prague's that mean, and our returns for our critic loss are going to be the advantage plus the proviso for that particular batch. And so our critic loss, then is going to be the returns minus critic value squared. And the mean value, our total loss after loss plus 0.5 Times critic loss. Remember, we're doing gradient ascent and there's a negative sign in front of the actor. So we're not doing descent that's another thing that's kind of suboptimal by the way the paper is written, you can get kind of confused about negative signs if you're not paying very careful attention. Next, we have to zero our gradients. I think you can probably hear that my son is giving a concert downstairs, he's playing the drums by whacking on his toy box with some drumsticks. So we're going to back propagate our total loss. And then step our optimizers. And finally, at the end of every epoch Yeah, I think that's the right indentation, we want to clear our memory. So at the end of all whoops, at the end of all the epochs we want to clear our memory. Let me just make sure I'm not doing that. Eg POC No, I'm not okay. That is good. So now let's do a write quit. And I have an indentation error here. I see. Oh, that came in when I did the yank and paste. Alright, so that is it. For our agent file. Let's go ahead and take a look at Main. So we start with our imports. We'll need a gym will need NumPy to keep track of the right Average our scores from PPO torch will need our agent. And if you're new here, I have a utility file that I use a map plot live pie plot function to plot the running average of the previous 100 games. For the learning curve, it's pretty trivial, you can just do a plot of the running average, just do a git clone. If you want to use my exact version, I don't go over it in every video because it's kind of redundant, but I'll leave a link in the description to the GitHub. So go ahead and do a clone of that. So you have that file or just write your own. So we're going to use the very basic cardpool v zero. Reason being, we don't need to spend a whole lot of time on a very computationally complex environment to realize we made a mistake. So it's very easy to see if something got screwed up with the card pole environment. This certainly will work on more advanced environments. But it does require a little bit of fine tuning. So we'll just start with the card pool. And then you can play around with other environments at your leisure. So we'll use parameters I dictated in the lecture, I think I change the number of epochs to four to three, we get the number of actions directly from our environment, very handy. Pass in all the other relevant parameters got a number of input dimensions from our environment. And we're only going to play 300 games as I'm looking at this, I do realize that the parameters I did the last time I ran, I did do a policy clip of 0.1. The 0.2 comes from the paper at hand. I'm pretty sure it works both ways. So we will find out if we need to, we can go back and change a policy clip. Not a big deal. So plot slash card poll dot png need to keep track of our best score this minimum score for the environment, empty list for a score history. And number of learning times we call the Learn function, you can make this a member variable of your agent if you want. And an average score starting at a zero we don't actually need that, but whatever. So we'll say at the top of every episode to reset our environment. So the terminal flag to false and a cumulative score to zero or we're not done, we need to choose an action based on the current state of the environment, get the new state reward done and debuginfo back from the environment, increment our score by the reward and store that transition and the agents memory worn and done. And if n and do need an extra variable here say n steps equals zero and that's the number of steps to take. And we need that because we have to know how often or when it's time to perform the learning function. So every time we take an action, the number of steps goes up by one. So then steps modulus, n equals zero then agent dot learn fitters plus equals one. And then no matter what happens, we want to set the current state to the new state of the environment. At the end of every episode, append our score and calculate our mean that's the previous 100 games then average score is better than the best known score then set that The best score to the current average, and save your models. And we also want some debug information is so I score pore size should be an average score. The I like to pronounce this isn't necessary. But the number of steps that have transpired in total, and the number of times the agent has called the learning function. This gives you an idea. This is I did this because when I compare with the results of the paper, it wasn't clear to me if they were talking about the number of times they called the learning function or the actual absolute number of time steps in the environment. So I print out both these time steps and learning steps are totally optional. You don't have to print it out. It's not something that is required. So I just do it for my own clarification. We need an X axis for our plot. One score history and plot learning. Oh, certainly. Let's do this. We don't want to do it every single game. We want to do it at the end of all games. All right. Now moment of truth. Let's see how many typos I made. So it's telling me it got an unexpected argument, input dims. That's interesting. What do I call it? I don't have it there. And the reason is, my computer had a hard lock up and I had to do a reboot. And it mutilated my cheat sheet for this. So there's bound to be some errors in here. I didn't do my make directories. So temp, PPO and plots. Let's try it again. Named Dunn's array is not defined, it's probably done array that is in line 161. So it is Dunn's array. Yeah, just change it there. I guess. Old property is not defined, that's probably the same thing. Where am I? Yeah, old prop array. I'll do the opposite here. I'll make it singular, just for just for the sake of not being consistent. Index eight is out of bounds. Okay. So then something has gone extremely wonky with the generation of the batches. Okay, let's take a look at that. Oh, wait, let's read this a little bit more carefully. It says index eight is out of bounds for access zero with size zero. So our action array Oh, you know what? Let's take a look at our memory. So it's action array. So here we have self dot actions. We return the self actions. Self thought, there we go. That's why. So action and action. All right. Now let's try it. Name advantage is not defined that as a typo, that is in line 183. A advantage today, yes, try that. try once more. has no memory underscore clear memory, it's memory. Clear memory 197. Alright, so now it is running. So I'll let that go for a few minutes and we will see how it does. Alright, so it has been running for a little bit, maybe just a few minutes. Now it runs relatively quickly. And what I'm seeing is that we do get some oscillations in performance, you see, it'll hit 200 for, you know, several games in a row, and then it'll drop down into the mid one hundreds, even, you know, 66, something relatively low like that. And there's a little chunk here where it dips below 100 points. So it's not a silver bullet, but it looks to be recovering. So we'll give it another 80 rounds and see how it does. Okay, so it has finished up. And you can see that it finished strong with a long run of about 50 games 45 games have a score of 200. So I commented, when I was writing the agent that I was looking at my cheat sheet and had a policy clip value of 0.1, it could be that I'd settled on that value based on some experimentation, and then changed it back just to be more consistent with the paper for this particular video. So that's something I would play with. Other thing to consider is that there is significant run to run variation that is a facet of pretty much every algorithm in deep reinforcement learning, it just has to do with the way the neural networks are initialized, as well as how they number a random number generators initialize the environment. So when you see papers, you'll typically report average values for say, five or 10, or whatever number of games and then a band to show the range of variants for run to run variation. But this is clear evidence of learning in order to achieve the score of 200 and under 300 games. So I call this good to me this is fully functional. Now there are a number of things you can do to improve on this, you can get it to work with continuous environments, you can bolt on some stuff for doing Atari Games, where you would need to add in convolutional neural networks as your input layers, and then flatten them out to pass them to a linear layer for the actor and the critic. And you can see my earlier video on an AI learns to be Pong for Q learning. There, I go over all of the a lot of the stuff you need to do to modify the open AI, gym Atari environment to do a frame repeating that something they do in Q learning. That's an exercise to the reader. Actually, I don't know, thinking back to the paper, I don't recall if they actually do any frame repeating or not in this particular algorithm PbO. But it's just something to look at anyway. So there's a number of things you can do to improve upon it. I haven't added this module to my course yet. I'm still working on it, I really want to take some time to give more thought to the paper because the paper isn't very well written. And I'll probably have to do a lecture like what I did for this YouTube video in the course, because the paper isn't very easy to implement just by reading it. So I hope that was helpful. That is PPO and just a few 100 lines full implementation in pytorch. Solving the carpool environment, then you can easily modify this to do other environments as you wish. If you've made it this far, please consider subscribing hit the bell icon, leave a like a comment down below. And I'll see you in the next video. Welcome, do a crash course and soft actor critic methods, you're going to learn the least painful way to quickly implement the salt actor critic algorithm using TensorFlow two, we're going to implement this and tested on the PI board environment, the inverted pendulum, because it's relatively quick to compute, it runs pretty fast. So we'll know whether or not we got it right relatively quickly. So what exactly is assault actor critic algorithm? So this algorithm sets out to address a pretty fundamental issue in deep reinforcement learning, which is how do we use maximum entropy framework in actor critic methods? We're going to go a little more detail into this in a moment, but that is the basic idea behind what we want to do. So why would this even be something worth considering? Well, as you may be aware, actor critic methods have a number of fundamental limitations, not the least of which is the fact that they have what is called brutal convergence, meaning that they suffer from a high degree of hyper parameter tuning. If you go monkeying around with hyper parameters, the agent breaks and doesn't know what to do in the environment. And so once you find a set of hyper parameters, you really have to stick with them. Worse than that, you have a problem called high sample complexity, which is just a fancy way of saying you need to play a whole bunch of games for the agent to figure out how it works. It's not very efficient for environments in which you have a large number of large state spaces and high number of actions in a continuous action space. Worst of all, these two fundamental drawbacks really limit room replicability? And is probably one of the big reasons behind why we haven't seen any widespread adoption of deep reinforcement learning something like say robotics. So sometimes your critic, as I said, is a maximum entropy framework. And what this means specifically is that the agent is going to maximize both long term rewards and entropy. Well, what does entropy even mean in this context? Well, if you're not familiar with the concept of entropy, it's strictly speaking, a measure of disorder in your system. It stems from statistical physics and thermodynamics. It's basically the log of the multiplicity of your system, the number of ways of arranging the components of your system. What does that mean in the context of deep reinforcement learning? It means the randomness of the actions of your agent. And you might wonder, why would you want to maximize both long term rewards and randomness of your agent, the region, the reason is that we need to have some degree of random actions to test our model of the world, the agent starts out knowing absolutely nothing about the world, and so should act as randomly as possible. And then as it starts to figure out what actions lead to rewards, it should eventually start to converge on taking mostly those actions, but still spend some time exploring to make sure that there isn't some better action out there. This isn't a totally alien concept to you, if you're familiar to cue learning, or familiar with cue learning, there, we use what's called epsilon greedy action selection, where some proportion of the time say 10% of the time, we take a random action no matter what even if we know what the best possible action is, we may still take a totally random action, simply because you can never be 100% certain that you're right. In this algorithm, we're gonna be modeling entropy by reward scaling, so we're gonna have a multiplicative factor for our reward. And there's going to be an inverse relationship between our reward scale and the degree of entropy in the system. And you can kind of think of this intuitively, because if you increase the scaling on the reward, you're going to increase the contribution of that reward to the cost function. And so you're going to kind of tilt the neural network parameters towards maximizing that reward. Whereas if you lower that parameter, then you're going to lessen the contribution of the signal of the reward to the updates of the neural network, and so decrease its overall importance to the cost function. This algorithm is also going to leverage all the neural networks possible around networks for actors, value network and critic networks. And in particular, we're actually going to have two critic networks, which is going to be an exact analogue of the double q learning algorithm, as well as twin delayed deep deterministic policy gradients, another awesome algorithm for continuous action space environments. Please see other videos on this channel. If you don't know much about those, I have a multitude of videos covering both of those topics. They also make use of another innovation from Q learning, which is the use of a target value function. So the idea here is that we're gonna be using our value function to track the values of states to tell us which states are viable. So we can, you know, seek out those states again, that's the idea behind reinforcement learning. But the problem is that we're going to be updating that value function with each time step. And so if you're updating at each time step and using it to evaluate the values of the states, then you're really chasing a moving target. And so the algorithm will suffer from instability. And that happens a lot in Q learning as well. The solution is to have a target value function, which is updated only periodically or very slowly. So in queue learning, we do a periodic update, where we just exactly copy the networks, the network parameters from one network to another from the online to the target network, here, we're going to make use of a soft that date, where it's going to be some kind of moving average of the online and previous values of this target value network. I'll show you that equation later in this little lecture. So our actor network is going to model both the mean and sigma of a distribution. So this is a probabilistic policy, it is not a deterministic policy, like in ddpg. So we're our actor network is going to output two parameters, a mean and sigma. And then we're going to put that mean and sigma into a normal distribution and sample a to get an action. Now the original paper use what they call a re parameterization trick. And I will fully admit I'm a little fuzzy on exactly what that means. It kind of sounds like they are sampling some normal distribution, then adding on some additional noise to it. I implement this in what I do a what I believe to be a faithful implementation of this and pytorch but I'm not going to do it in this tutorial. I think it's a little superfluous and I'm actually getting, you know, really good results without it so I'm kind of throwing it away. We do have a special function to enforce And bounds as well as to calculate the log of the probabilities of taking some actions because the log of the probability will play a big role in our update rules for our networks, which we'll get to momentarily, and I'll show you that function later on in lecture as well. Another thing to note is that we can use multiple steps of gradient descent like we do in proximal policy optimization or PPO. But we're not going to do that, we're going to just use one step of Adam opposite Adam optimization per time step of the environment. Some other implementation notes, we're going to have a replay buffer based on NumPy arrays, I prefer NumPy. arrays, that is by no means the only way to do it not even necessarily the best way, it's just my preferred solution, change that if you want. Now we're going to keep track of the states the agent sees the actions it took the rewards that received the new states that resulted from those actions, as well as the terminal flags from our environment. And the terminal flags are important, because we have to use that in the update for our deep neural networks. Because when we're evaluating device states, we have to take into account whether or not that state was terminal. The reason is that terminal states have no value. Because the episode is over no feature rewards follow the value is just the present value of the discounted future reward. So the reward is the episode is done, it's zero by default. Now, this is the equation I alluded to earlier, where we are calculating the log of the probability of selecting some action given some state. That's what that pie means. It's a probability slotting an action which is good continuous parameter, given some state, this mu is our actual sample of a of a distribution with mean and segment given by our deep neural network. So what is not the output of our deep neural network, it is the sampling of a distribution where the mean and sigma are given by our deep neural network. And another thing to note here is that we are going to have to multiply our action by the max action from the environment. Reason being is that the action is going to be proportional to the tan hyperbolic function. And that is bounded by plus or minus one, which not all environments are bound by plus or minus one. So you want to take into account that fact, we don't want to cut off half of our action space arbitrarily. So let's talk about updating our deep neural networks, our actor network update is pretty straightforward. What we're going to do is sample states from our buffer, but compute new actions. So this is an off policy learning method, where we're going to be using samples generated by one policy to update some newer policy. We're going to stick those states through our actor network and get new actions out on the other end, compute that log prob based on what we saw on the previous slide. And then we're going to subtract off the minimum value of the two critics. So we're going to pass the state sampled from our buffer the actions computed in our update, according to the current visor actor network, and then take the minimum of the critic evaluation of those state and action pairs. This is kind of the double q learning rule in action. That's why as cute men, and the one over N some tells you that we're taking a mean of that quantity, that difference. Our via network update is a little bit more convoluted. So we're going to have one half the mean squared error, that's what the one over N, the one half of the difference squared, tells you of the difference between the value function using the current parameters or a value function for the state sample from our buffer. Again, we're going to subtract off the minimum of the Q values, where the actions are chosen according to the new values of our hacker network. And we're gonna again, subtract off that log of pi or again, the action that is computed according to the current values of our actor network. And again, the log is computed according to a couple slides ago. The target value network, on the other hand, doesn't get any gradient descent to determine its new parameters, we're going to be updating its parameters with the following equation. So we're going to take this hyper parameter tau and multiplied by the values of our online via network given by Psy that's just the symbol for the parameters for our value network. And we're going to add on one minus tau. So point 995, multiplied by the current values of the target value network, take the sum of those two and upload those values to our target value network. And it's a slowly moving average of online and target networks. Our critic network gets rather interesting update. So here both critics get updated So there's no minimum opera operation here. So there's two cost functions, one for each critic. And it's just given by the mean squared error of the difference between the output of the critic for the states and actions sampled from the buffer. And we're not calculating new actions here, that is one difference between the update of the critic and the actor advised networks. And we're subtracting off this quantity Q hat. And that is where the actual entropy comes into play. So it is a scaled reward, plus the values of the new states according to our target value network. So this is where all the magic of the algorithm happens. That's where the maximum entropy framework comes into play in this one little equation. kind of neat, huh? So we're going to need a whole bunch of data structures. class for our replay buffer, again, that'll be NumPy. arrays, the actor network gets a class a critic network gets a class, so does the value network, our agent gets its own class. And that's going to tie everything together that'll have functionality for choosing actions, saving models, learning as well as saving memories. And we will have a main loop to train and evaluate the performance of our agent. Now we're going to need a number of packages will need TensorFlow, I prefer GPU obviously, this is a highly computationally expensive algorithm. So if you have the GPU, please use it, but need pi bullet for our environment, Jim, of course, NumPy and TensorFlow dash probability, that is, the only way we can get access to this probability distributions are not built into the base TensorFlow package for some reason. Okay, that's it for mini lecture, your Crash Course. Now let's go ahead and throw you in the water and start coding. Alright, so let's go ahead and get started with our buffer class. And then when we want to do our networks and our aging class, so we're going to be using NumPy arrays for our memories and so NumPy as our NumPy will be our only package dependence for this particular part of the agent will need a max size, in good shape and number of actions as input, we will need to save our MAX SIZE. And we will also need a meme counter to keep track of the first available memory position. And our agents memory is represented by NumPy arrays that we will initialize it as zeros in the shape mem size by input shape or input shape is some tuple or list. We need the new state memory as well that are the new states that the agent sees as a consequence of the actions it takes in the environment, we do the action memory. And that is in shape men size by a number of actions. Keep in mind number of actions means number of components to the actions since these are continuous actions, they will have components along some dimension and action space. It's not a actual number of discrete actions, it's a little bit Miss aptly named reward memory. And that's just a vector and an array to keep track of the terminal flags we received from the environment. And I'll go into that as num, type NumPy bool. Next, we need a function to store Trent store transition. And if it isn't clear, the reason we need to store the terminal flags is because the value of the terminal state is always zero because no future rewards follow the terminal state and so it has no value. So we will need to save all those parameters in our agents memory. And we want to know what is the position of the first available memory. And that's given by the modulus of memory counter and mem size. This has the property that when we overwrite the agent's memory, we will go back to the very beginning. So we don't have to worry about going beyond the bounds of our array. So why don't we just go ahead and save our variables and terminal memory. And most importantly, we need to increment our memory counter by one. Next we need to function to sample our buffer. And I want to make a quick I want to make a quick aside here. So in a question in the comment section someone asked Is there a way to sample memories according to how viable they were reading our early memories agents acting randomly doesn't know what it's doing. So what You know, what good is it to sample those memories, isn't it more important to sample memories where we kind of knew what we were doing and had some reasonable model the environment. And you can do that in something like prioritized experience replay where you prioritize memories based on their utility. But in this case, we're just doing uniform sampling. So we just take the good with the bad. So we want to know the first available memory how far we have gone in our memory. And that's given by the minimum of the meme size and the meme counter, so that we're not, you know, if we've filled up half of our memory, we don't want to sample the latter half, which is all zeros, we only want to sample the first half, which is filled up with useful transitions. So then we're going to go ahead and take a batch size of memories from from our memory bank, and go ahead and dereference. Sorry, we need an extra space there. And then go ahead and return those states actions towards new states and terminal flags. And that is it for our memory buffer class. Next up, we're going to handle our networks for the agent. So the first thing obviously, we want to do is come up and fix our imports. Because we're going to need more stuff than just NumPy. We're going to need OS, the base TensorFlow package, care OS and we will need that flow probability has TFP now, you will have to install this package separately, it is not included in the base TensorFlow package. So see the TensorFlow documentation if that is something you don't know how to do. But you do need to do that installation separately. So our critic network will derive from Kerris dot model, I guess has access to making use of the gradients and the gradient tape and all the other good stuff that we get from the base class. We will need a number of actions number of F's DIMMs FC one FC two dims a name so the name is useful for model checkpointing. So we can keep track of which file corresponds to which network and a directory for saving models you have to do make dir on that otherwise you'll get an error. First thing you want to do is call your super constructor. Save the relevant parameters. One of these days I'm going to shorten this checkpoint dur name, but not today. So then we want to join the checkpoint file name checkpoint directory, excuse me, with the name and underscore assault actor critic so that when we save a bunch of stuff in a single directory, we know which file corresponds to which algorithm. Next we can define our model. Our first fully connected layer is just a dense layer that has implied input dimensionality and takes FC one dims as output with a row activation. Likewise for a second layer. And our final output is single valued with excuse me a single node with no activation. So, we have to input a state and action to our call function, because the critic network evaluates the values of state action pairs. And so the first thing we want to do is pass the concatenated state and action pair through the first fully connected layer. And we want to concatenate along the first axis and pass out through the second fully connected layer. Pass through The topic layer and return. Pretty straightforward there. Next, let's handle our value network. And again, that arise from karass model. We don't have to specify any dimensionality here other than for our first and second fully connected layers. And we want to call our super constructor again. And if you want to get really fancy, you could write a base model based network class, and then have everything derived from both the I guess the base model class would derive from the karass model. And then everything that derives from that would derive from the base class base model, sorry, base network class. But that can get kind of messy really quickly, you have to deal with multiple inheritance. Maybe it's not as hard as I initially think it is. But it's an alternative paths, it's an alternative way so that you don't have to keep writing like the same stuff over and over again. Because you notice there's a lot of overlap between the code of the network so you can fix that with inheritance in our output is again going to be single valued. No activation. Now, the call function here only takes the current state of the environment as input, the value function is only concerned with the values of states not state action pairs. Then we just go ahead and pass all of those through and return them. Pretty straightforward. The real difficulty doesn't occur until the actor network and the difficulty will come in the call function as well as the sample normal. So we want to pass in a max action all using that here. Sorry, I always kind of make stuff on the fly kind of modify stuff on the fly. Yeah, so we do need that FC one dims. FC two dims. A name number of actions are used to by default again, that's number of components. And a checkpoint directory. protocol are super constructor and save the relevant parameters. Okay, then we have our model name. And just as an aside, you can't use name, it's a reserved variable, we can say self dot name equals name, because name is reserved by the base class karass model, so you have to use modeling. So we're not going to be using three parameterization. But we will need some noise factor to make sure we don't get something that blows up when we take the log. You'll see what I mean very shortly. Our neural network by now should look pretty familiar. Row your activations. Here's where things get interesting. We're going to have a new for our distribution as well as a sigma no activation So let's go and handle our call function. So we're gonna get our I've called it prop here, but I don't know if I like that name now that I'm rewriting it. But I'm gonna roll with it. So mu, really, that problem is just the input of our first two layers, you know, the pastor of the state through the first couple layers. Why are sigma which is the standard deviation of our distribution. And we're going to want to clip this. It may be the case that TensorFlow two doesn't need it. But when I was coding this up and pie torch, it, the distribution would blow up. If we had a noise of sorry, a sigma of zero, it didn't like zero standard deviation distribution, which I can understand. I guess that's a direct delta function, it's not exactly something that you would encounter every day. And I think the TensorFlow might actually fail a little bit more gracefully than the PI torture in that respect. But I go ahead and clip it. So that goes between one by 10 to the minus six and one, so we're going to constrain the standard deviation here Do not be too large. That is something you can optimize on your own. I don't do extensive and detailed studies on all this stuff, I just get it working. Check that it conforms to the specifications laid out the paper, and then I get something reasonable on the output. But we're not done for the actor network, we have the output of our deep neural network, which is a mean and standard deviation for our distribution. Now we have to sample that distribution to get the actual action for our agent. And I've stuck it on a function called sample normal. And as I stated, we're not going to do in the re parameterization trick, show students how to do that in the course. And in fact, in my pie torch video, I show you how to do it. Pie torch has a very simple function called our sample that does basically the parameterization for you. You can implement the code yourself, if you really want to be completionist about it. But I'm not bothering here because I do get good results without it. It's kind of like what the ddpg paper where they use the Orenstein will lindbeck noise. And then people that implemented a later kind of threw it away, because they're like, Wait a second, this is unnecessary complication. That kind of seems to be the case here where it worked without it. And it's an unnecessary complication. But I mean, I could always be wrong. But I do get solid results without it. So here, we're going to go ahead and instantiate our normal distribution defined by mu, and sigma. And our action is going to be our actions are going to be a sample problem bill it T's dot sample, if you want to do the re parameterization. You could re parameterize it there. And the action is math, a tan hyperbolic times max action, that max action takes care of the fact that our environment may have max actions outside of the bounds plus or minus one, which are of course, the bounds of the tan hyperbolic function. So you want to not arbitrarily cut off half of your action space if you don't have to. Log probs it's probabilities, bilities dot log prob of actions, and then go ahead and subtract off math. Log one minus Tf dot map dot path, excuse me. Action, so we're going to square the action plus self dot REAP program. Call it noise did self dot noise. And so then I had to change it here as well, don't I? Yeah. self dot noise plus self dot noise. Do I have the right number of parentheses here? Yes, I do. And so since you have a logarithmic function here, you don't want to take the log of zero that is oftentimes not advisable. So I just add in some really small number to make sure we're taking the log of something finite. So then we're going to go ahead and do a reduce some on that. Log probs x equals one to keep them true, in return the actual and log props. So that wraps up the network's portion of our code. Now we have to handle the agent class, which will tie everything all together. So of course, again, we have to come back up to the top and add in some imports. We're going to need our optimizer. So we'll say from TensorFlow Kerris optimizers import Adam, and I think that's the only other import we will need. And then we come back down here and start our agent class, which derives from nothing. Our initializer is going to take a number of learning rates. Now we're going to use two separate learning rates, you could in principle, use three one for the critic one for the actor, one for the value network. In this case, we're gonna use one single learning rate for the actor and then the same, the beta learning rate will be for the value and critic networks. And book dims something like eight, that's just the default for the lunar lander, it's the only default I know off the top of my head, we're gonna want to pass in the environment to get some useful information from it a gamma, the discount factor for our update equation backsies for a memory, a million transactions. Default Value of towel was 0.005. layer one size layer two size to fit the six bite size default to 36. And a reward scale default of two. Let's go ahead and start saving replay buffer, Mac size and input dims interactions pretty straightforward. Then we're going to go ahead and define our networks. Sir Max, Max action is going to be the high value of the access space from our environment. We need to our first critic, second critic. And these are you know, identical in terms of their definitions, except for the name. Because again, when we save files, we want to save separate files for both critics otherwise, we're going to get the most value network in our target value. Okay, so now we have to compile our networks. We'll need our critical one. Great a beta, or environment network and our target. Now, of course, the target via network doesn't perform any optimization, we just copy of weights using our soft update rule. But it is part of a which is part of the framework that we have to compile the model before we can use it our scale factor and our initial update network parameters, where we're going to do a hard copy of the parameters from the online network to the target value network. The first function I want to handle is the Choose action function. Then we'll get to the remember function, the update network parameters. And our saved model parameters. Finally, we'll get to the Learn function will save the best for last. So we want to convert our state our observation tensor. And the neural network expects that we want to have a batch dimension. So we have to do that. We have to add an extra dimension to get our batch. We want to get the actions. We don't really care about the log prompts at this stage. And then we just go ahead and return actions zero, because the tensor I believe is the output and we want to return a NumPy array because the environment doesn't accept the TensorFlow tensor as input to this function. So this is a simple interstate interface function between the agent and its memory. And this is necessary because you don't want to directly access the values of the memory class from the aging class. It's just bad software design, you don't want to go overriding parameters of one class with another, you want to have an interface function that handles all that for you that where you use one function to call another, it's just clean software design. Always keep that in mind. I think state action reward new state. And it looks kind of silly, we could get away with it in this context, because we're not going to be building on this codebase later. But I always find it important to use strong and consistent software design principles wherever possible. Our update network parameters function. Pretty straightforward. So we're going to pass in a towel, which is a default value of none. So tau, is none, then we want to go ahead and use our default value for towel. And this has to do with the fact that appear on line 153. We're calling update now parameters and the value of tau equals one to facilitate the hard network of hard network weight copy. So we're basically going to just go ahead and iterate over our network weights, do the calculation, append those to a list, and then upload that list to or by you target by your network. wait times to helpless targets, times one minus tau. one too many parentheses there. That looks right. weights. And that is it for update network parameters. Pretty straightforward. Next, we have two bookkeeping functions to save our models. Those don't take any inputs. Sylvain is not used word formation. So we just want to save the weights to the checkpoint final checkpoint, I'll take a point that sounds pretty good. And then the load models function is basically the inverse. So I'm just going to go ahead and yank and paste then make sure to change save to load. So that way, we don't do anything wonky. After dot save weights, we want to change to load weights. And same deal we want to load from the check point files. Okay, now we come to the hard part of the problem, the Learn function, we've got a kitty cat joining us, perhaps she will hop up on the desk, the first consideration we have. So the first consideration we have is what we would do what we do in the event that we haven't filled up enough of our memories to actually load a batch size of those memories. And you can do many different things. You can play batch size of transitions randomly and store the transitions and then call your learning function. Or in this case, you can just say, hey, if I haven't filled up at least batch size of memories, just go ahead and return. So that's what we're going to do. And this is a this violates my principles of good software design where I'm calling the mem counter or the memory class directly here instead of using a function to get it. This is bad design. So don't do this. If you have an option not to I'm just doing this, because it's like I said, it's not gonna matter if this isn't a huge code base that we're going to be building upon later. But just know that this isn't consistent with what I said earlier. I'm kind of backpedaling. A little bit. Now we have a sample our memory, we're gonna pass in our batch size, then we want to go ahead and convert those two tensors. And I want to be really consistent with my data types here. Reason being a lot of these frameworks, TensorFlow and pytorch, specifically, get a little bit finicky when you start mixing up data types. They don't like to mix data types and calculations, because that screws up the precision of the calculation. They want everything to be. They want everything to be consistent and explicit, so that you get exactly what you expect, because of course, there is a rather a significant difference in large calculations between floating point 32 floating point 64, even in floating point 16 calculations. So it's very beneficial to go ahead and specify what data type you're using. In this case, we use float 32. And we even in theory could get away with sorry, these are TF not NP, we could get away with 16 point because 16 point precision 16 bit precision, excuse me, because we don't need, you know, a whole bunch of decimal places in our rewards or anything like that. But we use 32. Just to be safe. So now we're going to handle the update rule for our value network. So we need our gradient tape. So we're going to pass the state states through our network and then squeeze to get rid of that batch dimensionality, then we're going to do the same thing with our new states. except we're going to pass through the target by a network. Squeeze again, what are the actions according according to our current policy, and the log probs. So we're going to sample normal states. And we of course, don't do any real parameterization. Within our log probs, we have to squeeze and we get the values according to the new policy critic one state's policy actions, and then the value according to the second critic, then our critic value you TF that squeeze. Do you have math minimum between the q1 new policy and q2 policy and I'm missing a privacy there. There we go. Now we're to squeeze along the first dimension. So then our value target is going to be our critic value minus our log prompts. That's from the paper value loss. One and a half keros losses dot mean squared error between the value and the target value value targets sorry. Sorry, my toddler's rampaging again. So now we need to calculate our gradient. Good grief. I have the cat on my lap as well. It's making it very difficult to type value loss self dot value dot train abort variables, and we can stick that on new line to be good little programmers. Then we have to apply our gradients optimizer dot apply gradients zip by network gradient value trainable variables, and this is just how we do gradient descent using the gradient tape. And so that is it for our value network loss. Now, we have our Actor, network and critic networks to worry about. So we need more gradient tapes. So we need our new policy actions and their log probs on a sample are states Oh, that's interesting. I have I see. That's interesting in my cheat sheet here I have the NumPy arrays, and it works. Interesting. I'll go ahead and fix that. So let's see if it breaks when I pass in the TensorFlow tensors it shouldn't I just have a typo in my cheat sheet. So let's go ahead and squeeze our log probs and get some new policy values one policy actions here two new policy we have our critic value squeeze math minimum then I'll actor loss just the log probs minus i critic by Michael loss goals. Math reduce me Petra loss. Then we have our actor network gradient. Tape gradients after loss. The gradient of our loss with respect to our accurate trainable variables that we want to apply our gradients, actor optimizer. And of course that expects zip as input. And that handles our actor loss. And finally we come to the critic loss. Now we have to pass the persistent evil true parameter to our function call, because the loss is going to have two components. So we'll have a critic one and critic two loss. If you don't pass versus an equals true flag, it only keeps track of stuff for the application of a single set of gradients. So you can only do the update to one of your critic networks instead of both. So you tell it to just keep track of the gradients even after it Go ahead, even after it applies gradients one time. So you can apply gradients twice. Or q hat is our scale factor and multiplied by reward. That's where the entropy comes in. gamma times value underscore one minus done. That is interesting. So this gives me a little bit of pause, because first of all, I know the code works, I've tested this, and I haven't accidentally deleted a line. But what gives me pause here is that I have this value underscore, which is defined up in this other scope here. I'm actually missing a equal sign there. That would have triggered an error. So it does work. That's interesting. I didn't know that the context manager shared the scoping of variables. I didn't know that that is new information to me. So I want to go ahead and roll with it for now. If this is suddenly broken and doesn't work, then I will come back and re edit this and put in the new code that actually works, of course. But that is new information to me. Say I learned stuff even while making content. So we have to get the values according to the old policy. Why do I call it old policy? Perhaps go ahead and check the GitHub for this when I upload it. I will probably do some variable name swapping to make things a little bit more logical. If I have to scratch my head while I'm typing out the code here. You're probably scratching your head watching it Of course, the losses are pretty straightforward, just mean squared error between the q1 and q2 old policies and this Q hat value, then we can go ahead and calculate our gradients. Critical network gradients. Want to go ahead and apply our bruneians. Sorry, so blood critic to network. Brilliant, predict two trainable variables. And then we want to call our update number of parameters function after we've done all of our updates, okay, so that wraps up all of the heavy lifting for our code. Now we can move on to coding up the main loop, which is going to be a cakewalk by comparison. And of course, I have an invalid syntax error. Oh, that's easy. There is no self there actually is blank equals. And I've done it yet again. And oh, because it's a sorry, my cat wants to steal the limelight. There we go. I did the same thing here. Of course, why wouldn't I do the same thing there, I like to be consistent. Okay, so now we're going to handle the main function. So of course, we will need our pie bullets. And these could be because we're going to be dealing with the inverted pendulum and bullet environment, we need our gym we need NumPy we need our agent. We need our plot learning curve. And that is it. Want to make our environment inverted? pendulum board envy me zero. Need to make far agent passing all of our roles and parameters to play 250 games. Figure file is just the directory plus the file name, you can condense that into one line if you want, it's not a problem. We save best score in the reward range zero score history want to keep track of the scores the agent receives over time so we can see if it's learning as well as to plot them later. And a load checkpoint variable, which you can set to false if you want to load a checkpoint. So we're going to load the checkpoint and load your models. And one thing I like to do is set the render mode to human so that we can see the agent and play the game because if you're loading a checkpoint, you probably want to evaluate the performance. If not just comment out this line. In other words, if you're doing checkpointing so that you can do more training later than just get rid of that line. No big deal. Let's go ahead and play our game. To reset your environment, at the top of every episode, you set your done flag and set your score to zero. Play your episode by choosing an action. Take your action, get the new state reward done and debuginfo back from the environment to crack your score. Remember your transition not load checkpoint, then you want to learn the logic here being that if you're loading a checkpoint, you're probably evaluating. If it's the case that you're just loading a checkpoint to perform more training later, then you're going to need to put that agent dot learn outside of that if statement, just get rid of the if statement. Okay, so that you're, you know, gonna do what you actually intend. No matter what you need to set the current state to the new state. And at the end of the episode, you want to append the score to the score history, calculate an average of the previous 100 games. If that average score is better than your best score, then set the best score to that average. So that you know you're learning. And again, if not load checkpoint agent does save models and you want to print episode score average score and then when all the games are over. Go ahead and what's your learning curve? Okay, moment of truth. Let's see how many typos I made. Hello. That's easy. plog replay buffer as attribute new size. It should be mem size self dot new size. I didn't do that elsewhere. I don't think Okay, once that I not call it call that is interesting. So that is for my value network. Oh, okay. replay buffer critic and value network. Oh, ah. There we go. Sorry about that. indentation error. As no attribute sample. That's because it is sample normal. That is in line 223. It does sample normal. persistent. Because I forgot to t that is in line 239. Verses 10. equals true that's another typo perfect. Your ops is not callable. See self dot gamma times value underscore oh I'm missing a multiplication sign. That is in line 240 Sorry about that. On 241 mine is done. Good grief critic. One underscore That is in line 251. Oh, because it's not there's no self in there. Oh, okay. Good grief. There is no self there. Yep. Okay. Greens do not exist for one of my layers. Oh, okay. means I have forgotten something in deed that's problematic because it doesn't tell me. Well, these warnings here, don't worry about those those aren't a problem this Grady does not exist is in fact a problem. So let me see, I don't even though which network it is talking about there doesn't tell me says dense kernel zero. So that's probably whichever network I made first. So I instantiate my Acura network first. So here is value after a loss. Let's check out our handy dandy cheat sheet here. And so I have located the source of the issue. And I will annotate this when I edit the video. But the issue here is that I was passing the state through both the first fully connected and second fully connected layer, instead of allowing the output of the first fully connected layer defeat into the second layer. So of course, that's how deep neural networks work. And you won't get anything useful if you don't feed things through properly. So let's go ahead and try it again. Okay, so now we got the same warning. And that's not concerning. It's just has to do with the precision. So that's something you can deal with on your own. If you want, I'm not going to go changing the backend settings, I could suppress the warning, but it doesn't really bother me that much. So I'm going to come back in a few minutes and make sure that this is learning. And when I do, I'll go ahead and show you the output of a fully trained network. And if it doesn't learn them to go back and debug it, and you'll know because I'll tell you in a few minutes from your perspective. Okay, so here we are, it's just a few minutes later, by game 60, we can see that the average score is improving with each episode, meaning that it is in fact learning and it's getting well over 100. And so I'm gonna switch to the other terminal where I have finished running it, I'm not gonna wait for it to play all 250 games, because it does take quite a while once it gets closer to 1000 steps. So hold on one moment. So here is the output of an earlier model I trained. And just checking the other window to make sure it is indeed still learning it is it's saving models pretty much every game. So this is an output of a model I ran earlier, where you can see that for the last several games, it gets a consistent score with 1000, with a little bit of a low flyer here at 751. Still a respectable score. It doesn't have 100 games 1000. So the average score hasn't hit 1000. But it is trending well up on its way. So I consider that to be a fully trained model. If it ran another 50 games 100 games, then the average score would be 100 new, and we would have beaten the environment. So that assault after critic in TensorFlow two, I left out the re parameterization. Because I don't think it's entirely necessary. I don't know why it was put in the original paper, I have implemented it in pytorch. You can see that video, if you want to see how that works. It's just you passing in a parameter to your sample function. And if that parameter is true, then you use the our sample instead of the sample function from your distribution, if you want to implement that in TensorFlow to leave that as an exercise to the viewer, but you would have to do some simple calculation of adding in some spherically sampled noise to it. So that's not you know, what's not particularly difficult, which is something I didn't want to bother with. So this agent learn that is sought after critic in TensorFlow to an incredibly powerful algorithm. Indeed, state of the art I prefer TD three, I think it tends to perform a little bit better, but this is certainly no slouch in and of itself. I hope that was helpful. Leave a comment a question? Subscribe, certainly, if you made it this far, and I will see you in the next video. If you give me about 45 minutes of your time, I will show you how to code a fully functional asynchronous advantage actor critic agent in the pytorch framework starting from scratch. We're gonna have about 10 to 15 minutes of lecture followed by an about 30 minute interactive coding tutorial. Let's get started. Really quick if you're the type A person that likes to read content, I have an associated blog post where I'm going to go into much more detail, check the link in the description. Deep reinforcement learning really exploded in 2015. With the development of the deep q learning algorithm. One of the main innovations in this algorithm that helped it to achieve such popularity is the use of a replay buffer. The replay buffer solves a very fundamental problem in deep reinforcement learning. And that problem is that neural networks tend to produce garbage output when their inputs are correlated. What could be more correlated than an agent playing a game where each time step depends on the one taken immediately before it, these correlations cause the agent to exhibit very strange behaviors, where we'll know how to play the game and suddenly forget, when an account or some new set of states that have never seen before, the neural network really isn't able to generalize from previously seen states to unseen states. Due to the complexity of the parameter space of the underlying problems. The replay buffer fixes this problem by allowing the agent to randomly sample agents from many many different episodes. This guarantees that those time steps taken are totally uncorrelated. And so the agent gets a broad sampling of parameter space and is therefore able to learn a more robust policy with respect to new inputs. As I've shown before on this channel problems arise when you attempt to simply bolt on a replay buffer onto the actor critic algorithm, it doesn't really seem to work. And in fact, it's not very robust. Actor critic methods in particular, suffer from being especially brittle. And so adding on a replay buffer really doesn't help to address that problem. In 2016, a group of researchers managed to solve this problem using something called asynchronous deep reinforcement learning. It's a totally different paradigm for approaching the deep reinforcement learning problem. And in fact, the technology can be applied to a wide variety of algorithms, and the original paper they detail solutions for deep q learning. And step sarsa. Excuse me, instead, q learning as well as sarsa, and actor critic methods as well. So what is this big innovation? Well, instead of having a replay buffer, we're going to allow a large number of agents to play independently on totally separate and self contained environments. Each of these environments will live on a CPU thread, in contrast to a GPU for most deep learning applications. This has the additional benefit that while if we don't use a replay buffer, we don't have to store a million transitions, we were trivial environments really doesn't matter. But if you're dealing with something like say, the Atari library, a million transitions can take up a significant amount of RAM, which can be a limiting factor for enthusiast. So having the agent play a bunch of different games in parallel on separate environments, only keeping track of a small number of transitions, vastly reduces the memory footprint required for deep reinforcement learning. So in what sense exactly is this algorithm a synchronous? Well, this means exactly in this context is that we're going to have a large number of parallel CPU threads with agents playing in their own environments, they're going to be acting at the same time, but at various times, they're going to be deciding what to do as well as updating their deep neural network parameters. And so we're not going to have any one agent sitting around waiting on another agent to finish playing the game to update its own set of deep neural network parameters. Each one will be totally independent, and learning on its own. Now, we're not going to be simply throwing away the learning from each agent average finishes the episode, rather, we're going to be updating the network parameters of some global optimizer as well as some global actor, critic agent to have one actor, critic agent that sits atop all the others, and the local agents that do all the learning by interacting with our environments. So what is the advantage part of a three scene. So the advantage essentially means what is the relative advantage of one state over another, it stands to reason that an agent can maximize his total score over time by seeking out those states which are most advantageous or have the highest expected future return. The paper gives a relatively straightforward calculation for this, all we have to do is take the discounted sum of the rewards received over some fixed length trajectory, and then add on an appropriately discounted value estimate for the final state, the agent saw in that trajectory. Please note that this could be some fixed number, like say five steps, or it could be three steps that the agent encountered in Terminal Terminal state along the way, we're then going to go ahead and subtract off the agents estimate of the value of whatever current time step it's in, in the trajectory. So that way, we're always taking the value of the next state minus the current state. That's what gives us the relative advantage. So what does the actor critic portion of a three c mean? Specifically, this refers to a class of algorithms that use two separate neural networks to do two separate things. So the actor network is responsible for telling the agent how to act kind of a clever name, right? It does is by approximating a mathematical function known as the policy, the policy is just the probability of selecting any of the available actions for the agent given it's in some state. And so for discrete action space, it's going to be relative probability selecting one action over another. So in our car pool, it's going to be say 60%, move left 40% move right, so on and so forth. We're going to facilitate this by having two separate networks, the actor network will take a state or set of states as input and output a softmax probability distribution that we're going to be feeding into a categorical distribution from the pytorch. framework, we can then sample that categorical distribution to get the actual action for our agent. And we can also use that to calculate the log of the probability of selecting that action according to the distribution, probability distribution. And we use that for the update rule for our actor. Now, the critic has a little bit of a different role, the critic essentially criticizes what the agent the actor did, it said, you know, that action you took gave us a pretty lousy state that doesn't have a very large expected future return. And so we shouldn't really try to take that action given that state any other time that we encounter it. So the critic essentially criticizes what the actor does, and the to kind of play off of each other to access more and more advantageous states over time. Before we go ahead and talk about the specifics of each class, let's get some idea of the general structure and flow of the program. The basic idea is that we're going to have some global optimizer and global actor critic agent that sits on top that keeps track of everything the the local agents learn in their own individual threads. Each agent will get its own specific thread work and interact with its own totally distinct and separate environment, the agent will play either some fixed number of time steps or until it encounters a terminal state, at which point it will perform the loss calculation to do the gradient descent on the global optimizer. Once it calculates those gradients, it's going to upload it to the global optimizer, and then redownload the parameters from that global optimizer. Now keep in mind, each agent is going to be doing this asynchronously. So while one agent is performing its loss calculations, another agent may have already finished that loss calculation and updated the global optimizer. That's why right after calculating the gradients, we want to go ahead and download the global parameters from the global actor critic. So that way, we make sure we are always operating with the most up to date parameters. After each time the agent performs an update to its deep neural network, we're going to want to go ahead and zero out its memory so that it can start fresh for another sequence of five or until it encounters a terminal state number of steps. So now let's talk implementation details. We're gonna have a few separate distinct classes for this, the first of which is going to be overriding the atom optimizer from the base pie torch package. So we're gonna have a shared atom class that derives from the base torch optim, Adam class. And this will have the simple functionality of telling pytorch me want to share the parameters of a global optimizer among a pool of threads, it's only going to be a few lines long, and it's much easier than it sounds, and I'll show you how to do it in code. Our next class will be the actor critic network. Now, typically, we would use shared input layers between an actor and critic where we simply have one input layer and two outputs correspond to the probability distribution pie and the value network meme. But in this case, we're going to host two totally separate distinct networks within one class. It's a relatively simple problem, the car pole. And so we're going to be able to get away with this. The reason I'm doing it this way is because I frankly could not get shared input layers to work with the pytorch multi processing framework. Our agent will also have a memory which we're just going to use simple lists. For that, we're going to append states actions and rewards to those lists, and then go ahead and set those lists back to empty lists. When we need to clear the agent's memory, we're going to have a function for calculating the returns where we're going to use the calculation according to the algorithm presented within the paper. So the idea is that we're going to start at the terminal step, or the final step and the trajectory. If that step is terminal, the R or the return gets set to zero, if it's not, it gets set to the current estimate of the value of that particular state, then we're going to work backward from the T minus one time step all the way to the beginning. And we're going to update our as r sub i plus gamma times the previous value of r, I'm going to do a calculation in the video, the coding portion to show you that these two are equivalent, meaning this calculation as well as the earlier advantage description I gave you, I'm going to make sure that you understand that those are actually equivalent. And it's just a few lines of mathematics. So it's not really that difficult. And I've taken the liberty of doing it for you, then we're going to be calculating the loss functions. And these will be done according to the loss functions given in the paper. So for our critic, we're going to be taking the delta between those returns and the values and taking the mean squared error. For our actor, we're going to be taking the log prop of the policy and multiplying it by the advantage. And with the negative one factor thrown in there as well. Now that's a really cool way of calculating the loss for the actor because it has a pretty neat property. So when we multiply the advantage by the log of the probability, what we're actually doing is waiting at probabilities according to the advantage they produce. So actions that produce a high advantage are going to get naturally weighted higher and higher over time. And so we're going to naturally evolve our policy towards being better Over time, which is precisely what we want, right? Our final class will be the agent class. And this will derive from the multi processing process subclass. So here's where all of the real main type functionality is going to happen. So we're going to be passing in our global optimizer as well as our global actor, critic agent, instantiating 16, in the case of 16, threads for a CPU, local critics with 16 separate environments. And then each one of those is going to have you know, two separate loops, where it's going to go up until the number of episodes that we dictate, and it's going to play each episode, as I described earlier, within each episode is going to play some fixed sequence number of steps. And then it is going to perform some update to the global optimizer and then download the parameters from the global actor, critic agent. Our main loop is basically going to set everything up, we're going to go ahead and define all of our parameters, create our global actor, critic, our global optimizer and tell pytorch if we want to share the memory for our global actor, critic, agent, and then we're going to make a list of workers or agents. And then we're going to go ahead and send each of those a start command as well as a join command so that we can get everything rockin and rollin. So what are some critiques of this algorithm overall? Well, one is that it is exceptionally brittle. Most actor critic methods require a fair amount of hyper parameter tuning. And this one is no exception. I tried to use the lunar lander environment, but couldn't really get a good set of parameters to make it run effectively and get a you know, a consistent score of 200 or above, or Heck, even a consistent score of over 100 out of call that good enough for YouTube. Another one is that there is a significant amount of run to run variation. So it's highly sensitive to initial parameters. You can solve this by setting global seeds for the random number generators so that you're getting consistent random numbers over time, and so you're going to know exactly how you're starting. But to me, it's a little bit kind of like cheating. So I don't do it in this video, but it is something to take note of. And in the original paper, I think they do something like 50 different runs of each evaluation some large number to get a pretty tight or to get a pretty solid distribution of scores. And that is, I think, because of the high degree of run to run variation. Okay, I have lectured at you enough. Again, if you'd like to read written content, I have a link in the description to a blog post, where I talk about this in a little bit more detail. But nonetheless, let's go ahead and jump right into the coding tutorial. Let's go ahead and start with our inputs to the gym for our environment. They are based towards package we'll need torture, multi processing, to handle all the multi processing type stuff, we will need torch penon to handle our layers will need nn functional to handle our activation functions. And we're going to need our distribution as well. And in this case, we're going to need a categorical distribution. All this does is takes a probability output from a deep neural network maps into a distribution so that you can do some actual sampling to get the real actions for your agent. Now I want to start with a shared Adam class. This will handle the fact that we are going to be sharing a single optimizer among all of our different agents that interact with separate environments. All we're going to do here is called the base Adam initializer. And then iterate over the parameters in our parameter groups, setting the steps exponential average and exponential average squared to zeros effectively and then telling it to share those parameters amongst the different pools in our multi threading pool. And this will derive from the base atom class. Our default values are going to be I believe, identical to the defaults for the atom class. And then we want to call our super constructor. Now we're going to handle setting our initial values And then we're going to tell torchiere, we want to share the memory for our parameters are for our gradient descent. And note the presence of the underscore at the end of memory there. Okay, that is it for the shared atom, pretty straightforward. Next up, we want to handle the actor critic network, which will also encapsulate a lot of the functionality I would typically put into an agent class because of my understanding of the design principles of object oriented software programming. In this case, I do shimmy a few things around, because the agent class is going to handle the multi processing elements of our problem. And so it doesn't really make sense to stick this, like the Choose action, or memory, or memory functionality in the agent class. So we're gonna stick it in the network class, it's not a huge deal. It's just a departure from how I normally do things. And certainly not everybody does things the same way I do. So our initializer takes input dims from our environment, number of actions from our agent, and a default value for gamma of 0.99. We also have to save our gamma. And the next thing we want to handle is writing our actual deep neural network. Now, this is also a little bit different than the way I normally do things. Normally, I would have a shared input layer that branches out into a policy and evaluate network as two separate outputs with that shared input layer. When I tried to do that, I found that the software doesn't actually run, it doesn't handle the threading aspect very well. In that case, when you have shared input layers from a deep neural network, I don't know exactly why that is, if you know, please leave a comment down below, because I'd be very curious to hear the explanation, and simply what I found out through my own experimentation, so we're gonna have two separate inputs, one for the policy and one for the value network, as well as two separate outputs. So they're effectively two distinct networks within one single class. We're only going to be using 128 neurons here and not a very large network. And our output will take those 128 hidden neurons and converted into number of actions. And our value function will take likewise, 120 in hidden layers, hidden elements and convert it to a single value. Or if you pass in a batch of states a batch of values. The agent also, excuse me, the network also has some basic memory, so rewards, actions, and states. These we will handle just by appending stuff to a list and then and each time we call the learning function, we're going to want to reset that memory. So let's go ahead and handle that functionality first. So the remember just appends a state action reward to the relevant list. And a clear memory function just zeros out all those lists. Pretty straightforward. Next, we have our feed forward function that takes a state as input. So we're going to pass that state through our first input layer for our policy and perform a value activation on that and do something similar for the value input layer. And the outputs of those two are going to be passed to the roles to the relevant policy and value outputs. And then we just returned pi and V. Pretty straightforward yet again. Next, we're going to have our function to calculate the returns from are a sequence of steps. So this will only take a single input, and that will be the terminal flag. Recall that the return for the terminal step is identically zero. So we need the terminal flag or the done flag to accommodate that. So we want to go ahead and convert the states from our memory to a torch tensor of T dot float data type, because it is a little particular about the data type, you don't want to pass it in double it gives you an error. So best to take care of it. Now, we're going to go ahead and pass that through our neural network. And we're not going to be concerned with the policy output at this stage, we just want to know what the value evaluations The Critic has for that set of states. So our return is going to be is going to start out as the last element of that list, so the terminal step, or the last step in the sequence of steps. And we're going to multiply that by one minus done so that if the episode is over, one minus done is zero, so you're multiplying by zero, you get zero. Pretty handy way of handling that, then we're going to handle the calculation of the returns at all the other time steps. So we're going to go ahead and iterate over the reversed memory. And say that our return is the reward at that time step plus gamma times are and then just return actually append that return to the list of batch returns. And then finally, at the end, you want to go ahead and reverse that list again. So that's in the same order in which you encounter the states. This calculation reverses it up as you're starting at the end, when you know the value of the final state, or at least the estimate of the value according to the critic, and then reversing it to get it back in order for passing it into our loss calculation function. Now, this may be a strange form to you. If you write it out by hand, maybe I can show you something here where I did it for you. If you write it out by hand, this particular chunk of code, you can see that it's identical to what they tell you the calculation is in the paper. So you can do that exercise on your own to convince yourself or I can just show it to you so that I can convince you of it. But this is indeed, the return calculation from the paper, everything is as it should be. And then I want to convert that to a tensor and return. Next, we have to handle the calculation of our loss function. And this, again, is only a single input a terminal flag from the environment, we're going to go ahead and get the value, excuse me the tensor representations of our state's actions right at the beginning. And then we're going to go ahead and calculate our returns. And then we're going to perform the update. So we're going to be passing the states through our actor critic network to get the new values as well as then a distribution according to the current values of our deep neural network, we're going to use that distribution to get the log problems of the actions the agent actually took at the time it took them. And then we're going to use those quantities for our loss functions via squeeze. Now, this squeeze is very important. If you don't squeeze here, it won't trigger an error, but it will give you the wrong answer. The reason it will do that is because the actor loss and the critic loss, I believe, will come out as a shape of five by five. And that is not the shape we want, we want something in the case of five time steps, team x equals five, so it'll give you a five by five matrix instead of a five element vector. So you have to perform the squeeze here to get the five by one output of the depot network into some advantages five, a list of five elements or vector of five elements instead of five by one. So definitely that squeeze. If you don't believe me, by all means, raised a line or commented out and print out the shapes of things to the terminal. I always recommend doing that. It's a good way of solidifying your understanding of how all this stuff works. So then our credit loss. It's just the returns minus values, squared, pretty straightforward. So now let's go ahead and say We want the softmax activation of our output. And that has a property that, of course, the softmax guarantees that every action has a finite value. And that's the by the probabilities add up to one as all probability distributions, distributions should. So then we use that output to create a categorical distribution, and calculate the log probability distribution of our actions actually taken than our actual loss, minus log probs times the quantity turns minus values that's from the paper, and then our total loss is just predict loss after loss. That mean, we have to sum the two together, because of the way that the backpropagation is handled by pytorch. And, of course, I did forget to choose action function. But that is not a big deal, we'll just go ahead and handle that now. So that will take, we're going to call it observation as input, because we're gonna be passing on the raw observation from our environment. So we have to convert that to a tensor right off the bat. And we have to add a batch dimension to that, for compatibility with the inputs of our deep neural net. And we're gonna call it a D type of float, we pass that through our neural network, get our policy and value function out, perform a softmax activation on our policy along the first dimension, then we're going to create our distribution based on those probabilities and sample it and convert it to a NumPy quantity. Take the zeroeth element and return it. So that is it for our actor network class. It encapsulates most of the functionality I would associate with the agent, but it does everything we're going to need each of the independent actors within their own thread to do now we're going to move on to the agent classes going to handle our multi processing functionality. So I'm going to call this agent, you will sometimes see it referred to as worker and that is that fine name, that is kind of precisely what it is. I'm just using agent to be consistent with my previous nomenclature, it acid derived from the NP dot process class. So we get some access to some goodies there. So we will need our global actor predict that is what is going to handle the functionality of keeping track of all the learning from all of our environment specific agents. The optimizer that is going to be the shared atom optimizer that we wrote earlier. Input damns number of actions, gamma in case you want to use something other than 0.99 a learning rate, a name to keep track of each of the workers from our multi processing, global episode, index. So this will keep track of the total number of episodes run by all of our agents. It's not as easy as our content is it may seem because you're doing asynchronous processing, as well as an environment ID. So the first thing we want to do is call our super constructor. And go ahead and start saving stuff. So our local actor critic, is just going to be our new actor critic, with inputs of input demos, number of actions and gamma. You want to save our global actor critic. So that we can update its parameters. We're going to have a name for each worker and its own independent thread. And that's just going to be just work or a number name. And then we'll have an episode index. Well, welcome so that it backs our environment. So environment it is a string here if that isn't clear. And our optimizer, I spell that correctly. Yes, I believe I did. So we have to define a very specific function. So that stuff actually works and that function is the run function. This gets called behind the scenes. By the worker dot start function that we're going to handle in the main loop. But this handles effectively all the main loop type functionality of our problems. So our global time step is going to get set to one, while our episode ID x dot value, so episode ID x is a global parameter from the multiprocessing class, and that we want to get the value by using the dot value dereference. And while that's less than number of games, some some global variable we're going to define. In other words, what while we have not completed all of our games, set your terminal flag to false, reset your environment and set the score to zero. And go ahead and clear the agent's memory at the top of every episode. And then while you're not done, so go ahead and play your sequence of episodes. I'm sorry, a sequence of steps within an episode. Action is chosen by a local actor, critic to pass the observation into each local actor critic. So each of the 16 in this case, threads will get its own local actor critic and that'll synchronize took a global but you never actually use the global network directly to do things like choosing actions. So get the new state reward done and a bug info back from your vironment. Keep track, other award received as part of the total score. Remember that observation, action reward, then we have to say, if the number of steps modulus, the maximum number of steps is zero, in other words, if it is every if it is every fifth time step, or we have finished the episode with that last time step, then we're going to go ahead and perform our learning step. Last T, Max, or we're done, then we're going to go ahead and handle the learning functionality. So we'll say loss equals local actor predict loss. Of course, we need the most recent terminal flag as the parameter for that function, the argument, go ahead and zero your gradient and back propagate your loss. So we're going to set the parameter for the global gradient to the local agents gradient at this step. And then, after doing that, we can tell our optimizer to step and then we can go ahead and synchronize our networks. And we do that by loading the state dictionary from our global actor critic to our local one. And then we want to go ahead and clear the memory for each learning step, and then tell the algorithm to increment t step by one the global time step and set the current state to the new state. Then at the end of every episode, we have to handle the the fact that we may have finished an episode from another agent while one thread was running, because this is running a synchronous asynchronously. So we say with self dot global sorry, self dot episode ID x get locked. So we want to make sure that no other thread is trying to access that variable right now. And if we can go ahead and increment that episode value by one. Then we do the usual print the debug stuff to the terminal. We're going to print out the name of The worker, the episode that is self episode, Id x and the reward the total score from the episode, Matt, is it for our agent class. So this is relatively straightforward. All we're doing is handling the fact that we have our local parameter, our local agent that uses actions, we're going to perform the gradient descent optimizer update using the gradients calculated by the actions of that agent. And then upload that to our global network. So that every every agent uploads this learnings to the global network, and then we're going to go ahead and download that from our global network as well so that we make each agent in its own thread better over time. Now we have our main loop. And we're going to do some stuff like declaring our learning rate, or environment ID. Card poll, the zero number of actions for that environment is just to left and right, input them. So it's just a four vector, we'll say number of games 5000. You know, we can actually get away with 3000, I believe, team Max, every five steps that comes from the paper, a global actor critic, gets initialized here. And that takes input demos and number of actions. Want to tell that global actor critic object that we want to share its memory? And we have our optimizer? What are we going to be optimizing and sharing the parameters of a global actor critic network learning rate defined by learning rate and betas? 0.920999 I get that from more banjos stuff, I haven't experimented with that too much. A little bit of cargo cargo called programming here, I apologize for that. Now we have our episode nine is just going to be a value of Type II, that means unsigned integer, doesn't mean I as an account or variable. You could also have D for a double, if you wanted to keep track of like a score or something like that. I just mean unsigned integer, or no, it's a it's a signed integer can be negative as well as positive. Now we have to create a list of workers. And that's going to be a list of our agents. And this is what's this is the construct that we're going to use to handle the starting and running of our specific threads. So that's an agent, we have to pass on our global actor, critic, or optimizer, our input number of actions, a gamma learning rate, it's helpful to use commas, our name is just going to be the the name is just going to be the AI in our list comprehension. Our global episode ID x is going to be that global variable we just defined. And we're going to pass in our environment ID for i in range, multi processing dot CSV count. Now, I haven't noticed a huge difference. And running this with different numbers. So something other than CPU count and the range, I have 16 threads, if I tell it eight, it still uses all 16. So I'm probably missing something here, I could do a deeper dive into the documentation to figure it out. But for the purposes of the video, I'm just going to go ahead and roll with it. So now we want to go ahead and start all of our workers and then go ahead and do our join operation. Okay, so now moment of truth, I want to find out how many typos I made in the recording of this. Let's go ahead and do a right quit. I already have an invalid syntax. So I say for reward in that's an easy one. No others. So let's say a through c.pi. Okay, name, envy is not defined that's because it is self thought envy. So that is in line 114 self thought can be reset cannot assign torch flow tensor as child module pi one torch and in module or non expected so that is when I handle the feed forward in our choose action function who this is an interesting one. I haven't encountered this before let's take a look cannot assign as child module pi one. Oh, it's because interesting. Oh, of course. Yeah. Obviously, I don't want that self here. odd to annotate that one I am editing the video, that is obviously going to be problematic. So you'll probably have seen that already that I put in a no self.in. There. Okay. As the beauty of doing things for an O scoters. Log problem, that log probs that's in 85. Problem. Oh, by Remember, all these non type object has no attribute backward, I probably forgot to return my loss. Yeah, that's right. There we go. All right, now it's running. Hopefully, it's still gonna record, even though it is running across all of our threads. Looks like we're still good. Now you see that executed really, really fast. It was so fast, I didn't even have time to get up the thread count the Resource Monitor to show you guys. But you can see here that it achieves a non negligible scores that indicates some learning is going on. Now. Let's go ahead and run it again. And you can see now this time, it's not doing any learning at all. So there is a significant amount of runner run variation. So I'll let it run and finish. And we'll try one more time. And maybe we'll get lucky. And it will actually approach a score of 200. Now, that's one of my big criticisms of this algorithm is that there is a significant amount of run to run variation. And it is pretty brittle with respect to some other algorithms that came later. Basically, the main advantage here is that it's a synchronous. And this paradigm can be employed on many different types of algorithms. As you read in the paper, you know, it can be applied to deep learning, as well as and step learn stuff like that. Okay, so it's getting a little closer. Okay, so this one took a little bit longer, because the episodes actually achieve scores of 200, which is the max, not every thread manages to be a winner, not all of us can be winners in life. That's a nice little microcosm of the universe. But you can see that for the most part, some of the workers actually one throws consistency amongst so worker 11, not 200. And if I scan down here, another worker 11 140 to 161. Interesting, so you do get actual learning across your agents. It's just not super consistent. And that is my biggest criticism of this algorithm is that it has a high degree of render and variation, as you can see, but it's still pretty good. It's an interesting take on deep reinforcement learning. And I do enjoy the algorithm. It's just not what I would categorize as the top two or three algorithm and actor critic methods. I hope this was illustrative for you. I hope you found it very helpful. If you have made it this far, please consider subscribing, leave a like a comment down below and I'll see you in the next video.
