With timestamps:

00:00 - multi-threading is an important Concept
00:01 - in computer science in this course
00:04 - you'll learn everything you need to know
00:06 - about multi-threading in Java but the
00:08 - concepts apply to other programming
00:10 - languages as well for each concept
00:13 - you'll learn the theory and then see
00:15 - some code examples hello and welcome I
00:18 - hope you're doing super good my name is
00:20 - rendu and I'm working as a senior
00:22 - engineer with Uber I have been
00:25 - programming for more than a decade now I
00:27 - believe that multi-threading is one such
00:29 - concept which is way too abstract and
00:32 - difficult to understand if it is not
00:33 - taught in a proper manner however if the
00:36 - concepts are explained with relatable
00:38 - examples it becomes a fun and engaging
00:41 - experience that's what I have done in
00:44 - this course I have broken down the
00:46 - difficult and Abstract Concepts in
00:48 - simple English which is really easy to
00:51 - understand to make the things even more
00:53 - clear I have presented relatable
00:55 - examples I strongly believe that
00:58 - multi-threading is one such tool which
01:00 - should be in the toolkit of every good
01:02 - programmer the entire video is split in
01:04 - a smaller sections wherein I teach about
01:07 - a particular topic each topic is
01:09 - explained with some theoretical concept
01:12 - followed by the examples and then I
01:14 - implement the topic of discussion in
01:16 - Java to give you a proper working code
01:18 - example the topics are taught in a
01:21 - bottomup manner where I start from the
01:23 - very Basics and then I build on the
01:25 - concepts layer by layer by the end of
01:27 - this tutorial you would become very
01:30 - confident and comfortable with the
01:32 - concepts of multi-threading and that's
01:34 - my guarantee to you the code examples
01:37 - are in Java however most of the concepts
01:40 - should be transferable in other
01:42 - languages as well which support
01:44 - multi-threading in some capacity so with
01:46 - that in place let's get
01:52 - started so what is the motivation for
01:55 - multi-threading by default programming
01:57 - languages are sequential in nature code
02:00 - execution happens line by line in usual
02:02 - scenario consider the below code so in
02:05 - this method we have init de call then we
02:08 - have download data call then we have
02:10 - process on data and then finally show
02:12 - the results so in the usual scenario all
02:14 - these things will be executing one by
02:16 - one so first this will be called then
02:18 - this will be called then this then this
02:21 - but we have a problem in a single
02:22 - threaded program these instructions will
02:24 - be executed one by one and the time
02:27 - consuming section of the code can freeze
02:29 - the entire
02:30 - application what is the solution well
02:33 - figure out the timec consuming tasks and
02:35 - decide if they can be run separately if
02:38 - yes run such tasks in separate trades
02:41 - let's have a quick Layman explanation of
02:43 - how a timec consuming a step in your
02:45 - code can slow down or freeze your entire
02:48 - process let's say you invited your
02:50 - friend over to your place to watch this
02:52 - super cool movie being a great host you
02:55 - decided to make some popcorn for your
02:57 - friend but here is the catch it will
02:59 - take some 5 to 7 minutes to prepare the
03:02 - popcorn during the time you are involved
03:04 - in preparing the popcorn your friend
03:06 - asks which movie are we going toward
03:08 - wors today since you are super involved
03:10 - in making the popcorn you don't respond
03:13 - your friend even though feeling a bit
03:15 - weird about the situation asks you again
03:17 - if you're okay but thanks to your
03:19 - involvement in the process of making the
03:21 - popcorn you don't respond situation
03:23 - becomes super strange however your
03:25 - friend tries one final time and asks you
03:28 - if there did something wrong
03:30 - and thanks to your deep dedication in
03:32 - the process of popcorn making you don't
03:34 - respond by this time your friend gets
03:37 - freaked out and punches you in the face
03:39 - and you reboot but we all know this does
03:42 - not happen in real life unless you are
03:43 - playing a prank we humans are naturally
03:46 - equipped to multitask in this example
03:48 - since you would be aware of the time it
03:50 - takes to prepare popcorn you would
03:52 - probably prepare the recipe and put the
03:54 - pot on the stove and let the popcorn get
03:57 - prepared while it's getting prepared you
03:59 - are available ble to do anything if
04:00 - there is a need so you figured out the
04:02 - task which is going to be timec
04:04 - consuming started its execution and let
04:07 - it finish in its own line of execution
04:09 - effectively you did not block other
04:12 - tasks on you and did not freeze entirely
04:15 - if you follow line by line execution of
04:17 - tasks in your program this kind of
04:19 - freezing situation may arise in your
04:20 - code if there is a task which takes
04:22 - longer time to execute so what is the
04:25 - Improvement so in this case let's go
04:27 - through the different calls so in it de
04:29 - DB is where you are initializing certain
04:31 - DB related things then you have download
04:34 - data then you process the data then you
04:36 - show the results so to me it looks like
04:38 - downloading of the data is something
04:40 - which could take the major chunk of time
04:42 - what we can do now is put this download
04:45 - data in some sort of other threade and
04:47 - everything else in some other threade
04:49 - and in that sense we can do a parallel
04:51 - processing and it will ensure that by
04:54 - the time we are waiting for downloading
04:56 - the data everything else is not getting
04:58 - Frozen up and as a Sy is not lagging so
05:01 - this is one such Improvement we could do
05:03 - by the virtue of multi-threading so to
05:05 - give it a formal definition
05:06 - multi-threading is the ability of CPU to
05:09 - perform different tasks concurrently now
05:12 - let's have a quick explanation around
05:14 - concurrency versus parallelism
05:16 - concurrency is like having multiple
05:18 - tasks to do but you only have one set of
05:21 - hands you switch between the tasks doing
05:23 - a little bit of each one at a time if
05:26 - you play a guitar it's similar to that
05:28 - where you play different notes and cords
05:30 - using your nine fingers even though you
05:32 - play each note separately the switch is
05:34 - so fast and smooth that overall it
05:37 - appears as if everything is being played
05:39 - together parallelism on the other hand
05:41 - is again having multiple tasks but now
05:44 - you have many friends to help you out
05:45 - each friend works on a different task at
05:48 - the same time so all the tasks get done
05:50 - faster so in summary concurrency is
05:53 - doing multiple things all at once by
05:55 - quickly switching between the tasks and
05:57 - parallelism is doing multiple things at
05:59 - At Once by having different parts of the
06:02 - task been done simultaneously by
06:03 - different entities now let's learn about
06:06 - concurrency versus parallelism in
06:08 - somewhat more technical terms so
06:10 - concurrency and parallelism are two
06:12 - terms which are used quite a lot and
06:14 - that to interchangeably while discussing
06:17 - multi- threading but there is a subtle
06:19 - difference let's talk more about it
06:21 - concurrency refers to the ability of a
06:22 - system to execute multiple tasks at the
06:25 - same time or nearly overlapping times so
06:27 - they seem like being executed at the
06:29 - same time in concurrent systems tasks
06:32 - may start execute and complete
06:34 - independently of each other but they may
06:36 - not necessarily be executing
06:38 - simultaneously at any given moment
06:40 - concurrency is often achieved through
06:43 - techniques like multitasking where a
06:45 - single processor switches between
06:47 - executing multiple tasks rapidly or
06:49 - through the use of multiple threads or
06:51 - processes parallelism on the other hand
06:54 - refers to the simultaneous execution of
06:56 - multiple tasks to achieve faster
06:58 - performance of increased throughput in
07:01 - parel system tasks are truly executed
07:04 - simultaneously either on multiple
07:06 - processors or multiple processor course
07:09 - or through other means of parall
07:11 - processing like distributed computing or
07:13 - GPU Computing parallelism is all about
07:16 - breaking down a task into smaller
07:18 - non-related subtasks which can be
07:21 - executed concurrently to speed up the
07:23 - overall execution time thus in the
07:26 - context of a hardware with a single CPU
07:28 - code currency could be understood as a
07:31 - perceived parallelism or fake
07:32 - parallelism even more so in scenarios
07:35 - where tasks appear to be running
07:37 - simultaneously but are actually being
07:39 - executed sequentially or in an
07:41 - interleaved manner this is done by
07:43 - something called as time slicing
07:45 - algorithm so in summary concurrency is
07:48 - about managing multiple tasks or
07:50 - processes potentially interleaving their
07:53 - execution to give an appearance of
07:55 - simultaneous execution whereas
07:57 - parallelism on the other hand is about
07:59 - truly executing multiple tasks or
08:02 - processes simultaneously to achieve a
08:04 - fast performance while the terms are
08:06 - related and often used together they
08:09 - refer to distinct Concepts in the
08:10 - context of computing now let's
08:13 - understand what is a process and thread
08:15 - process is an instance of program
08:17 - execution when you enter an application
08:19 - it's a process the operating system
08:21 - assigns its own stack and Heap area
08:24 - whereas threade is a lightweight process
08:26 - it is a unit of execution within a given
08:29 - program a single process may contain
08:31 - multiple threads each thread in the
08:33 - process shares the memory and the
08:35 - resources of the parent process one
08:38 - single process could contain many other
08:41 - threads now let's learn a bit about the
08:44 - time slicing algorithm let's imagine we
08:47 - have multiple threads associated with
08:49 - the process somehow the CPU has to
08:51 - ensure that all these threads are given
08:53 - a fair chance to execute one such
08:56 - approach is to use the time slicing
08:58 - algorithm so uses time for the CPU is
09:01 - shared among the different threads so
09:04 - here is what happens so you see sharing
09:07 - is time slicing let's say the green
09:09 - boxes represent one thread and the
09:13 - Yellow Boxes represent another thread
09:15 - thread T1 and T2 respectively and
09:18 - consider that this is the timeline and
09:21 - at this particular time thread T1 is
09:24 - assigned to the CPU then after some time
09:26 - thre T1 takes a break and we assign
09:29 - thread T2 to the CPU and after some time
09:32 - T2 is given some rest and thread T1 is
09:35 - assigned again to the CPU so as you see
09:38 - it's going into a back and forth manner
09:40 - where each and every threade is taking
09:43 - turns to run on the CPU one by one so
09:46 - here what we are doing is we are
09:48 - basically slicing the time and we are
09:50 - assigning certain time Quantum to the
09:53 - CPU so here we have a CPU and these are
09:56 - the two different threads which are kind
09:58 - of taking its turn to be executed on the
10:01 - CPU so this is how the time slicing
10:03 - algorithm works now what happens when we
10:06 - have enough CPU at our disposal so let's
10:09 - say we have thread one and we have
10:10 - thread 2 and there are two CPUs so in
10:13 - that case thread 1 will run entirely on
10:16 - CPU 1 and thread 2 will run entirely on
10:19 - CPU 2 so it's effectively a parallel
10:23 - kind of processing wherein we are not
10:25 - sharing anything on a given CPU rather
10:28 - each threade has has a dedicated CPU and
10:30 - it does not need to bother about whether
10:33 - it has to share the CPU with the other
10:35 - thread or not and please note that I
10:37 - have put CPU here but it could be a
10:40 - different core in the CPU itself so it
10:43 - could be either different cores of a
10:44 - given CPU or it could be different CPUs
10:47 - so that depends on the hardware in such
10:49 - kind of setup we can achieve the
10:51 - parallel
10:53 - processing now let's look at some of the
10:55 - pros and cons of multi- threading the
10:57 - first one is we can build responsive
11:00 - applications so now you don't have to
11:01 - worry about freezing uh situation and
11:04 - thus you can build your applications to
11:06 - be responsive second is you will have a
11:09 - better resource utilization because now
11:11 - with the use of multi-threading you
11:13 - could ensure that your Hardware or your
11:15 - CPU is not sitting idle rather once it's
11:18 - idle it could be taken up by some other
11:20 - thread for execution and the third thing
11:22 - is it helps us into building performant
11:25 - applications so with the help of
11:27 - multiple core CPUs we can build parallel
11:30 - programs and essentially we could get
11:33 - some benefit on the side of performance
11:35 - as well now coming to the cons of the
11:38 - multi-threading the first one is
11:40 - synchronization needs to be done and it
11:42 - can get tricky at times so essentially
11:45 - when you are doing multi-threading you
11:47 - need to share the memory space and other
11:50 - resources with the process and in that
11:53 - case let's say when there is a process
11:54 - and there are certain number of threads
11:57 - you need to share the resources so so we
11:59 - need to ensure that we are not running
12:01 - into funny situations and those things
12:03 - are handled by something called as
12:05 - synchronization we will have a much more
12:07 - focused discussion around all these
12:09 - things later in the video the second
12:11 - thing is it is difficult to design and
12:13 - test multi-threading apps so essentially
12:15 - you don't have a control in which the
12:17 - different threads could execute so in
12:19 - that sense it's difficult to predict the
12:21 - behavior of the threads so it's
12:24 - difficult to design and test
12:25 - multi-threaded applications and the
12:27 - third thing is thread context switch is
12:30 - expensive so if there are more than
12:32 - required number of threads then it
12:34 - becomes detrimental to your system
12:36 - performance so multi-threading is not a
12:38 - silver bullet which will help you with
12:40 - all the situations rather we should use
12:43 - it
12:46 - judiciously now let's have a look on the
12:48 - thread life cycle any thread will start
12:50 - its lifetime in the new state and every
12:53 - threade is in this state until we call
12:55 - start on it after we have called a start
12:58 - on it it goes to something called as
13:00 - active State and this active state has
13:02 - two substates either it could be
13:04 - runnable or running as we saw in the
13:06 - earlier slides in some cases we may have
13:09 - to do some sort of time slicing and in
13:12 - that case there could be five threads
13:14 - which are ready to run but there is no
13:17 - CPU available on which it could run and
13:19 - we have called a start on such threads
13:22 - so those trades will be runnable State
13:24 - and there could be certain threads which
13:26 - will be in running State and as soon as
13:28 - those running State threads are done
13:30 - then they could allow the threads in the
13:32 - runnable state to run again and this is
13:35 - what we mean when we say that it has two
13:37 - substates which is runnable and running
13:39 - effectively this is the active State and
13:41 - the third state is the blocked state so
13:44 - every threade is in this state when it
13:46 - is waiting for some thread to finish so
13:48 - let's imagine there are two threads T1
13:50 - and T2 and then they both started
13:53 - running on the CPU and after some time
13:55 - T1 got a chance and it was executing its
13:58 - task after some time it had to be taken
14:01 - out of the CPU and T2 got a chance but
14:05 - now T1 is not completed it's waiting for
14:08 - its execution to complete because T2 is
14:11 - now on the CPU so T1 is in a blocked
14:14 - State and this is what we mean by the
14:16 - blogged state now T1 will get a chance
14:18 - to execute on the CPU and maybe it may
14:22 - be done with its entirity of execution
14:24 - and then it goes to a state called as
14:27 - terminated state so every threade is in
14:29 - this state after it's done doing its
14:31 - required
14:34 - task here we are in the ID I have
14:38 - created a normal Java project and it's
14:40 - called as multi-threading So the plan is
14:42 - that for the entire duration of this
14:44 - tutorial I'll be using the same project
14:46 - and I'll be creating different packages
14:48 - inside the project to discuss the
14:50 - concepts of the multi-threading so in
14:52 - this particular section we will be
14:54 - discussing about the sequential
14:55 - execution so in order to demonstrate the
14:58 - code let's create a class call it a
15:00 - sequential execution demo and here is
15:03 - the idea behind this particular class I
15:05 - will be creating certain methods and the
15:08 - intent of this particular class is to
15:10 - Showcase that in a normal Java program
15:13 - the execution happens line by line and
15:15 - there is no jumping around from this
15:17 - part of the code to the other one so
15:19 - let's get going so to begin with I'll
15:21 - create the main method and in the main
15:24 - method I will have two
15:27 - methods let's call those as demo 1 and
15:30 - demo 2 and let's create those two
15:36 - methods I won't be doing anything fancy
15:39 - I'll just create a normal for Loop which
15:42 - will be iterating in certain range and
15:45 - then it
15:47 - will print some
15:51 - message and that's it so let's copy this
15:55 - one
15:57 - and let's change the
16:02 - name and here let's change the message
16:05 - as well and now let's run the
16:09 - program on running this this is the
16:11 - outcome that we have so first we were
16:14 - executing demo one method so the
16:15 - entirety of demo one is executed wherein
16:18 - it will be printing from 0 to 4 with
16:22 - this message which is from demo 1 plus I
16:25 - so from demo 1 0 from demo 1 2 till 4
16:28 - and likewise
16:29 - we have executed demo 2 and in that case
16:32 - we print this message which is from demo
16:34 - 2 and the IAT number which is from demo
16:38 - to 0 to from demo to 4 so what we see is
16:42 - that the execution happens line by line
16:44 - so the main method is the first one to
16:47 - get started and the first thing it sees
16:49 - is that we are invoking a method called
16:51 - as demo one it goes there it executes it
16:54 - it comes back then the next line it says
16:57 - is that it's demo 2
16:59 - it goes to demo 2 it executes it and it
17:01 - comes back here and then the execution
17:04 - terminates so this is what we mean by
17:06 - the sequential execution so in the
17:08 - context of multi-threading what we can
17:10 - understand is that each and every
17:12 - program is single threaded unless
17:14 - otherwise instructed so here we just
17:16 - have a single threade and that is the
17:18 - threade that is created by the jvm for
17:20 - the execution of this main method and
17:22 - this could also be called as the parent
17:24 - thread or maybe the main thread
17:30 - now let's learn about the way in which
17:32 - we can create threads in Java and the
17:34 - first way is to implement a runnable
17:37 - interface so we will create a class
17:39 - let's call it as
17:42 - runable thread
17:45 - example and let's have a main method
17:47 - created the way it works is that we will
17:50 - have to Define some sort of class and
17:53 - the class will Implement our enable
17:55 - interface so let's do that so let's call
17:59 - it as thread one and it will implement
18:02 - the runnable interface and the runable
18:05 - interface has one method which we need
18:07 - to implement so that's run method and
18:10 - the logic is whatever we Implement
18:12 - inside the run method that is executed
18:15 - by this thread so let's do that so let's
18:18 - have a for Loop which runs from i0 to
18:23 - I4 and it prints a message let's call it
18:28 - as thread one and I would be the ith
18:32 - time it has been called now let's create
18:35 - an another
18:36 - thread let's call it as
18:39 - trade two which implements
18:43 - runnable and let's implement the run
18:47 - method and here as well we can run
18:52 - from i0 to I4
18:57 - and let's print the message call it as
19:01 - thread to and I so this is a way in
19:05 - which we can Define the threads and once
19:07 - the threads are created they need to be
19:09 - somehow started so in order to do that
19:12 - what we can do is we can define a thread
19:15 - let's call it as one then new thread and
19:19 - we can pass the class that we have
19:21 - created so thread
19:24 - one and likewise we can say thread two
19:29 - new thread new thread
19:33 - two and we have the handle for these two
19:36 - threads 1 and two so how do we start
19:38 - these threads well we have a method
19:40 - called as start so let's do
19:45 - that so what happens is once you call
19:47 - the start method jbm will start these
19:50 - two threads and they are in the runnable
19:52 - state so they could be either
19:54 - immediately running or they will have to
19:56 - wait because they don't have any CPU
19:59 - available at their disposal where they
20:01 - could go and run so let's run this and
20:04 - see what is the outcome
20:06 - like so what we see here is first thread
20:09 - one is running then we have thread two
20:11 - running but this may not be the case
20:13 - always so in order to see a clear
20:15 - example let's increment the number of
20:17 - times we are going to print this message
20:19 - so let's increment it to 10 and let's do
20:23 - this to 15 now let's run the program and
20:26 - see the outcome so here here is what we
20:29 - see first we have thread one running and
20:31 - then thread two takes over and then
20:34 - thread one is running and then thread
20:36 - two takes over and so on and so forth so
20:39 - once everything is executed the
20:41 - execution will stop and the program will
20:43 - terminate so what we are observing is we
20:46 - have created two threads we have started
20:48 - the threads but there is no sequence in
20:50 - which they are executing rather the
20:53 - thread has been created and it's
20:54 - available to be scheduled by the thread
20:56 - scheduler and once the thread scheder
20:59 - finds an available spot for a particular
21:01 - thread to be run on the CPU it's
21:03 - assigned to the CPU and that's the time
21:05 - it's running for the time when it does
21:07 - not have the access to the CPU the
21:10 - thread will have to wait and that is the
21:12 - reason we are seeing a back and forth
21:13 - execution pattern wherein first one
21:16 - thread will run for some time then
21:17 - thread two will take over and then
21:19 - thread two will wait for some time and
21:21 - then thread one will take over there is
21:23 - also a different way in which we can
21:25 - create a threade using the runable
21:26 - interface and that is by making use of
21:29 - the anonymous in a classes so let's
21:33 - create trade three then new trade three
21:37 - and what we can do is new
21:43 - runable and
21:45 - let's print a similar kind of message so
21:48 - I less than let's say
21:51 - 15 I
21:54 - ++ the message could
21:56 - be 33 plus
22:00 - I and we can do the same thing which is
22:03 - 3. start because three is the handle
22:06 - that we have given for this particular
22:08 - thread one thing which you can observe
22:10 - is that this could be easily turned to a
22:13 - Lambda so let's do
22:15 - that and here we have a much cleaner way
22:18 - of creating a thread using the runnable
22:20 - interface so all we need to do is inside
22:23 - the Lambda we can provide the logic
22:25 - which needs to be executed by that
22:27 - particular thread now let's run it and
22:29 - see its
22:31 - outcome so we can see thread 2 is
22:33 - running then thread 1 is running then
22:35 - thread three is running and every thread
22:38 - gets some time of execution with the CPU
22:40 - and eventually all the threads are
22:42 - executed and terminated so this is how
22:45 - we can create threads in Java by
22:47 - implementing the runnable
22:51 - interface the other way of creating a
22:53 - thread in Java by extending the thread
22:56 - class with the help of extend scale
22:58 - keyword let's learn about the same so
23:01 - let's create a class and let's call it
23:03 - as
23:04 - extends thread
23:07 - example let's create the main method and
23:11 - now let's create the different threads
23:14 - so let's call it as thread one
23:18 - extends thread class and likewise we had
23:21 - to overwrite the run method in the
23:23 - example of runable approach we need to
23:26 - do something similar here as well so
23:28 - let's do
23:33 - that and we can have a for
23:36 - Loop which runs from I as 0 to 9 print
23:42 - some
23:48 - message let's copy
23:54 - this let's call This Thread two
23:59 - let's change the message as
24:01 - well so we have created two threads
24:05 - which is thread one and thread two now
24:07 - we need to instantiate it so let's call
24:10 - this as thread one and then new thread
24:16 - one then
24:19 - threade
24:21 - two and New threade
24:24 - 2 please note that here we are directly
24:27 - creating the thread as we are not
24:29 - passing this object inside the thread
24:32 - Constructor like we were doing in the
24:34 - case of runnable approach so now once we
24:36 - have the handle for the threads we can
24:39 - call do start on these
24:41 - two let's run this
24:44 - method and see the results so what do we
24:47 - see we have thread one running then
24:50 - thread 2 takes over now again we have
24:52 - threade one and then threade two takes
24:55 - over finally everything is executed and
24:57 - the program gets terminated so the basic
25:00 - idea Remains the Same once we call do
25:02 - start on these trades they are in the
25:04 - runnable state and based on the
25:06 - availability of the CPU they will be
25:08 - submitted to one CPU and they could
25:11 - start with their
25:14 - execution now that we have seen both the
25:17 - approaches of creating a threade one by
25:19 - implementing the renewable interface
25:21 - other by extending the thread class
25:23 - let's see which approach is better so if
25:26 - we extend thread then we cannot extend
25:29 - any other class usually it's a big
25:31 - disadvantage however a class May
25:34 - Implement more than one interface so
25:36 - while using the implements runnable
25:37 - approach there is no restriction to
25:39 - extension of class now or in the future
25:41 - so in most of the cases runnable is a
25:44 - better approach to create a
25:48 - thread now let's learn about do join
25:51 - method in Java let's create this class
25:54 - let's call it as join thread
25:56 - example and first we create create the
25:58 - main
25:59 - method let's create thread
26:02 - one and we can use the anonymous in a
26:06 - class or maybe Lambda to create the
26:09 - thread So This Thread is going to print
26:13 - from 0 to
26:17 - 4 and it will have a message call this
26:21 - as thread 1 followed by I let's copy
26:26 - this one
26:29 - let's call this as thread
26:31 - 2 let's change the message as well now
26:34 - thread
26:35 - 2 and this is going to be printed for
26:38 - let's say 25
26:40 - times and first we call do start on the
26:45 - one then we call do start on the two and
26:49 - then let's have a message which
26:53 - says done
26:55 - executing
26:56 - the threads
26:59 - so what do you think is going to be the
27:01 - output for this particular program if
27:03 - you're new to multi trading then you
27:05 - could say that first these two threads
27:07 - will be executed and we will see all
27:09 - these messages getting printed on the
27:11 - console and finally we will have this
27:13 - message printed but looks like it's not
27:15 - going to be the case so let's run it and
27:18 - find it
27:20 - out and definitely it's not the case in
27:23 - fact done executing the threads is the
27:25 - first thing to get printed on the screen
27:28 - so why is it happening so in order to
27:31 - understand this we will have to take a
27:32 - step back and understand how does the
27:34 - main method works so in this particular
27:37 - program main method is the first thing
27:39 - which is getting called by the jbm and
27:42 - when this happens this main method is
27:44 - run by your main threade so this main
27:47 - threade is the first one which gets
27:49 - assigned to the CPU with the highest
27:51 - priority we will learn about priority
27:53 - and all those things in some time but
27:55 - for now understand that this main thread
27:58 - has the highest priority so it starts
28:00 - with its execution first thing it does
28:03 - is it creates the definition for threade
28:05 - one second thing it does is it creates
28:07 - the definition for this trade two and it
28:10 - comes to line number 17 then to 18 and
28:13 - in these two lines it moves these two
28:15 - threads in the runable state and finally
28:18 - on line number 19 we have this message
28:22 - since the main thread has the highest
28:24 - priority for now this message is printed
28:26 - first so what happens is all these
28:29 - trades are executing independent of each
28:32 - other so threade one will start with its
28:35 - execution independently so will thread
28:38 - two and the main thread anyway has the
28:40 - access to the CPU for now it's going to
28:42 - print this one as soon as possible and
28:45 - that is the time it's done with its
28:47 - execution and it waits for these two
28:49 - threads to complete their execution and
28:51 - once that is done then the program is
28:53 - going to terminate so what should I do
28:56 - if I have this functionality wherein I
28:58 - want that thread one should be completed
29:00 - and only after that happens the main
29:03 - method or the main thread should proceed
29:05 - with its execution so in order to
29:08 - implement that functionality I can make
29:10 - use of dot join method so let's call do
29:13 - join on thread one and do join method
29:17 - throws an interrupted exception in order
29:19 - to correct this we can either surround
29:21 - this with TR catch or could add the
29:24 - exception being thrown in the method
29:26 - signature itself I'll go with the second
29:28 - option now with this in place let's run
29:31 - the program and see what is the outcome
29:33 - so this is what happens threade one
29:36 - needs to be executed five times and
29:38 - threade two needs to be executed 25
29:41 - times so first threade one is executed
29:43 - one is getting executed and looks like
29:46 - by this time thread one is executed and
29:48 - at the same time thread 2 got hold of
29:51 - the CPU so it started executing and you
29:55 - notice that for the next time thread one
29:57 - was was supposed to be executed but
29:59 - threade one did not have anything left
30:01 - for execution effectively it was done
30:04 - with its execution and that is where the
30:07 - dot join came in effect and it
30:09 - instructed to the jvm that one is done
30:11 - with its execution now it's time for the
30:14 - main thread to take over and proceed
30:16 - with its execution which is printing
30:18 - this particular line so now we print
30:21 - this line and then thread 2 proceeds
30:23 - with its execution it's going to print
30:25 - all the messages by thread 2 and as soon
30:28 - as this is done both the threads are
30:30 - completed and now the main thread also
30:32 - shuts down so what we learned here is 1.
30:35 - join is kind of hinting to the jvm that
30:38 - as soon as I am done with my execution
30:40 - then you can start with the execution of
30:43 - other threads which are in the Que in
30:45 - this case we had two and the main thread
30:48 - so first two was executed for a while on
30:50 - the CPU then the main thread started
30:53 - with its execution which was printing
30:54 - this line so what happens if I place two
30:57 - do join as well so in that case jbm will
31:01 - not mly wait for the thread 1 to get
31:03 - completed rather it will also Wait For
31:05 - Thread 2 get completed and only after
31:08 - that this message will be printed so
31:10 - let's try that out as well let's rerun
31:13 - the code and see what is the
31:15 - outcome so you see all the threads are
31:18 - executed so basically one and two are
31:20 - finished and only after that we see the
31:23 - outcome that is done executing the
31:24 - threads now let's print a message before
31:27 - executing these threads and what we can
31:29 - say is before executing the
31:37 - threads now let's run
31:41 - it so you see first we see the message
31:45 - before executing the threads now all the
31:47 - threads are executed because we have
31:49 - placed dot join on both of these two and
31:52 - finally done executing the threads is
31:55 - being
31:56 - printed so basically why this is
31:58 - happening is till this point of time we
32:00 - have not put the threads into the
32:02 - runable state so the main thread is the
32:04 - only active thread in this context and
32:07 - this is the reason we printed this as
32:09 - soon as weed at this line so now let's
32:12 - understand about the join operation with
32:14 - some theoretical Concepts so first thing
32:17 - to notice is that main thread is the
32:19 - parent thread so when we start a program
32:21 - usually the execution begins with the
32:23 - main method this method runs on the main
32:26 - thread this can be understood as the
32:28 - parent thread since it responds the
32:31 - other threads as well then the other
32:33 - important point to notice here is the
32:35 - independent execution of threads under
32:37 - normal circumstances so when you create
32:39 - and a start the threads they run
32:41 - concurrently with the main thread unless
32:43 - instructed otherwise so under normal
32:45 - circumstances all threads run
32:47 - independent of each other more
32:49 - explicitly no thread waits for other
32:51 - thread so what is join method well
32:54 - imagine threads to be lines of execution
32:56 - so when we call do chwine on a certain
32:58 - thread it means the parent thread which
33:01 - is the main thread in this case it's
33:03 - saying hey thread once you are done
33:04 - executing your task join my flow of
33:07 - execution it's like the parent thread
33:09 - waits for the completion of the child
33:11 - threade and then continues with its
33:14 - execution and here is my perspective on
33:17 - this concept well personally I find the
33:19 - join keyword is not very intuitive at
33:22 - first for the kind of operation it's
33:24 - doing somewhat better terms could have
33:26 - been wait for completion or complete
33:29 - then continue what's your perception
33:31 - about the joint method in Java let me
33:33 - know in the comment
33:37 - section now let's learn about the
33:39 - concept of ton and user threads on the
33:42 - basis of surface of execution threads
33:44 - can be of two types demon threads or
33:46 - user threads demon threads usually run
33:49 - in the background wherein user threads
33:52 - are the active threads so when a Java
33:54 - program starts the main thread starts
33:56 - running immediately we can start
33:58 - children threads from the main threade
34:01 - the main threade is the last threade to
34:03 - finish its execution under normal
34:05 - circumstances because it has to perform
34:08 - various shutdown operations demon
34:10 - threads are intended to be helper
34:12 - threads which can run in the background
34:15 - and are of low priority for example
34:17 - garbage collection thread demon threads
34:19 - are terminated by the jvm when all other
34:22 - user threads are terminated or they are
34:24 - done with their execution so under the
34:27 - normal circumstance stances user threads
34:29 - are allowed to be terminated once they
34:31 - are done with their execution however
34:33 - the demon threads are shut down by jbm
34:35 - once all the other threads are done
34:37 - executing now let's have a quick code
34:40 - demo for the concept of demon threads
34:42 - and user threads so let's create this
34:44 - class called as demon user crate demo
34:48 - and uh first of all we will create the
34:50 - main method now let's create two threads
34:54 - first
34:55 - is demon helper
34:58 - implements
35:04 - run
35:09 - let's overwrite the run
35:12 - method the second thread is
35:17 - user
35:19 - thread
35:21 - helper lements
35:23 - runable let's overwrite the the run
35:27 - method
35:29 - now let's write the code which needs to
35:30 - be executed by the demon threade let's
35:33 - have a counter variable start with zero
35:37 - and while count is less than
35:42 - 500 do a thread do
35:44 - sleep and then do a account
35:48 - Plus+ finally print this
35:52 - message which is
35:54 - demon helper
35:58 - running the sleep will expect us to pass
36:01 - certain time let's give the time for
36:03 - 1,000 milliseconds and the Sleep Method
36:06 - throws an interrupted exception so let's
36:08 - surround this with try catch for the
36:11 - user thread method let's have a sleep
36:14 - timer for let's
36:16 - say 5,000
36:19 - milliseconds and let's surround this
36:21 - with dry
36:23 - catch
36:26 - then print this message user
36:29 - thread done with
36:37 - execution now let's create these threads
36:39 - in the men
36:41 - thread so first we start with the demon
36:45 - thread let's call this as a background
36:46 - thread that is bz thread so
36:50 - new thread new demon helper
36:57 - for the user helper maybe we can call
37:00 - this as user
37:02 - thread new thread
37:05 - new user helper by default any threade
37:08 - is not a demon threade the way to make a
37:10 - normal threade as a demon threade is by
37:12 - calling do set demon method so let's do
37:15 - that bz thread. set demon and
37:19 - true finally we will start these two
37:22 - threads so BG thread. start and user
37:26 - thread. start
37:28 - now let's run this
37:36 - program so here is what happens the user
37:39 - thread has a sleep timer of 5 Seconds so
37:42 - as long as the thread was started it got
37:45 - assigned to the CPU and once it was
37:47 - assigned to the CPU it went into a sleep
37:49 - stage and then the demon thread was
37:52 - assigned the CPU and during that time
37:55 - there was no threade which was
37:56 - contesting for the CP CPU so the demon
37:58 - threade kept on running in the
38:00 - background so it ran for 4 seconds so
38:03 - remember we had the sleep timer for,
38:05 - millisecond so after every second this
38:07 - was running and printing the message
38:09 - demon helper running as soon as the 5sec
38:12 - got completed this thread came back into
38:15 - existence and it printed user thread
38:18 - done with execution and once this
38:20 - happened then the user threade helper
38:23 - got concluded it got finished with its
38:25 - execution and now what jbmc is is that
38:28 - there is no other threade which needs to
38:30 - be executed so by this time all the user
38:33 - threads which is basically just this
38:35 - threade is done with its execution and
38:37 - now it's time to shut down the jvm and
38:40 - that is where even though this demon
38:41 - thread was not completed because in the
38:43 - normal case of operation it could have
38:45 - ran till the count was less than 500
38:48 - which was certainly not the case because
38:50 - it ran just for four times but still it
38:53 - was terminated because the user thread
38:55 - was terminated so this is what we
38:57 - learned from from this particular demo
38:58 - that user threads are given the priority
39:01 - demon threads run in the background and
39:03 - once all the user threads are concluded
39:06 - the jvm shuts down the main thread and
39:08 - along with that the demain thread is
39:10 - also forced to be shut
39:14 - down let's learn about a very important
39:16 - concept of thread priority so let's say
39:19 - there are 10 threads in runnable state
39:22 - however there is only one available CPU
39:24 - so only one threade can execute at a
39:26 - given point point of time others will
39:29 - have to wait so who decides which trade
39:31 - gets to run on the CPU well the
39:34 - component who decides this is called as
39:36 - the thread scheduler so each threade has
39:39 - a certain priority to begin with and
39:41 - under normal circumstances the thread
39:43 - with the higher priority gets to run on
39:46 - the CPU please note the keyword under
39:48 - normal circumstances the priority value
39:51 - varies from 1 to 10 and it can be
39:54 - assigned to any thread one priority is
39:56 - represented as the Min priority and 10
39:58 - priority is represented as the max
40:00 - priority by default the priority of a
40:03 - thread is five and is represented as
40:05 - Norm priority Norm as normal threads of
40:08 - the same priority are executed in fifo
40:11 - manner so the thread scheduler rest
40:12 - stores the threats in a que now let's
40:15 - just spend some time to understand this
40:17 - part of normal circumstance so well what
40:20 - happens is all the main threads are
40:22 - started at a normal priority of five but
40:25 - still they are privileged to be executed
40:28 - first on the CPU by the thread scheduler
40:31 - why so well it's by Design since it's a
40:34 - main thread it has to be executed first
40:36 - otherwise your program will have to wait
40:38 - unnecessarily so this is the first thing
40:41 - which gets to be executed in any case
40:44 - even though the priority of the main
40:45 - thread is five it gets the first
40:48 - priority to be executed on the CPU for
40:50 - the first time now let's have a code
40:52 - example to have a demo of the thread
40:54 - priority concept so let's create this
40:56 - class called as threade priority example
41:00 - let's have a main method we can get the
41:02 - priority of the current threade by
41:04 - calling threade do current trade. get
41:08 - name so let's print this out now let's
41:11 - get the priority of the current trade so
41:14 - we can call trade. current trade. getet
41:18 - priority let's print this out as well
41:21 - now let's get the current threade so
41:23 - threade do current threade do set
41:25 - priority and we have Max priority let's
41:29 - print the updated
41:35 - priority let's run this so the outcome
41:38 - we will expect is the name of the main
41:40 - threade then the current priority which
41:42 - is the default priority because we did
41:44 - not change it earlier and after we
41:47 - change it what is the priority let's run
41:49 - and find
41:52 - out so the name is main to begin with
41:56 - the default priority is five we updated
41:58 - the priority to maximum and then we
42:01 - printed the
42:03 - same let's print
42:07 - something so thread
42:10 - dot current threade
42:12 - doget
42:15 - name says hi
42:18 - now let's create one
42:24 - trade say
42:28 - thre
42:30 - one says high as
42:36 - well and by default the new threade
42:39 - which is threade one will have default
42:41 - priority of five now let's give this the
42:44 - maximum priority possible which is trade
42:46 - do Max priority and then 1 do start so
42:51 - what do you think will happen now that
42:53 - trade one has the maximum priority and
42:55 - the main threade is still at at the
42:57 - default priority of five I should expect
43:00 - that this one should be executed first
43:02 - then I should see the outcome for this
43:04 - one so let's run it out and see what is
43:06 - going to
43:07 - happen well what we see is the first one
43:10 - to get executed is main says High then
43:14 - the second is threade one says high as
43:15 - well so please note that this property
43:18 - of main threade getting the first
43:20 - priority or the higher priority to get
43:23 - executed on the CPU regardless of this
43:25 - having the lower priority which is five
43:27 - as compared to the max priority is only
43:30 - for the first time this thing gets
43:32 - executed once execution is starts it
43:35 - will go by the fif manage and then the
43:38 - thread scheder will schedule the threads
43:40 - based on their priority and if they
43:42 - happen to be of the same priority fif
43:44 - will be used to schedule the threads so
43:46 - this is how we deal with priority in the
43:49 - case of threads in
43:55 - Java now that we have learned the basic
43:58 - concepts of multi- threading regarding
44:00 - how we can create trades and how we can
44:02 - wait for Threads to complete using the
44:04 - join keyword Etc now it's time to learn
44:06 - about the important concept of thread
44:09 - synchronization so let me create a
44:12 - package to write the code so let's call
44:16 - this as trade
44:18 - synchronization and I will Implement a
44:21 - class let's call this as synchronization
44:24 - demo
44:31 - so here is what I want to implement in
44:33 - this class I want to have a counter
44:35 - variable that will be initialized with
44:38 - zero and I will be using two threads to
44:40 - increment this number up to certain
44:42 - times and in the end I will check the
44:45 - final value of the counter variable so
44:47 - let's do this so first thing is let's
44:50 - create the main method and uh let's have
44:53 - a
44:54 - counter variable as well let's let's
44:57 - call this as private static int and
45:00 - counter let's initialize this to zero
45:04 - let's create two threads so as seen in
45:07 - the earlier video that we can create
45:10 - threads in line using the Lambda
45:12 - expression so let's do
45:15 - that and uh in this trade what I want to
45:19 - do is that I want to increment the value
45:23 - of counter let's say 10,000
45:25 - times and let's call this as counter
45:29 - Plus+ let's do the same using another
45:32 - thread so I have a code template that I
45:35 - could make use of so that will save me
45:39 - some time
45:40 - to type the code again let's have this
45:44 - for Loop run 10,000 times and those many
45:48 - times we will increment the counter
45:51 - variable then in the last what I will do
45:55 - is I will start these two
46:02 - threads so let's name this as
46:08 - two
46:11 - oops and uh two. start and before
46:15 - printing the final value of the counter
46:18 - I want to make sure these two threads
46:20 - are
46:21 - completed so the way to do is I would
46:24 - call join on this so let's surr this
46:27 - with try catch let's duplicate it and
46:31 - let's call the same
46:32 - for trade two as well and finally we
46:38 - could print
46:39 - the counter
46:42 - value now let's run this one and see
46:46 - what is the outcome like so let's run
46:48 - this and see the output so the output
46:52 - that we have is
46:54 - 16583 but what we expect I is that since
46:58 - this trade is executing it 10,000 times
47:01 - and this trade is running this for
47:03 - 10,000 times then effectively it should
47:06 - have been 20,000 right but that is not
47:08 - the case what we are having is
47:12 - 16583 how about let's run it one more
47:15 - time and this time we are getting a
47:17 - totally different number let's run it
47:19 - one more time and we are getting a
47:21 - totally different number so this is not
47:23 - correct right then how can we correct
47:26 - this so even before we learn how can we
47:29 - correct this it makes sense to learn why
47:31 - this is happening so the reason for this
47:34 - behavior is something called as a
47:37 - nonatomic operation so what exactly is a
47:40 - non-atomic operation well what we see
47:43 - here is we are incrementing the counter
47:46 - variable counter is counter plus one
47:51 - even though it appears as if the counter
47:54 - variable will be incremented by one in
47:56 - one shot but that is not the case under
47:58 - the hood under the hood what will happen
48:01 - is first the counter variable will be
48:03 - loaded in the memory then it will be
48:05 - incremented by one then this will be
48:08 - assigned back to the counter variable
48:11 - which is holding this particular value
48:13 - and that is where we set the variable
48:16 - value back to counter plus one so
48:18 - essentially this is consisting of three
48:20 - steps so the first step you could think
48:23 - of it as load second step is the actual
48:27 - increment and then set back
48:31 - the value so let's say at this point of
48:35 - time thread one starts operating on this
48:38 - particular logic which is incrementing
48:41 - the counter variable let's say at that
48:43 - point of time the value is zero and uh
48:48 - it incremented the value the
48:52 - incremented value became as one and now
48:55 - it's time to apply this value
48:58 - incremented value back to counter
49:00 - variable but at the same time threade 2
49:02 - came into the picture and what it did is
49:05 - it said hey let's load the value for
49:07 - counter and it found that the counter
49:10 - value still is zero and then it went
49:12 - ahead so let's call this as this being
49:16 - uh the operation by thread one and now
49:20 - thread 2 came into the picture it said
49:22 - hey let's load the value for counter and
49:25 - it is zero and what I'll do is I'll
49:28 - increment the value so after
49:30 - incrementing the value it became as one
49:33 - and this is where we have thread 2 doing
49:37 - this operation so effectively this value
49:40 - should have been two by this point of
49:42 - time but unfortunately since the time at
49:45 - which this threade one could have set
49:47 - the value as one and applied that value
49:51 - back to the counter variable and that is
49:53 - where the thread 2 came and it
49:55 - intercepted the uh
49:57 - execution and it found out that the
49:59 - value was Zero it incremented it back to
50:01 - one what should have been uh a two here
50:05 - that is still one right so this is the
50:09 - inconsistency and this is the
50:10 - inconsistency due to which we are seeing
50:12 - this error that we are not getting the
50:14 - final value of 20,000 here rather we are
50:17 - stuck at some lesser number so what
50:20 - exactly we call this phenomenon well so
50:22 - this thing is called as race condition
50:24 - in the terminology of multi- threading
50:26 - and concurrent programming so what we
50:28 - have here is a shared resource that is
50:31 - the counter we have two threads working
50:33 - on the same shared resource and these
50:36 - kind of scenarios lead to
50:38 - inconsistencies which are called as the
50:40 - risk conditions so how exactly can we
50:43 - fix this so the way to fix this is if
50:46 - you have different uh counters on which
50:48 - these two threads could work on then
50:50 - probably this could work on one counter
50:52 - this could work on another counter but
50:54 - that is not the case here right the
50:56 - whole point of discussing this example
50:58 - is that we have a shared resource and we
51:00 - want to make use of multi-threading
51:02 - concept to have some sort of operation
51:04 - applied to this particular share
51:05 - resource so in order to avoid this kind
51:08 - of situation what we can do is we could
51:11 - somehow ensure that this operation right
51:15 - this operation is being done by one and
51:18 - only one threade at a given point of
51:20 - time so that way what we could achieve
51:23 - is this is not being done simultaneously
51:26 - at the same time by two threads and in
51:29 - that sense we could achieve something
51:31 - called as Mutual exclusion and it will
51:34 - ensure that once thread one has the
51:37 - access to this particular logic which is
51:40 - incrementing the counter then it will do
51:42 - all the three parts which is loading the
51:45 - counter incrementing the value and
51:48 - setting back the value to counter so now
51:53 - counter has one and once thread one is
51:56 - done with its execution then the other
51:58 - thread could make use of this particular
52:01 - shared resource in its own execution so
52:04 - essentially we are restricting the users
52:07 - the access to the shared resource by
52:09 - multiple threads at a given point of
52:11 - time and how can we achieve this so one
52:14 - way to achieve this is with the help of
52:16 - keyword synchronized and uh we could do
52:19 - this at multiple levels like have this
52:21 - synchronization at the method level or
52:24 - the Block Level so now let's see how can
52:26 - can be fixed this using the synchronized
52:28 - keyword create an another method and uh
52:32 - let's call this
52:34 - as
52:36 - increment and what this will do is it
52:38 - will simply increment the counter
52:40 - variable let's replace this counter ++
52:43 - with this
52:45 - increment let's do the same here as well
52:48 - and what we have here now is a method
52:51 - which is going to increment the variable
52:54 - and as I told earlier that we want to
52:56 - ensure that this particular operation is
52:59 - being called by just one method at a
53:01 - given point of time what we can do is we
53:04 - could restrict its access and the way to
53:06 - do this is put a synchronized keyword
53:09 - here so essentially by uses of the
53:12 - synchronized keyword what we are saying
53:14 - that hey jvm please allow this
53:17 - particular method to be accessed by just
53:19 - one and only one threade at a given
53:22 - instant of time at any cost and that is
53:25 - how we have achieved synchronization in
53:27 - a code so this thing is also called as
53:31 - critical section so one way to
53:33 - understand this is that this section is
53:35 - quite critical to the execution of the
53:37 - program in the context of multi-threaded
53:40 - environment so we are limiting its
53:42 - access or the users by one and one one
53:45 - thread at a given point of time so with
53:47 - that in mind let's go ahead and run the
53:49 - code and see what is the outcome like so
53:52 - let's run this and now we can see that
53:55 - we are getting the the exact value of
53:57 - 20,000 let's run it a couple of more
54:00 - times and each and every time we see the
54:02 - value remains to be 20,000 itself and
54:05 - there is no changes as
54:12 - such so it feels like we have fixed the
54:15 - issue of synchronization by using this
54:17 - synchronized keyword but there are a few
54:20 - inherent problems with this particular
54:22 - approach the approach that we are
54:24 - talking is using the synchronized
54:26 - keyword at the method level so in order
54:29 - to understand what are those problems
54:32 - let's first understand how does the
54:33 - synchronized keyword work in Java so
54:36 - first thing is let's understand the
54:38 - monitor locks so each object in Java is
54:41 - associated with a monitor which is a
54:43 - mutual exclusion mechanism used for
54:46 - synchronization so when a thread enters
54:48 - a synchronized block or a method it
54:51 - attempts to acquire the monitor lock
54:53 - associated with the object on which the
54:55 - synchronization is applied so imagine
54:58 - that there is a shared room that room
55:01 - could be used by just one person at a
55:03 - time then you have to enter inside the
55:05 - room using some sort of lock you use the
55:08 - lock you open the door you go inside and
55:11 - then once you come out you release the
55:13 - lock you hand over the key to someone
55:15 - else who wants to make use of the room
55:17 - next likewise in every Java object we
55:20 - have this monitor lock which is also
55:22 - sometimes called as the intrinsic lock
55:24 - which needs to be acquired by the thread
55:26 - which wants to make use of the
55:28 - synchronized block or the synchronized
55:30 - method Or the critical section when a
55:32 - threade enters the synchronized blck or
55:34 - method it attempts to acquire the
55:36 - monitor Lock And if the lock is
55:38 - available the thread acquires the lock
55:40 - and proceeds to execute the synchronized
55:42 - code so what do we mean by the lock is
55:44 - available well what it means is no other
55:47 - thread will be currently holding that
55:49 - lock so if the lock is not available
55:52 - that is another thread is holding that
55:53 - lock then the thread enters a blocked
55:55 - state and it has to wait until the lock
55:58 - becomes available the second step of
56:00 - this process is releasing of the monitor
56:03 - lock so when threade exits let's say the
56:05 - threade is going to exit after this
56:07 - execution is completed it has to release
56:10 - the monitor lock and thus it allows
56:13 - other threads waiting for to acquire the
56:15 - lock and to proceed with their execution
56:18 - so the monitor lock used by the
56:20 - synchronized keyword is sometimes
56:22 - referred to as the intrinsic lock or the
56:24 - monitored lock of the object instance
56:26 - each object in Java has its own
56:29 - intrinsic lock and the synchronized
56:31 - keyword acquires and releases this lock
56:33 - implicitely when used at the method
56:36 - level or in the synchronized block so
56:38 - what is the problem exactly if we use
56:40 - this synchronized keyword at the method
56:42 - level so the first problem is that it's
56:44 - a kind of course grained locking so when
56:48 - we use synchronized at the method level
56:50 - it applies the log to the entire method
56:52 - body even though in this case our method
56:55 - body is just a one line but in actual
56:57 - code base it could span to multiple
57:00 - lines so essentially what you are doing
57:02 - is you are blocking any other threade
57:04 - from entering that particular method
57:06 - when you have applied the synchronized
57:08 - block on that particular method but your
57:11 - critical section could be just two or
57:12 - three lines so in that sense it does not
57:15 - make sense to block an entire method
57:17 - which could probably let's say have 30
57:19 - or 40 lines right so that is the first
57:22 - problem with using the synchronized
57:24 - keyword at the method level and needless
57:26 - to say this leads to reduced concurrency
57:29 - and performance bottlenecks and uh the
57:31 - second thing is kind of related to the
57:34 - first issue itself so when synchronized
57:36 - is used at the method level we lose the
57:39 - fine grin control needed in the more
57:41 - complex scenarios so for example you
57:44 - might want to synchronize only a
57:46 - specific section of the code within the
57:48 - method or you might need to synchronized
57:50 - multiple methods together as an atomic
57:52 - operation using method level
57:54 - synchronization we you don't have this
57:56 - granularity level and the third issue is
57:59 - let's say uh when a subass is overriding
58:02 - a synchronized method from its super
58:04 - class it must also explicitly declare
58:07 - the method as synchronized if it wants
58:09 - to maintain the synchronization Behavior
58:11 - and the failure to do so can lead to
58:13 - unexpected behavior and potential
58:15 - synchronization issues so you see there
58:17 - are multiple issues with the
58:19 - synchronized keyword when we use this at
58:21 - the method level so let's understand
58:23 - this with some code example in the same
58:25 - code
58:26 - let me do some refactoring let's call
58:29 - this as counter one let's create another
58:31 - variable let's call this as counter 2
58:34 - and let's call this as increment one and
58:36 - this as increment two and uh what we can
58:40 - do next is we can duplicate this
58:44 - method and
58:46 - uh let's call this as increment one and
58:50 - it's going to increment the counter one
58:53 - let's call this as increment two and
58:55 - it's going to increment in the counter
58:57 - two let's print these values in the last
59:01 - so counter
59:04 - two so from the functionality
59:07 - perspective it's surely going to work
59:09 - fine so counter one will be 10,000 and
59:13 - counter two will be 10,000 let's run it
59:15 - and
59:16 - confirm yeah this is working fine let's
59:19 - rerun it a couple of more times to
59:21 - ensure that it's working as expected so
59:25 - it's working fine but there is a problem
59:27 - so if you look closely what we have done
59:29 - is we have used the synchronize at the
59:31 - method level so let's say thread one
59:34 - starts working and at that particular
59:36 - point of time increment one is getting
59:39 - invoked so thread one has acquired the
59:41 - lock to this particular code this
59:44 - particular method and it's working on
59:46 - counter one but if you notice threade 2
59:49 - has no business with counter one right
59:51 - because it's going to operate only for
59:53 - incrementing the counter two so in in
59:56 - this case even though threade 2 is not
59:58 - directly working on counter one it will
60:01 - be blocked because this synchronized
60:04 - behavior is going to acquire the class
60:07 - level log so this is the class level log
60:10 - for this particular class there is only
60:12 - one log which is this intrinsic log or
60:15 - the class level log or the Monitor and
60:17 - that is acquired or held by the thread
60:20 - one and in that sense threade 2 even
60:22 - though it was not supposed to be blocked
60:24 - it's blocked now so I I think you see
60:26 - the problem now and that is one major
60:29 - problem with the synchronization at the
60:31 - method level so uh to address these
60:34 - concerns it's often recommended to use
60:36 - explicit locking with synchronized
60:38 - blocks or to use more flexible
60:40 - concurrency utilities provided in the
60:42 - Java util concurrent package such as
60:44 - lock interfaces reant locks read right
60:47 - lock Etc and uh we will be learning all
60:50 - these things going forward in this
60:51 - tutorial but for now let's understand
60:53 - how can we fix this scenario by the uses
60:56 - of custom logs so let me create another
61:00 - class let's call this as loog with
61:04 - custom objects so first of all let's
61:08 - have the counter variables so private
61:10 - static
61:12 - in
61:14 - counter 1
61:16 - as zero let's duplicate this let's call
61:21 - this as counter 2 and let's have the
61:23 - main method in the main method we will
61:25 - have have two
61:27 - threads so let's call this as thread one
61:32 - let's call this as thread
61:34 - to and this will be incremented
61:40 - for 10,000
61:42 - times let's call the
61:46 - increment
61:48 - one let's copy this
61:52 - one and let's rename this to increment
61:57 - 2 and
61:59 - uh let's
62:02 - start
62:04 - the threads so two do start and let's
62:09 - wait them to complete their
62:11 - execution so in catch we will have
62:14 - interrupted exception maybe call this as
62:17 - e and then we want to throw a runtime
62:22 - exception let's call 1. joint
62:27 - and
62:29 - then do.
62:34 - join and let's print the
62:41 - value counter
62:43 - to what we want to do now is we want to
62:47 - implement the increment
62:50 - methods so increment
62:54 - one and this time we don't want to have
62:56 - the synchronized keyword at the method
62:59 - level rather where we want to place this
63:02 - is at the Block Level where we want to
63:04 - do the
63:05 - synchronization so this will be counter
63:09 - 1 ++ and if you notice when we place
63:12 - this at the Block Level it's expecting
63:15 - us to pass something what does it expect
63:17 - us to pass well it expects us to pass
63:19 - the lock on which we want to lock this
63:22 - critical section so that two different
63:25 - thread for this particular critical
63:27 - section cannot enter in this code block
63:30 - so the way we will do it is that we will
63:32 - Define two logs so let's call this as
63:35 - private static
63:36 - final object log
63:40 - one new
63:42 - object and let's do for log two as well
63:46 - in the similar Manner and now we can
63:50 - have this on lock
63:53 - one so let's duplicate
63:57 - this let's call this as increment 2 and
64:02 - increment 2 right so basically what we
64:06 - have done is that we have the increment
64:09 - one method and then increment two method
64:12 - and these two methods are supposed to
64:13 - work for two different variables counter
64:17 - 1 and counter 2 and since these two
64:19 - methods are being used by two different
64:21 - threads and moreover these two methods
64:24 - are working on two different different
64:25 - shared resources not the same one which
64:28 - is counter one and counter two we want
64:30 - to lock it on two different locks so
64:33 - basically for this critical section if
64:35 - you want to acquire this particular area
64:38 - you have to access lock one if you want
64:41 - to acquire this particular area you have
64:43 - to access loog two and this is how we
64:46 - have decoupled we have separated out the
64:49 - concerns for these two different threads
64:52 - thread one and thread two so this is how
64:54 - we make use of this synchronized keyword
64:56 - at the Block Level and fix the issues
64:59 - with the users of synchronized keyword
65:01 - at the method level but having said so
65:04 - it's not a rule rather it's just a
65:06 - guidance or a general opinion but if in
65:10 - your case if your method happens to be
65:12 - quite a short one and does not have any
65:14 - other implementation and moreover let's
65:17 - say the entire method block is itself
65:20 - the critical section in that case you
65:22 - can go ahead with the method level
65:24 - synchronized keyword as well but more
65:26 - often than not this is not the
65:28 - recommended approach rather you should
65:30 - explore the option of going with the
65:32 - synchronized block
65:39 - itself now that we have seen the concept
65:41 - of synchronization it's time to
65:43 - understand a very important concept of
65:45 - wait and notify so before we start with
65:48 - the concept of weight and notify let's
65:50 - see what we have on the screen so let's
65:53 - imagine there is a room and the room
65:55 - always contains some sort of parcel and
65:58 - the catch here is that the parcel could
66:00 - belong to anyone so let's say there are
66:02 - four or five people and their parcel is
66:04 - getting delivered so you need to go
66:07 - inside the room and then take out that
66:09 - parcel but the room is locked and there
66:12 - is a security who is there on the room
66:14 - entry and the only way to access the
66:16 - room is to get that lock from the
66:19 - security once you have the lock you can
66:21 - go inside the room F your parcel and
66:24 - come out but here is the catch at a
66:26 - given point of time only one person is
66:28 - allowed to access the room everybody
66:31 - else will have to wait so how this can
66:34 - be implemented so the way it could work
66:36 - is that once the security is giving a
66:39 - lock to anyone they will allow them to
66:41 - go inside and F their parcel until the
66:44 - time whosoever is coming to access the
66:47 - room the security could tell them that
66:49 - somebody's already inside the room so
66:51 - you will have to wait till that person
66:53 - comes out so once the person who is
66:54 - inside the room gets his parcel he comes
66:57 - out and he hands the lock back to the
66:59 - security Now the security will notify
67:03 - that the room has been vacated by the
67:05 - earlier person and anybody else who was
67:08 - waiting for the room to access it could
67:11 - go inside and get their parcel but again
67:14 - they will have to ask the security for
67:17 - the access to the lock and then they
67:18 - will open the lock and then they will go
67:20 - inside the room to get their parcel so
67:23 - you see there is a mechanism which is is
67:25 - kind of allowing just a single person
67:28 - entry inside the room in order for them
67:30 - to do something inside the room which is
67:32 - the activity of collecting their parcel
67:35 - until the time anyone is inside the room
67:37 - the entire room is considered to be
67:39 - frozen or kind of locked and nobody
67:42 - could access it and as soon as that
67:43 - person is coming out then they are
67:45 - handing the key to the security and
67:47 - after that is done the security is
67:50 - notifying to everybody else that now
67:52 - room is available to be accessed and
67:54 - then in anyone from the waiting line
67:56 - could come and collect it so the wait
67:59 - and notify methods work in a very
68:01 - similar way so when a threade calls wait
68:04 - on the lock then that thread is
68:06 - suspended from its uh execution and it
68:09 - goes to something called as a waiting
68:11 - State and when it goes to the waiting
68:13 - State then others who were in The
68:15 - Waiting State earlier they have a chance
68:18 - to get hold of the lock and then they
68:20 - could start with their execution and as
68:22 - soon as they are done with their
68:24 - execution they could call notify and
68:26 - notify is a message to all the waiting
68:29 - threads that now whatever they were
68:31 - waiting for that lock has been released
68:34 - and then they could take that lock and
68:36 - access their critical section to work
68:38 - upon so that is the whole idea behind
68:41 - the wait and notify methods in Java now
68:43 - we will see this with one code example
68:45 - and it will become more
68:48 - clear so in order to demonstrate the
68:51 - working of weit and notify I have
68:53 - created this class it's called as cre
68:55 - and notify demo so first let's start
68:58 - with writing the main method next we
69:00 - will create two
69:02 - threads
69:04 - so let's call this
69:07 - as thread one and this one as threade
69:13 - two let's start the first one let's
69:16 - start the second
69:22 - one and uh let's create a l as well so
69:27 - call it as public static private static
69:30 - final object lock and new
69:36 - object let's have two worker methods
69:40 - which will be invoked by the different
69:42 - threads that we have
69:43 - created so first one is public static
69:47 - void let's call this as one and it will
69:50 - throw an interrupted exception why we
69:54 - will see it later
69:56 - so what we will do is we will
69:58 - synchronize on the
70:00 - lock and the first thing that we do is
70:04 - we print some message like hello
70:08 - from method one and then on the Lock
70:14 - This Thread is going to call wait and
70:17 - then we print some
70:19 - message back again from
70:22 - the method one
70:27 - or maybe back
70:29 - again in the method one makes more
70:33 - sense let's create the other method as
70:37 - well it will be called as
70:41 - two so why did we throw the interrupted
70:43 - exception so when we call wait or notify
70:46 - these are interruptible and they will
70:48 - throw an interrupted except that that is
70:50 - the reason we need to have this in the
70:52 - method signature so
70:55 - let's put synchronization on the
70:59 - lock and uh let's have this message
71:03 - hello from method
71:06 - 2 and now this time let's call
71:11 - notify
71:13 - and let's print some
71:16 - message hello
71:19 - from method 2 even
71:23 - after notifying
71:25 - and uh let's call these methods in the
71:29 - threads so this will call one let's add
71:32 - this in
71:34 - the TR catch
71:36 - block let's call to
71:39 - here and let's add this in the TR catch
71:41 - block as
71:44 - well and now let's try to analyze what's
71:47 - going to happen so what we have done is
71:49 - we have created the thread one and it's
71:52 - going to make use of this particular
71:53 - method so what this method is doing is
71:56 - let's say thread one is started and
71:58 - after that the control comes here so in
72:01 - the synchronized block the lock has been
72:03 - acquired and this print message will be
72:06 - executed so we will see this message on
72:08 - the console now what does this thread do
72:10 - is that it calls weight so this thread
72:14 - will be suspended and it will go in a
72:16 - waiting State since the other thread was
72:18 - just started as well and what this
72:20 - thread is doing is it is calling method
72:22 - two the execution comes here and since
72:26 - weit was called so lock was available so
72:30 - this lock was acquired by this
72:31 - particular thread and this message will
72:33 - be printed that hello from method 2 and
72:37 - then we call do notify so one important
72:40 - thing to note here is that even when we
72:42 - call notify then whatever code execution
72:45 - is left out for that particular
72:47 - synchronized block all those things will
72:49 - be
72:50 - executed only after they are executed
72:53 - then the notify will come in effect
72:55 - other threads which are waiting for the
72:56 - lock to be released they will acquire it
72:59 - and start with their process so
73:00 - basically after notify is called it's
73:03 - not that the control will go directly to
73:05 - this particular part rather it will
73:07 - print out the remaining thing which is
73:10 - Hello from method to even after
73:11 - notifying and then the control comes
73:14 - here to this threade and then it will
73:16 - print this message that back again in
73:18 - the method one so let's execute this and
73:21 - see the outcome so we are here run wait
73:24 - an
73:26 - demo so here is the outcome so first is
73:29 - Hello from method one and then we called
73:32 - wait so it goes to the other thread it
73:34 - says hello from method 2 and after
73:37 - notify we had another message as well so
73:40 - that is getting printed that is Hello
73:42 - from method 2 even after notifying then
73:46 - we have the control back here which is
73:48 - back again in the method one so you may
73:50 - have a question that what is the
73:52 - difference between weight and sleep are
73:53 - in these both same same right so on the
73:56 - surface they do look quite similar but
73:59 - there is a key difference that weight is
74:01 - used for inter thre communication and
74:03 - synchronization while sleep is used for
74:06 - just pausing the execution of the given
74:08 - thread for a specified duration and
74:11 - please note that this is just one
74:13 - implementation of weight method there
74:15 - are other implementations as well so
74:17 - when you call weight with this time out
74:19 - information it causes the current
74:21 - threade to wait until it is awakened
74:23 - typically by being notified or
74:25 - interrupted otherwise if not so then
74:29 - after the time has elapsed it will be
74:31 - automatically awakened and there is
74:33 - another variant of notify which is
74:36 - notify all so in the case of notify it
74:39 - pickes up single thread but in the case
74:41 - of notify all it notifies all the
74:43 - waiting threads that are waiting for a
74:46 - given lock so this should be it for the
74:48 - introduction of wait and notify methods
74:51 - in Java now let's go ahead and make use
74:54 - of it to to implement the producer and
74:56 - consumer
75:01 - problem so what is producer consumer
75:03 - problem the producer consumer problem is
75:06 - a synchronization scenario where one or
75:08 - more producer trades generate data and
75:11 - put it into a shared buffer while one or
75:13 - more consumer trades retrieve and
75:16 - process the data from the buffer
75:18 - concurrently such kind of a pattern
75:20 - which is used to teach the concept of
75:23 - synchronization so consider this image
75:25 - here so let's assume that this thing is
75:28 - some sort of buffer some sort of
75:30 - container or placeholder where we want
75:33 - to put some data but the catch here is
75:35 - that producer is the one which is going
75:38 - to put this data here and the consumer
75:40 - is the one which is going to consume the
75:42 - data from here so what I want to
75:44 - implement now is two threads one is the
75:46 - producer threade which will be putting
75:48 - data in this particular shared container
75:50 - and the other thread is the consumer
75:52 - thread which will be consuming data from
75:55 - from this shared container and at the
75:57 - time when the container is full the
75:59 - producer will stop producer and at the
76:01 - time when the consumer is empty the
76:03 - consumer will stop consuming so let's go
76:06 - ahead and implement the same so to
76:08 - implement the producer consumer problem
76:11 - here's the class that I have created and
76:13 - uh it's called as producer consumer and
76:16 - uh there would be a supportive class
76:18 - called as worker so let's create that as
76:21 - well let's call it as worker and the
76:24 - worker will have two requirements one is
76:26 - to produce the element and put in the
76:28 - shared space so let's call this as
76:32 - public void
76:34 - produce and the other would be public
76:37 - void
76:39 - consume using this the worker can
76:41 - consume from the shared space and this
76:44 - worker will need certain properties so
76:46 - the first one is sequence so essentially
76:50 - what we are going to put in the shared
76:52 - space is number so 0 1 2 3 4 5 something
76:57 - like that and it will start from zero so
77:00 - we are calling the same as sequence as
77:03 - zero the next thing is private final
77:10 - integer let's call this as top so this
77:14 - is the maximum number of elements that
77:16 - can be stored in the shared area shared
77:19 - container let's have another parameter
77:24 - called as bottom so this is the least
77:26 - amount of elements that can be kept in
77:28 - the shared container and the next thing
77:32 - is private final
77:36 - list so this is the container
77:39 - itself here we will be placing these
77:42 - numbers that we are generating so here
77:44 - the producer will be generating the
77:46 - numbers and the consumer will be
77:47 - consuming from the next is that we need
77:50 - some sort of lock let's create the lock
77:53 - as well let's let call this as
77:56 - lock
77:59 - and initialize this as an
78:03 - object in order to initialize all these
78:05 - things we will have to have a
78:12 - Constructor all right looks fine to me
78:15 - now let's go ahead and implement the
78:17 - produce
78:19 - method so first thing is that we need to
78:22 - synchronize on the
78:23 - lock and and what I want is that produce
78:26 - should be able to keep on producing
78:29 - these numbers infinitely until a certain
78:32 - condition is met and likewise and the
78:35 - consumer should keep on consuming
78:37 - infinitely until certain condition is
78:40 - met so let's start with
78:42 - producer so let's call this as while
78:46 - true and
78:49 - if container do
78:52 - size is the maximum one
78:55 - then let's print a message so
78:57 - essentially the container has become
78:59 - full so Container
79:03 - full waiting for items to
79:08 - be
79:11 - removed and since we are waiting we will
79:15 - call do
79:16 - weight and this weight we will have to
79:19 - throw an interrupted exception so let's
79:21 - add that and if it's not full
79:25 - then let's print a message so
79:30 - sequence added to the
79:34 - container and let's
79:38 - add this to the
79:43 - container and let's call
79:46 - notify and in order to see the demo we
79:50 - should add some sort of delay so let's
79:53 - add a delay of of let's say 500
79:57 - millisecond so what's happening here
79:59 - exactly well this thing for sure is
80:01 - going to run infinitely and that is the
80:03 - reason we are getting certain warnings
80:06 - so may not be the most optimal way to
80:08 - implement this scenario but this is fine
80:10 - for the demo purposes moving on coming
80:12 - to the logic so first thing which is
80:14 - checked is is the container full if the
80:16 - container is full then this message is
80:18 - thrown and the thread goes to a waiting
80:20 - State else and then we start adding to
80:23 - the container what we is we first print
80:26 - the message and then we add that
80:27 - sequence number to the container and
80:30 - after it is added we increment that
80:32 - number so it's a post increment so that
80:34 - the next number could be added to the
80:36 - container in the sequence now we are
80:38 - calling log. notify so if you remember
80:41 - the notify discussion from a few minutes
80:43 - back what we understood is that notify
80:46 - does not come into existence immediately
80:49 - rather whatever is there in the
80:51 - synchronized block everything will be
80:53 - executed and only after that is executed
80:56 - then notify will come into effect and
80:58 - the other waiting threads will be
80:59 - notified so here what happens is that we
81:02 - call notify and then the call comes here
81:04 - so since we are into while. true and we
81:06 - check that container size still not full
81:08 - then we come to the else and then we go
81:10 - about executing this so this happens
81:13 - till the time the size is not full and
81:15 - then log. WID is encountered and then
81:18 - this thread goes to a waiting state so
81:21 - this is how this thing is implemented
81:23 - and executed now let's go ahead and
81:25 - implement the consume method as
81:28 - well now let's implement the consume
81:30 - method so for the consume as well let's
81:32 - throw the interrupted exception because
81:34 - we will need to add the later point of
81:38 - time all right so let's put all of this
81:42 - inside a synchronized
81:45 - block and like we did for the case of
81:48 - produce we want this to run infinitely
81:51 - as well so we will put this inside a
81:54 - continuous file Loop and if container do
82:00 - size so if the container is empty then
82:04 - we don't need to consume anything so
82:06 - let's print a message container
82:09 - empty waiting for items to
82:13 - be added and then let's go to a waiting
82:20 - state in the else section let's start
82:23 - consuming
82:24 - so what we say
82:27 - is
82:28 - container do
82:31 - remove
82:35 - first and then removed from
82:42 - the container so basically we are going
82:45 - to remove from the list that is the
82:48 - shared container and print the message
82:51 - and once this is removed then we will
82:53 - call log dot
82:57 - notify and here as well we will have a
83:01 - sleep for let's say 500 milliseconds to
83:04 - show the simulation in somewhat slower
83:08 - manner so that it's easy to read and
83:10 - understand right so let's understand
83:13 - what is happening here we are going to
83:15 - do this
83:16 - infinitely and uh initially we check if
83:19 - the container is empty if it's empty
83:21 - print a message and go to a waiting
83:23 - state if it's not empty come to the else
83:26 - section and then let's remove the
83:29 - element from container and uh print it
83:32 - on the console and then we call notify
83:36 - and given the fact that notify is not
83:38 - realized immediately it will keep on
83:40 - executing whatever is there in the
83:42 - synchronized block and uh since this
83:44 - entire thing is there inside this file
83:46 - Loop this will keep on executing till
83:49 - the time we are not meeting a scenario
83:51 - of dot weight so this keeps on happening
83:54 - and finally we encounter a situation
83:56 - wherein it becomes empty the container
83:59 - becomes empty wait the lock. weight and
84:01 - we go to the waiting State and then this
84:04 - particular thread which had gone to the
84:08 - waiting State earlier comes into effect
84:10 - and then it starts adding again so this
84:12 - is how this is going to work now let's
84:14 - implement the main method so of course
84:17 - we will have to move all this worker
84:20 - class outside this particular class so
84:22 - let's skip it outside so in the producer
84:24 - consumer class let's create the main
84:26 - method and uh in the main method first
84:30 - of all we will create a worker object
84:34 - which will
84:36 - be initialize with five and zero so the
84:39 - shared container could contain up to
84:41 - five elements maximum and uh minimum
84:44 - would be
84:46 - empty so let's create the
84:49 - threads call this as
84:52 - producer create another thread let's
84:56 - call this
84:58 - as consumer and inside the producer
85:02 - threade we can say worker
85:04 - dot produce and of course it's going to
85:08 - throw interrupt exception so let's
85:10 - handle it
85:11 - accordingly and here we could call
85:14 - consume let's handle it
85:17 - accordingly and finally let's start
85:20 - these threads so producer. start
85:24 - consumer do
85:27 - start so let's run this and see what we
85:30 - are
85:31 - getting so let's run this all right so
85:36 - here is the
85:37 - output so you can see Zero added to the
85:40 - container one added two added three
85:42 - added four added so and so forth now
85:44 - let's stop it and analyze so basically
85:47 - first zero is added then one is added
85:49 - and so and so forth four is added and
85:52 - once fourth is added then the container
85:54 - is full and then it's going for a state
85:58 - where it wants the items to be removed
86:00 - and then the consumer part is activated
86:02 - and it's going to remove the items from
86:05 - the container so zero removed one
86:07 - removed and so on and so forth and
86:09 - finally the container becomes empty and
86:11 - then it's waiting for the items to be
86:13 - added this process will keep on
86:15 - happening because we had implemented in
86:17 - such a way that uh it's going to happen
86:19 - for infinite time period And this is one
86:22 - way in which we could implement the
86:24 - producer consumer problem of course
86:27 - there are many other ways as well but
86:28 - this is one way in which we could
86:30 - implement
86:35 - it till now we have learned a couple of
86:38 - ways in which we can create threads in
86:40 - Java such as creating the threads using
86:43 - extending the thread class and
86:45 - implementing the runnable interface how
86:47 - can we create multiple threads let's say
86:50 - if we want to run five tasks then we can
86:53 - create threads in the for Loop and run
86:56 - these tasks what can we do if we want to
86:59 - execute 500 tasks synchronously we can
87:02 - for sure create 500 trades using the for
87:05 - Loop right but where does it stop what
87:08 - do we do if we have to run 1,000 tasks
87:11 - create th000 threads in a loop well
87:14 - something does not sound right here here
87:16 - is the problem in Java one thread is
87:19 - equal to 1 OS level thread creating a
87:22 - thread is an expensive operation so
87:24 - creating thousand trades in a loop is
87:27 - certainly not a scalable approach what
87:29 - could be a more practical approach is to
87:32 - have a fixed number of threads and let's
87:34 - create them up front imagine a pool of
87:37 - end threads and these trades handle the
87:40 - Thousand tasks among themselves and this
87:42 - is exactly what the executor service
87:45 - helps us in achieving in one line we can
87:48 - Define the executor Service as a tool in
87:50 - Java for managing and running tasks
87:53 - concurrently across multiple threads so
87:56 - executor service helps us in creating a
87:59 - bunch of threads a pool of threads does
88:01 - the name threade pool and the threads
88:04 - are not killed once they are done
88:06 - executing the task rather they are
88:09 - reused to execute the another task thus
88:12 - by making use of the executor service we
88:14 - save time needed for thread creation and
88:17 - making things more efficient and
88:19 - manageable there are four types of
88:22 - executors provided by the execut service
88:25 - and these are single thread executor
88:27 - fixed thread pool executor cach thread
88:30 - pool executor and sheduled
88:33 - executor now let's quickly visualize how
88:36 - internally the executor service looks
88:39 - and functions so imagine a execution of
88:42 - the main threade which is this one this
88:44 - is the execution line and the main
88:47 - threade calls for the executor service
88:50 - and now the executor service the way it
88:52 - functions is that it maintains a thread
88:55 - pool there are couple of threads that
88:58 - are there inside the thread pool this
89:00 - depends on the type of executor we are
89:02 - using but essentially you will have a
89:04 - couple of Trades which are created in
89:07 - advance then each type of the executor
89:10 - service will have a blocking queue and
89:12 - this is where the tasks are placed so
89:15 - when we say executor do execute and the
89:18 - task so what I want is that a task
89:21 - should be picked by either of these
89:23 - threads and it should be executed but
89:25 - for the time being it's not getting
89:27 - executed it will be sitting inside this
89:29 - blocking queue so in nutshell the
89:32 - executor will have two things first is
89:34 - the threade pool and then there is a
89:36 - priority queue and there are just two
89:38 - steps the new task will keep on adding
89:41 - to the blocking queue and the thread
89:44 - which is available to execute the new
89:45 - task can pick the task from the blocking
89:48 - queue and it starts its execution and
89:50 - once the execution is done and the
89:52 - threade is available to pick another
89:54 - another task it will pick another task
89:56 - from the blocking queue so more or less
89:58 - all the executors look similar to this
90:01 - and this is the way in which they
90:02 - function of course there are certain
90:04 - fundamental differences in which the que
90:07 - will be managed and the way in which the
90:09 - threats will be created now what we will
90:12 - be doing is write some code making use
90:14 - of these executors and then understand
90:17 - how do they work so let's begin
90:25 - in order to demonstrate the working of
90:28 - the executor service I have already
90:30 - created this package that is the
90:32 - executor service and now we will learn
90:35 - about the single threade
90:37 - executor so let's create a class let's
90:40 - call this as
90:42 - single
90:45 - executor
90:48 - demo we will have a class let's call
90:51 - this as task and it's going to
90:54 - implementer
90:55 - unable and it will have a task ID so
90:58 - let's call this as task
91:02 - ID let's have the
91:06 - Constructor let's overwrite the run
91:14 - method and in the run method we will
91:16 - print a
91:18 - message task with ID
91:24 - task
91:26 - ID being
91:28 - executed by
91:32 - thread and
91:35 - then trade dot current trade dog
91:42 - name
91:44 - and let's put some sleep in the
91:48 - thread let's have it for half second and
91:51 - in the catch handle the interrupted
91:56 - exception so we will just throw the
91:59 - exception for now let's call this as a
92:02 - runtime exception and pass the exception
92:05 - here here we will create the main method
92:08 - and in the main method what we will do
92:10 - is we will create the single thread
92:14 - executor now so in order to create it we
92:18 - have the executors dot new single thread
92:22 - executor and let's assign this to
92:26 - service now the idea is complaining and
92:28 - showing some warning message let's see
92:31 - what does it say it says that executor
92:34 - service used without private resources
92:37 - statement so the executor service should
92:40 - be closed once we are done using it
92:43 - there is a way in which we could close
92:45 - it and that is by calling the service do
92:48 - shutdown but there is a different
92:50 - approach as well in which we can
92:52 - implement the same and that is is to use
92:54 - the TR with
92:55 - resources so let's do this and put it
93:00 - under the try with resources and now we
93:03 - should be good to
93:05 - go so let's say I want to run this task
93:09 - 5 times so for in I
93:14 - as 0 I less than 5
93:19 - i++ and I can say service. and then call
93:24 - the execute so what is the runable that
93:27 - I want to execute that is my task and
93:30 - pass I as the task ID so as opposed to
93:34 - what we were doing earlier that after
93:37 - creating the threade we used to call the
93:39 - dot start in order for the thread to get
93:41 - started with its operation here we don't
93:44 - need to do it rather once we submit this
93:48 - to the executor service by calling do
93:51 - execute it is handled by the execut
93:54 - service itself now let's run this code
93:57 - and see what is the output
93:59 - like so let's run
94:04 - this so here is the output so basically
94:08 - it's going to print task ID with zero
94:11 - being executed by this particular thread
94:13 - and task ID with one being executed by
94:16 - this particular thread and so on so
94:18 - forth this is what we have implemented
94:20 - in the code as well right the idea is
94:23 - that there is a runable and what it does
94:26 - is it's simply going to print the task
94:29 - ID along with the threade by which it is
94:32 - getting
94:33 - executed so if we had not used the
94:36 - executor service what we would have done
94:38 - is we could have created Five threads
94:41 - and using those five threads I could
94:44 - have assigned the reable for each of the
94:46 - thread and then should have called The
94:49 - Dot start on that but here we are able
94:52 - to achieve the same kind of result using
94:55 - the executor service and this
94:57 - implementation in particular which is
95:00 - new single thre
95:01 - executor so now the implementation looks
95:04 - much more cleaner and we don't have to
95:06 - deal with the creation and destruction
95:08 - of the threads now everything is being
95:11 - handled by the executor service
95:13 - itself let's learn how the single thread
95:16 - executor Works under the hood so we have
95:19 - the main trade and this way we can
95:22 - create in tasks and submit or execute
95:26 - those tasks one by one so here we have
95:29 - the threade pool and in the threade pool
95:31 - there is a single thread there is a task
95:33 - Q which is containing task zero task one
95:37 - task 2 up to task n and what this single
95:40 - thread executor will do is it will pick
95:43 - the task from the task pool or the task
95:46 - queue and then execute it so Fitch task
95:50 - and execute so in the case of single
95:53 - thread executor the size of the threade
95:55 - pool is just one so we have just one
95:58 - threade which is going to Fitch the
96:00 - tasks from the task Cube and run this
96:02 - and if it so happens that due to some
96:05 - exception the thread is killed the
96:07 - executor will make sure to recreate the
96:09 - thread and the execution of the tasks
96:12 - won't be stopped by using this kind of
96:15 - thread pool we can ensure that the task
96:18 - zero is always ran before task one and
96:21 - task one is always ran before task 2
96:24 - since we have just one threade here it's
96:27 - guaranteed that the tasks would be run
96:36 - sequentially now we will learn about
96:38 - fixed threade pool so let's create the
96:41 - class let's call this as
96:44 - fixed trade
96:46 - pool
96:49 - demo let's maximize this one
96:54 - and as we had done earlier we will be
96:57 - creating a runable which will be the
97:00 - task which needs to be executed by the
97:02 - executor
97:03 - service so let's do that let's have a
97:06 - class called as work it's going to
97:09 - implement
97:11 - runable and we can start by overwriting
97:14 - the run
97:15 - method public void
97:19 - run and it will have certain
97:21 - parameters so the first field
97:26 - is work
97:29 - ID and then it will have a Constructor
97:32 - which is going to initialize this work
97:34 - ID and in the run method what we will do
97:38 - is we will print a similar kind of
97:40 - message the task
97:44 - ID let's provide the work ID
97:48 - here being
97:51 - executed by thread
97:56 - threade
97:57 - dot current thread do get
98:01 - name let's put some sleep timer here so
98:06 - again let's go with half a
98:10 - second and in the catch let's handle the
98:12 - interrupted
98:15 - exception let's throw runtime
98:19 - exception and pass the exception
98:24 - and now let's go to the main method and
98:27 - very similar to what we did for the
98:30 - single thread
98:31 - executor we will also be creating the
98:33 - executor using the try with
98:36 - resources so
98:40 - try executor service let's call this as
98:45 - service and executors Dot new
98:50 - fixed trade pool let's say that I want
98:54 - to have a thread pool of size
98:56 - two and uh I want to execute this task
99:02 - seven times or what you could say is I
99:06 - want to run seven tasks so let's make
99:09 - that so int I as z i less than 7 I ++
99:15 - and let's call
99:17 - service. execute
99:20 - new work pass the ID
99:25 - so let's print this and see how is the
99:28 - outcome like so let's run
99:32 - this so here is the output so task ID
99:36 - one is being executed by thread 2 task
99:40 - id0 is being executed by thread 1 task
99:43 - ID 2 is being executed by thread 2 and
99:46 - so and so forth so one important thing
99:49 - to note here is that since we had two
99:52 - threads created these two threads are
99:54 - there in the thread pool and they are
99:56 - taking turns in order to execute my task
100:00 - so first thread 2 is being used to
100:03 - execute the task then we have thread one
100:05 - then I have threade two then we have
100:07 - thread 1 thread 2 thread 1 thre 2 and so
100:10 - and so forth please note that this may
100:13 - not be alternating in the stricted terms
100:16 - this is just instance wherein we got
100:18 - this kind of pattern but this does not
100:21 - mean that in all the cases it will be
100:23 - one by one it could so happen that
100:26 - threade two could execute certain tasks
100:28 - in one sequence and then after that
100:31 - thread one could be involved and then
100:33 - thread one could be assigned with
100:35 - certain tasks for execution so this is
100:38 - what we have achieved by making use of
100:40 - the fixed threade pool so to submit up
100:43 - we have created a new fixed trade pool
100:45 - and there are two trades and when I want
100:48 - to execute seven tasks then those seven
100:51 - tasks are picked one by one by either of
100:54 - the available two threads and they are
100:56 - executed now let's have a quick look
100:59 - around how exactly does the fixed
101:01 - threade pool work under the
101:05 - hood so what you see here is a visual
101:08 - representation of the fixed thread pool
101:10 - executor and here in the case of fixed
101:13 - thread pool there is a fixed number of
101:15 - threads as the name suggests that there
101:18 - will be certain number of threads which
101:20 - is pre-created and this executor also
101:22 - has a task que where all the tasks will
101:25 - be placed so consider this as a task que
101:28 - of some sort the threads in the fixed
101:30 - thread pool pick the task from the task
101:33 - que and execute them so when the thread
101:36 - is done with the execution of a task it
101:39 - goes ahead and picks another task and
101:42 - this keeps on happening until all the
101:44 - tasks are completed so the basic idea
101:47 - Remains the Same that there is a thread
101:49 - pool and the threads are fixed in the
101:52 - count and once you have tasks flowing in
101:55 - in the task queue these trades will
101:58 - start taking out the task from the task
102:00 - queue start to work on those and once
102:03 - all the tasks are done and this keeps on
102:06 - happening till the time the task que had
102:08 - certain tasks in it so overall This
102:11 - Thread pool executor is fing the task
102:14 - and executing it over and over
102:21 - again now let's learn about the cach
102:24 - threade pool executor in order to do so
102:27 - we will write a code to demonstrate its
102:29 - functionality and let's name the class
102:32 - as cast threade
102:37 - demo we will have a task so create a
102:41 - class for the same let's call this as
102:44 - task one it will implement the runable
102:48 - interface let's overwrite the run method
102:54 - and this class will have some sort of
102:56 - task
102:57 - ID let's call this as task
103:02 - ID initialize the same inside the
103:07 - Constructor and it can give some message
103:10 - something like
103:14 - task then task ID
103:20 - plus being executed
103:24 - by
103:26 - trade dot current trade. get name then
103:32 - we will put this threade to sleep for
103:36 - let's say 500
103:40 - milliseconds and Surround this with try
103:44 - catch now let's write the main
103:47 - method and we have execut a service
103:50 - let's call this as
103:52 - service and what we want is new cached
103:55 - threade
103:57 - pool let's keep it inside the trate
104:05 - resources and one key thing to observe
104:07 - here is that there is no value that we
104:09 - are going to pass inside the new cach
104:12 - threade pool un like the fixed trade
104:15 - pool where we were passing the number of
104:17 - Trades that we want to create a front
104:20 - why is it so we will see in the next
104:22 - section for now let's run this thing
104:26 - probably for maybe th000 tasks so I less
104:31 - than 1,000
104:34 - i++ and service. called execute new task
104:39 - one and give it a task ID of I so let's
104:44 - execute this and say it's
104:48 - output all right so here is our output
104:52 - you see
104:55 - all the tasks are being executed by the
104:58 - threads that are getting created so task
105:01 - 27 is being executed by the thread 28
105:04 - task 41 is by thread 42 task 4 is by
105:08 - five and then Task 1 by two so that's it
105:12 - for the code demonstration now let's
105:14 - understand how does it work
105:18 - internally now let's visualize and learn
105:21 - the internal working s of cast thread
105:24 - pool executor in the case of casted
105:26 - thread pool we don't have a fixed number
105:28 - of threads so you see in the code
105:31 - implementation we did not provide any
105:33 - number to the method which was creating
105:36 - the new cast threade pool the task you
105:39 - here does not hold a number of tasks
105:41 - which we submit the queue here is of a
105:44 - special type and it's called as a
105:46 - synchronous queue the synchronous queue
105:48 - has space only for a single task so
105:51 - every time you submit a new task the
105:53 - casted threade pool holds the task in
105:56 - the synchronous queue and it searches
105:58 - for the threads which has been already
106:00 - created and they are not working
106:02 - actively on any task if no such thread
106:06 - is available the executor will create a
106:08 - new thread and add it to the thread pool
106:11 - and this newly created thread starts to
106:13 - execute the task which has been
106:15 - submitted let's imagine a scenario where
106:18 - 10 threads are executing 10 tasks at a
106:21 - given point of time and then 11th task
106:24 - comes in and since we don't have a
106:26 - thread 11 to C to this task the thread
106:29 - pool will create the 11th thread to
106:32 - execute this new task so in theory the
106:35 - cast thread pool is capable of creating
106:37 - thousands and thousands of threads you
106:39 - may wonder where does this stop if
106:42 - threads are getting created at such a
106:44 - rapid paase so what is the upper limit
106:47 - well there is a guard rail to keep the
106:49 - threade count in check after some time
106:51 - the threads are done EX executing the
106:53 - tasks and they are available to pick
106:55 - another task and if they don't have any
106:58 - other task they are killed and this time
107:00 - of idleness in 60 seconds so what I mean
107:03 - is let's say there is a trade and it has
107:06 - not received any task to work upon it
107:08 - will be terminated so cash trade pool is
107:11 - pretty much autoscaling in nature on the
107:14 - basis of task load it has when there are
107:16 - new tasks to be executed threads are
107:19 - added and when there are less number of
107:21 - tasks to be executed a few among the
107:23 - created threads could handle the task so
107:25 - the other threads the idle ones are
107:27 - retired and killed so that's how the
107:30 - cash threade pool executor Works behind
107:32 - the scenes so the basic takeaway from
107:34 - here is that the queue will contain only
107:37 - one task at a given point of time that
107:39 - is your synchronous queue and the
107:42 - maximum time for which a thread could be
107:44 - there in the thread pool without any
107:46 - work is 60 seconds so now let's move
107:49 - ahead and learn about the final type of
107:52 - executor which is the scheduled
108:01 - executor now let's write the code for
108:04 - demonstrating the scheduled executor
108:06 - service let's create this class let's
108:09 - call this as scheduled executor
108:15 - demo we will create a
108:18 - task let's call this as probe task it
108:22 - will implement
108:23 - the runnable
108:26 - interface let's overwrite the run
108:29 - method and run method is going to print
108:32 - a message let's say probing
108:37 - endpoint for updates let's create the
108:40 - main
108:41 - method so we have
108:44 - scheduled executor service let's call
108:48 - this ad service and from the executors
108:51 - let's call the new scheduled threade
108:53 - pool new
108:55 - shedu threade pool let's have the number
108:59 - of Trades as one and the scheduled
109:01 - executor service does offer a couple of
109:04 - methods in which we could schedule the
109:06 - thades to be executed we will go ahead
109:09 - with the implementation which is
109:11 - scheduled at fixed rate so the idea is
109:14 - that this particular threade will be
109:16 - executed at a fixed rate after certain
109:19 - point of time so the way to write this
109:21 - is service Dot schedule at fixed rate so
109:26 - we have the actual runable then the
109:28 - initial delay followed by the period and
109:31 - then the time unit so runable is the
109:33 - runable initial delay is after how much
109:36 - time this execution will start so let's
109:39 - say you could pause for 2 seconds and
109:42 - then after 2 seconds every 3 seconds
109:46 - this trade could be executed so let's
109:49 - write it so the runable here is the
109:51 - probe task
109:54 - and let's say the first delay that I
109:56 - want is of 1,000
109:58 - milliseconds and I want this to run
110:00 - every 2,000
110:02 - milliseconds and the time unit is going
110:04 - to be milliseconds now let's execute
110:08 - this and see how is the output
110:13 - like so we are seeing that this thing is
110:17 - getting printed probing in point for
110:20 - updates and it will keep on printing
110:22 - this
110:23 - every 2 seconds and it will keep on
110:25 - printing this probing in point for
110:27 - updates till the time we shut down the
110:29 - executor service so how can we do that
110:32 - let's stop this and write some code for
110:34 - the same so the way to do this is we can
110:37 - call the await termination method on the
110:39 - service handle this object and Supply
110:42 - certain time so it will wait for the
110:46 - executor to be terminated in that time
110:49 - range and if it's not terminated in that
110:52 - time time range it will call the
110:54 - shutdown method of the executor service
110:56 - which is going to gracefully terminate
110:59 - the executor service so let's write the
111:01 - code for same so we will put this inside
111:04 - a tri block because of a termination is
111:06 - going to thr an interrupted exception
111:09 - let's have the interrupted exception as
111:11 - well and uh and in the tri block we can
111:15 - check
111:16 - service. AWA termination let's give it a
111:19 - time of 5,000 milliseconds
111:22 - and then time unit is milliseconds so if
111:25 - the time is more than 5,000 milliseconds
111:28 - we can call
111:30 - the shutdown method so let's say
111:34 - shutdown now else if it goes to the
111:38 - interrupted block there as well we could
111:40 - call shut down now let's increment this
111:44 - time to maybe 10 seconds and now let's
111:47 - run the code and see its output
111:55 - so you see the delay that we had was of
111:58 - 2 seconds that every 2 seconds this
112:00 - should be printed and after printing it
112:03 - for five times that is like 10 seconds
112:05 - it got terminated on its own so that is
112:08 - how we could terminate the scheduled
112:10 - executor so of course this is a very
112:12 - crude and basic way of writing things in
112:14 - real world probably we could have this
112:16 - termination Logic on certain condition
112:19 - and when that condition is getting
112:20 - fulfilled we can call the shut shut down
112:22 - method and close the executor service so
112:26 - as I told earlier that this is one of
112:28 - the methods which is provided by the
112:29 - sheduled executor service I will suggest
112:32 - you that as part of exercise you could
112:34 - try to go inside the scheduled executor
112:37 - service and explore the entire interface
112:40 - as in what all different scheduling
112:42 - options are there for example we have a
112:43 - schedule then we have this schedule
112:46 - method signature then we have something
112:48 - called that schedule that fix rate that
112:50 - we implemented then we have have
112:52 - something which is called as schedule
112:54 - with fixed DeLay So I think it will be a
112:56 - good exercise for you to go inside this
112:58 - class and learn about this methods and
113:01 - the other thing is that we have called
113:03 - shutdown now so what it means is that
113:06 - this is a strict guideline to the
113:08 - executor service that whatever you are
113:10 - doing stop doing that and shut down the
113:13 - operations of the executor service there
113:16 - is another method however which is
113:17 - called as shutdown so shutdown is
113:19 - somewhat more graceful in nature and the
113:21 - way it works
113:23 - that it initiates an orderly shutdown in
113:25 - which previously submitted tasks are
113:28 - executed so whatever is there inside
113:30 - your task Q those will be executed and
113:32 - only after that the executor service
113:35 - will be shut down but once you call shut
113:38 - down then the new tasks won't be allowed
113:41 - to come inside the task queue but in the
113:44 - case of shutdown now all the actively
113:46 - executing tasks are also halted and
113:49 - stopped so this is the basic difference
113:52 - now we will have a quick look to
113:54 - understand and visualize the internal
113:56 - workings of the scheduled executor
113:59 - service so as we saw earlier that this
114:02 - trade pool is for the kind of tasks
114:05 - which we want to schedule in some Manner
114:07 - and the way this works is all the tasks
114:10 - are submitted in a task queue but this
114:13 - queue is a delay queue in the delay
114:15 - queue tasks are not kept in sequential
114:18 - manner we can understand this to be some
114:20 - sort of priority queue where the
114:22 - priority is the time of execution so if
114:25 - there are two tasks for example task
114:28 - zero it's supposed to be run after 10
114:30 - seconds from the current time and task
114:33 - one is supposed to be run after 15
114:35 - seconds of the current time then in that
114:37 - case task zero should come earlier as
114:40 - compared to the task one in the task Q
114:43 - likewise if task one is supposed to be
114:45 - executed first before task two then it
114:48 - should be coming earlier in the task Q
114:51 - so everything everything else Remains
114:53 - the Same other than the Q which is a
114:56 - delay Q in the case of scheduled threade
114:58 - pool
115:03 - executor how do we find the ideal pool
115:06 - size for the thread pool well like most
115:09 - of the answers in computer science the
115:11 - answer to this question as well is that
115:13 - it depends however it's worth discussing
115:16 - what it depends on so let's get started
115:20 - imagine that you have a CPU intensive
115:22 - task task which needs to be executed
115:24 - please note that in Java one thread is
115:26 - one O Level thread so if your CPU has
115:29 - four cores then at maximum you could
115:32 - execute four tasks in parallel at once
115:35 - so on a hardware which can only support
115:38 - up to four course creating hundreds of
115:40 - threads won't help if they are CPU
115:43 - intensive in fact it leads to a
115:45 - performance degradation let's focus more
115:48 - on why there is a performance
115:50 - degradation when you have 100 trades to
115:52 - run your CPU intensive task then all
115:55 - such trades will try their best to get
115:58 - their P of the time allocation with the
116:00 - CPU when this time slicing happens
116:02 - Beyond a certain threshold the expense
116:04 - of content switching overpowers the
116:06 - benefit we get from making use of the
116:09 - multi-threading so lots of Trades
116:12 - contesting for CPU time is
116:14 - counterproductive and in such a scenario
116:17 - having too many threads is not a good
116:18 - idea what we can do rather is have the
116:21 - same number of threads as the number of
116:23 - course in your CPU but please note that
116:26 - having created the same number of
116:28 - threads as the number of course in your
116:30 - CPU does not ensure that all the course
116:33 - will be running your threads only some
116:36 - of the course will be utilized by other
116:38 - tasks and Os level processes as well so
116:41 - how can we do this let's say a quick
116:43 - code example in the executor service
116:46 - package let's create this class let's
116:48 - call this as CPU intensive task so let's
116:53 - create a task class and maybe let's call
116:56 - this as CPU task it implements our
117:00 - unable let's overwrite the run
117:05 - method and all it does is it prints some
117:09 - message like some CPU
117:12 - intensive task being done by
117:16 - the and thread dot current thre do get
117:25 - name let's write the main method in the
117:28 - main method first we will try to get the
117:30 - number of the course and we can do so by
117:33 - making use of the runtime so runtime.
117:36 - get
117:37 - runtime do available
117:40 - processors now let's create the executor
117:43 - service call this ad service executors
117:47 - do new fixed threade pool and it could
117:51 - accept the number of of course as the
117:53 - number of threads it wants to create
117:55 - let's print a message for debug purposes
117:58 - so
117:59 - created trade pool
118:05 - with
118:07 - course next let's execute our task so
118:12 - let's say I want to execute 20
118:16 - tasks and service.
118:19 - execute and new CPU task
118:23 - let's run this and see the
118:30 - output here is the output so created
118:32 - thread pool with 14 cores so my machine
118:35 - has 14 cores and what it does is it says
118:39 - like some CPU intensive task being done
118:42 - by thread 1 thread 2 thread 10 thread 14
118:46 - thread 14 so what we see is it has
118:49 - created 14 threads which is the number
118:51 - of cod
118:52 - and then those 14 trates are being used
118:55 - to execute the 20 tasks that we want to
118:58 - execute so now let's learn about the
119:00 - other type of task so there is another
119:03 - type of task which is IO intensive such
119:06 - IO intensive tasks are inherently fire
119:09 - and forget in nature this is because IO
119:12 - operations such as reading from files or
119:14 - making Network calls often involve
119:17 - waiting for external resources to
119:19 - respond during this waiting period the
119:21 - CPU is idle and having more threads
119:24 - allows other tasks to execute while one
119:27 - thread is waiting for the io operation
119:29 - to complete however here as well it's
119:32 - not recommended to go crazy with the
119:34 - countless number of threads rather the
119:36 - more St approach is to experiment with
119:38 - threade pool sizes this will help you to
119:41 - find the optimal balance between
119:43 - concurrency and resource utilization so
119:46 - to summarize there is no single answer
119:48 - for the question of what's the ideal
119:50 - thread pool size what you could do
119:52 - instead is analyze your task patterns
119:55 - and based on their type utilize a
119:57 - combination of the above mentioned
120:04 - approaches till now we have seen how can
120:07 - we execute trades using the executor
120:10 - service but something seems not complete
120:13 - in all the examples that we have seen so
120:15 - far we are not returning anything what
120:17 - if we want to return something from the
120:19 - thread execution how can we do that
120:22 - let's done about the same so here I have
120:24 - created a class this is called as
120:26 - callable demo and we have a class for
120:29 - the task which is implementing the
120:31 - runnable
120:32 - interface and inside the run method that
120:34 - we have overridden I'm trying to return
120:37 - something well of course it's giving
120:39 - some compilation error because at the
120:41 - moment there is no way in which I could
120:43 - return some value from this method why
120:45 - so let's go to the runnable interface
120:48 - and what we see here is that the
120:50 - runnable interface has just one method
120:52 - which is run and it's returning a void
120:55 - that means we cannot return anything so
120:57 - what is the solution so if we want to
121:00 - return anything from a thread the way to
121:03 - do this is implement the callable
121:06 - interface so callable is another
121:08 - interface provided in Java and the way
121:11 - it works is that you implement this
121:13 - interface and it's a generic based class
121:16 - so you'll have to pass the generics of
121:19 - what exactly you want to return so let's
121:21 - say that I I want to return an integer
121:23 - so I'll pass the same and
121:26 - now let's get rid of this one and see
121:30 - what are my implementation options so
121:33 - like we had run in the case of runable
121:35 - in the case of callable we have call and
121:38 - here whatever you provide in the
121:40 - generics the same thing is applied here
121:42 - that is the integer and let's say I want
121:44 - to return 12 so now this 12 should be
121:48 - returned whenever I'm going to run this
121:49 - trade but now this part of the code
121:52 - starts to give some error so what is the
121:54 - problem well the problem is that
121:57 - executor service provides two ways in
121:59 - which we can execute or submit threads
122:02 - to the executor service one is the
122:04 - execute other is the submit so let's
122:07 - replace this with the submit and now
122:09 - this should work fine so what submit
122:12 - expects us is to pass either a runable
122:14 - or a callable so even if you want to
122:17 - pass a unable that is fine with submit
122:19 - and for callable anyway you have to use
122:21 - the sub MIT method itself because you
122:23 - cannot submit a callable by using the
122:26 - execute method so now that we have
122:28 - called the do submit on the execut
122:30 - service what does it return should it
122:32 - return an integer let's find it
122:35 - out so what we see here is
122:39 - that this thing is going to return a
122:42 - future so what is a future future is
122:44 - again a class which is generic based and
122:47 - in this case since we were returning an
122:49 - integer it's going to supply this inte
122:51 - is a value over here and using the
122:54 - result we can do all couple of things
122:57 - and one of those things is calling the
122:59 - do get method on this result so let's
123:02 - print it out and it's giving some error
123:05 - reason is that it is expecting us to
123:08 - either add a catch clause or maybe
123:10 - surround with try catch so for now maybe
123:13 - I'll just add the exception method to
123:15 - the signature so now let's run this code
123:18 - and find out what is the outcome so yes
123:21 - indeed it's is going to return the value
123:23 - that is 12 and this is how we are
123:25 - supposed to make use of callable if we
123:28 - want to return some value from a thread
123:30 - execution but there are a few things we
123:33 - should be mindful of while using the
123:34 - colable interface I'm calling the do get
123:37 - method on the future class let's learn
123:40 - about the
123:44 - same so what we have here is the main
123:46 - threade so assume that this line is the
123:48 - main threade execution and then at some
123:51 - point of time you created a callable and
123:53 - then you did a submission of that
123:56 - callable so once that callable is
123:58 - submitted to the threade pool you will
123:59 - immediately get a future of the type of
124:02 - the generics for the callable that you
124:04 - have implemented and using the future
124:06 - object sometime down the execution you
124:09 - could call future.get so what happens is
124:12 - future is a placeholder and it's empty
124:15 - till the time the processing is not
124:17 - completed by your thread and that time
124:19 - could be anything depending on the
124:21 - execution that you have in your code
124:23 - right so let's say at this point of time
124:25 - you called future.get and the future did
124:29 - not have the final calculated value so
124:31 - what happens please note that future is
124:34 - a blocking operation what it means is
124:36 - that if you call Future and if there is
124:39 - no value inside the future then your
124:41 - entire main threade execution will be
124:43 - blocked so whatever was being executed
124:45 - by the main threade that will be
124:47 - entirely blocked let's say at some point
124:49 - of time the future got the value so you
124:51 - will be able to get the value from here
124:53 - and then the execution will resume or
124:56 - start so this is one thing that you
124:58 - should always keep in mind while making
125:00 - use of the future.get call so now let's
125:04 - demonstrate the same thing let's give
125:06 - another print statement let's call this
125:09 - as main
125:11 - trade
125:13 - execution completed let's run
125:17 - this so we can see that the result is
125:20 - pretty much instantaneous so we are
125:22 - getting the result as soon as possible
125:24 - now let's put some sleep in this threade
125:28 - so let's call thread. sleep maybe let's
125:31 - have a sleep timer of 5 Seconds and with
125:34 - this let's try to execute this one more
125:38 - time so we are waiting there is no
125:40 - result as of
125:43 - now so just after 5 seconds once the
125:46 - execution got completed then we see this
125:48 - line that main thread execution
125:50 - completed so basically what we did was
125:53 - we introduced some sort of latency in
125:56 - the call method and when we called
125:58 - result doget since the future did not
126:01 - have the result populated for this so
126:03 - the main thread was blocked and as soon
126:05 - as this future got its value the value
126:08 - was printed and the execution continued
126:10 - so is there any work around for this
126:12 - Behavior so result. gate has an
126:15 - overloaded method with the signature
126:17 - where you could pass in the timeout so
126:19 - what this will do is it will wait for a
126:22 - certain time before giving a timeout
126:24 - exception and in that time duration if
126:27 - the future does not have the value it
126:29 - will throw an exception and move ahead
126:31 - in the execution otherwise if you get
126:34 - the value in that timeout it will work
126:36 - as expected so let's try the same here
126:39 - it's expected that this call method will
126:41 - be taking 5 Seconds to run so let's try
126:44 - to put a time out of 1 second and let's
126:47 - pass the time unit of second this thing
126:50 - throws an exception let's handle
126:53 - that now let's run it one more
127:02 - time after 1 second it could not find
127:05 - the result and then it threw an
127:07 - exception that is the timeout exception
127:10 - and it ended the execution so if you
127:12 - don't want to end the execution this
127:14 - abruptly and probably continue with your
127:16 - execution you could handle this
127:18 - exception in some other manner so that
127:20 - the execution of your main thread
127:22 - continues now let's increase this value
127:24 - to 6 seconds and find out the
127:33 - result so now since we have provided a
127:35 - timeout of 6 seconds and this thread
127:38 - needs 5 Seconds to complete the
127:40 - execution got completed and then we had
127:43 - this message printed on the console that
127:44 - is main thre execution completed so this
127:47 - is good but is there any other way in
127:50 - which we could handle the f Futures so
127:52 - there are three important methods that
127:54 - we should know so the first one is
127:56 - future. cancel and it expects us to pass
127:59 - a true or false so let's say if I pass
128:03 - true and let's say what exactly does it
128:05 - mean so this value is May interrupt if
128:08 - running so if you pass true then let's
128:10 - say if the threade is running then in
128:12 - that case it may get interrupted if you
128:15 - pass false then it will not be
128:16 - interrupted so this is about the do
128:19 - cancel method the next thing is Future
128:21 - dot is
128:23 - cancelled and this returns a Boolean so
128:26 - using this you can check in your code
128:29 - that the given future is it cancelled or
128:31 - not and based on that you could
128:33 - Implement certain functionality in your
128:35 - class there is another important method
128:37 - which is called as is done so it's
128:40 - result. is done and this again results
128:43 - into a Boolean value and what it
128:45 - signifies is that whatever threade was
128:47 - being executed has that completed please
128:50 - note that this will return true for both
128:53 - the cases where the thread is going to
128:55 - run successfully and return some value
128:58 - and in those cases as well wherein the
129:00 - trade is getting interrupted or may be
129:03 - exiting out due to certain error or
129:05 - exceptions this is the way in which we
129:07 - can make use of the callable interface
129:09 - and based on your use case you can use
129:12 - either runnable or callable to implement
129:14 - threads in Java
129:21 - Java has a rich set of collection
129:23 - classes however most of them are not
129:26 - thread safe we need to take precaution
129:28 - to ensure that the collections are
129:29 - behaving as intended when used in a
129:32 - multi-threaded context there are two
129:34 - ways to do so the first approach is to
129:37 - use the collections. synchronized method
129:40 - and the second approach is that we can
129:42 - use concurrent collections which are
129:44 - essentially concurrent versions of the
129:47 - collections in this video we will learn
129:49 - about the first approach
129:58 - in order to demonstrate the uses of
130:00 - collections. synchronized method I have
130:02 - created this class called as
130:04 - synchronized collection so let's start
130:06 - by creating
130:08 - uh array list let's call this as
130:14 - list and I will be creating two threads
130:18 - the first one is called as one
130:23 - what this will do
130:25 - is it will add some
130:28 - value to the list thousand
130:31 - times so list. add
130:35 - I and let's close this
130:38 - one I will be creating an another thread
130:41 - and it will also add some value to the
130:44 - same list, times
130:52 - list. add
130:56 - I so let's move these things to the main
130:58 - method otherwise this will not work
131:01 - let's create a main method let's place
131:03 - the things here and now 1 do start to do
131:08 - start and in order to ensure that these
131:12 - threads are getting completed let's call
131:15 - the do join on these two
131:17 - methods second is to do join
131:22 - and now let's print the size of the list
131:26 - after the operation has been completed
131:28 - now let's run this program and see what
131:29 - is the output so you see the output is 1
131:33 - 146 however we are adding value to the
131:35 - list a, times and this is being done by
131:38 - two threads so what we should have
131:40 - expected is that the value would be
131:43 - 2,000 but here the ism is mat let's run
131:45 - it one more time and this time we are
131:47 - getting a totally different value so the
131:50 - point here being that it's leading to
131:52 - some sort of inconsistency when this is
131:55 - being used by different threads so how
131:57 - can we solve this so one approach is to
132:01 - make this thing as synchronized so as
132:03 - discussed earlier in the video that
132:06 - there are two approaches to make use of
132:08 - the concurrent collections in Java one
132:10 - is by using the collections.
132:11 - synchronized method and the other is to
132:13 - use the concurrent collection itself so
132:16 - here we will be learning about the
132:17 - synchronized method so let's do that and
132:20 - the wi to do this
132:22 - is let's comment it out let's create a
132:26 - new one so list integer list and we can
132:30 - call
132:32 - collections dot
132:36 - synchronized so you see there are lots
132:38 - of options synchronized list
132:40 - synchronized collection map s navigable
132:43 - map so and so forth but in this case we
132:45 - have to synchronize the list so we will
132:48 - be calling synchronized list and we will
132:50 - pass an array list so this is the way in
132:54 - which we can create a synchronized list
132:56 - using the collections. synchronized
132:58 - method and based on your need you could
133:00 - also invoke other things like
133:02 - synchronized map and synchronized set
133:04 - and all those things so let's run this
133:06 - thing one more time and see the output
133:09 - so we are getting the value as 2,000 so
133:11 - basically list. size is 2,000 now let's
133:14 - run it one more time and now we should
133:17 - be getting the same value through and
133:18 - through so this means now there is no
133:20 - income consistency in the list even
133:23 - though we are using it across multiple
133:25 - threads so the list is a synchronized
133:27 - list now so uh this is the way in which
133:30 - we can use collections. synchronized to
133:32 - help us with creating a concurrent
133:34 - collection however there are certain
133:36 - downsides of using the collections.
133:39 - synchronized approach let's learn about
133:40 - those one by one so the first one is
133:42 - score scint locking what it means is
133:45 - that it uses a single lock to
133:47 - synchronize all the operations on the
133:49 - collection this means not only one
133:52 - thread can access the collection at a
133:53 - given time even if multiple threads are
133:56 - performing unrelated operations this can
133:58 - lead to contention and reduced
134:00 - concurrency especially if there are
134:02 - frequent read and write operations on
134:04 - the collection from different threads
134:07 - the second is the limited functionality
134:09 - and what do we mean by the Limited
134:10 - functionality well the synchronized
134:12 - rapper returned by the collections.
134:14 - synchronized list does not expose any
134:16 - additional methods for fine grin locking
134:18 - or custom synchronization strategies
134:21 - this limits your ability to optimize
134:22 - synchronization based on specific uses
134:25 - or patterns of your application so when
134:27 - we discuss about the concurrent
134:29 - collections we'll try to cover how
134:31 - exactly those collections provide
134:32 - certain other custom synchronization
134:34 - strategies as well so the third downside
134:37 - is that there are no fail fast iterators
134:39 - so collections returned by collections.
134:41 - synchronized list do not support fail
134:43 - fast iterators the fail fast iterators
134:46 - through a concurrent modification
134:48 - exception if the collection is
134:50 - structurally modified when an iterator
134:52 - is iterating over it without fail fast
134:55 - iterators it's possible for concurrent
134:57 - modifications to the collection to go
134:59 - unnoticed and this can lead to certain
135:02 - unexpected behaviors the fourth downside
135:05 - is performance overhead so
135:07 - synchronization introduces overhead due
135:10 - to lock acquisition and release this
135:12 - overhead can degrade performance
135:15 - especially in the high throughput or
135:16 - latency sensitive applications so if at
135:19 - all you don't need these four things
135:21 - then probably you could consider making
135:23 - use of the collections do synchronize
135:25 - method in your use case but let's say if
135:28 - you require some of these
135:29 - functionalities or these features uh
135:32 - it's better to go with the concurrent
135:34 - collections which are provided by the
135:35 - Java so now let's start learning about
135:38 - those concurrent
135:45 - collections let's learn about the
135:47 - countdown latch countdown latch is a
135:49 - synchronization utility that allows one
135:52 - or more threads to wait until a set of
135:54 - operations which is being performed in
135:56 - another threads completes it is part of
135:59 - the Java util concurrent package and can
136:02 - be used for controlling the flow of
136:04 - execution in the concurrent
136:06 - programs the key concept of countdown
136:08 - latch is that it maintains a count that
136:11 - it starts from a specific number and
136:13 - decreases each time the countdown method
136:15 - is called threats that need to wait for
136:18 - the countdown to reach zero can call the
136:20 - await me method which will block until
136:22 - the count becomes zero so when to use
136:25 - countdown latch let's understand the
136:28 - scenario in which we can use countdown
136:30 - latch with a very simple and easy to
136:32 - understand example imagine you are a
136:35 - take lead who is leading a complex
136:37 - project being the expert which you are
136:40 - you divide the complex task to some
136:42 - smaller and simple to implement tasks
136:44 - after splitting the task you distribute
136:47 - them with your team and they start
136:49 - working on their respective tasks
136:51 - however now you cannot proceed further
136:53 - on this project until they all complete
136:56 - their task in scenario like this you can
136:58 - use a countdown latch countdown latch is
137:01 - like a checkpoint for each team member's
137:04 - progress you set up the latch with the
137:06 - total number of task or team members
137:09 - each member upon completing their task
137:12 - calls the countdown method of the latch
137:14 - it indicates that they are done as the
137:17 - organizer you wait for all the tasks to
137:20 - be completed
137:21 - by calling the await method this method
137:24 - hals your progress until the count
137:26 - reaches to zero meaning all tasks are
137:29 - done once all the tasks are completed
137:32 - the latch opens up and you can proceed
137:35 - with the next phase of the project so
137:37 - countdown latch helps in coordinating
137:40 - multiple threads or tasks to synchronize
137:43 - their work it ensures that they all
137:45 - reach a certain point before proceeding
137:47 - further so let's have a quick code
137:49 - demonstration on on how we can use the
137:52 - countdown latch for developing certain
137:53 - use case we are in the concurrent
137:56 - collection
137:57 - package and I will create a class let's
138:00 - call this as
138:02 - restaurant and uh I will Implement a
138:06 - task let's call that as
138:09 - chef and let it implement the
138:13 - runable so before we move ahead with the
138:16 - implementation let's discuss a bit about
138:19 - what we are planning to implement
138:21 - so what we are planning to build here is
138:23 - a simulation where a group of Chef need
138:26 - to prepare different dishes in a
138:28 - restaurant kitchen each Chef is
138:31 - responsible for preparing a specific
138:33 - type of dish and the kitchen manager
138:35 - wants to start serving customers only
138:38 - when all the dishes are ready so here is
138:41 - how we can use the countdown latch to
138:43 - coordinate all the chefs so let's
138:45 - proceed with the implementation of the
138:47 - chef class so first thing is let's
138:51 - overwrite the run method so with that in
138:54 - place let's have a few properties of the
138:57 - class so first is uh
139:01 - string which is
139:04 - name the second
139:07 - is again a string which is a a dish and
139:11 - Third Field
139:13 - is going to be the countdown latch
139:17 - itself let's initialize all these fields
139:19 - in a Constructor
139:22 - so now that we have initialized all the
139:24 - fields in the Constructor here is what
139:26 - we are going to have in the run method
139:29 - so uh we try to simulate the preparation
139:31 - of the dish and for that
139:34 - let's have a triy
139:36 - block and then a catch Block it's going
139:40 - to throw an interrupted exception let's
139:42 - call this as e and for now just throw a
139:47 - runtime
139:49 - exception and for the tri block what we
139:52 - will be doing is first we print a
139:54 - message like name is
139:59 - preparing and then the
140:02 - dish next let's have a sleep time of 200
140:08 - milliseconds and the idea is that this
140:10 - is the simulation for the cooking time
140:13 - so essentially your Chef is going to
140:15 - cook for 2 seconds quite a fast Chef
140:18 - right he is cooking in just 2 seconds
140:22 - anyway so let's have another print
140:26 - statement and what this will be printing
140:28 - is name plus has
140:32 - finished
140:34 - repairing and the dish so till now what
140:38 - we have tried to mimic is that the chef
140:41 - is going to prepare some food and once
140:43 - they are done preparing the food this
140:45 - message will be printed and what it
140:47 - means is that the task has been
140:49 - completed so what we will do is we will
140:53 - call latch do
140:56 - countdown so that should be it for this
140:58 - class that is the chef class now let's
141:01 - implement the main method in the
141:03 - restaurant
141:05 - class so let's have an integer value
141:10 - let's call this as number of
141:13 - chefs let's say we have three
141:16 - chef and what we will do now is we will
141:20 - create the trades for the Three Chefs
141:22 - and then we will start those trades the
141:26 - way we can do this is new thread and uh
141:29 - it could accept a new
141:31 - Chef let's name this as Chef
141:36 - a let's assume the chef
141:39 - is preparing pizza and then we pass the
141:44 - latch so we have not created the latch
141:47 - yet we will create in some
141:49 - time and and then let's call the do
141:51 - start method on this one so let's create
141:53 - the
141:54 - latch so let's have the countdown latch
141:58 - let's call this as latch and new
142:01 - countdown latch number of
142:06 - shfs let's create a same kind of threade
142:09 - for the another Chef as well so new Chef
142:13 - let's call this guy as Chef
142:16 - B and let's assume that this guy is
142:19 - preparing
142:21 - fter so let's call Larch and let's call
142:24 - start let's duplicate
142:27 - this and
142:28 - uh let's say this guy is
142:31 - preparing maybe
142:35 - salad and this guy is Chef
142:39 - C so when we have created the threads
142:42 - and called the start on these threads
142:45 - these threads will start running at some
142:47 - point of time and then we should make
142:50 - use of the latch somehow in order to
142:53 - ensure that this entire thread of
142:55 - execution that is this one is blocked
142:59 - till the time all these three threads
143:00 - are completed right and the way to do
143:03 - this using the latch is call the await
143:07 - method what does a method do it will
143:10 - ensure that the thread is waiting for
143:13 - all the dishes to be completed
143:15 - eventually three threads are there all
143:17 - those three threads are done with their
143:19 - execution and and a wait will expect us
143:21 - to handle some
143:23 - exception and uh either we could add
143:26 - into the method signature or surround
143:28 - with trr for now let's add to the method
143:31 - signature that is an interrupted
143:33 - exception and once this is done then we
143:36 - can print a message that all the
143:40 - dishes are
143:44 - ready let's start
143:47 - soling customers
143:52 - so this is what we have implemented
143:54 - let's try to run this and see its
144:03 - output so when we call do start the
144:07 - message is printed
144:08 - that Chef C is preparing salad Chef a is
144:12 - preparing Pizza Chef B is preparing this
144:15 - fasta and after 2 seconds all these
144:17 - three ships are done and what they do is
144:20 - they print Chef C has finished Chef a
144:23 - has finished Chef B has finished and
144:26 - when these guys are finishing with their
144:29 - preparation they will be eventually
144:31 - calling the last dot
144:33 - countdown and since we have defined a
144:36 - value of three that is the number of
144:38 - shfs which was three here so this
144:42 - countdown latch has a value three to
144:44 - begin with and the weight works is that
144:46 - last. a we will hold true or rather it
144:49 - will hold the execution of the thread on
144:51 - which this has been called till the time
144:54 - it does not reach zero and by the time
144:57 - the third Chef is done that is Chef B
144:59 - has finished preparing the pasta then in
145:02 - that case that threade will be calling
145:05 - the latch dot countdown and once that
145:08 - countdown is invoked it means that the
145:10 - countdown becomes the counter rather
145:12 - becomes zero and at that point of time
145:16 - this will be unlashed and then the
145:18 - threade will be unblocked for the
145:19 - further processing which is system.out
145:22 - and this message will be
145:24 - printed so this is what the countdown
145:27 - latge does and this is the way in which
145:29 - we can make use of this for our use
145:32 - cases now you may have a doubt you may
145:35 - think that in the beginning of the video
145:37 - we learned about the join method so is
145:40 - it not very functionally similar to the
145:42 - join so let's understand that so from
145:45 - the purpose perspective countdown latch
145:48 - is designed to allow one one or more
145:50 - threads to wait until a set of
145:52 - operations in other threads complete it
145:55 - is typically used for coordination among
145:58 - the multiple threads join on the other
146:01 - hand is used to wait for a thread to
146:03 - complete s execution before proceeding
146:06 - with the rest of the code and it's
146:08 - specifically used for thread
146:10 - synchronization within a single threaded
146:13 - context so how exactly are these two
146:15 - things different from the user
146:16 - perspective so the countdown latch is
146:19 - useful when you have multi trades
146:20 - performing independent tasks and you
146:23 - want to coordinate them before moving
146:25 - forward and join is useful when you have
146:28 - a main threade that respawns worker
146:30 - trades and needs to wait for them to
146:32 - finish before continuing its execution
146:35 - so I hope these points should have drawn
146:37 - a clear picture of the differences
146:39 - between the countdown latge and join
146:41 - method however you might have one more
146:44 - doubt left you may think if there are
146:47 - three threads and there is a countdown
146:49 - latch for them so either use the
146:51 - countdown latch or call join methods
146:53 - three times which is on all the three
146:56 - threads is it not the same right so why
147:00 - take a pain of learning A New Concept
147:02 - all together think of the scenario where
147:05 - you have a dynamic number of threads so
147:08 - what do you do how many times and in
147:11 - what Manner are you going to call join
147:13 - method on those threads in situations
147:16 - like this countdown latch shines as
147:18 - compared to the uses of join method
147:20 - overall countdown latch is a much
147:23 - cleaner and elegant approach especially
147:25 - if you are coordinating among multiple
147:27 - thread operations so the final thing
147:30 - about the countdown latches can we reset
147:32 - the count well countdown latch is
147:35 - supposed to be used as a oneshot
147:37 - solution so you cannot reset the count
147:40 - if you need a solution which resets the
147:42 - count you are looking for something
147:44 - called as a cyclic barrier and with this
147:47 - let's learn about the cyclic barrier
147:49 - next
147:55 - now let's learn about a very important
147:57 - concurrent collection in Java which is
147:59 - blocking queue since it's quite commonly
148:02 - used we will try to learn it in somewhat
148:04 - more detailed manner imagine a blocking
148:07 - queue to be like a conveyor belt in a
148:09 - factory where items are placed and taken
148:11 - off in Java a blocking queue is a data
148:14 - structure which allows multiple threads
148:16 - to safely put items onto the queue and
148:20 - take items off the queue and this is
148:23 - done in concurrent manner there are two
148:25 - aspects of blocking queue so the first
148:27 - part is the blocking aspect the term
148:30 - blocking means that if a threade tries
148:33 - to take an item from a queue which is
148:35 - empty it will be paused or blocked until
148:38 - an item becomes available similarly if a
148:41 - thread tries to add an item to a full
148:43 - queue it will be blocked until the space
148:45 - becomes available the second aspect is
148:48 - the uses of the queue so the blocking
148:50 - queue follows the first in first out
148:52 - principle meaning the item that was
148:54 - added first will be the first one to be
148:57 - taken out now let's talk about the
148:59 - blocking queue interface in Java the
149:02 - blocking queue interface is the parent
149:03 - interface to a few other interfaces and
149:06 - the concrete implementational classes so
149:09 - this is the parent interface which is
149:10 - the blocking que and the two such
149:12 - interfaces which extend this blocking Q
149:15 - are blocking DQ and transfer Q so this
149:19 - is also sometimes called called as
149:20 - blocking deck or DQ so whatever you wish
149:23 - to call you could call it now coming to
149:25 - the blocking deck so blocking deck is a
149:28 - double-ended q that blocks threads when
149:30 - it reaches its capacity or becomes empty
149:34 - thereby facilitating flow control
149:36 - between producers and consumers it
149:38 - provides methods to access the queue
149:41 - from both the ends in a thread safe
149:42 - manner due to its double-ended nature
149:45 - the performance characteristics of
149:46 - blocking deck may differ from those of
149:49 - blocking queue especially in the
149:51 - scenarios where there is a contention
149:53 - for access to both ends of the deck by
149:55 - multiple threads simultaneously key
149:58 - takeaway from the implementational
150:00 - perspective here is that it provides
150:01 - access from both the ends in a
150:03 - concurrent manner regarding the transfer
150:05 - queue it's a specialized queue where
150:08 - producers can block until a consumer
150:10 - directly receives an item it extends the
150:13 - functionality of blocking queue by
150:15 - providing a method called as
150:17 - transfer what this method does is it
150:19 - allows one threade to transfer an item
150:21 - directly to another waiting threade
150:23 - potentially avoiding the need for
150:26 - blocking if there are no waiting trades
150:28 - transfer behaves like put and blocks
150:31 - until there is a space available for the
150:33 - item transfer queue ensures a strong
150:36 - hand of coordination what does this mean
150:39 - let's understand this with an example
150:41 - imagine a factory where one worker
150:44 - produces items and another worker
150:46 - packages them for shipping the producer
150:49 - worker can directly transfer the
150:51 - produced item to the packaging worker
150:53 - without waiting if the packaging worker
150:55 - is ready to receive it if the packaging
150:58 - worker is busy the producer worker Waits
151:01 - or blocks until the packaging worker
151:03 - becomes available to receive the item
151:05 - this direct mechanism can improve the
151:07 - efficiency in certain scenarios compared
151:10 - to a traditional blocking queue so now
151:12 - let's learn about the major
151:13 - implementations of The Blocking queue
151:15 - the first one on the list is array
151:16 - blocking queue it implements a bounded
151:20 - blocking queue and it is backed by an
151:22 - array data structure the next one is the
151:24 - linked blocking queue so this one
151:27 - implements a bounded or unbounded
151:29 - blocking queue backed by a link list the
151:31 - priority blocking queue implements a
151:33 - blocking queue that orders elements
151:36 - based on their natural ordering or
151:38 - according to a specified
151:40 - comparator then we have delay Q what it
151:43 - does is it implements a blocking queue
151:44 - of delayed elements where an element can
151:47 - be only taken out when its delay has
151:50 - expired it is particularly useful for
151:52 - scheduling tasks to be executed after a
151:55 - certain delay or a specific time the
151:58 - final one is the synchronous queue this
152:01 - implements a zero capacity blocking
152:03 - queue where each insert operation must
152:05 - wait for a corresponding remove
152:07 - operation by another threade and vice
152:09 - versa so based on your requirement and
152:11 - uses you could use either of these
152:14 - implementations but for most practical
152:16 - purposes the uses of array blocking
152:19 - queue are linked blocking queue or a
152:21 - priority queue should be good enough now
152:23 - let's have a look on the blocking queue
152:25 - operations so these are the certain
152:27 - operations which are supported by the
152:29 - blocking queue put method adds the
152:31 - specified Element e to the que if the
152:34 - space is available if the queue is full
152:37 - the operation blocks until the space
152:39 - becomes available next is the take
152:41 - method it retrieves and removes the head
152:44 - of the que if the queue is empty the
152:46 - operation blocks until an element
152:48 - becomes available the third one on the
152:49 - list is the offer method what this does
152:52 - is it adds the specified element to the
152:55 - Q if the space is available it returns
152:58 - true if the element was successfully
153:00 - added or false if the Q is full as
153:02 - opposed to the put method which blocks
153:05 - until the space is available pole method
153:08 - retrieves elements from the head of the
153:10 - queue it returns null if the queue is
153:12 - empty it does not block the Q if the Q
153:16 - is empty as opposed to the take
153:17 - operation which blocks the Q until an
153:20 - element is available the peak method
153:23 - retrieves but does not remove the head
153:25 - of the queue it returns null if the
153:28 - queue is empty some of the above methods
153:31 - also provide overloaded signatures which
153:34 - allow you to pass timeout values as well
153:37 - so I suggest that you explore the
153:38 - blocking queue interface there you'll
153:41 - find all the variations of the measor
153:43 - methods which we have described here so
153:46 - all the implementations of The Blocking
153:47 - deck interface are functionally similar
153:49 - methods which provide these operations
153:52 - on both ends as an exercise you could
153:54 - try exploring the interface and the
153:56 - implementations to understand them
153:58 - better so now let's have a code
154:00 - demonstration which will showcase the
154:02 - uses of The Blocking queue interface in
154:04 - order to demonstrate the users of The
154:06 - Blocking queue interface I'll be
154:08 - creating this class inside the
154:09 - concurrent collection
154:11 - package let's call this as blocking
154:13 - queue
154:16 - demo here is what I'm planning to
154:19 - implement it will be of some capacity
154:21 - and there would be a couple of threads
154:23 - one thread would be a producer thread
154:25 - then we can have a couple of more
154:27 - threads for example let's say two
154:29 - threads as the consumer threads and once
154:32 - we start the threads then the producer
154:35 - thread should be able to produce or
154:37 - write data to the que and consumer
154:39 - threads should be able to consume from
154:41 - the queue so let's implement this so
154:45 - let's start by having the Q capacity
154:48 - let's call this as
154:53 - Q next we can have the blocking Q
154:57 - itself so let's declare that and we want
155:01 - to put an integer value in the blocking
155:04 - queue let's call this as a task q and we
155:08 - are going to make use of the
155:09 - implementation of array blocking queue
155:11 - let's pass the capacity that is the Q
155:14 - capacity and let's start by implementing
155:16 - the main
155:17 - method first on the list would be the
155:19 - the producer
155:22 - thread here in the producer thread what
155:24 - we will be doing is we will be iterating
155:27 - for certain amount of time and then
155:29 - putting that number to the task CU so
155:32 - let's Implement that so we have a tri
155:35 - block let's have a catch block as well
155:37 - we will have an interupted exception so
155:40 - let's put this for
155:44 - now and now coming to the dry block we
155:48 - will have a loop
155:50 - which is running
155:52 - from 0 to 20 0 to 19 rather or maybe
155:57 - let's change the numbers from 1 to 20
156:01 - and all it does is it puts this number
156:04 - to the task CU so let's call put and the
156:07 - value would be
156:09 - I let's print a
156:11 - message
156:13 - task
156:15 - produced
156:17 - I and let's have a sleep timer for let's
156:21 - say 100 milliseconds so this is
156:23 - essentially the task generation time
156:25 - that we are having
156:27 - here and now that we have written the
156:30 - producer thread let's write the consumer
156:32 - thread as well so let's have the first
156:35 - consumer
156:36 - thre let's call this as consumer
156:42 - 1 and let's rename this to producer
156:50 - so what this consumer thread does is it
156:52 - consumes from the task CU infinitely so
156:54 - let's Implement that so let's start by
156:57 - having a triy block let's have a catch
156:59 - block let's have an interrupted
157:01 - exception and
157:04 - uh let's throw this one as a runtime
157:07 - exception so while
157:12 - true what we are doing to do is we can
157:15 - retrieve the task which we have in the
157:17 - task Q
157:19 - do
157:20 - te and then we will call a method let's
157:24 - call this as process task so let's pass
157:28 - the task and
157:32 - uh the consumer
157:35 - ID so let's now implement the process
157:38 - method before implementing the second
157:41 - consumer so private static void process
157:46 - task then we have the in task then we
157:50 - have the consumer
157:53 - name and let it through the interrupted
157:56 - exception because I wish to provide some
157:58 - sort of sleep in this particular
158:01 - method so first we start by printing a
158:04 - message that task
158:07 - being
158:08 - processed
158:10 - by consumer name
158:20 - and then print the
158:23 - task then we will try to sleep this for
158:28 - 1,000
158:29 - milliseconds so the idea is that in real
158:31 - scenario as well when you will be
158:34 - processing a task that will involve
158:36 - certain time right so here since we have
158:39 - a pretty much Bare Bones implementation
158:41 - what we are going to do is we are going
158:43 - to provide some sort of sleep and that
158:45 - will mimic the processing time which is
158:48 - 1,000 milliseconds in this cas
158:49 - case and now once we have done the
158:54 - processing we can print the message the
158:58 - task consumed by consumer name
159:02 - and this is the task and uh now we will
159:07 - be implementing the second consumer
159:09 - thread as
159:10 - well so let's call this
159:13 - as consumer
159:16 - 2 and we will have a similar kind of
159:19 - implementation here as well so let's
159:21 - copy this one really
159:23 - quick and what we are going to do is
159:26 - everything Remains the Same so this is
159:28 - going to run
159:29 - infinitely it's going to extract out the
159:31 - task from the task Q process task
159:34 - Remains the Same it's just that we
159:36 - change the name here so now let's call
159:39 - this as consumer
159:40 - 2 and everything else Remains the Same
159:44 - so now let's start the threads so
159:47 - producer. start cons consumer 1. start
159:51 - consumer 2.
159:53 - start so I think this should be it for
159:56 - the implementation side of things let's
159:58 - run this code and see how is the output
160:03 - like so as you can see that task being
160:07 - produced is being printed by the
160:09 - producer method and then task being
160:12 - processed by consumer
160:14 - one and uh task consumed by consumer one
160:19 - so we had introduced some sort of sleep
160:21 - right so during that time another
160:23 - threade takes over that is the producer
160:25 - threade and it's going to produce so
160:29 - basically task being produced so task ID
160:32 - with two is produced and kept in the
160:35 - queue then uh this task is being
160:37 - consumed by consumer 2 as soon as the
160:40 - task is available in the uh processing
160:42 - queue right so this is the way in which
160:45 - the producer is uh creating tasks and
160:49 - pushing to the queue and the consumer on
160:51 - the other hand which we have a pair
160:53 - consumer one and two they are pulling
160:56 - the queue and they are taking out from
160:58 - the queue and working on that and uh
161:01 - consuming the tasks in the task queue so
161:05 - you could try to implement it yourself
161:08 - so using the block Q we have implemented
161:11 - a simulation wherein we are writing to a
161:14 - threade safe concurrent data structure
161:16 - which is the blocking queue and we are
161:18 - able to operate on the data structure
161:20 - using multiple threads that is the
161:22 - producer thread and two consumer threads
161:25 - so you could go through the code one
161:26 - more time and try to get a hang of it if
161:29 - it's not very
161:30 - clear so as an exercise you could go
161:33 - ahead and explore these classes and try
161:35 - to come with certain use cases and
161:37 - Implement those use cases using these
161:39 - classes so this will really help you
161:41 - with your understanding of the uses of
161:43 - The Blocking queue in the Java
161:50 - now let's learn about the concurrent map
161:52 - in Java so concurrent map is an
161:54 - interface in Java which represents a map
161:57 - that can be safely accessed and modified
161:59 - concurrently by multiple threads it
162:01 - extends the map interface and provides
162:04 - additional automic operations to support
162:06 - concurrent access without the need for
162:08 - explicit
162:10 - synchronization concurrent map is needed
162:12 - in Java to handle the situations where
162:14 - in multiple threads need to access and
162:16 - modify a map concurrently concurrent map
162:19 - has a couple of implementations such as
162:21 - concurrent hash map concurrent escap
162:23 - list map concurrent linked hash map
162:26 - concurrent navigable map Etc out of
162:28 - these concurrent hashmap is probably the
162:30 - most widely used one and this should be
162:32 - sufficient for most of your practical
162:34 - use cases so now let's have a quick code
162:38 - demonstration and uh we will Implement a
162:40 - simple feature using concurrent hashmap
162:42 - which showcases how this concurrent
162:44 - collection can be used in a multi-
162:46 - threaded context
162:57 - so here is a brief idea behind what I'm
162:59 - trying to implement for this
163:01 - demonstration so I am planning to
163:04 - implement uh concurrent cache and uh
163:08 - basically there would be a couple of
163:09 - threads that will be writing certain
163:11 - values to the cach and uh those trates
163:15 - will be reading the value from the cash
163:17 - right so how we could could achieve this
163:19 - using the concurrent hashmap is what we
163:21 - are going to build so let's start first
163:24 - we have
163:26 - the data structure that will be holding
163:28 - the cache which is a map of
163:30 - course and it's a string string let's
163:34 - call this as cach this will be a
163:37 - concurrent hash
163:39 - map now let's implement the main
163:41 - method and we are going to iterate
163:45 - from is0 to I 9
163:51 - so let's assume that this I value is the
163:53 - trade count so maybe call this as
163:58 - threade
163:59 - number and this is
164:02 - I and what we will be doing is we will
164:05 - be creating a
164:06 - thread so let's create this thing
164:09 - anonymously and finally we will also
164:11 - need to call start on this one so we
164:13 - have written start
164:15 - here so first thing is we create the key
164:19 - and the key could be a key string at the
164:21 - rate of value and then the thread
164:25 - num what we will do is we will try to
164:28 - Fitch the same key three times so that
164:31 - we could demonstrate that one time the
164:34 - cache is empty so we are going to put
164:36 - the value in the cache and for the next
164:38 - two times the value should be faced from
164:39 - the cache right so in order to mimic
164:42 - that functionality let's write an
164:44 - another
164:46 - loop so from z0 to J Less Than 3
164:52 - j++ and uh let's capture the
164:56 - value which is going to be let's say a
164:59 - method called as cast value let's pass
165:02 - the key we will implement this gate cast
165:05 - value in a moment for now let's just
165:08 - have this method name itself now let's
165:10 - print some message here so the message
165:13 - could be
165:15 - threade threade do current threade dog
165:19 - name let me format this
165:23 - one and uh let's say
165:28 - key pass the key
165:32 - value
165:34 - and
165:40 - value pass the
165:43 - value so this should be it for the main
165:46 - method now let's implement the other
165:49 - methods which is the cach value for
165:52 - now so let's call this as private
165:56 - static
165:58 - string gate
166:01 - cast
166:03 - value let's
166:06 - say the parameter is the
166:09 - key and what we do first is that we try
166:13 - to fix the value from the cache so cache
166:16 - doget key
166:19 - if the let's correct this
166:24 - spelling so if value happens to be null
166:27 - it means that the key is not present in
166:30 - the map so we need to calculate the key
166:33 - somehow the value rather somehow so
166:36 - let's have another method for the same
166:38 - let's call this as compute pass the key
166:41 - it's not implemented we will Implement
166:43 - us in a while now let's put the same
166:46 - value in the cache so cach output key
166:50 - then the
166:51 - value and finally return
166:55 - the
166:59 - value all
167:02 - right so let's implement the compute
167:05 - method as well so that we can complete
167:08 - our entire
167:11 - implementation so let's return this as
167:14 - an
167:17 - string so
167:22 - compute it takes a
167:26 - key and first we print a
167:29 - message which
167:31 - is
167:34 - key not
167:36 - present in the
167:40 - cache
167:42 - so I'm going to
167:46 - compute now let's put a
167:49 - try and catch block because we will be
167:53 - needing
167:54 - this
167:58 - so now what we are doing to implement
168:00 - here is that we have a compute method
168:03 - and it's going to compute certain value
168:07 - so we want to mimic the logic for
168:10 - computation and uh so that will need
168:13 - certain time to execute so let's mimic
168:15 - the Same by putting a
168:19 - sleep here maybe for 500
168:22 - milliseconds and once it's done return
168:24 - the value that is value for
168:27 - key so I think this should be it for the
168:31 - entire
168:33 - implementation now let's run this one so
168:36 - let me clean this up a
168:39 - bit all right let's run this and find
168:42 - out its
168:45 - output so what we see here is that the
168:49 - thread get is started and uh first is
168:53 - that key five not present in the cache
168:56 - so going to compute then key zero not
168:59 - present in the cache so going to compute
169:01 - so essentially uh for all the threads
169:03 - from 0 to 9 the key has been created
169:07 - that is key at the rate of five key at
169:08 - the rate of Zer so and so forth and in
169:11 - the first try they are not present in
169:12 - the cash so uh those will be calculated
169:16 - now and once they are calculated they
169:18 - getting getting faced from the map
169:20 - itself from the cache itself so let's
169:22 - try to find this for key atate
169:25 - 5 so here five is being faced right so
169:30 - the for the first time it's being
169:31 - created by the compute method and for
169:33 - the this two times it's being faced in
169:36 - this manner wherein you have the thread
169:38 - five and the key is this one and the
169:41 - value is value for key 5 it's same for
169:44 - the other Keys as well for example for
169:46 - key 7 for key 6 for key four and so and
169:49 - so forth so what we have implemented
169:52 - essentially is uh concurrent cache and
169:55 - the idea is very simple that there are a
169:58 - couple of threads which are going to put
170:01 - certain value in the cach and uh get the
170:04 - value out of the cache how it's being
170:06 - done is like in the G cast value
170:09 - implementation itself we are trying to
170:11 - fetch the value from the cache if it's
170:13 - not present we are Computing it and
170:16 - putting in the cache if it's present
170:18 - then this will not be executed and
170:20 - directly we will be returning the value
170:22 - so the first call to the cach is a cash
170:25 - Miss and then the cash will be populated
170:27 - with the value which is being generated
170:28 - by this compute method which is doing
170:31 - nothing it just has a sleep timer of 500
170:34 - millisecs and then the value is returned
170:36 - as value for this particular key and
170:39 - once the value is put in the cache then
170:41 - for the subsequent calls the cach is
170:44 - being uh queried and we're getting the
170:46 - value from it so as you you can see that
170:49 - we have not used any um lcks or
170:52 - synchronization and all those things so
170:54 - it's a very neat implementation because
170:56 - we are using the concurrent hashmap and
170:58 - this particular collection is is a
171:01 - concurrent collection and we don't need
171:03 - to handle any sort of synchronization as
171:05 - such so that should be it for the
171:08 - implementation and the code demo now
171:10 - let's learn a few more things about the
171:12 - concurrent hashmap so let's understand
171:15 - about the internal implementation and
171:17 - working of the concurrent map let's
171:19 - break this into two parts the first is
171:21 - the adding an element to the concurrent
171:23 - hash map and the second is fetching an
171:26 - element from the concurrent hashmap and
171:28 - why we are learning about the concurrent
171:30 - hashmap itself the reason is that it's
171:32 - the most commonly used implementation of
171:34 - the concurrent map interface and for
171:36 - most of the use cases this should be
171:38 - sufficient for you as well so let's
171:40 - learn how the elements are added to a
171:42 - concurrent hashmap so as the first step
171:44 - we start with hashing and determining
171:46 - the segment internally the concurrent
171:49 - hash map is arranged as a smaller
171:51 - segments when a new element is added to
171:53 - a concurrent hash map its key is hashed
171:56 - to determine which segment it belongs to
171:58 - and each segment acts like a small
172:00 - hashmap within the larger concurrent
172:02 - hashmap in the Second Step the lock is
172:04 - acquired so once the segment is
172:06 - determined the thread needs to acquire
172:09 - the lock for that segment this ensures
172:11 - that only one thread at a time can
172:13 - modify that particular
172:15 - segment and in the third step data is
172:18 - inserted in this segment with the lock
172:20 - acquired the new key value pair is added
172:22 - to the segment's internal array if
172:25 - needed the segment May reside itself to
172:28 - accommodate the new element and finally
172:31 - as the fourth step the lock is released
172:34 - after the insertion is completed this
172:36 - allows other threads to concurrently
172:38 - modify the other segments of the map and
172:41 - likewise here are the steps that are
172:43 - needed for fetching an element from the
172:45 - concurrent hashmap so here as well the
172:47 - first step is hashing and determining
172:49 - the segment when a thread wants to fetch
172:52 - an element from the concurrent hash map
172:54 - it first hashes the key to determine the
172:57 - segment which it belongs to and as part
172:59 - of the Second Step the activity is
173:02 - similar to adding an element the thread
173:04 - needs to acquire the lock for the
173:06 - segment that contains the desired key in
173:09 - the third step we search in the segment
173:11 - once the lock is acquired the thread
173:14 - searches for the key in the segment's
173:15 - internal array please note that since
173:18 - only one thread can modify the segment
173:20 - at a time the search operation is safe
173:23 - from concurrent modifications if the key
173:25 - is found the corresponding value is
173:28 - returned if not found null or other
173:30 - designated value is returned to indicate
173:33 - absence of the element being searched
173:35 - for and as the final step after F
173:38 - operation is completed the lock for that
173:41 - segment is released here onwards other
173:43 - threads are allowed to access and modify
173:46 - the map concurrently so so this process
173:49 - continues over and over again for all
173:52 - the data rights and deeds from the
173:53 - concurrent map and this is how the
173:55 - concurrent Map works in Java so as we
173:59 - can see that here the Locking and
174:02 - synchronization of the concurrent map is
174:05 - on a very granular level as compared to
174:08 - the approach of synchronization or
174:10 - creating the synchronized collection
174:12 - wherein the synchronization was being
174:14 - done on the entire map or the entire
174:17 - collection itself self so this is pretty
174:20 - much faster in that sense because we are
174:22 - not holding up other threads from
174:25 - modifying or interacting with the map
174:27 - and this is the reason that concurrent
174:29 - map is a better choice when you are
174:31 - dealing with a multi-threaded context
174:33 - and you need to have some sort of map
174:35 - based
174:41 - collection imagine you are playing a
174:43 - game with a group of your friends the
174:46 - rule states that everyone must together
174:49 - at a certain spot before you can all
174:51 - move forward to the next level this spot
174:54 - is like a checkpoint now you cannot move
174:58 - forward until all your friends are there
175:00 - with you in Java this is like using the
175:03 - cyclic barrier each friend represents a
175:06 - thread in your Java
175:08 - program when a thread reaches the
175:10 - checkpoint it calls a weight method on
175:13 - the cyclic
175:14 - barrier the thread Waits there until all
175:17 - the other threads have also reached the
175:21 - checkpoint once everyone is there the
175:23 - barrier is stripped and all threads can
175:26 - move forward now let's have a quick code
175:29 - demonstration using which we will
175:31 - understand how we can make use of the
175:33 - cyclic barrier in our Java program so to
175:37 - demonstrate the cylic pario let's create
175:39 - a class I'll call this as multi-stage
175:44 - tool the idea here is that I want to
175:48 - Implement a feature wherein there is a
175:50 - group of tourist along with a tour guide
175:54 - and there are different stages of the
175:56 - tour and during a stay the tourists are
175:59 - supposed to arrive at a particular
176:01 - location and once all the tourists
176:04 - arrive at a particular location then
176:07 - they will move along to the next stage
176:09 - of the tour with the tour guide so this
176:12 - is what we are going to implement
176:14 - overall this is a small simulation to
176:16 - demonstrate the cyclic barrier users so
176:19 - let's implement the
176:21 - same so first let's have the member
176:23 - variables first we will have the number
176:25 - of
176:26 - tourists so let's call this as public
176:29 - static final
176:37 - int let's say this is
176:41 - five then the next one we could have as
176:44 - the number of stages
176:49 - let's say this is three and let's create
176:52 - the cyclic
176:54 - barrier public static final let's call
176:57 - this as the cyclic
176:59 - barrier let's call this as
177:02 - barrier and new cyclic barrier so cyclic
177:06 - barrier does accept a number which is
177:08 - going to be the count on the basis of
177:11 - which it's going to maintain that how
177:13 - many threads have returned to the
177:15 - barrier point so let's provide the
177:21 - same and meanwhile let's also check the
177:24 - other options that we
177:26 - have so one option is this
177:29 - Constructor there is another Constructor
177:32 - as well which is giving us the uh
177:36 - integer value that we are supplying the
177:38 - parties basically and along with that we
177:41 - could also Supply the barrier action so
177:44 - what is the barrier action so the
177:45 - barrier action is the command to execute
177:48 - when the barrier is stripped or null if
177:50 - there is no action so I think for
177:54 - demonstration purposes let's include a
177:56 - small action for
177:58 - this
177:59 - so let's provide a unable
178:06 - here and what we are going to provide is
178:11 - a simple message that is tool guide
178:15 - starts speaking
178:19 - so don't worry if it's not making sense
178:22 - as of now as soon as I finish the
178:25 - implementation I will explain everything
178:27 - in as much details as needed to
178:30 - understand this concept so let's quickly
178:33 - create the static class as well which is
178:35 - going to be the tourist class and it's
178:40 - going to implement the runable
178:43 - interface let's have the
178:48 - run method
178:50 - overridden and
178:52 - uh it will have a member
178:57 - variable has tourist
179:01 - ID we will initialize this tourist ID
179:04 - inside the Constructor and inside the
179:07 - run method what we do is we iterate for
179:10 - the number of stases and during each
179:13 - iteration we do some operation so let's
179:16 - do that so for in I as zero I less than
179:22 - number of
179:23 - stases I ++ and the first thing that we
179:26 - try to do is we introduce some sort of
179:30 - sleep in the thread this is to mimic the
179:33 - activity being done by the tourist so
179:36 - let's do
179:38 - it so we will put this inside the try
179:43 - block let's have a sleep of th000
179:46 - milliseconds
179:48 - and in
179:50 - the catch
179:52 - block let's have the the interrupted
179:57 - exception let's throw it as runtime
179:59 - exception for now and uh once the
180:02 - tourist is done exploring the area
180:05 - whatever activity they are trying to do
180:07 - in this particular time frame then we
180:09 - are going to print a message the message
180:11 - could
180:13 - be
180:16 - tourist then the tourist ID
180:19 - arrives at
180:23 - stays then i+
180:26 - one so what we are trying to achieve is
180:31 - that in the given stas the tourist comes
180:35 - the tourist performs certain activity
180:38 - that is the sleep for th000 milliseconds
180:40 - essentially in 1,000 MCS they do some
180:43 - activity and then once they are done
180:47 - with their
180:48 - activity we print this message that
180:51 - tourist with so and so ID has sort of
180:55 - completed their activity and that is the
180:57 - reason that they are arriving at a
180:59 - particular stage where from we are
181:01 - supposed to move to the next stage but
181:04 - catch here is that it cannot move to the
181:06 - next stage until and unless all the
181:09 - other tourists have arrived as well and
181:12 - that is where we are going to make use
181:14 - of the cyclic
181:16 - barrier so as I mentioned mentioned in
181:18 - the theory
181:19 - section that we can call barrier. weit
181:23 - so we will do the
181:25 - same so let's put a weit
181:28 - here and it expects us to provide
181:31 - certain error handling let's do
181:37 - that and uh it's a weit not weight so
181:41 - let's correct it and when we do await
181:44 - then we will have to add another catch
181:47 - Clause as well that is broken barrier
181:49 - exception let's collapse this to one
181:51 - catch
181:54 - block and uh this should be it for the
181:57 - run method now let's go to the main and
182:00 - implement
182:01 - it so what we want to do is that for all
182:06 - the tourists that we have that is five
182:09 - tourist number of tourists I want to
182:12 - create threads for each and every
182:14 - tourist and then I want to start their
182:17 - thread
182:18 - so let's do
182:20 - that so for INT I
182:23 - as0 I less than number of
182:27 - tourists then
182:30 - i++ let's create the
182:33 - thread let's call this as tourist
182:37 - thread new thread and new tourist the
182:41 - thread ID could be I
182:46 - itself let's let's call the start on
182:49 - this one so now let's run this and see
182:52 - the
183:00 - output so here is what we are seeing
183:03 - there is a delay of 1 second and uh
183:07 - during that 1 second all the tourists
183:10 - are there doing their activity and uh
183:13 - they're arriving at stage one so once
183:17 - all the tourists have arrived at a stage
183:20 - one that is 0 1 2 3
183:22 - 4 then the tour guide starts
183:28 - speaking and uh assume that this is the
183:30 - instruction about the next stage of the
183:33 - tour right and again the similar thing
183:36 - happens wherein the tourists go to the
183:40 - second stage of the tour and they do
183:43 - their activity and then the tourists
183:45 - arrive at the stage two before
183:48 - progressing forward for the next stage
183:52 - and the same thing happens here as well
183:55 - and since we have just three stages this
183:57 - is where the tourist stops but the key
184:00 - thing to note here is that till the time
184:04 - all the tourists have not
184:06 - arrived the tour is not progressing to
184:10 - the next stage so essentially the tour
184:12 - guide or the tour group is sort of
184:16 - waiting for the time being
184:18 - until all the people in the group have
184:21 - arrived to a particular point to a
184:23 - particular checkpoint so that is what we
184:26 - have implemented here let's go through
184:28 - the code ones so first we start by
184:31 - initializing the number of tourist and
184:33 - number of stases and we have a cyclic
184:36 - barrier it's accepting two
184:39 - things one is the number of parties
184:42 - involved that is the number of tourists
184:45 - so it's going to track five tourists the
184:47 - five tourist threads and then there is a
184:50 - runable which is going to print this
184:52 - message once the barrier is getting
184:55 - tripped or barrier is getting broken
184:57 - right and uh in the main what we have
185:00 - done is we have just
185:01 - created number of tourist uh
185:04 - threads and we are calling do start on
185:07 - them and uh in the tourist class itself
185:11 - which is implementing the runable the
185:13 - key area to look here is this run method
185:17 - wherein for all the number of stages
185:20 - what we do is first we allow the tourist
185:23 - so you can visualize in this way that
185:26 - let's say that you are at the stage one
185:30 - and when you are at a stage one then you
185:33 - will be doing certain activity and like
185:36 - you the other tourists as well in the
185:39 - group they would be also doing certain
185:41 - activity and let's say everybody is
185:44 - doing certain activity in certain time
185:45 - range so this is what is being mimicked
185:48 - by this thread do sleep and once you and
185:53 - other tourists like you in the group
185:56 - they are done with their activity we
185:58 - print a message that tourist with this
186:00 - ID has arrived at these stage and then
186:04 - we call the barrier. AIT so let's say
186:07 - this is your threade which is being
186:09 - executed then your threade will call
186:12 - barrier. await and what this barrier.
186:15 - AIT will do is it will wait for all the
186:19 - other threads which are part of this
186:22 - group so basically we have created this
186:25 - thread group right and uh this threade
186:29 - is having the same cyclic barrier which
186:31 - is this particular cyclic
186:33 - barrier and this cyclic barrier is being
186:36 - used by all the threads so like you
186:41 - there could be other tourists as well
186:43 - and they would also be calling barrier
186:45 - dot AIT and essentially what what will
186:47 - happen is until and unless not everybody
186:51 - in the group that is all the five
186:53 - threads have not arrived at this
186:56 - particular position till that point of
186:58 - time the further execution will be
187:01 - halted what could be the further
187:03 - execution it could be anything so this
187:06 - is the way in which this is going to
187:08 - work and in this way we can make use of
187:11 - the cyclic barrier in a code please note
187:14 - that once the barrier do await is Reed
187:18 - and once the barrier is broken the
187:20 - barrier gets reset on its own you don't
187:23 - need to do it explicitly or manually so
187:26 - the barrier count is again uh
187:28 - initialized or reset rather with the
187:30 - initial value that you have provided
187:32 - which is five in this case so you could
187:35 - use it for the next round so that's
187:38 - about the code demonstration now let's
187:40 - learn about how does cyclic barrier work
187:43 - under the hood under the hood a cyclic
187:45 - barrier uses a combination of of a
187:47 - counter and a condition to manage the
187:50 - waiting threads when you create a cyclic
187:53 - barrier object you specify the number of
187:56 - threads that must call the await method
187:59 - before the barrier is broken each thread
188:01 - that calls the await decrements the
188:04 - internal counter of the cyclic barrier
188:07 - if the counter has not reached zero yet
188:10 - the calling thread inter a waiting state
188:12 - so let's say the calling thread is U and
188:15 - you have called barrier do a we
188:18 - but what the cyclic barrier finds out
188:21 - that this internal counter is not
188:24 - reached zero yet so let's say to begin
188:27 - with it was five and then you called it
188:30 - so it became four so it's not zero of
188:32 - course so what happens is you go into
188:35 - the waiting State and when a specific
188:37 - number of threads have called the await
188:40 - the barrier is stripped so let's say
188:42 - that after you other four threads came
188:44 - in and they also called barrier. AIT
188:47 - and eventually the value of the count
188:50 - internal count became zero so at this
188:53 - point all the waiting trads are released
188:56 - and they can proceed with their
188:58 - execution so that is where all the
189:00 - waiting threads that is youu and the
189:02 - other four threads will be released they
189:05 - will be notified that they could come
189:07 - out of the waiting State and after the
189:09 - barrier is stripped the internal counter
189:12 - is resets to its initial value and the
189:15 - barrier is ready to be used again for
189:17 - sub sequent synchronization points this
189:20 - reusability distinguishes cyclic barrier
189:23 - from the countdown latch which cannot be
189:25 - reused once the count reaches zero so
189:28 - this is pretty much about the cyclic
189:31 - barrier and the way in which we can use
189:33 - it and uh we also learned about how does
189:36 - it work internally under the hood so
189:39 - that's all about the cyclic barrier
189:49 - now let's learn about the
189:51 - exchanger in Java an exchanger refers to
189:54 - a synchronization point at which threads
189:57 - can pair and swap elements within a
190:00 - concurrent
190:01 - environment the Java util concurrent
190:03 - exchanger class facilitates this
190:06 - functionality the way this works is that
190:09 - two threads can call exchange method on
190:12 - the exchanger object passing the object
190:15 - they want to exchange why and me to use
190:19 - exchangers exchangers are useful in
190:21 - scenarios where two threads need to
190:24 - synchronize and exchange data before
190:26 - proceeding with their respective tasks
190:29 - for example imagine you have a pipeline
190:32 - of processing steps and each step is
190:35 - executed by a different trade in such a
190:38 - case you might want to use a exchanger
190:42 - to exchange data between adjacent stepes
190:44 - in the pipeline so how is it exchanger
190:47 - implemented in Java exchanger is part of
190:51 - java util concurrent package exchanger
190:53 - is a class and it does not Implement any
190:57 - interface or extend any class also
191:00 - exchanger is not implemented by any
191:02 - other class in Java it is a standalone
191:05 - class which is specifically designed for
191:08 - synchronizing the exchange of data
191:10 - between two
191:12 - threads the class has two important
191:14 - methods first is exchange which an
191:17 - object and this method is used to
191:20 - perform a blocking exchange operation it
191:23 - waits until another thread arrives at
191:25 - the exchange point and then exchanges
191:28 - the object X provided by the current
191:30 - thread with the object provided by the
191:32 - other thread it Returns the object
191:35 - provided by the other thread if the
191:37 - other thread has not arrived yet the
191:39 - current thread will block until it
191:42 - arrives the other method is the exchange
191:45 - variation it's an over loaded method
191:48 - which is going to accept an object and
191:51 - other than that it will also expect us
191:53 - to pass our timeout it's similar to the
191:56 - above one except it has a timeout
191:59 - parameter if the other thread has not
192:01 - arrived at the exchange Point within the
192:04 - specified timeout duration a timeout
192:07 - exception is thrown now let's have a
192:09 - quick code demonstration which will show
192:12 - us how we can use exchanger in our code
192:16 - so here we are in the concurrent
192:17 - collection package let's create the
192:19 - class let's call this as exchanger demo
192:23 - so what we are going to implement is two
192:25 - threads let's call them as first threade
192:27 - and second threade and then they will
192:30 - have exchanger at their disposal using
192:33 - which these two trades will be
192:35 - exchanging certain information with each
192:37 - other so let's start by implementing the
192:40 - first threade let's call this as class
192:43 - first
192:45 - threade implements
192:51 - runable so let's implement the run
192:59 - method it will have the
193:05 - exchanger and uh it's a generic class so
193:08 - we will have to pass the type let's say
193:11 - we pass the type as
193:14 - integer let's name this as exchanger
193:17 - let's initialize this exchanger inside
193:19 - the
193:21 - Constructor and in the run
193:24 - method let's have a data to send
193:28 - variable with a value
193:30 - 10 and uh we will print this
193:33 - value
193:36 - so first trade is sending
193:44 - data data to send
193:47 - and uh now let's call exchanger do
193:51 - exchange and pass the data to send value
193:54 - this will return a value and uh let's
193:57 - capture that as part of a
194:01 - variable which we could call as received
194:05 - data and exchange will expect us to
194:08 - handle some
194:09 - exceptions let's surround this with TR
194:13 - catch and once we have received the data
194:16 - let's print it out on the
194:18 - console so first
194:22 - creade
194:23 - received
194:25 - data now let's implement the second
194:28 - thread as well and once we are done with
194:30 - the implementation we will see how it's
194:33 - interacting with each
194:34 - other so let's call this
194:38 - as second trade
194:40 - implements runable
194:52 - let's have the exchanger here as
194:55 - well and we will pass the integer as the
194:59 - generic
195:00 - type initialize this particular
195:03 - extension in the
195:06 - Constructor and in the run method let's
195:09 - start by having a sleep of let's say 3
195:14 - seconds and here as well we have a data
195:17 - to send variable with a value
195:21 - 20 you will print some sort of
195:25 - message
195:28 - second thread is sending
195:32 - data and let's say data to
195:35 - send and then exchanger do
195:39 - exchange let's pass the data to send
195:42 - let's capture this as part
195:44 - of received
195:48 - data and let's print this on the
195:51 - console so
195:54 - second
195:57 - trade
195:59 - received and received
196:02 - data let's handle these
196:07 - exceptions I
196:11 - catch and what we can do is we can also
196:15 - capture this and place inside the tri
196:19 - block so we should be good with the
196:21 - implementation of the second
196:24 - threade now let's implement the demo
196:28 - class and in order to do so let's have
196:31 - the main
196:32 - method in the main method let's have the
196:38 - exchanger new
196:41 - exchanger and uh we will be creating two
196:45 - threads so first one new
196:49 - threade and uh first threade let's pass
196:53 - the exchanger because the thread is
196:55 - going to expect us to pass the exchanger
196:58 - object let's do the same for the other
197:00 - thread as well let's call this as
197:03 - two we will start these two so 1 do
197:07 - start two do start so this should be
197:11 - second
197:13 - threade now we will run this and see the
197:15 - output
197:18 - so we can see that we are seeing first
197:21 - thread is sending data and second thread
197:24 - is sending
197:25 - data and second threade received 10 that
197:28 - the first thread is sending and the
197:30 - first threade received 20 that is the
197:32 - second threade sending but there is a
197:35 - key information that we must notice
197:38 - please note that in the first threade we
197:41 - are sending the data right away wherein
197:44 - we are calling the exchanger do Exchange
197:47 - but in order for this exchanger do
197:49 - exchange to be
197:51 - executed the other thread should be able
197:54 - to receive it that means as long as the
197:58 - other thread is not calling its
197:59 - exchanger do exchange then in that case
198:02 - this threade will be blocked and what we
198:05 - are seeing in the second threade is that
198:08 - before calling exchanger do exchange we
198:11 - are putting a sleep of 3 seconds so that
198:15 - is the reason we are observing a delay
198:18 - because threade one here is blocked for
198:21 - 3 seconds
198:22 - because when the execution comes inside
198:25 - the second trade it's sleeping for 3
198:28 - seconds before calling the exchanger do
198:31 - exchange and once the execution comes
198:34 - here then it means that thread one will
198:38 - be sending its data that is data to send
198:41 - 10 and uh that will be received by the
198:45 - second trade and captured in the
198:47 - received data and likewise exchanger do
198:51 - exchange calls this data to send and
198:54 - this data to send value that is 20 will
198:57 - be received by the first threade which
199:00 - is captured by this received data
199:03 - variable and the same will be
199:05 - printed with this value that is First
199:08 - Rate received received data so let's run
199:11 - it one more time to observe the blocking
199:14 - nature of the exchange method so so we
199:17 - can see that first thread is sending
199:19 - data and there is a delay before all of
199:21 - these things are
199:22 - executed so we called First Trade is
199:26 - sending data then exchanger do exchange
199:28 - was called and because the second
199:32 - threade went into the sleep for that
199:34 - amount of time that is 3 seconds the
199:37 - first threade got blocked and as soon as
199:39 - the second threade is active The
199:42 - Exchange happens and the first threade
199:44 - receives the data to send value from the
199:47 - second threade which is 20 and likewise
199:50 - the second threade is receiving the data
199:53 - from the first threade this is a data
199:55 - send value of 10 so this is it for the
200:00 - code demo I hope it would have somewhat
200:02 - cleared your doubts with respect to how
200:05 - exactly can we use
200:06 - exchanger now back in the theory section
200:09 - let's do a quick comparison between the
200:12 - queue and the
200:13 - exchanger so you may feel that a queue
200:17 - could achieve similar kind of
200:19 - synchronization between the threads
200:20 - right and yes you are partially correct
200:23 - in thinking so however the choice
200:26 - between using an exchanger and a queue
200:30 - depends on specific requirements of the
200:32 - synchronization pattern which you are
200:34 - trying to implement and the
200:37 - characteristics of your
200:39 - application so here is a quick
200:41 - comparison the exchanger facilitates a
200:44 - direct exchange of data between two
200:46 - threads at a synchronization Point each
200:49 - threade Waits until both the threads has
200:52 - arrived at the exchange point before
200:55 - proceeding if you have exactly two
200:57 - threads that needs to synchronize and
200:59 - exchange data the exchanger provides a
201:02 - simple and efficient mechanism to
201:04 - accomplish this exchanges occur
201:07 - synchronously meaning both the trades
201:10 - wait for each other at the exchange
201:12 - Point both traes exchange data of the
201:14 - same type and quantity making it
201:17 - suitable for symmetric communication
201:20 - patterns the que on the other hand
201:22 - however is different from the exchanger
201:25 - in these
201:26 - parameters so the first is that cues are
201:29 - more versatile and could handle
201:31 - communication between multiple producer
201:33 - and consumer threats and depending on
201:36 - the Queue implementation producers and
201:38 - consumers May operate
201:40 - asynchronously producers can inq data
201:43 - without waiting for consumers and
201:46 - consumers can theq data without waiting
201:49 - for the producers qes can act as buffers
201:53 - allowing producers to continue producing
201:55 - data even if consumers are not
201:57 - immediately available to process it and
202:00 - in some cases The Exchange may not be
202:03 - symmetric however the exchanger is very
202:06 - similar to a type of q that we saw
202:09 - earlier and that type of queue is called
202:11 - as synchronous queue so both synchronous
202:15 - queue and ex change of facilitate
202:17 - blocking synchronization between threads
202:20 - however while a synchronous CU is
202:23 - unidirectional the exchanger happens to
202:25 - be bidirectional so with the use of
202:27 - exchanger you could achieve a
202:29 - bidirectional data exchange between two
202:31 - threads which means when you do call do
202:35 - exchange data at one hand you are
202:38 - sending certain value but you are also
202:41 - receiving certain value from the second
202:43 - thread as well so that is the B
202:45 - directional nature of the exchanger so
202:48 - that should be it for the topic of
202:54 - exchanger now let's learn about the copy
202:57 - on WR array copy on WR array is like
203:00 - having your own copy of book to read
203:02 - when someone wants to write in the book
203:04 - instead of disturbing your reading they
203:06 - make a new copy this way you can keep on
203:09 - reading without any interruptions you
203:12 - can use copy on right array when you
203:14 - have multiple threads accessing and
203:15 - modifying data at the same time it
203:18 - ensures that readers don't get disturbed
203:21 - by writers and writers don't interfere
203:24 - with each other making your program
203:26 - safer and more efficient let's have a
203:28 - code demonstration which will tell us
203:31 - how we can use copy on right array in
203:33 - our Java code we are in the concurrent
203:35 - collection let's create a class and
203:38 - let's call that as copy on WR array
203:42 - demo so what we are going to implement
203:45 - is two threads one is the read task and
203:49 - the other one is the right task and as
203:51 - the name suggests that read task is
203:54 - going to read something and WR task is
203:57 - going to write to something and what
204:00 - it's going to read and write from is the
204:02 - copy on right array so let's start the
204:06 - implementation let's create the read
204:11 - task and this will Implement a runnable
204:14 - interface
204:16 - let's implement the run
204:20 - method and uh let's have a list
204:24 - of integers let's call this as list the
204:28 - type would be integer let's name this
204:31 - object as list and we can initialize
204:34 - this using a
204:36 - Constructor and what we want to do
204:38 - inside a y Loop is we want to put some
204:41 - sort of uh sleep to the thread and then
204:44 - print the list so that is is going to
204:46 - mimic the infinite reading operation so
204:50 - let's do that so while
204:54 - [Music]
204:55 - true let's call the thread.
204:58 - Sleep and maybe it could be for one
205:03 - second let's do the exception
205:08 - handling and uh once it's done sleeping
205:13 - we will be printing the
205:15 - list so that's about the read
205:18 - task now let's create the right
205:22 - task and it will implement or
205:26 - unable let's implement the run
205:31 - method and this as well is going to
205:34 - expect us to pass a integer so let's do
205:38 - that and uh let's have a random
205:43 - [Music]
205:45 - generator and and uh we could have a
205:51 - Constructor for the random we could just
205:55 - say new.
205:58 - random and in the run method we are
206:01 - going to do two things infinitely first
206:04 - is we will wait for certain time second
206:07 - is we will be writing certain value to
206:09 - the list so let's do that so let's have
206:14 - the while loop while
206:18 - true trade.
206:21 - sleep let's have this for 1200
206:25 - milliseconds let's handle the
206:30 - exceptions and outside the TR catch
206:33 - block let's call list. set random. next
206:39 - integer and it's going to be in the size
206:43 - of list
206:47 - and then random do next int and the
206:50 - upper limit is going to be
206:52 - 10 so what we are doing essentially is
206:55 - that we have a list and in that list we
206:58 - are picking any random index and on that
207:00 - random index we are setting any random
207:03 - value that is in the range of 0 to 10 so
207:06 - that's about the right task now let's
207:09 - write another class that is going to
207:11 - mimic the simulation that we are trying
207:13 - to do so let's call that as
207:17 - simulation let's have the list let's
207:20 - call this as private
207:23 - final list
207:26 - integer
207:28 - list and uh let's create the
207:31 - Constructor so what we want to pass
207:34 - instead is new copy on right array
207:39 - list and uh let's remove this one
207:42 - because this is not needed
207:44 - now and and
207:47 - uh to this list I want to add certain
207:51 - values so
207:53 - array do
207:56 - aslist and uh let's pass some
208:03 - zeros all right so we have basically
208:07 - instantiated a list made that as a type
208:11 - of copy on right array and in that
208:13 - particular copy on WR array we have
208:15 - provided some values which is all zeros
208:19 - now let's have a method call that as
208:25 - simulate and what this thing is going to
208:27 - do is it's going to create certain
208:30 - threads so first could be new right task
208:33 - let's pass the list let's duplicate
208:38 - this this one could be
208:41 - two this one could be
208:43 - three and this one could be four
208:48 - this as well could be a right task this
208:50 - as well could be a right task and let's
208:52 - have this one as a read
208:55 - task so basically we have three writer
208:58 - task and one reader task finally let's
209:02 - start
209:06 - these 3 do
209:08 - start 4 dot start and in the main class
209:15 - let's have the main method to begin with
209:17 - and uh we will create the object for
209:21 - simulation
209:22 - simulation new
209:25 - simulation and then let's call
209:27 - simulation. simulate so let's go through
209:31 - the code once before we run this so what
209:33 - we have done effectively is we have
209:35 - created four threads three out of which
209:38 - are the wrer threads and one is a reader
209:42 - thread what the writer thread is going
209:44 - to do is is it's going to run infinitely
209:48 - and uh every 1.2 seconds it's going to
209:53 - set certain value in the list that we
209:56 - are going to provide which is a
209:58 - specifically the type of copy on WR
210:00 - array and the way it's going to write is
210:03 - it's going to pick any random index and
210:05 - going to place any random value and uh
210:09 - what the read task thre is going to do
210:11 - is it's going to execute infinitely it's
210:14 - going to run infinitely
210:16 - and in every run it's going to sleep for
210:20 - 1 second and then print the list and
210:23 - finally we are running all these trades
210:26 - and uh this is basically we are creating
210:29 - the object and calling the simulate
210:31 - method which is going to do all of these
210:34 - things so now let's run this and see the
210:40 - output so you could see that to begin
210:43 - with we have this array then the writer
210:47 - thread wrote something and then uh that
210:50 - is being read by the reader thread so it
210:53 - keeps on happening that the writer
210:55 - thread will be writing something to the
210:56 - array that is pretty much random and
210:59 - then the reader thread will be reading
211:01 - from the array the point to note here is
211:04 - that this is being done in a
211:06 - multi-threaded context and we are not
211:09 - leading to any exceptions for example
211:12 - any modification exception or any kind
211:14 - of uh incon consistency so situations
211:18 - like this when we should have copy on
211:21 - right array list and uh this particular
211:23 - data structure will help us in ensuring
211:26 - that an array list is being used in a
211:29 - multi-threaded environment without any
211:32 - of the synchronization or concurrency
211:34 - concerns now let's learn a few more
211:37 - things about the copy on right array so
211:40 - now we will learn how it works so when a
211:43 - thread wants to read from the array it
211:45 - creates a snapshot of the array and uses
211:48 - the same to read if a thread wants to
211:51 - write it also creates a snapshot of the
211:53 - array and performs the right operation
211:56 - once the right is completed the changed
211:58 - array is considered as the latest
212:00 - version of the array from which
212:02 - snapshots could be created for the read
212:05 - and write operation this way readers
212:08 - don't see the changes until they get a
212:10 - new snapshot it ensures that they always
212:14 - have a consistent view of of the data so
212:17 - we could Loosely draw panels between the
212:20 - git branching approach and the way in
212:22 - which copy on WR array Works each
212:24 - modification to the array creates a new
212:27 - version much like creating a new branch
212:29 - in the git just as git would allow for
212:33 - parallel development without conflict
212:35 - copy and right array allows for multiple
212:38 - threads to modify the data
212:40 - concurrently and it does so without
212:42 - interfering with each other and just
212:45 - just as git consolidates changes during
212:48 - the merging copy on wrer eventually
212:51 - consolidates the modifications into a
212:53 - single reference version this version
212:56 - ensures data consistency both systems
212:59 - provide a structured approach to
213:01 - managing concurrent changes so I think
213:04 - it would make sense to visualize what we
213:07 - just discussed so let's do the
213:13 - same so let's assume that this is our
213:16 - array and it has certain values as in
213:20 - 3512 and the first call on this array is
213:23 - a read call imagine this line to be the
213:27 - reference line and consider that this
213:30 - line has the latest value of the array
213:33 - so whenever a read or a write operation
213:35 - is supposed to be done those operations
213:38 - are going to create certain snapshots so
213:41 - the first call that we are getting on
213:43 - this array is a read call so as soon as
213:46 - we have a read call a snapshot of this
213:49 - thing is created and the read will be
213:52 - performed on this one so what
213:54 - effectively happens is that even though
213:57 - if there is any right operation that is
213:59 - going to happen it won't impact this
214:02 - value because we have taken a snapshot
214:04 - of this particular array at this
214:06 - particular time since this line is the
214:09 - source of Truth during creating a
214:11 - snapshot the same value remains to be
214:14 - seen here and this is the array from
214:17 - which the read operation will be served
214:20 - now let's say going forward at this
214:23 - point of time a write operation is
214:25 - invoked what it says is that write a
214:28 - value four at index 2 so since this
214:32 - latest value or the reference value has
214:35 - this array that is 3512 a snapshot is
214:38 - created which is
214:40 - 3512 and on this snapshot the right
214:44 - operation is applied so as soon as the
214:46 - right operation is applied the array
214:48 - changes and the value becomes 3 5 4 2
214:53 - and the same value would be written to
214:56 - this source of Truth or this line or
214:59 - this reference right so let's say we
215:01 - have a read operation now and this read
215:04 - operation comes after the update has
215:06 - been written so since this reference has
215:10 - this value of
215:12 - 3542 the read operation is again going
215:14 - to create a snapshot and now the
215:17 - snapshot will contain this value
215:20 - 3542 so when the read operation is
215:22 - served the same value is returned that
215:24 - is
215:25 - 3542 and this process keeps on happening
215:29 - so you see what we have is our reference
215:31 - value to begin with and a line which is
215:34 - serving as a source of Truth and
215:37 - whenever we are going to have any read
215:40 - operation we are creating a snapshot and
215:42 - whenever we are having a WR operation we
215:45 - are creating a snapshot and after that
215:47 - right operation is complete then the
215:50 - same is merged back to the main line so
215:52 - that is the reason I told that it is
215:54 - somewhat similar to the way in which G
215:56 - branching works so you could consider
215:59 - this as the master branch and uh read is
216:02 - anywh readed it's not going to write
216:04 - anything or impact the data but still we
216:08 - want to avoid a situation where in the
216:10 - master Branch could change during the
216:12 - read is being performed so that is the
216:14 - reason the snap is being created to
216:16 - serve the read as well but it's very
216:18 - evident that in the right scenario uh
216:21 - another branch is created and on that
216:24 - Branch whatever rights are supposed to
216:26 - be done those rights are done the four
216:29 - in this case at index two and once this
216:32 - is done once it is successful the same
216:34 - thing is written back to the main line
216:38 - to the main branch and once this thing
216:40 - is merged now the main branch has this
216:43 - particular value that is 35 42 so this
216:46 - is the way in which you could visualize
216:49 - the right on copy array and I hope that
216:52 - it will help you in understanding this
216:54 - concept in a much better
217:01 - way lcks are an important topic in Java
217:05 - so let's learn about the locks what
217:07 - exactly are locks well imagine you have
217:09 - multiple threads running simultaneously
217:11 - in your Java program and all are trying
217:13 - to access the same resource without
217:16 - proper synchronization it could lead to
217:18 - inconsistency in data and other chaotic
217:21 - situations that's where locks come in
217:23 - handy locks provide a way to control
217:26 - access to Shared resources ensuring that
217:28 - only one thread can access the resource
217:31 - at a given point of time thus it helps
217:33 - in preventing data corruption and other
217:36 - concurrency issues you may wonder that
217:38 - it's sounding very similar to what the
217:40 - synchronized blocks do well you are
217:42 - right in thinking so so now let's learn
217:44 - the difference between using
217:46 - synchronized blocks versus the locks in
217:49 - Java when it comes to managing
217:50 - concurrent access to Shared resources
217:52 - two commonly used mechanisms are
217:54 - available these are synchronized blocks
217:57 - and locks let's start with the
217:59 - synchronized blocks synchronized blocks
218:01 - use the synchronized keyword to ensure
218:04 - that only one thread can execute a
218:06 - particular section of code at a given
218:08 - point of time they provide intrinsic
218:10 - locking which means that the lock
218:12 - associated with the object is acquired
218:14 - and released automatically by the jvm
218:17 - synchronized blocks are easy to use and
218:19 - require less boiler plate code compared
218:21 - to the locks however they have
218:24 - limitations such as lack of flexibility
218:26 - and lock acquisition and inability to
218:28 - handle the interrupts on the other hand
218:31 - locks provide more flexibility and
218:33 - control over locking mechanisms Java
218:36 - lock interface and its implementations
218:39 - allow to manually acquire and release
218:41 - the locks moreover you could acquire and
218:44 - release the locks blocks in any sequence
218:47 - and in any scope which is not possible
218:50 - if you use synchronized approach so when
218:53 - you should use synchronized blocks
218:55 - versus locks use synchronized blocks for
218:58 - simple synchronization needs where
219:00 - flexibility and performance are not that
219:02 - critical use logs for complex
219:04 - synchronization scenarios where fine
219:06 - grain control and flexibility are
219:09 - required in conclusion both the
219:12 - synchronized blocks and locks are
219:13 - essential tools for managing concurrent
219:15 - access to a shared resource in Java
219:18 - understanding their differences and
219:20 - choosing the right synchronization
219:22 - mechanisms for your specific
219:23 - requirements is a crucial part of
219:26 - writing a robust stand efficient
219:27 - concurrent code so now it's time to
219:31 - learn how can we Implement and make use
219:33 - of locks in our code while doing so we
219:35 - will also learn about a couple of
219:37 - important concepts related to Locks such
219:39 - as conditions different types of logs
219:42 - Etc
219:50 - till now we have learned that locks are
219:53 - an important part of implementing the
219:55 - concurrency in Java but we also need to
219:58 - have some sort of interaction between
220:00 - the threads and locks so the mechanism
220:03 - which helps us in managing these
220:05 - interactions is called as condition
220:07 - let's understand what are conditions
220:09 - with a very simple and easy to
220:11 - understand explanation here I would
220:13 - request you to make a very weird
220:15 - assumption which could help in avoiding
220:17 - any confusion with the terminology of
220:19 - lock assume that lock here is the key as
220:23 - well what I mean to say is when you have
220:26 - access to the lock the door which is
220:28 - guarding the protected resource is
220:30 - unlocked magically and thus the lock is
220:33 - a key as well in that context hence as
220:36 - soon as you acquire the lock you get
220:38 - access to the resource please note that
220:41 - only one person is allowed to access the
220:44 - resource no one else can tailgate behind
220:47 - you with this in place let's learn
220:50 - further a lock in Java can have its own
220:52 - set of rules these rules are called as
220:56 - conditions effectively this means that a
220:59 - lock could have more than one condition
221:01 - associated with it conditions help us in
221:04 - controlling how threads interact with
221:06 - the lock you could visualize threads as
221:09 - people who are trying to access some
221:11 - protected resource and think of
221:14 - condition as a way waiting room attached
221:16 - to the lock imagine a person comes up
221:19 - and tries to get hold of the lock but
221:22 - lock is being held by some other person
221:25 - thus this guy won't be able to acquire
221:28 - the lock so what he does instead is he
221:31 - goes to wait in this room where other
221:33 - people may also be waiting for the lock
221:36 - to be available inside the waiting room
221:38 - people wait until someone signals that
221:41 - it's their turn to use the lock now this
221:44 - signal could would come from a person
221:46 - that is currently holding the log or
221:49 - from some other source so who gets the
221:52 - chance to acquire the log well that's
221:55 - something which depends on the
221:57 - implementation the key takeaway from
221:59 - here is that after a signal is given any
222:02 - one person from the waiting room is
222:04 - allowed to acquire the lock and do their
222:07 - processing in Java these conditions help
222:11 - in managing the interaction between
222:13 - threads and locks now let's visualize
222:16 - how conditions
222:19 - work imagine there are two threads
222:22 - thread one and thread two to begin with
222:24 - threade one is executing something
222:27 - however it cannot move forward until
222:30 - some particular condition is met this
222:32 - condition could be anything based on
222:35 - your use case for example a que being
222:38 - full could be a condition in this case
222:41 - as soon as the thread cannot move
222:42 - forward it will call condition do a wait
222:46 - if you read this from the right it
222:48 - appears as if a threade is saying that I
222:51 - am waiting for this condition to be
222:53 - fulfilled so I am awaiting for a given
222:56 - condition effectively the thread one
222:59 - goes to a waiting State now let's
223:02 - imagine there is another thread thread 2
223:04 - and this thread is running and doing
223:06 - some operations due to the operations
223:08 - being performed the condition gets
223:11 - fulfilled and as soon as the condition
223:13 - gets fulfilled this trade to calls
223:16 - condition do signal it signifies that
223:18 - the condition has been fulfilled so
223:20 - convey this message to any of the
223:23 - threads which have been waiting for this
223:25 - condition to get fulfilled as soon as
223:27 - the signaling happens jbm finds all the
223:30 - threads which are in the weight State
223:32 - anticipating this condition to be
223:34 - fulfilled and wake the one which has
223:37 - been waiting the longest in this example
223:40 - there is only one thread which is thread
223:42 - one so the state for thread one goes
223:44 - from bit state to the renable state so
223:47 - the threade one which was blocked at
223:49 - this point of time is now active from
223:52 - this point of time because at this point
223:54 - of time threade 2 signals that the
223:56 - condition for which threade one was
223:58 - waiting it has now been fulfilled so
224:01 - thre one could proceed with its
224:03 - processing now let's analize this
224:06 - scenario where we call condition do
224:09 - signal all like we had condition. signal
224:12 - there is an another method signal all so
224:15 - in this example we have three threads
224:18 - and during its execution threade one
224:20 - calls do of wa and after some time
224:24 - threade two also calls doit there is
224:27 - another threade which is threade three
224:29 - and during its execution the condition
224:31 - for which thread 1 and thread 2 are
224:34 - waiting gets fulfilled so the thread 3
224:37 - calls condition do signal all as soon as
224:40 - signal all is called all the threads
224:42 - which are waiting for the condition to
224:44 - be fulfilled build are waked up by the
224:47 - jvm jvm pulls them out of the blocked
224:50 - State and moves them to the runnable
224:51 - state so depending on the number of
224:53 - course your CPU has threads could be
224:56 - scheduled for Execution accordingly so
224:58 - if dot signal was called instead of
225:01 - signal all thread one would have been
225:03 - waken up since it's the one which has
225:06 - been waiting for the longest so this is
225:08 - the way in which condition helps us into
225:11 - maintaining or managing the interaction
225:13 - between the threads and the locks there
225:16 - are two methods that is signal and
225:18 - Signal all which are used to notify that
225:21 - a particular condition has been
225:23 - fulfilled and there is one method which
225:25 - is condition. which is used to hint that
225:29 - a condition needs to be fulfilled before
225:31 - a given thread could proceed further in
225:33 - its
225:36 - execution in case if you are wondering
225:39 - that condition methods discussed above
225:41 - are very similar to the weight and
225:43 - notify methods in the object class you
225:45 - are correct indeed these methods are
225:47 - similar to wait and notify the main
225:50 - difference however is that conditions
225:52 - provide a much granular control over the
225:54 - synchronization of the protected
225:56 - resource now with that in place let's
225:59 - have a quick code demonstration which
226:01 - will teach us how can we use the
226:03 - conditions and its related methods let's
226:06 - create a package and we can call this as
226:10 - logs and uh let's create a class call
226:14 - this as
226:15 - condition
226:17 - demo and here is what we are going to
226:20 - build so basically we are going to build
226:22 - a producer consumer implementation using
226:25 - the locks and conditions so let's start
226:29 - so first thing that we will have is a
226:32 - Max size which is the number of elements
226:35 - that could be kept in a buffer which is
226:37 - going to be a q so let's have that so it
226:40 - would be private
226:42 - final integer Max X size let's have the
226:47 - value as
226:49 - five the next thing is we will Define
226:52 - the lock so lock and the type of lock
226:55 - that we are going to use is reint lock
226:59 - so reint lock is one type of lock in
227:02 - Java we will have to create the Q or the
227:06 - buffer where we are going to place the
227:09 - elements so let's create the que let's
227:13 - call this as
227:15 - buffer and uh this could be of type link
227:19 - list so basically from this buffer the
227:22 - consumer will be consuming and the
227:24 - producer will be writing to it next we
227:27 - will be creating certain conditions one
227:29 - condition would be that buffer not full
227:32 - and the other condition is buffer not
227:33 - empty so let's create the same so
227:37 - private
227:38 - final this is condition and uh buffer
227:43 - not full
227:45 - lock the way to create the condition is
227:47 - to call the new condition method and we
227:51 - will do the same for the other condition
227:53 - as well so this condition could be
227:56 - something like uh buffer not empty and
228:00 - we could have log. new condition so we
228:04 - will have two methods the first one
228:07 - is producer so private void produce and
228:13 - in item
228:17 - so let's have this and uh so what we
228:21 - will be implementing is the produce
228:22 - section of the implementation and uh we
228:25 - want this to be synchronized while this
228:27 - particular code block is getting
228:29 - executed we don't to want something else
228:32 - or let's say some other thread to come
228:34 - and work on this
228:36 - simultaneously so the order to do this
228:39 - is first we will have to acquire the
228:41 - loog and the way to do this in Java is
228:44 - called log. talk and once we are done
228:47 - with our uh processing finally we need
228:50 - to call log.
228:53 - unlock so this looks fine but there is a
228:56 - problem let's say when the processing is
228:58 - under the way there is some exception
229:01 - which happened so in that case the code
229:04 - will terminate then and there itself
229:06 - right the lock. unlock will be never
229:09 - executed and the other threads which are
229:11 - waiting for this lock will go into a
229:13 - state of Deadlock lock and that is not
229:16 - good so the way to do this is call this
229:20 - lock. unlock in the finally section or
229:22 - the finally block so in either case this
229:25 - lock. unlock is always executed so this
229:29 - is a common pattern which we use while
229:31 - making use of the locks in Java and this
229:34 - is the way to implement this so let's
229:37 - have the tri block and here we will put
229:40 - the processing right and we will have a
229:44 - finally block and that is where we call
229:47 - the lock do
229:49 - unlock so with this in place let's
229:52 - implement the processing
229:56 - part so here is what we are going to
229:58 - implement now first we will check if the
230:00 - buffer is totally full then for that
230:03 - duration we will have to make the
230:05 - producer thread weight otherwise we will
230:08 - put some item in the queue and then we
230:11 - will notify if there is any consumer
230:13 - thread which which is waiting for the
230:15 - producer to put certain item in the
230:17 - queue so let's implement
230:20 - this so we start by
230:23 - checking if the buffer size is equal to
230:27 - the max size which is five if that is
230:31 - the case then we have buffer not full
230:35 - and on this condition this threade will
230:38 - wait after that we will add certain item
230:42 - to the buffer
230:45 - and next we will have a print message
230:48 - which say something like
230:52 - produced and uh produce this particular
230:55 - item and for the buffer not empty we are
230:59 - going to put the signal so let's also
231:02 - complete the consumer method then we can
231:04 - have a complete look over the entire
231:07 - implementation so let's have private
231:11 - void
231:12 - consume and like the produce method it's
231:16 - also going to throw an interrupted
231:17 - exception so let's have that in the
231:19 - method
231:20 - signature we start by acquiring the lock
231:24 - so lock. lock and we will follow the
231:27 - same pattern that we saw earlier that we
231:30 - will have a try and finally
231:32 - block and in the finally we call the
231:35 - unlock method and in the try we have the
231:39 - implementation so here we will check if
231:42 - the buffer is empty
231:45 - and if it's empty then we call Buffer
231:48 - not empty and await on this condition
231:53 - and if this is not the case then we are
231:56 - going to consume from the buffer and the
231:58 - way we do this is we print a message
232:01 - which says
232:03 - consumed and uh from the
232:06 - buffer we take out the element by
232:09 - calling the do pole and next we call a
232:14 - signal on the buffer not
232:17 - full so this is the consumer method so
232:20 - you may have a doubt here what you may
232:23 - think is that the nomenclature of buffer
232:25 - not full and buffer not empty is kind of
232:28 - misleading and confusing so I totally
232:31 - understand that and I also had this
232:33 - confusion for quite some time but the
232:36 - way to interpret this is that this
232:39 - nomenclature should be understood from
232:42 - the perspective of this signal so so the
232:45 - condition is buffer not empty so what it
232:47 - means is that this buffer not empty
232:50 - condition is being awaited by certain
232:54 - threade and which threade may await this
232:57 - the threade which may await this is this
232:59 - one right that is going to consume so if
233:02 - it's consuming then at that point of
233:04 - time it needs to have something in the
233:07 - buffer otherwise there is nothing to
233:08 - consume so that is the reason if the
233:11 - buffer is empty then on this particular
233:13 - condition this consume method is going
233:17 - to wait and let's say if it's waiting
233:20 - for something to be put in the buffer
233:23 - and then the producer method is executed
233:26 - by some thread the producer thread then
233:29 - at that point of time something was put
233:31 - in the buffer some item was put in the
233:33 - buffer and that is where once we have
233:35 - put something in the buffer we are going
233:38 - to call this signal so what essentially
233:40 - we are saying is that buffer not empty
233:43 - condition is is going to signify that
233:46 - something needs to be there in the queue
233:49 - for the consumer to consume and that is
233:51 - the reason consumer is going to wait on
233:54 - this condition that is buffer not empty
233:57 - likewise we have buffer not full so
234:00 - buffer not full is being signaled by the
234:02 - consumer and the idea Remains the Same
234:05 - right so when producer is producing
234:08 - something then it needs to ensure that
234:10 - the buffer is not full and if it's full
234:13 - then it will have to it and this
234:15 - condition is being signaled by the
234:17 - consumer and what it means is once the
234:20 - consumer has consumed something then
234:23 - there is certain space in the queue in
234:25 - the buffer in which the producer could
234:28 - produce so that is the reason if there
234:30 - is a producer which is waiting for this
234:32 - condition then this particular thread
234:35 - the producer thread will be awakened
234:38 - when this buffer not full is signaled so
234:41 - that is the idea and that is the
234:43 - intention behind this nomenclature and
234:45 - behind this implementation now let's
234:47 - complete this by writing the main method
234:50 - and then we will have a code
234:51 - demonstration by running the code so
234:54 - let's implement the main method and
234:58 - uh let's have the condition demo object
235:02 - created so new condition demo right and
235:08 - uh let's have the producer thread let's
235:11 - call this as
235:13 - producer
235:15 - trade and uh let's have a triy block and
235:21 - have a catch block as well so when we
235:24 - call producer it's going to throw that
235:26 - interrupt exception so that is the
235:28 - reason we need to have it and we are
235:31 - also going to introduce some sort of
235:33 - sleep so that will be handled by the
235:35 - catch block itself so what we are going
235:39 - to do is we are going to run this
235:42 - producer threade for
235:45 - times and
235:49 - uh we call the demo do produce and pass
235:53 - I in this one so I is the item that
235:57 - needs to be put so essentially this
235:59 - producer threade is going to put 10
236:02 - items in the Que in the
236:05 - buffer next let's have the
236:09 - uh thread dot sleep maybe for one second
236:15 - and in the catch we have uh a runtime
236:19 - exception which is this
236:21 - e
236:23 - so this should be for the producer
236:28 - thread and uh for the consumer thread
236:32 - let's have another
236:33 - one first let me rename this one to
236:38 - Consumer threade and uh again we will
236:42 - have the similar pattern we will have
236:44 - have a try and
236:46 - catch let's have the interruped
236:49 - exception and uh let's through the
236:52 - runtime
236:54 - exception and what we do is we are going
236:58 - to call consume 10
237:01 - times so I less than 10 and demo do
237:07 - consume and we are going to put a sleep
237:10 - for let's say 2 seconds
237:14 - and once we have this threade created we
237:18 - will have to call the dot run on these
237:20 - two threads so first is producer thread.
237:24 - start then consumer thread. start so
237:28 - that should be it from the entire
237:29 - implementation perspective now let's run
237:32 - this and see the
237:38 - output so you see we have a
237:41 - demonstration ass simulation of the
237:43 - producer cons a problem so produced 0
237:47 - consumed 0 produced One consumed one and
237:51 - uh produced two produced three so
237:54 - remember we had put a sleep timer of 1
237:57 - second in the produce method and 2
238:00 - second in the consume method so in that
238:02 - sense consume method is on the slower
238:04 - side and that is the reason here we have
238:06 - uh the producer being producing
238:08 - something twice and consumer is kind of
238:12 - on the slower side so we are producing
238:13 - twice and consumer is going to consume
238:15 - something once right but this pattern is
238:17 - not something which is very
238:19 - deterministic the entire idea behind
238:21 - having that uh sleep was just to
238:23 - showcase the
238:24 - demonstration uh in somewhat presentable
238:27 - manner right so that should be it for
238:30 - the conditions in Java and uh please
238:34 - note that these conditions could be only
238:37 - derived from the lock interface
238:40 - implementations which are reint lock
238:42 - read right lock and any other
238:44 - implementations which you could explore
238:46 - in the jdk and uh these conditions are
238:49 - not available for the logs that we have
238:52 - on the object level the implicit locks
238:55 - which are available with the object in
239:02 - Java now let's learn about the reant
239:05 - locks reent lock is one of the many
239:08 - implementations of lock interface in
239:10 - Java it allows a threade to acquire the
239:13 - same lock multiple times without causing
239:15 - any Deadlocks what this means is that a
239:19 - threade which holds a lock can acquire
239:21 - it again without blocking itself if the
239:24 - lock is
239:25 - reentrant in contrast with a non
239:28 - reentrant lock an attempt to acquire the
239:31 - lock again within the same thread
239:33 - without reducing it first would
239:35 - typically result in blocking it could
239:38 - also potentially cause a deadlock
239:40 - situation if it's not handled properly
239:43 - how does this this work the way this
239:45 - works is that the locking mechanism
239:48 - keeps track of the lock being held by a
239:51 - thread in the case of non-intent locks
239:54 - attempting to acquire the lock again
239:56 - within the same thread typically results
239:59 - in blocking the thread however in the
240:02 - case of reinen lock the lock mechanism
240:05 - allows the same thread to acquire the
240:07 - lock multiple times without blocking
240:09 - itself effectively the held count for
240:13 - the lock by the given thread is
240:15 - incremented since the lock is already
240:17 - acquired there is no sense of
240:20 - reacquiring it this is achieved by
240:23 - maintaining a count of how many times
240:25 - the lock has been acquired by the same
240:27 - thread the lock is only released when
240:30 - the count reaches zero it indicates that
240:33 - the threade has released the lock the
240:35 - same number of times it has acquired it
240:38 - you may be wondering why and in what
240:40 - situations a threade would need to
240:42 - acquire a lock again without releasing
240:44 - the log here is the answer imagine a
240:47 - scenario where a threade invokes another
240:49 - method or code blog which also requires
240:52 - the same log to maintain consistency in
240:56 - the multi-threaded context this pattern
240:58 - is quite common in complex
241:00 - implementation s methods call other
241:02 - methods within the same class these
241:05 - methods require synchronized access to
241:08 - some shared resource and in such
241:09 - situations reant locks are quite useful
241:13 - and in fact very much needed so with
241:15 - that in place let's have a very quick
241:18 - code demonstration of using the reen
241:23 - clogs so to begin with I have created
241:26 - this class called as reint loog demo and
241:30 - here are the two fields that we are
241:31 - going to need so in this line we are
241:34 - initializing the reant lock the way to
241:37 - do this is by calling new reint lock
241:41 - there are other variations of this
241:43 - particular Constructor wherein we could
241:45 - pass the fairness criteria and all those
241:47 - things we will learn about those in a
241:50 - while so for now we have created the log
241:53 - and then we have a variable called as
241:55 - shared data which is initialized to zero
241:58 - next we have a method a what this method
242:01 - does is it's going to increment the
242:04 - value of the shared data and then it's
242:06 - going to call another method which is
242:08 - Method B and uh method a needs to get
242:12 - hold of the lock for this section of the
242:15 - code block that is incrementing the
242:18 - share data and then calling the method B
242:21 - and finally this particular lock is
242:23 - released now let's see what we have in
242:26 - the method B so in the method B again we
242:29 - are going to work on the shared data and
242:31 - the way it's being implemented is the
242:33 - Shar data value is getting decremented
242:36 - but we need some sort of Lock and that
242:39 - lock is going to protect this particular
242:41 - section which is decrementing the share
242:44 - data and printing the value and finally
242:47 - we call the unlock method so what we see
242:51 - is that we have a method a which is
242:53 - getting called by some thread and it's
242:56 - going to acquire a lock but in the same
242:58 - block when it's uh being protected by
243:01 - the lock there is a call to Method B and
243:04 - that method b as well is going to need
243:06 - that lock because that is also handling
243:08 - some sort of critical section so this is
243:11 - the reason that we have made use of the
243:13 - reentrant lock so this particular thread
243:16 - is already holding this lock but at this
243:19 - point of time this will need to acquire
243:21 - the log again and if it was not a reant
243:24 - log we could have encountered a blocking
243:26 - scenario and that could have also led to
243:29 - a dead log so that is the reason we have
243:32 - used the reant log now let's see the
243:35 - further implementation so we have a main
243:38 - method and what we are doing is we are
243:42 - uh creating uh and starting multiple
243:44 - threads which are essentially uh five
243:47 - threads and we are calling the start on
243:49 - those so let's try to run this and see
243:52 - the
243:53 - output
243:56 - so here is the output that meod a had
243:59 - share data as one method B has shareed
244:02 - data as zero method a and Method B so
244:04 - and so forth so the key takeaway is that
244:07 - we are able to deal with the shared data
244:10 - value in a concurrent uh environment in
244:13 - a concurrent fashion in multi- threaded
244:15 - context and the same thread is going to
244:17 - need to acquire a log multiple times and
244:20 - that is the reason we have used the
244:22 - reint log so at any given point of time
244:26 - let's say here if you would have called
244:29 - log. G hold count then the value would
244:31 - be two so essentially what is happening
244:33 - is the first time the lock is acquired
244:36 - here and the second time the lock is
244:37 - acquired here so essentially this trade
244:40 - has acquired this lock twice but under
244:42 - the hood of course it has been acquired
244:44 - just once it is just the count which is
244:47 - getting incremented and which is two and
244:50 - finally this lock will be unlocked it
244:52 - will be released so the count will
244:54 - become one and then the Call Comes Here
244:57 - and finally this lock. unlock will
244:59 - decrement the value further and it will
245:01 - become as zero and that is where we will
245:03 - conclude that the given threade has
245:06 - released this particular lock so uh that
245:09 - is for the code demonstration now let's
245:11 - look further into certain other aspects
245:13 - of reant lock which is the lock fairness
245:17 - so Reen lock provides a Constructor in
245:20 - which we can pass a Boolean value for
245:22 - the fairness of the lock when this value
245:24 - is true the lock is called a fair lock
245:27 - if it's false it's called as an unfair
245:29 - lock by default the rant locks are
245:32 - unfair let's understand this fairness
245:35 - with an example imagine there are a
245:38 - couple of threads which are trying to
245:40 - acquire a lock among all the threads
245:42 - which are competing for the lock let's
245:44 - say threade one gets hold of the lock
245:47 - all the remaining threads go to some
245:49 - sort of waiting queue after certain time
245:52 - threade One releases the lock by calling
245:54 - the unlock method at that point of time
245:57 - one of the locks in the weight queue
245:59 - will get a chance to acquire the lock
246:03 - imagine that out of all the threads in
246:05 - the weight queue threade two has been
246:07 - waiting the longest for acquiring the
246:09 - lock then if the rint lock is fair trade
246:13 - 2 will get a chance to acquire the lock
246:16 - and do its processing in the case of
246:18 - unfair inant locks there is no
246:20 - deterministic guarantee as to which
246:22 - threade gets to acquire the lock as a
246:24 - sweet side effect the unfair locks could
246:27 - potentially provide a beta performance
246:29 - as compared to the fair loocks what is
246:32 - the reason well they avoid the overhead
246:35 - associated with maintaining a weight que
246:37 - and enforcing some sort of deterministic
246:40 - ordering now let's learn about a few
246:42 - important methods of of the reentrant
246:44 - lock the first one is get hold count
246:47 - this method returns an integer which is
246:50 - the number of times the current thread
246:52 - has acquired the lock in other words it
246:54 - indicates how many times the current
246:56 - thread has successfully acquired the
246:58 - lock without releasing it the next one
247:02 - is the trylock and as the name suggests
247:05 - it is an approach where we request a
247:07 - thread to try acquiring a lock result is
247:10 - a Boolean which tells if a thread was
247:13 - successful fully acquired or not if the
247:16 - outcome is true we can proceed with
247:18 - processing and if false we can do
247:21 - something else and what we gain from
247:23 - this approach is if the lock has not
247:26 - been acquired then we are not blocked
247:28 - rather we can do some sort of other
247:30 - processing as well in our code there is
247:33 - another variation of trylock method
247:35 - which accepts timeout and time unit so
247:39 - this is an overloaded variation of the
247:41 - trylock method using this me method we
247:44 - can request a thread to acquire the lock
247:46 - and be blocked for the given time
247:48 - duration if the lock is being held by
247:51 - some other thread we hope that the lock
247:53 - becomes available during the waiting
247:55 - time period and that's where the lock is
247:58 - acquired by the thread which has called
248:01 - the trylock with duration and if not
248:04 - then of course the output of this
248:05 - particular trylock is a false and then
248:08 - we can do something else because we have
248:10 - not acquired theog due to X and Y
248:12 - reasons well trylock has a problem even
248:15 - if your reent lock is fair and trylock
248:18 - is called by a thread at the moment the
248:21 - lock is released by some other thread in
248:23 - this scenario The Waiting threads for
248:25 - this lock are not given the priority the
248:28 - new threade which made the trilock call
248:31 - gets to acquire the lock the work around
248:34 - for this problem is to pass a timeout
248:36 - value of zero time units and this will
248:39 - ensure that the fairness criteria of the
248:42 - lock is honored
248:44 - then we have a couple of auditory
248:45 - methods so we have something like is
248:48 - held by current threade and it returns a
248:51 - Boolean which tells if the current
248:52 - thread is holding a given lock or not
248:55 - the next one is gate Q length and it
248:58 - tells the length of the weight Q which
248:59 - is the number of Trades these two in
249:02 - particular are mainly used for debugging
249:05 - and profiling purposes but having said
249:08 - that if there is a need in your code
249:09 - implementation or in your business logic
249:12 - you could surely have have these Methods
249:14 - at your disposal the final one in our
249:16 - list is the most important one so the
249:18 - new condition as we saw earlier this
249:21 - returns a condition on the given lock
249:23 - and what it does is it helps in
249:25 - orchestrating the interaction between
249:27 - the thread and the lock so that should
249:29 - be it for the re interent
249:35 - lock in Java a read right lock is a
249:38 - synchronization mechanism which allows
249:40 - multiple threads to read a shared
249:42 - resource currently but only one threade
249:46 - can write to the resource at a time this
249:49 - mechanism is particularly useful when
249:52 - the resource is predominantly read heavy
249:54 - rather than write heavy let's understand
249:57 - this by visualizing an
250:04 - example imagine that there is a booking
250:06 - system using which you could book seats
250:09 - in a bus there are two main
250:12 - functionalities when it comes comes to
250:13 - the seat selection first is you should
250:16 - be able to view the available seats and
250:19 - the other one is where you can book a
250:22 - particular seat so in the image that we
250:25 - have we have threads from one to in
250:28 - which you want to view the available
250:30 - seats and threads from 1 to n which want
250:33 - to book a seat as for what we have
250:36 - learned already for any thread to be
250:38 - able to do some processing it needs to
250:40 - acquire the lock so let's assume that
250:43 - threade one which wants to just read the
250:45 - seat availability acquires the lock and
250:48 - proceeds with it please note that this
250:51 - is not efficient why so it is because
250:55 - there are other threads which also
250:56 - wanted to just read the data since they
250:59 - are not writing anything there would not
251:02 - be any concurrency issue involved if
251:04 - more than one threads acquire the lock
251:07 - and do its processing and this
251:09 - processing is just to read the data so
251:12 - how about if if we could somehow Club
251:14 - the reader threads together and it would
251:17 - be much efficient right and yes
251:20 - certainly it would be very much
251:21 - efficient and the read right log indeed
251:25 - does the same thing after some time the
251:28 - read threads are done reading the data
251:30 - and at some point of time the read lock
251:32 - is released once released either of the
251:35 - wrer threads could acquire the right
251:38 - lock and do their processing effectively
251:41 - in the case of a read right lock one
251:43 - writer thread could get hold of the
251:45 - writer log at a time or multiple reader
251:49 - threads could get hold of the read log
251:51 - at a time please note that we have two
251:54 - logs one for the reader other for the
251:57 - writer so even though we have two
251:59 - different logs two different objects
252:02 - only one can be acquired by your thread
252:05 - or a group of threads at a given point
252:06 - of time so either read lock is used by n
252:10 - threads or write lock is used by one
252:13 - trade but it can never happen that both
252:16 - the locks could be used at the same time
252:19 - ever so what we have seen essentially
252:22 - that there are two kind of threads the
252:24 - first category is that of the reader
252:26 - threads the second category is of the
252:28 - writer threads since reader threads are
252:31 - not mutating anything in the data they
252:34 - are safe to run together and that is the
252:38 - reason that all these reader trades
252:40 - could acquire the reader log at once and
252:43 - do their processing when it comes to the
252:45 - writer thread this could not be the case
252:48 - because writer thread is going to change
252:50 - something to the data and that is why
252:53 - only one writer thread is allowed to
252:56 - acire the lock so in such scenarios
252:59 - wherein you have a very much Clear
253:01 - condition of reader versus writer this
253:03 - is one such lock which could be used in
253:06 - such cases and please note that you
253:09 - would gain performance if your
253:11 - application is or rather if your use
253:13 - cases read heavy not a write heavy so
253:17 - now we will have a quick code
253:19 - demonstration to see how can we make use
253:22 - of the read write lock so as part of the
253:25 - code demonstration what we are going to
253:27 - build is a class which will have a
253:30 - counter there would be couple of reader
253:32 - threads and a writer thread the writer
253:35 - thread will try to increment the value
253:37 - and the reader threads will try to read
253:39 - the value and we wish to do so using the
253:43 - read write lock so let's implement it we
253:45 - will create this class in the locks
253:48 - package let's call this as shared
253:56 - resource we will have a variable let's
254:00 - call this as counter the value is
254:03 - zero then we will have the read write
254:09 - lock so read write lock let's call this
254:14 - as lock and we will be instantiating the
254:17 - object of new reent read write
254:21 - lock we have the first method which is
254:24 - supposed to increment the
254:27 - value and the way we do this is we
254:30 - acquire the lock so lock dot WR
254:34 - lock do lock
254:38 - and we will have the implementation
254:40 - inside a tri block finally is used to
254:45 - unlock the lock that we have acquired
254:49 - previously and what we will do is we
254:51 - will increment the counter by calling
254:53 - counter
254:55 - Plus+ and we will print a message the
254:59 - message could be something like thread
255:00 - do current thread. getet
255:03 - name and it
255:07 - writes and the value that is
255:10 - counter so this should be it for for the
255:13 - increment method now let's implement the
255:15 - other method as well which
255:18 - is going to get me the value of the
255:21 - counter in a given threade and idea is
255:25 - that this is supposed to be made use of
255:27 - by the reader threade let's minimize
255:30 - this
255:31 - one now back to the implementation of
255:33 - the G value first we try to get hold of
255:37 - the read lock and call lock on this one
255:43 - next we will have the dry block and
255:46 - inside the finally we will close the
255:49 - read loock that we have acquired by
255:52 - calling the unlock method and we will
255:56 - read the value for the counter so let's
255:59 - have the message like trade. current
256:02 - trade doget
256:04 - name
256:06 - and
256:08 - beeds
256:10 - counter so this should be for for the
256:13 - get Value method and for the read WR log
256:17 - demo we will create another
256:21 - class
256:23 - so let's call this class as read
256:28 - WR lock demo and uh we would have a main
256:34 - method inside the main method we create
256:37 - the object for the shared
256:40 - resource and we will create a couple of
256:44 - reader threads so let's do that so for I
256:49 - and inside the loop what I do is I
256:52 - create
256:55 - the reg
256:57 - thread so new
257:04 - thread and uh for
257:09 - INT J as zero J Less Than 3
257:17 - j++ resource dot get
257:21 - value
257:23 - and now let's assign a name to this
257:27 - trade so let's call this as reader
257:34 - threade and pass the value as I + 1 now
257:39 - we will start this thread so let's call
257:42 - do start on this one for the writer
257:44 - trade let's create one so
257:48 - trade wrer trade new
257:53 - trade
258:04 - and we do this
258:07 - for five
258:11 - elements so share resource do
258:17 - increment and likewise we will provide a
258:20 - name to the writer thread so let's call
258:23 - this as writer thread and let's start
258:28 - this
258:29 - thread so let's run this code and see
258:32 - how is the outcome
258:34 - like all right so what we see is that we
258:39 - have reader trade and there is another
258:41 - reader trade the two that we have
258:43 - created right and they're reading a
258:45 - value of zero initially because the wrer
258:48 - thread has not been executed yet the
258:51 - value remains to be zero the initial
258:53 - value then we have the writer thread and
258:56 - what the writer thread is doing is it's
258:58 - going to increment the value five times
259:01 - and uh as a result what we see is the
259:04 - value gets incremented to five now we
259:08 - have the reader thread and the reader
259:10 - thread reads the value and the value
259:12 - that that it's reading is five again the
259:15 - reader thread is invoked and it's
259:17 - reading the value as five so this is how
259:20 - you could make use of the read write
259:22 - lock if you have such a use case wherein
259:24 - you want to bate the lock acquisition
259:28 - for readers and writers separately so
259:31 - that should be it for the code
259:32 - demonstration let's complete this
259:34 - section of read WR lock by learning
259:36 - about the behavior of weight Q in the
259:39 - read write log so let's visualize how
259:42 - the we cues are arranged and managed in
259:45 - the case of a redr lock in the case of a
259:48 - redite lock there would be two types of
259:50 - threads in the we Cube one type of
259:52 - threads are those which want to read the
259:56 - data and the other type is which want to
259:58 - write the data let's say that initially
260:01 - there is one writer thread and two
260:03 - reader threads in this case the reader
260:05 - threads will be allowed to acquire the
260:08 - readlock this is done because it's
260:10 - sufficient essentially what you are
260:13 - allowing is two threads to be executed
260:16 - as compared to the one once all these
260:18 - reader threads are done with their
260:20 - operation the right of thread baiting in
260:23 - the same que is allowed to acquire the
260:25 - right log what happens in the scenarios
260:28 - when there is a writer thread waiting in
260:31 - the queue and the reader threads keep on
260:33 - coming to the queue should they all be
260:35 - allowed because it's faster to execute
260:37 - well if we do so the writer threads will
260:40 - be staffed and this is not good so the
260:42 - this kind of lowlevel optimization is
260:44 - taken care of depending on the type of
260:47 - implementation you choose for the read
260:49 - write lock for example reant read write
260:52 - lock is one such
260:54 - implementation the key takeaway here is
260:56 - that read write locks are optimized for
260:58 - reading and writing operations in the
261:00 - most efficient manner possible and you
261:04 - as a developer should be good to make
261:06 - use of this lock without much
261:09 - optimization or configuration concerns
261:12 - so whenever you have such requirement
261:15 - consider making use of this
261:22 - lock let's learn about this very
261:24 - important concept of visibility problem
261:27 - before we start learning about the
261:29 - visibility problem let's take a
261:30 - important detour to understand and
261:33 - visualize how cores registers and caches
261:36 - are placed along with the ram
261:43 - the image here is showing a machine
261:45 - which has six cores on the first layer
261:48 - we have the course course are the piece
261:51 - of Hardware which are responsible for
261:53 - all the computer activity Loosely
261:55 - speaking code itself does not have any
261:57 - storage capability it can only compute
262:01 - but in order for the computation to
262:03 - happen we need to have some sort of data
262:05 - storage as well right this is where we
262:08 - have different type of storage units
262:11 - let's know about them in a very brief
262:13 - manner first on the list is a register
262:17 - every core has a dedicated storage
262:19 - component which holds the data
262:21 - temporarily during the execution of the
262:23 - instructions this storage component is
262:26 - known as register it has the least
262:29 - amount of storage
262:30 - capacity each core has a dedicated
262:34 - register the next one in the line is the
262:36 - component of L1 cache L1 cache is a
262:40 - small but a very fast memory
262:43 - it plays a crucial role in improving the
262:45 - performance of the CPU by reducing the
262:48 - data access time to and from the main
262:50 - memory which is also known as the ram
262:53 - each core has a dedicated L1
262:56 - cache the next component is the L2 cache
263:00 - it is a larger capacity cache as
263:02 - compared to the L1 cache its role is to
263:04 - complement the smaller and faster 111
263:07 - cache it does so by providing additional
263:10 - storage for frequently accessed data
263:12 - data and instructions L2 cach is
263:15 - integrated in two ways depending on the
263:18 - architecture of the processor in one
263:20 - approach the L2 cach is shared across
263:23 - different cores whereas in the other
263:25 - approach we could have dedicated L2
263:28 - cache for a core the presentation that
263:31 - we have here we are showing a shared El
263:33 - to Cache where the cash is being shared
263:36 - by two course for example here Core 1
263:39 - and Core 2 are sharing this L2 cache
263:41 - likewise core 3 and core 4 are sharing
263:44 - this particular L2 cache and so and so
263:47 - forth the third type of the cache is L3
263:50 - cache it sits between the L2 cache and
263:53 - the ram which is also known as the main
263:56 - memory its role is to provide a larger
263:58 - pool of cast data and instructions which
264:01 - can be shared among multiple CPU cores
264:05 - the final type of a stor layer is RAM
264:07 - and as told earlier that this is also
264:10 - known as the main memory it Serv serves
264:12 - as a temporary storage for data and
264:15 - instructions that are actively being
264:17 - used by the CPU so on the one end of the
264:20 - spectrum registers store the data
264:23 - specific to a core and on the other end
264:25 - of the spectrum we have Ram which stores
264:28 - data and instructions being worked upon
264:30 - by all the cores of the CPU when we go
264:33 - farther from the core towards the ram
264:36 - the latency and memory size increases so
264:39 - Ram would be of larger size as compared
264:42 - to L3 cach L3 cach would be of larger
264:45 - size as compared to L2 cach and likewise
264:48 - register is the one which has the least
264:50 - amount of storage capacity but since
264:53 - register is closer to the core as
264:56 - compared to the L1 cache and L2 cache
264:58 - registor would be faster in fact the
265:00 - fastest type of memory whereas Ram is
265:04 - the most farthest one from the core so
265:07 - it is the slowest of the lot so the ram
265:09 - would be the slowest memory and the
265:11 - registor would be the fast fastest
265:12 - memory let's see this diagram what we
265:15 - have here is a code snippet and
265:17 - basically we have two methods assume
265:20 - this write method is being used by a
265:23 - writer thread and this read method is
265:25 - being used by a reader thread and this
265:28 - particular code snippet is being
265:30 - executed in this kind of a machine
265:32 - wherein we have two cores and core one
265:36 - is executing threade one and core two is
265:38 - executing threade 2 so to begin with
265:41 - there is a variable able whose value is
265:43 - zero the count variable starts with
265:46 - value zero and assume at certain point
265:49 - of time thread one gets to execute on
265:51 - core one and thread two gets to execute
265:54 - On Core 2 so basically both of these two
265:56 - threads are running in parallel so the
265:59 - value of count is updated as one by the
266:01 - writer thread and at the same time the
266:04 - value count is loaded in the value so
266:08 - what would happen if you try to access
266:10 - the value of value by either printing it
266:13 - or debugging it so you may see something
266:16 - like this so to begin with count is zero
266:19 - that is in the shared cache then the
266:22 - count is updated to Value one and this
266:25 - value is there in the register which is
266:27 - local to Core 1 now Core 2 is running
266:31 - threade 2 and it is trying to access the
266:35 - value for count so remember that as of
266:38 - now the value of count is one is only
266:40 - with this register it has has not
266:42 - percolated to the shared cach level so
266:46 - the value that is present for this count
266:48 - variable is still zero so the value that
266:51 - is assigned to Val is still zero in this
266:54 - register and when we try to print this
266:57 - value we will get the value as zero as
267:00 - opposed to what we were expecting which
267:02 - should have been one so this is not
267:05 - something which is expected and correct
267:07 - right so this kind of behavior is called
267:10 - as the visibility problem Java provides
267:13 - a solution for this with a keyword
267:15 - called as volatile using this keyword
267:18 - ensures two things first is whenever
267:21 - there is a change in the value it's
267:23 - flushed to the shared memory second is
267:26 - whenever there is a value which is
267:29 - supposed to be read it's read from the
267:31 - shared memory the shared memory is
267:33 - usually and in most of the cases L3
267:36 - cache which we saw earlier to be the
267:38 - shared cache and this one is a level up
267:42 - to the ram so basically what I'm trying
267:44 - to say is let's say if I used volatile
267:47 - with this variable so what I'll do is I
267:49 - will write this as volatile keyword then
267:52 - int count right and then when thread one
267:57 - is executed and any value to this
267:59 - variable is changed it won't be only
268:02 - there with the register rather it will
268:04 - be written to the shared cach as well
268:06 - and since this variable has been made
268:08 - volatile the thread won't read this
268:11 - value from the register or any other
268:13 - cache rather it will directly read from
268:15 - the shared cache so on the right we're
268:18 - ensuring that the value for the
268:20 - particular variable which has been made
268:22 - volatile is going to the shared cache
268:24 - and for the read as well we're ensuring
268:26 - that Valu is raid from the shared cach
268:29 - itself so that would avoid Us in Reading
268:32 - wrong value and this is how the volatile
268:35 - keybo helps us with the visibility
268:38 - problem there is a word of caution we
268:41 - should always follow while making use of
268:43 - the volatile keyword however while
268:46 - volatile does help with the visibility
268:48 - problem as a side effect it slows down
268:51 - the code and does the application it's a
268:53 - no-brainer that with the uses of
268:55 - volatile the many layers of caching are
268:58 - removed and so are the latency
268:59 - optimization benefits which we gain when
269:02 - we use the caching so when not needed
269:05 - the uses of volatile keyword should be
269:07 - strictly avoided
269:14 - dat loock is an important concept we
269:16 - should know about when we are writing
269:18 - multi-threaded code in Java let's learn
269:20 - about the same what is a
269:25 - deadlock here is an image which tells us
269:29 - about a deadlock situation let's have a
269:31 - closer look here we have two trades
269:34 - which are running threade one at some
269:36 - point of time is going to need lock a
269:40 - which is represented by a green color
269:42 - color then after some point in time
269:44 - threade one is going to need lock b
269:47 - which is represented as blue
269:49 - color now let's look at threade two this
269:53 - threade is going to need lock b at some
269:56 - point of time and then it's going to
269:58 - need lock a after some time please note
270:02 - that in both the threads both the locks
270:05 - are needed to be acquired in order for
270:07 - the processing to get completed only
270:10 - after the processing has been completed
270:12 - both the logs A and B will be released
270:15 - let's imagine the scenario at time t
270:19 - threade one needs lock b while it has
270:21 - already acquired the lock a likewise
270:25 - threade 2 needs lock a while it has
270:28 - already acquired the lock b but both the
270:31 - threads won't be able to acquire the
270:34 - needed lock because their need is being
270:36 - held by the other thread and neither of
270:39 - the threads are in a situation to let go
270:41 - of their acquired lock since it's needed
270:45 - to complete the processing of the
270:47 - threade so in this situation none of the
270:50 - trades could proceed with their
270:52 - execution and this situation is called
270:55 - as a deadlock so here is formal
270:58 - definition of the Dee loock in a
271:01 - multi-threaded context a de loock occurs
271:03 - when two or more threads are blocked
271:06 - forever each waiting for the other
271:08 - threade to release a resource they need
271:10 - to proceed this situation creates a
271:13 - cycle of dependencies with no thread
271:15 - able to continue with its execution so
271:20 - how can we spot deadlocks in our code
271:23 - there could be two approaches one is the
271:25 - manual approach other is the
271:27 - programmatic approach so let's learn
271:29 - about the
271:30 - same well detecting or spotting locks at
271:33 - the compile time is not an easy task
271:36 - this is so due to multiple reasons Java
271:39 - comes with multiple types of logs so
271:42 - multi-threaded code can be littered with
271:44 - different types of explicit and implicit
271:46 - logs also there could be multiple
271:49 - sources of threads in the code base due
271:52 - to these two factors combined with the
271:55 - fact that threade execution and CPU
271:57 - location is not deterministic it's
271:59 - nearly impossible to figure out when
272:02 - which lock is going to be acquired by
272:04 - which threade moreover the example we
272:08 - saw earlier was a simplified version
272:10 - where we had two threads and two locks
272:13 - however in real world it won't be this
272:16 - simple there is a good amount of
272:18 - possibility to have multiple threads and
272:20 - multiple locks which could lead to a
272:23 - deadlock situation and thanks to all of
272:25 - these complexities it's nearly
272:27 - impossible to detect a date log by just
272:30 - looking at the code so the manual
272:32 - approach is something which is not very
272:35 - efficient neither it's the suggested
272:38 - approach so how do we detect the
272:40 - deadlock in code let's have a look on
272:42 - the programmatic approach of detecting
272:44 - the de log so now that we have learned
272:47 - that detecting a dead log is very
272:49 - difficult in manual way are there any
272:51 - other tools which can help us in this
272:53 - detection good news is yes there are
272:56 - tools and ways in which we can detect
272:58 - deadlocks in our code two of the most
273:01 - common approaches are taking the thread
273:03 - dump and uses of the thread MX Bean to
273:07 - programmatically detect cyclic thread
273:09 - dependencies which can lead to a
273:11 - Deadlock Lo so let's have a brief code
273:14 - demonstration where we will be knowingly
273:17 - introducing deadlocks in our code and
273:19 - then use these methods to detect the
273:22 - deadlock situation so in order to
273:25 - demonstrate the dead log code I'll
273:27 - create this package and let's call this
273:30 - package
273:31 - as
273:34 - other
273:36 - Concepts and uh let me create this
273:40 - class as as dat lock
273:45 - demo we will have two
273:48 - logs so
273:51 - lock lock
273:53 - a and make this as a reint lock let's
273:58 - duplicate it create an another lock lock
274:04 - b and we will have two methods first
274:07 - would be a worker one method other would
274:10 - be worker 2 method
274:12 - and uh let's Implement
274:14 - these so public
274:17 - void
274:22 - worker
274:24 - one it will start by acquiring the log a
274:30 - and then there would be some message
274:32 - which will be printed like worker one
274:36 - acquired log
274:40 - a next we will introduce some sort of
274:44 - sleep time so let's call thread. sleep
274:48 - for 200
274:49 - millisecs let's handle the interrupted
274:53 - exception let's throw it as a runtime
275:02 - exception and uh then we acquire log
275:07 - B then let's print some message it could
275:11 - something like worker one
275:16 - acquired lock
275:19 - b and then we start by unlocking the
275:24 - locks which is lock a. unlock and loock
275:28 - B do
275:29 - unlock so this should complete the first
275:32 - method which is worker one now we will
275:36 - duplicate this and we will write the
275:39 - worker two method implementation
275:43 - what worker 2 does is it starts by
275:46 - acquiring the lock b and then we will of
275:50 - course change the messaging
275:56 - here and again we have a sleep for 200
276:00 - milliseconds and now after log B is
276:03 - acquired log a will be
276:05 - acquired and again we will adjust the
276:08 - messaging and then we can have Lo lock a
276:11 - unlock itself and lock b unlock
276:14 - itself now let's write the main method
276:18 - where we will be creating the threats
276:20 - and uh executing these
276:26 - things so we start by creating the
276:30 - object for the
276:32 - class so that we can invoke the
276:36 - methods now what we can do is we can say
276:41 - new threade
276:43 - demo do worker one we will give this
276:47 - thing a name this threade a name and we
276:51 - call do
276:52 - start likewise we'll create a threade
276:58 - which will be making use of the worker 2
277:01 - and we will adjust the name here for the
277:03 - threade which will be worker
277:05 - 2 so let's run this thing and see what
277:08 - is the output like
277:15 - all right so what we see is worker one
277:19 - has acquired lock a and worker one
277:22 - acquired lock b so essentially the
277:25 - worker one got a chance to
277:28 - run and after that point of time worker
277:32 - 2 was supposed to be run but now we are
277:34 - in a deadlock situation because worker
277:37 - one has acquired the log B and here work
277:41 - worker 2 requires log B in order to do
277:45 - its processing but since the worker one
277:48 - has acquired lock b and not released it
277:51 - yet the worker 2 cannot get hold of it
277:54 - and that is the reason we have entered
277:56 - it into a situation of Deadlock what we
277:59 - can do about it now well in such cases
278:02 - you cannot do anything but your code
278:05 - will be sort of in a DAT log situation
278:08 - and you will have to terminate it so as
278:11 - told earlier that we can either take a
278:13 - threade dump or maybe we could use the
278:16 - other approach of using the thread mxp
278:19 - to understand what part of the code is
278:21 - leading to this dat log situation right
278:24 - so let's understand the first approach
278:27 - which is using the thread dump so what
278:30 - we will do
278:31 - is we will go to the terminal so let me
278:35 - come out of the distraction free mode
278:37 - and here we go to the terminal and in
278:41 - the the terminal we will list all the
278:44 - processes which are running
278:47 - Java so we see that these are the
278:50 - processes and the one which we are
278:53 - concerned about is this deadlock demo
278:56 - and here is the process ID so what we do
279:00 - is we call kill hyphen 3 and we pass the
279:05 - process ID which is this one hit
279:08 - enter and go back to the output console
279:12 - so here we can see that we have lots of
279:14 - messages which are getting printed so
279:17 - this is something that we had earlier
279:19 - right where we had the deadlock
279:21 - situation now we are seeing something
279:23 - called as a thread dump which is the
279:26 - full thread dump and it's going to show
279:29 - certain information regarding the
279:31 - threade classes certain messages and so
279:34 - on so forth of course you could try to
279:36 - do it yourself on your system and uh
279:39 - read through all these things what exact
279:41 - is being told but the most important
279:44 - thing is in the last so scroll to the
279:47 - last and here we have this message and
279:50 - this thing says that found one Java
279:53 - level deadlock what does it say it says
279:56 - waiting for ownable synchronizer a fancy
279:58 - name for the lock which is held by
280:01 - worker 2 so worker one is saying that
280:05 - it's waiting for a lock which is being
280:07 - held by worker 2 and the worker 2 says
280:10 - it's w hting for a lock which is hailed
280:13 - by a worker one right so what you could
280:16 - do is you can scroll further down and
280:19 - you could see that this is one line
280:22 - which is being hinted in your code click
280:24 - it out and you could see that this is a
280:28 - probable reason so in your use case in
280:32 - your real world use case whenever you
280:34 - are faced with this kind of situation
280:36 - this is the way in which you could find
280:39 - out the process ID and kill it with
280:42 - hyphen 3 as the argument and this will
280:45 - help you out with this threade dump and
280:47 - you could figure out at least you have a
280:49 - point of Investigation where you could
280:52 - try to understand what is the DAT log
280:54 - situation and what part of the code is
280:56 - leading to this scenario so this is the
280:59 - approach in which you could analyze the
281:01 - threade dump to understand the date lock
281:04 - there is another approach however so
281:07 - let's go to the code and we will have to
281:09 - modify something in our code in order to
281:12 - understand that particular
281:15 - approach so what we are going to do is
281:19 - we are going to make use of this thing
281:22 - called as thread MX bean and uh we will
281:26 - create an another threade let's say a
281:28 - threade three which is going to execute
281:32 - forever and it will be done after an
281:35 - interval of every 5 seconds so the idea
281:38 - is to probe the code and to understand
281:41 - if there are any trades which are in
281:43 - deadlock so let's write the code it will
281:46 - be much simpler to understand that way
281:48 - so let's start by creating a thread and
281:52 - uh let's Supply the code for the same so
281:58 - the first thing is we have trade MX Bean
282:01 - let's call this as MX bean and it
282:04 - provides a management Factory using
282:07 - which you could create the threade MX
282:09 - Bean
282:11 - and uh while
282:14 - true what we can do is we can
282:19 - say
282:22 - mxb find me all the date log
282:25 - trades it will return a array of long
282:29 - which are basically the trade
282:33 - IDs and if the threade IDS does not
282:39 - happen to be a null
282:42 - so we can print this message that dat
282:47 - log
282:49 - detected and uh we can iterate over
282:54 - the
282:55 - long trade ID for each of the trade IDs
283:00 - in the trade
283:01 - IDs and then print this
283:04 - message the threade with
283:08 - id threade id
283:11 - is in
283:15 - deadlock and uh once we have printed
283:17 - this message what we need to do is break
283:20 - out of the infinite
283:23 - Loop and uh if that is not the
283:27 - case
283:29 - so we need to do this every 5 seconds so
283:34 - let's put a timer of 5,000
283:37 - milliseconds and catch the interrupted
283:39 - exception
283:41 - and we will throw it as a runtime
283:45 - exception and finally let's call the do
283:48 - start on this
283:51 - thread so here is what is happening we
283:54 - have created a thread which is going to
283:56 - run every 5
283:58 - seconds and we are making use of the
284:00 - management Factory to get an object of
284:02 - the mxp and the MX Bean has a method
284:06 - which will give me the deadlock threade
284:08 - IDs if there are any trades which are de
284:11 - loock and I am simply going to iterate
284:14 - over the given trade ID right and print
284:19 - the message here you may be wondering
284:21 - that trade ID is fine but what if I need
284:24 - certain extra information how can I do
284:26 - that well there is an another method in
284:30 - the thread MX Bean which is called as
284:32 - get thread info so let's make use of
284:35 - that as well so we can say mxb get
284:39 - thread info and it's going to accept a
284:42 - long array which is the threade IDS and
284:46 - what it returns is an object of thread
284:49 - info array and let's call this as thread
284:53 - info so thread info contains a lots of
284:57 - information as in what is the threade
284:59 - name what is the lock name uh where
285:03 - exactly it is waiting for which lck and
285:07 - all those things so in this example we
285:09 - don't really need to showcase all those
285:11 - things so but the idea here is that if
285:14 - needed you could certainly make use of
285:17 - this particular line and uh iterate over
285:20 - the trade info and get the required
285:22 - information so let's run this and see
285:25 - the
285:26 - output so
285:30 - run and initially we have executed these
285:34 - trades there is a timer of 2 seconds
285:38 - remember 2 milliseconds rather and then
285:41 - after some point of time this thread
285:43 - kicks in and it's going to say thread
285:45 - lock detected and thread with id21 is in
285:48 - deadlock and thread with id22 is in dat
285:51 - loock and as I told earlier that if you
285:54 - need certain more information you could
285:55 - make use of the threade info object or
285:59 - the class and uh these are the
286:02 - information it contains and you could
286:04 - make use of these information in your
286:06 - code as per your need so now that we
286:09 - have seen the code demonstration for the
286:11 - dead log let's understand how can we
286:13 - prevent Deadlocks so are there any
286:16 - approaches or suggestive measures using
286:19 - which you could get rid of the Deadlocks
286:22 - or at least try to write a deadlock free
286:24 - code so the first and foremost is use
286:27 - timeouts so let's say when your trade is
286:30 - trying to acquire a lock giving a
286:32 - timeout will ensure so it's not going to
286:35 - end up into a situation where it's
286:37 - trying to acquire a lock for infinite
286:41 - time right and thus it will help in
286:44 - avoiding the deadlock situation so uses
286:47 - of timeouts is a good approach in order
286:49 - to avoid Deadlocks the second thing is
286:53 - the global ordering of the locks so
286:56 - another thing we can do is to pay
286:58 - attention to the global ordering of the
286:59 - locks what it means is let's say we have
287:02 - two trades and two locks to acquire for
287:05 - example log a and log B both the threads
287:09 - should first acquire log lock a followed
287:11 - by lock b so in that sense it won't be a
287:15 - cyclic dependency scenario and it will
287:17 - avoid the DAT lock
287:20 - situation the third thing is to avoid
287:23 - any sort of nested locking so whenever
287:26 - feasible we should always avoid locking
287:29 - within already logged section and the
287:32 - fourth preventive measure is to make use
287:34 - of the thread safe
287:36 - Alternatives so whenever feasible to do
287:39 - so use thread safe collection and atomic
287:41 - variables and minimize the uses of logs
287:44 - sure the thread collections do make use
287:47 - of the logs under the hood however in
287:50 - general the code implementation of the
287:52 - standard threade safe are battle tested
287:54 - industry Solutions so there is a little
287:57 - to no chance of running into the
287:59 - deadlock
288:00 - situation so this should be it for the
288:03 - concept of Deadlock and uh these are the
288:06 - certain measures using which you could
288:08 - try to avoid dat lock situation and even
288:12 - if you are in one situation of a date
288:15 - loock what are the different approaches
288:17 - using which you could analyze your code
288:20 - to correct
288:26 - it Atomic variables are an important
288:29 - tool which help us in writing concurrent
288:32 - code in Java let's learn about the same
288:35 - so what's read modify right cycle
288:42 - in order to understand what is a read
288:44 - modify right cycle let's have a quick
288:46 - look on this image there are two threads
288:48 - and a variable called as count which has
288:51 - the value as zero there is a processing
288:54 - part which needs to be done on this
288:56 - value which is to Simply increment the
288:59 - value by the count one two threads get
289:02 - to execute this process of incrementing
289:05 - the value imagine if these two threads
289:07 - are going to be executed 1,000 times
289:11 - what we would expect is that the final
289:13 - value of the count should be 2,000
289:16 - however this is not the case so th times
289:19 - the value will be incremented by threade
289:21 - 1 and another thousand times the value
289:24 - should be incremented by threade 2
289:26 - effective value expected is 2,000 but
289:29 - when you try to run this code you will
289:31 - find the value would be less than 2,000
289:34 - moreover it will be very much
289:36 - inconsistent so at sometimes you could
289:39 - get some different value and at the
289:41 - other time the value would be totally
289:43 - different so there is some sort of
289:45 - inconsistency let's learn why so well
289:48 - the reason lies in the fact that how do
289:51 - we actually understand the increment
289:53 - operation and how it is actually
289:55 - performed on the low level so on a very
289:57 - low level there are three steps which
290:00 - are performed when we increment the
290:01 - value of a variable as part of the step
290:04 - one the value of the variable is loaded
290:06 - in the register remember register is the
290:09 - local storage which which is the nearest
290:11 - and almost native to the processor core
290:14 - as the Second Step this value is
290:16 - operated on as part of which it gets
290:19 - incremented by one and finally as the
290:22 - third step the incremented value the new
290:24 - value is assigned back to the variable
290:26 - count so to understand with an example
290:29 - at step one the value is zero which is
290:33 - loaded in the register and at a step two
290:36 - it becomes one and finally at the step
290:39 - three the value one which which has been
290:41 - incremented from 0 to 1 assigned back to
290:44 - the counter this entire flow is called
290:47 - as the read modify right cycle and this
290:50 - is the core reason behind the
290:52 - inconsistency in the value if it's
290:55 - incremented across multiple threads
290:57 - since I have already covered this
290:59 - concept in more depth with examples I
291:01 - won't go into much details however if
291:04 - you could remember from the previous
291:06 - discussion what's exactly happening is
291:08 - that the entire process of increment ing
291:10 - the value count Plus+ is divided into
291:13 - three steps and and let's say if threade
291:16 - one is operating on the count operation
291:18 - which is involving these three steps and
291:21 - at the same time somewhere between these
291:23 - steps if threade 2 is going to interrupt
291:26 - then this problem will occur and this is
291:28 - the sole reason behind the inconsistency
291:31 - so now let's talk how can we solve this
291:34 - without using any kind of explicit
291:36 - synchronization
291:37 - mechanism so what are Atomic variables
291:40 - what we saw earlier is something called
291:43 - as a nonatomic operation what it means
291:46 - is essentially an operation for example
291:49 - incrementing a value can be broken down
291:51 - into three steps thus it's not Atomic
291:54 - something which can be broken down so
291:57 - what Atomic variables in Java do is that
291:59 - there are a type of variable that
292:01 - support Lock Free thread safe operations
292:04 - on single variables they ensure that any
292:08 - kind of read modify right operation such
292:10 - as incrementing or updating a value are
292:13 - performed automically it prevents race
292:16 - conditions and this makes them crucial
292:18 - in concurrent programming as they help
292:20 - in maintaining the data consistency and
292:23 - integrity without the overhead of
292:25 - traditional synchronization mechanisms
292:28 - Atomic variables provide built-in
292:30 - methods that perform Atomic
292:33 - operations it ensures that actions like
292:36 - incrementing and updating are completed
292:39 - as a single and non visible
292:41 - step this eliminates the need for
292:44 - synchronized locks which can be costly
292:46 - due to locking overhead and potential
292:48 - thread contention as a result Atomic
292:52 - variables offer a more efficient way to
292:54 - achieve thread safety in the concurrent
292:56 - programming there are different types of
292:58 - atomic variables the atomic variables in
293:01 - Java are basically classes each of these
293:04 - classes are used for a particular data
293:06 - type for example integer long Etc the
293:09 - Java do U.C concurrent do atomic package
293:13 - contains all such Atomic variables
293:16 - however Atomic integer Atomic Boolean
293:19 - Atomic long Etc are some of the
293:21 - important and the most commonly used
293:23 - ones so if you wish you could go to the
293:26 - concurrent Atomic package and explore
293:28 - the other Atomic variable options that
293:30 - we have now with that in mind let's
293:32 - understand a few of the basic operations
293:34 - that we can perform on these Atomic
293:36 - variables these Atomic variables provide
293:39 - quite a few operations which we can make
293:41 - use of as per our need however there are
293:43 - a few which are very commonly used let's
293:46 - learn about those the first one is get
293:49 - so it fetches the current value next one
293:52 - on the list is set operation and as the
293:54 - name suggest it will set some value the
293:57 - third one is compare and set and this
293:59 - operation expects two values which are
294:02 - expected and update it sets the value to
294:05 - the update value if the current value is
294:07 - equal to the expected value then we have
294:10 - have these two operations which are
294:13 - equivalent to the pre and post increment
294:15 - this operation automically increments
294:17 - the value by one and returns either the
294:20 - new value or the old value the fifth one
294:23 - and the final operation are list is gate
294:26 - and decrement and decrement and get
294:29 - which are equivalent to the pre and post
294:31 - decrement this operation automically
294:34 - decrements the value by one and returns
294:36 - either the new or the old value please
294:39 - note that these are the most commonly
294:41 - used operations and there are much more
294:43 - depending on the kind of atomic variable
294:46 - you're planning to use you could explore
294:48 - the respective classes to find about
294:50 - their supported operations in more
294:52 - details but having said that these are a
294:55 - few of the commonly seen operations that
294:58 - you would come across when you write the
295:00 - code related to Atomic variables now
295:03 - let's conclude the discussion on the
295:05 - atomic variables by going through a code
295:07 - demonstration and we will be using the
295:09 - atomic integer Ser as the one of the
295:12 - atomic variables to showcase the uses of
295:15 - atomic variables so in the other
295:18 - Concepts package let's create a class
295:21 - let's call this as atomic
295:32 - variable and what we are planning to
295:34 - implement it's some code which is going
295:36 - to work on a counter variable and there
295:39 - would be a couple of trades working on
295:41 - this and we will see how we could make
295:43 - use of the atomic integer to help us
295:46 - with the inconsistencies of
295:48 - multi-threaded programming so let's
295:50 - start with having the count variable so
295:54 - the initial value is going to be
295:57 - zero and then we will have the main
296:02 - method so we will first create the
296:06 - threade one let's call this as threade
296:10 - one
296:11 - and what this will do is it
296:14 - will run
296:16 - for it will call count Plus+ for
296:20 - thousand
296:21 - times let's have the similar
296:26 - trade let's call this as threade two and
296:31 - it will also do the same that is it is
296:33 - going to increment the value by 10,000
296:35 - times so let's start these two threads
296:41 - let's remove the one and type it as
296:46 - two we will have to wait for these
296:50 - threads to be
296:51 - completed so let's do that surround with
296:55 - try
296:57 - catch let's
297:00 - put two
297:02 - here and now let's print the count value
297:07 - so count value is
297:14 - count so let's execute this code and see
297:17 - what is the output
297:20 - like all right what we are seeing is the
297:23 - value is
297:25 - 14,113 which is not correct of course
297:27 - because what we wanted is that we have
297:30 - two threads which are going to increment
297:32 - the value 10,000 times so effectively
297:35 - the value should be
297:37 - 20,000 when we print it here but that is
297:40 - not the case so the reason this is
297:42 - happening is because we are using count
297:44 - Plus+ in a multi-threaded context and
297:47 - which is of course not an atomic
297:49 - operation and due to the reason that we
297:51 - learned in the theory section this is
297:53 - the problem we are facing so how can we
297:56 - correct this so let's make use of the
298:00 - atomic integer so private static final
298:04 - Atomic integer let's call this as
298:07 - counter and let's create the atomic
298:09 - integer initial value is going to be
298:13 - zero so instead of having this count
298:17 - Plus+ let me comment it out and what we
298:20 - can have is counter do increment and get
298:25 - so what this increment and get does is
298:27 - it is going to increment the value of
298:29 - the counter which is zero to begin with
298:33 - and it will get the value so essentially
298:35 - if you want you could capture it
298:36 - somewhere but even if you don't capture
298:39 - it the counter object is going to hold
298:41 - the updated value so let's say if it's
298:44 - getting executed for the very first time
298:46 - and the initial value is zero then the
298:48 - value for counter will become as one
298:51 - because it's going to increment it and
298:53 - get the value which is self assigned if
298:56 - you want you could capture it as some
298:58 - variable as I showed earlier now for the
299:01 - second thread let's comment it out as
299:03 - well and again here we could call
299:06 - increment and get on the
299:08 - counter so that is about the uses of
299:13 - counter the atomic integer in the thread
299:16 - section now everything else Remains the
299:19 - Same and here we will try to print the
299:22 - value of counter instead of
299:25 - count so let's print it out and let's
299:29 - run this and say the
299:32 - output so here is the output what we see
299:35 - is the value is 20,000 let's run it a
299:38 - couple of times to see if it's working
299:41 - consistently so yes we are getting the
299:44 - same value over and over again what it
299:46 - means is the value is consistent and the
299:49 - problem that we were facing earlier of
299:51 - inconsistency because we using a
299:54 - nonatomic operation that was the count
299:56 - Plus+ that problem has been fixed when
299:59 - we started using the atomic integer so
300:03 - as I told you earlier that Atomic
300:04 - integer is part of the Java util
300:07 - concurrent Atomic package and likewise
300:10 - we have many other options so if you go
300:13 - to this package you will see that we
300:15 - have Atomic and we have Atomic Boolean
300:17 - Atomic integer Atomic array and so on so
300:20 - forth there are lots of options that we
300:22 - have in the atomic package so you could
300:24 - explore all of these things depending on
300:26 - your need but having said that this
300:28 - should conclude the discussion on the
300:31 - uses of atomic variables
300:40 - SEMA Force are an important concept
300:42 - whenever we talk about multi-threading
300:44 - in Java so let's learn about the same
300:47 - what are semap fores a semap for is a
300:50 - synchronization mechanism used to
300:52 - control access to a shared resource in
300:55 - concurrent programming it uses counters
300:58 - to manage the number of allowed accesses
301:00 - to the resources preventing rise
301:03 - conditions and ensuring safe concurrent
301:05 - operations SEMA force can be binary or
301:08 - counting it depends on the number of
301:10 - permitted axess while I was learning
301:13 - this concept the terminology of semap 4
301:15 - used to confuse me a lot the word
301:18 - semaphor is not very intuitive and it
301:20 - does not reflect something which is
301:23 - common in our day-to-day uses so why do
301:26 - we name a particular concept at
301:28 - semaphore if you are in the similar
301:30 - situation let's learn what does this
301:32 - word mean the term semap for comes from
301:36 - the signaling devices used in Railway
301:39 - and M time contexts to control traffic
301:43 - in programming it similarly signals and
301:46 - controls access to Shared resources
301:49 - among concurrent processes essentially
301:52 - this term semaphore is not commonly used
301:54 - in everyday language among native
301:57 - English speakers it is primarily known
302:00 - in a specialized context like
302:02 - programming operating systems and
302:05 - historical references to Signal devices
302:07 - in transportation so with having known
302:10 - the definition of the topic let's
302:12 - understand how is this semap for used
302:15 - with some
302:19 - visualization imagine we are building an
302:22 - application which needs to connect to a
302:24 - third party service so this is our
302:26 - application and this is the third party
302:35 - service so this is the application and
302:38 - this is the third party service
302:40 - due to sub constraints the third party
302:42 - service could be accessed only by a
302:44 - limited number of Trades at a given
302:46 - point of a time so for example what it
302:49 - means is only three concurrent calls
302:51 - would be allowed to this third party
302:54 - service from our application so in order
302:57 - to implement such scenario we need to
303:00 - ensure that we have some sort of
303:02 - restriction in place in such situations
303:05 - we can think of making use of the semap
303:08 - 4 to implement such restrictions for
303:10 - example assume that our application has
303:13 - 50 threads in thread full however the
303:16 - third party service is going to allow
303:17 - only three threads so in that case we
303:21 - can think of using the semap for now
303:24 - let's understand the concept of SEMA for
303:26 - with this
303:27 - image let's start with an assumption
303:30 - that SEMA 4 at the core of it is nothing
303:32 - but a permit mechanism the image shown
303:36 - has three boxes these three boxes
303:39 - are nothing but permits so at the moment
303:43 - this semap 4 is going to allow three
303:45 - threads to be run so essentially if
303:48 - there is a threade that wants to execute
303:51 - it will ask the semap for that hey I
303:53 - want to be executed so in that case
303:57 - simaf for will check that does it have
304:00 - any permit or not so if there is a
304:02 - permit that permit will be given to a
304:05 - threade so basically this permit is like
304:08 - a token and once a thread acquires it
304:11 - then it's going to do its processing so
304:14 - this is what we mean when we say that at
304:17 - this moment this semop 4 is going to
304:19 - allow three threads to be run to begin
304:23 - with we have thread one which wants to
304:25 - execute so it calls acquire on the
304:28 - semaphore the acquire method is like
304:31 - it's requesting for a permit now that at
304:34 - the given instant we have three permits
304:36 - available the threade one is going to be
304:40 - allowed and it will execute and access
304:42 - the third party service so essentially
304:45 - thread one comes and it says Hey I want
304:47 - one permit to be executed sea 4 is like
304:50 - all right I have three permits here you
304:53 - take this one and you can proceed so
304:56 - once it's given you could imagine that
304:59 - this permit is no more so essentially
305:01 - you have two permits left and once this
305:04 - permit is assigned to thread one it goes
305:07 - ahead with its execution so thread 1 is
305:10 - essentially allowed to execute so once
305:13 - thread one has been given the permit
305:15 - Sima 4 will have two available permits
305:18 - and while thread one is executing and
305:20 - accessing the service let's say we have
305:22 - another threads which are threade two
305:25 - and trade three which come to SEMA 4
305:28 - asking for permits and at this moment
305:30 - the SEMA 4 has two permits available so
305:33 - it will give the permits to threade 2
305:35 - and threade three and essentially after
305:38 - these permits are given to to threade 2
305:40 - and threade three they also go about
305:42 - with their execution so threade one
305:44 - starts with its execution and so does
305:47 - threade 2 and threade three after
305:49 - acquiring the
305:51 - permit now let's imagine at some point
305:53 - of time in Future threade 4 comes into
305:56 - picture and it's going to the SEMA 4 and
306:00 - say hey SEMA 4 I want to execute and in
306:03 - that to happen I want to acquire the
306:06 - permit from you so at this moment SEMA 4
306:09 - is is like all right that's fine that
306:11 - you want to get executed but I don't
306:13 - have a permit with me as of now so what
306:15 - you can do is you could wait and due to
306:18 - this threade 4 gets blocked so threade 4
306:22 - had called the acquire method and it's
306:24 - going to get blocked on the acquire
306:26 - method and sometime in future when
306:28 - thread one is done with its
306:30 - execution it will say all right I am
306:33 - done with my execution now I am going to
306:35 - release the permit that I had acquired
306:38 - so the permit will be given back to the
306:41 - SEMA 4 and as soon as that permit is
306:44 - given back to the SEMA 4 the threade
306:47 - which was waiting for a permit in the
306:49 - SEMA 4 will acquire it and proceed with
306:52 - its execution so let me revert back the
306:55 - image and now we have all the three
306:58 - permits available with the SEMA 4 so at
307:00 - some point of time Trade four got a
307:03 - chance to
307:05 - execute by acquiring the permit so this
307:08 - is the way in which SEMA 4 works so
307:11 - let's understand what exactly is this
307:13 - release method well thre says Hey SEMA 4
307:16 - I'm done with my execution thus I'm
307:19 - returning back your token your permit
307:22 - that you gave me earlier and you could
307:24 - make use of it as per your wish so you
307:27 - could understand that SEMA for permits
307:30 - are nothing but kind of reusable tokens
307:33 - in that sense so if you notice at any
307:36 - given point of time in this semaphore we
307:39 - can only have three threads executing at
307:43 - maximum and that is what SEMA 4 is used
307:46 - for so essentially what we have done is
307:49 - we have restricted the count of Trades
307:51 - which are going to access the third
307:53 - party service using your Sor now that we
307:56 - have understood how exactly SEMA 4 is
307:58 - working under the hood let's have some
308:01 - quick code demonstration which will
308:03 - showcases how we can make use of the
308:05 - SEMA for in our
308:08 - code so here I am in the IDE I will
308:12 - create a new class for the demonstration
308:14 - but before I create the class let me
308:17 - tell you briefly what exactly I'm
308:19 - planning to build so I'm planning to
308:21 - build a small use case where we will be
308:26 - making use of some third party service
308:28 - to scrape the data don't worry it's not
308:31 - going to be something difficult it's
308:33 - rather just a dummy kind of method the
308:36 - entire idea is to Showcase how we can
308:39 - use the semaphore rather focusing on the
308:42 - scripting part of it so let's get
308:44 - started and I will create a class I'll
308:48 - call this as a
308:49 - scraper
308:52 - and let's have this m method to begin
308:56 - with and I will create the scrape
308:59 - service which is the service which will
309:03 - be called by the scraper
309:05 - class
309:07 - so just as a good practice I am going to
309:12 - make the scrape Service as a Singleton
309:14 - class not that it's needed for this
309:17 - demonstration but just as a good
309:19 - practice let's have it as a singl ton
309:21 - and let's have this instance and uh what
309:24 - we will be needing is a seap for so
309:26 - let's initialize the semaphor it
309:30 - provides a Constructor wherein we can
309:32 - pass some value so this semap 4 is going
309:36 - to have three
309:37 - permits and we we have a method that is
309:40 - called
309:42 - scrape what this scrape is going to do
309:46 - is let's going to run in a TR catch
309:50 - block let's have this interrupted
309:52 - exception and this will throw a new
309:55 - runtime exception and we will also have
309:58 - a finally why we will look at it in some
310:02 - time so to begin with this thing is
310:05 - going to acquire the SEMA 4 so it will
310:08 - call the do acquire method on the semap
310:11 - 4 and then we have a helper method which
310:14 - is something like
310:16 - invoke scrape
310:20 - bot so invoke a scrape bot is the actual
310:23 - third party service invocation wherein
310:27 - we are going to call the third party
310:29 - service and uh do the scrapping
310:32 - task
310:34 - so we will implement this method in some
310:37 - time but for now let's have the
310:38 - signature
310:39 - here and imagine that uh some thread has
310:43 - acquired the same of 4 permit and it got
310:46 - terminated due to some reason or maybe
310:48 - it went into a deadlock situation so
310:51 - what happens is the Sim for permit is
310:53 - acquired forever but we don't want that
310:56 - to happen right what we want essentially
310:58 - is that once a thread is done with its
311:01 - execution it should be releasing it so
311:03 - similar to the way in which we used to
311:05 - acquire and unlock the logs right we
311:08 - will be using the try and finity block
311:11 - and in The Finity block we will say SEMA
311:14 - 4 do
311:16 - release idea is that in either case of
311:20 - success of failure the execution inside
311:24 - the finary block will take place and
311:27 - that is when we are going to release the
311:31 - semaphore so let's implement the invoke
311:35 - script B method so it's private void
311:39 - invoke script part and this is nothing
311:43 - this is just going to print some message
311:45 - so let's have the message
311:47 - as
311:49 - scrapping
311:51 - data and to mimic some sort of uh
311:54 - execution time we will
311:57 - have some sleep introduced in the code
312:01 - let's say this is for 2 seconds and
312:04 - since we have given thre do sleep we
312:07 - need to handle the exception of
312:08 - interruption
312:10 - and the way we will do this is we will
312:12 - throw a runtime exception with the
312:15 - exception not the best way to do it this
312:18 - should do for the demonstration purposes
312:21 - so now let's go to the main method and
312:24 - try to make use of this so what I'll be
312:27 - doing is I'll be creating a new cast
312:30 - threade pool and using that cast threade
312:33 - pool I'll be submitting the tasks for
312:36 - the script service so let's to
312:40 - that so I'll be making use of the try
312:42 - with resources pattern so executor
312:46 - service
312:48 - service and executors do
312:52 - new cached trade
312:56 - pool and
312:58 - uh let's say I want to do this for 15
313:04 - times and service.
313:08 - execute new
313:10 - runable and this will call the scrape
313:15 - service
313:19 - scrape so what we have done is we are
313:22 - creating uh this service that is the new
313:26 - cast threade pool and we are supplying
313:30 - this runnable we are calling
313:33 - this script service method inside this
313:35 - runnable 15 times so what we would
313:40 - expect is that at any given point of
313:43 - time only three threads are executing so
313:47 - let's run this and see the output so
313:49 - here is the output we have a scraping
313:52 - data then we have a scraping data then
313:54 - we have a scraping data finally we have
313:56 - a scraping data so what is happening
313:59 - essentially is out of all the 15 times
314:02 - we have submitted the runnable we are
314:05 - just allowing three threads to execute
314:09 - simultaneously so scraping data is
314:12 - printed twice scraping data is printed
314:14 - twice and so and so forth so this is
314:18 - kind of demonstrating that using a SEMA
314:20 - 4 we are restricting how many trades can
314:24 - run at a given point of time please note
314:27 - that how many trades will be actually
314:29 - running at a given point of time will
314:31 - also depend on how many course you have
314:34 - and all those things but the whole idea
314:36 - is that if you use a semop for and some
314:39 - sort of permit then in the worst case
314:43 - possible at Max only those many threads
314:46 - will be running at a given point of time
314:48 - and that is the whole intention and idea
314:51 - behind using the semaphor so now that we
314:53 - have seen the code demonstration let's
314:55 - learn a few more things about the semap
314:57 - for we also have a concept of multiple
315:00 - permits please note that we can acquire
315:03 - multiple permits at a time for this we
315:06 - have an overloaded method of acquire
315:09 - which allows us to enter the number of
315:11 - permits we want so when we call S 4.
315:14 - acquire you can pass a value so let's
315:16 - say Sima for acquire and you pass a
315:19 - value of two so two permits out of three
315:22 - will be taken by the threade however we
315:25 - should always ensure that the number of
315:28 - permits we take should be equal to the
315:30 - number of permits we are returning back
315:33 - by calling the release so like we had
315:36 - passed the number of permits in the
315:38 - acire method we also need to pass the
315:40 - number of permits in the release method
315:42 - as well so if do acquire is two do
315:45 - release should also be two and uh there
315:48 - are some other methods in semap 4 as
315:50 - well let's learn about those the first
315:53 - one is try acquire so when this method
315:56 - is used the thread will try to acquire
315:58 - the permit if there is no permit
316:01 - available the thread won't be blocked
316:03 - rather it can do something else then we
316:06 - have try acquire with timer out which is
316:10 - essentially the same as TR acquire
316:12 - except it accepts some sort of timeout
316:15 - the next one is the available permits
316:17 - and as the name suggests the available
316:20 - permits Returns the number of available
316:22 - permits with a given semaphore the final
316:25 - one on our list is this new semop 4
316:29 - which accepts count and fairness well
316:32 - this is an overloaded Constructor of the
316:34 - semaphor it accepts a fairness criteria
316:37 - which is a Boolean value
316:39 - and when given as true the threade
316:41 - waiting the longest gets the chance to
316:44 - acquire the permit once it's available
316:47 - with the semap 4 so that's all about the
316:49 - semap 4 it's very easy and intuitive to
316:53 - understand essentially it's a mechanism
316:55 - to introduce a controlled bottleneck
316:57 - situation to ensure that there is an
317:00 - upper limit on how many threads can work
317:03 - on a shared resource at a given point of
317:05 - a Time
317:13 - let's learn about the mutex so what is a
317:15 - mutex mutex is a short form of the word
317:19 - mutual exclusion it is a synchronization
317:22 - mechanism used to control access to a
317:25 - shared resource in a multi-threaded
317:26 - environment in Java the primary purpose
317:29 - of a mutex is to ensure that only one
317:32 - thread can access a critical section or
317:34 - shared resource at any given time it
317:38 - prevents raise conditions and ensures
317:41 - data
317:42 - consistency wait a second hold on are
317:45 - you thinking that it sounds very similar
317:47 - to some other Concepts we learned
317:49 - earlier related to multi-threading well
317:52 - you are very correct if you are thinking
317:54 - so mutex is nothing but a fancy term for
317:57 - locks and synchronized blocks moreover
318:00 - the concept of mutex encompasses any
318:03 - mechanism that ensures any sort of
318:06 - mutual exclusion it means only one
318:09 - thread can access a critical section at
318:11 - a time in Java this concept is realized
318:15 - through synchronized blocks and lock
318:17 - interface you may have another doubt you
318:20 - may be thinking if the mutex is indeed
318:23 - just a fancy term for locks and
318:24 - synchronization why do we have this
318:26 - concept at all well the reason is
318:29 - somewhat historical in nature the term
318:32 - mutex is a fundamental Concept in
318:34 - computer science and concurrent
318:36 - programming which predates this specific
318:39 - implementations found in languages like
318:42 - Java the concept of a mutex is used
318:45 - widely in both theoretical and practical
318:48 - contexts to describe a mechanism which
318:51 - ensures Mutual exclusion and thereby
318:54 - preventing multiple threats from
318:56 - accessing a shared resource
318:57 - simultaneously so effectively mutex is
319:00 - the term for General concept of mutual
319:02 - exclusion in context of multi-threaded
319:05 - environment and the approaches like
319:08 - using a blocks and synchronized blocks
319:11 - Etc are tools which help us in achieving
319:13 - this requirement of mutual exclusion so
319:16 - if you understand any of the concepts
319:18 - like lock synchronization Etc which
319:20 - provide Mutual exclusion of a shared
319:22 - resource you very well understand mutex
319:26 - so that's all about mutex there is
319:27 - nothing new to learn here as such
319:29 - because we have already covered the
319:31 - concepts related to Locks and
319:33 - synchronized blocks in Java
319:42 - Fork join framework is another important
319:45 - Concept in Java let's learn about the
319:48 - same the fork join framework is a
319:50 - concurrency framework which was
319:52 - introduced in Java 7 to simplify the
319:54 - process of parallel programming it is
319:57 - designed to take advantage of multicore
319:59 - processors by dividing tasks into
320:02 - smaller subtasks executing them in
320:04 - parallel and then combining their
320:06 - results together the fourth joint is
320:08 - very similar to executor service which
320:11 - we learned about
320:13 - earlier in the interest of refreshing
320:15 - your memory executor service mainly
320:18 - consists of some sort of data structure
320:21 - which stores the task to be executed and
320:24 - there is a pool of stes which pick the
320:27 - tasks from this data structure and work
320:30 - on the task depending on the use case
320:33 - you could just execute a task and not
320:35 - return anything or you could return a
320:37 - value from the execution of the task so
320:40 - in these two aspects the fork joint
320:43 - framework is exactly the same as the
320:46 - executor service however there are
320:49 - certain dissimilarities between the
320:51 - executor service and folk join framework
320:54 - the fork join is different from the
320:56 - executor service in the aspects of
320:59 - subtask creation and the fork join
321:02 - framework can create subtasks which is
321:05 - not the case with the executor service
321:08 - now let let's learn about the
321:09 - differences between the FK join
321:11 - framework and the executor service the
321:14 - FK joint framework differs from the
321:15 - executor service in two aspects the
321:18 - first one is the task producing subtask
321:21 - and the second one is per threade
321:23 - queuing and work Stealing In the case of
321:25 - folk joint framework a task is capable
321:28 - of creating subtasks to simplify the
321:31 - problem being solved consider this as
321:33 - the standard divide and conquer approach
321:36 - which is not the case with the executor
321:38 - service
321:39 - unlike the executor service where there
321:41 - is a shared task Q the threads in the
321:44 - for joint framework have their dedicated
321:46 - task queue there is also this mechanism
321:49 - of load balancing where a threade could
321:51 - pick task from the other queue this
321:53 - approach is called as work stealing now
321:56 - it's time to learn why do we have for
321:59 - join framework you may be wondering why
322:01 - do we have yet another framework on top
322:04 - of all the things that we have learned
322:06 - so far in the multi-threading well well
322:09 - Fork joint framework is not there
322:11 - without any reasons below are a few
322:13 - reasons why the fork joint framework was
322:16 - provided as part of the Java the first
322:18 - one is the utilization of multi-core
322:21 - processors modern processors have
322:23 - multiple cores and traditional
322:25 - single-threaded or poorly managed
322:27 - multi-threaded applications do not fully
322:30 - utilize the computational power of these
322:33 - processors the FK join mechanism
322:35 - efficiently manages multiple threads to
322:38 - leverage all the available CPU CES the
322:41 - next one is simplified
322:43 - parallelism writing parallel code
322:45 - manually is a complex and error prone
322:48 - activity the fork join mechanism
322:51 - abstracts out much of this complexity
322:54 - and it makes it easier to develop
322:56 - parallel applications finally on the
322:59 - list of why do we need Fork joint
323:01 - framework we have this concept of
323:03 - efficient work stealing algorithm the
323:05 - framework uses a work stealing algorithm
323:07 - where idle work worker threads still
323:09 - task from The Busy threads this ensures
323:13 - that all the threads remain productive
323:15 - and proove the overall performance so as
323:18 - I told earlier that we have qes on per
323:21 - threade basis and let's say a threade is
323:24 - done with its execution of the tasks in
323:26 - its queue so in that case that threade
323:28 - could dis steal work from the other
323:31 - threads or from the other queue where it
323:33 - has some other tasks aligned to be
323:36 - executed it's very equivalent to the
323:38 - mechanism of load balancing so you see
323:41 - the fork joint framework has a certain
323:43 - pattern to it and due to this we have
323:46 - certain use cases which fit very well
323:48 - with the users of this framework for
323:51 - example sorting a large data set
323:53 - performing operations on large matrices
323:55 - processing large collections of data in
323:57 - a parallel fashion and so on so forth
324:00 - now it's time to learn about the key
324:02 - Concepts in the fork joint framework
324:05 - forking in the fork joint framework is
324:07 - the process of break breaking down large
324:09 - task into smaller independent subtasks
324:12 - which can be executed
324:14 - concurrently this is achieved using the
324:17 - fork method provided by the fork join
324:19 - task class the next concept is that of
324:22 - joining what it refers to is the process
324:24 - of waiting for the completion of a folk
324:26 - task and combining its results this is
324:30 - done using the join method then we have
324:33 - the recursive task the recursive task is
324:36 - an abstract class and it is used for the
324:39 - task which return a result it is
324:42 - parameterized which means that the type
324:45 - is the type of the result which is
324:47 - produced and returned by the task so
324:49 - whenever there is a need to compute a
324:51 - result and return it after completing
324:53 - the task the recursive task class should
324:56 - be used Loosely speaking you could
324:58 - imagine this to be parallel to the
325:00 - callable interface then on the list we
325:02 - have recursive action it is used for
325:05 - tasks which don't return any result it
325:08 - does not accept any types when the task
325:11 - performs an operation which does not
325:13 - need to return a value or a result it
325:16 - should use recursive action and Loosely
325:18 - speaking you could imagine this to be
325:20 - parallel to a runable interface so what
325:23 - is a fog join pool let's learn about the
325:26 - FK joint pool and I promise that this is
325:28 - going to be the last theoretical Topic
325:30 - in this section before we move to the
325:32 - more juicy implementational side of fog
325:35 - joint framework well fog joint pool in
325:38 - Java is a specialized implementation of
325:40 - the executor service interface and it is
325:42 - designed so to support the folk joint
325:45 - framework it is optimized for parallel
325:47 - processing and efficient management of
325:49 - tasks which can be broken down into
325:51 - smaller subtasks below are some key
325:54 - Concepts and aspects which are related
325:57 - to the F joint pool we have this concept
325:59 - of work steing algorithm so F joint pool
326:02 - uses a work stealing algorithm to
326:04 - efficiently manage and balance the
326:06 - workload among threads when a thread
326:09 - finishes its task it can steal task from
326:12 - the work cues of the other threads
326:15 - ensuring that all threats remain
326:17 - productive and it reduces the ideal time
326:19 - as well next we have parallelism so the
326:22 - pool can automatically determine the
326:24 - level of parallelism based on the number
326:26 - of available processors you can also
326:29 - specify the level of parallelism when
326:31 - creating the fork joint pooling stance
326:34 - so we have two operations like fork and
326:36 - Joint so the tasks submitted to the fork
326:39 - join pool can be split into smaller
326:41 - subtasks and their results can be
326:43 - combined together this mechanism
326:45 - supports the divide and conquer approach
326:48 - for solving the complex problems finally
326:51 - The fol Joint pool is designed to manage
326:53 - the instances of the FK Joint Task and
326:56 - its subclasses which are recursive task
326:59 - and recursive action so these tasks
327:01 - incapsulate the logic for splitting the
327:04 - work and combining the results so in
327:07 - nutshell to summarize the theoretical
327:09 - section what we have is a fol join
327:12 - framework and fol join pool is the
327:14 - implementation of it so essentially FK
327:16 - joint pool is a special implementational
327:19 - case of the executor service where the
327:21 - focus is around the parallelism and it
327:24 - does so by utilizing two concepts first
327:27 - is Fork other is join so Fork is
327:30 - basically creating the subtasks creating
327:32 - the sub of a particular task and join is
327:36 - like collecting the results from the fog
327:39 - subtasks so essentially what we are
327:41 - doing using the fog joint framework is
327:44 - dividing the tasks into subtasks and
327:46 - then asking the different threads to
327:48 - work on the subtasks in parallel and
327:51 - finally the result is collected and the
327:53 - final outcome is given back to the
327:55 - caller so in a way it's a parallel
327:58 - implementation it's a parallelized
328:01 - implementation of the divide and conquer
328:04 - strategy so now let's move forward and
328:06 - now that we have learned most of the
328:07 - basic the concepts related to the for
328:09 - joint framework we shall start with some
328:12 - code implementation for these Concepts
328:14 - so I will start by creating this package
328:18 - let's call this package as Fork join and
328:22 - as I mentioned earlier that we have two
328:24 - classes one is the recursive task other
328:26 - is the recursive action so this
328:28 - particular demonstration would be for
328:30 - the recursive task in the next one we
328:32 - will see how we can make use of the
328:34 - recursive action so what I'm planning to
328:36 - implement is a code wherein I would like
328:38 - to search for occurrence of a given
328:41 - number into an integer array so let's
328:44 - start with the implementation so let's
328:46 - create this class and let's call this at
328:52 - search
328:53 - occurrence task this is going to extend
328:58 - the recursive
329:00 - task and this is going to return
329:05 - integer so basically integer is is the
329:08 - type which it accepts so if you see the
329:14 - implementation it's going to extend the
329:16 - fog Joint Task and the fog Joint Task is
329:20 - going to implement future so now going
329:22 - back to the code it's giving certain
329:24 - error so the idea is that it's enforcing
329:27 - to implement certain unimplemented
329:28 - methods so let's do
329:31 - that and the method that we need to
329:33 - implement is called as compute and by
329:37 - the virtue of the type that we provide
329:39 - which is the integer it's going to
329:41 - return integer from this compute Method
329:45 - All right so this particular class will
329:47 - have a few things for example first is
329:50 - it will have an integer array called as
329:53 - ARR then it will have a start index then
329:57 - it will have an end index and it will
330:00 - have a value which is called as search
330:03 - element so let's create the Constructor
330:06 - for this class and we are going to
330:08 - initialize all these values so to begin
330:12 - with we just have this compute method so
330:15 - let's say I am going to provide a
330:17 - private method which is going to search
330:19 - for the occurence in this particular
330:22 - array for a given element so let's call
330:24 - that method as
330:26 - search and uh let's implement it and
330:30 - probably we will return the
330:32 - value so private integer search
330:38 - and uh let's have the value as zero to
330:42 - begin
330:43 - with and uh what we will do is we will
330:47 - start from the start index then it will
330:52 - go till the end
330:55 - index and we increment the index value
330:59 - if the value at index I happens to be
331:03 - the search
331:04 - element then we increment the count
331:08 - and finally when we are done iterating
331:11 - over the array we are going to return
331:13 - the count
331:14 - value so this is the search method so
331:17 - please note that as of now we are not
331:19 - making use of any fog join
331:22 - Concepts so we will introduce the fog
331:24 - join Concepts in a while but for now
331:27 - let's complete the
331:28 - implementation so this is about this
331:30 - class and let's have the runner class
331:33 - wherein we will be invoking this
331:35 - particular compute method somehow that
331:39 - we will see in a moment using a main
331:41 - method so let's create a class and let's
331:44 - call this as F join pool
331:49 - demo and we have a main method
331:52 - so we will have an
331:56 - array let's say the value is up to 100
332:00 - and we will have a random variable why
332:03 - we are going to need it we will come to
332:05 - see it in a moment and we populate this
332:09 - array with certain random
332:11 - values so for each and every array index
332:16 - we are going to provide certain
332:20 - value and that will be random dot next
332:25 - int so that is 10 + 1 so essentially I
332:31 - am going to populate each index in the
332:34 - array with a value ranging from 1 to 10
332:39 - and we have a search
332:42 - element which is again a random value So
332:45 - Random dot next
332:46 - int then we have
332:50 - 10 +
332:53 - 1 and what we are going to do is we are
332:56 - going to create a for join
332:59 - pool so new for join pool and remember
333:04 - that I told in the theory section that
333:07 - if you don't Pro provide the number of
333:09 - processors it will by default figure out
333:12 - how much parallelism is required but in
333:15 - this case let's provide the number of
333:17 - processors as well so runtime Dog
333:20 - runtime. available
333:22 - processors and it's complaining that we
333:25 - should close this resource so probably
333:27 - we can go ahead with drive with
333:30 - resources all right now what we need to
333:33 - do is we need to make use of the fne
333:35 - pool and the way to do this is we need
333:38 - to create a instance of this task which
333:41 - has extended this recursive task so
333:43 - let's do that let's create an instance
333:45 - of this task which is search occurring
333:48 - task and let's call this as task new
333:52 - search occurrence task let's pass the
333:55 - values which is the array start would be
333:58 - zero end would be array do
334:02 - length minus one these are the indices
334:05 - of the array then the search element
334:08 - and we are going to call pull do invoke
334:13 - and here we will pass the for Joint
334:16 - Task and let's capture this as the value
334:20 - of
334:21 - occurrence all
334:24 - right so let's print the value that the
334:28 - array
334:30 - is arrays
334:32 - dot two
334:35 - string AR
334:38 - and finally system out print
334:41 - Ln percent
334:43 - D found percent D
334:48 - times then search element and
334:53 - occurrence and it should be print F not
334:56 - print Ln so let's correct
334:59 - this and uh now let's run this code and
335:02 - say its
335:03 - output so please note that we have not
335:06 - made use of any fol joint pool concept
335:08 - as of now we will do so in a
335:12 - moment so let's run
335:15 - this and we can see that the value 9 has
335:19 - been found six times so you could go
335:22 - ahead implement this code or probably
335:24 - copy the code from the GitHub link that
335:25 - will be shared in this video and you
335:27 - could try to execute this and from this
335:30 - particular value you could validate it
335:32 - so it's working fine I have validated it
335:34 - now what we will do is we will try to
335:37 - reduce the folk and join Concepts so in
335:40 - order to do so what we do is we go to
335:43 - the compute method and we first of all
335:46 - compute the size of the array which is
335:49 - end minus start +
335:52 - 1 and if the size happens to be greater
335:55 - than 50 then we are going to do some
335:58 - operations
336:00 - otherwise I am just going to call the
336:03 - vanilla search method which is return
336:06 - this search method method that we were
336:08 - using anyway so when the size happens to
336:11 - be greater than 50 we will divide this
336:13 - task in two halves and the way to do
336:16 - this
336:17 - is find the middle index say start plus
336:21 - End ided by
336:23 - two and in order to create the fork of
336:27 - the task we will have to first create
336:30 - the task objects so what we do is we can
336:33 - create the task object which is search
336:35 - uring task let's call this as task one
336:39 - then new search occurrence task pass the
336:43 - value the start value is the start index
336:46 - the end value becomes the mid and then
336:49 - we have the search
336:51 - element next we create the other task
336:54 - which is Task two and then we call
336:57 - search aen task then we have array and
337:01 - the start value becomes the mid + one
337:04 - and then the end value becomes the end
337:07 - and then then we have the search element
337:09 - so if you are familiar with the concept
337:11 - of binary search or any divide and
337:14 - concur algorithm implementation you
337:16 - would find it very similar so
337:18 - essentially what we are doing is let's
337:20 - say there is an array with some values
337:23 - right something like this so what we are
337:26 - doing is we have to do some processing
337:29 - some operation on this array so
337:30 - essentially what we are saying is that
337:32 - all right let's say if I have these many
337:35 - values which is let's say six values so
337:39 - I don't want to do operations on the
337:42 - entire array rather what I want to do is
337:45 - I want to split it so if I were to
337:48 - search this array for all the six
337:50 - elements it will take some time what if
337:52 - if I wanted to search for some value on
337:55 - this particular array itself it will
337:57 - take less time right because we are
337:59 - doing the search operation on a smaller
338:02 - sized array so that is the whole concept
338:05 - where we are finding the mid value which
338:07 - is let's say somewhere here and from
338:11 - here to here this is my first section of
338:13 - the array in which I need to do the
338:15 - search and this is the second section of
338:17 - the array in which I need to do the
338:19 - search and this needs to be done till
338:22 - the time this threshold is not M so for
338:25 - example let's say this array is somewhat
338:28 - of let's say bigger size maybe 100 or
338:31 - 200 200 then in that case first split
338:33 - would be 100 100 second split would be
338:35 - 50/50 and when the size is less than 50
338:39 - then in that case it will come to this
338:41 - section and it will say all right the
338:43 - size is less than 50 the size is less
338:46 - than the threshold then in that case we
338:48 - can go ahead and do the normal search
338:51 - but till the time size is greater than
338:53 - the threshold that is 50 in this case it
338:56 - will go and break the task into smaller
338:59 - tasks and that is where we have made use
339:01 - of this task so the next process in this
339:05 - implementation would be to call Fork
339:09 - method on these tasks so task one.
339:13 - Fork then task two do Fork so what
339:17 - happens is that this
339:20 - task. Fork is going to create another
339:23 - subtask so if you click on Fork you can
339:26 - see that it is going to return the fork
339:28 - Joint Task itself which is basically
339:31 - this recursive task whatever
339:34 - implementation you have here since this
339:36 - class is imple minting recursive task
339:39 - then this particular Fork will also
339:41 - return this recursive task which is the
339:43 - search occurrence task but for a lesser
339:45 - search space right so if that is clear
339:49 - let's move ahead and understand how we
339:51 - can face the value from this subtasks
339:54 - that we have
339:55 - created and the way to do that is by
339:57 - calling the join operation so in a way
340:00 - you could try to correlate this with the
340:02 - way in which the recursion works right
340:05 - and it is pretty much evident by the
340:07 - name as well so basically it's nothing
340:09 - but a kind of recursive call in a
340:11 - multi-threaded context wherein you have
340:14 - created different subtasks and all those
340:17 - smaller subtasks could be picked up by
340:20 - different threads and could be executed
340:22 - in parallel so it's nothing but some
340:25 - sort of recursion on stereoids due to
340:27 - reason that we are making use of
340:29 - parallelism and multi-threading and all
340:31 - those Concepts which are going to give
340:33 - us some boost with respect to the
340:35 - performance so now let's move and see
340:38 - how we can collect the value so the way
340:40 - to collect the value out of a task is by
340:43 - calling dot join method so let's say
340:46 - task one do
340:49 - join if you try to capture it it will be
340:53 - some integer why integer because that is
340:55 - the parameter that we have provided here
340:59 - so you could do this way or probably we
341:02 - could just simplify it by returning task
341:06 - one do join plus task 2.
341:12 - join and that's the whole implementation
341:15 - so now let's run this code and see the
341:18 - output after executing this code I will
341:20 - give you an another work through so
341:22 - things will be much more
341:25 - clearer so see this value has been
341:29 - found this many times so basically 10
341:32 - has been found these many times so
341:34 - basically what we have done here is the
341:37 - that we have implemented one of the
341:39 - implementations of the task which is
341:43 - this recursive task and one of the
341:45 - implementations of the fork Joint Task
341:49 - and
341:50 - uh we have certain data structure on
341:53 - which this task is going to be performed
341:57 - and the data structure of the choice
341:58 - here is an integer array and our use
342:01 - case is to find how many times a given
342:05 - value is going to be repeated or is
342:07 - going to occur in the given array and
342:10 - that is the search part of it and that
342:12 - is what we had in the beginning then
342:14 - what we did is we started with the
342:16 - implementation for the compute and in
342:19 - the compute we first found out what is
342:22 - the current size of the array on which
342:24 - we are performing this
342:25 - operation and we had a check that if the
342:28 - size happens to be less than 50 then we
342:31 - are simply going to make use of this
342:34 - search method and if that is not the
342:36 - case then we are going to break this
342:38 - larger task into smaller tasks smaller
342:42 - subtasks and the way to break this is
342:45 - find the middle point in the array the
342:47 - middle index in the array and then
342:50 - create two tasks out of it wherein we
342:53 - have the array itself then we have a
342:55 - start then we have a mid value and the
342:58 - element that we need to find and in the
343:01 - second task we are going to have the
343:03 - same array and the search space is
343:06 - provided from the mid + one as the start
343:09 - value and end as the end value and
343:12 - search element is any way the element
343:15 - that we need to find and after we have
343:17 - created this template for these two
343:19 - tasks we are going to call Fork on these
343:23 - two tasks and when a fork is called what
343:26 - essentially happens behind the scene is
343:28 - this thing is called recursively with
343:31 - this particular search space this
343:34 - particular search space so the search
343:37 - space is going to reduce it's going to
343:39 - get smaller and smaller and likewise
343:42 - when this Fork is called similar thing
343:44 - is going to happen and finally this will
343:47 - keep on happening till the time we have
343:49 - the size the search space greater than
343:52 - 50 and after some time the recursion
343:54 - will reach its base case and then we
343:57 - will like to retrieve the value from
344:00 - that recursive task that we have
344:02 - executed and the way to do so is by
344:05 - calling Dot join there are some other
344:08 - methods as well but in this example we
344:11 - are going to stick with DOT join so
344:14 - let's say task one has accumulated
344:17 - certain value and task two has
344:19 - accumulated certain value so both the
344:21 - values will be added because consider
344:24 - this as a binary tree where we are
344:26 - making calls in two directions in the
344:29 - left side and in the right side and once
344:31 - you are done with the traversal at every
344:33 - point of time from every node you are
344:36 - going to get some value so that is what
344:39 - the intent is and that is the way to
344:41 - visualize this code so that is how we
344:46 - are going to make use of the recursive
344:48 - task to implement Fork join framework
344:52 - and do this operation so before we move
344:54 - to the next implementation which is for
344:56 - the recursive action I would like to
344:59 - give you a small homework and what you
345:01 - could try is you could try to implement
345:04 - this for the use case where we wish to
345:07 - find out the summation of all the
345:09 - elements in a given array so it would be
345:12 - something similar to this but I think
345:14 - you should try it out on your own and it
345:17 - would really help you in cementing the
345:19 - concepts related to the recursive tasks
345:22 - so now let's move to the implementation
345:24 - of the other thing which is the
345:26 - recursive action so let's create an
345:29 - another
345:30 - class and uh let's call this as
345:35 - workload splitter
345:40 - and what I'm trying to build here is
345:43 - that there is a class and it has some
345:45 - workload and based on certain workload
345:48 - it's going to either do its operation on
345:51 - its own or else it's going to split the
345:54 - workload or the task rather into two
345:57 - halves so this is what we are going to
345:59 - implement so let's start with the
346:01 - implementation and this workload
346:03 - splitter class is going to extend
346:06 - recursive
346:07 - action and as I told earlier in the
346:10 - theory section that this recursive
346:12 - action is not going to accept or expect
346:15 - any sort of parameter and uh neither
346:18 - does it return anything so let's
346:21 - implement the unimplemented methods and
346:25 - again the unimplemented method is
346:27 - compute the only difference is that
346:29 - since it does not return anything we
346:31 - have a void over here there is nothing
346:33 - to return right
346:36 - so let's have a
346:40 - value and maybe call this as a
346:44 - workload let's have a Constructor
346:47 - initialize this
346:49 - value and in the workload what we are
346:51 - going to do is we are going to inspect
346:55 - the value of workload if the value
346:58 - happens to be greater than 16 then we
347:00 - are going to do certain things
347:03 - otherwise this particular task is going
347:06 - to be implemented
347:07 - without any splitting so let's print
347:09 - some message we are not going to have
347:11 - any fancy implementation as such rather
347:14 - we are just going to print certain
347:16 - messages to explain the
347:19 - concept so work load within limits task
347:25 - being
347:27 - executed for
347:29 - workload and then the workload value and
347:33 - what happens when the workload is
347:35 - greater than 16 well
347:37 - first start by printing some
347:40 - message
347:41 - workload too
347:43 - big thus
347:48 - splitting and then the workload is given
347:51 - in the
347:52 - message so first of
347:55 - all we
347:56 - find the first
347:59 - workload and this is going to be
348:01 - workload divided by
348:03 - two then we find the second
348:09 - workload and this is going to be
348:12 - workload
348:14 - minus first
348:16 - workload right so again very similar to
348:19 - the recursive task that we had in the
348:21 - previous section in the previous example
348:24 - rather we will create two tasks here for
348:28 - the workload splitter so maybe call this
348:31 - as first split then workload splitter
348:36 - pass the first workload
348:38 - value and then workload is
348:41 - splitter second
348:46 - split then call workload
348:48 - splitter provide the second
348:52 - workload and we can call first split do
348:58 - Fork then second split do
349:03 - Fork all right and that's it since we we
349:06 - are not intending to extract some value
349:09 - and that is the reason we have made use
349:11 - of the recursive action so we are not
349:13 - going to call any sort of join or any
349:16 - value that we need to fetch out of this
349:19 - so that is all with respect to the
349:21 - implementation here we are just making
349:23 - certain load balancing kind of mechanism
349:26 - wherein if needed if the workload is too
349:29 - high then in that case we are going to
349:31 - Fork two new tasks and those tasks will
349:34 - be working on that workload so let's say
349:36 - to begin with if the value is 64 then
349:39 - the value is certainly greater than 16
349:42 - then the workload will be divided as 32
349:44 - 32 so the first load is 32 second load
349:48 - is 32 again and again this value is
349:52 - greater than 16 so again the load will
349:55 - be divided into two so the first load
349:56 - becomes 16 second load becomes 16 and
350:00 - since that is the value we don't need to
350:02 - go inside this thing rather we will come
350:04 - to this thing and we will do the
350:06 - execution on its own so as I told
350:09 - earlier that there is no any execution
350:12 - as such rather it is just going to mimic
350:15 - certain Behavior around the workload by
350:18 - making use of this recursive action so
350:21 - now let's demonstrate this
350:24 - implementation and uh write the code for
350:27 - the same so let's create a class let's
350:31 - call this as workload
350:33 - split demo
350:36 - let's have the main method and uh we
350:39 - will create the fog join
350:43 - pool so let's call this as
350:46 - pool and new for join
350:49 - pool again let's pass
350:53 - the available
350:56 - course and again this will complain
350:59 - regarding an open resource so we can
351:02 - resolve this by putting inside the try
351:04 - with resources block and the way to use
351:07 - this is create an object for the
351:10 - recursive action class that we have
351:12 - created which is the workload
351:16 - splitter so let's call it as a
351:20 - splitter then workload splitter pass the
351:24 - value as
351:25 - 128 then call pool. invoke and let's
351:31 - pass the splitter object and that's
351:35 - it so so this is what we have
351:37 - implemented let's execute this task
351:41 - let's execute this code and see the
351:46 - output so here is what we have to begin
351:49 - with workload to Big this is splitting
351:52 - workload too big this is splitting and
351:54 - so on so forth so 128 got to split into
351:58 - two halves 64 and 64 32 got split into
352:02 - two halves 64 rather got split into two
352:06 - halves 32 and
352:08 - 32 and uh so basically 32 and 32 and
352:13 - this one is 32 and 322 so as I told
352:16 - earlier that these things are executed
352:19 - all these tasks are executed by threads
352:22 - in parallel so based on the thread
352:24 - scheduling you are seeing that we are
352:26 - having some sort of not in sync kind of
352:30 - response but the idea is that the number
352:33 - is getting split into two halves so 64
352:36 - 64 each 64 is getting split into two
352:39 - halves right one half here other half
352:42 - here and further the 32 is getting split
352:45 - into 16 and since 16 happens to be well
352:48 - within the reasonable limits for the
352:50 - workload we have this message that
352:53 - workload within the limits task being
352:55 - executed for the given workload so these
352:58 - are the messages that we are
353:00 - seeing so that's it about the
353:02 - implementation of the recursive action
353:04 - and that is the way in which you could
353:07 - implement it so that should be it for
353:09 - this particular topic of fork join in
353:19 - Java thanks for staying with me till the
353:22 - end of this video as per one statistics
353:25 - less than 10% people finish watching a
353:27 - tutorial video once they start watching
353:30 - well guess what you are the top 10% of
353:33 - the lot you are awesome so where Do We
353:36 - Go From Here I am hopeful that you
353:39 - learned a few important Concepts which
353:41 - lay the foundation's stone in learning
353:43 - the super awesome yet complex topics of
353:46 - multi-threading and
353:48 - concurrency I sincerely request you to
353:50 - start implementing these new found
353:52 - Concepts in your day-to-day software
353:54 - engineering activities if not at least
353:57 - try reading some code which makes use of
353:59 - the multi-threading Concepts doing these
354:02 - things will cement all the new skills
354:05 - that you have learned and acquired with
354:07 - me in the last couple of hours needless
354:09 - to say the concepts in this video are
354:12 - introductory and foundational in nature
354:15 - yet they are very important so what this
354:18 - means is there are even more Exquisite
354:21 - and complex topics which I did not cover
354:23 - in this video but whatever you learned
354:26 - should be good enough to get you started
354:28 - good news is that I am already working
354:31 - on an another video which will cover
354:33 - more advanced Topics in more details I
354:36 - hope you are excited so to summarize
354:39 - first thing is start implementing the
354:41 - concepts learned in this video then read
354:45 - code which makes use of the
354:47 - multi-threading and concurrency third
354:49 - step is watch out for the next video on
354:52 - the more advanced topics in multi-
354:54 - trading and most importantly did I tell
354:57 - you that I also have an YouTube channel
355:00 - here is the link please do subscribe it
355:02 - would mean a world to me
355:09 - with this I sincerely thank you once
355:11 - again for your time and attention and I
355:14 - hope that you found this video useful
355:18 - hope to connect with you again in my
355:20 - next video tutorial thanks for watching

Cleaned transcript:

multithreading is an important Concept in computer science in this course you'll learn everything you need to know about multithreading in Java but the concepts apply to other programming languages as well for each concept you'll learn the theory and then see some code examples hello and welcome I hope you're doing super good my name is rendu and I'm working as a senior engineer with Uber I have been programming for more than a decade now I believe that multithreading is one such concept which is way too abstract and difficult to understand if it is not taught in a proper manner however if the concepts are explained with relatable examples it becomes a fun and engaging experience that's what I have done in this course I have broken down the difficult and Abstract Concepts in simple English which is really easy to understand to make the things even more clear I have presented relatable examples I strongly believe that multithreading is one such tool which should be in the toolkit of every good programmer the entire video is split in a smaller sections wherein I teach about a particular topic each topic is explained with some theoretical concept followed by the examples and then I implement the topic of discussion in Java to give you a proper working code example the topics are taught in a bottomup manner where I start from the very Basics and then I build on the concepts layer by layer by the end of this tutorial you would become very confident and comfortable with the concepts of multithreading and that's my guarantee to you the code examples are in Java however most of the concepts should be transferable in other languages as well which support multithreading in some capacity so with that in place let's get started so what is the motivation for multithreading by default programming languages are sequential in nature code execution happens line by line in usual scenario consider the below code so in this method we have init de call then we have download data call then we have process on data and then finally show the results so in the usual scenario all these things will be executing one by one so first this will be called then this will be called then this then this but we have a problem in a single threaded program these instructions will be executed one by one and the time consuming section of the code can freeze the entire application what is the solution well figure out the timec consuming tasks and decide if they can be run separately if yes run such tasks in separate trades let's have a quick Layman explanation of how a timec consuming a step in your code can slow down or freeze your entire process let's say you invited your friend over to your place to watch this super cool movie being a great host you decided to make some popcorn for your friend but here is the catch it will take some 5 to 7 minutes to prepare the popcorn during the time you are involved in preparing the popcorn your friend asks which movie are we going toward wors today since you are super involved in making the popcorn you don't respond your friend even though feeling a bit weird about the situation asks you again if you're okay but thanks to your involvement in the process of making the popcorn you don't respond situation becomes super strange however your friend tries one final time and asks you if there did something wrong and thanks to your deep dedication in the process of popcorn making you don't respond by this time your friend gets freaked out and punches you in the face and you reboot but we all know this does not happen in real life unless you are playing a prank we humans are naturally equipped to multitask in this example since you would be aware of the time it takes to prepare popcorn you would probably prepare the recipe and put the pot on the stove and let the popcorn get prepared while it's getting prepared you are available ble to do anything if there is a need so you figured out the task which is going to be timec consuming started its execution and let it finish in its own line of execution effectively you did not block other tasks on you and did not freeze entirely if you follow line by line execution of tasks in your program this kind of freezing situation may arise in your code if there is a task which takes longer time to execute so what is the Improvement so in this case let's go through the different calls so in it de DB is where you are initializing certain DB related things then you have download data then you process the data then you show the results so to me it looks like downloading of the data is something which could take the major chunk of time what we can do now is put this download data in some sort of other threade and everything else in some other threade and in that sense we can do a parallel processing and it will ensure that by the time we are waiting for downloading the data everything else is not getting Frozen up and as a Sy is not lagging so this is one such Improvement we could do by the virtue of multithreading so to give it a formal definition multithreading is the ability of CPU to perform different tasks concurrently now let's have a quick explanation around concurrency versus parallelism concurrency is like having multiple tasks to do but you only have one set of hands you switch between the tasks doing a little bit of each one at a time if you play a guitar it's similar to that where you play different notes and cords using your nine fingers even though you play each note separately the switch is so fast and smooth that overall it appears as if everything is being played together parallelism on the other hand is again having multiple tasks but now you have many friends to help you out each friend works on a different task at the same time so all the tasks get done faster so in summary concurrency is doing multiple things all at once by quickly switching between the tasks and parallelism is doing multiple things at At Once by having different parts of the task been done simultaneously by different entities now let's learn about concurrency versus parallelism in somewhat more technical terms so concurrency and parallelism are two terms which are used quite a lot and that to interchangeably while discussing multi threading but there is a subtle difference let's talk more about it concurrency refers to the ability of a system to execute multiple tasks at the same time or nearly overlapping times so they seem like being executed at the same time in concurrent systems tasks may start execute and complete independently of each other but they may not necessarily be executing simultaneously at any given moment concurrency is often achieved through techniques like multitasking where a single processor switches between executing multiple tasks rapidly or through the use of multiple threads or processes parallelism on the other hand refers to the simultaneous execution of multiple tasks to achieve faster performance of increased throughput in parel system tasks are truly executed simultaneously either on multiple processors or multiple processor course or through other means of parall processing like distributed computing or GPU Computing parallelism is all about breaking down a task into smaller nonrelated subtasks which can be executed concurrently to speed up the overall execution time thus in the context of a hardware with a single CPU code currency could be understood as a perceived parallelism or fake parallelism even more so in scenarios where tasks appear to be running simultaneously but are actually being executed sequentially or in an interleaved manner this is done by something called as time slicing algorithm so in summary concurrency is about managing multiple tasks or processes potentially interleaving their execution to give an appearance of simultaneous execution whereas parallelism on the other hand is about truly executing multiple tasks or processes simultaneously to achieve a fast performance while the terms are related and often used together they refer to distinct Concepts in the context of computing now let's understand what is a process and thread process is an instance of program execution when you enter an application it's a process the operating system assigns its own stack and Heap area whereas threade is a lightweight process it is a unit of execution within a given program a single process may contain multiple threads each thread in the process shares the memory and the resources of the parent process one single process could contain many other threads now let's learn a bit about the time slicing algorithm let's imagine we have multiple threads associated with the process somehow the CPU has to ensure that all these threads are given a fair chance to execute one such approach is to use the time slicing algorithm so uses time for the CPU is shared among the different threads so here is what happens so you see sharing is time slicing let's say the green boxes represent one thread and the Yellow Boxes represent another thread thread T1 and T2 respectively and consider that this is the timeline and at this particular time thread T1 is assigned to the CPU then after some time thre T1 takes a break and we assign thread T2 to the CPU and after some time T2 is given some rest and thread T1 is assigned again to the CPU so as you see it's going into a back and forth manner where each and every threade is taking turns to run on the CPU one by one so here what we are doing is we are basically slicing the time and we are assigning certain time Quantum to the CPU so here we have a CPU and these are the two different threads which are kind of taking its turn to be executed on the CPU so this is how the time slicing algorithm works now what happens when we have enough CPU at our disposal so let's say we have thread one and we have thread 2 and there are two CPUs so in that case thread 1 will run entirely on CPU 1 and thread 2 will run entirely on CPU 2 so it's effectively a parallel kind of processing wherein we are not sharing anything on a given CPU rather each threade has has a dedicated CPU and it does not need to bother about whether it has to share the CPU with the other thread or not and please note that I have put CPU here but it could be a different core in the CPU itself so it could be either different cores of a given CPU or it could be different CPUs so that depends on the hardware in such kind of setup we can achieve the parallel processing now let's look at some of the pros and cons of multi threading the first one is we can build responsive applications so now you don't have to worry about freezing uh situation and thus you can build your applications to be responsive second is you will have a better resource utilization because now with the use of multithreading you could ensure that your Hardware or your CPU is not sitting idle rather once it's idle it could be taken up by some other thread for execution and the third thing is it helps us into building performant applications so with the help of multiple core CPUs we can build parallel programs and essentially we could get some benefit on the side of performance as well now coming to the cons of the multithreading the first one is synchronization needs to be done and it can get tricky at times so essentially when you are doing multithreading you need to share the memory space and other resources with the process and in that case let's say when there is a process and there are certain number of threads you need to share the resources so so we need to ensure that we are not running into funny situations and those things are handled by something called as synchronization we will have a much more focused discussion around all these things later in the video the second thing is it is difficult to design and test multithreading apps so essentially you don't have a control in which the different threads could execute so in that sense it's difficult to predict the behavior of the threads so it's difficult to design and test multithreaded applications and the third thing is thread context switch is expensive so if there are more than required number of threads then it becomes detrimental to your system performance so multithreading is not a silver bullet which will help you with all the situations rather we should use it judiciously now let's have a look on the thread life cycle any thread will start its lifetime in the new state and every threade is in this state until we call start on it after we have called a start on it it goes to something called as active State and this active state has two substates either it could be runnable or running as we saw in the earlier slides in some cases we may have to do some sort of time slicing and in that case there could be five threads which are ready to run but there is no CPU available on which it could run and we have called a start on such threads so those trades will be runnable State and there could be certain threads which will be in running State and as soon as those running State threads are done then they could allow the threads in the runnable state to run again and this is what we mean when we say that it has two substates which is runnable and running effectively this is the active State and the third state is the blocked state so every threade is in this state when it is waiting for some thread to finish so let's imagine there are two threads T1 and T2 and then they both started running on the CPU and after some time T1 got a chance and it was executing its task after some time it had to be taken out of the CPU and T2 got a chance but now T1 is not completed it's waiting for its execution to complete because T2 is now on the CPU so T1 is in a blocked State and this is what we mean by the blogged state now T1 will get a chance to execute on the CPU and maybe it may be done with its entirity of execution and then it goes to a state called as terminated state so every threade is in this state after it's done doing its required task here we are in the ID I have created a normal Java project and it's called as multithreading So the plan is that for the entire duration of this tutorial I'll be using the same project and I'll be creating different packages inside the project to discuss the concepts of the multithreading so in this particular section we will be discussing about the sequential execution so in order to demonstrate the code let's create a class call it a sequential execution demo and here is the idea behind this particular class I will be creating certain methods and the intent of this particular class is to Showcase that in a normal Java program the execution happens line by line and there is no jumping around from this part of the code to the other one so let's get going so to begin with I'll create the main method and in the main method I will have two methods let's call those as demo 1 and demo 2 and let's create those two methods I won't be doing anything fancy I'll just create a normal for Loop which will be iterating in certain range and then it will print some message and that's it so let's copy this one and let's change the name and here let's change the message as well and now let's run the program on running this this is the outcome that we have so first we were executing demo one method so the entirety of demo one is executed wherein it will be printing from 0 to 4 with this message which is from demo 1 plus I so from demo 1 0 from demo 1 2 till 4 and likewise we have executed demo 2 and in that case we print this message which is from demo 2 and the IAT number which is from demo to 0 to from demo to 4 so what we see is that the execution happens line by line so the main method is the first one to get started and the first thing it sees is that we are invoking a method called as demo one it goes there it executes it it comes back then the next line it says is that it's demo 2 it goes to demo 2 it executes it and it comes back here and then the execution terminates so this is what we mean by the sequential execution so in the context of multithreading what we can understand is that each and every program is single threaded unless otherwise instructed so here we just have a single threade and that is the threade that is created by the jvm for the execution of this main method and this could also be called as the parent thread or maybe the main thread now let's learn about the way in which we can create threads in Java and the first way is to implement a runnable interface so we will create a class let's call it as runable thread example and let's have a main method created the way it works is that we will have to Define some sort of class and the class will Implement our enable interface so let's do that so let's call it as thread one and it will implement the runnable interface and the runable interface has one method which we need to implement so that's run method and the logic is whatever we Implement inside the run method that is executed by this thread so let's do that so let's have a for Loop which runs from i0 to I4 and it prints a message let's call it as thread one and I would be the ith time it has been called now let's create an another thread let's call it as trade two which implements runnable and let's implement the run method and here as well we can run from i0 to I4 and let's print the message call it as thread to and I so this is a way in which we can Define the threads and once the threads are created they need to be somehow started so in order to do that what we can do is we can define a thread let's call it as one then new thread and we can pass the class that we have created so thread one and likewise we can say thread two new thread new thread two and we have the handle for these two threads 1 and two so how do we start these threads well we have a method called as start so let's do that so what happens is once you call the start method jbm will start these two threads and they are in the runnable state so they could be either immediately running or they will have to wait because they don't have any CPU available at their disposal where they could go and run so let's run this and see what is the outcome like so what we see here is first thread one is running then we have thread two running but this may not be the case always so in order to see a clear example let's increment the number of times we are going to print this message so let's increment it to 10 and let's do this to 15 now let's run the program and see the outcome so here here is what we see first we have thread one running and then thread two takes over and then thread one is running and then thread two takes over and so on and so forth so once everything is executed the execution will stop and the program will terminate so what we are observing is we have created two threads we have started the threads but there is no sequence in which they are executing rather the thread has been created and it's available to be scheduled by the thread scheduler and once the thread scheder finds an available spot for a particular thread to be run on the CPU it's assigned to the CPU and that's the time it's running for the time when it does not have the access to the CPU the thread will have to wait and that is the reason we are seeing a back and forth execution pattern wherein first one thread will run for some time then thread two will take over and then thread two will wait for some time and then thread one will take over there is also a different way in which we can create a threade using the runable interface and that is by making use of the anonymous in a classes so let's create trade three then new trade three and what we can do is new runable and let's print a similar kind of message so I less than let's say 15 I ++ the message could be 33 plus I and we can do the same thing which is 3. start because three is the handle that we have given for this particular thread one thing which you can observe is that this could be easily turned to a Lambda so let's do that and here we have a much cleaner way of creating a thread using the runnable interface so all we need to do is inside the Lambda we can provide the logic which needs to be executed by that particular thread now let's run it and see its outcome so we can see thread 2 is running then thread 1 is running then thread three is running and every thread gets some time of execution with the CPU and eventually all the threads are executed and terminated so this is how we can create threads in Java by implementing the runnable interface the other way of creating a thread in Java by extending the thread class with the help of extend scale keyword let's learn about the same so let's create a class and let's call it as extends thread example let's create the main method and now let's create the different threads so let's call it as thread one extends thread class and likewise we had to overwrite the run method in the example of runable approach we need to do something similar here as well so let's do that and we can have a for Loop which runs from I as 0 to 9 print some message let's copy this let's call This Thread two let's change the message as well so we have created two threads which is thread one and thread two now we need to instantiate it so let's call this as thread one and then new thread one then threade two and New threade 2 please note that here we are directly creating the thread as we are not passing this object inside the thread Constructor like we were doing in the case of runnable approach so now once we have the handle for the threads we can call do start on these two let's run this method and see the results so what do we see we have thread one running then thread 2 takes over now again we have threade one and then threade two takes over finally everything is executed and the program gets terminated so the basic idea Remains the Same once we call do start on these trades they are in the runnable state and based on the availability of the CPU they will be submitted to one CPU and they could start with their execution now that we have seen both the approaches of creating a threade one by implementing the renewable interface other by extending the thread class let's see which approach is better so if we extend thread then we cannot extend any other class usually it's a big disadvantage however a class May Implement more than one interface so while using the implements runnable approach there is no restriction to extension of class now or in the future so in most of the cases runnable is a better approach to create a thread now let's learn about do join method in Java let's create this class let's call it as join thread example and first we create create the main method let's create thread one and we can use the anonymous in a class or maybe Lambda to create the thread So This Thread is going to print from 0 to 4 and it will have a message call this as thread 1 followed by I let's copy this one let's call this as thread 2 let's change the message as well now thread 2 and this is going to be printed for let's say 25 times and first we call do start on the one then we call do start on the two and then let's have a message which says done executing the threads so what do you think is going to be the output for this particular program if you're new to multi trading then you could say that first these two threads will be executed and we will see all these messages getting printed on the console and finally we will have this message printed but looks like it's not going to be the case so let's run it and find it out and definitely it's not the case in fact done executing the threads is the first thing to get printed on the screen so why is it happening so in order to understand this we will have to take a step back and understand how does the main method works so in this particular program main method is the first thing which is getting called by the jbm and when this happens this main method is run by your main threade so this main threade is the first one which gets assigned to the CPU with the highest priority we will learn about priority and all those things in some time but for now understand that this main thread has the highest priority so it starts with its execution first thing it does is it creates the definition for threade one second thing it does is it creates the definition for this trade two and it comes to line number 17 then to 18 and in these two lines it moves these two threads in the runable state and finally on line number 19 we have this message since the main thread has the highest priority for now this message is printed first so what happens is all these trades are executing independent of each other so threade one will start with its execution independently so will thread two and the main thread anyway has the access to the CPU for now it's going to print this one as soon as possible and that is the time it's done with its execution and it waits for these two threads to complete their execution and once that is done then the program is going to terminate so what should I do if I have this functionality wherein I want that thread one should be completed and only after that happens the main method or the main thread should proceed with its execution so in order to implement that functionality I can make use of dot join method so let's call do join on thread one and do join method throws an interrupted exception in order to correct this we can either surround this with TR catch or could add the exception being thrown in the method signature itself I'll go with the second option now with this in place let's run the program and see what is the outcome so this is what happens threade one needs to be executed five times and threade two needs to be executed 25 times so first threade one is executed one is getting executed and looks like by this time thread one is executed and at the same time thread 2 got hold of the CPU so it started executing and you notice that for the next time thread one was was supposed to be executed but threade one did not have anything left for execution effectively it was done with its execution and that is where the dot join came in effect and it instructed to the jvm that one is done with its execution now it's time for the main thread to take over and proceed with its execution which is printing this particular line so now we print this line and then thread 2 proceeds with its execution it's going to print all the messages by thread 2 and as soon as this is done both the threads are completed and now the main thread also shuts down so what we learned here is 1. join is kind of hinting to the jvm that as soon as I am done with my execution then you can start with the execution of other threads which are in the Que in this case we had two and the main thread so first two was executed for a while on the CPU then the main thread started with its execution which was printing this line so what happens if I place two do join as well so in that case jbm will not mly wait for the thread 1 to get completed rather it will also Wait For Thread 2 get completed and only after that this message will be printed so let's try that out as well let's rerun the code and see what is the outcome so you see all the threads are executed so basically one and two are finished and only after that we see the outcome that is done executing the threads now let's print a message before executing these threads and what we can say is before executing the threads now let's run it so you see first we see the message before executing the threads now all the threads are executed because we have placed dot join on both of these two and finally done executing the threads is being printed so basically why this is happening is till this point of time we have not put the threads into the runable state so the main thread is the only active thread in this context and this is the reason we printed this as soon as weed at this line so now let's understand about the join operation with some theoretical Concepts so first thing to notice is that main thread is the parent thread so when we start a program usually the execution begins with the main method this method runs on the main thread this can be understood as the parent thread since it responds the other threads as well then the other important point to notice here is the independent execution of threads under normal circumstances so when you create and a start the threads they run concurrently with the main thread unless instructed otherwise so under normal circumstances all threads run independent of each other more explicitly no thread waits for other thread so what is join method well imagine threads to be lines of execution so when we call do chwine on a certain thread it means the parent thread which is the main thread in this case it's saying hey thread once you are done executing your task join my flow of execution it's like the parent thread waits for the completion of the child threade and then continues with its execution and here is my perspective on this concept well personally I find the join keyword is not very intuitive at first for the kind of operation it's doing somewhat better terms could have been wait for completion or complete then continue what's your perception about the joint method in Java let me know in the comment section now let's learn about the concept of ton and user threads on the basis of surface of execution threads can be of two types demon threads or user threads demon threads usually run in the background wherein user threads are the active threads so when a Java program starts the main thread starts running immediately we can start children threads from the main threade the main threade is the last threade to finish its execution under normal circumstances because it has to perform various shutdown operations demon threads are intended to be helper threads which can run in the background and are of low priority for example garbage collection thread demon threads are terminated by the jvm when all other user threads are terminated or they are done with their execution so under the normal circumstance stances user threads are allowed to be terminated once they are done with their execution however the demon threads are shut down by jbm once all the other threads are done executing now let's have a quick code demo for the concept of demon threads and user threads so let's create this class called as demon user crate demo and uh first of all we will create the main method now let's create two threads first is demon helper implements run let's overwrite the run method the second thread is user thread helper lements runable let's overwrite the the run method now let's write the code which needs to be executed by the demon threade let's have a counter variable start with zero and while count is less than 500 do a thread do sleep and then do a account Plus+ finally print this message which is demon helper running the sleep will expect us to pass certain time let's give the time for 1,000 milliseconds and the Sleep Method throws an interrupted exception so let's surround this with try catch for the user thread method let's have a sleep timer for let's say 5,000 milliseconds and let's surround this with dry catch then print this message user thread done with execution now let's create these threads in the men thread so first we start with the demon thread let's call this as a background thread that is bz thread so new thread new demon helper for the user helper maybe we can call this as user thread new thread new user helper by default any threade is not a demon threade the way to make a normal threade as a demon threade is by calling do set demon method so let's do that bz thread. set demon and true finally we will start these two threads so BG thread. start and user thread. start now let's run this program so here is what happens the user thread has a sleep timer of 5 Seconds so as long as the thread was started it got assigned to the CPU and once it was assigned to the CPU it went into a sleep stage and then the demon thread was assigned the CPU and during that time there was no threade which was contesting for the CP CPU so the demon threade kept on running in the background so it ran for 4 seconds so remember we had the sleep timer for, millisecond so after every second this was running and printing the message demon helper running as soon as the 5sec got completed this thread came back into existence and it printed user thread done with execution and once this happened then the user threade helper got concluded it got finished with its execution and now what jbmc is is that there is no other threade which needs to be executed so by this time all the user threads which is basically just this threade is done with its execution and now it's time to shut down the jvm and that is where even though this demon thread was not completed because in the normal case of operation it could have ran till the count was less than 500 which was certainly not the case because it ran just for four times but still it was terminated because the user thread was terminated so this is what we learned from from this particular demo that user threads are given the priority demon threads run in the background and once all the user threads are concluded the jvm shuts down the main thread and along with that the demain thread is also forced to be shut down let's learn about a very important concept of thread priority so let's say there are 10 threads in runnable state however there is only one available CPU so only one threade can execute at a given point point of time others will have to wait so who decides which trade gets to run on the CPU well the component who decides this is called as the thread scheduler so each threade has a certain priority to begin with and under normal circumstances the thread with the higher priority gets to run on the CPU please note the keyword under normal circumstances the priority value varies from 1 to 10 and it can be assigned to any thread one priority is represented as the Min priority and 10 priority is represented as the max priority by default the priority of a thread is five and is represented as Norm priority Norm as normal threads of the same priority are executed in fifo manner so the thread scheduler rest stores the threats in a que now let's just spend some time to understand this part of normal circumstance so well what happens is all the main threads are started at a normal priority of five but still they are privileged to be executed first on the CPU by the thread scheduler why so well it's by Design since it's a main thread it has to be executed first otherwise your program will have to wait unnecessarily so this is the first thing which gets to be executed in any case even though the priority of the main thread is five it gets the first priority to be executed on the CPU for the first time now let's have a code example to have a demo of the thread priority concept so let's create this class called as threade priority example let's have a main method we can get the priority of the current threade by calling threade do current trade. get name so let's print this out now let's get the priority of the current trade so we can call trade. current trade. getet priority let's print this out as well now let's get the current threade so threade do current threade do set priority and we have Max priority let's print the updated priority let's run this so the outcome we will expect is the name of the main threade then the current priority which is the default priority because we did not change it earlier and after we change it what is the priority let's run and find out so the name is main to begin with the default priority is five we updated the priority to maximum and then we printed the same let's print something so thread dot current threade doget name says hi now let's create one trade say thre one says high as well and by default the new threade which is threade one will have default priority of five now let's give this the maximum priority possible which is trade do Max priority and then 1 do start so what do you think will happen now that trade one has the maximum priority and the main threade is still at at the default priority of five I should expect that this one should be executed first then I should see the outcome for this one so let's run it out and see what is going to happen well what we see is the first one to get executed is main says High then the second is threade one says high as well so please note that this property of main threade getting the first priority or the higher priority to get executed on the CPU regardless of this having the lower priority which is five as compared to the max priority is only for the first time this thing gets executed once execution is starts it will go by the fif manage and then the thread scheder will schedule the threads based on their priority and if they happen to be of the same priority fif will be used to schedule the threads so this is how we deal with priority in the case of threads in Java now that we have learned the basic concepts of multi threading regarding how we can create trades and how we can wait for Threads to complete using the join keyword Etc now it's time to learn about the important concept of thread synchronization so let me create a package to write the code so let's call this as trade synchronization and I will Implement a class let's call this as synchronization demo so here is what I want to implement in this class I want to have a counter variable that will be initialized with zero and I will be using two threads to increment this number up to certain times and in the end I will check the final value of the counter variable so let's do this so first thing is let's create the main method and uh let's have a counter variable as well let's let's call this as private static int and counter let's initialize this to zero let's create two threads so as seen in the earlier video that we can create threads in line using the Lambda expression so let's do that and uh in this trade what I want to do is that I want to increment the value of counter let's say 10,000 times and let's call this as counter Plus+ let's do the same using another thread so I have a code template that I could make use of so that will save me some time to type the code again let's have this for Loop run 10,000 times and those many times we will increment the counter variable then in the last what I will do is I will start these two threads so let's name this as two oops and uh two. start and before printing the final value of the counter I want to make sure these two threads are completed so the way to do is I would call join on this so let's surr this with try catch let's duplicate it and let's call the same for trade two as well and finally we could print the counter value now let's run this one and see what is the outcome like so let's run this and see the output so the output that we have is 16583 but what we expect I is that since this trade is executing it 10,000 times and this trade is running this for 10,000 times then effectively it should have been 20,000 right but that is not the case what we are having is 16583 how about let's run it one more time and this time we are getting a totally different number let's run it one more time and we are getting a totally different number so this is not correct right then how can we correct this so even before we learn how can we correct this it makes sense to learn why this is happening so the reason for this behavior is something called as a nonatomic operation so what exactly is a nonatomic operation well what we see here is we are incrementing the counter variable counter is counter plus one even though it appears as if the counter variable will be incremented by one in one shot but that is not the case under the hood under the hood what will happen is first the counter variable will be loaded in the memory then it will be incremented by one then this will be assigned back to the counter variable which is holding this particular value and that is where we set the variable value back to counter plus one so essentially this is consisting of three steps so the first step you could think of it as load second step is the actual increment and then set back the value so let's say at this point of time thread one starts operating on this particular logic which is incrementing the counter variable let's say at that point of time the value is zero and uh it incremented the value the incremented value became as one and now it's time to apply this value incremented value back to counter variable but at the same time threade 2 came into the picture and what it did is it said hey let's load the value for counter and it found that the counter value still is zero and then it went ahead so let's call this as this being uh the operation by thread one and now thread 2 came into the picture it said hey let's load the value for counter and it is zero and what I'll do is I'll increment the value so after incrementing the value it became as one and this is where we have thread 2 doing this operation so effectively this value should have been two by this point of time but unfortunately since the time at which this threade one could have set the value as one and applied that value back to the counter variable and that is where the thread 2 came and it intercepted the uh execution and it found out that the value was Zero it incremented it back to one what should have been uh a two here that is still one right so this is the inconsistency and this is the inconsistency due to which we are seeing this error that we are not getting the final value of 20,000 here rather we are stuck at some lesser number so what exactly we call this phenomenon well so this thing is called as race condition in the terminology of multi threading and concurrent programming so what we have here is a shared resource that is the counter we have two threads working on the same shared resource and these kind of scenarios lead to inconsistencies which are called as the risk conditions so how exactly can we fix this so the way to fix this is if you have different uh counters on which these two threads could work on then probably this could work on one counter this could work on another counter but that is not the case here right the whole point of discussing this example is that we have a shared resource and we want to make use of multithreading concept to have some sort of operation applied to this particular share resource so in order to avoid this kind of situation what we can do is we could somehow ensure that this operation right this operation is being done by one and only one threade at a given point of time so that way what we could achieve is this is not being done simultaneously at the same time by two threads and in that sense we could achieve something called as Mutual exclusion and it will ensure that once thread one has the access to this particular logic which is incrementing the counter then it will do all the three parts which is loading the counter incrementing the value and setting back the value to counter so now counter has one and once thread one is done with its execution then the other thread could make use of this particular shared resource in its own execution so essentially we are restricting the users the access to the shared resource by multiple threads at a given point of time and how can we achieve this so one way to achieve this is with the help of keyword synchronized and uh we could do this at multiple levels like have this synchronization at the method level or the Block Level so now let's see how can can be fixed this using the synchronized keyword create an another method and uh let's call this as increment and what this will do is it will simply increment the counter variable let's replace this counter ++ with this increment let's do the same here as well and what we have here now is a method which is going to increment the variable and as I told earlier that we want to ensure that this particular operation is being called by just one method at a given point of time what we can do is we could restrict its access and the way to do this is put a synchronized keyword here so essentially by uses of the synchronized keyword what we are saying that hey jvm please allow this particular method to be accessed by just one and only one threade at a given instant of time at any cost and that is how we have achieved synchronization in a code so this thing is also called as critical section so one way to understand this is that this section is quite critical to the execution of the program in the context of multithreaded environment so we are limiting its access or the users by one and one one thread at a given point of time so with that in mind let's go ahead and run the code and see what is the outcome like so let's run this and now we can see that we are getting the the exact value of 20,000 let's run it a couple of more times and each and every time we see the value remains to be 20,000 itself and there is no changes as such so it feels like we have fixed the issue of synchronization by using this synchronized keyword but there are a few inherent problems with this particular approach the approach that we are talking is using the synchronized keyword at the method level so in order to understand what are those problems let's first understand how does the synchronized keyword work in Java so first thing is let's understand the monitor locks so each object in Java is associated with a monitor which is a mutual exclusion mechanism used for synchronization so when a thread enters a synchronized block or a method it attempts to acquire the monitor lock associated with the object on which the synchronization is applied so imagine that there is a shared room that room could be used by just one person at a time then you have to enter inside the room using some sort of lock you use the lock you open the door you go inside and then once you come out you release the lock you hand over the key to someone else who wants to make use of the room next likewise in every Java object we have this monitor lock which is also sometimes called as the intrinsic lock which needs to be acquired by the thread which wants to make use of the synchronized block or the synchronized method Or the critical section when a threade enters the synchronized blck or method it attempts to acquire the monitor Lock And if the lock is available the thread acquires the lock and proceeds to execute the synchronized code so what do we mean by the lock is available well what it means is no other thread will be currently holding that lock so if the lock is not available that is another thread is holding that lock then the thread enters a blocked state and it has to wait until the lock becomes available the second step of this process is releasing of the monitor lock so when threade exits let's say the threade is going to exit after this execution is completed it has to release the monitor lock and thus it allows other threads waiting for to acquire the lock and to proceed with their execution so the monitor lock used by the synchronized keyword is sometimes referred to as the intrinsic lock or the monitored lock of the object instance each object in Java has its own intrinsic lock and the synchronized keyword acquires and releases this lock implicitely when used at the method level or in the synchronized block so what is the problem exactly if we use this synchronized keyword at the method level so the first problem is that it's a kind of course grained locking so when we use synchronized at the method level it applies the log to the entire method body even though in this case our method body is just a one line but in actual code base it could span to multiple lines so essentially what you are doing is you are blocking any other threade from entering that particular method when you have applied the synchronized block on that particular method but your critical section could be just two or three lines so in that sense it does not make sense to block an entire method which could probably let's say have 30 or 40 lines right so that is the first problem with using the synchronized keyword at the method level and needless to say this leads to reduced concurrency and performance bottlenecks and uh the second thing is kind of related to the first issue itself so when synchronized is used at the method level we lose the fine grin control needed in the more complex scenarios so for example you might want to synchronize only a specific section of the code within the method or you might need to synchronized multiple methods together as an atomic operation using method level synchronization we you don't have this granularity level and the third issue is let's say uh when a subass is overriding a synchronized method from its super class it must also explicitly declare the method as synchronized if it wants to maintain the synchronization Behavior and the failure to do so can lead to unexpected behavior and potential synchronization issues so you see there are multiple issues with the synchronized keyword when we use this at the method level so let's understand this with some code example in the same code let me do some refactoring let's call this as counter one let's create another variable let's call this as counter 2 and let's call this as increment one and this as increment two and uh what we can do next is we can duplicate this method and uh let's call this as increment one and it's going to increment the counter one let's call this as increment two and it's going to increment in the counter two let's print these values in the last so counter two so from the functionality perspective it's surely going to work fine so counter one will be 10,000 and counter two will be 10,000 let's run it and confirm yeah this is working fine let's rerun it a couple of more times to ensure that it's working as expected so it's working fine but there is a problem so if you look closely what we have done is we have used the synchronize at the method level so let's say thread one starts working and at that particular point of time increment one is getting invoked so thread one has acquired the lock to this particular code this particular method and it's working on counter one but if you notice threade 2 has no business with counter one right because it's going to operate only for incrementing the counter two so in in this case even though threade 2 is not directly working on counter one it will be blocked because this synchronized behavior is going to acquire the class level log so this is the class level log for this particular class there is only one log which is this intrinsic log or the class level log or the Monitor and that is acquired or held by the thread one and in that sense threade 2 even though it was not supposed to be blocked it's blocked now so I I think you see the problem now and that is one major problem with the synchronization at the method level so uh to address these concerns it's often recommended to use explicit locking with synchronized blocks or to use more flexible concurrency utilities provided in the Java util concurrent package such as lock interfaces reant locks read right lock Etc and uh we will be learning all these things going forward in this tutorial but for now let's understand how can we fix this scenario by the uses of custom logs so let me create another class let's call this as loog with custom objects so first of all let's have the counter variables so private static in counter 1 as zero let's duplicate this let's call this as counter 2 and let's have the main method in the main method we will have have two threads so let's call this as thread one let's call this as thread to and this will be incremented for 10,000 times let's call the increment one let's copy this one and let's rename this to increment 2 and uh let's start the threads so two do start and let's wait them to complete their execution so in catch we will have interrupted exception maybe call this as e and then we want to throw a runtime exception let's call 1. joint and then do. join and let's print the value counter to what we want to do now is we want to implement the increment methods so increment one and this time we don't want to have the synchronized keyword at the method level rather where we want to place this is at the Block Level where we want to do the synchronization so this will be counter 1 ++ and if you notice when we place this at the Block Level it's expecting us to pass something what does it expect us to pass well it expects us to pass the lock on which we want to lock this critical section so that two different thread for this particular critical section cannot enter in this code block so the way we will do it is that we will Define two logs so let's call this as private static final object log one new object and let's do for log two as well in the similar Manner and now we can have this on lock one so let's duplicate this let's call this as increment 2 and increment 2 right so basically what we have done is that we have the increment one method and then increment two method and these two methods are supposed to work for two different variables counter 1 and counter 2 and since these two methods are being used by two different threads and moreover these two methods are working on two different different shared resources not the same one which is counter one and counter two we want to lock it on two different locks so basically for this critical section if you want to acquire this particular area you have to access lock one if you want to acquire this particular area you have to access loog two and this is how we have decoupled we have separated out the concerns for these two different threads thread one and thread two so this is how we make use of this synchronized keyword at the Block Level and fix the issues with the users of synchronized keyword at the method level but having said so it's not a rule rather it's just a guidance or a general opinion but if in your case if your method happens to be quite a short one and does not have any other implementation and moreover let's say the entire method block is itself the critical section in that case you can go ahead with the method level synchronized keyword as well but more often than not this is not the recommended approach rather you should explore the option of going with the synchronized block itself now that we have seen the concept of synchronization it's time to understand a very important concept of wait and notify so before we start with the concept of weight and notify let's see what we have on the screen so let's imagine there is a room and the room always contains some sort of parcel and the catch here is that the parcel could belong to anyone so let's say there are four or five people and their parcel is getting delivered so you need to go inside the room and then take out that parcel but the room is locked and there is a security who is there on the room entry and the only way to access the room is to get that lock from the security once you have the lock you can go inside the room F your parcel and come out but here is the catch at a given point of time only one person is allowed to access the room everybody else will have to wait so how this can be implemented so the way it could work is that once the security is giving a lock to anyone they will allow them to go inside and F their parcel until the time whosoever is coming to access the room the security could tell them that somebody's already inside the room so you will have to wait till that person comes out so once the person who is inside the room gets his parcel he comes out and he hands the lock back to the security Now the security will notify that the room has been vacated by the earlier person and anybody else who was waiting for the room to access it could go inside and get their parcel but again they will have to ask the security for the access to the lock and then they will open the lock and then they will go inside the room to get their parcel so you see there is a mechanism which is is kind of allowing just a single person entry inside the room in order for them to do something inside the room which is the activity of collecting their parcel until the time anyone is inside the room the entire room is considered to be frozen or kind of locked and nobody could access it and as soon as that person is coming out then they are handing the key to the security and after that is done the security is notifying to everybody else that now room is available to be accessed and then in anyone from the waiting line could come and collect it so the wait and notify methods work in a very similar way so when a threade calls wait on the lock then that thread is suspended from its uh execution and it goes to something called as a waiting State and when it goes to the waiting State then others who were in The Waiting State earlier they have a chance to get hold of the lock and then they could start with their execution and as soon as they are done with their execution they could call notify and notify is a message to all the waiting threads that now whatever they were waiting for that lock has been released and then they could take that lock and access their critical section to work upon so that is the whole idea behind the wait and notify methods in Java now we will see this with one code example and it will become more clear so in order to demonstrate the working of weit and notify I have created this class it's called as cre and notify demo so first let's start with writing the main method next we will create two threads so let's call this as thread one and this one as threade two let's start the first one let's start the second one and uh let's create a l as well so call it as public static private static final object lock and new object let's have two worker methods which will be invoked by the different threads that we have created so first one is public static void let's call this as one and it will throw an interrupted exception why we will see it later so what we will do is we will synchronize on the lock and the first thing that we do is we print some message like hello from method one and then on the Lock This Thread is going to call wait and then we print some message back again from the method one or maybe back again in the method one makes more sense let's create the other method as well it will be called as two so why did we throw the interrupted exception so when we call wait or notify these are interruptible and they will throw an interrupted except that that is the reason we need to have this in the method signature so let's put synchronization on the lock and uh let's have this message hello from method 2 and now this time let's call notify and let's print some message hello from method 2 even after notifying and uh let's call these methods in the threads so this will call one let's add this in the TR catch block let's call to here and let's add this in the TR catch block as well and now let's try to analyze what's going to happen so what we have done is we have created the thread one and it's going to make use of this particular method so what this method is doing is let's say thread one is started and after that the control comes here so in the synchronized block the lock has been acquired and this print message will be executed so we will see this message on the console now what does this thread do is that it calls weight so this thread will be suspended and it will go in a waiting State since the other thread was just started as well and what this thread is doing is it is calling method two the execution comes here and since weit was called so lock was available so this lock was acquired by this particular thread and this message will be printed that hello from method 2 and then we call do notify so one important thing to note here is that even when we call notify then whatever code execution is left out for that particular synchronized block all those things will be executed only after they are executed then the notify will come in effect other threads which are waiting for the lock to be released they will acquire it and start with their process so basically after notify is called it's not that the control will go directly to this particular part rather it will print out the remaining thing which is Hello from method to even after notifying and then the control comes here to this threade and then it will print this message that back again in the method one so let's execute this and see the outcome so we are here run wait an demo so here is the outcome so first is Hello from method one and then we called wait so it goes to the other thread it says hello from method 2 and after notify we had another message as well so that is getting printed that is Hello from method 2 even after notifying then we have the control back here which is back again in the method one so you may have a question that what is the difference between weight and sleep are in these both same same right so on the surface they do look quite similar but there is a key difference that weight is used for inter thre communication and synchronization while sleep is used for just pausing the execution of the given thread for a specified duration and please note that this is just one implementation of weight method there are other implementations as well so when you call weight with this time out information it causes the current threade to wait until it is awakened typically by being notified or interrupted otherwise if not so then after the time has elapsed it will be automatically awakened and there is another variant of notify which is notify all so in the case of notify it pickes up single thread but in the case of notify all it notifies all the waiting threads that are waiting for a given lock so this should be it for the introduction of wait and notify methods in Java now let's go ahead and make use of it to to implement the producer and consumer problem so what is producer consumer problem the producer consumer problem is a synchronization scenario where one or more producer trades generate data and put it into a shared buffer while one or more consumer trades retrieve and process the data from the buffer concurrently such kind of a pattern which is used to teach the concept of synchronization so consider this image here so let's assume that this thing is some sort of buffer some sort of container or placeholder where we want to put some data but the catch here is that producer is the one which is going to put this data here and the consumer is the one which is going to consume the data from here so what I want to implement now is two threads one is the producer threade which will be putting data in this particular shared container and the other thread is the consumer thread which will be consuming data from from this shared container and at the time when the container is full the producer will stop producer and at the time when the consumer is empty the consumer will stop consuming so let's go ahead and implement the same so to implement the producer consumer problem here's the class that I have created and uh it's called as producer consumer and uh there would be a supportive class called as worker so let's create that as well let's call it as worker and the worker will have two requirements one is to produce the element and put in the shared space so let's call this as public void produce and the other would be public void consume using this the worker can consume from the shared space and this worker will need certain properties so the first one is sequence so essentially what we are going to put in the shared space is number so 0 1 2 3 4 5 something like that and it will start from zero so we are calling the same as sequence as zero the next thing is private final integer let's call this as top so this is the maximum number of elements that can be stored in the shared area shared container let's have another parameter called as bottom so this is the least amount of elements that can be kept in the shared container and the next thing is private final list so this is the container itself here we will be placing these numbers that we are generating so here the producer will be generating the numbers and the consumer will be consuming from the next is that we need some sort of lock let's create the lock as well let's let call this as lock and initialize this as an object in order to initialize all these things we will have to have a Constructor all right looks fine to me now let's go ahead and implement the produce method so first thing is that we need to synchronize on the lock and and what I want is that produce should be able to keep on producing these numbers infinitely until a certain condition is met and likewise and the consumer should keep on consuming infinitely until certain condition is met so let's start with producer so let's call this as while true and if container do size is the maximum one then let's print a message so essentially the container has become full so Container full waiting for items to be removed and since we are waiting we will call do weight and this weight we will have to throw an interrupted exception so let's add that and if it's not full then let's print a message so sequence added to the container and let's add this to the container and let's call notify and in order to see the demo we should add some sort of delay so let's add a delay of of let's say 500 millisecond so what's happening here exactly well this thing for sure is going to run infinitely and that is the reason we are getting certain warnings so may not be the most optimal way to implement this scenario but this is fine for the demo purposes moving on coming to the logic so first thing which is checked is is the container full if the container is full then this message is thrown and the thread goes to a waiting State else and then we start adding to the container what we is we first print the message and then we add that sequence number to the container and after it is added we increment that number so it's a post increment so that the next number could be added to the container in the sequence now we are calling log. notify so if you remember the notify discussion from a few minutes back what we understood is that notify does not come into existence immediately rather whatever is there in the synchronized block everything will be executed and only after that is executed then notify will come into effect and the other waiting threads will be notified so here what happens is that we call notify and then the call comes here so since we are into while. true and we check that container size still not full then we come to the else and then we go about executing this so this happens till the time the size is not full and then log. WID is encountered and then this thread goes to a waiting state so this is how this thing is implemented and executed now let's go ahead and implement the consume method as well now let's implement the consume method so for the consume as well let's throw the interrupted exception because we will need to add the later point of time all right so let's put all of this inside a synchronized block and like we did for the case of produce we want this to run infinitely as well so we will put this inside a continuous file Loop and if container do size so if the container is empty then we don't need to consume anything so let's print a message container empty waiting for items to be added and then let's go to a waiting state in the else section let's start consuming so what we say is container do remove first and then removed from the container so basically we are going to remove from the list that is the shared container and print the message and once this is removed then we will call log dot notify and here as well we will have a sleep for let's say 500 milliseconds to show the simulation in somewhat slower manner so that it's easy to read and understand right so let's understand what is happening here we are going to do this infinitely and uh initially we check if the container is empty if it's empty print a message and go to a waiting state if it's not empty come to the else section and then let's remove the element from container and uh print it on the console and then we call notify and given the fact that notify is not realized immediately it will keep on executing whatever is there in the synchronized block and uh since this entire thing is there inside this file Loop this will keep on executing till the time we are not meeting a scenario of dot weight so this keeps on happening and finally we encounter a situation wherein it becomes empty the container becomes empty wait the lock. weight and we go to the waiting State and then this particular thread which had gone to the waiting State earlier comes into effect and then it starts adding again so this is how this is going to work now let's implement the main method so of course we will have to move all this worker class outside this particular class so let's skip it outside so in the producer consumer class let's create the main method and uh in the main method first of all we will create a worker object which will be initialize with five and zero so the shared container could contain up to five elements maximum and uh minimum would be empty so let's create the threads call this as producer create another thread let's call this as consumer and inside the producer threade we can say worker dot produce and of course it's going to throw interrupt exception so let's handle it accordingly and here we could call consume let's handle it accordingly and finally let's start these threads so producer. start consumer do start so let's run this and see what we are getting so let's run this all right so here is the output so you can see Zero added to the container one added two added three added four added so and so forth now let's stop it and analyze so basically first zero is added then one is added and so and so forth four is added and once fourth is added then the container is full and then it's going for a state where it wants the items to be removed and then the consumer part is activated and it's going to remove the items from the container so zero removed one removed and so on and so forth and finally the container becomes empty and then it's waiting for the items to be added this process will keep on happening because we had implemented in such a way that uh it's going to happen for infinite time period And this is one way in which we could implement the producer consumer problem of course there are many other ways as well but this is one way in which we could implement it till now we have learned a couple of ways in which we can create threads in Java such as creating the threads using extending the thread class and implementing the runnable interface how can we create multiple threads let's say if we want to run five tasks then we can create threads in the for Loop and run these tasks what can we do if we want to execute 500 tasks synchronously we can for sure create 500 trades using the for Loop right but where does it stop what do we do if we have to run 1,000 tasks create th000 threads in a loop well something does not sound right here here is the problem in Java one thread is equal to 1 OS level thread creating a thread is an expensive operation so creating thousand trades in a loop is certainly not a scalable approach what could be a more practical approach is to have a fixed number of threads and let's create them up front imagine a pool of end threads and these trades handle the Thousand tasks among themselves and this is exactly what the executor service helps us in achieving in one line we can Define the executor Service as a tool in Java for managing and running tasks concurrently across multiple threads so executor service helps us in creating a bunch of threads a pool of threads does the name threade pool and the threads are not killed once they are done executing the task rather they are reused to execute the another task thus by making use of the executor service we save time needed for thread creation and making things more efficient and manageable there are four types of executors provided by the execut service and these are single thread executor fixed thread pool executor cach thread pool executor and sheduled executor now let's quickly visualize how internally the executor service looks and functions so imagine a execution of the main threade which is this one this is the execution line and the main threade calls for the executor service and now the executor service the way it functions is that it maintains a thread pool there are couple of threads that are there inside the thread pool this depends on the type of executor we are using but essentially you will have a couple of Trades which are created in advance then each type of the executor service will have a blocking queue and this is where the tasks are placed so when we say executor do execute and the task so what I want is that a task should be picked by either of these threads and it should be executed but for the time being it's not getting executed it will be sitting inside this blocking queue so in nutshell the executor will have two things first is the threade pool and then there is a priority queue and there are just two steps the new task will keep on adding to the blocking queue and the thread which is available to execute the new task can pick the task from the blocking queue and it starts its execution and once the execution is done and the threade is available to pick another another task it will pick another task from the blocking queue so more or less all the executors look similar to this and this is the way in which they function of course there are certain fundamental differences in which the que will be managed and the way in which the threats will be created now what we will be doing is write some code making use of these executors and then understand how do they work so let's begin in order to demonstrate the working of the executor service I have already created this package that is the executor service and now we will learn about the single threade executor so let's create a class let's call this as single executor demo we will have a class let's call this as task and it's going to implementer unable and it will have a task ID so let's call this as task ID let's have the Constructor let's overwrite the run method and in the run method we will print a message task with ID task ID being executed by thread and then trade dot current trade dog name and let's put some sleep in the thread let's have it for half second and in the catch handle the interrupted exception so we will just throw the exception for now let's call this as a runtime exception and pass the exception here here we will create the main method and in the main method what we will do is we will create the single thread executor now so in order to create it we have the executors dot new single thread executor and let's assign this to service now the idea is complaining and showing some warning message let's see what does it say it says that executor service used without private resources statement so the executor service should be closed once we are done using it there is a way in which we could close it and that is by calling the service do shutdown but there is a different approach as well in which we can implement the same and that is is to use the TR with resources so let's do this and put it under the try with resources and now we should be good to go so let's say I want to run this task 5 times so for in I as 0 I less than 5 i++ and I can say service. and then call the execute so what is the runable that I want to execute that is my task and pass I as the task ID so as opposed to what we were doing earlier that after creating the threade we used to call the dot start in order for the thread to get started with its operation here we don't need to do it rather once we submit this to the executor service by calling do execute it is handled by the execut service itself now let's run this code and see what is the output like so let's run this so here is the output so basically it's going to print task ID with zero being executed by this particular thread and task ID with one being executed by this particular thread and so on so forth this is what we have implemented in the code as well right the idea is that there is a runable and what it does is it's simply going to print the task ID along with the threade by which it is getting executed so if we had not used the executor service what we would have done is we could have created Five threads and using those five threads I could have assigned the reable for each of the thread and then should have called The Dot start on that but here we are able to achieve the same kind of result using the executor service and this implementation in particular which is new single thre executor so now the implementation looks much more cleaner and we don't have to deal with the creation and destruction of the threads now everything is being handled by the executor service itself let's learn how the single thread executor Works under the hood so we have the main trade and this way we can create in tasks and submit or execute those tasks one by one so here we have the threade pool and in the threade pool there is a single thread there is a task Q which is containing task zero task one task 2 up to task n and what this single thread executor will do is it will pick the task from the task pool or the task queue and then execute it so Fitch task and execute so in the case of single thread executor the size of the threade pool is just one so we have just one threade which is going to Fitch the tasks from the task Cube and run this and if it so happens that due to some exception the thread is killed the executor will make sure to recreate the thread and the execution of the tasks won't be stopped by using this kind of thread pool we can ensure that the task zero is always ran before task one and task one is always ran before task 2 since we have just one threade here it's guaranteed that the tasks would be run sequentially now we will learn about fixed threade pool so let's create the class let's call this as fixed trade pool demo let's maximize this one and as we had done earlier we will be creating a runable which will be the task which needs to be executed by the executor service so let's do that let's have a class called as work it's going to implement runable and we can start by overwriting the run method public void run and it will have certain parameters so the first field is work ID and then it will have a Constructor which is going to initialize this work ID and in the run method what we will do is we will print a similar kind of message the task ID let's provide the work ID here being executed by thread threade dot current thread do get name let's put some sleep timer here so again let's go with half a second and in the catch let's handle the interrupted exception let's throw runtime exception and pass the exception and now let's go to the main method and very similar to what we did for the single thread executor we will also be creating the executor using the try with resources so try executor service let's call this as service and executors Dot new fixed trade pool let's say that I want to have a thread pool of size two and uh I want to execute this task seven times or what you could say is I want to run seven tasks so let's make that so int I as z i less than 7 I ++ and let's call service. execute new work pass the ID so let's print this and see how is the outcome like so let's run this so here is the output so task ID one is being executed by thread 2 task id0 is being executed by thread 1 task ID 2 is being executed by thread 2 and so and so forth so one important thing to note here is that since we had two threads created these two threads are there in the thread pool and they are taking turns in order to execute my task so first thread 2 is being used to execute the task then we have thread one then I have threade two then we have thread 1 thread 2 thread 1 thre 2 and so and so forth please note that this may not be alternating in the stricted terms this is just instance wherein we got this kind of pattern but this does not mean that in all the cases it will be one by one it could so happen that threade two could execute certain tasks in one sequence and then after that thread one could be involved and then thread one could be assigned with certain tasks for execution so this is what we have achieved by making use of the fixed threade pool so to submit up we have created a new fixed trade pool and there are two trades and when I want to execute seven tasks then those seven tasks are picked one by one by either of the available two threads and they are executed now let's have a quick look around how exactly does the fixed threade pool work under the hood so what you see here is a visual representation of the fixed thread pool executor and here in the case of fixed thread pool there is a fixed number of threads as the name suggests that there will be certain number of threads which is precreated and this executor also has a task que where all the tasks will be placed so consider this as a task que of some sort the threads in the fixed thread pool pick the task from the task que and execute them so when the thread is done with the execution of a task it goes ahead and picks another task and this keeps on happening until all the tasks are completed so the basic idea Remains the Same that there is a thread pool and the threads are fixed in the count and once you have tasks flowing in in the task queue these trades will start taking out the task from the task queue start to work on those and once all the tasks are done and this keeps on happening till the time the task que had certain tasks in it so overall This Thread pool executor is fing the task and executing it over and over again now let's learn about the cach threade pool executor in order to do so we will write a code to demonstrate its functionality and let's name the class as cast threade demo we will have a task so create a class for the same let's call this as task one it will implement the runable interface let's overwrite the run method and this class will have some sort of task ID let's call this as task ID initialize the same inside the Constructor and it can give some message something like task then task ID plus being executed by trade dot current trade. get name then we will put this threade to sleep for let's say 500 milliseconds and Surround this with try catch now let's write the main method and we have execut a service let's call this as service and what we want is new cached threade pool let's keep it inside the trate resources and one key thing to observe here is that there is no value that we are going to pass inside the new cach threade pool un like the fixed trade pool where we were passing the number of Trades that we want to create a front why is it so we will see in the next section for now let's run this thing probably for maybe th000 tasks so I less than 1,000 i++ and service. called execute new task one and give it a task ID of I so let's execute this and say it's output all right so here is our output you see all the tasks are being executed by the threads that are getting created so task 27 is being executed by the thread 28 task 41 is by thread 42 task 4 is by five and then Task 1 by two so that's it for the code demonstration now let's understand how does it work internally now let's visualize and learn the internal working s of cast thread pool executor in the case of casted thread pool we don't have a fixed number of threads so you see in the code implementation we did not provide any number to the method which was creating the new cast threade pool the task you here does not hold a number of tasks which we submit the queue here is of a special type and it's called as a synchronous queue the synchronous queue has space only for a single task so every time you submit a new task the casted threade pool holds the task in the synchronous queue and it searches for the threads which has been already created and they are not working actively on any task if no such thread is available the executor will create a new thread and add it to the thread pool and this newly created thread starts to execute the task which has been submitted let's imagine a scenario where 10 threads are executing 10 tasks at a given point of time and then 11th task comes in and since we don't have a thread 11 to C to this task the thread pool will create the 11th thread to execute this new task so in theory the cast thread pool is capable of creating thousands and thousands of threads you may wonder where does this stop if threads are getting created at such a rapid paase so what is the upper limit well there is a guard rail to keep the threade count in check after some time the threads are done EX executing the tasks and they are available to pick another task and if they don't have any other task they are killed and this time of idleness in 60 seconds so what I mean is let's say there is a trade and it has not received any task to work upon it will be terminated so cash trade pool is pretty much autoscaling in nature on the basis of task load it has when there are new tasks to be executed threads are added and when there are less number of tasks to be executed a few among the created threads could handle the task so the other threads the idle ones are retired and killed so that's how the cash threade pool executor Works behind the scenes so the basic takeaway from here is that the queue will contain only one task at a given point of time that is your synchronous queue and the maximum time for which a thread could be there in the thread pool without any work is 60 seconds so now let's move ahead and learn about the final type of executor which is the scheduled executor now let's write the code for demonstrating the scheduled executor service let's create this class let's call this as scheduled executor demo we will create a task let's call this as probe task it will implement the runnable interface let's overwrite the run method and run method is going to print a message let's say probing endpoint for updates let's create the main method so we have scheduled executor service let's call this ad service and from the executors let's call the new scheduled threade pool new shedu threade pool let's have the number of Trades as one and the scheduled executor service does offer a couple of methods in which we could schedule the thades to be executed we will go ahead with the implementation which is scheduled at fixed rate so the idea is that this particular threade will be executed at a fixed rate after certain point of time so the way to write this is service Dot schedule at fixed rate so we have the actual runable then the initial delay followed by the period and then the time unit so runable is the runable initial delay is after how much time this execution will start so let's say you could pause for 2 seconds and then after 2 seconds every 3 seconds this trade could be executed so let's write it so the runable here is the probe task and let's say the first delay that I want is of 1,000 milliseconds and I want this to run every 2,000 milliseconds and the time unit is going to be milliseconds now let's execute this and see how is the output like so we are seeing that this thing is getting printed probing in point for updates and it will keep on printing this every 2 seconds and it will keep on printing this probing in point for updates till the time we shut down the executor service so how can we do that let's stop this and write some code for the same so the way to do this is we can call the await termination method on the service handle this object and Supply certain time so it will wait for the executor to be terminated in that time range and if it's not terminated in that time time range it will call the shutdown method of the executor service which is going to gracefully terminate the executor service so let's write the code for same so we will put this inside a tri block because of a termination is going to thr an interrupted exception let's have the interrupted exception as well and uh and in the tri block we can check service. AWA termination let's give it a time of 5,000 milliseconds and then time unit is milliseconds so if the time is more than 5,000 milliseconds we can call the shutdown method so let's say shutdown now else if it goes to the interrupted block there as well we could call shut down now let's increment this time to maybe 10 seconds and now let's run the code and see its output so you see the delay that we had was of 2 seconds that every 2 seconds this should be printed and after printing it for five times that is like 10 seconds it got terminated on its own so that is how we could terminate the scheduled executor so of course this is a very crude and basic way of writing things in real world probably we could have this termination Logic on certain condition and when that condition is getting fulfilled we can call the shut shut down method and close the executor service so as I told earlier that this is one of the methods which is provided by the sheduled executor service I will suggest you that as part of exercise you could try to go inside the scheduled executor service and explore the entire interface as in what all different scheduling options are there for example we have a schedule then we have this schedule method signature then we have something called that schedule that fix rate that we implemented then we have have something which is called as schedule with fixed DeLay So I think it will be a good exercise for you to go inside this class and learn about this methods and the other thing is that we have called shutdown now so what it means is that this is a strict guideline to the executor service that whatever you are doing stop doing that and shut down the operations of the executor service there is another method however which is called as shutdown so shutdown is somewhat more graceful in nature and the way it works that it initiates an orderly shutdown in which previously submitted tasks are executed so whatever is there inside your task Q those will be executed and only after that the executor service will be shut down but once you call shut down then the new tasks won't be allowed to come inside the task queue but in the case of shutdown now all the actively executing tasks are also halted and stopped so this is the basic difference now we will have a quick look to understand and visualize the internal workings of the scheduled executor service so as we saw earlier that this trade pool is for the kind of tasks which we want to schedule in some Manner and the way this works is all the tasks are submitted in a task queue but this queue is a delay queue in the delay queue tasks are not kept in sequential manner we can understand this to be some sort of priority queue where the priority is the time of execution so if there are two tasks for example task zero it's supposed to be run after 10 seconds from the current time and task one is supposed to be run after 15 seconds of the current time then in that case task zero should come earlier as compared to the task one in the task Q likewise if task one is supposed to be executed first before task two then it should be coming earlier in the task Q so everything everything else Remains the Same other than the Q which is a delay Q in the case of scheduled threade pool executor how do we find the ideal pool size for the thread pool well like most of the answers in computer science the answer to this question as well is that it depends however it's worth discussing what it depends on so let's get started imagine that you have a CPU intensive task task which needs to be executed please note that in Java one thread is one O Level thread so if your CPU has four cores then at maximum you could execute four tasks in parallel at once so on a hardware which can only support up to four course creating hundreds of threads won't help if they are CPU intensive in fact it leads to a performance degradation let's focus more on why there is a performance degradation when you have 100 trades to run your CPU intensive task then all such trades will try their best to get their P of the time allocation with the CPU when this time slicing happens Beyond a certain threshold the expense of content switching overpowers the benefit we get from making use of the multithreading so lots of Trades contesting for CPU time is counterproductive and in such a scenario having too many threads is not a good idea what we can do rather is have the same number of threads as the number of course in your CPU but please note that having created the same number of threads as the number of course in your CPU does not ensure that all the course will be running your threads only some of the course will be utilized by other tasks and Os level processes as well so how can we do this let's say a quick code example in the executor service package let's create this class let's call this as CPU intensive task so let's create a task class and maybe let's call this as CPU task it implements our unable let's overwrite the run method and all it does is it prints some message like some CPU intensive task being done by the and thread dot current thre do get name let's write the main method in the main method first we will try to get the number of the course and we can do so by making use of the runtime so runtime. get runtime do available processors now let's create the executor service call this ad service executors do new fixed threade pool and it could accept the number of of course as the number of threads it wants to create let's print a message for debug purposes so created trade pool with course next let's execute our task so let's say I want to execute 20 tasks and service. execute and new CPU task let's run this and see the output here is the output so created thread pool with 14 cores so my machine has 14 cores and what it does is it says like some CPU intensive task being done by thread 1 thread 2 thread 10 thread 14 thread 14 so what we see is it has created 14 threads which is the number of cod and then those 14 trates are being used to execute the 20 tasks that we want to execute so now let's learn about the other type of task so there is another type of task which is IO intensive such IO intensive tasks are inherently fire and forget in nature this is because IO operations such as reading from files or making Network calls often involve waiting for external resources to respond during this waiting period the CPU is idle and having more threads allows other tasks to execute while one thread is waiting for the io operation to complete however here as well it's not recommended to go crazy with the countless number of threads rather the more St approach is to experiment with threade pool sizes this will help you to find the optimal balance between concurrency and resource utilization so to summarize there is no single answer for the question of what's the ideal thread pool size what you could do instead is analyze your task patterns and based on their type utilize a combination of the above mentioned approaches till now we have seen how can we execute trades using the executor service but something seems not complete in all the examples that we have seen so far we are not returning anything what if we want to return something from the thread execution how can we do that let's done about the same so here I have created a class this is called as callable demo and we have a class for the task which is implementing the runnable interface and inside the run method that we have overridden I'm trying to return something well of course it's giving some compilation error because at the moment there is no way in which I could return some value from this method why so let's go to the runnable interface and what we see here is that the runnable interface has just one method which is run and it's returning a void that means we cannot return anything so what is the solution so if we want to return anything from a thread the way to do this is implement the callable interface so callable is another interface provided in Java and the way it works is that you implement this interface and it's a generic based class so you'll have to pass the generics of what exactly you want to return so let's say that I I want to return an integer so I'll pass the same and now let's get rid of this one and see what are my implementation options so like we had run in the case of runable in the case of callable we have call and here whatever you provide in the generics the same thing is applied here that is the integer and let's say I want to return 12 so now this 12 should be returned whenever I'm going to run this trade but now this part of the code starts to give some error so what is the problem well the problem is that executor service provides two ways in which we can execute or submit threads to the executor service one is the execute other is the submit so let's replace this with the submit and now this should work fine so what submit expects us is to pass either a runable or a callable so even if you want to pass a unable that is fine with submit and for callable anyway you have to use the sub MIT method itself because you cannot submit a callable by using the execute method so now that we have called the do submit on the execut service what does it return should it return an integer let's find it out so what we see here is that this thing is going to return a future so what is a future future is again a class which is generic based and in this case since we were returning an integer it's going to supply this inte is a value over here and using the result we can do all couple of things and one of those things is calling the do get method on this result so let's print it out and it's giving some error reason is that it is expecting us to either add a catch clause or maybe surround with try catch so for now maybe I'll just add the exception method to the signature so now let's run this code and find out what is the outcome so yes indeed it's is going to return the value that is 12 and this is how we are supposed to make use of callable if we want to return some value from a thread execution but there are a few things we should be mindful of while using the colable interface I'm calling the do get method on the future class let's learn about the same so what we have here is the main threade so assume that this line is the main threade execution and then at some point of time you created a callable and then you did a submission of that callable so once that callable is submitted to the threade pool you will immediately get a future of the type of the generics for the callable that you have implemented and using the future object sometime down the execution you could call future.get so what happens is future is a placeholder and it's empty till the time the processing is not completed by your thread and that time could be anything depending on the execution that you have in your code right so let's say at this point of time you called future.get and the future did not have the final calculated value so what happens please note that future is a blocking operation what it means is that if you call Future and if there is no value inside the future then your entire main threade execution will be blocked so whatever was being executed by the main threade that will be entirely blocked let's say at some point of time the future got the value so you will be able to get the value from here and then the execution will resume or start so this is one thing that you should always keep in mind while making use of the future.get call so now let's demonstrate the same thing let's give another print statement let's call this as main trade execution completed let's run this so we can see that the result is pretty much instantaneous so we are getting the result as soon as possible now let's put some sleep in this threade so let's call thread. sleep maybe let's have a sleep timer of 5 Seconds and with this let's try to execute this one more time so we are waiting there is no result as of now so just after 5 seconds once the execution got completed then we see this line that main thread execution completed so basically what we did was we introduced some sort of latency in the call method and when we called result doget since the future did not have the result populated for this so the main thread was blocked and as soon as this future got its value the value was printed and the execution continued so is there any work around for this Behavior so result. gate has an overloaded method with the signature where you could pass in the timeout so what this will do is it will wait for a certain time before giving a timeout exception and in that time duration if the future does not have the value it will throw an exception and move ahead in the execution otherwise if you get the value in that timeout it will work as expected so let's try the same here it's expected that this call method will be taking 5 Seconds to run so let's try to put a time out of 1 second and let's pass the time unit of second this thing throws an exception let's handle that now let's run it one more time after 1 second it could not find the result and then it threw an exception that is the timeout exception and it ended the execution so if you don't want to end the execution this abruptly and probably continue with your execution you could handle this exception in some other manner so that the execution of your main thread continues now let's increase this value to 6 seconds and find out the result so now since we have provided a timeout of 6 seconds and this thread needs 5 Seconds to complete the execution got completed and then we had this message printed on the console that is main thre execution completed so this is good but is there any other way in which we could handle the f Futures so there are three important methods that we should know so the first one is future. cancel and it expects us to pass a true or false so let's say if I pass true and let's say what exactly does it mean so this value is May interrupt if running so if you pass true then let's say if the threade is running then in that case it may get interrupted if you pass false then it will not be interrupted so this is about the do cancel method the next thing is Future dot is cancelled and this returns a Boolean so using this you can check in your code that the given future is it cancelled or not and based on that you could Implement certain functionality in your class there is another important method which is called as is done so it's result. is done and this again results into a Boolean value and what it signifies is that whatever threade was being executed has that completed please note that this will return true for both the cases where the thread is going to run successfully and return some value and in those cases as well wherein the trade is getting interrupted or may be exiting out due to certain error or exceptions this is the way in which we can make use of the callable interface and based on your use case you can use either runnable or callable to implement threads in Java Java has a rich set of collection classes however most of them are not thread safe we need to take precaution to ensure that the collections are behaving as intended when used in a multithreaded context there are two ways to do so the first approach is to use the collections. synchronized method and the second approach is that we can use concurrent collections which are essentially concurrent versions of the collections in this video we will learn about the first approach in order to demonstrate the uses of collections. synchronized method I have created this class called as synchronized collection so let's start by creating uh array list let's call this as list and I will be creating two threads the first one is called as one what this will do is it will add some value to the list thousand times so list. add I and let's close this one I will be creating an another thread and it will also add some value to the same list, times list. add I so let's move these things to the main method otherwise this will not work let's create a main method let's place the things here and now 1 do start to do start and in order to ensure that these threads are getting completed let's call the do join on these two methods second is to do join and now let's print the size of the list after the operation has been completed now let's run this program and see what is the output so you see the output is 1 146 however we are adding value to the list a, times and this is being done by two threads so what we should have expected is that the value would be 2,000 but here the ism is mat let's run it one more time and this time we are getting a totally different value so the point here being that it's leading to some sort of inconsistency when this is being used by different threads so how can we solve this so one approach is to make this thing as synchronized so as discussed earlier in the video that there are two approaches to make use of the concurrent collections in Java one is by using the collections. synchronized method and the other is to use the concurrent collection itself so here we will be learning about the synchronized method so let's do that and the wi to do this is let's comment it out let's create a new one so list integer list and we can call collections dot synchronized so you see there are lots of options synchronized list synchronized collection map s navigable map so and so forth but in this case we have to synchronize the list so we will be calling synchronized list and we will pass an array list so this is the way in which we can create a synchronized list using the collections. synchronized method and based on your need you could also invoke other things like synchronized map and synchronized set and all those things so let's run this thing one more time and see the output so we are getting the value as 2,000 so basically list. size is 2,000 now let's run it one more time and now we should be getting the same value through and through so this means now there is no income consistency in the list even though we are using it across multiple threads so the list is a synchronized list now so uh this is the way in which we can use collections. synchronized to help us with creating a concurrent collection however there are certain downsides of using the collections. synchronized approach let's learn about those one by one so the first one is score scint locking what it means is that it uses a single lock to synchronize all the operations on the collection this means not only one thread can access the collection at a given time even if multiple threads are performing unrelated operations this can lead to contention and reduced concurrency especially if there are frequent read and write operations on the collection from different threads the second is the limited functionality and what do we mean by the Limited functionality well the synchronized rapper returned by the collections. synchronized list does not expose any additional methods for fine grin locking or custom synchronization strategies this limits your ability to optimize synchronization based on specific uses or patterns of your application so when we discuss about the concurrent collections we'll try to cover how exactly those collections provide certain other custom synchronization strategies as well so the third downside is that there are no fail fast iterators so collections returned by collections. synchronized list do not support fail fast iterators the fail fast iterators through a concurrent modification exception if the collection is structurally modified when an iterator is iterating over it without fail fast iterators it's possible for concurrent modifications to the collection to go unnoticed and this can lead to certain unexpected behaviors the fourth downside is performance overhead so synchronization introduces overhead due to lock acquisition and release this overhead can degrade performance especially in the high throughput or latency sensitive applications so if at all you don't need these four things then probably you could consider making use of the collections do synchronize method in your use case but let's say if you require some of these functionalities or these features uh it's better to go with the concurrent collections which are provided by the Java so now let's start learning about those concurrent collections let's learn about the countdown latch countdown latch is a synchronization utility that allows one or more threads to wait until a set of operations which is being performed in another threads completes it is part of the Java util concurrent package and can be used for controlling the flow of execution in the concurrent programs the key concept of countdown latch is that it maintains a count that it starts from a specific number and decreases each time the countdown method is called threats that need to wait for the countdown to reach zero can call the await me method which will block until the count becomes zero so when to use countdown latch let's understand the scenario in which we can use countdown latch with a very simple and easy to understand example imagine you are a take lead who is leading a complex project being the expert which you are you divide the complex task to some smaller and simple to implement tasks after splitting the task you distribute them with your team and they start working on their respective tasks however now you cannot proceed further on this project until they all complete their task in scenario like this you can use a countdown latch countdown latch is like a checkpoint for each team member's progress you set up the latch with the total number of task or team members each member upon completing their task calls the countdown method of the latch it indicates that they are done as the organizer you wait for all the tasks to be completed by calling the await method this method hals your progress until the count reaches to zero meaning all tasks are done once all the tasks are completed the latch opens up and you can proceed with the next phase of the project so countdown latch helps in coordinating multiple threads or tasks to synchronize their work it ensures that they all reach a certain point before proceeding further so let's have a quick code demonstration on on how we can use the countdown latch for developing certain use case we are in the concurrent collection package and I will create a class let's call this as restaurant and uh I will Implement a task let's call that as chef and let it implement the runable so before we move ahead with the implementation let's discuss a bit about what we are planning to implement so what we are planning to build here is a simulation where a group of Chef need to prepare different dishes in a restaurant kitchen each Chef is responsible for preparing a specific type of dish and the kitchen manager wants to start serving customers only when all the dishes are ready so here is how we can use the countdown latch to coordinate all the chefs so let's proceed with the implementation of the chef class so first thing is let's overwrite the run method so with that in place let's have a few properties of the class so first is uh string which is name the second is again a string which is a a dish and Third Field is going to be the countdown latch itself let's initialize all these fields in a Constructor so now that we have initialized all the fields in the Constructor here is what we are going to have in the run method so uh we try to simulate the preparation of the dish and for that let's have a triy block and then a catch Block it's going to throw an interrupted exception let's call this as e and for now just throw a runtime exception and for the tri block what we will be doing is first we print a message like name is preparing and then the dish next let's have a sleep time of 200 milliseconds and the idea is that this is the simulation for the cooking time so essentially your Chef is going to cook for 2 seconds quite a fast Chef right he is cooking in just 2 seconds anyway so let's have another print statement and what this will be printing is name plus has finished repairing and the dish so till now what we have tried to mimic is that the chef is going to prepare some food and once they are done preparing the food this message will be printed and what it means is that the task has been completed so what we will do is we will call latch do countdown so that should be it for this class that is the chef class now let's implement the main method in the restaurant class so let's have an integer value let's call this as number of chefs let's say we have three chef and what we will do now is we will create the trades for the Three Chefs and then we will start those trades the way we can do this is new thread and uh it could accept a new Chef let's name this as Chef a let's assume the chef is preparing pizza and then we pass the latch so we have not created the latch yet we will create in some time and and then let's call the do start method on this one so let's create the latch so let's have the countdown latch let's call this as latch and new countdown latch number of shfs let's create a same kind of threade for the another Chef as well so new Chef let's call this guy as Chef B and let's assume that this guy is preparing fter so let's call Larch and let's call start let's duplicate this and uh let's say this guy is preparing maybe salad and this guy is Chef C so when we have created the threads and called the start on these threads these threads will start running at some point of time and then we should make use of the latch somehow in order to ensure that this entire thread of execution that is this one is blocked till the time all these three threads are completed right and the way to do this using the latch is call the await method what does a method do it will ensure that the thread is waiting for all the dishes to be completed eventually three threads are there all those three threads are done with their execution and and a wait will expect us to handle some exception and uh either we could add into the method signature or surround with trr for now let's add to the method signature that is an interrupted exception and once this is done then we can print a message that all the dishes are ready let's start soling customers so this is what we have implemented let's try to run this and see its output so when we call do start the message is printed that Chef C is preparing salad Chef a is preparing Pizza Chef B is preparing this fasta and after 2 seconds all these three ships are done and what they do is they print Chef C has finished Chef a has finished Chef B has finished and when these guys are finishing with their preparation they will be eventually calling the last dot countdown and since we have defined a value of three that is the number of shfs which was three here so this countdown latch has a value three to begin with and the weight works is that last. a we will hold true or rather it will hold the execution of the thread on which this has been called till the time it does not reach zero and by the time the third Chef is done that is Chef B has finished preparing the pasta then in that case that threade will be calling the latch dot countdown and once that countdown is invoked it means that the countdown becomes the counter rather becomes zero and at that point of time this will be unlashed and then the threade will be unblocked for the further processing which is system.out and this message will be printed so this is what the countdown latge does and this is the way in which we can make use of this for our use cases now you may have a doubt you may think that in the beginning of the video we learned about the join method so is it not very functionally similar to the join so let's understand that so from the purpose perspective countdown latch is designed to allow one one or more threads to wait until a set of operations in other threads complete it is typically used for coordination among the multiple threads join on the other hand is used to wait for a thread to complete s execution before proceeding with the rest of the code and it's specifically used for thread synchronization within a single threaded context so how exactly are these two things different from the user perspective so the countdown latch is useful when you have multi trades performing independent tasks and you want to coordinate them before moving forward and join is useful when you have a main threade that respawns worker trades and needs to wait for them to finish before continuing its execution so I hope these points should have drawn a clear picture of the differences between the countdown latge and join method however you might have one more doubt left you may think if there are three threads and there is a countdown latch for them so either use the countdown latch or call join methods three times which is on all the three threads is it not the same right so why take a pain of learning A New Concept all together think of the scenario where you have a dynamic number of threads so what do you do how many times and in what Manner are you going to call join method on those threads in situations like this countdown latch shines as compared to the uses of join method overall countdown latch is a much cleaner and elegant approach especially if you are coordinating among multiple thread operations so the final thing about the countdown latches can we reset the count well countdown latch is supposed to be used as a oneshot solution so you cannot reset the count if you need a solution which resets the count you are looking for something called as a cyclic barrier and with this let's learn about the cyclic barrier next now let's learn about a very important concurrent collection in Java which is blocking queue since it's quite commonly used we will try to learn it in somewhat more detailed manner imagine a blocking queue to be like a conveyor belt in a factory where items are placed and taken off in Java a blocking queue is a data structure which allows multiple threads to safely put items onto the queue and take items off the queue and this is done in concurrent manner there are two aspects of blocking queue so the first part is the blocking aspect the term blocking means that if a threade tries to take an item from a queue which is empty it will be paused or blocked until an item becomes available similarly if a thread tries to add an item to a full queue it will be blocked until the space becomes available the second aspect is the uses of the queue so the blocking queue follows the first in first out principle meaning the item that was added first will be the first one to be taken out now let's talk about the blocking queue interface in Java the blocking queue interface is the parent interface to a few other interfaces and the concrete implementational classes so this is the parent interface which is the blocking que and the two such interfaces which extend this blocking Q are blocking DQ and transfer Q so this is also sometimes called called as blocking deck or DQ so whatever you wish to call you could call it now coming to the blocking deck so blocking deck is a doubleended q that blocks threads when it reaches its capacity or becomes empty thereby facilitating flow control between producers and consumers it provides methods to access the queue from both the ends in a thread safe manner due to its doubleended nature the performance characteristics of blocking deck may differ from those of blocking queue especially in the scenarios where there is a contention for access to both ends of the deck by multiple threads simultaneously key takeaway from the implementational perspective here is that it provides access from both the ends in a concurrent manner regarding the transfer queue it's a specialized queue where producers can block until a consumer directly receives an item it extends the functionality of blocking queue by providing a method called as transfer what this method does is it allows one threade to transfer an item directly to another waiting threade potentially avoiding the need for blocking if there are no waiting trades transfer behaves like put and blocks until there is a space available for the item transfer queue ensures a strong hand of coordination what does this mean let's understand this with an example imagine a factory where one worker produces items and another worker packages them for shipping the producer worker can directly transfer the produced item to the packaging worker without waiting if the packaging worker is ready to receive it if the packaging worker is busy the producer worker Waits or blocks until the packaging worker becomes available to receive the item this direct mechanism can improve the efficiency in certain scenarios compared to a traditional blocking queue so now let's learn about the major implementations of The Blocking queue the first one on the list is array blocking queue it implements a bounded blocking queue and it is backed by an array data structure the next one is the linked blocking queue so this one implements a bounded or unbounded blocking queue backed by a link list the priority blocking queue implements a blocking queue that orders elements based on their natural ordering or according to a specified comparator then we have delay Q what it does is it implements a blocking queue of delayed elements where an element can be only taken out when its delay has expired it is particularly useful for scheduling tasks to be executed after a certain delay or a specific time the final one is the synchronous queue this implements a zero capacity blocking queue where each insert operation must wait for a corresponding remove operation by another threade and vice versa so based on your requirement and uses you could use either of these implementations but for most practical purposes the uses of array blocking queue are linked blocking queue or a priority queue should be good enough now let's have a look on the blocking queue operations so these are the certain operations which are supported by the blocking queue put method adds the specified Element e to the que if the space is available if the queue is full the operation blocks until the space becomes available next is the take method it retrieves and removes the head of the que if the queue is empty the operation blocks until an element becomes available the third one on the list is the offer method what this does is it adds the specified element to the Q if the space is available it returns true if the element was successfully added or false if the Q is full as opposed to the put method which blocks until the space is available pole method retrieves elements from the head of the queue it returns null if the queue is empty it does not block the Q if the Q is empty as opposed to the take operation which blocks the Q until an element is available the peak method retrieves but does not remove the head of the queue it returns null if the queue is empty some of the above methods also provide overloaded signatures which allow you to pass timeout values as well so I suggest that you explore the blocking queue interface there you'll find all the variations of the measor methods which we have described here so all the implementations of The Blocking deck interface are functionally similar methods which provide these operations on both ends as an exercise you could try exploring the interface and the implementations to understand them better so now let's have a code demonstration which will showcase the uses of The Blocking queue interface in order to demonstrate the users of The Blocking queue interface I'll be creating this class inside the concurrent collection package let's call this as blocking queue demo here is what I'm planning to implement it will be of some capacity and there would be a couple of threads one thread would be a producer thread then we can have a couple of more threads for example let's say two threads as the consumer threads and once we start the threads then the producer thread should be able to produce or write data to the que and consumer threads should be able to consume from the queue so let's implement this so let's start by having the Q capacity let's call this as Q next we can have the blocking Q itself so let's declare that and we want to put an integer value in the blocking queue let's call this as a task q and we are going to make use of the implementation of array blocking queue let's pass the capacity that is the Q capacity and let's start by implementing the main method first on the list would be the the producer thread here in the producer thread what we will be doing is we will be iterating for certain amount of time and then putting that number to the task CU so let's Implement that so we have a tri block let's have a catch block as well we will have an interupted exception so let's put this for now and now coming to the dry block we will have a loop which is running from 0 to 20 0 to 19 rather or maybe let's change the numbers from 1 to 20 and all it does is it puts this number to the task CU so let's call put and the value would be I let's print a message task produced I and let's have a sleep timer for let's say 100 milliseconds so this is essentially the task generation time that we are having here and now that we have written the producer thread let's write the consumer thread as well so let's have the first consumer thre let's call this as consumer 1 and let's rename this to producer so what this consumer thread does is it consumes from the task CU infinitely so let's Implement that so let's start by having a triy block let's have a catch block let's have an interrupted exception and uh let's throw this one as a runtime exception so while true what we are doing to do is we can retrieve the task which we have in the task Q do te and then we will call a method let's call this as process task so let's pass the task and uh the consumer ID so let's now implement the process method before implementing the second consumer so private static void process task then we have the in task then we have the consumer name and let it through the interrupted exception because I wish to provide some sort of sleep in this particular method so first we start by printing a message that task being processed by consumer name and then print the task then we will try to sleep this for 1,000 milliseconds so the idea is that in real scenario as well when you will be processing a task that will involve certain time right so here since we have a pretty much Bare Bones implementation what we are going to do is we are going to provide some sort of sleep and that will mimic the processing time which is 1,000 milliseconds in this cas case and now once we have done the processing we can print the message the task consumed by consumer name and this is the task and uh now we will be implementing the second consumer thread as well so let's call this as consumer 2 and we will have a similar kind of implementation here as well so let's copy this one really quick and what we are going to do is everything Remains the Same so this is going to run infinitely it's going to extract out the task from the task Q process task Remains the Same it's just that we change the name here so now let's call this as consumer 2 and everything else Remains the Same so now let's start the threads so producer. start cons consumer 1. start consumer 2. start so I think this should be it for the implementation side of things let's run this code and see how is the output like so as you can see that task being produced is being printed by the producer method and then task being processed by consumer one and uh task consumed by consumer one so we had introduced some sort of sleep right so during that time another threade takes over that is the producer threade and it's going to produce so basically task being produced so task ID with two is produced and kept in the queue then uh this task is being consumed by consumer 2 as soon as the task is available in the uh processing queue right so this is the way in which the producer is uh creating tasks and pushing to the queue and the consumer on the other hand which we have a pair consumer one and two they are pulling the queue and they are taking out from the queue and working on that and uh consuming the tasks in the task queue so you could try to implement it yourself so using the block Q we have implemented a simulation wherein we are writing to a threade safe concurrent data structure which is the blocking queue and we are able to operate on the data structure using multiple threads that is the producer thread and two consumer threads so you could go through the code one more time and try to get a hang of it if it's not very clear so as an exercise you could go ahead and explore these classes and try to come with certain use cases and Implement those use cases using these classes so this will really help you with your understanding of the uses of The Blocking queue in the Java now let's learn about the concurrent map in Java so concurrent map is an interface in Java which represents a map that can be safely accessed and modified concurrently by multiple threads it extends the map interface and provides additional automic operations to support concurrent access without the need for explicit synchronization concurrent map is needed in Java to handle the situations where in multiple threads need to access and modify a map concurrently concurrent map has a couple of implementations such as concurrent hash map concurrent escap list map concurrent linked hash map concurrent navigable map Etc out of these concurrent hashmap is probably the most widely used one and this should be sufficient for most of your practical use cases so now let's have a quick code demonstration and uh we will Implement a simple feature using concurrent hashmap which showcases how this concurrent collection can be used in a multi threaded context so here is a brief idea behind what I'm trying to implement for this demonstration so I am planning to implement uh concurrent cache and uh basically there would be a couple of threads that will be writing certain values to the cach and uh those trates will be reading the value from the cash right so how we could could achieve this using the concurrent hashmap is what we are going to build so let's start first we have the data structure that will be holding the cache which is a map of course and it's a string string let's call this as cach this will be a concurrent hash map now let's implement the main method and we are going to iterate from is0 to I 9 so let's assume that this I value is the trade count so maybe call this as threade number and this is I and what we will be doing is we will be creating a thread so let's create this thing anonymously and finally we will also need to call start on this one so we have written start here so first thing is we create the key and the key could be a key string at the rate of value and then the thread num what we will do is we will try to Fitch the same key three times so that we could demonstrate that one time the cache is empty so we are going to put the value in the cache and for the next two times the value should be faced from the cache right so in order to mimic that functionality let's write an another loop so from z0 to J Less Than 3 j++ and uh let's capture the value which is going to be let's say a method called as cast value let's pass the key we will implement this gate cast value in a moment for now let's just have this method name itself now let's print some message here so the message could be threade threade do current threade dog name let me format this one and uh let's say key pass the key value and value pass the value so this should be it for the main method now let's implement the other methods which is the cach value for now so let's call this as private static string gate cast value let's say the parameter is the key and what we do first is that we try to fix the value from the cache so cache doget key if the let's correct this spelling so if value happens to be null it means that the key is not present in the map so we need to calculate the key somehow the value rather somehow so let's have another method for the same let's call this as compute pass the key it's not implemented we will Implement us in a while now let's put the same value in the cache so cach output key then the value and finally return the value all right so let's implement the compute method as well so that we can complete our entire implementation so let's return this as an string so compute it takes a key and first we print a message which is key not present in the cache so I'm going to compute now let's put a try and catch block because we will be needing this so now what we are doing to implement here is that we have a compute method and it's going to compute certain value so we want to mimic the logic for computation and uh so that will need certain time to execute so let's mimic the Same by putting a sleep here maybe for 500 milliseconds and once it's done return the value that is value for key so I think this should be it for the entire implementation now let's run this one so let me clean this up a bit all right let's run this and find out its output so what we see here is that the thread get is started and uh first is that key five not present in the cache so going to compute then key zero not present in the cache so going to compute so essentially uh for all the threads from 0 to 9 the key has been created that is key at the rate of five key at the rate of Zer so and so forth and in the first try they are not present in the cash so uh those will be calculated now and once they are calculated they getting getting faced from the map itself from the cache itself so let's try to find this for key atate 5 so here five is being faced right so the for the first time it's being created by the compute method and for the this two times it's being faced in this manner wherein you have the thread five and the key is this one and the value is value for key 5 it's same for the other Keys as well for example for key 7 for key 6 for key four and so and so forth so what we have implemented essentially is uh concurrent cache and the idea is very simple that there are a couple of threads which are going to put certain value in the cach and uh get the value out of the cache how it's being done is like in the G cast value implementation itself we are trying to fetch the value from the cache if it's not present we are Computing it and putting in the cache if it's present then this will not be executed and directly we will be returning the value so the first call to the cach is a cash Miss and then the cash will be populated with the value which is being generated by this compute method which is doing nothing it just has a sleep timer of 500 millisecs and then the value is returned as value for this particular key and once the value is put in the cache then for the subsequent calls the cach is being uh queried and we're getting the value from it so as you you can see that we have not used any um lcks or synchronization and all those things so it's a very neat implementation because we are using the concurrent hashmap and this particular collection is is a concurrent collection and we don't need to handle any sort of synchronization as such so that should be it for the implementation and the code demo now let's learn a few more things about the concurrent hashmap so let's understand about the internal implementation and working of the concurrent map let's break this into two parts the first is the adding an element to the concurrent hash map and the second is fetching an element from the concurrent hashmap and why we are learning about the concurrent hashmap itself the reason is that it's the most commonly used implementation of the concurrent map interface and for most of the use cases this should be sufficient for you as well so let's learn how the elements are added to a concurrent hashmap so as the first step we start with hashing and determining the segment internally the concurrent hash map is arranged as a smaller segments when a new element is added to a concurrent hash map its key is hashed to determine which segment it belongs to and each segment acts like a small hashmap within the larger concurrent hashmap in the Second Step the lock is acquired so once the segment is determined the thread needs to acquire the lock for that segment this ensures that only one thread at a time can modify that particular segment and in the third step data is inserted in this segment with the lock acquired the new key value pair is added to the segment's internal array if needed the segment May reside itself to accommodate the new element and finally as the fourth step the lock is released after the insertion is completed this allows other threads to concurrently modify the other segments of the map and likewise here are the steps that are needed for fetching an element from the concurrent hashmap so here as well the first step is hashing and determining the segment when a thread wants to fetch an element from the concurrent hash map it first hashes the key to determine the segment which it belongs to and as part of the Second Step the activity is similar to adding an element the thread needs to acquire the lock for the segment that contains the desired key in the third step we search in the segment once the lock is acquired the thread searches for the key in the segment's internal array please note that since only one thread can modify the segment at a time the search operation is safe from concurrent modifications if the key is found the corresponding value is returned if not found null or other designated value is returned to indicate absence of the element being searched for and as the final step after F operation is completed the lock for that segment is released here onwards other threads are allowed to access and modify the map concurrently so so this process continues over and over again for all the data rights and deeds from the concurrent map and this is how the concurrent Map works in Java so as we can see that here the Locking and synchronization of the concurrent map is on a very granular level as compared to the approach of synchronization or creating the synchronized collection wherein the synchronization was being done on the entire map or the entire collection itself self so this is pretty much faster in that sense because we are not holding up other threads from modifying or interacting with the map and this is the reason that concurrent map is a better choice when you are dealing with a multithreaded context and you need to have some sort of map based collection imagine you are playing a game with a group of your friends the rule states that everyone must together at a certain spot before you can all move forward to the next level this spot is like a checkpoint now you cannot move forward until all your friends are there with you in Java this is like using the cyclic barrier each friend represents a thread in your Java program when a thread reaches the checkpoint it calls a weight method on the cyclic barrier the thread Waits there until all the other threads have also reached the checkpoint once everyone is there the barrier is stripped and all threads can move forward now let's have a quick code demonstration using which we will understand how we can make use of the cyclic barrier in our Java program so to demonstrate the cylic pario let's create a class I'll call this as multistage tool the idea here is that I want to Implement a feature wherein there is a group of tourist along with a tour guide and there are different stages of the tour and during a stay the tourists are supposed to arrive at a particular location and once all the tourists arrive at a particular location then they will move along to the next stage of the tour with the tour guide so this is what we are going to implement overall this is a small simulation to demonstrate the cyclic barrier users so let's implement the same so first let's have the member variables first we will have the number of tourists so let's call this as public static final int let's say this is five then the next one we could have as the number of stages let's say this is three and let's create the cyclic barrier public static final let's call this as the cyclic barrier let's call this as barrier and new cyclic barrier so cyclic barrier does accept a number which is going to be the count on the basis of which it's going to maintain that how many threads have returned to the barrier point so let's provide the same and meanwhile let's also check the other options that we have so one option is this Constructor there is another Constructor as well which is giving us the uh integer value that we are supplying the parties basically and along with that we could also Supply the barrier action so what is the barrier action so the barrier action is the command to execute when the barrier is stripped or null if there is no action so I think for demonstration purposes let's include a small action for this so let's provide a unable here and what we are going to provide is a simple message that is tool guide starts speaking so don't worry if it's not making sense as of now as soon as I finish the implementation I will explain everything in as much details as needed to understand this concept so let's quickly create the static class as well which is going to be the tourist class and it's going to implement the runable interface let's have the run method overridden and uh it will have a member variable has tourist ID we will initialize this tourist ID inside the Constructor and inside the run method what we do is we iterate for the number of stases and during each iteration we do some operation so let's do that so for in I as zero I less than number of stases I ++ and the first thing that we try to do is we introduce some sort of sleep in the thread this is to mimic the activity being done by the tourist so let's do it so we will put this inside the try block let's have a sleep of th000 milliseconds and in the catch block let's have the the interrupted exception let's throw it as runtime exception for now and uh once the tourist is done exploring the area whatever activity they are trying to do in this particular time frame then we are going to print a message the message could be tourist then the tourist ID arrives at stays then i+ one so what we are trying to achieve is that in the given stas the tourist comes the tourist performs certain activity that is the sleep for th000 milliseconds essentially in 1,000 MCS they do some activity and then once they are done with their activity we print this message that tourist with so and so ID has sort of completed their activity and that is the reason that they are arriving at a particular stage where from we are supposed to move to the next stage but catch here is that it cannot move to the next stage until and unless all the other tourists have arrived as well and that is where we are going to make use of the cyclic barrier so as I mentioned mentioned in the theory section that we can call barrier. weit so we will do the same so let's put a weit here and it expects us to provide certain error handling let's do that and uh it's a weit not weight so let's correct it and when we do await then we will have to add another catch Clause as well that is broken barrier exception let's collapse this to one catch block and uh this should be it for the run method now let's go to the main and implement it so what we want to do is that for all the tourists that we have that is five tourist number of tourists I want to create threads for each and every tourist and then I want to start their thread so let's do that so for INT I as0 I less than number of tourists then i++ let's create the thread let's call this as tourist thread new thread and new tourist the thread ID could be I itself let's let's call the start on this one so now let's run this and see the output so here is what we are seeing there is a delay of 1 second and uh during that 1 second all the tourists are there doing their activity and uh they're arriving at stage one so once all the tourists have arrived at a stage one that is 0 1 2 3 4 then the tour guide starts speaking and uh assume that this is the instruction about the next stage of the tour right and again the similar thing happens wherein the tourists go to the second stage of the tour and they do their activity and then the tourists arrive at the stage two before progressing forward for the next stage and the same thing happens here as well and since we have just three stages this is where the tourist stops but the key thing to note here is that till the time all the tourists have not arrived the tour is not progressing to the next stage so essentially the tour guide or the tour group is sort of waiting for the time being until all the people in the group have arrived to a particular point to a particular checkpoint so that is what we have implemented here let's go through the code ones so first we start by initializing the number of tourist and number of stases and we have a cyclic barrier it's accepting two things one is the number of parties involved that is the number of tourists so it's going to track five tourists the five tourist threads and then there is a runable which is going to print this message once the barrier is getting tripped or barrier is getting broken right and uh in the main what we have done is we have just created number of tourist uh threads and we are calling do start on them and uh in the tourist class itself which is implementing the runable the key area to look here is this run method wherein for all the number of stages what we do is first we allow the tourist so you can visualize in this way that let's say that you are at the stage one and when you are at a stage one then you will be doing certain activity and like you the other tourists as well in the group they would be also doing certain activity and let's say everybody is doing certain activity in certain time range so this is what is being mimicked by this thread do sleep and once you and other tourists like you in the group they are done with their activity we print a message that tourist with this ID has arrived at these stage and then we call the barrier. AIT so let's say this is your threade which is being executed then your threade will call barrier. await and what this barrier. AIT will do is it will wait for all the other threads which are part of this group so basically we have created this thread group right and uh this threade is having the same cyclic barrier which is this particular cyclic barrier and this cyclic barrier is being used by all the threads so like you there could be other tourists as well and they would also be calling barrier dot AIT and essentially what what will happen is until and unless not everybody in the group that is all the five threads have not arrived at this particular position till that point of time the further execution will be halted what could be the further execution it could be anything so this is the way in which this is going to work and in this way we can make use of the cyclic barrier in a code please note that once the barrier do await is Reed and once the barrier is broken the barrier gets reset on its own you don't need to do it explicitly or manually so the barrier count is again uh initialized or reset rather with the initial value that you have provided which is five in this case so you could use it for the next round so that's about the code demonstration now let's learn about how does cyclic barrier work under the hood under the hood a cyclic barrier uses a combination of of a counter and a condition to manage the waiting threads when you create a cyclic barrier object you specify the number of threads that must call the await method before the barrier is broken each thread that calls the await decrements the internal counter of the cyclic barrier if the counter has not reached zero yet the calling thread inter a waiting state so let's say the calling thread is U and you have called barrier do a we but what the cyclic barrier finds out that this internal counter is not reached zero yet so let's say to begin with it was five and then you called it so it became four so it's not zero of course so what happens is you go into the waiting State and when a specific number of threads have called the await the barrier is stripped so let's say that after you other four threads came in and they also called barrier. AIT and eventually the value of the count internal count became zero so at this point all the waiting trads are released and they can proceed with their execution so that is where all the waiting threads that is youu and the other four threads will be released they will be notified that they could come out of the waiting State and after the barrier is stripped the internal counter is resets to its initial value and the barrier is ready to be used again for sub sequent synchronization points this reusability distinguishes cyclic barrier from the countdown latch which cannot be reused once the count reaches zero so this is pretty much about the cyclic barrier and the way in which we can use it and uh we also learned about how does it work internally under the hood so that's all about the cyclic barrier now let's learn about the exchanger in Java an exchanger refers to a synchronization point at which threads can pair and swap elements within a concurrent environment the Java util concurrent exchanger class facilitates this functionality the way this works is that two threads can call exchange method on the exchanger object passing the object they want to exchange why and me to use exchangers exchangers are useful in scenarios where two threads need to synchronize and exchange data before proceeding with their respective tasks for example imagine you have a pipeline of processing steps and each step is executed by a different trade in such a case you might want to use a exchanger to exchange data between adjacent stepes in the pipeline so how is it exchanger implemented in Java exchanger is part of java util concurrent package exchanger is a class and it does not Implement any interface or extend any class also exchanger is not implemented by any other class in Java it is a standalone class which is specifically designed for synchronizing the exchange of data between two threads the class has two important methods first is exchange which an object and this method is used to perform a blocking exchange operation it waits until another thread arrives at the exchange point and then exchanges the object X provided by the current thread with the object provided by the other thread it Returns the object provided by the other thread if the other thread has not arrived yet the current thread will block until it arrives the other method is the exchange variation it's an over loaded method which is going to accept an object and other than that it will also expect us to pass our timeout it's similar to the above one except it has a timeout parameter if the other thread has not arrived at the exchange Point within the specified timeout duration a timeout exception is thrown now let's have a quick code demonstration which will show us how we can use exchanger in our code so here we are in the concurrent collection package let's create the class let's call this as exchanger demo so what we are going to implement is two threads let's call them as first threade and second threade and then they will have exchanger at their disposal using which these two trades will be exchanging certain information with each other so let's start by implementing the first threade let's call this as class first threade implements runable so let's implement the run method it will have the exchanger and uh it's a generic class so we will have to pass the type let's say we pass the type as integer let's name this as exchanger let's initialize this exchanger inside the Constructor and in the run method let's have a data to send variable with a value 10 and uh we will print this value so first trade is sending data data to send and uh now let's call exchanger do exchange and pass the data to send value this will return a value and uh let's capture that as part of a variable which we could call as received data and exchange will expect us to handle some exceptions let's surround this with TR catch and once we have received the data let's print it out on the console so first creade received data now let's implement the second thread as well and once we are done with the implementation we will see how it's interacting with each other so let's call this as second trade implements runable let's have the exchanger here as well and we will pass the integer as the generic type initialize this particular extension in the Constructor and in the run method let's start by having a sleep of let's say 3 seconds and here as well we have a data to send variable with a value 20 you will print some sort of message second thread is sending data and let's say data to send and then exchanger do exchange let's pass the data to send let's capture this as part of received data and let's print this on the console so second trade received and received data let's handle these exceptions I catch and what we can do is we can also capture this and place inside the tri block so we should be good with the implementation of the second threade now let's implement the demo class and in order to do so let's have the main method in the main method let's have the exchanger new exchanger and uh we will be creating two threads so first one new threade and uh first threade let's pass the exchanger because the thread is going to expect us to pass the exchanger object let's do the same for the other thread as well let's call this as two we will start these two so 1 do start two do start so this should be second threade now we will run this and see the output so we can see that we are seeing first thread is sending data and second thread is sending data and second threade received 10 that the first thread is sending and the first threade received 20 that is the second threade sending but there is a key information that we must notice please note that in the first threade we are sending the data right away wherein we are calling the exchanger do Exchange but in order for this exchanger do exchange to be executed the other thread should be able to receive it that means as long as the other thread is not calling its exchanger do exchange then in that case this threade will be blocked and what we are seeing in the second threade is that before calling exchanger do exchange we are putting a sleep of 3 seconds so that is the reason we are observing a delay because threade one here is blocked for 3 seconds because when the execution comes inside the second trade it's sleeping for 3 seconds before calling the exchanger do exchange and once the execution comes here then it means that thread one will be sending its data that is data to send 10 and uh that will be received by the second trade and captured in the received data and likewise exchanger do exchange calls this data to send and this data to send value that is 20 will be received by the first threade which is captured by this received data variable and the same will be printed with this value that is First Rate received received data so let's run it one more time to observe the blocking nature of the exchange method so so we can see that first thread is sending data and there is a delay before all of these things are executed so we called First Trade is sending data then exchanger do exchange was called and because the second threade went into the sleep for that amount of time that is 3 seconds the first threade got blocked and as soon as the second threade is active The Exchange happens and the first threade receives the data to send value from the second threade which is 20 and likewise the second threade is receiving the data from the first threade this is a data send value of 10 so this is it for the code demo I hope it would have somewhat cleared your doubts with respect to how exactly can we use exchanger now back in the theory section let's do a quick comparison between the queue and the exchanger so you may feel that a queue could achieve similar kind of synchronization between the threads right and yes you are partially correct in thinking so however the choice between using an exchanger and a queue depends on specific requirements of the synchronization pattern which you are trying to implement and the characteristics of your application so here is a quick comparison the exchanger facilitates a direct exchange of data between two threads at a synchronization Point each threade Waits until both the threads has arrived at the exchange point before proceeding if you have exactly two threads that needs to synchronize and exchange data the exchanger provides a simple and efficient mechanism to accomplish this exchanges occur synchronously meaning both the trades wait for each other at the exchange Point both traes exchange data of the same type and quantity making it suitable for symmetric communication patterns the que on the other hand however is different from the exchanger in these parameters so the first is that cues are more versatile and could handle communication between multiple producer and consumer threats and depending on the Queue implementation producers and consumers May operate asynchronously producers can inq data without waiting for consumers and consumers can theq data without waiting for the producers qes can act as buffers allowing producers to continue producing data even if consumers are not immediately available to process it and in some cases The Exchange may not be symmetric however the exchanger is very similar to a type of q that we saw earlier and that type of queue is called as synchronous queue so both synchronous queue and ex change of facilitate blocking synchronization between threads however while a synchronous CU is unidirectional the exchanger happens to be bidirectional so with the use of exchanger you could achieve a bidirectional data exchange between two threads which means when you do call do exchange data at one hand you are sending certain value but you are also receiving certain value from the second thread as well so that is the B directional nature of the exchanger so that should be it for the topic of exchanger now let's learn about the copy on WR array copy on WR array is like having your own copy of book to read when someone wants to write in the book instead of disturbing your reading they make a new copy this way you can keep on reading without any interruptions you can use copy on right array when you have multiple threads accessing and modifying data at the same time it ensures that readers don't get disturbed by writers and writers don't interfere with each other making your program safer and more efficient let's have a code demonstration which will tell us how we can use copy on right array in our Java code we are in the concurrent collection let's create a class and let's call that as copy on WR array demo so what we are going to implement is two threads one is the read task and the other one is the right task and as the name suggests that read task is going to read something and WR task is going to write to something and what it's going to read and write from is the copy on right array so let's start the implementation let's create the read task and this will Implement a runnable interface let's implement the run method and uh let's have a list of integers let's call this as list the type would be integer let's name this object as list and we can initialize this using a Constructor and what we want to do inside a y Loop is we want to put some sort of uh sleep to the thread and then print the list so that is is going to mimic the infinite reading operation so let's do that so while true let's call the thread. Sleep and maybe it could be for one second let's do the exception handling and uh once it's done sleeping we will be printing the list so that's about the read task now let's create the right task and it will implement or unable let's implement the run method and this as well is going to expect us to pass a integer so let's do that and uh let's have a random generator and and uh we could have a Constructor for the random we could just say new. random and in the run method we are going to do two things infinitely first is we will wait for certain time second is we will be writing certain value to the list so let's do that so let's have the while loop while true trade. sleep let's have this for 1200 milliseconds let's handle the exceptions and outside the TR catch block let's call list. set random. next integer and it's going to be in the size of list and then random do next int and the upper limit is going to be 10 so what we are doing essentially is that we have a list and in that list we are picking any random index and on that random index we are setting any random value that is in the range of 0 to 10 so that's about the right task now let's write another class that is going to mimic the simulation that we are trying to do so let's call that as simulation let's have the list let's call this as private final list integer list and uh let's create the Constructor so what we want to pass instead is new copy on right array list and uh let's remove this one because this is not needed now and and uh to this list I want to add certain values so array do aslist and uh let's pass some zeros all right so we have basically instantiated a list made that as a type of copy on right array and in that particular copy on WR array we have provided some values which is all zeros now let's have a method call that as simulate and what this thing is going to do is it's going to create certain threads so first could be new right task let's pass the list let's duplicate this this one could be two this one could be three and this one could be four this as well could be a right task this as well could be a right task and let's have this one as a read task so basically we have three writer task and one reader task finally let's start these 3 do start 4 dot start and in the main class let's have the main method to begin with and uh we will create the object for simulation simulation new simulation and then let's call simulation. simulate so let's go through the code once before we run this so what we have done effectively is we have created four threads three out of which are the wrer threads and one is a reader thread what the writer thread is going to do is is it's going to run infinitely and uh every 1.2 seconds it's going to set certain value in the list that we are going to provide which is a specifically the type of copy on WR array and the way it's going to write is it's going to pick any random index and going to place any random value and uh what the read task thre is going to do is it's going to execute infinitely it's going to run infinitely and in every run it's going to sleep for 1 second and then print the list and finally we are running all these trades and uh this is basically we are creating the object and calling the simulate method which is going to do all of these things so now let's run this and see the output so you could see that to begin with we have this array then the writer thread wrote something and then uh that is being read by the reader thread so it keeps on happening that the writer thread will be writing something to the array that is pretty much random and then the reader thread will be reading from the array the point to note here is that this is being done in a multithreaded context and we are not leading to any exceptions for example any modification exception or any kind of uh incon consistency so situations like this when we should have copy on right array list and uh this particular data structure will help us in ensuring that an array list is being used in a multithreaded environment without any of the synchronization or concurrency concerns now let's learn a few more things about the copy on right array so now we will learn how it works so when a thread wants to read from the array it creates a snapshot of the array and uses the same to read if a thread wants to write it also creates a snapshot of the array and performs the right operation once the right is completed the changed array is considered as the latest version of the array from which snapshots could be created for the read and write operation this way readers don't see the changes until they get a new snapshot it ensures that they always have a consistent view of of the data so we could Loosely draw panels between the git branching approach and the way in which copy on WR array Works each modification to the array creates a new version much like creating a new branch in the git just as git would allow for parallel development without conflict copy and right array allows for multiple threads to modify the data concurrently and it does so without interfering with each other and just just as git consolidates changes during the merging copy on wrer eventually consolidates the modifications into a single reference version this version ensures data consistency both systems provide a structured approach to managing concurrent changes so I think it would make sense to visualize what we just discussed so let's do the same so let's assume that this is our array and it has certain values as in 3512 and the first call on this array is a read call imagine this line to be the reference line and consider that this line has the latest value of the array so whenever a read or a write operation is supposed to be done those operations are going to create certain snapshots so the first call that we are getting on this array is a read call so as soon as we have a read call a snapshot of this thing is created and the read will be performed on this one so what effectively happens is that even though if there is any right operation that is going to happen it won't impact this value because we have taken a snapshot of this particular array at this particular time since this line is the source of Truth during creating a snapshot the same value remains to be seen here and this is the array from which the read operation will be served now let's say going forward at this point of time a write operation is invoked what it says is that write a value four at index 2 so since this latest value or the reference value has this array that is 3512 a snapshot is created which is 3512 and on this snapshot the right operation is applied so as soon as the right operation is applied the array changes and the value becomes 3 5 4 2 and the same value would be written to this source of Truth or this line or this reference right so let's say we have a read operation now and this read operation comes after the update has been written so since this reference has this value of 3542 the read operation is again going to create a snapshot and now the snapshot will contain this value 3542 so when the read operation is served the same value is returned that is 3542 and this process keeps on happening so you see what we have is our reference value to begin with and a line which is serving as a source of Truth and whenever we are going to have any read operation we are creating a snapshot and whenever we are having a WR operation we are creating a snapshot and after that right operation is complete then the same is merged back to the main line so that is the reason I told that it is somewhat similar to the way in which G branching works so you could consider this as the master branch and uh read is anywh readed it's not going to write anything or impact the data but still we want to avoid a situation where in the master Branch could change during the read is being performed so that is the reason the snap is being created to serve the read as well but it's very evident that in the right scenario uh another branch is created and on that Branch whatever rights are supposed to be done those rights are done the four in this case at index two and once this is done once it is successful the same thing is written back to the main line to the main branch and once this thing is merged now the main branch has this particular value that is 35 42 so this is the way in which you could visualize the right on copy array and I hope that it will help you in understanding this concept in a much better way lcks are an important topic in Java so let's learn about the locks what exactly are locks well imagine you have multiple threads running simultaneously in your Java program and all are trying to access the same resource without proper synchronization it could lead to inconsistency in data and other chaotic situations that's where locks come in handy locks provide a way to control access to Shared resources ensuring that only one thread can access the resource at a given point of time thus it helps in preventing data corruption and other concurrency issues you may wonder that it's sounding very similar to what the synchronized blocks do well you are right in thinking so so now let's learn the difference between using synchronized blocks versus the locks in Java when it comes to managing concurrent access to Shared resources two commonly used mechanisms are available these are synchronized blocks and locks let's start with the synchronized blocks synchronized blocks use the synchronized keyword to ensure that only one thread can execute a particular section of code at a given point of time they provide intrinsic locking which means that the lock associated with the object is acquired and released automatically by the jvm synchronized blocks are easy to use and require less boiler plate code compared to the locks however they have limitations such as lack of flexibility and lock acquisition and inability to handle the interrupts on the other hand locks provide more flexibility and control over locking mechanisms Java lock interface and its implementations allow to manually acquire and release the locks moreover you could acquire and release the locks blocks in any sequence and in any scope which is not possible if you use synchronized approach so when you should use synchronized blocks versus locks use synchronized blocks for simple synchronization needs where flexibility and performance are not that critical use logs for complex synchronization scenarios where fine grain control and flexibility are required in conclusion both the synchronized blocks and locks are essential tools for managing concurrent access to a shared resource in Java understanding their differences and choosing the right synchronization mechanisms for your specific requirements is a crucial part of writing a robust stand efficient concurrent code so now it's time to learn how can we Implement and make use of locks in our code while doing so we will also learn about a couple of important concepts related to Locks such as conditions different types of logs Etc till now we have learned that locks are an important part of implementing the concurrency in Java but we also need to have some sort of interaction between the threads and locks so the mechanism which helps us in managing these interactions is called as condition let's understand what are conditions with a very simple and easy to understand explanation here I would request you to make a very weird assumption which could help in avoiding any confusion with the terminology of lock assume that lock here is the key as well what I mean to say is when you have access to the lock the door which is guarding the protected resource is unlocked magically and thus the lock is a key as well in that context hence as soon as you acquire the lock you get access to the resource please note that only one person is allowed to access the resource no one else can tailgate behind you with this in place let's learn further a lock in Java can have its own set of rules these rules are called as conditions effectively this means that a lock could have more than one condition associated with it conditions help us in controlling how threads interact with the lock you could visualize threads as people who are trying to access some protected resource and think of condition as a way waiting room attached to the lock imagine a person comes up and tries to get hold of the lock but lock is being held by some other person thus this guy won't be able to acquire the lock so what he does instead is he goes to wait in this room where other people may also be waiting for the lock to be available inside the waiting room people wait until someone signals that it's their turn to use the lock now this signal could would come from a person that is currently holding the log or from some other source so who gets the chance to acquire the log well that's something which depends on the implementation the key takeaway from here is that after a signal is given any one person from the waiting room is allowed to acquire the lock and do their processing in Java these conditions help in managing the interaction between threads and locks now let's visualize how conditions work imagine there are two threads thread one and thread two to begin with threade one is executing something however it cannot move forward until some particular condition is met this condition could be anything based on your use case for example a que being full could be a condition in this case as soon as the thread cannot move forward it will call condition do a wait if you read this from the right it appears as if a threade is saying that I am waiting for this condition to be fulfilled so I am awaiting for a given condition effectively the thread one goes to a waiting State now let's imagine there is another thread thread 2 and this thread is running and doing some operations due to the operations being performed the condition gets fulfilled and as soon as the condition gets fulfilled this trade to calls condition do signal it signifies that the condition has been fulfilled so convey this message to any of the threads which have been waiting for this condition to get fulfilled as soon as the signaling happens jbm finds all the threads which are in the weight State anticipating this condition to be fulfilled and wake the one which has been waiting the longest in this example there is only one thread which is thread one so the state for thread one goes from bit state to the renable state so the threade one which was blocked at this point of time is now active from this point of time because at this point of time threade 2 signals that the condition for which threade one was waiting it has now been fulfilled so thre one could proceed with its processing now let's analize this scenario where we call condition do signal all like we had condition. signal there is an another method signal all so in this example we have three threads and during its execution threade one calls do of wa and after some time threade two also calls doit there is another threade which is threade three and during its execution the condition for which thread 1 and thread 2 are waiting gets fulfilled so the thread 3 calls condition do signal all as soon as signal all is called all the threads which are waiting for the condition to be fulfilled build are waked up by the jvm jvm pulls them out of the blocked State and moves them to the runnable state so depending on the number of course your CPU has threads could be scheduled for Execution accordingly so if dot signal was called instead of signal all thread one would have been waken up since it's the one which has been waiting for the longest so this is the way in which condition helps us into maintaining or managing the interaction between the threads and the locks there are two methods that is signal and Signal all which are used to notify that a particular condition has been fulfilled and there is one method which is condition. which is used to hint that a condition needs to be fulfilled before a given thread could proceed further in its execution in case if you are wondering that condition methods discussed above are very similar to the weight and notify methods in the object class you are correct indeed these methods are similar to wait and notify the main difference however is that conditions provide a much granular control over the synchronization of the protected resource now with that in place let's have a quick code demonstration which will teach us how can we use the conditions and its related methods let's create a package and we can call this as logs and uh let's create a class call this as condition demo and here is what we are going to build so basically we are going to build a producer consumer implementation using the locks and conditions so let's start so first thing that we will have is a Max size which is the number of elements that could be kept in a buffer which is going to be a q so let's have that so it would be private final integer Max X size let's have the value as five the next thing is we will Define the lock so lock and the type of lock that we are going to use is reint lock so reint lock is one type of lock in Java we will have to create the Q or the buffer where we are going to place the elements so let's create the que let's call this as buffer and uh this could be of type link list so basically from this buffer the consumer will be consuming and the producer will be writing to it next we will be creating certain conditions one condition would be that buffer not full and the other condition is buffer not empty so let's create the same so private final this is condition and uh buffer not full lock the way to create the condition is to call the new condition method and we will do the same for the other condition as well so this condition could be something like uh buffer not empty and we could have log. new condition so we will have two methods the first one is producer so private void produce and in item so let's have this and uh so what we will be implementing is the produce section of the implementation and uh we want this to be synchronized while this particular code block is getting executed we don't to want something else or let's say some other thread to come and work on this simultaneously so the order to do this is first we will have to acquire the loog and the way to do this in Java is called log. talk and once we are done with our uh processing finally we need to call log. unlock so this looks fine but there is a problem let's say when the processing is under the way there is some exception which happened so in that case the code will terminate then and there itself right the lock. unlock will be never executed and the other threads which are waiting for this lock will go into a state of Deadlock lock and that is not good so the way to do this is call this lock. unlock in the finally section or the finally block so in either case this lock. unlock is always executed so this is a common pattern which we use while making use of the locks in Java and this is the way to implement this so let's have the tri block and here we will put the processing right and we will have a finally block and that is where we call the lock do unlock so with this in place let's implement the processing part so here is what we are going to implement now first we will check if the buffer is totally full then for that duration we will have to make the producer thread weight otherwise we will put some item in the queue and then we will notify if there is any consumer thread which which is waiting for the producer to put certain item in the queue so let's implement this so we start by checking if the buffer size is equal to the max size which is five if that is the case then we have buffer not full and on this condition this threade will wait after that we will add certain item to the buffer and next we will have a print message which say something like produced and uh produce this particular item and for the buffer not empty we are going to put the signal so let's also complete the consumer method then we can have a complete look over the entire implementation so let's have private void consume and like the produce method it's also going to throw an interrupted exception so let's have that in the method signature we start by acquiring the lock so lock. lock and we will follow the same pattern that we saw earlier that we will have a try and finally block and in the finally we call the unlock method and in the try we have the implementation so here we will check if the buffer is empty and if it's empty then we call Buffer not empty and await on this condition and if this is not the case then we are going to consume from the buffer and the way we do this is we print a message which says consumed and uh from the buffer we take out the element by calling the do pole and next we call a signal on the buffer not full so this is the consumer method so you may have a doubt here what you may think is that the nomenclature of buffer not full and buffer not empty is kind of misleading and confusing so I totally understand that and I also had this confusion for quite some time but the way to interpret this is that this nomenclature should be understood from the perspective of this signal so so the condition is buffer not empty so what it means is that this buffer not empty condition is being awaited by certain threade and which threade may await this the threade which may await this is this one right that is going to consume so if it's consuming then at that point of time it needs to have something in the buffer otherwise there is nothing to consume so that is the reason if the buffer is empty then on this particular condition this consume method is going to wait and let's say if it's waiting for something to be put in the buffer and then the producer method is executed by some thread the producer thread then at that point of time something was put in the buffer some item was put in the buffer and that is where once we have put something in the buffer we are going to call this signal so what essentially we are saying is that buffer not empty condition is is going to signify that something needs to be there in the queue for the consumer to consume and that is the reason consumer is going to wait on this condition that is buffer not empty likewise we have buffer not full so buffer not full is being signaled by the consumer and the idea Remains the Same right so when producer is producing something then it needs to ensure that the buffer is not full and if it's full then it will have to it and this condition is being signaled by the consumer and what it means is once the consumer has consumed something then there is certain space in the queue in the buffer in which the producer could produce so that is the reason if there is a producer which is waiting for this condition then this particular thread the producer thread will be awakened when this buffer not full is signaled so that is the idea and that is the intention behind this nomenclature and behind this implementation now let's complete this by writing the main method and then we will have a code demonstration by running the code so let's implement the main method and uh let's have the condition demo object created so new condition demo right and uh let's have the producer thread let's call this as producer trade and uh let's have a triy block and have a catch block as well so when we call producer it's going to throw that interrupt exception so that is the reason we need to have it and we are also going to introduce some sort of sleep so that will be handled by the catch block itself so what we are going to do is we are going to run this producer threade for times and uh we call the demo do produce and pass I in this one so I is the item that needs to be put so essentially this producer threade is going to put 10 items in the Que in the buffer next let's have the uh thread dot sleep maybe for one second and in the catch we have uh a runtime exception which is this e so this should be for the producer thread and uh for the consumer thread let's have another one first let me rename this one to Consumer threade and uh again we will have the similar pattern we will have have a try and catch let's have the interruped exception and uh let's through the runtime exception and what we do is we are going to call consume 10 times so I less than 10 and demo do consume and we are going to put a sleep for let's say 2 seconds and once we have this threade created we will have to call the dot run on these two threads so first is producer thread. start then consumer thread. start so that should be it from the entire implementation perspective now let's run this and see the output so you see we have a demonstration ass simulation of the producer cons a problem so produced 0 consumed 0 produced One consumed one and uh produced two produced three so remember we had put a sleep timer of 1 second in the produce method and 2 second in the consume method so in that sense consume method is on the slower side and that is the reason here we have uh the producer being producing something twice and consumer is kind of on the slower side so we are producing twice and consumer is going to consume something once right but this pattern is not something which is very deterministic the entire idea behind having that uh sleep was just to showcase the demonstration uh in somewhat presentable manner right so that should be it for the conditions in Java and uh please note that these conditions could be only derived from the lock interface implementations which are reint lock read right lock and any other implementations which you could explore in the jdk and uh these conditions are not available for the logs that we have on the object level the implicit locks which are available with the object in Java now let's learn about the reant locks reent lock is one of the many implementations of lock interface in Java it allows a threade to acquire the same lock multiple times without causing any Deadlocks what this means is that a threade which holds a lock can acquire it again without blocking itself if the lock is reentrant in contrast with a non reentrant lock an attempt to acquire the lock again within the same thread without reducing it first would typically result in blocking it could also potentially cause a deadlock situation if it's not handled properly how does this this work the way this works is that the locking mechanism keeps track of the lock being held by a thread in the case of nonintent locks attempting to acquire the lock again within the same thread typically results in blocking the thread however in the case of reinen lock the lock mechanism allows the same thread to acquire the lock multiple times without blocking itself effectively the held count for the lock by the given thread is incremented since the lock is already acquired there is no sense of reacquiring it this is achieved by maintaining a count of how many times the lock has been acquired by the same thread the lock is only released when the count reaches zero it indicates that the threade has released the lock the same number of times it has acquired it you may be wondering why and in what situations a threade would need to acquire a lock again without releasing the log here is the answer imagine a scenario where a threade invokes another method or code blog which also requires the same log to maintain consistency in the multithreaded context this pattern is quite common in complex implementation s methods call other methods within the same class these methods require synchronized access to some shared resource and in such situations reant locks are quite useful and in fact very much needed so with that in place let's have a very quick code demonstration of using the reen clogs so to begin with I have created this class called as reint loog demo and here are the two fields that we are going to need so in this line we are initializing the reant lock the way to do this is by calling new reint lock there are other variations of this particular Constructor wherein we could pass the fairness criteria and all those things we will learn about those in a while so for now we have created the log and then we have a variable called as shared data which is initialized to zero next we have a method a what this method does is it's going to increment the value of the shared data and then it's going to call another method which is Method B and uh method a needs to get hold of the lock for this section of the code block that is incrementing the share data and then calling the method B and finally this particular lock is released now let's see what we have in the method B so in the method B again we are going to work on the shared data and the way it's being implemented is the Shar data value is getting decremented but we need some sort of Lock and that lock is going to protect this particular section which is decrementing the share data and printing the value and finally we call the unlock method so what we see is that we have a method a which is getting called by some thread and it's going to acquire a lock but in the same block when it's uh being protected by the lock there is a call to Method B and that method b as well is going to need that lock because that is also handling some sort of critical section so this is the reason that we have made use of the reentrant lock so this particular thread is already holding this lock but at this point of time this will need to acquire the log again and if it was not a reant log we could have encountered a blocking scenario and that could have also led to a dead log so that is the reason we have used the reant log now let's see the further implementation so we have a main method and what we are doing is we are uh creating uh and starting multiple threads which are essentially uh five threads and we are calling the start on those so let's try to run this and see the output so here is the output that meod a had share data as one method B has shareed data as zero method a and Method B so and so forth so the key takeaway is that we are able to deal with the shared data value in a concurrent uh environment in a concurrent fashion in multi threaded context and the same thread is going to need to acquire a log multiple times and that is the reason we have used the reint log so at any given point of time let's say here if you would have called log. G hold count then the value would be two so essentially what is happening is the first time the lock is acquired here and the second time the lock is acquired here so essentially this trade has acquired this lock twice but under the hood of course it has been acquired just once it is just the count which is getting incremented and which is two and finally this lock will be unlocked it will be released so the count will become one and then the Call Comes Here and finally this lock. unlock will decrement the value further and it will become as zero and that is where we will conclude that the given threade has released this particular lock so uh that is for the code demonstration now let's look further into certain other aspects of reant lock which is the lock fairness so Reen lock provides a Constructor in which we can pass a Boolean value for the fairness of the lock when this value is true the lock is called a fair lock if it's false it's called as an unfair lock by default the rant locks are unfair let's understand this fairness with an example imagine there are a couple of threads which are trying to acquire a lock among all the threads which are competing for the lock let's say threade one gets hold of the lock all the remaining threads go to some sort of waiting queue after certain time threade One releases the lock by calling the unlock method at that point of time one of the locks in the weight queue will get a chance to acquire the lock imagine that out of all the threads in the weight queue threade two has been waiting the longest for acquiring the lock then if the rint lock is fair trade 2 will get a chance to acquire the lock and do its processing in the case of unfair inant locks there is no deterministic guarantee as to which threade gets to acquire the lock as a sweet side effect the unfair locks could potentially provide a beta performance as compared to the fair loocks what is the reason well they avoid the overhead associated with maintaining a weight que and enforcing some sort of deterministic ordering now let's learn about a few important methods of of the reentrant lock the first one is get hold count this method returns an integer which is the number of times the current thread has acquired the lock in other words it indicates how many times the current thread has successfully acquired the lock without releasing it the next one is the trylock and as the name suggests it is an approach where we request a thread to try acquiring a lock result is a Boolean which tells if a thread was successful fully acquired or not if the outcome is true we can proceed with processing and if false we can do something else and what we gain from this approach is if the lock has not been acquired then we are not blocked rather we can do some sort of other processing as well in our code there is another variation of trylock method which accepts timeout and time unit so this is an overloaded variation of the trylock method using this me method we can request a thread to acquire the lock and be blocked for the given time duration if the lock is being held by some other thread we hope that the lock becomes available during the waiting time period and that's where the lock is acquired by the thread which has called the trylock with duration and if not then of course the output of this particular trylock is a false and then we can do something else because we have not acquired theog due to X and Y reasons well trylock has a problem even if your reent lock is fair and trylock is called by a thread at the moment the lock is released by some other thread in this scenario The Waiting threads for this lock are not given the priority the new threade which made the trilock call gets to acquire the lock the work around for this problem is to pass a timeout value of zero time units and this will ensure that the fairness criteria of the lock is honored then we have a couple of auditory methods so we have something like is held by current threade and it returns a Boolean which tells if the current thread is holding a given lock or not the next one is gate Q length and it tells the length of the weight Q which is the number of Trades these two in particular are mainly used for debugging and profiling purposes but having said that if there is a need in your code implementation or in your business logic you could surely have have these Methods at your disposal the final one in our list is the most important one so the new condition as we saw earlier this returns a condition on the given lock and what it does is it helps in orchestrating the interaction between the thread and the lock so that should be it for the re interent lock in Java a read right lock is a synchronization mechanism which allows multiple threads to read a shared resource currently but only one threade can write to the resource at a time this mechanism is particularly useful when the resource is predominantly read heavy rather than write heavy let's understand this by visualizing an example imagine that there is a booking system using which you could book seats in a bus there are two main functionalities when it comes comes to the seat selection first is you should be able to view the available seats and the other one is where you can book a particular seat so in the image that we have we have threads from one to in which you want to view the available seats and threads from 1 to n which want to book a seat as for what we have learned already for any thread to be able to do some processing it needs to acquire the lock so let's assume that threade one which wants to just read the seat availability acquires the lock and proceeds with it please note that this is not efficient why so it is because there are other threads which also wanted to just read the data since they are not writing anything there would not be any concurrency issue involved if more than one threads acquire the lock and do its processing and this processing is just to read the data so how about if if we could somehow Club the reader threads together and it would be much efficient right and yes certainly it would be very much efficient and the read right log indeed does the same thing after some time the read threads are done reading the data and at some point of time the read lock is released once released either of the wrer threads could acquire the right lock and do their processing effectively in the case of a read right lock one writer thread could get hold of the writer log at a time or multiple reader threads could get hold of the read log at a time please note that we have two logs one for the reader other for the writer so even though we have two different logs two different objects only one can be acquired by your thread or a group of threads at a given point of time so either read lock is used by n threads or write lock is used by one trade but it can never happen that both the locks could be used at the same time ever so what we have seen essentially that there are two kind of threads the first category is that of the reader threads the second category is of the writer threads since reader threads are not mutating anything in the data they are safe to run together and that is the reason that all these reader trades could acquire the reader log at once and do their processing when it comes to the writer thread this could not be the case because writer thread is going to change something to the data and that is why only one writer thread is allowed to acire the lock so in such scenarios wherein you have a very much Clear condition of reader versus writer this is one such lock which could be used in such cases and please note that you would gain performance if your application is or rather if your use cases read heavy not a write heavy so now we will have a quick code demonstration to see how can we make use of the read write lock so as part of the code demonstration what we are going to build is a class which will have a counter there would be couple of reader threads and a writer thread the writer thread will try to increment the value and the reader threads will try to read the value and we wish to do so using the read write lock so let's implement it we will create this class in the locks package let's call this as shared resource we will have a variable let's call this as counter the value is zero then we will have the read write lock so read write lock let's call this as lock and we will be instantiating the object of new reent read write lock we have the first method which is supposed to increment the value and the way we do this is we acquire the lock so lock dot WR lock do lock and we will have the implementation inside a tri block finally is used to unlock the lock that we have acquired previously and what we will do is we will increment the counter by calling counter Plus+ and we will print a message the message could be something like thread do current thread. getet name and it writes and the value that is counter so this should be it for for the increment method now let's implement the other method as well which is going to get me the value of the counter in a given threade and idea is that this is supposed to be made use of by the reader threade let's minimize this one now back to the implementation of the G value first we try to get hold of the read lock and call lock on this one next we will have the dry block and inside the finally we will close the read loock that we have acquired by calling the unlock method and we will read the value for the counter so let's have the message like trade. current trade doget name and beeds counter so this should be for for the get Value method and for the read WR log demo we will create another class so let's call this class as read WR lock demo and uh we would have a main method inside the main method we create the object for the shared resource and we will create a couple of reader threads so let's do that so for I and inside the loop what I do is I create the reg thread so new thread and uh for INT J as zero J Less Than 3 j++ resource dot get value and now let's assign a name to this trade so let's call this as reader threade and pass the value as I + 1 now we will start this thread so let's call do start on this one for the writer trade let's create one so trade wrer trade new trade and we do this for five elements so share resource do increment and likewise we will provide a name to the writer thread so let's call this as writer thread and let's start this thread so let's run this code and see how is the outcome like all right so what we see is that we have reader trade and there is another reader trade the two that we have created right and they're reading a value of zero initially because the wrer thread has not been executed yet the value remains to be zero the initial value then we have the writer thread and what the writer thread is doing is it's going to increment the value five times and uh as a result what we see is the value gets incremented to five now we have the reader thread and the reader thread reads the value and the value that that it's reading is five again the reader thread is invoked and it's reading the value as five so this is how you could make use of the read write lock if you have such a use case wherein you want to bate the lock acquisition for readers and writers separately so that should be it for the code demonstration let's complete this section of read WR lock by learning about the behavior of weight Q in the read write log so let's visualize how the we cues are arranged and managed in the case of a redr lock in the case of a redite lock there would be two types of threads in the we Cube one type of threads are those which want to read the data and the other type is which want to write the data let's say that initially there is one writer thread and two reader threads in this case the reader threads will be allowed to acquire the readlock this is done because it's sufficient essentially what you are allowing is two threads to be executed as compared to the one once all these reader threads are done with their operation the right of thread baiting in the same que is allowed to acquire the right log what happens in the scenarios when there is a writer thread waiting in the queue and the reader threads keep on coming to the queue should they all be allowed because it's faster to execute well if we do so the writer threads will be staffed and this is not good so the this kind of lowlevel optimization is taken care of depending on the type of implementation you choose for the read write lock for example reant read write lock is one such implementation the key takeaway here is that read write locks are optimized for reading and writing operations in the most efficient manner possible and you as a developer should be good to make use of this lock without much optimization or configuration concerns so whenever you have such requirement consider making use of this lock let's learn about this very important concept of visibility problem before we start learning about the visibility problem let's take a important detour to understand and visualize how cores registers and caches are placed along with the ram the image here is showing a machine which has six cores on the first layer we have the course course are the piece of Hardware which are responsible for all the computer activity Loosely speaking code itself does not have any storage capability it can only compute but in order for the computation to happen we need to have some sort of data storage as well right this is where we have different type of storage units let's know about them in a very brief manner first on the list is a register every core has a dedicated storage component which holds the data temporarily during the execution of the instructions this storage component is known as register it has the least amount of storage capacity each core has a dedicated register the next one in the line is the component of L1 cache L1 cache is a small but a very fast memory it plays a crucial role in improving the performance of the CPU by reducing the data access time to and from the main memory which is also known as the ram each core has a dedicated L1 cache the next component is the L2 cache it is a larger capacity cache as compared to the L1 cache its role is to complement the smaller and faster 111 cache it does so by providing additional storage for frequently accessed data data and instructions L2 cach is integrated in two ways depending on the architecture of the processor in one approach the L2 cach is shared across different cores whereas in the other approach we could have dedicated L2 cache for a core the presentation that we have here we are showing a shared El to Cache where the cash is being shared by two course for example here Core 1 and Core 2 are sharing this L2 cache likewise core 3 and core 4 are sharing this particular L2 cache and so and so forth the third type of the cache is L3 cache it sits between the L2 cache and the ram which is also known as the main memory its role is to provide a larger pool of cast data and instructions which can be shared among multiple CPU cores the final type of a stor layer is RAM and as told earlier that this is also known as the main memory it Serv serves as a temporary storage for data and instructions that are actively being used by the CPU so on the one end of the spectrum registers store the data specific to a core and on the other end of the spectrum we have Ram which stores data and instructions being worked upon by all the cores of the CPU when we go farther from the core towards the ram the latency and memory size increases so Ram would be of larger size as compared to L3 cach L3 cach would be of larger size as compared to L2 cach and likewise register is the one which has the least amount of storage capacity but since register is closer to the core as compared to the L1 cache and L2 cache registor would be faster in fact the fastest type of memory whereas Ram is the most farthest one from the core so it is the slowest of the lot so the ram would be the slowest memory and the registor would be the fast fastest memory let's see this diagram what we have here is a code snippet and basically we have two methods assume this write method is being used by a writer thread and this read method is being used by a reader thread and this particular code snippet is being executed in this kind of a machine wherein we have two cores and core one is executing threade one and core two is executing threade 2 so to begin with there is a variable able whose value is zero the count variable starts with value zero and assume at certain point of time thread one gets to execute on core one and thread two gets to execute On Core 2 so basically both of these two threads are running in parallel so the value of count is updated as one by the writer thread and at the same time the value count is loaded in the value so what would happen if you try to access the value of value by either printing it or debugging it so you may see something like this so to begin with count is zero that is in the shared cache then the count is updated to Value one and this value is there in the register which is local to Core 1 now Core 2 is running threade 2 and it is trying to access the value for count so remember that as of now the value of count is one is only with this register it has has not percolated to the shared cach level so the value that is present for this count variable is still zero so the value that is assigned to Val is still zero in this register and when we try to print this value we will get the value as zero as opposed to what we were expecting which should have been one so this is not something which is expected and correct right so this kind of behavior is called as the visibility problem Java provides a solution for this with a keyword called as volatile using this keyword ensures two things first is whenever there is a change in the value it's flushed to the shared memory second is whenever there is a value which is supposed to be read it's read from the shared memory the shared memory is usually and in most of the cases L3 cache which we saw earlier to be the shared cache and this one is a level up to the ram so basically what I'm trying to say is let's say if I used volatile with this variable so what I'll do is I will write this as volatile keyword then int count right and then when thread one is executed and any value to this variable is changed it won't be only there with the register rather it will be written to the shared cach as well and since this variable has been made volatile the thread won't read this value from the register or any other cache rather it will directly read from the shared cache so on the right we're ensuring that the value for the particular variable which has been made volatile is going to the shared cache and for the read as well we're ensuring that Valu is raid from the shared cach itself so that would avoid Us in Reading wrong value and this is how the volatile keybo helps us with the visibility problem there is a word of caution we should always follow while making use of the volatile keyword however while volatile does help with the visibility problem as a side effect it slows down the code and does the application it's a nobrainer that with the uses of volatile the many layers of caching are removed and so are the latency optimization benefits which we gain when we use the caching so when not needed the uses of volatile keyword should be strictly avoided dat loock is an important concept we should know about when we are writing multithreaded code in Java let's learn about the same what is a deadlock here is an image which tells us about a deadlock situation let's have a closer look here we have two trades which are running threade one at some point of time is going to need lock a which is represented by a green color color then after some point in time threade one is going to need lock b which is represented as blue color now let's look at threade two this threade is going to need lock b at some point of time and then it's going to need lock a after some time please note that in both the threads both the locks are needed to be acquired in order for the processing to get completed only after the processing has been completed both the logs A and B will be released let's imagine the scenario at time t threade one needs lock b while it has already acquired the lock a likewise threade 2 needs lock a while it has already acquired the lock b but both the threads won't be able to acquire the needed lock because their need is being held by the other thread and neither of the threads are in a situation to let go of their acquired lock since it's needed to complete the processing of the threade so in this situation none of the trades could proceed with their execution and this situation is called as a deadlock so here is formal definition of the Dee loock in a multithreaded context a de loock occurs when two or more threads are blocked forever each waiting for the other threade to release a resource they need to proceed this situation creates a cycle of dependencies with no thread able to continue with its execution so how can we spot deadlocks in our code there could be two approaches one is the manual approach other is the programmatic approach so let's learn about the same well detecting or spotting locks at the compile time is not an easy task this is so due to multiple reasons Java comes with multiple types of logs so multithreaded code can be littered with different types of explicit and implicit logs also there could be multiple sources of threads in the code base due to these two factors combined with the fact that threade execution and CPU location is not deterministic it's nearly impossible to figure out when which lock is going to be acquired by which threade moreover the example we saw earlier was a simplified version where we had two threads and two locks however in real world it won't be this simple there is a good amount of possibility to have multiple threads and multiple locks which could lead to a deadlock situation and thanks to all of these complexities it's nearly impossible to detect a date log by just looking at the code so the manual approach is something which is not very efficient neither it's the suggested approach so how do we detect the deadlock in code let's have a look on the programmatic approach of detecting the de log so now that we have learned that detecting a dead log is very difficult in manual way are there any other tools which can help us in this detection good news is yes there are tools and ways in which we can detect deadlocks in our code two of the most common approaches are taking the thread dump and uses of the thread MX Bean to programmatically detect cyclic thread dependencies which can lead to a Deadlock Lo so let's have a brief code demonstration where we will be knowingly introducing deadlocks in our code and then use these methods to detect the deadlock situation so in order to demonstrate the dead log code I'll create this package and let's call this package as other Concepts and uh let me create this class as as dat lock demo we will have two logs so lock lock a and make this as a reint lock let's duplicate it create an another lock lock b and we will have two methods first would be a worker one method other would be worker 2 method and uh let's Implement these so public void worker one it will start by acquiring the log a and then there would be some message which will be printed like worker one acquired log a next we will introduce some sort of sleep time so let's call thread. sleep for 200 millisecs let's handle the interrupted exception let's throw it as a runtime exception and uh then we acquire log B then let's print some message it could something like worker one acquired lock b and then we start by unlocking the locks which is lock a. unlock and loock B do unlock so this should complete the first method which is worker one now we will duplicate this and we will write the worker two method implementation what worker 2 does is it starts by acquiring the lock b and then we will of course change the messaging here and again we have a sleep for 200 milliseconds and now after log B is acquired log a will be acquired and again we will adjust the messaging and then we can have Lo lock a unlock itself and lock b unlock itself now let's write the main method where we will be creating the threats and uh executing these things so we start by creating the object for the class so that we can invoke the methods now what we can do is we can say new threade demo do worker one we will give this thing a name this threade a name and we call do start likewise we'll create a threade which will be making use of the worker 2 and we will adjust the name here for the threade which will be worker 2 so let's run this thing and see what is the output like all right so what we see is worker one has acquired lock a and worker one acquired lock b so essentially the worker one got a chance to run and after that point of time worker 2 was supposed to be run but now we are in a deadlock situation because worker one has acquired the log B and here work worker 2 requires log B in order to do its processing but since the worker one has acquired lock b and not released it yet the worker 2 cannot get hold of it and that is the reason we have entered it into a situation of Deadlock what we can do about it now well in such cases you cannot do anything but your code will be sort of in a DAT log situation and you will have to terminate it so as told earlier that we can either take a threade dump or maybe we could use the other approach of using the thread mxp to understand what part of the code is leading to this dat log situation right so let's understand the first approach which is using the thread dump so what we will do is we will go to the terminal so let me come out of the distraction free mode and here we go to the terminal and in the the terminal we will list all the processes which are running Java so we see that these are the processes and the one which we are concerned about is this deadlock demo and here is the process ID so what we do is we call kill hyphen 3 and we pass the process ID which is this one hit enter and go back to the output console so here we can see that we have lots of messages which are getting printed so this is something that we had earlier right where we had the deadlock situation now we are seeing something called as a thread dump which is the full thread dump and it's going to show certain information regarding the threade classes certain messages and so on so forth of course you could try to do it yourself on your system and uh read through all these things what exact is being told but the most important thing is in the last so scroll to the last and here we have this message and this thing says that found one Java level deadlock what does it say it says waiting for ownable synchronizer a fancy name for the lock which is held by worker 2 so worker one is saying that it's waiting for a lock which is being held by worker 2 and the worker 2 says it's w hting for a lock which is hailed by a worker one right so what you could do is you can scroll further down and you could see that this is one line which is being hinted in your code click it out and you could see that this is a probable reason so in your use case in your real world use case whenever you are faced with this kind of situation this is the way in which you could find out the process ID and kill it with hyphen 3 as the argument and this will help you out with this threade dump and you could figure out at least you have a point of Investigation where you could try to understand what is the DAT log situation and what part of the code is leading to this scenario so this is the approach in which you could analyze the threade dump to understand the date lock there is another approach however so let's go to the code and we will have to modify something in our code in order to understand that particular approach so what we are going to do is we are going to make use of this thing called as thread MX bean and uh we will create an another threade let's say a threade three which is going to execute forever and it will be done after an interval of every 5 seconds so the idea is to probe the code and to understand if there are any trades which are in deadlock so let's write the code it will be much simpler to understand that way so let's start by creating a thread and uh let's Supply the code for the same so the first thing is we have trade MX Bean let's call this as MX bean and it provides a management Factory using which you could create the threade MX Bean and uh while true what we can do is we can say mxb find me all the date log trades it will return a array of long which are basically the trade IDs and if the threade IDS does not happen to be a null so we can print this message that dat log detected and uh we can iterate over the long trade ID for each of the trade IDs in the trade IDs and then print this message the threade with id threade id is in deadlock and uh once we have printed this message what we need to do is break out of the infinite Loop and uh if that is not the case so we need to do this every 5 seconds so let's put a timer of 5,000 milliseconds and catch the interrupted exception and we will throw it as a runtime exception and finally let's call the do start on this thread so here is what is happening we have created a thread which is going to run every 5 seconds and we are making use of the management Factory to get an object of the mxp and the MX Bean has a method which will give me the deadlock threade IDs if there are any trades which are de loock and I am simply going to iterate over the given trade ID right and print the message here you may be wondering that trade ID is fine but what if I need certain extra information how can I do that well there is an another method in the thread MX Bean which is called as get thread info so let's make use of that as well so we can say mxb get thread info and it's going to accept a long array which is the threade IDS and what it returns is an object of thread info array and let's call this as thread info so thread info contains a lots of information as in what is the threade name what is the lock name uh where exactly it is waiting for which lck and all those things so in this example we don't really need to showcase all those things so but the idea here is that if needed you could certainly make use of this particular line and uh iterate over the trade info and get the required information so let's run this and see the output so run and initially we have executed these trades there is a timer of 2 seconds remember 2 milliseconds rather and then after some point of time this thread kicks in and it's going to say thread lock detected and thread with id21 is in deadlock and thread with id22 is in dat loock and as I told earlier that if you need certain more information you could make use of the threade info object or the class and uh these are the information it contains and you could make use of these information in your code as per your need so now that we have seen the code demonstration for the dead log let's understand how can we prevent Deadlocks so are there any approaches or suggestive measures using which you could get rid of the Deadlocks or at least try to write a deadlock free code so the first and foremost is use timeouts so let's say when your trade is trying to acquire a lock giving a timeout will ensure so it's not going to end up into a situation where it's trying to acquire a lock for infinite time right and thus it will help in avoiding the deadlock situation so uses of timeouts is a good approach in order to avoid Deadlocks the second thing is the global ordering of the locks so another thing we can do is to pay attention to the global ordering of the locks what it means is let's say we have two trades and two locks to acquire for example log a and log B both the threads should first acquire log lock a followed by lock b so in that sense it won't be a cyclic dependency scenario and it will avoid the DAT lock situation the third thing is to avoid any sort of nested locking so whenever feasible we should always avoid locking within already logged section and the fourth preventive measure is to make use of the thread safe Alternatives so whenever feasible to do so use thread safe collection and atomic variables and minimize the uses of logs sure the thread collections do make use of the logs under the hood however in general the code implementation of the standard threade safe are battle tested industry Solutions so there is a little to no chance of running into the deadlock situation so this should be it for the concept of Deadlock and uh these are the certain measures using which you could try to avoid dat lock situation and even if you are in one situation of a date loock what are the different approaches using which you could analyze your code to correct it Atomic variables are an important tool which help us in writing concurrent code in Java let's learn about the same so what's read modify right cycle in order to understand what is a read modify right cycle let's have a quick look on this image there are two threads and a variable called as count which has the value as zero there is a processing part which needs to be done on this value which is to Simply increment the value by the count one two threads get to execute this process of incrementing the value imagine if these two threads are going to be executed 1,000 times what we would expect is that the final value of the count should be 2,000 however this is not the case so th times the value will be incremented by threade 1 and another thousand times the value should be incremented by threade 2 effective value expected is 2,000 but when you try to run this code you will find the value would be less than 2,000 moreover it will be very much inconsistent so at sometimes you could get some different value and at the other time the value would be totally different so there is some sort of inconsistency let's learn why so well the reason lies in the fact that how do we actually understand the increment operation and how it is actually performed on the low level so on a very low level there are three steps which are performed when we increment the value of a variable as part of the step one the value of the variable is loaded in the register remember register is the local storage which which is the nearest and almost native to the processor core as the Second Step this value is operated on as part of which it gets incremented by one and finally as the third step the incremented value the new value is assigned back to the variable count so to understand with an example at step one the value is zero which is loaded in the register and at a step two it becomes one and finally at the step three the value one which which has been incremented from 0 to 1 assigned back to the counter this entire flow is called as the read modify right cycle and this is the core reason behind the inconsistency in the value if it's incremented across multiple threads since I have already covered this concept in more depth with examples I won't go into much details however if you could remember from the previous discussion what's exactly happening is that the entire process of increment ing the value count Plus+ is divided into three steps and and let's say if threade one is operating on the count operation which is involving these three steps and at the same time somewhere between these steps if threade 2 is going to interrupt then this problem will occur and this is the sole reason behind the inconsistency so now let's talk how can we solve this without using any kind of explicit synchronization mechanism so what are Atomic variables what we saw earlier is something called as a nonatomic operation what it means is essentially an operation for example incrementing a value can be broken down into three steps thus it's not Atomic something which can be broken down so what Atomic variables in Java do is that there are a type of variable that support Lock Free thread safe operations on single variables they ensure that any kind of read modify right operation such as incrementing or updating a value are performed automically it prevents race conditions and this makes them crucial in concurrent programming as they help in maintaining the data consistency and integrity without the overhead of traditional synchronization mechanisms Atomic variables provide builtin methods that perform Atomic operations it ensures that actions like incrementing and updating are completed as a single and non visible step this eliminates the need for synchronized locks which can be costly due to locking overhead and potential thread contention as a result Atomic variables offer a more efficient way to achieve thread safety in the concurrent programming there are different types of atomic variables the atomic variables in Java are basically classes each of these classes are used for a particular data type for example integer long Etc the Java do U.C concurrent do atomic package contains all such Atomic variables however Atomic integer Atomic Boolean Atomic long Etc are some of the important and the most commonly used ones so if you wish you could go to the concurrent Atomic package and explore the other Atomic variable options that we have now with that in mind let's understand a few of the basic operations that we can perform on these Atomic variables these Atomic variables provide quite a few operations which we can make use of as per our need however there are a few which are very commonly used let's learn about those the first one is get so it fetches the current value next one on the list is set operation and as the name suggest it will set some value the third one is compare and set and this operation expects two values which are expected and update it sets the value to the update value if the current value is equal to the expected value then we have have these two operations which are equivalent to the pre and post increment this operation automically increments the value by one and returns either the new value or the old value the fifth one and the final operation are list is gate and decrement and decrement and get which are equivalent to the pre and post decrement this operation automically decrements the value by one and returns either the new or the old value please note that these are the most commonly used operations and there are much more depending on the kind of atomic variable you're planning to use you could explore the respective classes to find about their supported operations in more details but having said that these are a few of the commonly seen operations that you would come across when you write the code related to Atomic variables now let's conclude the discussion on the atomic variables by going through a code demonstration and we will be using the atomic integer Ser as the one of the atomic variables to showcase the uses of atomic variables so in the other Concepts package let's create a class let's call this as atomic variable and what we are planning to implement it's some code which is going to work on a counter variable and there would be a couple of trades working on this and we will see how we could make use of the atomic integer to help us with the inconsistencies of multithreaded programming so let's start with having the count variable so the initial value is going to be zero and then we will have the main method so we will first create the threade one let's call this as threade one and what this will do is it will run for it will call count Plus+ for thousand times let's have the similar trade let's call this as threade two and it will also do the same that is it is going to increment the value by 10,000 times so let's start these two threads let's remove the one and type it as two we will have to wait for these threads to be completed so let's do that surround with try catch let's put two here and now let's print the count value so count value is count so let's execute this code and see what is the output like all right what we are seeing is the value is 14,113 which is not correct of course because what we wanted is that we have two threads which are going to increment the value 10,000 times so effectively the value should be 20,000 when we print it here but that is not the case so the reason this is happening is because we are using count Plus+ in a multithreaded context and which is of course not an atomic operation and due to the reason that we learned in the theory section this is the problem we are facing so how can we correct this so let's make use of the atomic integer so private static final Atomic integer let's call this as counter and let's create the atomic integer initial value is going to be zero so instead of having this count Plus+ let me comment it out and what we can have is counter do increment and get so what this increment and get does is it is going to increment the value of the counter which is zero to begin with and it will get the value so essentially if you want you could capture it somewhere but even if you don't capture it the counter object is going to hold the updated value so let's say if it's getting executed for the very first time and the initial value is zero then the value for counter will become as one because it's going to increment it and get the value which is self assigned if you want you could capture it as some variable as I showed earlier now for the second thread let's comment it out as well and again here we could call increment and get on the counter so that is about the uses of counter the atomic integer in the thread section now everything else Remains the Same and here we will try to print the value of counter instead of count so let's print it out and let's run this and say the output so here is the output what we see is the value is 20,000 let's run it a couple of times to see if it's working consistently so yes we are getting the same value over and over again what it means is the value is consistent and the problem that we were facing earlier of inconsistency because we using a nonatomic operation that was the count Plus+ that problem has been fixed when we started using the atomic integer so as I told you earlier that Atomic integer is part of the Java util concurrent Atomic package and likewise we have many other options so if you go to this package you will see that we have Atomic and we have Atomic Boolean Atomic integer Atomic array and so on so forth there are lots of options that we have in the atomic package so you could explore all of these things depending on your need but having said that this should conclude the discussion on the uses of atomic variables SEMA Force are an important concept whenever we talk about multithreading in Java so let's learn about the same what are semap fores a semap for is a synchronization mechanism used to control access to a shared resource in concurrent programming it uses counters to manage the number of allowed accesses to the resources preventing rise conditions and ensuring safe concurrent operations SEMA force can be binary or counting it depends on the number of permitted axess while I was learning this concept the terminology of semap 4 used to confuse me a lot the word semaphor is not very intuitive and it does not reflect something which is common in our daytoday uses so why do we name a particular concept at semaphore if you are in the similar situation let's learn what does this word mean the term semap for comes from the signaling devices used in Railway and M time contexts to control traffic in programming it similarly signals and controls access to Shared resources among concurrent processes essentially this term semaphore is not commonly used in everyday language among native English speakers it is primarily known in a specialized context like programming operating systems and historical references to Signal devices in transportation so with having known the definition of the topic let's understand how is this semap for used with some visualization imagine we are building an application which needs to connect to a third party service so this is our application and this is the third party service so this is the application and this is the third party service due to sub constraints the third party service could be accessed only by a limited number of Trades at a given point of a time so for example what it means is only three concurrent calls would be allowed to this third party service from our application so in order to implement such scenario we need to ensure that we have some sort of restriction in place in such situations we can think of making use of the semap 4 to implement such restrictions for example assume that our application has 50 threads in thread full however the third party service is going to allow only three threads so in that case we can think of using the semap for now let's understand the concept of SEMA for with this image let's start with an assumption that SEMA 4 at the core of it is nothing but a permit mechanism the image shown has three boxes these three boxes are nothing but permits so at the moment this semap 4 is going to allow three threads to be run so essentially if there is a threade that wants to execute it will ask the semap for that hey I want to be executed so in that case simaf for will check that does it have any permit or not so if there is a permit that permit will be given to a threade so basically this permit is like a token and once a thread acquires it then it's going to do its processing so this is what we mean when we say that at this moment this semop 4 is going to allow three threads to be run to begin with we have thread one which wants to execute so it calls acquire on the semaphore the acquire method is like it's requesting for a permit now that at the given instant we have three permits available the threade one is going to be allowed and it will execute and access the third party service so essentially thread one comes and it says Hey I want one permit to be executed sea 4 is like all right I have three permits here you take this one and you can proceed so once it's given you could imagine that this permit is no more so essentially you have two permits left and once this permit is assigned to thread one it goes ahead with its execution so thread 1 is essentially allowed to execute so once thread one has been given the permit Sima 4 will have two available permits and while thread one is executing and accessing the service let's say we have another threads which are threade two and trade three which come to SEMA 4 asking for permits and at this moment the SEMA 4 has two permits available so it will give the permits to threade 2 and threade three and essentially after these permits are given to to threade 2 and threade three they also go about with their execution so threade one starts with its execution and so does threade 2 and threade three after acquiring the permit now let's imagine at some point of time in Future threade 4 comes into picture and it's going to the SEMA 4 and say hey SEMA 4 I want to execute and in that to happen I want to acquire the permit from you so at this moment SEMA 4 is is like all right that's fine that you want to get executed but I don't have a permit with me as of now so what you can do is you could wait and due to this threade 4 gets blocked so threade 4 had called the acquire method and it's going to get blocked on the acquire method and sometime in future when thread one is done with its execution it will say all right I am done with my execution now I am going to release the permit that I had acquired so the permit will be given back to the SEMA 4 and as soon as that permit is given back to the SEMA 4 the threade which was waiting for a permit in the SEMA 4 will acquire it and proceed with its execution so let me revert back the image and now we have all the three permits available with the SEMA 4 so at some point of time Trade four got a chance to execute by acquiring the permit so this is the way in which SEMA 4 works so let's understand what exactly is this release method well thre says Hey SEMA 4 I'm done with my execution thus I'm returning back your token your permit that you gave me earlier and you could make use of it as per your wish so you could understand that SEMA for permits are nothing but kind of reusable tokens in that sense so if you notice at any given point of time in this semaphore we can only have three threads executing at maximum and that is what SEMA 4 is used for so essentially what we have done is we have restricted the count of Trades which are going to access the third party service using your Sor now that we have understood how exactly SEMA 4 is working under the hood let's have some quick code demonstration which will showcases how we can make use of the SEMA for in our code so here I am in the IDE I will create a new class for the demonstration but before I create the class let me tell you briefly what exactly I'm planning to build so I'm planning to build a small use case where we will be making use of some third party service to scrape the data don't worry it's not going to be something difficult it's rather just a dummy kind of method the entire idea is to Showcase how we can use the semaphore rather focusing on the scripting part of it so let's get started and I will create a class I'll call this as a scraper and let's have this m method to begin with and I will create the scrape service which is the service which will be called by the scraper class so just as a good practice I am going to make the scrape Service as a Singleton class not that it's needed for this demonstration but just as a good practice let's have it as a singl ton and let's have this instance and uh what we will be needing is a seap for so let's initialize the semaphor it provides a Constructor wherein we can pass some value so this semap 4 is going to have three permits and we we have a method that is called scrape what this scrape is going to do is let's going to run in a TR catch block let's have this interrupted exception and this will throw a new runtime exception and we will also have a finally why we will look at it in some time so to begin with this thing is going to acquire the SEMA 4 so it will call the do acquire method on the semap 4 and then we have a helper method which is something like invoke scrape bot so invoke a scrape bot is the actual third party service invocation wherein we are going to call the third party service and uh do the scrapping task so we will implement this method in some time but for now let's have the signature here and imagine that uh some thread has acquired the same of 4 permit and it got terminated due to some reason or maybe it went into a deadlock situation so what happens is the Sim for permit is acquired forever but we don't want that to happen right what we want essentially is that once a thread is done with its execution it should be releasing it so similar to the way in which we used to acquire and unlock the logs right we will be using the try and finity block and in The Finity block we will say SEMA 4 do release idea is that in either case of success of failure the execution inside the finary block will take place and that is when we are going to release the semaphore so let's implement the invoke script B method so it's private void invoke script part and this is nothing this is just going to print some message so let's have the message as scrapping data and to mimic some sort of uh execution time we will have some sleep introduced in the code let's say this is for 2 seconds and since we have given thre do sleep we need to handle the exception of interruption and the way we will do this is we will throw a runtime exception with the exception not the best way to do it this should do for the demonstration purposes so now let's go to the main method and try to make use of this so what I'll be doing is I'll be creating a new cast threade pool and using that cast threade pool I'll be submitting the tasks for the script service so let's to that so I'll be making use of the try with resources pattern so executor service service and executors do new cached trade pool and uh let's say I want to do this for 15 times and service. execute new runable and this will call the scrape service scrape so what we have done is we are creating uh this service that is the new cast threade pool and we are supplying this runnable we are calling this script service method inside this runnable 15 times so what we would expect is that at any given point of time only three threads are executing so let's run this and see the output so here is the output we have a scraping data then we have a scraping data then we have a scraping data finally we have a scraping data so what is happening essentially is out of all the 15 times we have submitted the runnable we are just allowing three threads to execute simultaneously so scraping data is printed twice scraping data is printed twice and so and so forth so this is kind of demonstrating that using a SEMA 4 we are restricting how many trades can run at a given point of time please note that how many trades will be actually running at a given point of time will also depend on how many course you have and all those things but the whole idea is that if you use a semop for and some sort of permit then in the worst case possible at Max only those many threads will be running at a given point of time and that is the whole intention and idea behind using the semaphor so now that we have seen the code demonstration let's learn a few more things about the semap for we also have a concept of multiple permits please note that we can acquire multiple permits at a time for this we have an overloaded method of acquire which allows us to enter the number of permits we want so when we call S 4. acquire you can pass a value so let's say Sima for acquire and you pass a value of two so two permits out of three will be taken by the threade however we should always ensure that the number of permits we take should be equal to the number of permits we are returning back by calling the release so like we had passed the number of permits in the acire method we also need to pass the number of permits in the release method as well so if do acquire is two do release should also be two and uh there are some other methods in semap 4 as well let's learn about those the first one is try acquire so when this method is used the thread will try to acquire the permit if there is no permit available the thread won't be blocked rather it can do something else then we have try acquire with timer out which is essentially the same as TR acquire except it accepts some sort of timeout the next one is the available permits and as the name suggests the available permits Returns the number of available permits with a given semaphore the final one on our list is this new semop 4 which accepts count and fairness well this is an overloaded Constructor of the semaphor it accepts a fairness criteria which is a Boolean value and when given as true the threade waiting the longest gets the chance to acquire the permit once it's available with the semap 4 so that's all about the semap 4 it's very easy and intuitive to understand essentially it's a mechanism to introduce a controlled bottleneck situation to ensure that there is an upper limit on how many threads can work on a shared resource at a given point of a Time let's learn about the mutex so what is a mutex mutex is a short form of the word mutual exclusion it is a synchronization mechanism used to control access to a shared resource in a multithreaded environment in Java the primary purpose of a mutex is to ensure that only one thread can access a critical section or shared resource at any given time it prevents raise conditions and ensures data consistency wait a second hold on are you thinking that it sounds very similar to some other Concepts we learned earlier related to multithreading well you are very correct if you are thinking so mutex is nothing but a fancy term for locks and synchronized blocks moreover the concept of mutex encompasses any mechanism that ensures any sort of mutual exclusion it means only one thread can access a critical section at a time in Java this concept is realized through synchronized blocks and lock interface you may have another doubt you may be thinking if the mutex is indeed just a fancy term for locks and synchronization why do we have this concept at all well the reason is somewhat historical in nature the term mutex is a fundamental Concept in computer science and concurrent programming which predates this specific implementations found in languages like Java the concept of a mutex is used widely in both theoretical and practical contexts to describe a mechanism which ensures Mutual exclusion and thereby preventing multiple threats from accessing a shared resource simultaneously so effectively mutex is the term for General concept of mutual exclusion in context of multithreaded environment and the approaches like using a blocks and synchronized blocks Etc are tools which help us in achieving this requirement of mutual exclusion so if you understand any of the concepts like lock synchronization Etc which provide Mutual exclusion of a shared resource you very well understand mutex so that's all about mutex there is nothing new to learn here as such because we have already covered the concepts related to Locks and synchronized blocks in Java Fork join framework is another important Concept in Java let's learn about the same the fork join framework is a concurrency framework which was introduced in Java 7 to simplify the process of parallel programming it is designed to take advantage of multicore processors by dividing tasks into smaller subtasks executing them in parallel and then combining their results together the fourth joint is very similar to executor service which we learned about earlier in the interest of refreshing your memory executor service mainly consists of some sort of data structure which stores the task to be executed and there is a pool of stes which pick the tasks from this data structure and work on the task depending on the use case you could just execute a task and not return anything or you could return a value from the execution of the task so in these two aspects the fork joint framework is exactly the same as the executor service however there are certain dissimilarities between the executor service and folk join framework the fork join is different from the executor service in the aspects of subtask creation and the fork join framework can create subtasks which is not the case with the executor service now let let's learn about the differences between the FK join framework and the executor service the FK joint framework differs from the executor service in two aspects the first one is the task producing subtask and the second one is per threade queuing and work Stealing In the case of folk joint framework a task is capable of creating subtasks to simplify the problem being solved consider this as the standard divide and conquer approach which is not the case with the executor service unlike the executor service where there is a shared task Q the threads in the for joint framework have their dedicated task queue there is also this mechanism of load balancing where a threade could pick task from the other queue this approach is called as work stealing now it's time to learn why do we have for join framework you may be wondering why do we have yet another framework on top of all the things that we have learned so far in the multithreading well well Fork joint framework is not there without any reasons below are a few reasons why the fork joint framework was provided as part of the Java the first one is the utilization of multicore processors modern processors have multiple cores and traditional singlethreaded or poorly managed multithreaded applications do not fully utilize the computational power of these processors the FK join mechanism efficiently manages multiple threads to leverage all the available CPU CES the next one is simplified parallelism writing parallel code manually is a complex and error prone activity the fork join mechanism abstracts out much of this complexity and it makes it easier to develop parallel applications finally on the list of why do we need Fork joint framework we have this concept of efficient work stealing algorithm the framework uses a work stealing algorithm where idle work worker threads still task from The Busy threads this ensures that all the threads remain productive and proove the overall performance so as I told earlier that we have qes on per threade basis and let's say a threade is done with its execution of the tasks in its queue so in that case that threade could dis steal work from the other threads or from the other queue where it has some other tasks aligned to be executed it's very equivalent to the mechanism of load balancing so you see the fork joint framework has a certain pattern to it and due to this we have certain use cases which fit very well with the users of this framework for example sorting a large data set performing operations on large matrices processing large collections of data in a parallel fashion and so on so forth now it's time to learn about the key Concepts in the fork joint framework forking in the fork joint framework is the process of break breaking down large task into smaller independent subtasks which can be executed concurrently this is achieved using the fork method provided by the fork join task class the next concept is that of joining what it refers to is the process of waiting for the completion of a folk task and combining its results this is done using the join method then we have the recursive task the recursive task is an abstract class and it is used for the task which return a result it is parameterized which means that the type is the type of the result which is produced and returned by the task so whenever there is a need to compute a result and return it after completing the task the recursive task class should be used Loosely speaking you could imagine this to be parallel to the callable interface then on the list we have recursive action it is used for tasks which don't return any result it does not accept any types when the task performs an operation which does not need to return a value or a result it should use recursive action and Loosely speaking you could imagine this to be parallel to a runable interface so what is a fog join pool let's learn about the FK joint pool and I promise that this is going to be the last theoretical Topic in this section before we move to the more juicy implementational side of fog joint framework well fog joint pool in Java is a specialized implementation of the executor service interface and it is designed so to support the folk joint framework it is optimized for parallel processing and efficient management of tasks which can be broken down into smaller subtasks below are some key Concepts and aspects which are related to the F joint pool we have this concept of work steing algorithm so F joint pool uses a work stealing algorithm to efficiently manage and balance the workload among threads when a thread finishes its task it can steal task from the work cues of the other threads ensuring that all threats remain productive and it reduces the ideal time as well next we have parallelism so the pool can automatically determine the level of parallelism based on the number of available processors you can also specify the level of parallelism when creating the fork joint pooling stance so we have two operations like fork and Joint so the tasks submitted to the fork join pool can be split into smaller subtasks and their results can be combined together this mechanism supports the divide and conquer approach for solving the complex problems finally The fol Joint pool is designed to manage the instances of the FK Joint Task and its subclasses which are recursive task and recursive action so these tasks incapsulate the logic for splitting the work and combining the results so in nutshell to summarize the theoretical section what we have is a fol join framework and fol join pool is the implementation of it so essentially FK joint pool is a special implementational case of the executor service where the focus is around the parallelism and it does so by utilizing two concepts first is Fork other is join so Fork is basically creating the subtasks creating the sub of a particular task and join is like collecting the results from the fog subtasks so essentially what we are doing using the fog joint framework is dividing the tasks into subtasks and then asking the different threads to work on the subtasks in parallel and finally the result is collected and the final outcome is given back to the caller so in a way it's a parallel implementation it's a parallelized implementation of the divide and conquer strategy so now let's move forward and now that we have learned most of the basic the concepts related to the for joint framework we shall start with some code implementation for these Concepts so I will start by creating this package let's call this package as Fork join and as I mentioned earlier that we have two classes one is the recursive task other is the recursive action so this particular demonstration would be for the recursive task in the next one we will see how we can make use of the recursive action so what I'm planning to implement is a code wherein I would like to search for occurrence of a given number into an integer array so let's start with the implementation so let's create this class and let's call this at search occurrence task this is going to extend the recursive task and this is going to return integer so basically integer is is the type which it accepts so if you see the implementation it's going to extend the fog Joint Task and the fog Joint Task is going to implement future so now going back to the code it's giving certain error so the idea is that it's enforcing to implement certain unimplemented methods so let's do that and the method that we need to implement is called as compute and by the virtue of the type that we provide which is the integer it's going to return integer from this compute Method All right so this particular class will have a few things for example first is it will have an integer array called as ARR then it will have a start index then it will have an end index and it will have a value which is called as search element so let's create the Constructor for this class and we are going to initialize all these values so to begin with we just have this compute method so let's say I am going to provide a private method which is going to search for the occurence in this particular array for a given element so let's call that method as search and uh let's implement it and probably we will return the value so private integer search and uh let's have the value as zero to begin with and uh what we will do is we will start from the start index then it will go till the end index and we increment the index value if the value at index I happens to be the search element then we increment the count and finally when we are done iterating over the array we are going to return the count value so this is the search method so please note that as of now we are not making use of any fog join Concepts so we will introduce the fog join Concepts in a while but for now let's complete the implementation so this is about this class and let's have the runner class wherein we will be invoking this particular compute method somehow that we will see in a moment using a main method so let's create a class and let's call this as F join pool demo and we have a main method so we will have an array let's say the value is up to 100 and we will have a random variable why we are going to need it we will come to see it in a moment and we populate this array with certain random values so for each and every array index we are going to provide certain value and that will be random dot next int so that is 10 + 1 so essentially I am going to populate each index in the array with a value ranging from 1 to 10 and we have a search element which is again a random value So Random dot next int then we have 10 + 1 and what we are going to do is we are going to create a for join pool so new for join pool and remember that I told in the theory section that if you don't Pro provide the number of processors it will by default figure out how much parallelism is required but in this case let's provide the number of processors as well so runtime Dog runtime. available processors and it's complaining that we should close this resource so probably we can go ahead with drive with resources all right now what we need to do is we need to make use of the fne pool and the way to do this is we need to create a instance of this task which has extended this recursive task so let's do that let's create an instance of this task which is search occurring task and let's call this as task new search occurrence task let's pass the values which is the array start would be zero end would be array do length minus one these are the indices of the array then the search element and we are going to call pull do invoke and here we will pass the for Joint Task and let's capture this as the value of occurrence all right so let's print the value that the array is arrays dot two string AR and finally system out print Ln percent D found percent D times then search element and occurrence and it should be print F not print Ln so let's correct this and uh now let's run this code and say its output so please note that we have not made use of any fol joint pool concept as of now we will do so in a moment so let's run this and we can see that the value 9 has been found six times so you could go ahead implement this code or probably copy the code from the GitHub link that will be shared in this video and you could try to execute this and from this particular value you could validate it so it's working fine I have validated it now what we will do is we will try to reduce the folk and join Concepts so in order to do so what we do is we go to the compute method and we first of all compute the size of the array which is end minus start + 1 and if the size happens to be greater than 50 then we are going to do some operations otherwise I am just going to call the vanilla search method which is return this search method method that we were using anyway so when the size happens to be greater than 50 we will divide this task in two halves and the way to do this is find the middle index say start plus End ided by two and in order to create the fork of the task we will have to first create the task objects so what we do is we can create the task object which is search uring task let's call this as task one then new search occurrence task pass the value the start value is the start index the end value becomes the mid and then we have the search element next we create the other task which is Task two and then we call search aen task then we have array and the start value becomes the mid + one and then the end value becomes the end and then then we have the search element so if you are familiar with the concept of binary search or any divide and concur algorithm implementation you would find it very similar so essentially what we are doing is let's say there is an array with some values right something like this so what we are doing is we have to do some processing some operation on this array so essentially what we are saying is that all right let's say if I have these many values which is let's say six values so I don't want to do operations on the entire array rather what I want to do is I want to split it so if I were to search this array for all the six elements it will take some time what if if I wanted to search for some value on this particular array itself it will take less time right because we are doing the search operation on a smaller sized array so that is the whole concept where we are finding the mid value which is let's say somewhere here and from here to here this is my first section of the array in which I need to do the search and this is the second section of the array in which I need to do the search and this needs to be done till the time this threshold is not M so for example let's say this array is somewhat of let's say bigger size maybe 100 or 200 200 then in that case first split would be 100 100 second split would be 50/50 and when the size is less than 50 then in that case it will come to this section and it will say all right the size is less than 50 the size is less than the threshold then in that case we can go ahead and do the normal search but till the time size is greater than the threshold that is 50 in this case it will go and break the task into smaller tasks and that is where we have made use of this task so the next process in this implementation would be to call Fork method on these tasks so task one. Fork then task two do Fork so what happens is that this task. Fork is going to create another subtask so if you click on Fork you can see that it is going to return the fork Joint Task itself which is basically this recursive task whatever implementation you have here since this class is imple minting recursive task then this particular Fork will also return this recursive task which is the search occurrence task but for a lesser search space right so if that is clear let's move ahead and understand how we can face the value from this subtasks that we have created and the way to do that is by calling the join operation so in a way you could try to correlate this with the way in which the recursion works right and it is pretty much evident by the name as well so basically it's nothing but a kind of recursive call in a multithreaded context wherein you have created different subtasks and all those smaller subtasks could be picked up by different threads and could be executed in parallel so it's nothing but some sort of recursion on stereoids due to reason that we are making use of parallelism and multithreading and all those Concepts which are going to give us some boost with respect to the performance so now let's move and see how we can collect the value so the way to collect the value out of a task is by calling dot join method so let's say task one do join if you try to capture it it will be some integer why integer because that is the parameter that we have provided here so you could do this way or probably we could just simplify it by returning task one do join plus task 2. join and that's the whole implementation so now let's run this code and see the output after executing this code I will give you an another work through so things will be much more clearer so see this value has been found this many times so basically 10 has been found these many times so basically what we have done here is the that we have implemented one of the implementations of the task which is this recursive task and one of the implementations of the fork Joint Task and uh we have certain data structure on which this task is going to be performed and the data structure of the choice here is an integer array and our use case is to find how many times a given value is going to be repeated or is going to occur in the given array and that is the search part of it and that is what we had in the beginning then what we did is we started with the implementation for the compute and in the compute we first found out what is the current size of the array on which we are performing this operation and we had a check that if the size happens to be less than 50 then we are simply going to make use of this search method and if that is not the case then we are going to break this larger task into smaller tasks smaller subtasks and the way to break this is find the middle point in the array the middle index in the array and then create two tasks out of it wherein we have the array itself then we have a start then we have a mid value and the element that we need to find and in the second task we are going to have the same array and the search space is provided from the mid + one as the start value and end as the end value and search element is any way the element that we need to find and after we have created this template for these two tasks we are going to call Fork on these two tasks and when a fork is called what essentially happens behind the scene is this thing is called recursively with this particular search space this particular search space so the search space is going to reduce it's going to get smaller and smaller and likewise when this Fork is called similar thing is going to happen and finally this will keep on happening till the time we have the size the search space greater than 50 and after some time the recursion will reach its base case and then we will like to retrieve the value from that recursive task that we have executed and the way to do so is by calling Dot join there are some other methods as well but in this example we are going to stick with DOT join so let's say task one has accumulated certain value and task two has accumulated certain value so both the values will be added because consider this as a binary tree where we are making calls in two directions in the left side and in the right side and once you are done with the traversal at every point of time from every node you are going to get some value so that is what the intent is and that is the way to visualize this code so that is how we are going to make use of the recursive task to implement Fork join framework and do this operation so before we move to the next implementation which is for the recursive action I would like to give you a small homework and what you could try is you could try to implement this for the use case where we wish to find out the summation of all the elements in a given array so it would be something similar to this but I think you should try it out on your own and it would really help you in cementing the concepts related to the recursive tasks so now let's move to the implementation of the other thing which is the recursive action so let's create an another class and uh let's call this as workload splitter and what I'm trying to build here is that there is a class and it has some workload and based on certain workload it's going to either do its operation on its own or else it's going to split the workload or the task rather into two halves so this is what we are going to implement so let's start with the implementation and this workload splitter class is going to extend recursive action and as I told earlier in the theory section that this recursive action is not going to accept or expect any sort of parameter and uh neither does it return anything so let's implement the unimplemented methods and again the unimplemented method is compute the only difference is that since it does not return anything we have a void over here there is nothing to return right so let's have a value and maybe call this as a workload let's have a Constructor initialize this value and in the workload what we are going to do is we are going to inspect the value of workload if the value happens to be greater than 16 then we are going to do certain things otherwise this particular task is going to be implemented without any splitting so let's print some message we are not going to have any fancy implementation as such rather we are just going to print certain messages to explain the concept so work load within limits task being executed for workload and then the workload value and what happens when the workload is greater than 16 well first start by printing some message workload too big thus splitting and then the workload is given in the message so first of all we find the first workload and this is going to be workload divided by two then we find the second workload and this is going to be workload minus first workload right so again very similar to the recursive task that we had in the previous section in the previous example rather we will create two tasks here for the workload splitter so maybe call this as first split then workload splitter pass the first workload value and then workload is splitter second split then call workload splitter provide the second workload and we can call first split do Fork then second split do Fork all right and that's it since we we are not intending to extract some value and that is the reason we have made use of the recursive action so we are not going to call any sort of join or any value that we need to fetch out of this so that is all with respect to the implementation here we are just making certain load balancing kind of mechanism wherein if needed if the workload is too high then in that case we are going to Fork two new tasks and those tasks will be working on that workload so let's say to begin with if the value is 64 then the value is certainly greater than 16 then the workload will be divided as 32 32 so the first load is 32 second load is 32 again and again this value is greater than 16 so again the load will be divided into two so the first load becomes 16 second load becomes 16 and since that is the value we don't need to go inside this thing rather we will come to this thing and we will do the execution on its own so as I told earlier that there is no any execution as such rather it is just going to mimic certain Behavior around the workload by making use of this recursive action so now let's demonstrate this implementation and uh write the code for the same so let's create a class let's call this as workload split demo let's have the main method and uh we will create the fog join pool so let's call this as pool and new for join pool again let's pass the available course and again this will complain regarding an open resource so we can resolve this by putting inside the try with resources block and the way to use this is create an object for the recursive action class that we have created which is the workload splitter so let's call it as a splitter then workload splitter pass the value as 128 then call pool. invoke and let's pass the splitter object and that's it so so this is what we have implemented let's execute this task let's execute this code and see the output so here is what we have to begin with workload to Big this is splitting workload too big this is splitting and so on so forth so 128 got to split into two halves 64 and 64 32 got split into two halves 64 rather got split into two halves 32 and 32 and uh so basically 32 and 32 and this one is 32 and 322 so as I told earlier that these things are executed all these tasks are executed by threads in parallel so based on the thread scheduling you are seeing that we are having some sort of not in sync kind of response but the idea is that the number is getting split into two halves so 64 64 each 64 is getting split into two halves right one half here other half here and further the 32 is getting split into 16 and since 16 happens to be well within the reasonable limits for the workload we have this message that workload within the limits task being executed for the given workload so these are the messages that we are seeing so that's it about the implementation of the recursive action and that is the way in which you could implement it so that should be it for this particular topic of fork join in Java thanks for staying with me till the end of this video as per one statistics less than 10% people finish watching a tutorial video once they start watching well guess what you are the top 10% of the lot you are awesome so where Do We Go From Here I am hopeful that you learned a few important Concepts which lay the foundation's stone in learning the super awesome yet complex topics of multithreading and concurrency I sincerely request you to start implementing these new found Concepts in your daytoday software engineering activities if not at least try reading some code which makes use of the multithreading Concepts doing these things will cement all the new skills that you have learned and acquired with me in the last couple of hours needless to say the concepts in this video are introductory and foundational in nature yet they are very important so what this means is there are even more Exquisite and complex topics which I did not cover in this video but whatever you learned should be good enough to get you started good news is that I am already working on an another video which will cover more advanced Topics in more details I hope you are excited so to summarize first thing is start implementing the concepts learned in this video then read code which makes use of the multithreading and concurrency third step is watch out for the next video on the more advanced topics in multi trading and most importantly did I tell you that I also have an YouTube channel here is the link please do subscribe it would mean a world to me with this I sincerely thank you once again for your time and attention and I hope that you found this video useful hope to connect with you again in my next video tutorial thanks for watching
